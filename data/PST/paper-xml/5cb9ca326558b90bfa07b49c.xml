<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The opinion corner What makes good research in software engineering?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2002-06-13">13 June 2002</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mary</forename><surname>Shaw</surname></persName>
							<email>mary.shaw@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The opinion corner What makes good research in software engineering?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2002-06-13">13 June 2002</date>
						</imprint>
					</monogr>
					<idno type="MD5">C3207208D9D085500B7BCF3617F2DF8B</idno>
					<idno type="DOI">10.1007/s10009-002-0083-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Physics, biology, and medicine have wellrefined public explanations of their research processes. Even in simplified form, these provide guidance about what counts as "good research" both inside and outside the field. Software engineering has not yet explicitly identified and explained either our research processes or the ways we recognize excellent work. Science and engineering research fields can be characterized in terms of the kinds of questions they find worth investigating, the research methods they adopt, and the criteria by which they evaluate their results. I will present such a characterization for software engineering, showing the diversity of research strategies and the way they shift as ideas mature. Understanding these strategies should help software engineers design research plans and report the results clearly; it should also help explain the character of software engineering research to computer science at large and to other scientists.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many sciences have good explanations of their research strategies. These explanations include not only detailed guidance for researchers but also simplified views for the public and other observers. Acceptance of their results relies on the process of obtaining the results as well as analysis of the results themselves. Schoolchildren learn the experimental model of physics: hypothesis, controlled experiment, analysis, and possible refutation. The public understands large-scale double-blind medical studies well enough to discuss the risks of experimental treatment, the ethics of withholding promising treatment from the control group, and the conflicts of interest that are addressed by the blinding process.</p><p>Software engineering does not have this sort of wellunderstood guidance. Software engineering researchers rarely write explicitly about their paradigms of research and their standards for judging quality of results. A number of attempts to characterize software engineering research have contributed elements of the answer, but they do not yet paint a comprehensive picture. In 1980, I <ref type="bibr" target="#b6">[7]</ref> examined the relation of engineering disciplines to their underlying craft and technology and laid out expectations for an engineering discipline for software. In 1984-1985, Redwine, Riddle, and others <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> proposed a model for the way software engineering technology evolves from research ideas to widespread practice. More recently, software engineering researchers have criticized common practice in the field for failing to collect, analyze, and report experimental measurements in research reports <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. In 2001 I <ref type="bibr" target="#b7">[8]</ref> presented preliminary sketches of some of the successful paradigms for software engineering research, drawing heavily on examples from software architecture.</p><p>Scientific and engineering research fields can be characterized by identifying what they value:</p><p>• What kinds of questions are "interesting"? Redwine and Riddle presented timelines for several software technologies as they progressed through these phases up until the mid-1980s. I presented a similar analysis for the maturation of software architecture in the 1990s <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our interest here is in the first three phases, i.e., the research phases. Software engineering research is intended to help improve the practice of software development, so research planning should make provisions for the transition. The Redwine-Riddle data suggests that around 10 of the 15-20 years of evolution are spent in concept formation and in development and extension (still more time is spent in basic research, but it is very difficult to identify the beginning of this phase). As a result, full understanding of research strategy must account for the accumulation of evidence over time as well as for the form and content of individual projects and papers.</p><p>The IMPACT project <ref type="bibr" target="#b2">[3]</ref> is tracing the path from research into practice. The objectives of the project include identifying the kinds of contributions that have substantial impact and the types of research that are successful. Preliminary results are now being discussed at conferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Prior reflections on software engineering and related research</head><p>Software engineering research includes, but is not limited to, experimental research. Further, it resembles in some respects research in human-computer interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1">Critiques of experimental software engineering</head><p>In 1993, Basili laid out experimental research paradigms appropriate for software engineering <ref type="bibr" target="#b0">[1]</ref>. Later, Tichy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and colleagues criticized the lack of quantitative experimental validation reported in conference papers: "Computer scientists publish relatively few papers with experimentally validated results . . . The low ratio of validated results appears to be a serious weakness in CS research. This weakness should be rectified." <ref type="bibr" target="#b8">[9]</ref> They classified 246 papers in computer science and, for comparison, 147 papers in two other disciplines, according to the type of contribution in the article. The majority of the papers (259 of 403) produced design and modeling results. They then assessed each paper's evaluation of its results on the basis of the fraction of the article's text devoted to evaluation. They found, for example, that hypothesis testing was rare in all samples, that a large fraction (43%) of computer science design and modeling papers lacked any experimental evaluation, and that software engineering samples were worse than computer science in general.</p><p>Zelkowitz and Wallace <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> built on Basili's description of experimental paradigms and evaluated over 600 computer science papers and over 100 papers from other disciplines published over a 10-year period. Again, they found that too many papers had no experimental validation or only informal validation, though they did notice some progress over the 10-year period covered by their study.</p><p>These critiques start from the premise that software engineering research should follow a classical experimental paradigm. Here I explore a different question: what are the characteristics of the software engineering research that the field recognizes as being of high quality?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Analyzing research approaches with pro forma abstracts</head><p>Newman compared research in human-computer interaction (HCI) to research in engineering <ref type="bibr" target="#b3">[4]</ref>. He characterized engineering practice, identified three main types of research contributions, and performed a preliminary survey of publications in five engineering fields. He found that over 90% of the contributions were of three kinds:</p><p>EM Enhanced analytical modeling techniques, based on relevant theory, that can be used to tell whether the design is practicable or to make performance predictions. ES Enhanced solutions that overcome otherwise insoluble aspects of problems, or that are easier to analyze with existing modeling techniques. ET Enhanced tools and methods for applying analytical models and for building functional models or prototypes <ref type="bibr" target="#b3">[4]</ref>.</p><p>Newman created pro forma abstracts -templates for stylized abstracts what would capture the essence of the papers -for each of these types of contributions. For example, the pro forma abstract for enhanced modeling techniques is "Existing &lt;model-type&gt; models are deficient in dealing with &lt;properties&gt; of &lt;solution strategy&gt;. An enhanced &lt;model-type&gt; is described, capable of providing more accurate analyses/predictions of &lt;properties&gt; in &lt;solution strategy&gt; designs. The model has been tested by comparing analyses/predictions with empirically measured values of &lt;properties&gt;." <ref type="bibr" target="#b3">[4]</ref> He found that in order to account for a comparable fraction of the HCI literature, he needed two more templates, for "radical solutions" and for "experience and/or heuristics". Newman reported that in addition to helping to identify the kind of research reported in a paper, the pro forma abstracts also helped him focus his attention while reading the paper. It seems reasonable to assume that if authors were more consciously aware of typical paper types, they would find it easier to write papers that presented their results and supporting evidence clearly. The approach of characterizing papers through pro forma abstracts is also useful for software engineering, though a more expressive descriptive model as described below provides better matches with the papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.3">Broad view of research</head><p>Brooks reflected on the tension in human-computer interaction research between:</p><p>• "narrow truths proved convincingly by statistically sound experiments" and • "broad 'truths', generally applicable, but supported only by possibly unrepresentative observations" <ref type="bibr" target="#b1">[2]</ref>.</p><p>The former satisfy the gold standard of science, but are few and narrow compared to the decisions designers make daily. The latter provide pragmatic guidance, but at risk of over-generalization. Brooks proposes to relieve the tension through a certainty-shell structure -to recognize three nested classes of results, • Findings: well-established scientific truths, judged by truthfulness and rigor; • Observations: reports on actual phenomena, judged by interestingness; • Rules of thumb: generalizations, signed by their author but perhaps incompletely supported by data, judged by usefulness with freshness as a criterion for all three. This tension is as real in software engineering as in human-computer interaction. Observations and rules of thumb provide valuable guidance for practice when findings are not available. They also help to understand an area and lay the groundwork for the research that will, in time, yield findings.</p><p>2 Questions, results, and validation in software engineering</p><p>Generally speaking, software engineering researchers seek better ways to develop and evaluate software. They are motivated by practical problems, and key objectives of the research are often quality, cost, and timeliness of software products. This section presents a model that explains software engineering research papers by classifying the types of research questions they ask, the types of results they produce, and the character of the validation they provide. This model has evolved over several years. It refines the version I presented at ICSE 2001 <ref type="bibr" target="#b7">[8]</ref> based on discussion of the model in a graduate class and review of abstracts submitted to ICSE 2002. Its status is, in Brooks' sense, a set of observations, perhaps becoming a generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Types of research questions</head><p>Research questions may be about methods for developing software, about methods for analyzing software, about the design, evaluation, or implementation of specific systems, about generalizations over whole classes of systems, or about the sheer feasibility of a task. Table <ref type="table" target="#tab_1">1</ref> shows the types of research questions software engineers ask, together with some examples of specific typical questions.</p><p>Among ICSE submissions, the most common kind of paper reports an improved method or means of developing software. Papers about methods for analysis are also fairly common, principally analysis of correctness (testing and verification).</p><p>Looking back over the history of software engineering, there is some indication that the types of questions have changed as the field matures. For example, generalizations, especially in the form of more formal models, are becoming more common, and feasibility papers seem to be becoming less common as the field matures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Types of research results</head><p>Research yields new knowledge. This knowledge is expressed in the form of a particular result. The result may be a specific procedure or technique for software development or for analysis. It may be more general, capturing a number of specific results in a model; such models are of many degrees of precision and formality. Sometimes, the result is the solution to a specific problem or the outcome of a specific analysis. Finally, as Brooks observed, observations and rules of thumb   . . . a system constructed like this would . . .</p><p>. . . this model seems reasonable Note that if the original question was about feasibility, a working system, even without analysis, can be persuasive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blatant assertion</head><p>No serious attempt to evaluate result</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Types of research validations</head><p>Good research requires not only a result, but also clear and convincing evidence that the result is sound. This evidence should be based on experience or systematic analysis, not simply persuasive argument or textbook examples. Table <ref type="table" target="#tab_3">3</ref> shows some common types of validation, indicating that validation in practice is not always clear and convincing. The most common kinds of validation among ICSE papers are experience in actual use and systematic analysis. A significant number of ICSE submissions depend only on blatant assertion to demonstrate the validity of their results, or offer no evidence at all; such submissions are only rarely accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Research strategies</head><p>Section 2 identifies the three important aspects of an individual research result as reported in a typical conference or journal paper. It is clear that the spectrum of good research strategies includes experimental computer science in the sense of <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>; it is also clear that the spectrum is much broader than just experimental research. Of course, not all the combinations of question, result, and validation make sense.</p><p>Inspection of Tables 1-3 suggests combinations that make sense. Continuing to report on what strategies are accepted, rather than setting a prescriptive standard, in this section I observe and comment on some of the patterns that appear in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Creating research strategies</head><p>The most common research strategy in software engineering solves some aspect of a software development problem by producing a new procedure or technique and validating it by analysis or by discussing an example of its use; examples of use in actual practice are more compelling than examples of use on idealized problems.</p><p>Another common research strategy provides a way to analyze some aspect of software development by developing an analytic, often formal, model and validating it through formal analysis or experience with its use. Table <ref type="table" target="#tab_4">4</ref> shows the strategies for 40 research papers accepted for ICSE 2002, based on the submitted abstracts for those papers. Some papers are not included because the strategy was not clear from the abstract.</p><p>These descriptions are consistent with Newman's pro forma abstracts. Those templates identify sets of compatible questions, results, and validations. For example, the "enhanced model" quoted in Sect. 1.2.2 corresponds to a generalization or characterization question answered by an analytic or empirical (or precise descriptive) model, validated by empirical analysis or controlled experiment. By packaging several choices together and naming the set, Newman identifies the selected strategies clearly. However, attempts to apply them to the software engineering literature revealed shortcomings in coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Building good results from good papers</head><p>This discussion has focused on individual results as reported in conference and journal papers. Major results, however, gain credibility over time as successive papers provide incremental improvement of the result and progressively stronger credibility. Assessing the significance of software engineering results should be done in this larger context.</p><p>As increments of progress appear, they offer assurance that continued investment in research will pay off. Thus initial reports in an area may be informal and qualitative but present a persuasive case for exploratory research, while later reports present empirical and later formal models that justify larger investment. This pattern of growth is consistent with the Redwine-Riddle model of technology maturation.</p><p>The model presented here does not address this cumulative confidence building.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary</head><p>Software engineering will benefit from a better understanding of the research strategies that have been most successful. The model presented here reflects the character of the discipline: it identifies the types of questions software engineers find interesting, the types of results we produce in answering those questions, and the types of evidence that we use to evaluate the results.</p><p>Research questions are of different kinds, and research strategies vary in response. The strategy of a research project should select a result, an approach to obtaining the result, and a validation strategy appropriate to the research question. More explicit awareness of these choices may help software engineers design research projects and report their results; it may also help readers read and evaluate the literature.</p><p>The questions of interest change as the field matures. One indication that ideas are maturing is a shift from qualitative and empirical understanding to precise and quantitative models.</p><p>This analysis has considered individual research reports, but major results that influence practice rely on accumulation of evidence from many projects. Each individual paper thus provides incremental knowledge, and collections of related research projects and reports provide both confirming and cumulative evidence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Redwine and Riddle<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> reviewed a number of software technologies to see how they develop and propagate. They found that it typically takes 15-20 years for a technology to evolve from concept formulation to the point where it's ready for popularization. They identify six typical phases:</figDesc><table><row><cell>• Basic research. Investigate basic ideas and concepts,</cell></row><row><cell>put initial structure on the problem, frame critical re-</cell></row><row><cell>search questions.</cell></row><row><cell>• Concept formulation. Circulate ideas informally, de-</cell></row><row><cell>velop a research community, converge on a compatible</cell></row><row><cell>set of ideas, publish solutions to specific subproblems.</cell></row><row><cell>• Development and extension. Make preliminary use of</cell></row><row><cell>the technology, clarify underlying ideas, generalize the</cell></row><row><cell>approach.</cell></row><row><cell>• Internal enhancement and exploration. Extend ap-</cell></row><row><cell>proach to another domain, use technology for real prob-</cell></row><row><cell>lems, stabilize technology, develop training materials,</cell></row><row><cell>show value in results.</cell></row><row><cell>• What kinds of results help to answer these ques-</cell></row><row><cell>tions, and what research methods can produce these</cell></row><row><cell>results?</cell></row><row><cell>• What kinds of evidence can demonstrate the validity of</cell></row><row><cell>a result, and how are good results distinguished from</cell></row><row><cell>bad ones?</cell></row></table><note><p>In this paper I attempt to make generally accepted research strategies in software engineering explicit by examining research in the area to identify what is widely accepted in practice. 1.1 Software technology maturation • External enhancement and exploration. Similar to internal, but involving a broader community of people who weren't developers, show substantial evidence of value and applicability. • Popularization. Develop production-quality, supported versions of the technology, commercialize and market technology, expand user community.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Research questions in software engineering may be good preliminary results. Table2lists these types, together with some examples of specific typical questions.By far, the most common kind of ICSE paper reports a new procedure or technique for software development or analysis. It may be described narratively, or it may be em-</figDesc><table><row><cell>Type of question</cell><cell>Examples</cell></row><row><cell>Method or means of</cell><cell>How can we do/create (or automate doing) X?</cell></row><row><cell>development</cell><cell>What is a better way to do/create X?</cell></row><row><cell>Method for analysis</cell><cell>How can I evaluate the quality/correctness of X?</cell></row><row><cell></cell><cell>How do I choose between X and Y?</cell></row><row><cell>Design, evaluation,</cell><cell>What is a (better) design or implementation for application X?</cell></row><row><cell>or analysis of</cell><cell>What is property X of artifact/method Y?</cell></row><row><cell cols="2">a particular instance How does X compare to Y?</cell></row><row><cell></cell><cell>What is the current state of X/practice of Y?</cell></row><row><cell>Generalization or</cell><cell>Given X, what will Y (necessarily) be?</cell></row><row><cell>characterization</cell><cell>What, exactly, do we mean by X?</cell></row><row><cell></cell><cell>What are the important characteristics of X?</cell></row><row><cell></cell><cell>What is a good formal/empirical model for X?</cell></row><row><cell></cell><cell>What are the varieties of X, how are they related?</cell></row><row><cell>Feasibility</cell><cell>Does X even exist, and if so what is it like?</cell></row><row><cell></cell><cell>Is it possible to accomplish X at all?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Research results in software engineering</figDesc><table><row><cell></cell><cell>bodied in a tool. Analytic models and descriptive models</cell></row><row><cell></cell><cell>are also common; analytic models support predictive an-</cell></row><row><cell></cell><cell>alysis, whereas descriptive models explain the structure</cell></row><row><cell></cell><cell>of a problem area or expose important design decisions.</cell></row><row><cell></cell><cell>Empirical models backed up by good statistics are un-</cell></row><row><cell></cell><cell>common.</cell></row><row><cell>Type of result</cell><cell>Examples</cell></row><row><cell>Procedure or</cell><cell>New or better way to do some task, such as design, implementation,</cell></row><row><cell>technique</cell><cell>measurement, evaluation, selection from alternatives</cell></row><row><cell></cell><cell>Includes operational techniques for implementation, representation,</cell></row><row><cell></cell><cell>management, and analysis, but not advice or guidelines</cell></row><row><cell>Qualitative or</cell><cell>Structure or taxonomy for a problem area; architectural style,</cell></row><row><cell>descriptive model</cell><cell>framework, or design pattern; non-formal domain analysis</cell></row><row><cell></cell><cell>Well-grounded checklists, well-argued informal generalizations,</cell></row><row><cell></cell><cell>guidance for integrating other results,</cell></row><row><cell>Empirical model</cell><cell>Empirical predictive model based on observed data</cell></row><row><cell>Analytic model</cell><cell>Structural model precise enough to support formal analysis or</cell></row><row><cell></cell><cell>automatic manipulation</cell></row><row><cell>Notation or tool</cell><cell>Formal language to support technique or model (should have</cell></row><row><cell></cell><cell>a calculus, semantics, or other basis for computing or inference)</cell></row><row><cell></cell><cell>Implemented tool that embodies a technique</cell></row><row><cell>Specific solution</cell><cell>Solution to application problem that shows use of software</cell></row><row><cell></cell><cell>engineering principles -may be design, rather than implementation</cell></row><row><cell></cell><cell>Careful analysis of a system or its development</cell></row><row><cell></cell><cell>Running system that embodies a result; it may be the carrier of the</cell></row><row><cell></cell><cell>result, or its implementation may illustrate a principle that can be</cell></row><row><cell></cell><cell>applied elsewhere</cell></row><row><cell>Answer or judgment</cell><cell>Result of a specific analysis, evaluation, or comparison</cell></row><row><cell>Report</cell><cell>Interesting observations, rules of thumb</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Validation techniques in software engineering</figDesc><table><row><cell cols="2">Type of validation Examples</cell></row><row><cell>Analysis</cell><cell cols="2">I have analyzed my result and find it satisfactory through ...</cell></row><row><cell></cell><cell>(formal analysis)</cell><cell>. . . rigorous derivation and proof</cell></row><row><cell></cell><cell>(empirical model)</cell><cell>. . . data on controlled use</cell></row><row><cell></cell><cell>(controlled</cell><cell>. . . carefully designed statistical</cell></row><row><cell></cell><cell>experiment)</cell><cell>experiment</cell></row><row><cell>Experience</cell><cell cols="2">My result has been used on real examples by someone other than me, and</cell></row><row><cell></cell><cell cols="2">the evidence of its correctness/usefulness/effectiveness is . . .</cell></row><row><cell></cell><cell>(qualitative model)</cell><cell>. . . narrative</cell></row><row><cell></cell><cell>(empirical model,</cell><cell>. . . data, usually statistical, on practice</cell></row><row><cell></cell><cell>(notation, tool)</cell><cell>. . . comparison of this with similar results in</cell></row><row><cell></cell><cell>technique)</cell><cell>actual use</cell></row><row><cell>Example</cell><cell cols="2">Here's an example of how it works on</cell></row><row><cell></cell><cell>(toy example)</cell><cell>. . . a toy example, perhaps motivated</cell></row><row><cell></cell><cell></cell><cell>by reality</cell></row><row><cell></cell><cell>(slice of life)</cell><cell>. . . a system that I have been developing</cell></row><row><cell>Evaluation</cell><cell cols="2">Given the stated criteria, my result...</cell></row><row><cell></cell><cell>(descriptive model)</cell><cell>. . . adequately describes the phenomena</cell></row><row><cell></cell><cell></cell><cell>of interest . . .</cell></row><row><cell></cell><cell>(qualitative model)</cell><cell>. . . accounts for the phenomena of interest . . .</cell></row><row><cell></cell><cell>(empirical model)</cell></row></table><note><p>. . . is able to predict . . . because . . . , or . . . gives results that fit real data . . . Includes feasibility studies, pilot projects Persuasion I thought hard about this, and I believe . . . (technique) . . . if you do it the following way, . . . (system)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Research strategies for ICSE 2002, based on submitted abstracts</figDesc><table><row><cell>Question</cell><cell>Result</cell><cell>Validation</cell><cell>Count</cell></row><row><cell>Development method</cell><cell>Procedure</cell><cell>Analysis</cell><cell>3</cell></row><row><cell>Development method</cell><cell>Procedure</cell><cell>Experience</cell><cell>4</cell></row><row><cell>Development method</cell><cell>Procedure</cell><cell>Example</cell><cell>7</cell></row><row><cell>Development method</cell><cell>Qualitative model</cell><cell>Experience</cell><cell>2</cell></row><row><cell>Development method</cell><cell>Qualitative model</cell><cell>Persuasion</cell><cell>1</cell></row><row><cell>Development method</cell><cell>Analytic model</cell><cell>Experience</cell><cell>3</cell></row><row><cell>Development method</cell><cell>Notation or tool</cell><cell>Analysis</cell><cell>1</cell></row><row><cell>Development method</cell><cell>Notation or tool</cell><cell>Experience</cell><cell>1</cell></row><row><cell>Development method</cell><cell>Notation or tool</cell><cell>Example</cell><cell>2</cell></row><row><cell>Analysis method</cell><cell>Procedure</cell><cell>Analysis</cell><cell>1</cell></row><row><cell>Analysis method</cell><cell>Procedure</cell><cell>Experience</cell><cell>3</cell></row><row><cell>Analysis method</cell><cell>Procedure</cell><cell>Example</cell><cell>2</cell></row><row><cell>Analysis method</cell><cell>Analytic model</cell><cell>Analysis</cell><cell>1</cell></row><row><cell>Analysis method</cell><cell>Analytic model</cell><cell>Experience</cell><cell>1</cell></row><row><cell>Analysis method</cell><cell>Analytic model</cell><cell>Example</cell><cell>2</cell></row><row><cell>Analysis method</cell><cell>Tool</cell><cell>Example</cell><cell>1</cell></row><row><cell>Evaluation of instance</cell><cell>Specific analysis</cell><cell>Analysis</cell><cell>3</cell></row><row><cell>Evaluation of instance</cell><cell>Specific analysis</cell><cell>Example</cell><cell>1</cell></row><row><cell>Evaluation of instance</cell><cell>Answer</cell><cell>Analysis</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This paper was presented as an invited lecture at ETAPS 2002, the Joint European Conferences on Theory and Practice of Software, in April 2002 in Grenoble.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The development of these ideas has benefited from discussion with colleagues at Carnegie Mellon, at open discussion sessions at the FSE Conference, and with the program</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>committee of ICSE 2002. The work has been supported by the A.J. Perlis Chair at Carnegie Mellon.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The experimental paradigm in software engineering</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental software engineering issues: critical assessment and future directives. Proc Dagstuhl-Workshop</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Dieter</forename><surname>Rombach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Basili</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Selby</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">706</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grasping reality through illusion-interactive graphics serving science</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 1988 ACM SIGCHI Human Factors in Computer Systems Conference (CHI &apos;88)</title>
		<meeting>1988 ACM SIGCHI Human Factors in Computer Systems Conference (CHI &apos;88)</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Impact Project: Determining the impact of software engineering research upon practice. Panel summary</title>
	</analytic>
	<monogr>
		<title level="m">Proc 23rd International Conference on Software Engineering (ICSE 2001)</title>
		<meeting>23rd International Conference on Software Engineering (ICSE 2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">W: A preliminary analysis of the products of HCI research, using pro forma abstracts</title>
		<author>
			<persName><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI &apos;94)</title>
		<meeting>1994 ACM SIGCHI Human Factors in Computer Systems Conference (CHI &apos;94)</meeting>
		<imprint>
			<biblScope unit="page" from="278" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DoD Related software technology requirements, practices, and prospects for the future</title>
		<author>
			<persName><forename type="first">S</forename><surname>Redwine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984-06">June 1984</date>
			<biblScope unit="volume">1788</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">T: Software technology maturation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Redwine</surname></persName>
		</author>
		<author>
			<persName><surname>Riddle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 8th International Conference on Software Engineering</title>
		<meeting>8th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="1985-05">May 1985</date>
			<biblScope unit="page" from="189" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prospects for an engineering discipline of software</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="1990-11">November 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The coming-of-age of software architecture research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 23rd International Conference on Software Engineering (ICSE 2001)</title>
		<meeting>23rd International Conference on Software Engineering (ICSE 2001)</meeting>
		<imprint>
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EA: Experimental evaluation in computer science: a quantitative study</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Tichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Syst Software</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">WF: Should computer scientists experiment more? 16 reasons to avoid experimentation</title>
		<author>
			<persName><surname>Tichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comp</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experimental validation in software engineering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Zelkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Software Technol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="735" to="744" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experimental models for validating technology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Zelkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
