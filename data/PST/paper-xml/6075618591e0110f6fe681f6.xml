<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compiler Toolchains for Deep Learning Workloads on Embedded Platforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-08">8 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Max</forename><surname>Sponner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bernd</forename><surname>Waschneck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Akash</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><surname>Com</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Infineon Technologies Dresden GmbH &amp; Co. KG</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Infineon Technologies Dresden GmbH &amp; Co. KG</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Dresden University of Technology</orgName>
								<address>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>March 22</addrLine>
									<postCode>2021</postCode>
									<settlement>Burlingame &apos;21, Burlingame</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compiler Toolchains for Deep Learning Workloads on Embedded Platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-08">8 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.04576v1[cs.PL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>embedded</term>
					<term>deep learning compiler</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the usage of deep learning becomes increasingly popular in mobile and embedded solutions, it is necessary to convert the framework-specific network representations into executable code for these embedded platforms. This paper consists of two parts: The first section is made up of a survey and benchmark of the available open source deep learning compiler toolchains, which focus on the capabilities and performance of the individual solutions in regard to targeting embedded devices and microcontrollers that are combined with a dedicated accelerator in a heterogeneous fashion. The second part explores the implementation and evaluation of a compilation flow for such a heterogeneous device and reuses one of the existing toolchains to demonstrate the necessary steps for hardware developers that plan to build a software flow for their own hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts:</head><p>? Computing methodologies ? Artificial intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As AI is moving closer to the edge, the limitations of the popular deep learning frameworks in regard to embedded platforms become apparent. These platforms often employ Cortex-M CPUs or similar solutions and can operate on the (sub) milliwatt range for their power consumption. Deep learning frameworks are mostly designed for the server and workstation use and incorporate many features that are not relevant for the inference on low power devices, this prevents them from running on microcontrollers and other embedded solutions. Due to this, the usage of deep learning models on embedded devices typically relies on the manual implementation of the previously trained networks. The developers have to implement the required layer types, preferably using the vendor-provided math kernel libraries for the platform. This process is labour intensive, error prone and can easily result in inferior performance due to missing or incorrectly executed optimizations. In addition, the support of new platforms can result in extensive effort as the function kernels might need to be re-implemented for them. The necessary modifications increase even further if dedicated deep learning accelerators are employed. Due to their use of domain-specific instruction sets, which often utilize long pipelines that cover common neural network subpatterns, these accelerators cannot easily be targeted by standard compiler toolchains. Additional obstacles can be the coarse-granular accelerator instruction set architectures and the usage of custom data types, which can differ from the types used by the neural networks. A possible solution to automate these tasks are deep learning compilers. These toolchains operate similar to standard compilers, but introduce a number of important peculiarities: Instead of handwritten source code, deep learning compilers process serialized trained neural network descriptions. Additionally, they should be able to automatically employ the optimized math kernel libraries or alternative optimizations of the function kernels. A benefit of employing domain-specific compilers is the option to introduce additional optimizations that target deep learning models. Typically these are layer fusion or general network graph optimizations and quantization schemes. Lastly, these toolchains should be able to target heterogeneous platforms and dedicated accelerators by employing runtimes on the target devices. These runtimes take care of the scheduling and additional support operations that are necessary as well as the deserialization of the compiled networks. This paper will start with an overview of the available optimizations and math kernel libraries for deep learning workloads on embedded platform, which should be incorporated by the compiler toolchains for optimal performance. The following sections will cover a survey of the compiler features and the achieved performance on different embedded and low-power platforms, while the last part of the paper contains the implementation of a compilation flow for a new custom target. The intended target audience of this paper are research teams that are looking for a software stack to support their own deep learning hardware efforts, while achieving optimal performance and broad compatibility across the available frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Two recent studies focus on the employed techniques and the use with FPGA platforms <ref type="bibr" target="#b41">[43]</ref>, as well as an in-depth overview over the different approaches for common problems of the available deep learning compilers <ref type="bibr" target="#b24">[26]</ref>. In contrast to these publications, this work focuses more on embedded platforms. In addition to the survey and benchmark a compilation toolchain based on TVM has been implemented to target a heterogeneous platform. This was done to demonstrate the steps that are currently required to support custom accelerators with their own software stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning Optimizations</head><p>Deep Learning toolchains can employ a multitude of domainspecific optimizations in addition to standard compiler strategies. These include weight pruning, which cuts redundant and unnecessary weights from the networks to reduce their size and -depending on the implementation -the compute workload. While a wide range of pruning algorithms exist, none of them are currently employed by deep learning compilers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b39">41]</ref>. In contrast, quantization schemes are utilized by most deep learning compilers. This optimization converts the floating point representations for weights, intermediate results and outputs into smaller fixed-point formats, while mostly keeping the accuracy of the original network. This enables the network to take up less storage and reduces its bandwidth requirements during the execution as well as the computational intensity, if optimized function kernels for the quantized representations exist <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b40">42]</ref>. Additional strategies include optimizations on the neural network graph like layer fusion, dead node elimination and others <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b36">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Math Kernel Libraries</head><p>Math kernel libraries are a way to provide developers with optimized function kernels for common operations, increasing the efficiency of software solutions that employ them, while decreasing the redundancy of implementations. They are typically platform-specific and provided by the devicevendors <ref type="foot" target="#foot_0">1</ref> . These libraries differ largely in their implementation strategies and offered functionality. While all libraries provide function kernels, their implementations follow different approaches: ARM CMSIS-NN <ref type="bibr" target="#b20">[22]</ref> mostly resorts to a low number of kernels that deliver consistent performance across the majority of the configuration space. In contrast, Intel's oneDNN <ref type="foot" target="#foot_1">2</ref> [13] library implements most operations with multiple different strategies, based on the available instruction set extensions, data types and the configuration of the currently executed layer. The final selection of the function kernel takes places at runtime<ref type="foot" target="#foot_2">3</ref> to achieve the best performance under the current circumstances. While this strategy can be able to achieve better performance in certain cases, it requires much more maintenance and implementation effort compared to the more generalized function kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning Accelerators</head><p>In recent years plenty of deep learning accelerators emerged in commercial and research applications. Commercial accelerators for embedded platforms include the NVDLA from Nvidia <ref type="foot" target="#foot_3">4</ref> [31], Google's EdgeTPU <ref type="bibr" target="#b4">[5]</ref> as well as ARM's Ethos-U NPUs <ref type="bibr" target="#b1">[2]</ref>. Most of these solutions employ custom compilation toolchains that are limited to support of only one deep learning framework for its input formats. Research platforms include the Eyeriss accelerators (v1, v2) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, VTA <ref type="bibr" target="#b28">[30]</ref> as well as a many FPGA-based solutions <ref type="bibr" target="#b33">[35]</ref>. These typically do not focus on the software toolchain and explore novel architecture approaches instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Survey &amp; Benchmark</head><p>The survey section of the paper covers open source projects that are still in development. Its focus lies on the inference of deep learning models on embedded hardware. The support for the training step will not be evaluated as it is uncommon to execute it on the embedded device itself. The evaluated deep learning compilers are TensorFlow Lite (TFLite) <ref type="bibr" target="#b37">[39]</ref>, TensorFlow XLA (TF XLA) <ref type="bibr" target="#b38">[40]</ref>, Glow <ref type="bibr" target="#b32">[34]</ref>, TVM <ref type="bibr" target="#b6">[7]</ref>, ONNC <ref type="bibr" target="#b26">[28]</ref> and nGraph <ref type="bibr" target="#b12">[14]</ref>, which has been tested as part of Intel's openVINO toolkit.</p><p>As ?TVM was in an early stage at the time of testing, it has not been evaluated in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Common Strategies</head><p>All evaluated toolchains follow the typical compiler structure. The frontend converts the serialized pretrained models into a high-level intermediate representation (IR). Most toolchains utilize a two-level IR: The high-level IR is a graph-level representation of the compiled model and the low-level IR describes the operations on the tensor level. The graph-level IR is typically used for mostly target-independent optimizations and operator fusion. The tensor-level IR is used by the backend to optimize the individual operations. One exception is TFLite, which does not perform targetdependent optimizations at the compilation stage and only uses a graph-level representation. Instead, its compiler (called TFLite converter) generates a graph-level representation that does not contain execution details, as the device-specific function kernels are part of its runtime. This allows for a better portability of the compiler output across devices, but prevents more target-specific optimizations at the offline compilation stage. The majority of the evaluated toolchains employs a runtime <ref type="foot" target="#foot_4">5</ref> , which needs to be present on the target device to execute the compiled neural network. The functionality of the runtime differs between projects. All runtimes provide support infrastructure to unpack the compiled neural networks and an API for the integration into the user program. Solutions like TFLite and ONNC deliver the operation execution strategies for the platform as part of the runtime. Glow and TVM utilize the runtime for heterogeneous targets and in addition TVM requires the runtime for profiling during the auto-tuning step. TVM delivers function kernels that have been generated by its auto-tuner alongside the model description and weights to the runtime. The main difference between the evaluated projects is the provisioning of the function kernels. Most solutions utilize handcrafted implementations that integrate math kernel libraries. This requires maintenance and updating of implementations for each layer type across all supported platforms <ref type="foot" target="#foot_5">6</ref> . To circumvent these limitations, TVM employs an Auto-Tuning solutions which tries to find the best suited function kernels by using an AI-guided flow that incorporates measurements and estimations of execution times on the real target. Glow bypasses all of these concerns by reusing the same generalized function kernels across all targets, where its target-dependent optimizations are only applied by the LLVM backend for the selected target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User Accessibility</head><p>User Accessibility mostly depends on the user interface of the offline compiler stage, the integration of the compiler output into the target application and the supported input formats. For the supported frameworks and formats, ONNX <ref type="bibr" target="#b30">[32]</ref> is the most important, as it is an industry standard and converts from most frameworks that exist. See table <ref type="table">1</ref> for an overview of the supported formats and frameworks of each compiler toolchain. All compilers either support a command-line interface, like Table <ref type="table">1</ref>. Overview of the supported deep learning framework formats and target hardware platforms.</p><formula xml:id="formula_0">TVM TF Lite TF XLA Glow ONNC openVINO ONNX[32] ? ? ? ? ? ? TensorFlow[1] ? ? ? ? ? ? TensorFlow Lite flatbuffer ? ? ? ? ? ? PyTorch[33] ? ? ? ? ? ? MXNet[6] ? ? ? ? ? ? Caffe[29] ? ? ? ? ? ? Keras[11] ? ? ? ? ? ? x86_64 ? ? ? ? ? ? Cortex-A ? ? ? 7 ? ? ? Cortex-M ? 8 ? ? 7 ? ? ? GPU (CUDA) ? ? ? ? ? ? GPU (OpenCL) ? ? ? ? ? ? Deep Learning Accelerator ? ? ? ? ? ?</formula><p>traditional compiler toolchains, or the use through a Python API, which allows for the integration in the original training script of the deep learning model. One exception is Intel's openVINO that provides an additional graphical user interface through a web-interface <ref type="bibr" target="#b16">[18]</ref>. This enables more direct feedback to the developer on the impact of different optimizations on the overall model performance. For the integration into the user application, all toolchains provide a C or C++ API. TVM and TFLite provide an additional Python interface through their standard runtimes 9 . 7 inconclusive data 8 ?TVM was not ready at the time of testing 9 TFLite provides an additional runtime for microcontrollers, which does not come with a Python API</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Supported Platforms &amp; Extensibility</head><p>As the support for low-power embedded devices 10 in the currently available deep learning compilers is still limited, additional platforms have been used during the evaluation to allow for a more complete performance comparison. The range of supported platforms varies between the evaluated toolchains. In addition, the level of optimization for the supported platforms fluctuates widely. One such example is TFLite's support of the x86_64 platform: While its runtime can be compiled for it, the function kernels are not optimized, resulting in worse performance compared to other platforms or compilers.</p><p>For TF XLA no conclusive information about its support for different architectures could be found, as the official documentation and publications contradict each other <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b22">24]</ref>. The support for bare-metal use cases and embedded processors is much less common, as only TFLite, ONNC and Glow are able to target Cortex-M CPUs. For an overview of the supported platforms see table <ref type="table">1</ref>. While all toolchains include some kind of support for heterogeneous platforms, the implementations differ between them. The most complete solution has been provided by TVM in its Bring-Your-Own-Codegen (BYOC) flow <ref type="bibr" target="#b9">[10]</ref>, which allows developers to target new libraries and accelerators from TVM's high-level IR. It does not only provide an API to include new backends, it also supports the developer by implementing solutions for common problems, like the CPU fallback for unsupported operations, an infrastructure for custom operator fusion rules and the option to change the data layout of tensors. Most other toolchains only supply a simple API to access values and layer configurations and require the developer to reimplement many of these common support tasks. A stark contrast to TVM is TFLite. Its compilation stage does not provide an interface for the inclusion of additional targetspecific tasks and optimizations. New platforms are targeted by porting the runtime to them and deploying optimized function kernels with it. As this flow only allows the targeting of general purpose hardware, its support for the currently available accelerators has been realized by additional tools. These modify the flatbuffer file, which has been generated by the offline compilation stage, before it can be executed on the heterogeneous platform <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. This approach breaks the portability of TFLite and requires additional work to keep these tools compatible with the current scheme of the TFLite flatbuffer files. Some compiler toolchains like Glow, which reuse LLVM's backends 11 can easily target new architectures, if a LLVM backend already exists. In that case Glow's ahead-of-time (AOT) compilation mode can be reused, if other targets need to be supported, a separate runtime can be used and the AOT flow can no longer be utilized. 10 e.g. ARM Cortex-M and similar 11 for its AOT flow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head><p>For the embedded use case, the AOT compilation and quantization support are the most important compiler features. For the optimal execution on the target devices, the toolchains should be able to incorporate math kernel libraries or autotuning to provide optimal function kernels for the execution. Features like support for the backpropagation and training steps are not as important for embedded devices (yet) and are only supported by TF XLA, Glow and openVINO as they primarily target HPC and cloud applications. For the optimization of the layer execution TFLite, ONNC and openVINO rely on math kernel libraries, while Glow utilizes the same set of generalized function kernels across all targets, only utilizing the LLVM backend optimization steps. TVM is the only evaluated toolchain that employs an auto-tuning process to find good performing function kernels automatically, but can also exploit third party libraries through the BYOC flow. All toolchains -with the exception of TVM -implement only a limited number of static quantization schemes with fixed bit-widths for intermediates and weights. This is a limitation for the targeting of custom devices as they could employ alternative quantizations. TVM has implemented a more flexible quantization system, that offers different sizes and requantization strategies. However, it is more difficult to configure compared to the other solutions and did not achieve competitive accuracies in this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Performance</head><p>The performance was evaluated on an ARM Cortex-M55 fast model 12 , an Cortex-A72 13 and a Intel Celeron J1900 14 . The Cortex-A and Intel platform have been selected, to allow for a more complete performance overview due to the limited support of Cortex-M in the tested toolchains. This also allowed for a direct comparison to the standard TensorFlow framework on these two platforms. All of these platforms provide a SIMD vector extension and have been tested with the same simple MNIST test network, consisting of convolutional, fully connected, ReLU and maximum pooling layers. The batch size has been set to one and the final activation function has been removed after training. These are common optimizations for embedded applications, as it reduces the jitter that is introduced by the predicition as well as the latency, as the final activation does not change the classification result.</p><p>The Cortex-M55 could only be targeted by TFLite 15 and ONNC 16 . As no hardware of the M55 is available yet, a instruction-level simulator has been used instead. While 12 as no hardware was available at the time of testing 13 using a Raspberry Pi 4 with 4 GB of system memory 14 using 8 GB of DDR3 system memory 15 using its micro-runtime 16   Glow is able to target Cortex-M CPUs, it was not able to support the novel instruction set of the M55 (ARMv8.1M).</p><p>The testing showed that TFLite required less instructions to complete an inference run (2.6 M instead of 3 M instructions, see figure <ref type="figure" target="#fig_0">1</ref>), while ONNC allocated significantly less memory (1.6 MiB instead of 3 MiB. See figure <ref type="figure" target="#fig_0">1</ref> for details).</p><p>The next test platform was the Cortex-A72. ONNC could not be compiled for it, as it relied on Intel MKL for its function kernels 17 . Instead Glow, TVM and TFLite have been tested in addition to the standard TensorFlow Python runtime. An overview of the measured inference times can be seen at figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The quantized TFLite version achieved the fastest inference speed with 0.37 ms, followed by the quantized and autotuned TVM program, using its C runtime (0.41 ms). Glows fastest output achieved a mean inference time of 1.77 ms by using floating-point representations. Besides Glow, all compilers achieved faster inference times using quantized networks -suggesting that they employ optimized function kernels for quantized operations, while Glow uses its generalized standard kernels and wraps them into additional quantization and dequantization steps, which causes additional overhead. The worst result by a compiler was achieved by TVM, for its floating-point auto-tuned solution, as it tried to apply x86 function templates to the ARM platform 18 . However, the slowest result of 6.51 ms was still significantly faster than the use of the network in combination with the standard TensorFlow runtime -requiring 104.64 ms for a single inference run. This makes the slowest and incorrectly optimized compiled version 16 times faster, while the fastest compiled version achieved a speedup of 282.6 times. The Intel Celeron CPU allowed for the additional testing 17 While the CMake script for the standard runtime contained a parameter to disable the MKL integration, it could not be build when it was selected 18 it could not be determined, if it was caused by user error or by TVM, but it reoccurred over multiple tries and did not affect the quantized version  of nGraph 19 and ONNC's standard flow 20 . See figure <ref type="figure" target="#fig_1">2</ref> for the inference time results of the platform. In comparison to the Cortex-A results the ranking of the toolchains by their inference time changed, suggesting different levels of optimizations across the supported target devices for some deep learning toolchains. In addition, TVM was tested with the Intel MKL BYOC-based backend instead of its auto-tuning flow. This backend is not primarily optimized for performance as it is a demo for the BYOC functionality and was used to estimate the overhead which results from it. For the Celeron J1900, the floating-point versions of the compiled networks achieved faster inference speeds across all toolchains. This suggests either a lack of optimized kernels or a better implementation of the floating-point components of the hardware itself. The fastest results have been achieved by TVM with 0.68 ms (FP) and 1.01 ms (quantized). TVM did not show a significant difference between the standard and the BYOC flow results, which implies that the overhead of the BYOC flow is minimal. The next best results were achieved by TFLite's floating point network (1.08 ms, using the Python API), Glow (also floating point, 1.66 ms) and ONNC (1.71 ms). openVINO's compiled program did require 4.15 ms for a single inference, which made it the slowest floating point version out of the tested compiled networks. It was not able to quantize the network, as that is only supported on newer Intel CPU generations. Only TFLite's quantized networks took more time than openVINO to complete their inference run. In addition to the inference times, the peak memory allocations have been measured. The measured results varied by two orders of magnitude between the toolchains. Glow's compiled networks required 10 MiB of system memory at 19 as part of openVINO 20  peak, followed by TVM with 21 MiB to 26 MiB. As the higher allocations have been measured for the MKL-BYOC variant, it suggests, that the BYOC flow requires some memory overhead compared to the standard flow during the execution. TFLite required 14 MiB for a quantized version of the network utilizing only its C-API, which took significantly longer than the other results for a inference. The same configuration, but with a floating point version of the network allocated 238 MiB which is more than the expected increase by four times <ref type="foot" target="#foot_6">21</ref> . ONNC could only be tested with a floating point network as its open source standard branch does not support quantization. Its peak memory allocation of 51 MiB is more in line with the expected memory allocation. open-VINO's implementation allocated 489 MiB of memory during the inference, only TFLite's Python runtime used more memory with 896 MiB (quantized) or 1,248 MiB (floating point). While this values are significantly higher than the results of the other toolchains, they are still an improvement in comparison to the standard TensorFlow framework that allocated up to 2.3 GiB. The prediction accuracy for the compiled networks stayed mostly the same, even for the quantized variants, with the exception of TVM. It only reached an accuracy of around 50 %, which might have been user error due to its configurable quantization scheme and the usage of a global quantization scheme. The benchmark has shown that the available deep learning compilers are all able to deliver significantly better performance compared to the use of the standard TensorFlow framework. Additionally, they allowed for fast deployment of models on the evaluated platforms, without the need for handcrafted layer implementations. TVM was able to deliver the best inference speeds on the larger test devices; nonetheless, its accuracy was limited by the quantization flow, which was the only flexible quantization system out of the tested toolchains. While its frontend offered support for the majority of neural network serialization formats, the backend and runtime are unable to target the Cortex-M platform yet 22 . Its newly introduced BYOC flow allows developers to target heterogeneous platforms, while reusing the frontend for different input formats and the standard components for the execution of unsupported operations on the system CPU. TFLite was able to target the Cortex-M55 platform and achieved a similar performance to TVM on the Cortex-A system for the measured inference times. Nevertheless, it allocated significantly more memory for its execution and does not offer a simple flow to target heterogeneous systems that include a dedicated accelerator. Additionally, users are limited to the capabilities and formats of the TensorFlow ecosystem. Glow's support for the ONNX standard makes it compatible with most deep learning frameworks and its quantization flow achieves similar accuracies to TFLite. While it achieved the lowest peak memory allocation out of the tested solutions, the use of generalized kernels lead to slower inference times compared to the other platforms. For the targeting of embedded microcontrollers, the inclusion of CMSIS-NN could result in a significant performance improvement. ONNC was able to target the Cortex-M55 system and delivered competitive performance to TFLite's micro-runtime. However, the separation of the quantization tool into a commercial product, outside of the open-source project and the lack of support for newer ONNX subsets might limit its use in future endeavors. Intel's openVINO is only able to target the Celeron platform, but did not achieve a competitive performance result on this platform. Its limitation to x86 CPU's makes it unable to target any kind of low-powered embedded platform. However, its graphical user interface (called workbench) made the optimization and compilation process more transparent for the user, which could be a helpful feature for other toolchains as well. While the number of supported layer functions is important for the user, it is difficult to compare these toolchains based on a single snapshot of their development stages, due to constant updating of most of them. This survey has shown that the support for embedded platforms in the current deep learning compiler toolchains is still at an early stage as only a small subset is able to target platforms like the Cortex-M55. 22 However, a mico-runtime is currently in development, but not production ready yet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>besides its Cortex-M version</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>For the implementation, an abstract accelerator has been defined and an instruction set simulator was implemented 23 . The simulator was verified with TensorFlow and is able to estimate the hardware utilization and cycle count for input and output parallel execution strategies 24 . The simulated accelerator uses an instruction set that is similar in its capabilities to other solutions like Nvidia's NVDLA <ref type="bibr" target="#b29">[31]</ref>. It only supports signed integer formats for operations and the majority of them are limited to a length of eight bit for the individual values in their input tensors while producing outputs with a length of either eight or 32 bit. For the software flow TVM was used due to its BYOC functionality. This flow starts with the definition of annotation rules for supported nodes and patterns in TVM's graph-level IR 25 . These are then merged into subgraphs, which will be executed by the accelerator. These steps are handled by TVM's BYOC flow and did only require the definition of supported graph patterns during the implementation of the new backend. TVM manages the execution of unsupported operations on the CPU as well as the invocation of the subgraphs from the standard runtime. After the annotation, the network graph is partitioned to separate the supported sections of the network into subgraphs. These subgraphs are then passed on to the custom code generation, where they are converted into a JSON format for better portability across different instruction set variants. The final generation of the accelerator command stream happens at runtime before the initial inference. This allows to target different ISA variants with varying memory sizes using a single serialized file. A custom runtime component executes the code generation and passes back a run function for each subgraph to the standard TVM graph runtime. During inference the subgraphs are executed by the simulator through the invocation of these run functions. Besides the quantization and data layout transformation functionality, which was provided by TVM, the memory planning for DMA operations between system and accelerator memory, the assembly generation, the configuration register file management and tiling for larger tensor sizes needed to be implemented by the custom runtime component. The tiling was implemented by primarily splitting the workload along the output channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The correct functionality was initially tested with neural networks that only contained supported operations as single nodes and patterns. Additional testing with the MNIST 23 the simulator was written to only target the desired ISA and operates on the custom output files of the new compiler backend, which contain the initial memory layout and instructions for each subgraph. 24 input parallel: input channels are processed in parallel; output parallel: same for output channels 25 called Relay network from the performance benchmark revealed that the current TVM quantization flow inserts additional meta nodes into the graph. These nodes prevent the merging of multiple compute layers into a single subgraph. Due to this, the network was split into three subgraphs, which requires the system to move the intermediate results back to the shared system memory between the subgraph executions. This resulted in reduced throughput and efficiency due to unnecessary bus transactions. Otherwise, the custom backend worked as intended and generated the expected inference results.</p><p>For a more realistic test case Google's MobileNetV1 <ref type="bibr" target="#b17">[19]</ref> with the ImageNet dataset <ref type="bibr" target="#b13">[15]</ref> has been used. The additional batch normalization layers prevented the use of the larger convolutional pipelines, as they are located between the convolutional and activation function nodes. This adds additional bus transactions between accelerator memory and compute units, which reduces the efficiency of real hardware.</p><p>The network was evaluated with three iterations of the accelerator and its toolchain:</p><p>? Without support for depthwise convolutions: Depthwise convolutional layers are executed by the standard runtime on the CPU. This results in a lower share of the network to be accelerated by the target device. The estimated cycle counts can be seen in figure <ref type="figure" target="#fig_5">4a</ref>. ? Software-only Implementation:</p><p>The depthwise convolutions are mapped to a the standard convolution instruction of the DLA. The ISA has not been changed. This allows for a larger share of the network to be accelerated, but the depthwise layers are constraint to a small utilization of the processing elements in the vector ALU. ? Full support:</p><p>The ISA has been extended to provide a dedicated instruction for deptwhise convolutional layers. This instruction allows for a higher utilization, resulting in shorter execution times.</p><p>This was done to evaluate the flexibility of the coarse-grained ISA for the support of new layer types, as the deep learning research community develops new layer types at a very rapid pace, which makes it difficult to develop accelerators that can stay up-to-date for extended time periods without changing the hardware architecture. A possible solution would be to update the software flow, to enable it to map new layers to existing instructions. In the case of depthwise convolutions, this approach was represented by the second test scenario. While the implementation was possible, it resulted in drastically increased cycle counts if compared to a native solution (compare figure <ref type="figure" target="#fig_5">4c</ref> and<ref type="figure" target="#fig_5">4e</ref>). This was caused by the low utilization of the processing elements (PEs) as only one filter could be processed at a time. Additionally, these scenarios were tested with different sizes of the accelerators exclusive memory and the vector ALU as well as input and output parallel mode for cycle and utilization estimations to evaluate the impact of operation tiling on the overall performance. The evaluated memory sizes were 512 KiB and 256 MiB. The last configuration does not require any tiling to take place, while the first is the smallest size which is supported by the implementing tiling methods for this network <ref type="foot" target="#foot_7">26</ref> .</p><p>As shown in figure <ref type="figure" target="#fig_5">4a</ref>, the output parallel hardware achieved lower cycle counts due to its higher utilization in the early layers of the network. However, this changed as soon as tiling was required due to the limited amount of device memory.</p><p>As the tiling splits the workload along the channel dimension of the output tensor, the parallelization opportunity for this architecture shrinks. This results in a lower utilization, and a faster execution of input parallel strategies. Additionally, it can be seen in figure <ref type="figure" target="#fig_5">4e</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This evaluation has shown that, while all evaluated toolchains deliver reasonable performance across different platforms, their support for low-powered embedded devices and heterogeneous solutions with dedicated accelerators is still at an early stage. Some are limited by the use of existing compiler backends, which prevent the targeting of dedicated hardware. Another limitation is the use of static quantization schemes that do not offer an easy solution to adapt them for different hardware implementations. Additionally, while it was possible to target a novel accelerator using TVM, our implementation showed two drawbacks:</p><p>The more flexible quantization flow of TVM introduces annotation nodes into the code generation, which prevent the solution from reaching higher efficiency. It is currently not possible to connect the BYOC flow with the micro-TVM runtime that is also still under development. This prevents the usage of TVM on (heterogeneous) embedded devices for TinyML applications, however, it can already be utilized during the hardware development to evaluate the performance of prototypes with real-world test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Performance of Hardware Variants</head><p>As the accelerator was simulated with different software flow versions, ISA variations and memory configurations, multiple performance estimations have been collected. Each of the three test scenarios from section 5 has been tested with 64 or 128 PEs in the compute module of the accelerator and 512 KiB, 1 MiB (only for the last scenario) and 256 MiB for the SRAM memory on the device. An additional parameter for the simulation was the hardware parallelization of the workload, which was either input or output parallel. The input parallel configuration would split the workload along the channel dimension of the input feature map, while the output parallel version would parallelize the workload along the output feature map's channel axis. The bars of the diagrams are segmented according to the cycles spent on each supported subgraph type. In the test case for MobileNetV1 only up to three different subgraph types could be accelerated:</p><p>? CONV subgraph:</p><p>The CONV (Convolutional) subgraphs contain the Conv2D layers of the network, which can be mapped to the accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? REQUANT subgraph:</head><p>The REQUANT subgraphs contain the requantization operations that are executed in-between layers to convert the 32-bit output of the previous layer back to an 8-bit format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? DEPTH subgraph:</head><p>The DEPTH (Depthwise Convolution) subgraph is only present in the second and third test scenario, as the first did not offer support for the depthwise convolutional layer that is contained in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Target Accelerator</head><p>To be more flexible a simulator has been used instead of a real hardware target as it came with several advantages, including a more flexible instruction set, easier configuration of hardware parameters. However, the simulator was still based on the typical concepts of other deep learning inference accelerators like the NVDLA including the coarsegrained ISA and the usage of highly optimized and separated hardware blocks for the different kinds of operations. Its instruction set architecture was specifically designed for deep learning operations, and features dedicated instructions for typical layer operations as well as common patterns like the combination of convolutional layers with activation functions and requantization operations, which would be mapped to separate pipelines, that would combine multiple different functional units. Most instructions only support the   processing of 8-bit integer data. A small subset of support instructions, like element-wise additions and shifts can also be executed on 32-bit data.</p><p>As most layers require a vast number of additional configuration parameters, which would not fit into a reasonably sized instruction, most of these parameters are stored in a separate register file and updated from the system CPU. This has the additional benefit that these values can stay unchanged over multiple operations, without having to submit the same values multiple times, improving the energy efficiency of the solution. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Instruction Counts and Peak Memory Allocation for the execution of the benchmark model on the Cortex M55.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of the inference times on the ARM Cortex-A72 and Intel Celeron J1900 platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of the peak memory allocation on the Intel Celeron J1900 platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The total cycle count for the accelerated operations of MobileNet without support for depthwise convolutions across testing configurations. The total cycle count for the accelerated operations of MobileNet with software emulated support for depthwise convolutions across testing configurations.(c) The total cycle count for the accelerated operations of MobileNet with software-only support for depthwise convolutions across testing configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The total cycle count for the accelerated operations of MobileNet with hardware support for depthwise convolutions across testing configurations.(e) The total cycle count for the accelerated operations of MobileNet without support for depthwise convolutions across testing configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The first part of the label describes the size of the on-device memory, the second value represents the number of processing elements in the compute unit and the last letter stands either for an input (I) or output (O) parallel hardware implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>using a special Cortex-M version of the toolchain</figDesc><table><row><cell></cell><cell cols="7">Cortex M55 Benchmark Results</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Million Instructions</cell></row><row><cell>2.30</cell><cell>2.40</cell><cell>2.50</cell><cell>2.60</cell><cell cols="2">2.70</cell><cell>2.80</cell><cell>2.90</cell><cell>3.00</cell><cell>3.10</cell></row><row><cell>ONNC (Debug)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1564 kB</cell><cell></cell><cell></cell><cell>3.00</cell></row><row><cell>ONNC (Release)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1561 kB</cell><cell></cell><cell>2.90</cell><cell></cell></row><row><cell>TFLite (micro)</cell><cell></cell><cell></cell><cell>2.58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3138 kB</cell></row><row><cell>0</cell><cell cols="4">500 Peak Memory Allocation 1,000 1,500</cell><cell cols="4">2,000 Instruction Count 2,500 3,000</cell><cell>3,500 kB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, which includes an additional 1024 KiB configuration, that the cycle count does not scale linearly with the available memory. Instead, a combination of larger vector ALU with 128 processing elements (PEs) and 1024 KiB of memory can be faster than a configuration with 64 PEs and 256 MiB of memory. The reason is, that the 1024 KiB configuration only requires tiling of the initial layers of the network, which allows it to compensate for the additional cycles in the later layers where it can calculate twice as many results per cycle as the 64 PE configuration with 256 MiB of memory. A benefit of the TVM BYOC flow is the ability to quickly evaluate the performance for different real-world network architectures across multiple hardware configurations during the hardware development.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Examples of instructions that are part of the accelerator ISA.</figDesc><table><row><cell>Mnemonic</cell><cell>Description</cell><cell cols="2">Input Type Output Type</cell></row><row><cell></cell><cell cols="2">Compute Layer Instructions</cell><cell></cell></row><row><cell>OP_CONV</cell><cell>standard Conv2D, op-</cell><cell>int8</cell><cell>int32 or int8</cell></row><row><cell></cell><cell>tional combined with</cell><cell></cell><cell></cell></row><row><cell></cell><cell>requantization</cell><cell></cell><cell></cell></row><row><cell>OP_CONV_RELU</cell><cell>Conv2D + ReLU</cell><cell>int8</cell><cell>int8</cell></row><row><cell cols="2">OP_DEPTH_CONV depthwise Conv2D,</cell><cell>int8</cell><cell>int32 or int8</cell></row><row><cell></cell><cell>optional combined</cell><cell></cell><cell></cell></row><row><cell></cell><cell>with requantization</cell><cell></cell><cell></cell></row><row><cell>OP_MAT_MUL</cell><cell>Matrix Mulitplication</cell><cell>int8</cell><cell>int32</cell></row><row><cell></cell><cell cols="2">Post-Processing Instructions</cell><cell></cell></row><row><cell>OP_ACT_RELU</cell><cell>ReLU Activation</cell><cell>int8</cell><cell>int8</cell></row><row><cell>OP_ACT_LRELU</cell><cell>Leaky ReLU Activa-</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell>tion</cell><cell></cell><cell></cell></row><row><cell>OP_POOL</cell><cell>Pooling Layer</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell cols="2">Elementwise Instructions</cell><cell></cell></row><row><cell>OP_E_ABS</cell><cell>calculates absolute</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell>values of single input</cell><cell></cell><cell></cell></row><row><cell></cell><cell>tensor</cell><cell></cell><cell></cell></row><row><cell>OP_C_MIN</cell><cell>compares two ten-</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell>sors of same shape,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>writes minimum val-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ues</cell><cell></cell><cell></cell></row><row><cell>OP_C_MAX</cell><cell>compares two ten-</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell>sors of same shape,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>writes maximum val-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ues</cell><cell></cell><cell></cell></row><row><cell>OP_E_ADD</cell><cell>adds two tensors of</cell><cell>int8</cell><cell>int8</cell></row><row><cell></cell><cell>same shape element-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>wise</cell><cell></cell><cell></cell></row><row><cell>OP_E32_ADD</cell><cell>adds two tensors of</cell><cell>int32</cell><cell>int32</cell></row><row><cell></cell><cell>same shape element-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>wise</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DMA Instructions</cell><cell></cell><cell></cell></row><row><cell>OP_DMA_READ</cell><cell>loads data from sys-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>tem memory to accel-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>erator SRAM</cell><cell></cell><cell></cell></row><row><cell>OP_DMA_WRITE</cell><cell>writes data from ac-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>celerator SRAM to</cell><cell></cell><cell></cell></row><row><cell></cell><cell>system memory</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Efforts for device-independent solutions exist as well, but have not found the same rate of adaption, e.g. XNNPack<ref type="bibr" target="#b14">[16]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>previously known as MKL-DNN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>only for its x86_64 CPU backend</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The NVDLA is available as an open source project, which provides the hardware description and software stack, but is also contained in several of Nvidia's own products like the Jetson Xavier NX, using an alternative closed-source software stack</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The only exception that never uses a runtime is TensorFlow XLA, while Glow's AOT flow for CPUs does not require a runtime, it is deployed on other platforms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>TFLite, ONNC and others employ this strategy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_6"><p>as 8-bit integers are 4 times smaller than 32-bit floating point values</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_7"><p>For convolutional layers only a split along the output channel dimension was implemented, as the splitting along the rows and columns requires extensive effort to implement and validate all edge cases that can occur</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>piler Toolchains for Deep Learning Workloads on Embedded Platforms. In Proceedings of <rs type="institution">TinyML Research Symposium(TinyML Research Symposium'21) (Burlingame '21). ACM, New York, NY, USA</rs>, 10 pages.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">More</forename><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Arm ethos-u</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/ip-products/processors/machine-learning/ethos-u55" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ARM Ethos-U: Tflite compiler</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://git.mlplatform.org/ml/ethos-u/ethos-u-vela.git/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What is the state of neural network pruning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Taking ai to the edge: Google&apos;s tpu now comes in a makerfriendly package</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tvm: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Amazon Web Services. How to bring your own codegen to tvm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Cody</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://tvm.apache.org/2020/07/15/how-to-bring-your-own-codegen-to-tvm" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Edge tpu compiler documentation</title>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<ptr target="https://coral.ai/docs/edgetpu/compiler/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bobba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kanawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Korovaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Lishka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Webb</surname></persName>
		</author>
		<title level="m">Intel ngraph: An intermediate representation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Xnnpack -github repository</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/google/XNNPACK" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TensorFlow: Xla documentation</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/xla?hl=en" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gorbachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Slavutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tugarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fatekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Quantizing deep convolutional networks for efficient inference: A whitepaper</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tensorflow graph optimizations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
		<ptr target="https://research.google/pubs/pub48051/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Xla: Tensorflow, compiled. TensorFlow Dev Summit</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<title level="m">The deep learning compiler: A comprehensive survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fixed point quantization of deep convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Annapureddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Onnc: A compilation framework connecting onnx to proprietary deep learning accelerators</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="214" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Caffe2: Portable high-performance deep learning framework from facebook</title>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<title level="m">A hardwaresoftware blueprint for flexible deep learning specialization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Nvidia deep learning accelerator</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://http://nvdla.org/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Onnx -official webpage</title>
		<author>
			<persName><surname>Onnx</surname></persName>
		</author>
		<ptr target="https://onnx.ai/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdulrasool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dzhabarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00907</idno>
		<title level="m">Graph lowering compiler techniques for neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fpga-based accelerators of deep learning networks for learning and classification: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shawahna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Sait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Maleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7823" to="7859" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">And the bit goes down: Revisiting the quantization of neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tensorflow 2 graph optimizations documentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/guide/graph_optimization" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><surname>Tensorflow Lite</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Xla: Domain-specific compiler for linear algebra that optimizes tensorflow computations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<title level="m">Structured pruning of large language models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How to quantize neural networks with tensorflow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<ptr target="https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An in-depth comparison of compilers for deep neural networks on hardware</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Embedded Software and Systems (ICESS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
