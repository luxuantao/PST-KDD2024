<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Shape Induction from 2D Views of Multiple Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
							<email>mgadelha@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts -Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smaji@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts -Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>ruiwang@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts -Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Shape Induction from 2D Views of Multiple Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">453FE91EAB11C0C39DA277BACF60AF7B</idno>
					<idno type="DOI">10.1109/3DV.2017.00053</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we investigate the problem of inducing a distribution over three-dimensional structures given twodimensional views of multiple objects taken from unknown viewpoints. Our approach called "projective generative adversarial networks" (PrGANs) trains a deep generative model of 3D shapes whose projections match the distributions of the input 2D views. The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained on 3D data for a number of shape categories including chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage is that our model allows us to predict 3D, viewpoint, and generate novel views from an input image in a completely unsupervised manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We live in a three-dimensional (3D) world, but all we see are its projections on to two dimensions (2D). Inferring the 3D shapes of objects from their 2D views is one of the central challenges of computer vision. For example, looking at a catalogue of different airplane views in Figure <ref type="figure" target="#fig_0">1</ref>, one can mentally infer their 3D shapes by simultaneously reasoning about the shared variability in the underlying geometry and viewpoint across instances. In this work we investigate learning a generative model of 3D shapes given a collection of images of an unknown set of objects taken from an unknown set of views, i.e., without any 3D supervision.</p><p>Although there are several cues for inferring the 3D shape from a single image, in this work we assume that shapes are rendered as binary images bounded by silhouettes. Even in this simplified setting the problem remains challenging since shading cues are no longer available. Moreover, which instance was used to generate each image, the viewpoint from which the image was taken, or even ... the number of underlying instances are not provided. This makes it difficult to apply exisiting approaches of estimating geometry based on structure from motion <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>, or computing visual hulls <ref type="bibr" target="#b16">[18]</ref>.</p><p>We take an analysis by synthesis approach and learn a 3D shape generator whose projections match the distribution of the provided images. We use the framework of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> and augment the generator with a projection module. The generator learns to produce 3D shapes, the projection module renders each shape from randomly-chosen views, and the adversarial network discriminates real images from generated ones. The projection module is a differentiable renderer that approximates the true rendering pipeline and allows us to map 3D shapes to 2D images, as well as back-propagate the gradients of 2D images to 3D shapes. Once trained, the model can be used to infer 3D shape distributions from a collection of images (Figure <ref type="figure" target="#fig_0">1</ref> shows some samples drawn from the generator), and to infer depth data from a single image, without using any 3D or viewpoint information during learning. We call our approach Projective GANs (PrGANs).</p><p>There are several challenges that need to be addressed for learning a generative model of 3D shapes from views. First is the choice of how the 3D shape is represented. Linear shape basis (or morphable models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>) are effective for categories like faces that have a fixed topology, but less so for categories with varying number of parts, e.g., airplanes and chairs. Other bases such as spherical harmonics are not well suited for modeling objects with holes and fine-details. Mesh-based representations are commonly used with rendering pipelines (e.g., OpenGL <ref type="bibr" target="#b30">[32]</ref>) and can also be adjusted to match image statistics using a differentiable renderer (e.g., OpenDR <ref type="bibr" target="#b18">[20]</ref>), but the variability of the mesh topology makes it difficult to generate them in a consistent manner across instances.</p><p>We make the following assumptions to tackle this problem. First, shapes are modeled using a voxel representation that indicates the occupancy of a volume in fixed-resolution 3D grid. Second, the effects of lighting and material properties are ignored and during training every input image is assumed to be a binary image (silhouettes) indicating whether a pixel is occupied or not. This allows us to design a volumetric feed-forward network that faithfully reproduces the true rendering pipeline. The layers in the feed-forward network are differentiable, allowing the ability to adjust the 3D volume based on projections. Real-world images can also be used by removing background and converting them to binary images. Finally we assume that the distribution over viewpoints is known (assumed to be uniform in our experiments, but could be any distribution). However, one can use a generic object viewpoint estimator <ref type="bibr" target="#b26">[28]</ref> as weak supervision for our problem.</p><p>Our main contributions are as follows: (i) we propose PrGANs, a framework to learn probabilistic distributions over 3D shapes from a collection of 2D views of objects. We demonstrate its effectiveness on learning complex shape categories such as chairs, airplanes, and cars sampled from online shape repositories <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">35]</ref>. The results are reasonable, even when views from multiple categories are combined; (ii) On the task of generating 3D shapes, PrGANs perform well in comparison to GANs trained directly on 3D data <ref type="bibr" target="#b32">[34]</ref>; (iii) The learned 3D representation can be used for unsupervised estimation of 3D shape and viewpoint given a novel 2D shape, and for interpolation between two different shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Estimating 3D shape from image collections. The difficulty of estimating 3D shape can vary widely based on how the images are generated. Visual-hull techniques <ref type="bibr" target="#b16">[18]</ref> can be used to infer the shape of a particular object given its views from known viewpoints by taking the intersection of the projected silhouettes. When the viewpoint is fixed and the lighting is known, photometric stereo <ref type="bibr" target="#b31">[33]</ref> can provide accurate geometry estimates for rigid and diffuse surfaces. Structure from motion (SfM) <ref type="bibr" target="#b11">[12]</ref> can be used to recover the shape of rigid objects from their views taken from un-known viewpoints by jointly reasoning about point correspondences and camera projections. Non-rigid SfM can be used to recover shapes from image collections by assuming a parametric family of deformations across 3D shapes. An early example of this approach is by Blanz and Vetter <ref type="bibr" target="#b3">[4]</ref> for estimating 3D shapes of faces from image collections. However, 3D data with consistent global correspondences is required in order to learn a morphable model. Recently, non-rigid SfM has been applied to categories such as cars and airplanes by manually annotating a fixed set of keypoints across instances in order to bootstrap the learning process <ref type="bibr" target="#b14">[15]</ref>. Our work augments non-rigid SfM using a learned 3D shape generator, which allows us to generalize the technique to categories with diverse structures without requiring correspondence annotations. Our work is also related to recent work of Kulkarni et al. <ref type="bibr" target="#b15">[16]</ref> for estimating a disentangled representation of images into shape, viewpoint, and lighting variables (dubbed "inverse graphics networks"). However, the shape representation is not explicit, and the approach requires the ability to generate training images while varying one factor at a time.</p><p>Inferring 3D shape from a single image. Optimizationbased approaches put priors on geometry, material, and light and estimate all of them by minimizing the reconstruction error when rendered [17, <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>. Recognitionbased methods have been used to estimate geometry of outdoor scenes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">26]</ref>, indoor environments <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">27]</ref>, and objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">25]</ref>. More recently, convolutional networks have been trained to generate views of 3D objects given their attributes and camera parameters <ref type="bibr" target="#b7">[8]</ref>, to generate 3D shape given a 2D view of the object <ref type="bibr" target="#b27">[29]</ref>, and to generate novel views of an object <ref type="bibr" target="#b35">[37]</ref>. Most of these approaches are trained in a fully-supervised manner and require 3D data or multiple views of the same object during training.</p><p>Generative models for images and shapes. Our work builds on the success of GANs for generating images across a wide range of domains <ref type="bibr" target="#b9">[10]</ref>. Recently, Wu et al. <ref type="bibr" target="#b32">[34]</ref> learned a generative model of 3D shapes using GAN equipped with 3D convolutions. However, the model was trained using aligned 3D shape data. Our work aims to solve a more difficult question of learning a 3D-GAN from 2D images. Several recent works are in this direction. Rezende et al. in <ref type="bibr" target="#b22">[24]</ref> show results for 3D shape completion for simple shapes when views are provided, but require the viewpoints to be known and the generative models are trained on 3D data. Yan et al. in <ref type="bibr" target="#b34">[36]</ref> learn a mapping from an image to 3D using multiple projections of the 3D shape from known viewpoints. Their approach employs a 3D volumetric decoder and optimizes a loss that measures the overlap of the projected volume on the multiple silhouettes from known viewpoints, similar to a visual-hull reconstruction. Tulsiani et al. <ref type="bibr" target="#b29">[31]</ref> learn a model to map images to 3D shape provided with color images or silhouettes of objects taken from known viewpoints using a "ray consistency" approach similar to our projection module. Our method on the other hand does not assume known viewpoints or object associations of the silhouettes making the problem considerably harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method builds upon the GAN architecture proposed in Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>. The purpose of the GAN is to train a generative model in an adversarial setup. The model consists of two different parts: a generator and a discriminator. The generator G aims to transform samples drawn from a simple distribution P that appear to have been sampled from the original dataset. The goal of the discriminator D is to distinguish synthetic samples (created by the generator) from real samples (drawn from a data distribution D). Both the generator and the discriminator are trained jointly to solving for the following optimization:</p><formula xml:id="formula_0">min G max D E x∼D [log (D(x))] + E z∼P [log (1 -D(G(z)))].</formula><p>(1) Our main task is to train a generative model capable of creating 3D shapes without relying on 3D data itself, but in 2D images from those shapes, without any view or shape annotation. In other words, the data distribution consists of 2D images taken from different views and are of different objects. To address this issue, we factorize the 2D image generator into a 3D shape generator, viewpoint generator, and a projection module (Fig. <ref type="figure" target="#fig_1">2</ref>). The challenge is to identify a representation suitable for a diverse set of shapes and a differentiable projection module to create final 2D images and enable end-to-end training. We describe the architecture employed for each of these next.</p><p>3D shape generator. The input to the entire generator is z ∈ R 201 with each dimension drawn independently from a uniform distribution U(-1, 1). Our 3D shape generator transforms the first 200 dimensions of z to a 32 × 32 × 32 voxel representation of the shape. Each voxel contains a value v ∈ [0, 1] that represents its occupancy. The architecture of the 3D shape generator is inspired by the DC-GAN <ref type="bibr" target="#b21">[23]</ref> and 3D-GAN <ref type="bibr" target="#b32">[34]</ref> architectures. It consists of a four-layer network shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The first layer transforms the 200 dimensional vector to a 256×4×4×4 vector using a fully-connected layer. Subsequent layers have batch normalization and ReLU layers between them and use 3D kernels of size 5 × 5 × 5. The last layer is succeeded by a sigmoid activation instead of a ReLU.</p><p>Viewpoint generator. The viewpoint generator takes the last dimension of z ∈ U(-1, 1) and transforms it to a viewpoint vector (θ, φ). The training images are assumed to have been generated from 3D models that are upright oriented along the y-axis, and are centered at the origin. Most models in online repositories and the real world satisfy this assumption (e.g., chairs are on horizontal planes). We generate images by sampling views uniformly at random from one of eight pre-selected directions evenly spaced around the y-axis (i.e., θ = 0 and φ = 0 • , 45 • , 90 • , ..., 315 • ), as seen in Fig 3 . Thus the viewpoint generator picks one of these directions uniformly at random. Projection module. The projection module works as follows. The first step is to rotate the voxel grid to the corresponding viewpoint. Let V : Z 3 → [0, 1] ∈ R be the voxel grid, a function that given given an integer 3D coordinate c = (i, j, k) returns the occupancy of the voxel centered at c. The rotated version of the voxel grid V (c) is defined as V θ,φ = V ( R(c, θ, φ) ), where R(c, θ, φ) is the coordinate obtained by rotating c around the origin according to the spherical angles (θ, φ). For simplicity we use nearest neighbor sampling (hence the floor operator).</p><p>The second step is to perform the projection to create an image from the rotated voxel grid. This is done by applying the projection operator P ((i, j), V ) = 1 -e -k V (i,j,k) . Intuitively, the operator sums up the voxel occupancy values along each line of sight (assuming othographic projection), and applies exponential falloff to create a smooth and differentiable function. When there is no voxel along the line of sight, the value is 0; as the number of voxels increases, the value approaches 1. Combined with the rotated version of the voxel grid, we define our final projection module as: P θ,φ ((i, j), V ) = 1 -e -k V θ,φ (i,j,k) . As seen in Fig. <ref type="figure" target="#fig_2">3</ref> the projection module can well approximate the rendering of a 3D shape as a binary silhouette image, and is differentiable.</p><p>Discriminator. The discriminator consists of a sequence of 2D convolutional layers with batch normalization and LeakyReLU layers <ref type="bibr" target="#b19">[21]</ref> between them. The sequence of transformations done in the network are:</p><formula xml:id="formula_1">32 × 32 → 256 × 16 × 16 → 512 × 8 × 8 → 1024 × 4 × 4 → 1.</formula><p>Similarly to the generator, the last layer of the discriminator is followed by a sigmoid activation instead of a LeakyReLU.</p><p>Training details. We train the entire architecture by optimizing the objective in Equation <ref type="formula">1</ref>. Usually, the training updates to minimize each one of the losses is applied once at each iteration. However, in our model, the generator and the discriminator have a considerably different number of parameters. The generator is trying to create 3D shapes, while the discriminator is trying to classify 2D images. To mitigate this issue, we employ an adaptive training strategy. At each iteration of the training, if the discriminator accuracy is higher than 75%, we skip its training. We also set different different learning rates for the discriminator and the generator: 10 -5 and 0.0025, respectively. Similarly to the DCGAN architecture <ref type="bibr" target="#b21">[23]</ref>, we use ADAM with β = 0.5 for the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we describe the set of experiments to evaluate our method and to demonstrate the extension of its capabilities. First, we show the effectiveness of our method as  both 2D and 3D shape generators. To this end, we compare our model with a traditional GAN for image generation and a GAN for 3D shapes. We present quantitative and qualitative results. Second, we demonstrate that our method is able to induce 3D shapes from unlabelled images even when there is only a single view per object. Third, we present 3D shapes induced by our model from a variety of categories such as airplanes, cars, chairs, motorbikes, and vases. Using the same architecture, we show how our model is able to induce coherent 3D shapes when the training data contains images mixed from multiple categories. Finally, we show applications of our method in predicting 3D shape from a novel 2D shape, and performing shape interpolation.</p><p>Input data. We generate training images synthetically using 3D shapes available in the ModelNet <ref type="bibr" target="#b33">[35]</ref> and ShapeNet <ref type="bibr" target="#b5">[6]</ref> databases. Each category contains a few hundred to thousand shapes. We render each shape from 8 evenly spaced viewing angles with orthographic projection to produce binary images. Hence our assumption is that the viewpoints of the training images (which are unknown to the network) are uniformly distributed. If we have prior knowledge about the viewpoint distribution (e.g. there may be more frontal views than side views), we can adjust the projection module to incorporate this knowledge. To reduce aliasing, we render each image at 64 × 64 resolution and downsample to 32 × 32. We have found that this generally improves the results. Using synthetic data allows us to easily perform controlled experiments to analyze our method. It is also possible to use real images downloaded from a search engine as discussed in Section 5.</p><p>Validation. We quantitatively evaluate our model by comparing its ability to generate 2D and 3D shapes. To do so, we use 2D image GAN similar to DCGAN <ref type="bibr" target="#b21">[23]</ref> and a 3D-GAN similar to the one presented at <ref type="bibr" target="#b32">[34]</ref>. At the time  of this writing the implementation of <ref type="bibr" target="#b32">[34]</ref> is not public yet, therefore we implemented our own version. We will refer to them as 2D-GAN and 3D-GAN, respectively. The 2D-GAN has the same discriminator architecture as the PrGAN, but the generator contains a sequence of 2D transposed convolutions instead of 3D ones, and the projection module is removed. The 3D-GAN has a discriminator with 3D convolutions instead of 3D ones. The 3D-GAN generator is the same of the PrGAN, but without the projection module.</p><p>The models used in this experiment are chairs from Mod-elNet dataset <ref type="bibr" target="#b33">[35]</ref>. From those models, we create two sets of training data: voxel grids and images. The voxel grids are generated by densely sampling the surface and inside of each mesh, and binning the sample points into 32 × 32 × 32 grid. A value 1 is assigned to any voxel that contains at least one sample point, and 0 otherwise. Notice that the voxel grids are only used to train the 3D-GAN, while the images are used to train the 2D-GAN and our PrGAN.</p><p>Our quantitative evaluation is done by taking the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b10">[11]</ref> between the data created by the generative models and the training data. We use a kernel bandwidth of 10 -3 for images and 10 -2 for voxel grids. The training data consists of 989 voxel grids and 7912 images. To compute the MMD, we draw 128 random data points from each one of the generative models. The distance metric between the data points is the hamming distance di- vided by the dimensionality of the data. Because the data represents continuous occupancy values, we binaritize them by using a threshold of 0.001 for images or voxels created by PrGAN, and 0.1 for voxels created by the 3D-GAN.</p><p>Results show that for 2D-GAN, the MMD between the generated images and the training data is 90.13. For PrGAN, the MMD is 88.31, which is slightly better quantitatively than 2D-GAN. Fig. <ref type="figure" target="#fig_4">4</ref> shows a qualitative comparison. The results are visually very similar. For 3D-GAN, the MMD between the generated voxel grids and the training voxel grids is 347.55. For PrGAN, the MMD is 442.98, which is worse compared to 3D-GAN. This is not surprising as 3D-GAN is trained on 3D data, while PrGAN is trained on the image views only. Fig. <ref type="figure" target="#fig_6">5</ref> presents a qualitative comparison. In general PrGAN has trouble generating interior structures because the training images are binary, carries no shading information, and are taken from a limited set of viewing angles. Nonetheless, it learns to generate exterior structures reasonably well.</p><p>Varying the number of views per model. In the default setting, our training data consists of 8 views per object. Here we study the ability of our method in the more challenging case where the training data contains fewer number of views per object. To do so, we generate a new training set that contains only 1 randomly chosen view per object and use it to train PrGAN. We then repeat the experiments for 2 randomly chosen views per object, and also 4. The results are shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The 3D shapes that PrGAN learns becomes increasingly better as the number of views increases. Even in the single view per object case, our method is able to induce reasonable shapes. Visualizations across categories. Our method is able to generate 3D shapes for a wide range of categories. Fig. <ref type="figure" target="#fig_10">9</ref> show a gallery of results, including airplanes, car, chairs, vases, motorbikes. For each category we show 64 randomly sampled training images, 64 generated images from PrGAN, and renderings of 128 generated 3D shapes (produced by randomly sampling the 200-d input vector of the generator). One remarkable property is that the generator produces 3D shapes in a consistent horizontal and vertical axes, even though the training data is only consistently oriented along the vertical axis. Our hypothesis for this is that the generator finds it more efficient to generate shapes in a consistent manner by sharing parts across models. Fig. <ref type="figure" target="#fig_11">10</ref> shows selected examples from Fig. <ref type="figure" target="#fig_10">9</ref> that demonstrates the quality and diversity of the generated shapes.</p><p>The last row in Fig. <ref type="figure" target="#fig_10">9</ref> shows an example of a 'mixed' category, where the training images combine the three categories of airplane, car, and motorbike. The same PrGAN network is used to learn the shape distributions. Results show that PrGAN learns to represent all three categories well, without any additional supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape interpolation.</head><p>Once the generator is trained, any encoding z supposedly generates a plausible 3D shape, hence z represents a 3D shape manifold. Similar to previous work, we can interpolate between 3D shapes by linearly interpolating their z codes. Fig. <ref type="figure" target="#fig_8">7</ref> shows the interpolation results for two airplane models and two chair models.</p><p>Unsupervised shape and viewpoint prediction. Our method is also able to handle unsupervised prediction of shapes in 2D images. Once trained, the 3D shape generator is capable of creating shapes from a set of encodings z ∈ R 201 . One application is to predict the encoding of the underlying 3D object given a single view image of the object. We do so by using the PrGAN's generator to produce a large number of encoding-image pairs, then use the data to train a neural network (called encoding network). In other words, we create a training set that consists of images synthesized by the PrGAN and the encodings that generated them. The encoding network is fully connected, with 2 hidden layers, each with 512 neurons. The input of the network is an image and the output is an encoding. The last dimen- sion of z describes the view, and the first 200 dimensions describe the code of the shape, which allows us to further reconstruct the 3D shape as a 32 3 voxel grid. With the encoding network, we can present to it a single view image, and it outputs the shape code along with the viewing angle. Experimental results are shown in in Figure <ref type="figure" target="#fig_9">8</ref>. This whole process constitutes a completely unsupervised approach to creating a model that infers a 3D shape from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>Failure cases. Comprared to 3D-GANs, the proposed PrGAN models cannot discover structures that are hidden due to occlusions from all views. For example, it fails to discover that some chairs have concave interiors and the generator simply fills these since it does not change the silhouette from any view as we can see at Figure <ref type="figure" target="#fig_12">11</ref>. However, this is a natural drawback of view-based approaches since some 3D ambiquities cannot be resolved (e.g. necker cubes) without relying on other cues. Despite this, one advantage over 3D-GAN is that our model does not require consistently aligned 3D shapes since it reasons over viewpoints.</p><p>Higher-resolution models. Another drawback of our approach is that we currently generate low-resolution (32 3 ) shapes. This is an inherent limitation of voxel-based representations since the size of the voxel grid scales cubically with the resolution. Recent results in learning generative models of images using residual architectures <ref type="bibr" target="#b12">[13]</ref> and multi-scale reasoning <ref type="bibr" target="#b6">[7]</ref>, may help scale the resolution of generative models to the next few octaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Generated images</head><p>Generated shapes  Using multiple cues for shape reasoning. Our approach currently only relies on binary silhouettes for estimating the shape. This contributes to the lack of geometric details, which can be improved by incorporating shading cues. One strategy is to train a more powerful differentiable function approximator, e.g., a convolutional network, to replicate the sophisticated rendering pipelines developed by the computer graphics community. Once trained, the resulting neural renderer could be a plug-in replacement for the projection module in the PrGAN framework. This would allow the ability to use collections of realistically-shaded images for infering probabilistic models of 3D shapes and other properties. Recent work on sceen-space shading using convnets are promising <ref type="bibr" target="#b20">[22]</ref>.</p><p>Learning from real images. Our approach can be extended to learning 3D shapes from real images by applying an exisiting approach for segmentation such as <ref type="bibr" target="#b17">[19]</ref>. However, the assumption that the viewpoints are uniformly distributed over the viewing sphere may not hold. In this situation, one can either learn a distribution over viewpoints by mapping a few dimensions of the input code z to a distribution over viewpoints (θ, φ) using a multi-layer network. More generally, one can also learn a distribution over a full set of camera parameters. An alternative is learn a conditional generator where the viewpoint is provided as input to the algorithm. This may be obtained using a generic viewpoint estimator such as <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b26">28]</ref>. We will explore these directions in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>In summary, we have proposed a framework for infering 3D shape distributions from 2D shape collec- tions by agumenting a convnet-based 3D shape generator with a projection module. This compliments exisiting approches for non-rigid SfM since these models can be trained without prior knowledge about the shape family, and can generalize to categories with variable structure. We showed that our models can infer 3D shapes for a wide range of categories, and can be used to infer shape and viewpoint from a single image in a completely unsupervised manner. We believe that the idea of using a differentiable render to infer distributions over unobserved scene properties from images can be applied to other problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Given a collection of 2D views of multiple objects, our algorithm infers a generative model of the underlying 3D shapes.</figDesc><graphic coords="1,320.68,213.77,212.63,188.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The PrGAN architecture for generating 2D images of shapes. A 3D voxel representation (32 3 ) and viewpoint are independently generated from the input z (201-d vector). The projection module renders the voxel shape from a given viewpoint (θ, φ) to create an image. The discriminator consists of 2D convolutional and pooling layers and aims to classify if the input image is generated or real.</figDesc><graphic coords="3,303.63,99.75,95.42,95.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our dataset samples eight viewpoints evenly spaced around the y-axis (only four are shown and the remaining four are diametrically opposite to these). The top four images on the right are training images from the dataset. The bottom four images are created by using the projection module on the voxel representation of the object. Note that the top and bottom match qualitatively.</figDesc><graphic coords="4,50.11,72.00,224.43,82.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Results from 2D-GAN. (a) Results from PrGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparision between 2D-GAN [10] and our PrGAN model for image generation on the chairs dataset. Refer to Fig. 9 third row, left column for samples of the input data.</figDesc><graphic coords="4,308.86,150.59,59.06,59.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Results from 3D-GAN. (a) Results from PrGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison between 3D-GAN [34] and our PrGAN for 3D shape generation. The 3D-GAN is trained on 3D voxel representation of the chair models, and the PrGAN is trained on images of the chair models (refer to Fig. 9 third row).</figDesc><graphic coords="5,50.11,268.80,56.70,56.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Shapes generated from PrGAN by varying the number of views per object in the training data. From the top row to the bottom row, the number of views per object in the training set are 1, 2, 4, and 8 respectively.</figDesc><graphic coords="5,308.86,72.00,224.43,190.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Shape interpolation by linearly interpolating the encodings of the starting shape and ending shape.</figDesc><graphic coords="6,308.86,72.00,224.43,225.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Shape infered (right) by a single view image (left) using the encoding network. Input images were segmented, binarized and resized to match the network input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Results for 3D shape induction using PrGANs. From top to bottom we show results for airplane, car, chair, vase, motorbike, and a 'mixed' category obtained by combining training images from airplane, car, and motorbike. At each row, we show on the left 64 randomly sampled images from the input data to the algorithm, on the right 128 sampled 3D shapes from PrGAN, and in the middle 64 sampled images after the projection module is applied to the generated 3D shapes. The model is able to induce a rich 3D shape distribution for each category. The mixed-category produces reasonable 3D shapes across all three combined categories. Zoom in to see details.</figDesc><graphic coords="7,56.52,572.14,94.05,94.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. A variety of 3D shapes generated by PrGAN trained on 2D views of (from the top row to the bottom row) airplanes, cars, vases, and bikes. These examples are chosen from the gallery in Fig.9and demonstrate the quality and diversity of the generated shapes.</figDesc><graphic coords="8,308.86,324.29,224.44,111.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Our method is unable to capture the concave interior structures in this chair shape. The pink shapes show the original shape used to generate the projected training data, shown by the three binary images on the top (in high resolution). The blue voxel representation is the inferred shape by our model. Notice the lack of internal structure.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="402" xml:id="foot_0"><p>2017 International Conference on 3D Vision (3DV) 2475-7888/17/31.00 ©2017 IEEE DOI 10.1109/3DV.2017.00053</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research was supported in part by the NSF grants IIS-1617917, IIS-1423082 and ABI-1661259. The experiments were performed using equipment obtained under a grant from the Collaborative R&amp;D Fund managed by the Massachusetts Tech Collaborative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular 3D pose estimation and tracking by detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recovering intrinsic scene characteristics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>Comput. Vis. Syst., A Hanson &amp; E. Riseman</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3D shape from image streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004">2014. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sampleproblem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1971">2015. 1971</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Lightness and retinex theory</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visual hull concept for silhouette-based image understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">OpenDR: an approximate differentiable renderer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Nalbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arabadzhiyska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06078</idno>
		<title level="m">Deep shading: Convolutional neural networks for screen-space shading</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D structure from images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d generic object categorization, localization and pose estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient exact inference for 3d indoor scene understanding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Render for CNN: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008">2015. 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-view 3D models from single images with a convolutional network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pose induction for novel object categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">OpenGL programming guide: the official guide to learning OpenGL, version 1.2</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shreiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photometric method for determining surface orientation from multiple images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191139" to="191139" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005">2016. 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2015. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
