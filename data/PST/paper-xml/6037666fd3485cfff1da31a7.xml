<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Visual Object Tracking Via Adaptive Attribute-Aware Discriminative Correlation Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-21">January 21, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xue-Feng</forename><surname>Zhu</surname></persName>
							<email>xuefeng_zhu95@163.com</email>
							<idno type="ORCID">0000-0003-0262-5891</idno>
						</author>
						<author>
							<persName><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
							<email>xiaojun_wu_jnu@163.com</email>
							<idno type="ORCID">0000-0002-0310-5778</idno>
						</author>
						<author>
							<persName><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
							<email>tianyang.xu@surrey.ac.uk</email>
							<idno type="ORCID">0000-0002-9015-3128</idno>
						</author>
						<author>
							<persName><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
							<email>z.feng@surrey.ac.uk</email>
							<idno type="ORCID">0000-0002-4485-4249</idno>
						</author>
						<author>
							<persName><roleName>Life Member, IEEE</roleName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
							<email>j.kittler@surrey.ac.uk</email>
							<idno type="ORCID">0000-0002-8110-9205</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Computer Science</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<addrLine>Xue-Feng Zhu and Xiao-Jun Wu</addrLine>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of IoT</orgName>
								<orgName type="department" key="dep2">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<settlement>Wuxi</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Visual Object Tracking Via Adaptive Attribute-Aware Discriminative Correlation Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-21">January 21, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3050073</idno>
					<note type="submission">received August 12, 2020; revised December 2, 2020; accepted December 19, 2020. Date of publication January 8, 2021; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Discriminative correlation filter</term>
					<term>spatial attention</term>
					<term>visual attribute</term>
					<term>visual object tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, attention mechanisms have been widely studied in Discriminative Correlation Filter (DCF) based visual object tracking. To realise spatial attention and discriminative feature mining, existing approaches usually apply regularisation terms to the spatial dimension of multichannel features. However, these spatial regularisation approaches construct a shared spatial attention pattern for all multi-channel features, without considering the diversity across channels. As each feature map (channel) focuses on a specific visual attribute, a shared spatial attention pattern limits the capability for mining important information from different channels. To address this issue, we advocate channel-specific spatial attention for DCF-based trackers. The key ingredient of the proposed method is an Adaptive Attribute-Aware spatial attention mechanism for constructing a novel DCF-based tracker (A 3 DCF). To highlight the discriminative elements in each feature map, spatial sparsity is imposed in the filter learning stage, moderated by the prior knowledge regarding the expected concentration of signal energy. In addition, we perform a post processing of the identified spatial patterns to alleviate the impact of less significant channels. The net effect is that the irrelevant and inconsistent channels are removed by the proposed method. The results obtained on a number of well-known benchmarking datasets, including OTB2015, DTB70, UAV123, VOT2018, LaSOT, GOT-10 K and TrackingNet, demonstrate the merits of the proposed A 3 DCF tracker, with improved performance compared to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V ISUAL object tracking is a fundamental research topic in computer vision, with the aim to precisely and continuously estimate the state of a target of interest in a video. It is a very challenging task to achieve robust and efficient tracking under unconstrained scenarios, due to a wide spectrum of appearance variations of the target in a video. To improve the performance of a visual object tracker, various innovative ideas have been explored and a significant progress has been made in recent years. Among existing visual tracking algorithms, the Discriminative Correlation Filter (DCF) based tracking algorithms have exhibited encouraging performance and drawn widespread attention.</p><p>A DCF-based tracker formulates the learning objective as a ridge regression problem with a circulant matrix structure, which simplifies the filter optimisation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and thus results in the use of more sophisticated features <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and regularised filter learning <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Most existing DCF trackers use multiple types of features, such as Histogram of Oriented Gradient (HOG) <ref type="bibr" target="#b7">[8]</ref>, Colour Names (CN) <ref type="bibr" target="#b8">[9]</ref> and Convolutional Neural Network (CNN) features <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. For each feature type, multiple channel features capturing different properties of the visual content are extracted for filter learning. Besides the exploitation of powerful features, the regularisation of the estimated filters also plays an important role in robust visual tracking by endowing tracking systems with an attention mechanism, which helps to highlight the most discriminative and important visual information of a target. To this end, both fixed <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and adaptive spatial regularisation <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> have been explored in the DCF paradigm to achieve attention-based discriminative spatial appearance modelling. With powerful feature representations and spatial attention mechanisms, the DCF-based trackers have witnessed a continuous performance enhancement.</p><p>In spite of the success of DCF, many aspects such as the relevance of multi-channel feature maps, regularised learning formulation, spatial attention mechanisms and discriminative data fitting, have not been adequately explored <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. First, the extracted multi-channel features maps, which may exceed thousands of channels, include irrelevant and redundant information <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The filters trained with such feature maps often contain negligible energy and may degrade the performance of a DCF tracker <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>. So far there are only a very few works focusing on reducing the information redundancy of the feature maps in DCF-based tracking <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Besides, for spatial regularisation, existing DCF trackers exploit a shared spatial attention pattern across all the feature channels, which neglects the descriptive diversity and discriminative competitiveness of the features in different channels. To address the above issue, we propose to adaptively optimise the spatial configuration pattern for each feature map (channel), reflecting the intrinsic link between visual attributes and attention. In essence, we propose to learn Adaptive Attribute-Aware Discriminative Correlation Filters (A 3 DCF) for robust visual tracking.</p><p>We start by noting that each feature map in a tensor representation potentially emphasises a unique visual attribute (pattern), and different feature maps focus on different visual attributes. For example, each channel of CN features reflects one specific colour attribute <ref type="bibr" target="#b8">[9]</ref> and each channel of HOG features corresponds to different gradient orientations <ref type="bibr" target="#b7">[8]</ref>. Similarly, the semantic information captured by different deep CNN feature channels is typically distinct <ref type="bibr" target="#b18">[19]</ref>.</p><p>Based on the above observation, we propose to perform adaptive channel-wise spatial attention learning for correlation filters, based on the discrimination relevance of each feature channel and its corresponding filter. Specifically, we simultaneously optimise the filter coefficients and channel-wise binary spatial attention pattern in our paradigm. Through this attribute-aware scheme, the discriminative elements in each feature map are highlighted in support of enhanced parsimony and compactness. Moreover, by virtue of the advocated attention mechanism, the proposed A 3 DCF method accomplishes adaptive feature suppression by enforcing a prior constraint on the filters so that the energy is concentrated on the central region of a search window. In consequence, we eliminate irrelevant and interfering information in the learning stage, enhancing the discriminative capability of the trained model and resulting in better tracking performance.</p><p>We illustrate the learned attention patterns of our A 3 DCF method in Fig. <ref type="figure" target="#fig_0">1</ref>. In the figure, the first column is the search window centred on the position of the target. The second column contains the multi-channel CN feature maps of the search window. The third column presents the initial spatial patterns which are adaptively learned from the input features according to the visual attribute of each channel. The fourth column shows the final learned attribute-related spatial patterns obtained by performing a post processing of the preliminary spatial patterns. Specifically, the fourth column is generated by imposing shared spatial patterns, that is a prior constraint, on the third column. The final column displays the corresponding correlation filters optimised with the attribute-related spatial patterns. As some feature channels provide ambiguous appearance information, they can be considered as less representative or irrelevant channels. Through the spatial attention learned by the proposed adaptive attribute-aware mechanism, these irrelevant channels can be eliminated. Therefore, the adaptive spatial regularisation and feature selection across channels are realised simultaneously, resulting in enhanced discrimination of the learned filters.</p><p>To summarise, the main innovations of the proposed A 3 DCF method include: r A new adaptive attribute-aware scheme is proposed to em- phasise channel-specific discriminative features, invariably related to the corresponding attribute it represents. With the learned adaptive spatial attention patterns, irrelevant information of multi-channel features is significantly reduced and the boundary effect is alleviated.</p><p>r The proposed post processing of the initial attribute-related spatial patterns preserves only the feature channels representing discriminative attributes, while irrelevant and inconsistent channels are suppressed.</p><p>r An extensive evaluation is performed on a number of well-known benchmarks, i.e. OTB2015 <ref type="bibr" target="#b19">[20]</ref>, DTB70 <ref type="bibr" target="#b20">[21]</ref>, UAV123 <ref type="bibr" target="#b21">[22]</ref>, VOT2018 <ref type="bibr" target="#b22">[23]</ref>, LaSOT <ref type="bibr" target="#b23">[24]</ref>, GOT-10K <ref type="bibr" target="#b24">[25]</ref> and TrackingNet <ref type="bibr" target="#b25">[26]</ref>. The experimental results demonstrate the superior performance of the proposed A 3 DCF method over the state-of-the-art algorithms, in terms of both effectiveness and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are numerous reputable studies in visual object tracking. In this section, we briefly discuss the recent filter based approaches, i.e., Siamese networks and Discriminative Correlation Filters. The main objective of Siamese networks is to learn an offline mapping function producing similar features for naturally varying target appearance, while suppressing the surroundings. The filters are generated by embedding a target instance (template) into an appearance variation preserved feature space, where the target's centre corresponds to high response <ref type="bibr" target="#b26">[27]</ref>. In view of the diversity of target categories, CFnet <ref type="bibr" target="#b27">[28]</ref> was proposed to generate adaptive filters with an additional structure to reflect the template appearance for different sequences better. To further improve the precision of the target centre location, other approaches were developed to support more complicated network constructions <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. To accurately predict the bounding box, the SiameseRPN framework <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b34">[35]</ref> was designed to simultaneously optimise the classification loss and a bounding box regression loss. Though advanced efficiency can be achieved by Siamese networks via offline learning the mapping function, the relevance and causality between the embedding features and the response generators are ambiguous and of limited interpretability.</p><p>Regarding DCF approaches, thanks to the seminal work of MOSSE <ref type="bibr" target="#b35">[36]</ref>, DCF has received much attention in visual object tracking, especially since the development of the CSK tracker <ref type="bibr" target="#b0">[1]</ref> that embeds kernel tricks in the circulant structure <ref type="bibr" target="#b36">[37]</ref>. Considering the importance of robust feature representation <ref type="bibr" target="#b37">[38]</ref> and the unexpected boundary effect produced by the implicit assumption of periodicity of the input signal induced by circulant samples <ref type="bibr" target="#b38">[39]</ref>, a variety of advanced DCF-based trackers have been proposed <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref>. As robust feature extraction is crucial to advanced DCF training, Henriques et al. adopted HOG features in KCF <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr">Danelljan et al.</ref> proposed to employ CN features <ref type="bibr" target="#b42">[43]</ref>, resulting in improved performance, as compared with the trackers using greyscale representations. Recently, due to the remarkable performance of Convolutional Neural Networks (CNN), deep features have been widely used in DCF-based trackers and shown to be instrumental in achieving advanced tracking performance <ref type="bibr" target="#b43">[44]</ref>. Modern DCF trackers usually use multiple features, such as HOG, CN and CNN, collaboratively for robust feature extraction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b44">[45]</ref>. However, these DCF-based trackers simply assemble all the features for learning discriminative filters, without considering the effect of the potentially inconsistent and redundant information they may convey, which may degrade the performance of the learned filters. These deficiencies motivate the development of our A 3 DCF which is designed to reduce the redundancy and interference of feature representations in the filter learning stage, which leads to improved robustness and better discriminative ability of the learned model.</p><p>To cope with the issue of spatial boundary effect, Danelljan et al. proposed the spatially regularised DCF method that introduced a fixed opposite Gaussian-shaped spatial weighting mask for correlation filters learning <ref type="bibr" target="#b38">[39]</ref>, concentrating the energy of the learned filters on the central target region. Similarly, CSRDCF <ref type="bibr" target="#b5">[6]</ref> proposed to generate a mask using colour-histogram-based image segmentation to suppress the background area. In the same spirit, BACF <ref type="bibr" target="#b4">[5]</ref> employs a predefined binary matrix to crop valid training samples. In contrast to a fixed spatial mask, an adaptive spatial regularisation is proposed in LADCF <ref type="bibr" target="#b12">[13]</ref> to learn an adaptive DCF via spatial feature selection. The above trackers effectively alleviate the problem of boundary effect and achieve convincing results in recent benchmarks and competitions <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref>. Nevertheless, these methods enforce the same attention mechanism on all filter channels and therefore cannot adapt to the diversity of visual attributes linked to different feature maps. We argue, therefore, that it is absolutely essential to identify an attribute-related spatial pattern for each filter channel separately, so as to enhance discrimination and simultaneously alleviate the boundary effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ADAPTIVE ATTRIBUTE-AWARE DCF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Standard DCF Formulation</head><p>We first briefly revisit the standard DCF tracking approach that follows a 2-stage tracking-learning framework for online object tracking. Let us assume that the location of a target in the t-th frame is known. Given this information, multi-channel correlation filters H are trained by minimising the following objective function in the learning stage:</p><formula xml:id="formula_0">E(H) = 1 2 K k=1 X k H k − Y 2 F + λ 2 K k=1 H k 2 F , (1)</formula><p>where X k ∈ R N ×N is the k-th channel feature representation of an image patch (search window) centred on the position of the predicted target, H k ∈ R N ×N denotes the corresponding k-th channel filter. Y ∈ R N ×N stands for the desired detector response map <ref type="bibr" target="#b0">[1]</ref> of Gaussian shape. K is the number of feature channels and λ is a regularisation parameter. • F means Frobenius norm and is the circular convolution operator <ref type="bibr" target="#b1">[2]</ref>. With the circulant structure <ref type="bibr" target="#b36">[37]</ref> and Fourier transform <ref type="bibr" target="#b48">[49]</ref>, the optimisation of Eqn. (1) can efficiently be solved in the frequency domain <ref type="bibr" target="#b1">[2]</ref>.</p><p>To prevent their temporal degradation, the final filters are updated online after obtaining the trained discriminative correlation filters H in the t-th frame:</p><formula xml:id="formula_1">H t = (1 − η)H t−1 + ηH, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where η ∈ (0, 1) is the online updating rate. Then the updated filters H t are used to localise the target in the (t+1)-th frame:</p><formula xml:id="formula_3">R = F −1 K k=1 Ẑt+1 k Ĥt k ,<label>(3)</label></formula><p>where F −1 denotes the inverse Discrete Fourier Transform (DFT), • denotes DFT and denotes point-wise multiplication.</p><formula xml:id="formula_4">Z t+1 k ∈ R N ×N</formula><p>is the k-th channel of multi-channel features extracted from the search region in the (t+1)-th frame and K is the number of feature channels. R ∈ R N ×N is the response map, in which the location of the maximal value is considered as the predicted target location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Attribute-Aware DCF Formulation</head><p>In the classical DCF paradigm, the filters are learned from multi-channel features extracted from a search window enclosing the target. However, these multi-channel features are often irrelevant and may contain inconsistent information that degrades the discriminative capability of the learned model. To address this issue, attention mechanisms realised by a spatial regularisation have been widely studied in recent advanced DCF-based trackers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In this work, noting that each feature map (channel) reflects one specific visual attribute, we propose an adaptive attribute-aware scheme in the filter learning stage to highlight the discriminative part of each channel. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the adaptive attribute-aware correlation filters can be learned by optimising the objective:</p><formula xml:id="formula_5">E(H, P ) = 1 2 K k=1 X k (H k P k ) − Y 2 F + λ 1 2 K k=1 H k 2 F + λ 2 2 K k=1 P k − P r k 2 F , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where R N ×N is the spatial attention pattern, denoted as a binary matrix, corresponding to the k-th channel of filters. P r k ∈ R N ×N is a predefined binary matrix with the values of one for the target and zero for others. λ 1 and λ 2 are regularisation parameters. In Eqn. (4), the first term is the data fitting term, where the adaptive spatial attention patterns P and filters H are optimised simultaneously. The second term is a regularisation term to prevent over-fitting. The third term incorporates the prior requirement for the filters to concentrate their energy on the central region of the search window. This is expressed in the form of a binary mask, P r . It should be noted that each channel of P r shares the same binary values.</p><formula xml:id="formula_7">X k ∈ R N ×N is the k-th channel feature map, H k ∈ R N ×N is the corresponding k-th correlation filter and Y ∈ R N ×N is the desired Gaussian shaped response map. P k ∈</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimisation</head><p>According to the existing DCF design methodology, the optimisation step is usually performed in the Fourier domain for efficient filter learning and target detection. To obtain an efficient solution of Eqn. ( <ref type="formula" target="#formula_5">4</ref>), the attention constraint can be formalised as H ≡ P H. Additionally, an auxiliary variable, G, with the constraint G − H = 0, is introduced. We can now employ the following Augmented Lagrange Method <ref type="bibr" target="#b49">[50]</ref> in the frequency domain:</p><formula xml:id="formula_8">L( Ĝ, Ĥ, P, Γ) = 1 2 K k=1 Xk Ĝk − Ŷ 2 F + λ 1 2 K k=1 Ĥk 2 F + λ 2 2 K k=1 P k − P r k 2 F + K k=1 tr ΓT k ( Ĝk − Ĥk ) + μ 2 K k=1 Ĝk − Ĥk 2 F , (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where Γ, μ, and tr denote the Lagrangian multiplier, penalty factor, and trace calculation, respectively. It can be reformulated as follows by combining the last two terms:</p><formula xml:id="formula_10">L( Ĝ, Ĥ, P, Γ) = 1 2 K k=1 Xk Ĝk − Ŷ 2 F + λ 1 2 K k=1 Ĥk 2 F + λ 2 2 K k=1 P k − P r k 2 F + μ 2 K k=1 Ĝk − Ĥk + Γk μ 2 F , (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where Γ is the Lagrange multiplier and μ is a penalty factor. The Lagrangian in Eqn. ( <ref type="formula" target="#formula_10">6</ref>) can be minimised by the Alternating Direction Method of Multipliers (ADMM) by solving the following sub-problems: Sub-problem G: Given Ĥ, P and Γ, Ĝ can be updated by solving:</p><formula xml:id="formula_12">Ĝ = arg min Ĝ 1 2 K k=1 Xk Ĝk − Ŷ 2 F + μ 2 K k=1 Ĝk − Ĥk + Γk μ 2 F . (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>To solve this problem efficiently, we decompose it to process simultaneously all channels of each spatial unit. Decomposing Eqn. <ref type="bibr" target="#b6">(7)</ref> into NN sub-problems as :</p><formula xml:id="formula_14">V j ( Ĝ) = arg min V j ( Ĝ) 1 2 V j ( X) T V j ( Ĝ) − Ŷj 2 2 + μ 2 V j ( Ĝ) − V j ( Ĥ) + 1 μ V j ( Γ) 2 2 , (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>where V j ( Ĝ) ∈ R K×1 denotes the vector containing the j-th spatial unit of Ĝ with elements across all the channels, we can apply the Sherman-Morrison formula to obtain the closed-form solution to Eqn. (8) as:</p><formula xml:id="formula_16">V j ( Ĝ) = 1 μ I − V j ( X)V j ( X) T μ + V j ( X) T V j ( X) q,<label>(9)</label></formula><p>where q = V j ( X) Ŷj + μV j ( Ĥ) − V j ( Γ).</p><p>Sub-problem H: Given Ĝ and Γ, each channel of Ĥ can be updated by solving the following sub-problem:</p><formula xml:id="formula_17">Ĥk = arg min Ĥk λ 1 2 Ĥk 2 F + μ 2 Ĝk − Ĥk + Γk μ 2 F</formula><p>. <ref type="bibr" target="#b9">(10)</ref> This sub-problem can be easily solved by setting the derivative to zero, and the closed-form solution is given by:</p><formula xml:id="formula_18">Ĥk = μ Ĝk + Γk λ 1 + μ . (<label>11</label></formula><formula xml:id="formula_19">)</formula><p>Sub-problem P: Since solving P is a binary optimisation problem, we relax and embed it into filters H. First, we introduce a variable J as J k = H k P k and a variable J r as J r k = H k P r k . Then given Ĝ, H and Γ, Ĵ can be solved by optimising:</p><formula xml:id="formula_20">Ĵ = arg min Ĵ 1 2 K k=1 Xk Ĵk − Ŷ 2 F + λ 2 2 K k=1 Ĵk − Ĵr k 2 F . (<label>12</label></formula><p>) Note that Eqn. ( <ref type="formula" target="#formula_20">12</ref>) can be solved in the same way as Eqn. <ref type="bibr" target="#b6">(7)</ref>. Hence we get the analytical solution as:</p><formula xml:id="formula_21">V j ( Ĵ) = 1 λ 2 I − V j ( X)V j ( X) T λ 2 + V j ( X) T V j ( X) × (V j ( X) Ŷj +λ 2 V j ( Ĵr )).<label>(13)</label></formula><p>In Eqn. <ref type="bibr" target="#b12">(13)</ref>, J r can be computed directly with predefined P r . Once J is found in the spatial domain by the inverse DFT of Ĵ, we subject it to a binarisation process to produce the binary matrix P. To apply binarisation, we set the first r% values in each channel to one and all the others to zero, according to the absolute value of J. In fact, in the t-th frame (except the first frame), for temporal consistency, we apply binarisation to J t to obtain P and J t = (1 − γ)J t−1 + γJ, where γ is a predefined constant.</p><p>The P is the initial attribute-related spatial patterns, as exhibited in the third column of Fig. <ref type="figure" target="#fig_0">1</ref>. To obtain the final attribute-related spatial patterns, a post processing operation using the identified spatial patterns, namely the prior constraint P r , is performed to update P. The specific steps are summarised as follows.</p><p>Post Processing: After obtaining the initial attribute-related spatial attention patterns, P, as illustrated in the third column of Fig. <ref type="figure" target="#fig_0">1</ref>, a post processing operation is applied to P. Specifically, we update P by enforcing a prior constraint as:</p><formula xml:id="formula_22">P k = P k P r k . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>The updated P are the final attribute-related spatial attention patterns as shown in the fourth column of Fig. <ref type="figure" target="#fig_0">1</ref>. By doing this, the spatial attention patterns focus more on the central region and alleviate the impact of less representative channels. Update H: After updating channel-specific spatial patterns P, we update the filters H by enforcing the attention patterns on filters H as:</p><formula xml:id="formula_24">H k = H k P k . (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>It should be noted that the filters H are updated in the spatial domain and should be transformed into the frequency domain to update the Lagrangian multipliers. Update Lagrangian Multiplier: Given Ĝ, Ĥ and μ, the Lagrangian multipliers Γ can be updated as :</p><formula xml:id="formula_26">Γ = Γ + μ( Ĝ − Ĥ). (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>After each iteration, the step size parameter μ is updated as:</p><formula xml:id="formula_28">μ = min(μ max , ρμ),<label>(17)</label></formula><p>where μ max denotes the predefined maximum value of μ, and ρ &gt; 1 is a factor that controls the convergence speed. Therefore, the optimisation of our proposed A 3 DCF can be processed by iteratively employing the above steps. The total complexity of our optimisation framework is O(LKN 2 log(N )), where L is the number of iterations. After optimisation, the filters H are used to update the filter model and localise the target in the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Online Update and Object Detection</head><p>For adaptation to appearance variations, the model is online updated by utilising the same updating strategy as other DCF-based trackers, as presented in Eqn. <ref type="bibr" target="#b1">(2)</ref>. Additionally, similar to many other DCF-based trackers, we follow the fast Discriminative Scale Space Tracking (fDSST) method <ref type="bibr" target="#b50">[51]</ref> to achieve position localisation and scale estimation simultaneously. When a new frame becomes available, multiple scales a s of the search region {X s } s∈{ 1−S 2 ,..., S−1 2 } are analysed to extract features, where S is the number of scales. Then a filter, H s , is designed for each X s in the Fourier domain to obtain multi-channel response maps by using Eqn. (3). The position and scale of target p t and s t are predicted according to the maximum of these response maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>The proposed A 3 DCF method is implemented in MAT-LAB2019a on a platform with one Intel i7-9700 3.00GHZ CPU and a single NVIDIA GeForce GTX 1660Ti GPU. For feature extraction, we use three hand-crafted features including HOG, CN and Intensity Channels, and the Res4e layer of the pretrained ResNet-50 model as deep CNN features. We set λ 1 , λ 2 in Eqn. ( <ref type="formula" target="#formula_5">4</ref>) and γ as λ 1 = 100, λ 2 = 1 and γ = 0.05. The number of iterations L is set to L = 2. For hand-crafted features, we set the corresponding parameters as η = 0.01, r = 5. For deep features, we set the corresponding parameters as η = 0.003, r = 20. The source code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Settings</head><p>We extensively evaluate our A 3 DCF tracker on four benchmarks including OTB2015 <ref type="bibr" target="#b19">[20]</ref>, DTB70 <ref type="bibr" target="#b20">[21]</ref>, UAV123 <ref type="bibr" target="#b21">[22]</ref>, VOT2018 <ref type="bibr" target="#b22">[23]</ref>, LaSOT <ref type="bibr" target="#b23">[24]</ref>, GOT-10K <ref type="bibr" target="#b24">[25]</ref> and Track-ingNet <ref type="bibr" target="#b25">[26]</ref>, compared with numerous state-of-the-art visual object tracking approaches, such as ASRCF <ref type="bibr" target="#b15">[16]</ref>, GFSDCF <ref type="bibr" target="#b13">[14]</ref>, BACF <ref type="bibr" target="#b4">[5]</ref>, SRDCF <ref type="bibr" target="#b38">[39]</ref>, Staple <ref type="bibr" target="#b51">[52]</ref>, STAPLE_CA <ref type="bibr" target="#b6">[7]</ref>, MD-Net <ref type="bibr" target="#b52">[53]</ref>, GOTURN <ref type="bibr" target="#b53">[54]</ref>, CF2 <ref type="bibr" target="#b54">[55]</ref> CSRDCF <ref type="bibr" target="#b5">[6]</ref>, C-COT <ref type="bibr" target="#b2">[3]</ref>, ECO <ref type="bibr" target="#b3">[4]</ref>, CREST <ref type="bibr" target="#b55">[56]</ref>, MCPF <ref type="bibr" target="#b56">[57]</ref>, STRCF <ref type="bibr" target="#b17">[18]</ref>, LADCF <ref type="bibr" target="#b12">[13]</ref>, SiamFC <ref type="bibr" target="#b26">[27]</ref>, DSiam <ref type="bibr" target="#b29">[30]</ref>, CFNet <ref type="bibr" target="#b27">[28]</ref>, StructSiam <ref type="bibr" target="#b57">[58]</ref>, LSART <ref type="bibr" target="#b58">[59]</ref>, DRT <ref type="bibr" target="#b59">[60]</ref>, MFT <ref type="bibr" target="#b22">[23]</ref>, SiamRPN <ref type="bibr" target="#b31">[32]</ref>, GCT <ref type="bibr" target="#b60">[61]</ref>, GradNet <ref type="bibr" target="#b61">[62]</ref>, UPDT <ref type="bibr" target="#b16">[17]</ref>, TADT <ref type="bibr" target="#b62">[63]</ref> and fdKCF <ref type="bibr" target="#b63">[64]</ref> respectively.</p><p>For OTB2015, DTB70, UAV123 and LaSOT, we use the precision plot (measure centre location error) and success plot (measure bounding box overlap ratio) as metrics. Besides, four numerical criteria, i.e., Distance Precision (DP), Overlap Precision (OP), Area Under Curve (AUC) and Frames Per Second (FPS) are used in the evaluation. For VOT2018, Expected Average Overlap (EAO), Accuracy (A) and Robustness (R) are used as criteria for performance evaluation. For GOT-10 K, Average Overlap (AO) and Success Rate (SR) are used to evaluate the performance of a tracker. For TrackingNet, Precision, Normalized Precision and Success are employed to quantitatively compare the performance of the algorithms. Specifically, DP is the percentage of location errors within a threshold of 20 pixels. OP is the percentage of overlap ratios surpassing a threshold of 0.5. AUC is the value of area under the curve of success plot. FPS measures the speed of a tracker. EAO is the main evaluation metric in VOT challenges which considers both bounding box overlap and failures (robustness). The A measure denotes the accuracy value of a tracker. The R measure quantifies the robustness of a tracker (the lower the better). The AO metric is the average of overlap rates between the tracking results and groundtruth over all the video frames. SR is the percentage of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self Analysis</head><p>Parameters Analysis: In this section, we first analyse the sensitivity of our A 3 DCF tracker to parameters λ 1 , λ 2 in Eqn. (4). In Fig. <ref type="figure" target="#fig_3">3</ref>, we report the variation of the AUC scores obtained by A 3 DCF on OTB2015 as a function of different values of λ 1 and λ 2 . Our A 3 DCF achieves stable performance (within 2% in terms of AUC) with λ 1 , λ 2 ∈ [0.01, 100].</p><p>Feature Configurations Analysis: Additionally, we employ different feature configurations to test our method and compare the results obtained by the state-of-the-art trackers using the same features. In Table <ref type="table" target="#tab_0">I</ref>, the OP scores and tracking speed on OTB2015 of different trackers are provided. Our A 3 DCF performs better than the other trackers regardless of the features adopted, demonstrating the merits of the proposed formulation. Ablation Experiments and Analysis: We further perform ablation studies to demonstrate the effectiveness of the key components of the proposed A 3 DCF tracker, including the attribute-aware attention (AAA) and post processing (PP) modules. Besides, we conduct an experiment to demonstrate the benefits of learning the attention maps for feature channels separately over the learning of a general attention map (GAM) for all the channels jointly. The baseline tracker is the original DCF equipped with the same hand-crafted and deep features, and the same updating rate as our A 3 DCF. We construct 5 trackers, including BaseLine, BaseLine_AAA, BaseLine_PP, Base-Line_GAM, BaseLine_ALL (A 3 DCF).</p><p>According to the results reported in Table <ref type="table" target="#tab_0">II</ref>, in general, the proposed attribute-aware attention and post processing modules improve the performance of the original DCF (BaseLine). In comparison with the BaseLine tracker, the attribute-aware attention and post processing modules improve the performance by 5.6%/ 6.3% and 6.0%/ 7.2% in terms of AUC and DP respectively. Note that the contributions of the proposed attribute-aware attention and post processing mechanisms to our A 3 DCF are not linearly additive. The proposed post processing stage enables the learned channel-wise attribute-aware attention maps to be more discriminative by concentrating the energy around the target centre. Besides, thanks to post processing, some irrelevant channels in the filters are removed. In this way, implicit feature selection across the channels is realised and the performance of BaseLine_ALL is significantly improved. Moreover, all the configurations of the BaseLine_GAM tracker are the same as the BaseLine_ALL tracker, except that BaseLine_GAM learns a general attention map for all the channels. As shown in the table, the performance of BaseLine_ALL is better than the performance of BaseLine_GAM. This demonstrates that learning attention for each channel separately is better than learning a global attention map for all the channels jointly. These results verify the effectiveness of the proposed spatial attention mechanism implemented via our adaptive attribute-aware strategy.</p><p>Merit Analysis: To intuitively demonstrate the superiority of the proposed attention mechanism, we visualize the attention maps and the corresponding filters of our A 3 DCF and the other two trackers in Fig. <ref type="figure" target="#fig_4">4</ref>. The existing DCF-based algorithms usually adopt a general spatial regularisation pattern for all the feature channels, which is not optimal for individual feature channels. In the figure, SRDCF and GFSDCF employ a fixed attention map for all the feature maps, while our A 3 DCF learns attention maps for each channel adaptively. It is apparent that the filters trained by our algorithm concentrate more on the target  area, thus enhancing the prominence of the most discriminative part of each input feature channel. As a result, the proposed attention mechanism is able to increase the discriminative capability and robustness of the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracking Failures and Demerit Analysis:</head><p>We present several failed tracking cases in Fig. <ref type="figure" target="#fig_5">5</ref>, and analyse the demerits of our A 3 DCF. From the figure, our method is incapable of dealing with the situations when the target is absent for many frames, which causes the tracking failures in the videos such as Bird1 and Soccer of dataset OTB2015. Additionally, DCF-based trackers are unable to estimate the size of the object precisely, when the scale of the target changes rapidly, especially when the aspect ratio of the target varies significantly over short time. As can be seen in the video sequence Jump, the aspect ratio of the target changes abruptly. Consequently, the bounding boxes predicted by all the exhibited DCF-based trackers are inaccurate. Besides, due to the strong semantics of the deep CNN features and their dominating influence on target localisation, the interference of similar objects to the target may cause tracking failures. As shown in video sequence Coupon, almost all the trackers using deep CNN features misinterpret the distractor as the target, while the trackers that use only shallow hand-crafted features such as BACF and SRDCF are able to track successfully. In summary, although our A 3 DCF has achieved the state-of-the-art performance, there is still a scope for improvement to rectify the imperfections due to the inherent limitations of the DCF framework, especially compared with the recent trackers that are offline trained with large amounts of video sequences.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison With SOTA Methods</head><p>VOT2018 has 60 challenging videos. We report the results on VOT2018 in Table <ref type="table">.</ref> Our A 3 DCF achieves the best EAO score, 0.406, outperforming recent advanced DCF trackers. Besides, A 3 DCF achieves advanced performance in terms of Accuracy (0.548) and Robustness (0.162) respectively.</p><p>OTB2015 contains 100 video sequences with 11 challenging attributes. We compare our method with other 10 trackers with deep structures/features on OTB2015. The precision and success plots with DP and AUC scores are presented in Fig. <ref type="figure">6</ref>. Our A 3 DCF achieves 92.5% in DP and 69.6% in AUC. Despite the DP score of A 3 DCF falling 0.75% behind the best tracker GFSDCF, our method achieves the top ranking AUC score, which is the same as the tracker LADCF.</p><p>DTB70 is a benchmark dataset of high diversity consisting of 70 videos captured by drone cameras. The precision and success plots with DP and AUC scores are exhibited in Fig. <ref type="figure" target="#fig_6">7</ref>. Our A 3 DCF achieves the best DP and AUC scores, namely 78.4% and 51.8% respectively. Compared with the second best tracker, our method achieves gains of 4.3% and 1.5% in terms of DP and AUC respectively.</p><p>UAV123 is a dataset composed of 123 challenging video sequences. We report the precision and success plot in Fig. <ref type="figure" target="#fig_7">8</ref>  LaSOT is a dataset consisting of 1400 sequences in 70 categories with an average length of more than 2500 frames in each sequence. In this paper, we use a simplified official version of LaSOT that contains 280 video sequences, 4 videos of each category. The precision and success plots are presented in Fig. <ref type="figure" target="#fig_8">9</ref>.</p><p>From the plots, A 3 DCF performs the best, achieving 38.2% and 37.6% in terms of DP and AUC scores, respectively.</p><p>GOT-10 K is a large benchmarking dataset for visual object tracking. It has 180 test video sequences in 84 categories. We report the results obtained on GOT-10 K in Table <ref type="table" target="#tab_1">IV</ref>. SR 0.5 and SR 0.75 are the SR metric with a threshold of 0.5 and 0.75 respectively. According to the table, A 3 DCF achieves the best AO and SR 0.5 scores of 0.427 and 0.467 respectively, outperforming the second best tracker CFNet by 14.2% and 16.0%.</p><p>TrackingNet is a large-scale tracking dataset that consists of 511 test videos. The evaluation of 7 advanced trackers on TrackingNet is presented in Table V. Our A 3 DCF surpasses other trackers on all three evaluation metrics. In detail, A 3 DCF outperforms the second best tracker, GFSDCF, with an improvement of 2.6%, 2.5% and 1.4% on Success, Precision and Normalized Precision respectively.</p><p>Furthermore, we present the evaluation results on OTB2015, UAV123 and LaSOT in terms of the OP criterion in Table <ref type="table" target="#tab_3">VI</ref>. The proposed A 3 DCF achieves 89.4%, 67.6% and 42.3% on OTB2015, UAV123 and LaSOT respectively. This top ranking performance achieves gains of 0.4% and 0.9% over the second best tracker GFSDCF on OTB2015 and LaSOT. On UAV123, A 3 DCF achieves the second best OP score, falling behind the best tracker GFSDCF by 0.2%. Compared with the state-of-the-art DCF-based tracker GFSDCF, we have realised adaptive feature selection across spatial and channel in another way. Overall, as a DCF-based tracker, the performance of our A 3 DCF is competitive.</p><p>Attribute-based Evaluation: We report an attribute-based evaluation of 14 trackers on OTB2015. Fig. <ref type="figure" target="#fig_0">10</ref> shows the success plots with AUC scores in terms of 11 video attributes including in-plane rotation, low resolution, deformation, outof-plane rotation, scale variation, fast motion, background clutter, occlusion, out of view, illumination variation and motion blur. From the AUC scores shown in the legend, our A 3 DCF outperforms other trackers in 5 attributes. In other challenging attributes, our method is among the top four performers. The merits of our attribute-aware mechanism, which adaptively selects the most discriminative and relevant information from a search region and ignores irrelevant and interfering information, is particularly evident when the target encounters appearance variations in rotation, deformation, and scale.</p><p>Qualitative Comparison: To intuitively demonstrate the advantages of our method, in Fig. <ref type="figure" target="#fig_9">11</ref>, we provide a qualitative comparison of 10 trackers including A 3 DCF, GFSDCF, ASRCF, STRCF, ECO, MCPF, TRACA, CREST, BACF and SRDCF respectively, on several video sequences. Although these sequences are challenging with severe appearance variations, our A 3 DCF performs accurately and steadily. The model trained with the attention mechanism accomplished by the proposed adaptive attribute-aware strategy is able to focus on the discriminative part of the feature input, while paying less attention to irrelevant information. Evidently, our A 3 DCF can successfully handle these complicated scenarios, delivering desired performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, to achieve robust visual object tracking, we developed an adaptive attribute-aware scheme for discriminative correlation filter learning. The major innovation of the proposed adaptive attribute-aware tracking method is to enable a channel specific regularisation, which has the ability to identify the discriminative information present in each feature channel. This information is typically linked to different visual attributes of the tracked target. Combined with an appropriate post processing in the filter learning stage, irrelevant channels are suppressed and inconsistent channels are removed by the proposed method. By alleviating the impact of irrelevant information, our model becomes more discriminative and robust in dealing with complex tracking situations. The results of extensive experimental studies on OTB2015, DTB70, UAV123, VOT2018, LaSOT, GOT-10 K and TrackingNet, demonstrate the superiority of our A 3 DCF method over the state-of-the-art trackers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the adaptive attribute-related spatial patterns. Noting that each feature map (channel) focuses on one exclusive attribute, the proposed adaptive attribute-aware spatial configurations are optimised jointly by discriminative data fitting.</figDesc><graphic url="image-1.png" coords="2,43.55,66.41,247.70,168.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of the pipeline of the proposed A 3 DCF. The adaptive attribute-related spatial patterns and attribute-aware discriminative filters are simultaneously optimised in our design. In the green box, the specific process of optimising the adaptive attribute-related spatial patterns and attribute-aware filters is presented. The diversity across different feature maps is considered in our attention mechanisms to support enhanced discrimination and to alleviate irrelevant appearance information.</figDesc><graphic url="image-2.png" coords="4,82.67,67.37,432.98,204.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:12:35 UTC from IEEE Xplore. Restrictions apply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. AUC score of A 3 DCF on OTB2015 dataset with different regularisation parameters λ 1 and λ 2 , ranging from 0.01 to 100.</figDesc><graphic url="image-3.png" coords="6,353.16,148.15,154.56,143.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Visualisation of attention maps and filters using the frame #28 of Jogging-2 in OTB2015. We visualise 4 channels of the CN feature maps, attention maps and corresponding learned filters.</figDesc><graphic url="image-4.png" coords="6,335.15,338.57,197.54,141.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Showing some failed cases on challenging video sequences including Bird1, Soccer, Coupon.The colour bounding boxes are the corresponding results of A 3 DCF, GFSDCF, ASRCF, STRCF, ECO, MCPF, TRACA, CREST, BACF, and SRDCF respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Experimental results on DTB70. This figure shows the precision and success plots in terms of the OPE protocol. The DP and AUC score of each tracker is shown in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Experimental results on UAV123. This figure shows the precision and success plots in terms of the OPE protocol. The DP and AUC score of each tracker is shown in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Experimental results on LaSOT. This figure shows the precision and success plots in terms of the OPE protocol. The DP and AUC score of each tracker is shown in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Illustration of qualitative experimental results on challenging sequences including Biker, Ironman, Matrix, Panda, MotorRolling, Rubik, Skating2-1 and ClifBar. The colour bounding boxes are the corresponding results of A 3 DCF, GFSDCF, ASRCF, STRCF, ECO, MCPF, TRACA, CREST, BACF, and 0.5,0.2,0.5SRDCF respectively.</figDesc><graphic url="image-6.png" coords="10,64.91,67.37,467.54,199.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="7,85.79,155.45,418.22,217.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TRACKING</head><label>I</label><figDesc>RESULTS WITH DIFFERENT FEATURES ON OTB2015 IN TERMS OF OP SCORES AND FPS</figDesc><table><row><cell>TABLE II</cell></row><row><cell>COMPARISON OF THE BASELINE, BASELINE_AAA, BASELINE_PP,</cell></row><row><cell>BASELINE_GAM, BASELINE_ALL ON THE OTB2015 DATASET</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc>OF 8 ADVANCED TRACKERS ON THE GOT-10 K DATASET IN TERMS OF AO, SR 0.5 , AND SR 0.75 . THE TOP THREE RESULTS ARE SHOWN IN RED, BLUE, AND GREEN COLOURS RESPECTIVELY Fig. 6. Experimental results on OTB2015. This figure shows the precision and success plots in terms of the OPE protocol. The DP and AUC score of each tracker is shown in the legend.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. The DP and AUC scores are shown in the figure legend. In terms of DP and AUC scores, our A 3 DCF outperforms all the other trackers, achieving 53.5% and 77.8% respectively. Compared with the second best tracker GFSDCF, A 3 DCF achieves gains of 1.4% in DP.</figDesc><table /><note>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:12:35 UTC from IEEE Xplore. Restrictions apply.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EVALUATION</head><label>V</label><figDesc>OF 7 ADVANCED TRACKERS ON THE TRACKINGNET DATASET IN TERMS OF SUCCESS, PRECISION, AND NORMALIZED PRECISION. THE TOP THREE RESULTS ARE SHOWN IN RED, BLUE, AND GREEN COLOURS RESPECTIVELY TABLE VI THE OP SCORES OF TRACKERS ON THE OTB2013, UAV123 AND LASOT DATASETS. THE TOP THREE RESULTS ARE SHOWN IN RED, BLUE, AND GREEN COLOURS RESPECTIVELY Fig. 10. Success plots of 14 trackers on the OTB2015 dataset in terms of 11 challenging attributes. The AUC scores are shown in the legend.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:12:35 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported by the National Natural Science Foundation of China under Grants 62020106012, U1836218, and 61672265, the 111 Project of Ministry of Education of China under Grant B12018, and the Engineering and Physical Sciences Research Council (EPSRC) under Grants EP/N007743/1, MURI/EPSRC/DSTL, EP/R018456/1. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Jianguo Zhang.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative filter with channel and spatial reliability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-aware correlation filter tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1396" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning color names for real-world applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1512" to="1523" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
				<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5596" to="5609" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint group feature selection and discriminative filter learning for robust visual object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7950" to="7960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning low-rank and sparse discriminative correlation filters for coarse-to-fine visual object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3727" to="3739" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive spatially-regularized correlation filters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4670" to="4679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning spatialtemporal regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4904" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">STM: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st AAAI Conf. Artif. Intell</title>
				<meeting>31st AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="0" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmark and simulator for Uav tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lasot: A. high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2957464</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell., to be published</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="300" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2805" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SANet: Structure-aware network for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1763" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4854" to="4863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SiamRPN++ : Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for realtime visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7952" to="7961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toeplitz and circulant matrices: A. review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations Trends Commun. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="239" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding and diagnosing visual tracking systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3101" to="3109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-correlation filters with triangle-structure constraints for object tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1122" to="1134" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning linear regression via singleconvolutional layer for visual object tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust visual tracking via constrained multi-kernel correlation filters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2820" to="2832" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1090" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4303" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-negative subspace representation learning scheme for correlation filter based tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
				<meeting>Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1888" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2017 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1949" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking VOT2019 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops</title>
				<meeting>IEEE Int. Conf. Comput. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">VisDrone-SOT2019: The vision meets drone single object tracking challenge results</title>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshops</title>
				<meeting>IEEE Int. Conf. Comput. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Signals and Systems: Solutions Manual</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Nawab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Downloaded on December 31,2022 at 08:12:35 UTC from IEEE Xplore. Restrictions apply</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discriminative scale space tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1575" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CREST: Convolutional residual learning for visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2555" to="2564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning multi-task correlation particle filters for visual tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
				<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning spatial-aware regressions for visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vision Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8962" to="8970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Correlation tracking via joint discrimination and reliability learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
				<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4649" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GradNet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6162" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Target-aware deep tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Xue-Feng Zhu received the B.Eng. degree in internet of things engineering from the Chongqing University of Posts and Telecommunications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
				<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Chongqing, China; Wuxi, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2019. 2017</date>
			<biblScope unit="page" from="4020" to="4029" />
		</imprint>
		<respStmt>
			<orgName>School of Artificial Intelligence and Computer Science, Jiangnan University</orgName>
		</respStmt>
	</monogr>
	<note>Fast-deepKCF without boundary effect. He is currently working toward the Ph.D. degree with the. His research interests include computer vision and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
