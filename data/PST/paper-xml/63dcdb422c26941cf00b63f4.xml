<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-01-04">4 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
							<email>&lt;jiaaochen@gatech.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
							<email>&lt;astonz@amazon.com&gt;</email>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">PARAMETER-EFFICIENT FINE-TUNING DESIGN SPACES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-04">4 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.01821v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parameter-efficient fine-tuning aims to achieve performance comparable to fine-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether certain design patterns exist for parameter-efficient fine-tuning. Thus, we present a parameter-efficient fine-tuning design paradigm and discover design patterns that are applicable to different experimental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efficient fine-tuning design spaces that parameterize tuning structures and tuning strategies. Specifically, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively refine the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uniformly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efficient fine-tuning methods. We show experimentally that these methods consistently and significantly outperform investigated parameter-efficient fine-tuning strategies across different backbone models and different tasks in natural language processing 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through fine-tuning on task-specific labeled data <ref type="bibr" target="#b0">[Devlin et al., 2019</ref><ref type="bibr">, Liu et al., 2019</ref><ref type="bibr" target="#b2">, Yang et al., 2019</ref><ref type="bibr">, Joshi et al., 2019</ref><ref type="bibr" target="#b4">, Sun et al., 2019</ref><ref type="bibr" target="#b5">, Clark et al., 2019</ref><ref type="bibr">, Lewis et al., 2020a</ref><ref type="bibr" target="#b7">, Bao et al., 2020</ref><ref type="bibr">, He et al., 2020</ref><ref type="bibr" target="#b9">, Raffel et al., 2020</ref><ref type="bibr" target="#b10">, Ziems et al., 2022]</ref>. However, fine-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa <ref type="bibr" target="#b1">[Liu et al., 2019]</ref> and 175B parameters for <ref type="bibr">GPT-3 [Brown et al., 2020]</ref>). This makes it difficult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks.</p><p>To adapt general knowledge in pretrained models to specific down-stream tasks in a more parameter-efficient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen <ref type="bibr">[Houlsby et al., 2019a</ref><ref type="bibr" target="#b13">, Pfeiffer et al., 2021</ref><ref type="bibr" target="#b14">, Li and Liang, 2021</ref><ref type="bibr" target="#b11">, Brown et al., 2020</ref><ref type="bibr">, Lester et al., 2021a</ref><ref type="bibr" target="#b16">, Schick and Sch?tze, 2021</ref><ref type="bibr" target="#b10">, Ziems et al., 2022]</ref>. Adapter tuning <ref type="bibr">[Houlsby et al., 2019a]</ref> is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the fine-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts <ref type="bibr" target="#b11">[Brown et al., 2020]</ref>, prefix tuning <ref type="bibr" target="#b14">[Li and Liang, 2021]</ref> and prompt tuning <ref type="bibr">[Lester et al., 2021b]</ref> prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when fine-tuning on downstream tasks. <ref type="bibr">BitFit [Zaken et al., 2021]</ref> updates the bias terms in pretrained models while freezing the remaining parameters. LoRA <ref type="bibr" target="#b19">[Hu et al., 2021]</ref> decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, <ref type="bibr" target="#b20">He et al. [2022]</ref> proposed a unified view of these existing strategies and A parameter-efficient fine-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be finetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Prefix, BitFit, and LoRA, to groups).</p><p>illustrated differences and connections among them. Like its antecedents, the resulting method is still equally assigned to different pretrained layers.</p><p>Despite being effective, most parameter-efficient fine-tuning strategies have been developed via manual design processes, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where <ref type="bibr" target="#b21">[Mao et al., 2022]</ref>, as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameterefficient fine-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings.</p><p>Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efficient fine-tuning design spaces that parameterize both tuning structures and strategies. More concretely, any of these design spaces is characterized by four major components as shown in Figure <ref type="figure">1</ref>: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment.</p><p>Starting from a relatively unconstrained parameter-efficient fine-tuning design space, we progressively refine the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efficient fine-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We further introduce new parameter-efficient fine-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efficient fine-tuning strategies. Although we use T5 <ref type="bibr" target="#b9">[Raffel et al., 2020]</ref> and classification tasks as the working example, we find that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa <ref type="bibr" target="#b1">[Liu et al., 2019]</ref>, BART <ref type="bibr">[Lewis et al., 2020b]</ref>, and XLNet <ref type="bibr" target="#b2">[Yang et al., 2019]</ref>) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets).</p><p>Our contributions can be summarized as follows: (i) We introduce parameter-efficient fine-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efficient fine-tuning via comprehensive experiments. (iii) Our discovered design patterns lead to parameter-efficient fine-tuning methods, consistently outperforming investigated parameter-efficient fine-tuning strategies across different backbone models and different NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is closely related to and built upon the research about the network design spaces and parameter-efficient fine-tuning. We discuss the connections and differences below.</p><p>Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances <ref type="bibr" target="#b23">[Radosavovic et al., 2019]</ref>, such as the use of deeper architectures or residuals.</p><p>Recently, there have been works <ref type="bibr" target="#b24">[Radosavovic et al., 2020</ref><ref type="bibr" target="#b25">, You et al., 2020</ref><ref type="bibr" target="#b23">, Radosavovic et al., 2019]</ref> performing at the design space level to discover new design principles for convolutional neural networks <ref type="bibr" target="#b24">[Radosavovic et al., 2020]</ref> and graph neural networks <ref type="bibr" target="#b25">[You et al., 2020]</ref>. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efficient fine-tuning, with the goal of discovering design patterns that are applicable to different experimental settings.</p><p>Parameter-Efficient Fine-Tuning for NLP As pretrained models grow in size, storing fine-tuned models becomes exceedingly expensive, and fine-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to finding parameter-efficient alternatives for adapting large-scale pretrained models with reduced memory and storage costs. <ref type="bibr">Houlsby et al. [2019b]</ref> proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains <ref type="bibr" target="#b27">[Stickland and Murray, 2019</ref><ref type="bibr" target="#b28">, Pfeiffer et al., 2020</ref><ref type="bibr" target="#b29">, Rebuffi et al., 2017</ref><ref type="bibr">, Lin et al., 2020]</ref>. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters <ref type="bibr" target="#b31">[Zhao et al., 2020</ref><ref type="bibr" target="#b32">, Guo et al., 2020</ref><ref type="bibr" target="#b33">, Mallya et al., 2018</ref><ref type="bibr" target="#b34">, Radiya-Dixit and Wang, 2020</ref><ref type="bibr" target="#b35">, Sung et al., 2021</ref><ref type="bibr" target="#b18">, Zaken et al., 2021]</ref>. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kronecker product <ref type="bibr">[Zhang et al., 2021a]</ref> and injecting trainable rank decomposition matrices into each layer <ref type="bibr" target="#b19">[Hu et al., 2021</ref><ref type="bibr" target="#b37">, Karimi Mahabadi et al., 2021]</ref>. <ref type="bibr" target="#b14">Li and Liang [2021]</ref> introduced prefix-tuning that prepends a set of prefixes to autoregressive language models or prepends prefixes for both encoders and decoders. The prefix parameters are updated while the pretrained parameters are fixed. <ref type="bibr">Lester et al. [2021a]</ref> proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts <ref type="bibr" target="#b38">[Deng et al., 2022</ref><ref type="bibr" target="#b39">, Zhong et al., 2022]</ref>. <ref type="bibr" target="#b40">Bari et al. [2022]</ref> proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, <ref type="bibr" target="#b20">He et al. [2022]</ref> and <ref type="bibr" target="#b41">Ding et al. [2022]</ref> proposed a unified view of the existing parameter-efficient fine-tuning strategies and illustrated the difference and connections among them. <ref type="bibr" target="#b21">Mao et al. [2022]</ref> also introduced a unified framework to combine different methods through mixtureof-experts.</p><p>In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efficient fine-tuning. This could provide a more comprehensive view of parameter-efficient fine-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively refine design spaces, we discover design patterns for parameter-efficient fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Components of Design Spaces</head><p>When defining design spaces of parameter-efficient fine-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efficient fine-tuning research.</p><p>Concretely, in our work, the parameter-efficient fine-tuning design spaces are formed by a representative set of choices in parameter-efficient fine-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space example in Figure <ref type="figure">1</ref>, we describe these four design components in detail below and will explore their design choices in Section 4.</p><p>Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, <ref type="bibr" target="#b42">Jawahar et al. [2019]</ref> found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efficient fine-tuning strategies.</p><p>For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the fine-tuning process.</p><p>Trainable Parameter Allocation In parameter-efficient fine-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predefined number of trainable parameters to layers.</p><p>Tunable Groups Zaken et al. <ref type="bibr">[2021]</ref> found that not all the parameters need to be tuned during fine-tuning on the downstream tasks. For instance, BitFit <ref type="bibr" target="#b18">[Zaken et al., 2021]</ref> only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efficient fine-tuning to attain better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy Assignment</head><p>In order to improve the parameter efficiency, different sets of strategies <ref type="bibr" target="#b14">[Li and Liang, 2021</ref><ref type="bibr">, Lester et al., 2021a</ref><ref type="bibr">, Houlsby et al., 2019a</ref><ref type="bibr" target="#b19">, Hu et al., 2021]</ref> have been proposed where only a small number of (extra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to specific down-stream tasks. Inspired by effectiveness of offering architectural flexibility <ref type="bibr">[Zhang et al., 2021a,b]</ref>, we hypothesize that different groups might benefit from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies A for assignment, for any group G i , assign a subset U i ? A to each layer in G i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discovering Design Patterns</head><p>Building on these four different design components of PEFT design spaces, we will start from a relatively unconstrained design space and progressively discover the design patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Space Experimental Setup</head><p>We first describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models.</p><p>Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widelyused GLUE benchmark <ref type="bibr" target="#b44">[Wang et al., 2018]</ref>. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Similarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Recognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks.</p><p>Pretrained Backbone Models and Model Settings We use T5-base/3b <ref type="bibr" target="#b9">[Raffel et al., 2020]</ref> as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face<ref type="foot" target="#foot_0">2</ref> for our implementations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following <ref type="bibr" target="#b20">He et al. [2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discovering Design Patterns Using T5-base</head><p>In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as S i ) consists of a set of models (S imodels) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space (S 0 ). Then we progressively refine design spaces (from S 0 to S 1:4 ) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space S i with a low-compute, low-epoch regime <ref type="bibr" target="#b24">[Radosavovic et al., 2020]</ref>, we randomly sample 100 models from S i , fine-tune with 3 epochs<ref type="foot" target="#foot_1">3</ref> , and compute the average of the GLUE average performances.</p><p>We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to find out the "best" design space or method. For computational efficiency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">The Initial S 0 Design Space</head><p>The initial relatively unconstrained design space S 0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Prefix, BitFit, and LoRA. One can think of this S 0 design space as a set of random models (S 0 -models) with random design patterns. Specifically, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer.</p><p>Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Prefix, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers.  These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S 0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S 0 design space that satisfy each grouping pattern constraint (Figure <ref type="figure" target="#fig_2">2</ref>); (ii) fine-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later.</p><p>The averaged performances are shown in Table <ref type="table" target="#tab_0">1</ref> <ref type="foot" target="#foot_3">5</ref> . We find that models from the design space with the spindle grouping pattern (Figure <ref type="figure" target="#fig_2">2</ref>) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern.</p><p>From now on, we will group layers in a spindle pattern. We refer to S 0 with this additional design pattern as the new S 1 design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">The S 2 Design Space with Additional Parameter Constraints</head><p>We continue to explore design patterns in trainable parameter allocation to refine the S 1 design space. Denote by n i the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (n i+1 ? n i ): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (n i+1 = n i ): the number of trainable parameters in every layer is the same; and (iii) Decreasing (n i+1 ? n i ): the number of trainable parameters in every layer gradually decreases (or remains the same).</p><p>Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. We will allocate the number of trainable parameters to layers uniformly. We refer to S 1 with this additional design pattern as the new S 2 design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">The S 3 Design Space with Additional Tunable Group Constraints</head><p>Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we find out which groups need to be fine-tuned. As shown in Table <ref type="table" target="#tab_2">3</ref>, we explore various design patterns in tunable groups to further constrain the S 2 design space. Based on the GLUE average performances, we find that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks.</p><p>We will tune all the groups. We refer to S 2 with this additional design pattern as the new S 3 design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">The S 4 Design Space with Additional Strategy Constraints</head><p>Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S 3 design space. Specifically, each design space consists of models that assign a subset of {Adapter (A), Prefix (P), BitFit (B), and LoRA (L)} to all layers of any group G i (i = 1, . . . , 4). We begin by adding different G 1 strategy assignment constraints to the S 3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G 1 . Then we progressively add G i (i &gt; 1) strategy assignment constraints together with the discovered strategy assignment patterns for all G j (j = 1, . . . , i -1) to the S 3 space. Due to space limit, we present results of this process in the Appendix (G 1 in Table <ref type="table">8</ref>, G 2 Table <ref type="table" target="#tab_6">9</ref>, G 3 in Table <ref type="table" target="#tab_0">10</ref>, and G 4 in Table <ref type="table" target="#tab_7">11</ref>), which suggests strategy assignment of G 1 -(A, L) -G 2 -(A, P) -G 3 -(A, P, B) -G 4 -(P, B, L) for the T5-base pretrained backbone model.</p><p>We will assign the discovered proper tuning strategies to groups. We refer to S 3 with this additional design pattern as the new S 4 design space, which consists of the final S 4 -model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discovering Design Patterns Using T5-3b</head><p>We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5base) still apply when we use larger models. The results are shown in Table <ref type="table" target="#tab_8">12</ref> (layer grouping), Table <ref type="table" target="#tab_9">13</ref> (trainable parameter allocation), Table <ref type="table" target="#tab_10">14</ref> (tunable groups) and Table <ref type="table" target="#tab_11">15</ref> (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table <ref type="table" target="#tab_8">12</ref>), (ii) uniformly allocating the number of trainable parameters to layers (Table <ref type="table" target="#tab_9">13</ref>), (iii) tuning all the groups 89.9</p><p>(Table <ref type="table" target="#tab_10">14</ref>), and (iv) tuning different groups with proper strategies (Table <ref type="table" target="#tab_11">15</ref>). For T5-3b, the discovered proper strategy assignment is G 1 -(P, L) -G 2 -(A, L) -G 3 -(P, B, L) -G 4 -(A, P, B). We refer to the final design space as S 4 -3b and the final model in this space as S 4 -3b-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The S 4 -model (Section 4.2.5) and S 4 -3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Datasets Besides the GLUE datasets <ref type="bibr" target="#b44">[Wang et al., 2018]</ref> (Section 4.1), we further evaluate our methods on two generation tasks used by <ref type="bibr" target="#b20">He et al. [2022]</ref>: (i) Abstractive Summarization using XSum <ref type="bibr" target="#b45">[Narayan et al., 2018]</ref>, and (ii) Machine Translation using the WMT 2016 en-ro dataset <ref type="bibr" target="#b46">[Bojar et al., 2016]</ref>. We report ROUGE scores <ref type="bibr">[Lin, 2004]</ref> on the XSum test set, and BLEU scores <ref type="bibr">[Papineni et al., 2002]</ref> on the en-ro test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Model Settings</head><p>We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it fine-tunes all the model parameters in the pretrained models; (ii) Adapter <ref type="bibr">[Houlsby et al., 2019a]</ref>: it adds adapter modules to each transformer layer; (iii) Prefix [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) <ref type="bibr">BitFit [Zaken et al., 2021]</ref>: it only updates the bias terms in pretrained models; (v) LoRA <ref type="bibr" target="#b19">[Hu et al., 2021]</ref>: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 <ref type="bibr" target="#b9">[Raffel et al., 2020]</ref>, we additionally apply our methods to other backbone models including RoBERTa-base/large <ref type="bibr" target="#b1">[Liu et al., 2019]</ref> and BART-base/large <ref type="bibr">[Lewis et al., 2020a]</ref>. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following <ref type="bibr" target="#b20">He et al. [2022]</ref>. Specifically, this value is set to 0.5% for Adapter, Prefix, LoRA, and our methods, and 0.1% for BitFit.</p><p>For all the experiments, we followed <ref type="bibr" target="#b1">Liu et al. [2019]</ref> to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e -5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness on GLUE with T5 Backbones</head><p>Table <ref type="table">6</ref>: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. With our discovered design patterns, we fine-tune T5-base (S 4 -model) and T5-3b (S 4 -3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table <ref type="table" target="#tab_3">4</ref>, where the key measure is the GLUE average performance (last column). We find that our S 4 -model and S 4 -3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full fine-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discovered PEFT design patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">General Effectiveness on GLUE with RoBERTa Backbones</head><p>We directly apply the S 4 -model and S 4 -3b-model (adopting design patterns discovered using T5base and T5-3b) to fine-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de-sign spaces: (i) S 0 -model, where all the designs are randomly selected for RoBERTa as in S 0 ; (ii) S 3 -model, where strategies are randomly assigned to different RoBERTa layer groups as in S 3 . Table <ref type="table" target="#tab_4">5</ref> shows that (i) the design patterns (adopted by S 4 -model and S 4 -3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances from S 0 -models, S 3 -models, to S 4 -(3b)-models support adding more constraints in the pattern discovery process (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">General Effectiveness on Generation Tasks with BART Backbones</head><p>Like in Section 5.3, we further directly apply the S 4 -model and S 4 -3b-model (adopting design patterns discovered using T5-base and T5-3b) to fine-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following <ref type="bibr" target="#b20">He et al. [2022]</ref>. We also compare with PA (parallel adapter) using the same number of trainable parameters <ref type="bibr" target="#b20">[He et al., 2022]</ref>. Table <ref type="table">6</ref> shows that our methods, although adopting design patterns discovered from classification tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efficient fashion. Instead of focusing on designing another strategy in the first place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing.</p><p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.</p><p>Table <ref type="table">8</ref>: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G 1 strategy assignment constraints to the S 3 design space. 2. Once figuring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes influential to examine which groups need to be learned during fine-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we figure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design.</p><p>Table <ref type="table" target="#tab_0">10</ref>: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G 3 strategy assignment constraints with G 1 -(L, A) -G 2 -(P, A) to the S 3 design space.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: A parameter-efficient fine-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be finetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Prefix, BitFit, and LoRA, to groups).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4.2.2 The S 1 Design Space with Additional Grouping ConstraintsInspired by<ref type="bibr" target="#b24">Radosavovic et al. [2020]</ref>, we also consider 4 groups (G 1 , . . . , G 4 , in the order of forward pass) in the experiments 4 . Denote by N i the number of layers in G i . As illustrated in Figure2, we compare the following layer grouping patterns: (i) Increasing (N i+1 &gt; N i ): the number of layers in groups gradually increases; (ii) Uniform (N i+1 = N i ): the number of layers in groups is the same; (iii) Decreasing (N i+1 &lt; N i ): the number of layers in groups gradually decreases; (iv) Spindle (N 1 &lt; N 2 = N 3 &gt; N 4 ): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N 1 &gt; N 2 = N 3 &lt; N 4 ): the numbers of layers in groups at both ends are bigger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G 1 , . . . , G 4 ) and numbers of layers in groups.</figDesc><graphic url="image-1.png" coords="5,118.80,340.83,374.39,50.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table2reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S 0 design space.</figDesc><table><row><cell cols="9">Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA</cell><cell>Avg</cell></row><row><cell>S0-models</cell><cell>76.9</cell><cell>70.1</cell><cell>72.5</cell><cell>73.3</cell><cell>63.6</cell><cell>71.7</cell><cell>73.8</cell><cell>24.3</cell><cell>65.7</cell></row><row><cell>Increasing</cell><cell>85.3</cell><cell>74.9</cell><cell>77.2</cell><cell>77.5</cell><cell>66.8</cell><cell>76.2</cell><cell>76.0</cell><cell>33.0</cell><cell>70.8</cell></row><row><cell>Uniform</cell><cell>84.8</cell><cell>73.7</cell><cell>78.1</cell><cell>78.6</cell><cell>68.5</cell><cell>77.8</cell><cell>79.2</cell><cell>36.1</cell><cell>72.1</cell></row><row><cell>Decreasing</cell><cell>81.9</cell><cell>72.1</cell><cell>78.3</cell><cell>76.7</cell><cell>67.3</cell><cell>75.9</cell><cell>78.6</cell><cell>28.7</cell><cell>70.0</cell></row><row><cell>Spindle</cell><cell>86.9</cell><cell>75.5</cell><cell>79.8</cell><cell>79.4</cell><cell>69.8</cell><cell>78.3</cell><cell>80.1</cell><cell>37.3</cell><cell>73.3</cell></row><row><cell>Bottleneck</cell><cell>84.5</cell><cell>74.6</cell><cell>76.9</cell><cell>78.1</cell><cell>69.2</cell><cell>76.2</cell><cell>78.6</cell><cell>32.1</cell><cell>71.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S 1 design space.</figDesc><table><row><cell cols="10">Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>Increasing</cell><cell>87.2</cell><cell>77.9</cell><cell>79.4</cell><cell>78.7</cell><cell>71.6</cell><cell>77.6</cell><cell>81.4</cell><cell>32.0</cell><cell>73.2</cell></row><row><cell>Uniform</cell><cell>87.8</cell><cell>77.4</cell><cell>80.1</cell><cell>80.5</cell><cell>73.9</cell><cell>78.1</cell><cell>80.4</cell><cell>34.3</cell><cell>74.0</cell></row><row><cell>Decreasing</cell><cell>86.4</cell><cell>75.8</cell><cell>78.4</cell><cell>77.0</cell><cell>70.4</cell><cell>77.1</cell><cell>78.7</cell><cell>35.8</cell><cell>72.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S 2 design space.</figDesc><table><row><cell cols="10">Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G1</cell><cell>82.6</cell><cell>72.1</cell><cell>77.6</cell><cell>70.6</cell><cell>65.3</cell><cell>71.9</cell><cell>77.6</cell><cell>27.6</cell><cell>68.2</cell></row><row><cell>G2</cell><cell>83.3</cell><cell>72.8</cell><cell>77.5</cell><cell>72.8</cell><cell>63.6</cell><cell>72.8</cell><cell>77.5</cell><cell>27.5</cell><cell>68.4</cell></row><row><cell>G3</cell><cell>83.6</cell><cell>73.3</cell><cell>78.2</cell><cell>73.3</cell><cell>66.4</cell><cell>71.3</cell><cell>77.9</cell><cell>22.9</cell><cell>68.4</cell></row><row><cell>G4</cell><cell>83.2</cell><cell>73.0</cell><cell>77.9</cell><cell>73.7</cell><cell>63.9</cell><cell>72.0</cell><cell>77.9</cell><cell>27.9</cell><cell>68.7</cell></row><row><cell>G1, G2</cell><cell>83.5</cell><cell>73.2</cell><cell>78.0</cell><cell>75.4</cell><cell>67.7</cell><cell>73.2</cell><cell>78.0</cell><cell>28.0</cell><cell>69.6</cell></row><row><cell>G3, G4</cell><cell>87.8</cell><cell>74.6</cell><cell>78.3</cell><cell>76.9</cell><cell>68.6</cell><cell>74.3</cell><cell>78.3</cell><cell>28.3</cell><cell>70.7</cell></row><row><cell>G1, G2, G3</cell><cell>86.0</cell><cell>75.8</cell><cell>79.0</cell><cell>77.8</cell><cell>71.8</cell><cell>78.8</cell><cell>79.0</cell><cell>33.0</cell><cell>72.6</cell></row><row><cell>G2, G3, G4</cell><cell>85.2</cell><cell>76.6</cell><cell>79.1</cell><cell>78.6</cell><cell>70.1</cell><cell>77.6</cell><cell>79.1</cell><cell>31.9</cell><cell>72.2</cell></row><row><cell>G1, G2, G3, G4</cell><cell>88.3</cell><cell>77.4</cell><cell>82.1</cell><cell>81.5</cell><cell>74.9</cell><cell>79.4</cell><cell>81.4</cell><cell>34.3</cell><cell>74.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S 4 -model and the S 4 -3b-model perform significantly better than the second-best PEFT methods in all the eight datasets at the significance level p &lt; 0.05( * ) or even p &lt; 0.01( * * ).</figDesc><table><row><cell>Method</cell><cell>SST-2</cell><cell>MNLI</cell><cell>QNLI</cell><cell>QQP</cell><cell>RTE</cell><cell>STS-B</cell><cell>MRPC</cell><cell>CoLA</cell><cell>Average</cell></row><row><cell>full</cell><cell>95.2</cell><cell>87.1</cell><cell>93.7</cell><cell>89.4</cell><cell>80.1</cell><cell>89.4</cell><cell>90.7</cell><cell>51.1</cell><cell>84.5</cell></row><row><cell>Adapter</cell><cell>94.6</cell><cell>85.5</cell><cell>89.8</cell><cell>86.7</cell><cell>75.3</cell><cell>86.7</cell><cell>89.1</cell><cell>59.2</cell><cell>83.3</cell></row><row><cell>Prefix</cell><cell>94.0</cell><cell>81.6</cell><cell>87.8</cell><cell>83.4</cell><cell>64.3</cell><cell>83.1</cell><cell>84.8</cell><cell>34.0</cell><cell>76.6</cell></row><row><cell>BitFit</cell><cell>94.4</cell><cell>84.5</cell><cell>90.6</cell><cell>88.3</cell><cell>74.3</cell><cell>86.6</cell><cell>90.1</cell><cell>57.7</cell><cell>83.3</cell></row><row><cell>LoRA</cell><cell>94.8</cell><cell>84.7</cell><cell>91.6</cell><cell>88.5</cell><cell>75.8</cell><cell>86.3</cell><cell>88.7</cell><cell>51.5</cell><cell>82.7</cell></row><row><cell>S4-model</cell><cell>95.5  *  *  1.7</cell><cell>87.6  *  *  1.0</cell><cell>92.7  *  *  1.1</cell><cell>88.8  *  *  1.0</cell><cell>80.4  *  2.3</cell><cell>87.4  *  2.0</cell><cell>91.2  *  *  2.4</cell><cell>62.2  *  3.2</cell><cell>85.7</cell></row><row><cell>full</cell><cell>97.4</cell><cell>91.4</cell><cell>96.3</cell><cell>89.7</cell><cell>91.1</cell><cell>90.6</cell><cell>92.5</cell><cell>67.1</cell><cell>89.5</cell></row><row><cell>Adapter</cell><cell>96.3</cell><cell>89.9</cell><cell>94.7</cell><cell>87.8</cell><cell>83.4</cell><cell>90</cell><cell>89.7</cell><cell>65.2</cell><cell>87.1</cell></row><row><cell>Prefix</cell><cell>96.3</cell><cell>82.8</cell><cell>88.9</cell><cell>85.5</cell><cell>78.3</cell><cell>83.5</cell><cell>85.4</cell><cell>42.7</cell><cell>80.4</cell></row><row><cell>BitFit</cell><cell>95.8</cell><cell>89.5</cell><cell>93.5</cell><cell>88.5</cell><cell>86.2</cell><cell>90.7</cell><cell>88.6</cell><cell>64.2</cell><cell>87.1</cell></row><row><cell>LoRA</cell><cell>96.2</cell><cell>90.6</cell><cell>94.9</cell><cell>89.1</cell><cell>91.2</cell><cell>91.1</cell><cell>91.1</cell><cell>67.4</cell><cell>88.9</cell></row><row><cell>S4-3b-model</cell><cell>97.2  *  *  1.8</cell><cell>91.6  *  *  1.2</cell><cell>96.6  *  *  1.0</cell><cell>89.5  *  *  1.5</cell><cell>91.5  *  2.8</cell><cell>91.5  *  2.5</cell><cell>91.9  *  2.0</cell><cell>69.7  *  3.4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S 0 -model, where all the designs are randomly selected for RoBERTa as in the S 0 design space; (ii) S 3 -model, where strategies are randomly assigned to different RoBERTa layer groups as in the S 3 design space. The S 4 -model and S 4 -3b-model perform significantly better than the second-best PEFT methods in all the eight datasets at the significance level p &lt; 0.05( * ) or even p &lt; 0.01( * * ).</figDesc><table><row><cell>Method</cell><cell>SST-2</cell><cell>MNLI</cell><cell>QNLI</cell><cell>QQP</cell><cell>RTE</cell><cell>STS-B</cell><cell>MRPC</cell><cell>CoLA</cell><cell>Average</cell></row><row><cell>full</cell><cell>94.8</cell><cell>87.6</cell><cell>92.8</cell><cell>91.9</cell><cell>80.8</cell><cell>90.3</cell><cell>90.2</cell><cell>63.6</cell><cell>86.5</cell></row><row><cell>Adapter</cell><cell>94.2</cell><cell>87.1</cell><cell>93.1</cell><cell>90.2</cell><cell>71.5</cell><cell>89.7</cell><cell>88.5</cell><cell>60.8</cell><cell>84.4</cell></row><row><cell>Prefix</cell><cell>94.0</cell><cell>86.8</cell><cell>91.3</cell><cell>90.5</cell><cell>74.5</cell><cell>90.3</cell><cell>88.2</cell><cell>61.5</cell><cell>84.6</cell></row><row><cell>BitFit</cell><cell>93.7</cell><cell>84.8</cell><cell>91.3</cell><cell>84.5</cell><cell>77.8</cell><cell>90.8</cell><cell>90.0</cell><cell>61.8</cell><cell>84.3</cell></row><row><cell>LoRA</cell><cell>94.9</cell><cell>87.5</cell><cell>93.1</cell><cell>90.8</cell><cell>83.1</cell><cell>90.0</cell><cell>89.6</cell><cell>62.6</cell><cell>86.4</cell></row><row><cell>S0-model</cell><cell>94.2</cell><cell>95.3</cell><cell>90.4</cell><cell>90.6</cell><cell>75.6</cell><cell>89.6</cell><cell>88.0</cell><cell>60.9</cell><cell>85.6</cell></row><row><cell>S3-model</cell><cell>94.3</cell><cell>87.2</cell><cell>92.8</cell><cell>91.0</cell><cell>81.8</cell><cell>90.3</cell><cell>89.2</cell><cell>63.2</cell><cell>86.2</cell></row><row><cell>S4-model</cell><cell>94.81.6</cell><cell>87.8  *  *  0.8</cell><cell>93.4  *  *  1.3</cell><cell>91.6  *  1.2</cell><cell>85.8  *  *  1.8</cell><cell>90.4  *  2.0</cell><cell>90.0  *  *  1.8</cell><cell>63.2  *  3.5</cell><cell>87.1</cell></row><row><cell>full</cell><cell>96.4</cell><cell>90.2</cell><cell>94.7</cell><cell>92.2</cell><cell>86.6</cell><cell>92.4</cell><cell>90.9</cell><cell>68.0</cell><cell>88.9</cell></row><row><cell>Adapter</cell><cell>96.6</cell><cell>90.5</cell><cell>94.8</cell><cell>91.7</cell><cell>80.1</cell><cell>92.1</cell><cell>90.9</cell><cell>67.8</cell><cell>88.1</cell></row><row><cell>Prefix</cell><cell>95.7</cell><cell>87.6</cell><cell>92.1</cell><cell>88.7</cell><cell>82.3</cell><cell>89.6</cell><cell>87.4</cell><cell>62.8</cell><cell>85.7</cell></row><row><cell>BitFit</cell><cell>96.1</cell><cell>88.0</cell><cell>93.4</cell><cell>90.2</cell><cell>86.2</cell><cell>90.9</cell><cell>92.7</cell><cell>64.2</cell><cell>87.7</cell></row><row><cell>LoRA</cell><cell>96.2</cell><cell>90.6</cell><cell>94.7</cell><cell>91.6</cell><cell>87.4</cell><cell>92.0</cell><cell>89.7</cell><cell>68.2</cell><cell>88.8</cell></row><row><cell>S0-model</cell><cell>95.5</cell><cell>86.5</cell><cell>92.3</cell><cell>89.8</cell><cell>84.6</cell><cell>89.2</cell><cell>86.3</cell><cell>61.2</cell><cell>85.6</cell></row><row><cell>S3-model</cell><cell>96.3</cell><cell>89.4</cell><cell>93.8</cell><cell>90.2</cell><cell>85.9</cell><cell>90.8</cell><cell>90.9</cell><cell>63.4</cell><cell>87.6</cell></row><row><cell>S4-3b-model</cell><cell>96.6  *  *  1.3</cell><cell>90.8  *  1.1</cell><cell>95.1  *  *  0.8</cell><cell>92.0  *  *  1.2</cell><cell>87.22.8</cell><cell>92.3  *  2.2</cell><cell>91.8  *  *  1.8</cell><cell>68.4  *  3.2</cell><cell>89.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G 2 strategy assignment constraints with G 1 -(L, A) to the S 3 design space.</figDesc><table><row><cell cols="10">Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G1-Adapter (A)</cell><cell>89.8</cell><cell>83.5</cell><cell>84.9</cell><cell>80.8</cell><cell>72.5</cell><cell>80.8</cell><cell>78.5</cell><cell>37.7</cell><cell>76.1</cell></row><row><cell>G1-Prefix (P)</cell><cell>89.3</cell><cell>83.1</cell><cell>84.4</cell><cell>80.1</cell><cell>70.1</cell><cell>80.0</cell><cell>77.6</cell><cell>33.0</cell><cell>74.7</cell></row><row><cell>G1-BitFit (B)</cell><cell>89.0</cell><cell>82.9</cell><cell>84.1</cell><cell>81.4</cell><cell>72.0</cell><cell>81.1</cell><cell>77.0</cell><cell>30.8</cell><cell>74.8</cell></row><row><cell>G1-LoRA (L)</cell><cell>89.9</cell><cell>83.6</cell><cell>85.0</cell><cell>81.1</cell><cell>71.8</cell><cell>81.0</cell><cell>78.8</cell><cell>35.3</cell><cell>75.8</cell></row><row><cell>G1-(P, L)</cell><cell>89.1</cell><cell>82.8</cell><cell>85.1</cell><cell>81.2</cell><cell>71.9</cell><cell>81.5</cell><cell>79.1</cell><cell>35.0</cell><cell>75.7</cell></row><row><cell>G1-(A, P)</cell><cell>89.8</cell><cell>82.8</cell><cell>84.8</cell><cell>81.1</cell><cell>72.2</cell><cell>81.3</cell><cell>79.2</cell><cell>36.4</cell><cell>75.9</cell></row><row><cell>G1-(A, L)</cell><cell>89.6</cell><cell>83.8</cell><cell>85.6</cell><cell>81.3</cell><cell>72.9</cell><cell>81.7</cell><cell>79.5</cell><cell>36.8</cell><cell>76.4</cell></row><row><cell>G1-(A, P, L)</cell><cell>89.6</cell><cell>83.5</cell><cell>85.2</cell><cell>81.5</cell><cell>72.2</cell><cell>81.4</cell><cell>79.2</cell><cell>35.2</cell><cell>75.9</cell></row><row><cell>G1-(P, B, L)</cell><cell>89.3</cell><cell>83.6</cell><cell>85.5</cell><cell>81.6</cell><cell>72.3</cell><cell>81.0</cell><cell>78.8</cell><cell>35.7</cell><cell>76.0</cell></row><row><cell>G1-(A, P, B)</cell><cell>89.2</cell><cell>83.3</cell><cell>84.8</cell><cell>81.8</cell><cell>72.5</cell><cell>81.1</cell><cell>78.6</cell><cell>35.6</cell><cell>75.8</cell></row><row><cell>G1-(A, B, L)</cell><cell>89.8</cell><cell>83.4</cell><cell>84.8</cell><cell>81.1</cell><cell>72.6</cell><cell>81.6</cell><cell>79.4</cell><cell>34.8</cell><cell>75.9</cell></row><row><cell>G1-(A, P, B, L)</cell><cell>90.0</cell><cell>83.1</cell><cell>85.3</cell><cell>81.6</cell><cell>72.6</cell><cell>81.4</cell><cell>79.2</cell><cell>36.5</cell><cell>76.1</cell></row><row><cell cols="10">Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G2-Adapter (A)</cell><cell>91.6</cell><cell>84.3</cell><cell>85.5</cell><cell>82.3</cell><cell>73.5</cell><cell>82.8</cell><cell>81.3</cell><cell>38.8</cell><cell>77.5</cell></row><row><cell>G2-Prefix (P)</cell><cell>89.6</cell><cell>84.0</cell><cell>86.5</cell><cell>81.5</cell><cell>73.3</cell><cell>82.5</cell><cell>80.5</cell><cell>36.2</cell><cell>76.7</cell></row><row><cell>G2-BitFit (B)</cell><cell>91.2</cell><cell>83.6</cell><cell>85.7</cell><cell>82.9</cell><cell>72.6</cell><cell>82.6</cell><cell>80.8</cell><cell>33.1</cell><cell>76.5</cell></row><row><cell>G2-LoRA (L)</cell><cell>91.4</cell><cell>84.4</cell><cell>86.1</cell><cell>82.0</cell><cell>72.8</cell><cell>81.8</cell><cell>81.6</cell><cell>39.8</cell><cell>77.4</cell></row><row><cell>G2-(P, L)</cell><cell>91.6</cell><cell>84.6</cell><cell>86.8</cell><cell>81.8</cell><cell>73.8</cell><cell>82.8</cell><cell>82.0</cell><cell>38.5</cell><cell>77.7</cell></row><row><cell>G2-(A, P)</cell><cell>92.2</cell><cell>84.2</cell><cell>87.1</cell><cell>82.2</cell><cell>74.4</cell><cell>83.0</cell><cell>82.5</cell><cell>40.8</cell><cell>78.3</cell></row><row><cell>G2-(A, L)</cell><cell>92.0</cell><cell>84.4</cell><cell>86.5</cell><cell>81.8</cell><cell>73.6</cell><cell>82.6</cell><cell>82.2</cell><cell>40.1</cell><cell>77.9</cell></row><row><cell>G2-(A, P, L)</cell><cell>91.8</cell><cell>84.8</cell><cell>86.8</cell><cell>81.8</cell><cell>74.1</cell><cell>83.0</cell><cell>82.1</cell><cell>37.9</cell><cell>77.7</cell></row><row><cell>G2-(P, B, L)</cell><cell>91.6</cell><cell>84.1</cell><cell>87.1</cell><cell>82.0</cell><cell>74.0</cell><cell>82.9</cell><cell>82.4</cell><cell>35.8</cell><cell>77.4</cell></row><row><cell>G2-(A, P, B)</cell><cell>91.8</cell><cell>84.2</cell><cell>86.8</cell><cell>82.1</cell><cell>73.7</cell><cell>83.3</cell><cell>82.2</cell><cell>41.2</cell><cell>78.1</cell></row><row><cell>G2-(A, B, L)</cell><cell>92.2</cell><cell>84.3</cell><cell>86.1</cell><cell>82.0</cell><cell>74.1</cell><cell>83.2</cell><cell>82.0</cell><cell>37.6</cell><cell>77.6</cell></row><row><cell>G2-(A, P, B, L)</cell><cell>92.0</cell><cell>84.1</cell><cell>87.0</cell><cell>81.9</cell><cell>74.2</cell><cell>83.1</cell><cell>81.3</cell><cell>42.4</cell><cell>78.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 :</head><label>11</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G 4 strategy assignment constraints with G</figDesc><table><row><cell cols="10">Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G3-Adapter (A)</cell><cell>92.5</cell><cell>85.3</cell><cell>87.5</cell><cell>83.3</cell><cell>73.9</cell><cell>84.0</cell><cell>83.8</cell><cell>44.9</cell><cell>79.4</cell></row><row><cell>G3-Prefix (P)</cell><cell>91.5</cell><cell>84.7</cell><cell>86.7</cell><cell>82.6</cell><cell>74.2</cell><cell>83.8</cell><cell>82.9</cell><cell>40.5</cell><cell>78.4</cell></row><row><cell>G3-BitFit (B)</cell><cell>91.9</cell><cell>84.3</cell><cell>87.0</cell><cell>82.0</cell><cell>73.6</cell><cell>84.1</cell><cell>83.3</cell><cell>36.1</cell><cell>77.8</cell></row><row><cell>G3-LoRA (L)</cell><cell>92.8</cell><cell>85.4</cell><cell>87.8</cell><cell>83.5</cell><cell>74.7</cell><cell>82.4</cell><cell>84.0</cell><cell>44.0</cell><cell>79.3</cell></row><row><cell>G3-(P, L)</cell><cell>93.0</cell><cell>85.2</cell><cell>88.3</cell><cell>83.8</cell><cell>75.2</cell><cell>84.4</cell><cell>84.2</cell><cell>37.9</cell><cell>79.0</cell></row><row><cell>G3-(A, P)</cell><cell>92.4</cell><cell>85.6</cell><cell>88.1</cell><cell>83.6</cell><cell>75.0</cell><cell>84.2</cell><cell>84.0</cell><cell>41.8</cell><cell>79.3</cell></row><row><cell>G3-(A, L)</cell><cell>92.0</cell><cell>85.9</cell><cell>88.2</cell><cell>83.1</cell><cell>75.3</cell><cell>84.3</cell><cell>83.9</cell><cell>42.2</cell><cell>79.4</cell></row><row><cell>G3-(A, P, L)</cell><cell>92.6</cell><cell>86.0</cell><cell>87.5</cell><cell>83.4</cell><cell>75.6</cell><cell>84.6</cell><cell>83.5</cell><cell>43.9</cell><cell>79.6</cell></row><row><cell>G3-(P, B, L)</cell><cell>92.7</cell><cell>85.8</cell><cell>87.2</cell><cell>83.7</cell><cell>75.2</cell><cell>84.5</cell><cell>83.8</cell><cell>40.8</cell><cell>79.2</cell></row><row><cell>G3-(A, P, B)</cell><cell>93.3</cell><cell>85.8</cell><cell>88.6</cell><cell>84.0</cell><cell>75.5</cell><cell>84.9</cell><cell>84.1</cell><cell>42.1</cell><cell>79.8</cell></row><row><cell>G3-(A, B, L)</cell><cell>93.7</cell><cell>86.5</cell><cell>88.0</cell><cell>83.2</cell><cell>75.8</cell><cell>84.2</cell><cell>84.2</cell><cell>39.7</cell><cell>79.4</cell></row><row><cell>G3-(A, P, B, L)</cell><cell>93.3</cell><cell>85.6</cell><cell>87.7</cell><cell>83.8</cell><cell>75.2</cell><cell>84.3</cell><cell>84.4</cell><cell>41.6</cell><cell>79.4</cell></row><row><cell cols="10">Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G4-Adapter (A)</cell><cell>93.8</cell><cell>85.8</cell><cell>88.6</cell><cell>84.8</cell><cell>76.3</cell><cell>85.8</cell><cell>86.0</cell><cell>48.5</cell><cell>81.2</cell></row><row><cell>G4-Prefix (P)</cell><cell>93.5</cell><cell>85.2</cell><cell>88.3</cell><cell>83.6</cell><cell>76.8</cell><cell>85.3</cell><cell>85.6</cell><cell>44.8</cell><cell>80.3</cell></row><row><cell>G4-BitFit (B)</cell><cell>94.1</cell><cell>85.3</cell><cell>88.9</cell><cell>84.4</cell><cell>77.1</cell><cell>85.4</cell><cell>86.2</cell><cell>46.1</cell><cell>80.9</cell></row><row><cell>G4-LoRA (L)</cell><cell>94.0</cell><cell>86.0</cell><cell>89.2</cell><cell>85.0</cell><cell>77.2</cell><cell>85.5</cell><cell>85.8</cell><cell>47.7</cell><cell>81.3</cell></row><row><cell>G4-(P, L)</cell><cell>94.3</cell><cell>86.2</cell><cell>89.3</cell><cell>85.8</cell><cell>78.0</cell><cell>86.0</cell><cell>88.2</cell><cell>47.2</cell><cell>81.8</cell></row><row><cell>G4-(A, P)</cell><cell>94.1</cell><cell>86.2</cell><cell>89.6</cell><cell>85.4</cell><cell>77.9</cell><cell>86.2</cell><cell>86.9</cell><cell>45.3</cell><cell>81.4</cell></row><row><cell>G4-(A, L)</cell><cell>94.2</cell><cell>85.9</cell><cell>89.2</cell><cell>85.5</cell><cell>77.8</cell><cell>86.2</cell><cell>88.0</cell><cell>46.8</cell><cell>81.7</cell></row><row><cell>G4-(A, P, L)</cell><cell>94.1</cell><cell>85.8</cell><cell>88.8</cell><cell>85.7</cell><cell>77.4</cell><cell>86.5</cell><cell>87.9</cell><cell>44.8</cell><cell>81.3</cell></row><row><cell>G4-(P, B, L)</cell><cell>94.6</cell><cell>86.4</cell><cell>90.4</cell><cell>86.1</cell><cell>78.2</cell><cell>86.8</cell><cell>88.5</cell><cell>47.2</cell><cell>82.3</cell></row><row><cell>G4-(A, P, B)</cell><cell>94.5</cell><cell>86.0</cell><cell>89.6</cell><cell>86.0</cell><cell>78.0</cell><cell>86.2</cell><cell>88.1</cell><cell>44.8</cell><cell>81.6</cell></row><row><cell>G4-(A, B, L)</cell><cell>94.3</cell><cell>86.4</cell><cell>89.2</cell><cell>85.6</cell><cell>78.2</cell><cell>86.4</cell><cell>88.3</cell><cell>46.6</cell><cell>81.9</cell></row><row><cell>G4-(A, P, B, L)</cell><cell>94.2</cell><cell>86.2</cell><cell>89.2</cell><cell>85.9</cell><cell>78.5</cell><cell>86.1</cell><cell>88.0</cell><cell>45.3</cell><cell>81.6</cell></row></table><note><p>1 -(A, L) -G 2 -(A, P) -G 3 -(A, P, B) to the S 3 design space.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 :</head><label>12</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S 0 design space.</figDesc><table><row><cell cols="10">Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>S0-models</cell><cell>80.3</cell><cell>72.1</cell><cell>74.7</cell><cell>72.8</cell><cell>76.9</cell><cell>75.2</cell><cell>71.0</cell><cell>32.2</cell><cell>69.4</cell></row><row><cell>Increasing</cell><cell>84.4</cell><cell>75.7</cell><cell>83.0</cell><cell>78.3</cell><cell>82.7</cell><cell>80.3</cell><cell>76.3</cell><cell>42.1</cell><cell>75.3</cell></row><row><cell>Uniform</cell><cell>86.8</cell><cell>77.1</cell><cell>82.6</cell><cell>76.2</cell><cell>83.8</cell><cell>81.6</cell><cell>77.3</cell><cell>48.9</cell><cell>76.8</cell></row><row><cell>Decreasing</cell><cell>83.2</cell><cell>74.3</cell><cell>81.8</cell><cell>77.3</cell><cell>82.8</cell><cell>79.9</cell><cell>76.5</cell><cell>40.8</cell><cell>74.5</cell></row><row><cell>Spindle</cell><cell>88.6</cell><cell>78.8</cell><cell>83.7</cell><cell>77.7</cell><cell>84.2</cell><cell>80.9</cell><cell>78.3</cell><cell>44.6</cell><cell>77.1</cell></row><row><cell>Bottleneck</cell><cell>86.3</cell><cell>77.0</cell><cell>82.2</cell><cell>75.6</cell><cell>83.3</cell><cell>80.2</cell><cell>77.1</cell><cell>41.5</cell><cell>75.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 13 :</head><label>13</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S 1 design space.</figDesc><table><row><cell cols="10">Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>Increasing</cell><cell>90.3</cell><cell>79.3</cell><cell>84.9</cell><cell>79.3</cell><cell>85.2</cell><cell>82.8</cell><cell>79.2</cell><cell>50.1</cell><cell>78.9</cell></row><row><cell>Uniform</cell><cell>90.6</cell><cell>80.8</cell><cell>84.6</cell><cell>79.7</cell><cell>85.5</cell><cell>82.4</cell><cell>78.9</cell><cell>50.8</cell><cell>79.1</cell></row><row><cell>Decreasing</cell><cell>88.6</cell><cell>78.2</cell><cell>83.5</cell><cell>78.1</cell><cell>84.4</cell><cell>81.5</cell><cell>78.1</cell><cell>49.6</cell><cell>77.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 14 :</head><label>14</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S 2 design space.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 15 :</head><label>15</label><figDesc>Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment constraints following the process in Section 4.2.5.</figDesc><table><row><cell cols="10">Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg</cell></row><row><cell>G1</cell><cell>88.3</cell><cell>78.3</cell><cell>82.2</cell><cell>77.4</cell><cell>82.1</cell><cell>80.7</cell><cell>76.1</cell><cell>49.4</cell><cell>76.8</cell></row><row><cell>G2</cell><cell>89.1</cell><cell>78.8</cell><cell>82.1</cell><cell>77.2</cell><cell>82.3</cell><cell>81.2</cell><cell>76.4</cell><cell>49.6</cell><cell>77.1</cell></row><row><cell>G3</cell><cell>89.6</cell><cell>78.5</cell><cell>82.6</cell><cell>78.1</cell><cell>83.8</cell><cell>81.9</cell><cell>77.4</cell><cell>48.7</cell><cell>77.5</cell></row><row><cell>G4</cell><cell>89.8</cell><cell>79.3</cell><cell>82.7</cell><cell>77.9</cell><cell>83.5</cell><cell>81.9</cell><cell>77.9</cell><cell>48.5</cell><cell>77.1</cell></row><row><cell>G1, G2</cell><cell>90.1</cell><cell>80.2</cell><cell>83.4</cell><cell>78.5</cell><cell>84.3</cell><cell>82.4</cell><cell>78.5</cell><cell>51.1</cell><cell>78.5</cell></row><row><cell>G3, G4</cell><cell>90.5</cell><cell>80.6</cell><cell>83.8</cell><cell>78.7</cell><cell>84.2</cell><cell>83</cell><cell>78.2</cell><cell>50.3</cell><cell>78.6</cell></row><row><cell>G1, G2, G3</cell><cell>90.6</cell><cell>80.3</cell><cell>84.9</cell><cell>79.3</cell><cell>84.7</cell><cell>82.9</cell><cell>79.3</cell><cell>50.2</cell><cell>79.0</cell></row><row><cell>G2, G3, G4</cell><cell>90.8</cell><cell>80.9</cell><cell>84.6</cell><cell>79.1</cell><cell>85.1</cell><cell>83.1</cell><cell>79.1</cell><cell>49.2</cell><cell>78.9</cell></row><row><cell>G1, G2, G3, G4</cell><cell>91.1</cell><cell>81.4</cell><cell>85.2</cell><cell>80.4</cell><cell>85.9</cell><cell>83.5</cell><cell>80.0</cell><cell>51.6</cell><cell>79.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://huggingface.co/docs/transformers/index</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table7in the Appendix).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The experimental results with 8 groups are shown in the Table16in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The training time for the step is shown in the Table18in the Appendix.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experimental Results</head><p>Table <ref type="table">7</ref>: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for five different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S 0 design space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B General Effectiveness on SuperGLUE with XLNet Backbones</head><p>We also directly use the S 4 -model and S 4 -3b-model (adopting design patterns discovered using T5-base and T5-3b) to fine-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table <ref type="table">17</ref> reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C On the Discovery Sequence</head><p>In this work, we follow the discovery sequence of "grouping patterns -trainable parameter allocation -tunable groups -strategy assignment":</p><p>1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efficient to study the layers in the unit of groups. So we start with the grouping patterns. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note>translation, and comprehension. SCL, 2020a</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VALUE: Understanding dialect disparity in NLU</title>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Ziems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.258</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.258" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3701" to="3720" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/houlsby19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AdapterFusion: Nondestructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.eacl-main.39" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
		<ptr target="https://aclanthology.org/2021.eacl-main.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.243" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformerbased masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.10199" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.09685" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UniPELT: A unified framework for parameter-efficient language model tuning</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.433</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.433" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6253" to="6264" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On network design spaces for visual recognition</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.13214" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.13678" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.08843" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bert and pals: Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName><forename type="first">Asa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5986" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adapterfusion: Nondestructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08045</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring versatile generative language model via parameterefficient transfer learning</title>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03829</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Masking as an efficient alternative to finetuning for pretrained language models</title>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12406</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07463</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How fine can fine-tuning be? learning efficient language models</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dixit</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2435" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training neural networks with fixed sparse masks</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/cb2653" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24193" to="24205" />
		</imprint>
	</monogr>
	<note>f548f8709598e8b5156738cc51-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1022" to="1035" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rlprompt: Optimizing discrete text prompts with reinforcement learning</title>
		<author>
			<persName><forename type="first">Mingkai</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ping</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianmin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.12548" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving task generalization via unified schema prompt</title>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2208.03229" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Spt: Semi-parametric prompt tuning for multitask prompted learning</title>
		<author>
			<persName><forename type="first">Bari</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10929</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.06904" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-instantiated recurrent units with dynamic soft recursion</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6503" to="6514" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BlackboxNLP@EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
		<ptr target="https://aclanthology.org/D18-1206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2301</idno>
		<ptr target="https://aclanthology.org/W16-2301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
