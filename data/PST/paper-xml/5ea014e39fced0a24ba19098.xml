<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenzhong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engi-neering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B6A0501705DEAECAB9F8567E690EC8FF</idno>
					<idno type="DOI">10.1109/TGRS.2020.2981051</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Feature Difference Convolutional Neural Network-Based Change Detection Method Min Zhang and Wenzhong Shi</head><p>Abstract-Change detection based on remote sensing (RS) images has a wide range of applications in many fields. However, many existing approaches for detecting changes in RS images with complex land covers still have room for improvement. In this article, a high-resolution RS image change detection approach based on a deep feature difference convolutional neural network (CNN) is proposed. This approach uses a CNN to learn the deep features from RS images and then uses transfer learning to compose a two-channel network with shared weight to generate a multiscale and multidepth feature difference map for change detection. The network is trained by a change magnitude guided loss function proposed in this article and needs to train with only a few pixel-level samples to generate change magnitude maps, which can help to remove some of the pseudochanges. Finally, the binary change map can be obtained by a threshold. The approach is tested on several data sets from different sensors, including WorldView-3, QuickBird, and Ziyuan-3. The experimental results show that the proposed approach achieves better performance compared with other classic approaches and has fewer missed detections and false alarms, which proves that the proposed approach has strong robustness and generalization ability.</p><p>Index Terms-Change detection, convolutional neural network (CNN), deep feature, high spatial resolution, remote sensing (RS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R EMOTE sensing (RS) imagery can provide land cover and land use information, which is an important source for obtaining change information on the Earth's surface, directly or indirectly. Change detection based on RS images has a wide range of applications in urban planning, environmental monitoring, agriculture, forestry, etc. <ref type="bibr" target="#b0">[1]</ref>. In recent decades, the emergence of high spatial resolution and high temporal resolution RS images from various sensors has created new demands on change detection methods. Practical applications usually need to quickly obtain the required change information from massive RS data.</p><p>Change detection is the process of identifying differences in RS images at different times in the same geographical location <ref type="bibr" target="#b1">[2]</ref>. Although many automatic or semiautomatic approaches have been proposed, the commonly used approaches based on high-resolution RS images can be classified into two groups. One group is based on the difference image (DI), which is generated by image differencing, image ratio, principal component analysis (PCA) <ref type="bibr" target="#b2">[3]</ref>, or change vector analysis (CVA) <ref type="bibr" target="#b3">[4]</ref>. A change map can be obtained by classifying DI into two classes, changed and unchanged, using threshold-based, clustering-based, or supervised classificationbased approaches <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. The other is based on classification and can provide a complete matrix of change directions, where the change map is obtained by comparing the classified RS images on multiple dates (i.e., postclassification comparison) or directly classifying multidate RS images (i.e., multidate classification). Both of the above approaches have their limitations. DI-based change detection results generally have many pseudochanges (e.g., shadow changes and vegetation color changes). Supervised classification is the best way to solve this problem but requires considerable prior knowledge. For example, feature extraction and selection for supervised learning is a complex process that requires experts with professional knowledge and experience to complete, which is a limitation of many traditional approaches.</p><p>Using computational intelligence technology to optimize the process of feature extraction and selection of RS images is a good solution <ref type="bibr" target="#b7">[8]</ref>. In recent years, deep learning techniques, especially convolutional neural networks (CNNs), have achieved very good results in a variety of computer vision tasks (e.g., scene classification, semantic segmentation, and object detection). The CNN automatically learns from lowlevel to high-level image features through training, which means that it does not require manual selection of features and has wide applicability to scene classification, pixel classification, ground object detection, and change detection of RS images as well <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Classification and supervised change detection approaches based on CNNs can achieve satisfactory results compared to traditional supervised approaches but require more training samples and longer training times for learning deep features of RS images. However, it is difficult to obtain a large number of training samples of RS images in practical applications, especially high-precision pixel-level samples. The pixel-level sample here means that each pixel has a corresponding ground truth, such as a sample for image semantic segmentation. The scene-level sample mentioned means that an entire image corresponds to one label, such as a sample for scene classification.</p><p>Change detection usually needs to identify change information for each pixel. The ideal situation is that training samples for CNN-based approaches should include at least two RS images acquired at different times and the corresponding pixel-to-pixel change information or include single-period RS images and the corresponding pixel-level classification ground truth. Although there are some open RS image data sets for deep learning, they still cannot meet the needs of change detection. Obtaining such a data set by manual interpretation is time-consuming and laborious, and the quality of the sample cannot be guaranteed.</p><p>In this article, we proposed a novel approach based on a CNN architecture called feature difference CNN (FDCNN) to produce change detection maps from high-resolution RS images. FDCNN requires only very few pixel-level samples to train the CNN and can effectively reduce the detection of pseudo-changes. Our main contributions are threefold. we proposed a novel loss function for FDCNN training, which uses the change magnitude of each pixel as prior knowledge. We finally prove the effectiveness of the proposed approach on several data sets. Experiments show that the approach is very practical. The implementation code including both pretrained models and data sets with corresponding reference maps are freely available. <ref type="foot" target="#foot_0">1</ref>The rest of this article is organized as follows. We discuss the related work in Section II. Our proposed approach is detailed in Section III. The experimental results are reported in Section IV. In Section V, the approach is further discussed, and several perspectives for future research are offered, and finally, Section VI concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Here, we provide a brief review of the general approaches with deep learning techniques for RS image change detection. In this article, we conclude that there are two main ideas for using CNNs for change detection.</p><p>One approach is based on postclassification, including two steps. First, a CNN is trained to classify RS images in two periods, and then, the two classification results are compared to obtain change information. Nemoto et al. <ref type="bibr" target="#b10">[11]</ref> used a classification CNN, which is the main method for learning deep features from RS images, to detect building changes from RGB aerial photographs. The final change detection accuracy of this approach is determined by the classification ability of the CNN. There are many CNN-based classification approaches for RS data from different sources <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Various CNNs have been widely used for RS classification and satisfactory accuracy can be achieved <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Advantageously, change detection based on a classification map can provide change directions except for the binary change map.</p><p>The other approach trains neural networks directly on patchlevel <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref> or pixel-level <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b43">[44]</ref> samples for change detection. For patch-level approaches, change and no change prediction becomes a similarity detection process by splitting images from two periods into many group patches. Pixellevel approaches use pixel-level samples to train the network and require a well-designed structure and many high-precision pixel-level samples. Saha et al. <ref type="bibr" target="#b44">[45]</ref> proposed an unsupervised method based on deep features to reduce dependence on changing samples but still requires a pretrained CNN, which needs a large number of pixel-level samples. Similarly, Hou et al. <ref type="bibr" target="#b45">[46]</ref> and Pomente et al. <ref type="bibr" target="#b46">[47]</ref> used a pretrained image classification CNN to extract change features. They are unsupervised change detection methods and not very suitable for RS images with very complex land cover changes.</p><p>RS images are different from real-world digital camera photos due to their characteristics of multisource, multiscale, multispectral, etc. RS image training samples with pixellevel ground truth are hard to obtain, even with manual interpretation. Inspired by the use of deep features for change detection <ref type="bibr" target="#b45">[46]</ref>, this article uses scene-level samples of RS scene classification, which are easier to obtain than pixellevel samples, for learning deep features from multiple RS scenes with different resolutions. These features learned from specific scenes (cultivated land, lakes, vegetation, etc.) are more affected. The changes in these scenes are usually more important. Based on this idea, we propose a new CNN structure and training strategies for RS image change detection. The approach is supervised but requires very few pixel-level training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we provide a detailed description of the change detection approach based on FDCNN, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Logically, the approach consists of three main steps. First, VGG16 <ref type="bibr" target="#b47">[48]</ref> is trained to learn deep features in scene-level RS image data sets. Second, based on the proposed change magnitude guided loss function, FDCNN is trained by sharing model parameters with the trained VGG16 using few pixellevel change detection training samples. Third, based on the change magnitude map generated by FDCNN, the final binary change map can be obtained by the threshold method.</p><p>For the convenience of description, we use X 1 to represent the first period RS image, X 2 to represent the second period RS image, GT to represent the ground truth or reference map, and CMM to represent the change magnitude map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Feature Learning</head><p>Deep features can be divided into two categories according to whether neighborhood relationships are considered. One type is a deep neural network represented by a deep belief network (DBN) and a stacked autoencoder (SAE) <ref type="bibr" target="#b48">[49]</ref>. Their input is based on 1-D data, which is a column vector converted from the input image or image patch that loses the spatial information. The other type uses 2-D data as input considering the spatial relationship, and the typical representative is the CNN. Compared with the former, CNN extracts features hierarchically through local connections and shared weight. From local features to global features, from low-level to highlevel features, they can be very abstract and conceptual, and closer to the principles of human vision. Thus, a CNN is more widely used in image processing.</p><p>Deep feature learning is different from shallow feature learning. For example, PCA, independent component analysis (ICA) <ref type="bibr" target="#b49">[50]</ref>, and other methods can reduce the dimensions of features but also lose the information of ground objects, making it difficult to process images in complex scenes. Sparse coding can obtain the image sparse representation and reduce the storage and computing resources, but when processing a large volume of data, computing complexity becomes higher. Therefore, with powerful feature extraction ability, CNNs have become a popular research topic in RS data processing.</p><p>In the classification and change detection task of RS images, the ideal situation is to provide pixel-level samples and directly realize an end-to-end network. However, unlike images in a computer vision task, RS images have different sensor types, resolutions, and imaging conditions, leading to fuzzy object boundaries, and "mixed" pixels; different objects have the same spectrum, which makes it difficult to obtain the pixel-level GT. Moreover, interpreting RS images is a very time-consuming and laborious process. The interpreter's professional knowledge and experience will directly affect the As a classic CNN model structure, the VGG16 is used in this article, as shown in part (a) of Fig. <ref type="figure" target="#fig_0">1</ref>. VGG16 is composed of convolutional layers, maximized pooling layers, and full connection layers (i.e., inner product layer). The convolutional layer combines the rectified linear unit (ReLU). The whole net includes 13 convolutional layers, five maxpooling layers, and three full connection layers. The "conv(i )" denotes the i th convolutional layer (e.g., conv3 denotes the third convolutional layer). The multiscale features of RS images are learned through the classification task of RS scenes. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the same ground object has different feature representations at different scales and depths. Fig. <ref type="figure" target="#fig_1">2(a2</ref>) and (a3) are two feature maps from conv2, and Fig. <ref type="figure" target="#fig_5">2(a4</ref>) is the feature map from conv4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Difference CNN</head><p>The deep features represent different object characteristics in different convolutional layers. The CNN is a black box, and it is difficult to analyze each of the learned features, although there is a certain similarity between adjacent channels, for example, the features in Fig. <ref type="figure" target="#fig_1">2</ref>(a2) and (a3). Most convolutional layers generate feature maps with unique characteristics, such as edge, texture, brightness, and higher abstract features. With the increase in network depth, the feature map from low-level to high-level becomes more meaningful. In the change detection task, if a ground object changes with time, its change information is also shown in some feature maps. Therefore, these deep features can be used to generate the feature difference map, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(c2)-(c4). This article draws on the traditional idea of the image similarity detection network <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> and designs an FDCNN for change detection, whose detailed structure is shown in Table <ref type="table" target="#tab_1">I</ref>. The network mainly consists of three parts:</p><p>1) Sub-VGG16 Net: As shown in part (b) of Fig. <ref type="figure" target="#fig_0">1</ref>, our approach uses a two-channel structure based on VGG16, which is based on the idea of transfer learning and used to solve problems with insufficient training samples, to extract multiscale and multidepth feature maps from X 1 and X 2 . VGG16 has five max-pooling layers, which means that there are five different scales of convolutional layers. Considering the size of the feature map, our approach uses only the first three scales, called sub-VGG16. To reduce the complexity and parameter size, the two sub-VGG16s share their weights. In this article, conv2 (conv2_p), conv4 (conv4_p), and conv7 (conv7_p) are selected to produce three different scales and depth feature maps. The input image size of the network is 224 × 224 pixels with three channels (i.e., red, green, and blue bands). The corresponding sizes of feature maps are 224 × 224 × 64, 112 × 112 × 128, and 56 × 56 × 256.</p><p>2) FD-Net: As shown in part (c) of Fig. <ref type="figure" target="#fig_0">1</ref>, X 1 and X 2 are input to generate deep feature maps for difference and normalization; three feature difference maps with different scales can be formulated as follows:</p><formula xml:id="formula_0">FD(i ) = F i 1 -F i 2 max F i 1 -F i 2 , i = 1, . . . , N<label>(1)</label></formula><p>where FD denotes the feature difference map, F i 1 is the feature map with X 1 as input, F i 2 is the feature map with X 2 as input, and N is the total number of feature maps.</p><p>In the FD-Net, the channels of the three different size maps are 64, 128, and 256. In addition, to obtain accurate boundary information of the change objects, X 2 is differentiated from X 1 by the band to obtain three band difference maps. Four or more bands can be used, but this is not considered in this article.</p><p>Since the feature map output of conv4 (conv4_p) and conv7 (conv7_p) are pooled, with sizes of 112 × 112 pixels and 56 × 56 pixels, respectively, an upsampling layer with magnification ratios of 2 and 4 is required to generate maps with sizes of 224 × 224. Finally, a total of 451 feature difference maps with a size of 224 × 224 pixels is obtained.</p><p>3) FF-Net: As shown in part (d) of Fig. <ref type="figure" target="#fig_0">1</ref>. Considering that the features of sub-VGG16 Net are learned through the training process of RS scene classification, its learning rate can be set to 0. FD-Net does not need any new TPs to generate feature DIs, so it does not need to realize the backpropagation of the network. Therefore, the problem of change detection is transformed into the problem of how to use these difference feature maps for change detection. Low-rank decomposition can provide a saliency map for change detection from these change features <ref type="bibr" target="#b45">[46]</ref>, which is essentially a dimensionality reduction process and is not suitable for complex scenes. There is no fixed selection and fusion of these difference feature maps, and thus, low-rank decomposition for change detection is not stable. We find that some changes in the feature maps are unnecessary to consider, such as changes in the shadow features. These changes do not mean that ground objects have actually changed. To address this problem, we use a simple CNN with few TPs to learn how to select and combine these difference feature maps, called FF-Net. The CNN's role is similar to that of a random forest and support vector machine (SVM), which implements classification using the input multiple features. FF-Net does not need to learn new deep features of RS images. The learning process of deep features is completed by VGG16. Thus, only a few pixel-level samples are required during FF-Net training compared with training a deep CNN. Through the design of this structure, we address the problem of a lack of pixel-level samples.</p><p>According to the above analysis, training the FDCNN is actually training the FF-Net. Parameter analysis of the FF-Net and training strategy are very important for obtaining a good performance change detection network. If the number of TPs is too small, which means very weak classification ability, the feature difference maps generated by FD-Net cannot form the optimal selection and fusion scheme for change detection. In addition, it is difficult to train. In contrast, if the number of TPs is too large, FF-Net easily overfits when the training sample size is small, resulting in weak generalization ability of the FDCNN. Based on these considerations, only two convolutional layers are used in FF-Net, a fusion layer (conv_f) with a 3 × 3 kernel size to combine feature difference maps, and an output layer (conv_cmm) to produce the final change magnitude map.  <ref type="bibr" target="#b39">[40]</ref>, softmax loss function, and cross-entropy loss function. Contrastive loss is used to evaluate the similarity between two images, while softmax loss is often used for multiclass classification. Crossentropy loss is used to measure the similarity between the two probability distributions, which is more suitable for change detection tasks because the purpose of change detection is to classify pixels into changes and no changes. The loss can be formulated as follows:</p><formula xml:id="formula_1">L = - 1 N N n=1 y n log ŷn + (1-y n ) log (1-ŷn ) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where N is the number of samples, y n is the true value of the sample, which is 0 or 1, indicating a change or no change, and ŷn is the prediction of the network. However, in the case that the number of training samples is not balanced, the network tends to predict the pixels to the class with the largest number of samples, which results in a small loss during training but low accuracy during prediction. For example, in the change detection task, the number of change pixels is usually smaller than that of no change pixels, which means the network trained with (2) tends to predict pixels as no change even if they actually change because when the number of samples with y n = 0 is much larger than the number of samples with y n = 1; therefore, (2) becomes the following formula:</p><formula xml:id="formula_3">L ≈ - 1 N N n=1 (1-y n ) log (1-ŷn ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>That is, the network punishes samples with y n = 1, and the network tends to predict ŷn as 0, which causes many missdetection occurrences in the change detection task. Weighting sample data <ref type="bibr" target="#b52">[53]</ref> or manually balancing the number of samples can effectively alleviate this problem. In addition, the threshold of change is fuzzy and uncertain, and it is difficult to define the bounds of change and no change even by manual interpretation. To address this problem, we use the change magnitude of each pixel as prior knowledge for learning and the weight loss function according to the number of samples in this article. The improved cross-entropy loss is formulated as follows:</p><formula xml:id="formula_5">L = - 1 N N n=1 W n β -y n log ŷn + β + (1-y n ) log (1-ŷn ) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where W is the weight matrix generated from the change magnitude information of each pixel, ranging from 0 to 1, β -denotes the ratio of the number of negative samples (i.e., no change pixels) to the total, β + denotes the ratio of the number of positive samples (i.e., change pixels) to the total, and</p><formula xml:id="formula_7">β -+ β + = 1.</formula><p>There are many approaches to generating change magnitude maps, such as CVA, multivariate alteration detection (MAD), iteration reweighted multivariate alteration detection (IR-MAD), and iterated PCA (ITPCA) <ref type="bibr" target="#b53">[54]</ref>. Because of its simplicity and high efficiency, CVA is used in this article to generate change magnitude maps as follows:</p><formula xml:id="formula_8">CMM = CVA(X 1 , X 2 ) = c j =1 (DN 1 j -DN 2 j ) 2<label>(5)</label></formula><p>where DN 1 j and DN 2 j denote the pixel values of the j th band of X 1 and X 2 , respectively, and c is the total number of bands. Thus, W can be obtained by the following formula:</p><formula xml:id="formula_9">W = mean(CMM), if CMM &lt; mean CMM CMM, otherwise<label>(6)</label></formula><p>where CMM = CMM/ max(CMM) represents the normalized CMM, mean is the mean value of all pixels, and max is the maximum value of all pixels. The mean value of CMM is used as the cutoff point because we consider that most areas of RS images have no change or have a low magnitude of change, resulting in a low learning rate. For the unchanged pixels, there is no learning rate. Training speed will drop if very low weights are used.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>  but not affected, such as the color changes in building and water. If C is detected as change, a high change magnitude weight will lead to great loss and allow the network to learn to predict these pixels as no change, which can help to reduce false alarm (FA) errors. D represents the pseudochange pixel with a change magnitude of 0.5, which belongs to the fuzzy area of change and no change but is not affected, such as shadow changes in vegetation. A and D have suitable weights because the network cannot distinguish them correctly with a low learning rate and they are difficult to train and converge with a high learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Binarization</head><p>After obtaining the CMM generated by conv_cmm of the trained FDCNN, a threshold method presented in <ref type="bibr" target="#b45">[46]</ref> is used to obtain the final binary change map, formulated as follows:</p><formula xml:id="formula_10">CM i, j = 1, if CMM i, j &gt; T 0, otherwise<label>(7)</label></formula><p>where T = α•mean(CMM) denotes the binarization threshold, CM i, j denotes the change value of the pixel (i, j ), and α is an adjustable parameter that often needs to be tuned for different data sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set Description</head><p>Details of the experimental data sets used in this article are shown in Table <ref type="table" target="#tab_1">II</ref>. The seven data sets include two training data sets and five test data sets, two of which are open data sets.</p><p>The aerial image data (AID) set <ref type="bibr" target="#b54">[55]</ref> is the RS image scene classification data set for VGG16 training, which has 10 000 RS images (R, G, and B), including 30 different scene types, each containing more than 220 images with a size of 600 × 600 pixels and a spatial resolution of 8-0.5 m. These images were collected in different countries (China, USA, U.K., France, etc.), at different times and in different imaging conditions. Thus, image diversity can be guaranteed, and the deep features learned by the VGG16 network are more universal. In addition, there are four change detection experimental data sets. Each of them consists of a reference map and two periods of an image. WorldView-3 (WV3) is a commercial satellite sensor that can provide very highresolution RS imagery. Our experiment used only three of its eight multispectral bands (R, G, and B). WV3 Site 1 and WV3 Site 2 are two different regions in Shenzhen, China, with a size of 1431 × 1431 pixels and a spatial resolution of 2 m. The first period acquired was in 2010, and the second period was in 2015. The Zi-Yuan 3 (ZY3) data set is from the region of Wuhan, Hubei, China, with a size of 458 × 559 pixels, three bands (R, G, and B), and a spatial resolution of 5.8 m. The QuickBird (QB) data set is also from Wuhan, with a size of 1154 × 740 pixels, three bands (R, G, and B), and a spatial resolution of 2.4 m. The acquisition times of the two periods were 2009 and 2014. The acquisition times of the two periods were 2014 and 2016. The Onera Satellite Change Detection data set (OSCD) <ref type="bibr" target="#b55">[56]</ref> provides multispectral RS images taken from the Sentinel-2 satellites between 2015 and 2018 with pixel-level change ground truth. A total of ten test pairs images with a spatial resolution of 10 m are used as a whole for testing. Their ground truth remains undisclosed, and our results need to be uploaded to the IEEE GRSS DASE website (http://dase.grss-ieee.org/) for evaluation. SZTAKI AirChange Benchmark set <ref type="bibr" target="#b56">[57]</ref> provides optical aerial images, taken with several years of time differences, with a spatial resolution of 1.5 m. To compare with other methods that have been tested on this data set, one of the most common image pairs is selected from the subdataset of SZTAKI dataset (i.e., SZADA/1) in our experiments.</p><p>To analyze whether the FDCNN is overfitting, only the WV3 Site 1 data set is used in the training stage, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. WV3 Site 2, ZY3, QB, OSCD, and SZADA/1 data sets are used to test its generalization ability and access the effectiveness of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>Our CNN models are implemented on a Caffe deep learning framework <ref type="bibr" target="#b57">[58]</ref> and trained using a single GPU NVIDIA Tesla K40c. In the VGG16 training phase, the AID is divided into a training data set and test data set, including 9500 images and 500 images, respectively. The inputs of the VGG16 are images of 224 × 224 pixels with data augmentation, including crop, flip, and color transformation. A softmax loss function is used to classify the image into 30 types. The stochastic gradient descent (SGD) with momentum is applied for training. The learning rate is fixed and set to 1e-7, and the momentum and the weight decay is set to 0.99 and 0.0005, respectively. The training was terminated after approximately 200k SGD iterations and took approximately 20 h. On the test data set, the top-1 accuracy of the scene classification is up to 90%, and the top-3 accuracy is up to 99%. The learning rate is fixed and set to 1e-7, and the momentum and the weight decay are set to 0.99 and 0.0005, respectively. The training was terminated after approximately 10k SGD iterations and took approximately 2 h. In the FDCNN test phase, to eliminate the splicing lines caused by image blocking, the test image is cropped into 224 × 224 pixel-sized blocks with 12 pixels of overlap as the inputs of the FDCNN. Each testing on the WV3 Site 1 data set took less than 20 s.</p><p>The values of the parameter α on different data sets used in the experiments are given as follows: α = 2.3 for WV3 Site 1 and 2, α = 2.0 for ZY3, α = 2.4 for QB, and α = 2.66 for SZADA/1. To test the ten pairs of images in OSCD as a whole, T in ( <ref type="formula" target="#formula_10">7</ref>) is set to 0.98 for OSCD directly, instead of setting the value of α for each pair of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Approaches</head><p>To ensure the fairness of the comparison experiment, there is no postprocessing such as noise filtering or morphological operations on the result. The comparison methods selected in this article are as follows.</p><p>1) RF500: Random forest-based approach, in which 500 trees are used. Only the WV3 Site 1 data set is used to train the RF model. 2) PCA: PCA-based approach <ref type="bibr" target="#b2">[3]</ref>, in which the contribution rate was set to 0.75, and the k-means algorithm is used for binary classification. 3) IR-MAD: IR-MAD-based approach <ref type="bibr" target="#b58">[59]</ref>, in which the maximum number of iterations is set to 30 and the k-means algorithm is used for binary classification. 4) Markov Random Field (MRF): MRF-based approach <ref type="bibr" target="#b6">[7]</ref>,</p><p>in which the maximum number of iterations is set to 30. 5) Fuzzy c-Means (FCM): FCM-based approach <ref type="bibr" target="#b5">[6]</ref>,</p><p>in which the exponent for the membership function matrix is set to 2, the maximum number of iterations is set to 500, and the minimum amount of improvement is set to 1e-5. 6) Inception v3: The Inception V3 CNN-based approach <ref type="bibr" target="#b46">[47]</ref>, in which the feature maps generated by the third and fourth convolution layer of the CNN, pretrained on ImageNet, are used for feature extraction, and the k-means algorithm is used for binary classification. 7) Siamese_KNN: The Siamese CNN-based approach <ref type="bibr" target="#b39">[40]</ref>,</p><p>in which the CNN is trained on the WV3 Site 1 data set and the k-nearest neighbor (KNN) algorithm is implemented to refine the binary map. 8) VGG16_LR: The approach based on VGG16 features and low-rank decomposition <ref type="bibr" target="#b45">[46]</ref>, in which CNN is trained on the AID data set. 9) VGG16_KM: Similar to <ref type="bibr" target="#b46">[47]</ref>, the conv2, conv4, and conv7 of the VGG16, instead of the Inception V3 CNN are used for feature extraction. The CNN is pretrained on the AID data set. 10) FDCNN_KM: The k-means algorithm is used as a binarization method to obtain the change map based on the proposed FDCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results Evaluation</head><p>Four quantitative evaluation indices, missed detection (MD), FA, overall error (OE), and Kappa coefficients (Kappa) are used for experimental comparison to evaluate the proposed approach quantitatively. In this process, N c represents the number of changed pixels in the GT; N u represents the number of unchanged pixels in the GT; N m is the number of changed pixels in the GT that were not detected, that is, the number of MD pixels; N f is the number of actually unchanged pixels in the GT but detected as changed, that is, the number of FA pixels. Therefore, the equations of MD, FA, and OE can be formulated as MD = N m /N c × ∼ 100%, FA = N f /N u × ∼ 100%, and OE = (N m + N f )/(N c + N u )× ∼ 100%, respectively. Kappa is a statistical measure of the consistency between the change map and the reference map.</p><p>As shown in Table <ref type="table" target="#tab_3">III</ref>, compared with other methods, the proposed approach achieved better performance in four different experimental data sets.</p><p>1) Experiment on Data Sets WV3 Site 1 and WV3 Site 2: WV3 Site 1 is used as the training data set of the FDCNN, which includes common land cover types. It can be seen from Fig. <ref type="figure" target="#fig_2">4</ref>(a) and (b) that this region is mainly an urban area with many buildings, and the change types in urban areas are mainly related to the changes in buildings, vegetation, and  bare land. For comparison, the change detection results of all approaches are shown in Fig. <ref type="figure" target="#fig_6">5</ref>. In Table <ref type="table" target="#tab_3">III</ref>, compared with other approaches, the proposed FDCNN achieves the best results with the lowest OE and highest Kappa. For better proof, we test them on a homologous WV3 Site 2 data set and obtain similar results with the same binarization threshold (α = 2.3), as shown in Fig. <ref type="figure" target="#fig_7">6</ref>. Compared with Inception_v3 and VGG16_LR approaches, which are unsupervised change detection approaches based on deep features of a CNN, the FDCNN is supervised and has proven to perform better on RS images with complex land covers. This is because supervised approaches can usually obtain more prior knowledge to improve accuracy. RF500 and Siamese_KNN approaches, which are supervised and trained by WV3 Site 1, have not fully learned the features of the ground object due to fewer trainable model parameters and training samples (unlike Siamese_KNN, FDCNN learns deep features from the AID data set), so their performance is slightly inferior. To further verify the rationality of FF-Net and binarization methods of the FDCNN, VGG16_KM, and FDCNN_KM approaches are implemented for comparison. The former uses the same deep features of VGG16 as employed in FDCNN and the same unsupervised change detection method as implemented in Inception_v3 to generate the result. The latter uses the k-means algorithm instead of the threshold method of FDCNN to generate the change map. The results show that the trainable FF-Net of FDCNN can effectively reduce MDs. Compared to VGG16_KM, our FDCNN_KM decreases MD by more than 30% on WV3 data sets. Considering the difference in the CMM of different RS data, the k-means algorithm usually cannot obtain the best binary change map, as the experimental results indicate. Another comparison employs the IR-MAD approach, which achieves the lowest FA error on the two data sets, 5.13% and 7.68%, but its MD error is very high, more than 50%. The results of other several traditional unsupervised approaches based on PCA, MRF, and FCM are similar and have many small and broken areas, which usually require postprocessing, such as morphological operations, to obtain the desired results.</p><p>In the two experimental areas where the building is the main land cover type, there are two main reasons for the FA error in the results of the other approaches.</p><p>1) The imaging conditions or surface color changes lead to spectral value changes in the roof, road, and other cement floors. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>, corresponding to the dense building area of the yellow rectangular area in Fig. <ref type="figure" target="#fig_7">6</ref>(a) due to the difference in imaging conditions and the mixed pixel problem caused by low spatial resolution, many buildings have inconsistent spectral values on two-period RS images, which means that these pixels may be detected as changes, but there is actually no change. In addition, even small geometric registration errors can cause such problems. 2) Changes in the shadows of buildings or vegetation.</p><p>As shown in the red elliptical area of Fig. <ref type="figure" target="#fig_9">8</ref>, the solar angles of X 1 and X 2 are different, resulting in differences in the shadow range of the building. This situation is serious around high-rise buildings. These two types of changes mentioned above are very common, and they are generally considered to be pseudochanges. It is very difficult to remove these pseudochanges without any prior knowledge. The experimental results show that there are many such FAs in other methods, and our approach barely detects these changes because in the training set WV3 Site 1, the shadow and building color change are not marked as change pixels in the GT, which proves the rationality of the FF-Net and its capability of learning effective knowledge of change detection.</p><p>2) Experiment on Data Sets ZY3 and QB: To prove the generalization ability and robustness of the proposed approach, two other data sets obtained from different sensors are also used for experiments. The ZY3 and QB data sets have different land cover types, spatial resolutions. and acquisition times, as shown in Figs. 9 and 10, respectively. In the QB data set, farmland, woodland, and grassland are the main land cover types, and the main change types are changes from vegetation to other types (e.g., road, building, and bare land). Change types in the ZY3 data set mainly involve construction land.</p><p>Table <ref type="table" target="#tab_3">III</ref> shows that the RF500 and Siamese_KNN approaches have poor detection results on the two data sets, especially in the QB data set, where Kappa is 0.15 and 0.30, respectively. ZY3 and QB are different sensor data from the training data set WV3, which means that the two approaches have weak generalization ability. The accuracy of other unsupervised approaches on the four data sets is unstable. Compared with these, the OE of our approach is always lower than 14%, and Kappa is always higher than 0.50, which proves that the proposed approach has strong generalization ability and robustness.</p><p>3) Experiment on Open Data Sets: To ensure fairness and accuracy of the results, two extra open data sets are used to evaluate the proposed approach. The comparison methods selected in this article have test results on the data set. On OSCD, two CNN-based methods [Siam. and early fusion (EF)] <ref type="bibr" target="#b55">[56]</ref> are used for comparison. On SZADA/1, CXM <ref type="bibr" target="#b56">[57]</ref>, SCNN <ref type="bibr" target="#b59">[60]</ref>, and Siamese_KNN <ref type="bibr" target="#b39">[40]</ref> methods are selected. The  Through the above analysis, the FDCNN can obtain satisfactory change detection results and has strong generalization ability. It can be applied to RS images from multiple sensors and with different spatial resolutions. Moreover, it is possible to reduce some of the pseudochanges (e.g., shadows), which  <ref type="formula" target="#formula_9">6</ref>) is the change magnitude weight matrix, generated by the CVA-based method, which is used to guide training. To analyze its effectiveness, a representative pseudochange region caused by water color changes is selected and visualized, as shown in Fig. <ref type="figure" target="#fig_14">12</ref>. For comparison, W in ( <ref type="formula" target="#formula_5">4</ref>) is set to 1 when training the FDCNN. It is found that this will cause more pseudochanges (e.g., water color changes) with great spectral changes to be detected as changes, causing more FAs, as can be seen from Fig. <ref type="figure" target="#fig_14">12</ref>(e)-(h). The proposed loss function uses the change magnitude information of spectral changes [see Fig. <ref type="figure" target="#fig_14">12(d)</ref>] to increase the penalty weight of these pseudochanges and can alleviate these FAs. In our experiments, FA is decreased by 0.51% and OE is decreased by 0.11% on WV3 Site 2 compared with the FDCNN using W = 1.</p><p>2) Change Threshold Sensitivity: As shown in Fig. <ref type="figure" target="#fig_15">13</ref>(a), the change magnitude distribution of the DI is a Gaussian mixture model, which is usually the hypothetical premise of DI-based unsupervised change detection approaches <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>     cannot be clearly classed into two types because the magnitude of no changes is mainly clustered around approximately 0.05 but cannot find the cluster center of the changes. Thus, the change threshold needs to be selected within a small range and its change will cause FA or MD errors to increase rapidly, which can be seen from Fig. <ref type="figure" target="#fig_14">12(d)</ref> and (e). The Euclidean distance map, equivalent to CMM, generated by the Siamese_KNN approach with the weighted contrastive loss encounters the same problem, as shown in Fig. <ref type="figure" target="#fig_14">12(c</ref>). It describes the similarity of pixel pairs. The distance threshold used to produce the change map is usually set to be the same as the margin value of the contrastive loss function, which is 1.0. The CMM generated by the FDCNN with Euclidean loss is mainly concentrated at approximately 0.55, and the change detection result is very sensitive to the change threshold [see Fig. <ref type="figure" target="#fig_15">13(f)</ref>] because the Euclidean loss function is generally used for fitting continuous values, which is not suitable for this binary classification. The CMM generated by the FDCNN with cross-entropy loss is mainly concentrated at approximately 0.02, and its change magnitude is generally low because of the imbalance of the number of samples, in which the no change pixels account for the main part, as analyzed in Section III-B. It leads to a rapid increase in the MDs as the threshold increases, although the OE changes rarely, as can be seen from Fig. <ref type="figure" target="#fig_14">12(g</ref>). However, our approach, the red solid line in Fig. <ref type="figure" target="#fig_14">12(b)</ref>, has high separability. C 1 is close to 0.02, and C 2 is close to 1. It can be seen from Fig. <ref type="figure" target="#fig_14">12</ref>(h) that our approach can smoothly balance the ratio of FA and MD by adjusting the threshold, which is important in practical applications. The free parameter α in ( <ref type="formula" target="#formula_5">4</ref>) is used to control the balance and needs to be adjusted manually according to actual requirements.</p><p>In summary, the training loss function proposed in this article can generate a change magnitude map that is easy to classify, and the result is less sensitive to the change threshold, which is very suitable for the change detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TP Analysis</head><p>In the FDCNN training process, because the learning rate of sub-VGG16 is 0, FD-Net has no TPs, so the actual TPs are only the parameters of the FF-Net. Therefore, the FF-Net training determines the final detection accuracy. Due to the small number of training samples, only one training set, WV3 Site 1, is available in this article, and parametric analysis is required. K is used to indicate the output channel of conv_f, and conv_cmm is a single channel output. Therefore, the number of TPs can be calculated using the following formula: </p><p>In a reasonable model, the number of training samples should be much larger than the TPs <ref type="bibr" target="#b62">[63]</ref>. Therefore, the ratio should be far less than 1, that is, K 453. In our experiments, when K = 5, the training process of the FDCNN is very unstable, and the accuracy of multiple runs varies widely. When K = 50, the accuracy is close to K = 30, but there are more FAs in the WV3 Site 1 data set, so K = 30 is used in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Structure</head><p>The learning process of deep features is automatic and random, so it is difficult to understand the actual meaning and role of all feature maps. In addition to scene classification, which can learn deep features of RS images, other image processing tasks, such as semantic segmentation and object detection, can perform this as well. Moreover, different network structures (e.g., ResNet <ref type="bibr" target="#b63">[64]</ref>, UNet <ref type="bibr" target="#b64">[65]</ref>, RefineNet <ref type="bibr" target="#b65">[66]</ref>, and ZFNet <ref type="bibr" target="#b66">[67]</ref>) can also lead to different learning outcomes. Feature extraction capabilities are stronger than VGG16 in some image processing tasks, which does not mean that they are suitable for change detection, so more experiments and data are required to prove the rationality of the network structure. The quantitative evaluation of these approaches is very complicated. Therefore, we generally choose a suitable approach based on the complexity of the problem and the experimental data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitations of the Proposed Methods</head><p>The approach proposed in this article is based on transfer learning, so it is necessary to ensure that sub-VGG16 learns effective deep features from RS images, which requires a sufficient number and variety of training data; otherwise, the generalization ability of the network will be weak. Second, the approach requires pixel-level samples to train the FF-Net. These samples usually require manual interpretation and the diversity of their change types needs to be ensured. The prior knowledge is very important, and although the number of pixel-level samples required is small, they are required to be of high precision. Finally, the FF-Net has very few TPs, so its training process is unstable and difficult, which can be remedied by implementing some deep learning knowledge. Otherwise, the FF-Net cannot be trained well, which leads to the poor performance of the FDCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Future Work</head><p>As discussed earlier, we intend to replace VGG16 with other better-performing networks. In the meantime, to obtain a more robust FDCNN, we intend to expand the RS scene classification data set by adding new RS images obtained from some new sensors that will enable the CNN to learn more deep features. With more pixel-level training samples, we will apply the approach to the change detection between RS images of different sensors. To obtain better results, we intend to increase postprocessing, such as utilizing morphological operations and level set evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this article, we proposed a new change detection approach based on the FDCNN, in which the sub-VGG16 is used to learn deep features from RS images, the FD-Net is used to generate feature difference maps and the FF-Net is used to fuse these maps by training with few pixel-level samples. Moreover, to reduce training time and improve network performance, we designed a change magnitude guided loss function for training based on the cross-entropy loss function, which makes it possible to use prior knowledge to reduce pseudo-changes and makes it easier to obtain the final binary change map.</p><p>Experiments on several data sets from different sensors have shown that the proposed approach achieved better performance compared with existing approaches. Most importantly, the proposed approach has strong robustness and generalization ability, which makes it advantageous in practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Deep feature maps in two periods and feature difference maps. (a1) X 1 . (a2) and (a3) Feature maps of X 1 from conv2 in two different channels. (a4) From conv4. (b1) X 2 . (b2) and (b3) Feature maps of X 2 from conv2_p in two different channels. (b4) From conv4. (c1) GT. (c2) Feature difference maps generated by (a2) and (b2). (c3) Generated by (a3) and (b3). (c4) Generated by (a4) and (b4).</figDesc><graphic coords="3,373.31,192.89,58.10,57.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>Change Magnitude Guided Loss Function: Training is a key step in obtaining a good performance network. Different tasks and the complexity of the problem have different training strategies, which require a suitable loss function. Training with a suitable loss function can improve the performance of the network and reduce the training time. The most commonly used loss function for change detection or two-class problems includes the contrastive loss function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Magnitude of changes. (a) X 1 . (b) X 2 . (c) GT of (a) and (b). 0 and 1 represent the change and no change pixel, respectively. (d) Change magnitude map produced by (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we use 0 for no change and 1 for a certain change. The pixels A, B, C, and D represent four different pixel changes. A represents the real change pixel with a 0.5 change magnitude, which belongs to the fuzzy area of change and no change, for example, slight changes in vegetation coverage. B represents the actual change pixel with a 1.0 change magnitude, which is a certain change pixel. If B is detected as no change, a high change magnitude weight leads to considerable loss, such as the change from water to buildings. C represents the pseudochange pixel with a change magnitude of 1.0, which is a certain change pixel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Training samples for FF-Net on the WV3 Site 1 data set. (a) X 1 obtained in 2010. (b) X 2 obtained in 2015. (c) GT of (a) and (b). (d) CMM generated by the CVA method.</figDesc><graphic coords="8,391.55,236.21,85.10,85.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Change maps on the WV3 Site 1 training data set produced by (a) RF500, (b) PCA, (c) IR-MAD, (d) MRF, (e) FCM, (f) Inception v3_KM, (g) Siamese_KNN, (h) VGG16_LR, (i) VGG16_KM, and (j) FDCNN thresholding with α = 2.3. (k) CMM from the FDCNN.</figDesc><graphic coords="8,134.63,330.89,85.10,85.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Change detection results on the WV3 Site 2 test data set. (a) X 1 obtained in 2010. (b) X 2 obtained in 2015. (c) GT of (a) and (b). Change maps produced by (d) RF500, (e) PCA, (f) IR-MAD, (g) MRF, (h) FCM, (i) Inception v3_KM, (j) Siamese_KNN, (k) VGG16_LR, (l) VGG16_KM, and (m) FDCNN thresholding with α = 2.3. (n) CMM from the FDCNN.</figDesc><graphic coords="9,49.31,281.09,102.14,102.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Change detection results in high-density residential area. (a) X 1 image, (b) X 2 image, (c) reference image, (d) results of RF500, (e) PCA, (f) IR-MAD, (g) MRF, (h) FCM, (i) Inception v3_KM, (j) Siamese_KNN, (k) VGG16_LR, and (l) FDCNN.</figDesc><graphic coords="10,50.51,229.01,60.62,76.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Change detection results caused by shadows. (a) X 1 image. (b) X 2 image. (c) GT of (a) and (b). Change maps produced by (d) RF500, (e) PCA, (f) IR-MAD, (g) MRF, (h) FCM, (i) Inception v3_KM, (j) Siamese_KNN, (k) VGG16_LR, and (l) FDCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>. The CMM can be classified into two types: change and no change according to the change threshold. Because the boundary between a change and no change is fuzzy and uncertain, the change threshold often directly affects the accuracy of the change detection. For the uncertain change range shown by the red region of Fig. 13(a), this part leads to the main FA and MD errors. Moreover, the decrease in MD often leads to an increase in FA and vice versa. Many unsupervised change detection approaches aim to find an equilibrium point as the change threshold. Reducing the sensitivity of the change threshold to the detection result is a good solution, which means that the small fluctuation of the change threshold does not cause a considerable change in the detection result and alleviates the difficulty of selecting the change threshold. This means that the area of the red region in Fig. 13(a) should be minimized, whereas the distance between C 1 (the mean value of the magnitude of no changes) and C 2 (the mean value of the magnitude of changes) should be as far as possible, indicating that the separability of the two classes is better. This article compares five CMMs obtained by different approaches and a distance map obtained by the Siamese_KNN. See Fig. 13(b) and (c) for the comparison. These experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Change detection results on the ZY3 data set. (a) X 1 obtained in 2014. (b) X 2 obtained in 2016. (c) GT of (a) and (b). Change maps produced by (d) RF500, (e) PCA, (f) IR-MAD, (g) MRF, (h) FCM, (i) Inception v3_KM, (j) Siamese_KNN, (k) VGG16_LR, (l) VGG16_KM, and (m) FDCNN thresholding with α = 2.4. (n) CMM from the FDCNN.</figDesc><graphic coords="11,178.31,283.37,85.77,61.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Change detection results on the QB data set. (a) X 1 obtained in 2009. (b) X 2 obtained in 2014. (c) GT of (a) and (b). Change maps produced by (d) RF500, (e) PCA, (f) IR-MAD, (g) MRF, (h) FCM, (i) Inception v3_KM, (j) Siamese_KNN, (k) VGG16_LR, (l) VGG16_KM, and (m) FDCNN thresholding with α = 2.0. (n) CMM from the FDCNN.</figDesc><graphic coords="11,93.47,427.25,85.67,61.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Proposed approach results on open data sets. Row (a) is the Montpellier test case of the OSCD and row (b) is the test case of SZADA/1. are all performed on the WV3 Site 2 data set. The CNN-based approaches stop training when the loss no longer decreases. The CMMs generated by CVA and IR-MAD approaches</figDesc><graphic coords="11,60.59,620.21,57.62,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Pseudochange region caused by water color changes. (a) X 1 image. (b) X 2 image. (c) Reference image. (d) Change magnitude weight matrix, that is, W in (6). CMM output by FDCNN (e) with W = 1 and (f) with W generated by our method. Change map (g) produced by (e) and change map (h) produced by (f).</figDesc><graphic coords="11,240.11,620.21,57.74,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (a) Schematic of the change threshold in the CMM with a Gaussian mixture distribution. (b) Histogram curves of different CMMs. (c) Histogram curves of Euclidean distance map generated by the Siamese_KNN approach with the weighted contrastive loss; relationship between detection accuracy and change threshold in CMM generated by (d) CVA, (e) IR-MAD, (f) FDCNN with the Euclidean loss, (g) FDCNN with the cross-entropy loss, and (h) our approach.</figDesc><graphic coords="12,50.03,185.57,126.02,115.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>TPs(FDCNN) = TPs(conv_f) + TPs(conv_cmm) = (w + b) × 451 × K + (w + b) × K = 4520K(8) where w = 3 × 3 and b = 1 denote the number of TPs of the weight filter and bias in the convolutional layer, respectively. Because the size of the training data set is 1431 × 1431 pixels, the ratio between the number of TPs and the number of samples can be calculated as follows: Ratio = TPs(FDCNN) The Number of Training Samples = 4520K 1431 × 1431 ≈ K 453 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I NETWORK</head><label>I</label><figDesc>STRUCTURE OF THE PROPOSED FDCNN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF CHANGE DETECTION RESULTS WITH BASELINE APPROACHES In the FDCNN training phase, each group training sample includes two-period RS images and a reference image, with the size of 224 × 224 pixels, which are randomly selected from the 1431 × 1431 pixels training set. The proposed change magnitude guided loss function is used to optimize the FF-Net.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>EXPERIMENTS ON OTHER OPEN DATA SETS means achieving a lower FA error and also meeting the needs of some practical applications.</figDesc><table><row><cell>V. DISCUSSION</cell></row><row><cell>A. Training Strategies Analysis</cell></row><row><cell>1) Change Magnitude Weight Matrix: W in (</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/MinZHANG-WHU/FDCNN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Authorized licensed use limited to: BOURNEMOUTH UNIVERSITY. Downloaded on June 24,2021 at 20:29:28 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the handling editor and anonymous reviewers whose valuable and constructive comments greatly improved this article.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Ministry of Science and Technology of the People's Republic of China under Project 2017YFB0503604.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object-based change detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M T</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="4434" to="4457" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Review article digital change detection techniques using remotely-sensed data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="1989-06">Jun. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in satellite images using principal component analysis and k-means clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="772" to="776" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Change vector analysis: A technique for the multispectral monitoring of land cover and condition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Kasischke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised change-detection methods for remote-sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3288" to="3297" />
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzy clustering algorithms for unsupervised change detection in remote sensing images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="699" to="715" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic analysis of the difference image for unsupervised change detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computational intelligence in optical remote sensing image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data a technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building change detection via a combination of CNNs using only RGB aerial imageries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">10431</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-resolution SAR image classification via deep convolutional autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2351" to="2355" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<ptr target="http://arxiv.org/abs/1606.02585" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilabel classification of UAV images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeggada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="5083" to="5086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large patch convolutional neural networks for the scene classification of high spatial resolution imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25006</biblScope>
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene classification of high resolution remote sensing images using convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="767" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using CNN-based highlevel features for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="2610" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sensors</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural network based classification for hyperspectral data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="5075" to="5078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification based on deep deconvolution network with skip architecture</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4781" to="4791" />
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D multi-resolution wavelet convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">420</biblScope>
			<biblScope unit="page" from="49" to="65" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using two-channel deep convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>-W. Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="5079" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">F.-Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE<address><addrLine>Art</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="volume">10615</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dense convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1254" to="1258" />
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building detection in very high resolution multispectral data with deep learning features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="1873" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building and road detection from large aerial imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE<address><addrLine>Art</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
			<biblScope unit="volume">9405</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CNN based suburban building detection using monocular high resolution Google Earth images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="661" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building extraction from multi-source remote sensing images via deep deconvolution neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge based 3D building model recognition using convolutional neural networks from LiDAR and aerial imageries</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alidoost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arefi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="833" to="840" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual U-Net</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Forest change detection in incomplete satellite images with deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5407" to="5423" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Land-use change detection with convolutional neural network methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environments</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Siamese network with multi-level features for patch-based change detection in satellite imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Cor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kerekes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Conf. Signal Inf. Process. (GlobalSIP)</title>
		<meeting>IEEE Global Conf. Signal Inf. ess. (GlobalSIP)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Change detection in synthetic aperture radar images based on deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An innovative neural-net method to detect temporal changes in high-resolution optical satellite imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Solimini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2940" to="2952" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spectral-spatialtemporal features via a recurrent convolutional neural network for change detection in multispectral imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Change detection in high resolution satellite images using an ensemble of convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. (APSIPA ASC)</title>
		<meeting>Asia-Pacific Signal Inf. ess. Assoc. Annu. Summit Conf. (APSIPA ASC)</meeting>
		<imprint>
			<date type="published" when="2018-11">Nov. 2018</date>
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual-dense convolution network for change detection of high-resolution panchromatic imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1785</biblScope>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generating high-accuracy urban distribution map for short-term change monitoring based on convolutional neural network by utilizing SAR imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">10428</biblScope>
			<biblScope unit="page">1042803</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised deep change vector analysis for multiple-change detection in VHR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3677" to="3693" />
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Change detection based on deep features and low rank</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2418" to="2422" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sentinel-2 change detection based on deep features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pomente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Del Frate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="6859" to="6862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-temporal remote sensing change detection based on independent component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2055" to="2061" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</editor>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised robust change detection on multispectral imagery using spectral and spatial features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wiemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Speck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kulbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bienlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Airborne Remote Sens. Conf. Exhib</title>
		<meeting>3rd Int. Airborne Remote Sens. Conf. Exhib</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Urban change detection for multispectral Earth observation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="2115" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Change detection in optical aerial images by a multilayer conditional mixed Markov model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Szirányi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3416" to="3430" />
			<date type="published" when="2009-10">Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The regularized iteratively reweighted MAD method for change detection in multi-and hyperspectral data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="463" to="478" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A deep convolutional coupling network for change detection based on heterogeneous optical and radar images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised change detection with expectation-maximization-based level set</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="214" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised change detection of satellite images using local gradual descent</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yetgin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1919" to="1929" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A deep convolutional auto-encoder with pooling-unpooling layers in caffe</title>
		<author>
			<persName><forename type="first">V</forename><surname>Turchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04949</idno>
		<ptr target="https://arxiv.org/abs/1701.04949" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
