<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Heterophily For Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
							<email>sitao.luan@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
							<email>chenqing.hua@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
							<email>qincheng.lu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
							<email>jiaqi.zhu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
							<email>mingde.zhao@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
							<email>shuyuan.zhang@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
							<email>chang@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Heterophily For Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-theart GNNs on most tasks without incurring significant computational burden.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Networks (NNs) <ref type="bibr" target="#b21">[22]</ref> have revolutionized many machine learning areas, including image recognition <ref type="bibr" target="#b20">[21]</ref>, speech recognition <ref type="bibr" target="#b12">[13]</ref> and natural language processing <ref type="bibr" target="#b1">[2]</ref>, due to their effectiveness in learning latent representations from Euclidean data. Recent research has shifted focus on non-Euclidean data <ref type="bibr" target="#b5">[6]</ref>, e.g., relational data or graphs. Combining graph signal processing and convolutional neural networks <ref type="bibr" target="#b22">[23]</ref>, numerous Graph Neural Network (GNN) architectures have been proposed <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, which empirically outperform traditional NNs on graph-based machine learning tasks such as node classification, graph classification, link prediction and graph generation, etc.GNNs are built on the homophily assumption <ref type="bibr" target="#b33">[34]</ref>: connected nodes tend to share similar attributes with each other <ref type="bibr" target="#b13">[14]</ref>, which offers additional information besides node features. This relational inductive bias <ref type="bibr" target="#b2">[3]</ref> is believed to be a key factor leading to GNNs' superior performance over NNs' in many tasks.</p><p>However, growing empirical evidence suggests that GNNs are not always advantageous compared to traditional NNs. In some cases, even simple Multi-Layer Perceptrons (MLPs) can outperform GNNs by a large margin on relational data <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref>. An important reason for this is believed to be the heterophily problem: the homophily assumption does not always hold, so connected nodes may in fact have different attributes. Heterophily has received lots of attention recently and an increasing number of models have been put forward to address this problem <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper, we first show that by only considering graph-label consistency, existing homophily metrics 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2210.07606v1 [cs.LG] 14 Oct 2022</head><p>are not able to describe the effect of some cases of heterophily on aggregation-based GNNs. We propose a post-aggregation node similarity matrix, and based on it, we derive new homophily metrics, whose advantages are illustrated on synthetic graphs (Sec. 3). Then, we prove that diversification operation can help to address some harmful cases of heterophily (Sec. 4). Based on this, we propose the Adaptive Channel Mixing (ACM) GNN framework which augments uni-channel baseline GNNs, allowing them to exploit aggregation, diversification and identity channels adaptively, node-wisely and locally in each layer. ACM significantly boosts the performance of 3 uni-channel baseline GNNs by 2.04% ? 27.5% for node classification tasks on 7 widely used benchmark heterophilic graphs, exceeding SOTA models (Sec. 6) on all of them. For 3 homophilic graphs, ACM-augmented GNNs can perform at least as well as the uni-channel baselines and are competitive compared with SOTA.</p><p>Contributions 1. To our knowledge, we are the first to analyze heterophily from post-aggregation node similarity perspective. 2. The proposed ACM framework is highly different from adaptive filterbank with multiple channels and existing GNNs for heterophily: 1) the traditional adaptive filterbank channels <ref type="bibr" target="#b38">[39]</ref> uses a scalar weight for each filter and this weight is shared by all nodes. In contrast, ACM provides a mechanism so that different nodes can learn different weights to utilize information from different channels to account for diverse local heterophily; 2) Unlike existing methods that leverage the high-order filters and global property of high-frequency signals <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> which require more computational resources, ACM successfully addresses heterophily by considering only the nodewise local information adaptively. 3. Unlike existing methods that try to facilitate learning filters with high expressive power <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, ACM aims that, when given a filter with certain expressive power, we can extract richer information from additional channels in a certain way to address heterophily. This makes ACM more flexible and easier to be implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we introduce notation and background knowledge. We use bold font for vectors (e.g., v). Suppose we have an undirected connected graph G = (V, E, A), where V is the node set with |V| = N ; E is the edge set without self-loops; A ? R N ?N is the symmetric adjacency matrix with A i,j = 1 if e ij ? E, otherwise A i,j = 0. Let D denote the diagonal degree matrix of G, i.e., D i,i = d i = j A i,j . Let N i denote the neighborhood set of node i, i.e., N i = {j : e ij ? E}. A graph signal is a vector x ? R N defined on V, where x i is associated with node i. We also have a feature matrix X ? R N ?F , whose columns are graph signals and whose i-th row X i,: is a feature vector of node i. We use Z ? R N ?C to denote the label encoding matrix, whose i-th row Z i,: is the one-hot encoding of the label of node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Laplacian, Affinity Matrix and Variants</head><p>The (combinatorial) graph Laplacian is defined as L = D -A, which is Symmetric Positive Semi-Definite (SPSD) <ref type="bibr" target="#b8">[9]</ref>. Its eigendecomposition is L = U ?U T , where the columns u i of U ? R N ?N are orthonormal eigenvectors, namely the graph Fourier basis, ? = diag(? 1 , . . . , ? N ) with ? 1 ? ? ? ? ? ? N . These eigenvalues are also called frequencies.</p><p>In additional to L, some variants are also commonly used, e.g., the symmetric normalized Laplacian L sym = D -1/2 LD -1/2 = I -D -1/2 AD -1/2 and the random walk normalized Laplacian L rw = D -1 L = I -D -1 A. The graph Laplacian and its variants can be considered as high-pass filters for graph signals. The affinity (transition) matrices can be derived from the Laplacians, e.g., A rw = I -L rw = D -1 A, A sym = I -L sym = D -1/2 AD -1/2 and are considered to be low-pass filters <ref type="bibr" target="#b32">[33]</ref>. Their eigenvalues satisfy ?</p><formula xml:id="formula_0">i (A rw ) = ? i (A sym ) = 1 -? i (L sym ) = 1 -? i (L rw ) ? (-1, 1].</formula><p>Applying the renormalization trick <ref type="bibr" target="#b18">[19]</ref> to affinity and Laplacian matrices respectively leads to ?sym = D-1/2 ? D-1/2 and Lsym = I -?sym , where ? ? A + I and D ? D + I. The renormalized affinity matrix essentially adds a self-loop to each node in the graph, and is widely used in Graph Convolutional Network (GCN) <ref type="bibr" target="#b18">[19]</ref> as follows:</p><formula xml:id="formula_1">Y = softmax( ?sym ReLU( ?sym XW 0 ) W 1 )<label>(1)</label></formula><p>where W 0 ? R F ?F1 and W 1 ? R F1?O are learnable parameter matrices. GCNs can be trained by minimizing the following cross entropy loss</p><formula xml:id="formula_2">L = -trace(Z T log Y )<label>(2)</label></formula><p>where log(?) is a component-wise logarithm operation. The random walk renormalized matrix ?rw = D-1 ?, which shares the same eigenvalues as ?sym , can also be applied in GCN. The corresponding Laplacian is defined as Lrw = I -?rw . The matrix ?rw is essentially a random walk matrix and behaves as a mean aggregator that is applied in spatial-based GNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. To bridge spectral and spatial methods, we use ?rw in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metrics of Homophily</head><p>The homophily metrics are defined by considering different relations between node labels and graph structures. There are three commonly used homophily metrics: edge homophily <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>, node homophily <ref type="bibr" target="#b34">[35]</ref> and class homophily <ref type="bibr" target="#b25">[26]</ref> <ref type="foot" target="#foot_0">1</ref> , defined as follows:</p><formula xml:id="formula_3">H edge (G) = {e uv | e uv ? E, Z u,: = Z v,: } |E| , H node (G) = 1 |V| v?V H v node = 1 |V| v?V {u | u ? N v , Z u,: = Z v,: } d v , H class (G) = 1 C -1 C k=1 h k - {v | Z v,k = 1} N + , h k = v?V {u | Z v,k = 1, u ? N v , Z u,: = Z v,: } v?{v|Z v,k =1} d v<label>(3</label></formula><p>) where H v node is the local homophily value for node v; [a] + = max(a, 0); h k is the class-wise homophily metric <ref type="bibr" target="#b25">[26]</ref>. All metrics are in the range of [0, 1]; a value close to 1 corresponds to strong homophily, while a value close to 0 indicates strong heterophily. H edge (G) measures the proportion of edges that connect two nodes in the same class; H node (G) evaluates the average proportion of edge-label consistency of all nodes; H class (G) tries to avoid sensitivity to imbalanced classes, which can make H edge (G) misleadingly large. The above definitions are all based on the linear featureindependent graph-label consistency. The inconsistency relation is implied to have a negative effect to the performance of GNNs. With this in mind, in the following section, we give an example to illustrate the shortcomings of the above metrics and propose new feature-independent metrics that are defined from post-aggregation node similarity perspective, which is novel. Metrics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis of Heterophily</head><formula xml:id="formula_4">H !"#! (G) = 0 H $%"! (G) = 0 H &amp;'()) (G) = 0 ? agg (G) = 1 ? agg * (G) = 1</formula><p>Class 1</p><p>Class 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Example of harmless heterophily</head><p>Heterophily is widely believed to be harmful for message-passing based GNNs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref> because, intuitively, features of nodes in different classes will be falsely mixed, leading nodes to be indistinguishable <ref type="bibr" target="#b44">[45]</ref>. Nevertheless, it is not always the case, e.g., the bipartite graph <ref type="foot" target="#foot_1">2</ref> shown in Figure <ref type="figure" target="#fig_8">1</ref> is highly heterophilic according to the existing homophily metrics in equation 3, but after mean aggregation, the nodes in classes 1 and 2 just exchange colors and are still distinguishable <ref type="foot" target="#foot_2">3</ref> . This example tells us that, besides graph-label consistency, we need to study the relation between nodes after aggregation step.</p><p>To this end, we first define the post-aggregation node similarity matrix as follows:</p><formula xml:id="formula_5">S( ?, X) ? ?X( ?X) T ? R N ?N (4)</formula><p>where ? ? R N ?N denotes a general aggregation operator. S( ?, X) is essentially the gram matrix that measures the similarity between each pair of aggregated node features.</p><p>Relationship Between S( ?, X) and Gradient of SGC SGC <ref type="bibr" target="#b40">[41]</ref> is one of the most simple but representative GNN models and its output can be written as:</p><formula xml:id="formula_6">Y = softmax( ?XW ) = softmax(Y )<label>(5)</label></formula><p>With the loss function in equation 2, after each gradient descent step, we have ?W = ? dL dW , where ? is the learning rate. The update of Y is (see Appendix E for derivation):</p><formula xml:id="formula_7">?Y = ?X?W = ? ?X dL dW ? ?X dL dW = ?XX T ?T (Z -Y ) = S( ?, X)(Z -Y )<label>(6)</label></formula><p>where Z -Y is the prediction error matrix. The update direction of the prediction for node i is essentially a weighted sum of the prediction error, i.e., ?(Y ) i,: = j?V S( ?, X) i,j (Z -Y ) j,:</p><p>and S( ?, X) i,j can be considered as the weights. Intuitively, a high similarity value S( ?, X) i,j means node i tends to be updated to the same class as node j. This indicates that S( ?, X) is closely related to a single layer GNN model.</p><p>Based on the above definition and observation, we define the aggregation similarity score as follows.</p><p>Definition 1. The aggregation similarity score is:</p><formula xml:id="formula_8">S agg S( ?, X) = 1 |V| v Mean u {S( ?, X) v,u |Z u,: = Z v,: } ? Mean u {S( ?, X) v,u |Z u,: = Z v,: }<label>(7)</label></formula><p>where Mean u ({?}) takes the average over u of a given multiset of values or variables.</p><p>S agg (S( ?, X)) measures the proportion of nodes v ? V as which the average weights on the set of nodes in the same class (including v) is larger than that in other classes. In practice, we observe that in most datasets, we will have S agg (S( ?, X)) ? 0.5<ref type="foot" target="#foot_3">4</ref> . To make the metric range in [0,1], like existing metrics, we rescale equation 7 to the following modified aggregation similarity,</p><formula xml:id="formula_9">S M agg S( ?, X) = 2S agg S( ?, X) -1 +<label>(8)</label></formula><p>In order to measure the consistency between labels and graph structures without considering node features and to make a fair comparison with the existing homophily metrics in equation 3, we define the graph (G) aggregation ( ?) homophily and its modified version <ref type="foot" target="#foot_4">5</ref> as:</p><formula xml:id="formula_10">H agg (G) = S agg S( ?, Z) , H M agg (G) = S M agg S( ?, Z)<label>(9)</label></formula><p>As the example shown in Figure <ref type="figure" target="#fig_8">1</ref>, when ? = ?rw , it is easy to see that H agg (G) = H M agg (G) = 1 and other metrics are 0. Thus, this new metric reflects the fact that nodes in classes 1 and 2 are still highly distinguishable after aggregation, while other metrics mentioned before fail to capture such information and misleadingly give value 0. This shows the advantage of H agg (G) and H M agg (G), which additionally exploit information from aggregation operator ? and the similarity matrix.</p><p>To comprehensively compare H M agg (G) with the existing metrics on their ability to elucidate the influence of graph structure on GNN performance, we generate synthetic graphs with different homophily levels and evaluate SGC <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on them in the next subsection.  Positive weights in intra-class blocks, Non-negative weights in cross-class blocks.</p><p>Class  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical Evaluation and Comparison on Synthetic Graphs</head><p>In this subsection, we conduct experiments on synthetic graphs generated with different levels of H M edge (G) to assess the output of H M agg (G) in comparison with existing metrics. Data Generation &amp; Experimental Setup We first generated 10 graphs for each of 28 edge homophily levels, from 0.005 to 0.95, for a total of 280 graphs. In every generated graph, we had 5 classes, with 400 nodes in each class. For nodes in each class, we randomly generated 800 intra-class edges and [ 800 Hedge(G) -800] inter-class edges. The features of nodes in each class are sampled from node features in the corresponding class of 6 base datasets (Cora, CiteSeer, PubMed, Chameleon, Squirrel, Film). Nodes were randomly split into train/validation/test sets, in proportion of 60%/20%/20%. We trained 1-hop SGC (sgc-1) <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on the synthetic graphs <ref type="foot" target="#foot_5">6</ref> . For each value of H edge (G), we take the average test accuracy and standard deviation of runs over the 10 generated graphs with that value. For each generated graph, we also calculate H node (G), H class (G) and H M agg (G). Model performance with respect to different homophily values is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Homophily Metrics</head><p>The performance of SGC-1 and GCN is expected to be monotonically increasing if the homophily metric is informative. However, Figure <ref type="figure" target="#fig_1">2</ref>(a)(b)(c) show that the performance curves under H edge (G), H node (G) and H class (G) are U -shaped<ref type="foot" target="#foot_6">7</ref> , while Figure <ref type="figure" target="#fig_1">2</ref>(d) reveals a nearly monotonic curve with a little numerical perturbation around 1. This indicates that H M agg (G) provides a better indication of the way in which the graph structure affects the performance of SGC-1 and GCN than existing metrics. (See more discussion on aggregation homophily and theoretical results for regular graphs in Appendix D.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Channel Mixing (ACM)</head><p>In prior work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>, it has been shown that high-frequency graph signals, which can be extracted by a high-pass filter (HP), is empirically useful for addressing heterophily. In this section, based on the similarity matrix in equation 6, we theoretically prove that a diversification operation, i.e., HP filter, can address some cases of harmful heterophily locally. Besides, a node-wise analysis shows that different nodes may need different filters to process their neighborhood information. Based on the above analysis, in Sec. 4.2 we propose Adaptive Channel Mixing (ACM), a 3-channel architecture which can adaptively exploit local and node-wise information from aggregation, diversification and identity channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Diversification Helps with Harmful Heterophily</head><p>We first consider the example shown in Figure <ref type="figure" target="#fig_3">3</ref>. From S( ?, X), we can see that nodes {1, 3} assign relatively large positive weights to nodes in class 2 after aggregation, which will make nodes {1, 3} hard to be distinguished from nodes in class 2. However, we can still distinguish nodes {1, 3} and {4, 5, 6, 7} by considering their neighborhood differences: nodes {1, 3} are different from most of their neighbors while nodes {4, 5, 6, 7} are similar to most of their neighbors. This indicates that although some nodes become similar after aggregation, they are still distinguishable through their local surrounding dissimilarities. This observation leads us to introduce the diversification operation, i.e., HP filter I -? <ref type="bibr" target="#b10">[11]</ref> to extract information regarding neighborhood differences, thereby addressing harmful heterophily. As S(I -?, X) in Fig. <ref type="figure" target="#fig_3">3</ref> shows, nodes {1, 3} will assign negative weights to nodes {4, 5, 6, 7} after the diversification operation, i.e., nodes 1,3 treat nodes 4,5,6,7 as negative samples and will move away from them during backpropagation. This example reveals that there are cases in which the diversification operation is helpful to handle heterophily, while the aggregation operation is not. Based on this observation, we first define the diversification distinguishability of a node and the graph diversification distinguishability value, which measures the proportion of nodes for which the diversification operation is potentially helpful. Definition 2. Diversification Distinguishability (DD) based on S(I -?, X).</p><p>Given S(I -?, X), a node v is diversification distinguishable if the following two conditions are satisfied at the same time,</p><formula xml:id="formula_11">1. Mean u {S(I -?, X) v,u |u ? V ? Z u,: = Z v,: } ? 0; 2. Mean u {S(I -?, X) v,u |u ? V ? Z u,: = Z v,: } ? 0 (10)</formula><p>Then, graph diversification distinguishability value is defined as</p><formula xml:id="formula_12">DD ?,X (G) = 1 |V| {v|v ? V ? v is diversification distinguishable}<label>(11)</label></formula><p>We can see that DD ?,X (G) ? [0, 1]. Based on Def. 2, the effectiveness of diversification in addressing heterophily can be theoretically proved under certain conditions:</p><p>Theorem 1. (See Appendix G for proof). For C = 2, suppose X = Z, ? = ?rw . Then for any I -?rw , all nodes are diversification distinguishable and DD ?,Z (G) = 1.</p><p>With the above results for HP filters, we will now introduce the concept of filterbank which combines both LP (aggregation) and HP (diversification) filters and can potentially handle various local heterophily cases. We then develop ACM framework in the following subsection. Filterbank For the graph signal x defined on G, a 2-channel linear (analysis) filterbank <ref type="bibr" target="#b10">[11]</ref> <ref type="foot" target="#foot_7">8</ref> includes a pair of filters H LP , H HP , which retain the low-frequency and high-frequency content of x, respectively. Most existing GNNs use a uni-channel filtering architecture <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15]</ref> with either LP or HP channel, which only partially preserves the input information. Unlike the uni-channel architecture, filterbanks with H LP + H HP = I do not lose any information from the input signal, which is called the perfect reconstruction property <ref type="bibr" target="#b10">[11]</ref>. Generally, the Laplacian matrices (L sym , L rw , Lsym , Lrw ) can be regarded as HP filters <ref type="bibr" target="#b10">[11]</ref> and affinity matrices (A sym , A rw , ?sym , ?rw ) can be treated as LP filters <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, we extend the concept of filterbank and view MLPs as using the identity (fullpass) filterbank with H LP = I and H HP = 0, which also satisfies</p><formula xml:id="formula_13">H LP + H HP = I + 0 = I.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-wise Channel Mixing for Diverse Local Homophily</head><p>The example in Figure <ref type="figure" target="#fig_3">3</ref> also shows that different nodes may need the local information extracted from different channels, e.g., nodes {1, 3} demand information from the HP channel while node 2 only needs information from the LP channel. Figure <ref type="figure">4</ref> reveals that nodes have diverse distributions of node local homophily H v node across different datasets. In order to adaptively leverage the LP, HP and identity channels in GNNs to deal with the diverse local heterophily situations, we will now describe our proposed Adaptive Channel Mixing (ACM) framework.</p><p>Adaptive Channel Mixing (ACM) We will use GCN <ref type="foot" target="#foot_8">9</ref> as an example to introduce the ACM framework in matrix form, but the framework can be combined in a similar manner to many different GNNs. The ACM framework includes the following steps:</p><p>Step 1. Feature Extraction for Each Channel:</p><formula xml:id="formula_14">Option 1: H l L = ReLU H LP H l-1 W l-1 L , H l H = ReLU H HP H l-1 W l-1 H , H l I = ReLU IH l-1 W l-1 I ; Option 2: H l L = H LP ReLU H l-1 W l-1 L , H l H = H HP ReLU H l-1 W l-1 H , H l I = I ReLU H l-1 W l-1 I ; H 0 = X ? R N ?F0 , W l-1 L , W l-1 H , W l-1 I ? R F l-1 ?F l , l = 1, . . . , L; Step 2. Row-wise Feature-based Weight Learning ?l L = Sigmoid H l L W l L , ?l H = Sigmoid H l H W l H , ?l I = Sigmoid H l I W l I , W l-1 L , W l-1 H , W l-1 I ? R F l ?1 ? l L , ? l H , ? l I = Softmax ( ?l L , ?l H , ?l I /T )W l Mix ? R N ?3 , T ? R temperature, W l Mix ? R 3?3 ;</formula><p>Step 3. Node-wise Adaptive Channel Mixing:</p><formula xml:id="formula_15">H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I</formula><p>We will refer to the instantiation which uses option 1 in step 1 as ACM and to the one using option 2 as ACMII. In step 1, ACM(II)-GCN implement different feature extractions for 3 channels using a set of filterbanks. Three filtered components, H l L , H l H , H l I , are obtained. To adaptively exploit information from each channel, ACM(II)-GCN first extract nonlinear information from the filtered signals, then use W l Mix to learn which channel is important for each node, leading to the row-wise weight vectors ? l L , ? l H , ? l I ? R N ?1 whose i-th elements are the weights for node i<ref type="foot" target="#foot_9">10</ref> . These three vectors are then used as weights in defining the updated H l in step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity The number of learnable parameters in layer</head><formula xml:id="formula_16">l of ACM(II)-GCN is 3F l-1 (F l + 1) + 9, compared to F l-1 F l in GCN. The computation of steps 1-3 takes N F l (8 + 6F l-1 ) + 2F l (nnz(H LP ) + nnz(H HP )) + 18N flops, while the GCN layer takes 2N F l-1 F l + 2F l (nnz(H LP )) flops, where nnz(?)</formula><p>is the number of non-zero elements. An ablation study and a detailed comparison on running time are conducted in Sec. 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations of Diversification</head><p>Like any other method, there exists some cases of harmful heterophily that diversification operation cannot work well. For example, suppose we have an imbalanced dataset where several small clusters with distinctive labels are densely connected to a large cluster. In this case, the surrounding differences of nodes in small clusters are similar, i.e., the neighborhood differences mainly come from their connections to the same large cluster, and this can lead to the diversification operation failing to discriminate them. See Appendix H for a more detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We now discuss relevant work on addressing heterophily in GNNs. <ref type="bibr" target="#b0">[1]</ref> acknowledges the difficulty of learning on graphs with weak homophily and propose MixHop to extract features from multi-hop neighborhoods to get more information. <ref type="bibr" target="#b16">[17]</ref> propose measurements based on feature smoothness and label smoothness that are potentially helpful to guide GNNs when dealing with heterophilic graphs. Geom-GCN <ref type="bibr" target="#b34">[35]</ref> precomputes unsupervised node embeddings and uses the graph structure defined by geometric relationships in the embedding space to define the bi-level aggregation process to handle heterophily. H 2 GCN <ref type="bibr" target="#b44">[45]</ref> combines 3 key designs to address heterophily: (1) ego-and neighbor-embedding separation; (2) higher-order neighborhoods; (3) combination of intermediate representations. CPGNN <ref type="bibr" target="#b43">[44]</ref> models label correlations through a compatibility matrix, which is beneficial for heterophilic graphs, and propagates a prior belief estimation into the GNN by using the compatibility matrix. FAGCN <ref type="bibr" target="#b3">[4]</ref> learns edge-level aggregation weights as GAT <ref type="bibr" target="#b39">[40]</ref> but allows the weights to be negative, which enables the network to capture high-frequency components in the graph signals. GPRGNN <ref type="bibr" target="#b7">[8]</ref> uses learnable weights that can be both positive and negative for feature propagation. This allows GPRGNN to adapt to heterophilic graphs and to handle both high-and low-frequency parts of the graph signals (See Appendix J for a more comprehensive comparison between ACM-GNNs, ACMII-GNNs and FAGCN, GPRGNN). BernNet <ref type="bibr" target="#b15">[16]</ref> designs a scheme to learn arbitrary graph spectral filters with Bernstein polynomial to address heterophily. <ref type="bibr" target="#b31">[32]</ref> points out that homophily is not necessary for GNNs and characterizes conditions that GNNs can perform well on heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Evaluation</head><p>In this section, we evaluate the proposed ACM and ACMII framework on real-world datasets (see Appendix D.2 for a performance comparison with basline models on synthetic datasets). We first conduct ablation studies in Sec. 6.1 to validate the effectiveness and efficiency of different components of ACM and ACMII. Then, we compare with state-of-the-art (SOTA) models in Sec. 6.2. The hyperparameter searching range and computing resources are described in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study &amp; Efficiency</head><p>We will now investigate the effectiveness and efficiency of adding HP, identity channels and the adaptive mixing mechanism in the proposed framework by performing an ablation study. Specifically, we apply the components of ACM to SGC-1 <ref type="bibr" target="#b40">[41]</ref>  <ref type="foot" target="#foot_10">11</ref> and the components of ACM and ACMII to GCN <ref type="bibr" target="#b18">[19]</ref> separately. We run 10 times on each of the 9 benchmark datatsets, Cornell, Wisconsin, Texas, Film, Chameleon, Squirrel, Cora, Citeseer and Pubmed used in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, with the same 60%/20%/20% random splits for train/validation/test used in <ref type="bibr" target="#b7">[8]</ref> and report the average test accuracy as well as the standard deviation. We also record the average running time per epoch (in milliseconds) to compare the computational efficiency. We set the temperature T in equation 4.2 to be 3, which is the number of channels.</p><p>The results in Table <ref type="table" target="#tab_1">1</ref> show that on most datasets, the additional HP and identity channels are helpful, even for strong homophily datasets such as Cora, CiteSeer and PubMed. The adaptive mixing mechanism also has an advantage over directly adding the three channels together. This illustrates the necessity of learning to customize the channel usage adaptively for different nodes. The t-SNE visualization in Figure <ref type="figure" target="#fig_6">5</ref> demonstrates that the high-pass channel(e) and identity channel(f) can extract meaningful patterns, which the low-pass channel(d) is not able to capture. The output of ACM-GCN(c) shows clearer boundaries among classes than GCN(b). The running time is approximately doubled in the ACM and ACMII framework compared to the original models.  <ref type="bibr" target="#b40">[41]</ref> with 1 hop and 2 hops (SGC-1, SGC-2), GCNII <ref type="bibr" target="#b6">[7]</ref>, GCNII * <ref type="bibr" target="#b6">[7]</ref>, GCN <ref type="bibr" target="#b18">[19]</ref> and snowball networks <ref type="bibr" target="#b28">[29]</ref> with 2 and 3 layers (snowball-2, snowball-3) and combine them with the ACM or ACMII framework <ref type="foot" target="#foot_11">12</ref> . We use ?rw as the LP filter and the corresponding HP filter is I -?rw <ref type="foot" target="#foot_12">13</ref> . Both filters are deterministic. We compare these approaches with several baselines and SOTA GNN models: MLP with 2 layers (MLP-2), GAT <ref type="bibr" target="#b39">[40]</ref>, APPNP <ref type="bibr" target="#b19">[20]</ref>, GPRGNN <ref type="bibr" target="#b7">[8]</ref>, H 2 GCN <ref type="bibr" target="#b44">[45]</ref>, MixHop <ref type="bibr" target="#b0">[1]</ref>, GCN+JK <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26]</ref>, GAT+JK <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26]</ref>, FAGCN <ref type="bibr" target="#b3">[4]</ref>, GraphSAGE <ref type="bibr" target="#b14">[15]</ref>, Geom-GCN <ref type="bibr" target="#b34">[35]</ref> and BernNet <ref type="bibr" target="#b15">[16]</ref>. In addition to the 9 benchmark datasets used in section 6.1, we further test the above models on a new benchmark dataset, Deezer-Europe <ref type="bibr" target="#b36">[37]</ref>  <ref type="foot" target="#foot_13">14</ref> .</p><p>On each dataset used in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we test the models 10 times following the same early stopping strategy, the same 60%/20%/20% random data split <ref type="foot" target="#foot_14">15</ref> and Adam <ref type="bibr" target="#b17">[18]</ref> optimizer as used in GPRGNN <ref type="bibr" target="#b7">[8]</ref>. For Deezer-Europe, we test the above models 5 times with the same early stopping strategy, the same fixed splits and Adam used in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure information channel and residual connection</head><p>Besides the filtered features, some recent SOTA models additionally use graph structure information, i.e., MLP ? (A), and residual connection to address heterophily problem, e.g., LINKX <ref type="bibr" target="#b24">[25]</ref> and GloGNN <ref type="bibr" target="#b23">[24]</ref>. MLP ? (A) and residual connection can be directly incorporated into ACM and ACMII framework, which leads us to ACM(II)-GCN+ and ACM(II)-GCN++. See the details of implementation in Appendix B.</p><p>To visualize the performance, in Fig. <ref type="figure" target="#fig_7">6</ref>, we plot the bar charts of the test accuracy of SOTA models, three selected baselines (GCN, snowball-2, snowball-3), their ACM(II) augmented models, ACM(II)- GCN+ and ACM(II)-GCN++ on the 6 most commonly used benchmark heterophily datasets (See Table <ref type="table" target="#tab_3">2</ref> in Appendix A.1 for the full results, comparison and ranking). From Fig. <ref type="figure" target="#fig_7">6</ref>, we can see that (1) after being combined with the ACM or ACMII framework, the performance of the three baseline models is significantly boosted, by 2.04%?27.50% on all the 6 tasks. The ACM and ACMII in fact achieve SOTA performance. (2) On Cornell, Wisconsin, Texas, Chameleon and Squirrel, the augmented baseline models significantly outperform the current SOTA models. Overall, these results suggest that the proposed approach can help GNNs to generalize better on node classification tasks on heterophilic graphs, without adding too much computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Limitations</head><p>We have presented an analysis of existing homophily metrics and proposed new metrics which are more informative in terms of correlating with GNN performance. To our knowledge, this is the first work analyzing heterophily from the perspective of post-aggregation node similarity. The similarity matrix and the new metrics we defined mainly capture linear feature-independent relationships of each node. This might be insufficient when nonlinearity and feature-dependent information is important for classification. In the future, it would be useful to investigate if a similarity matrix could be defined which is capable of capturing nonlinear and feature-dependent relations between aggregated node.</p><p>We have also proposed a multi-channel mixing mechanism which leverages the intuitions gained in the first part of the paper and can be combined with different GNN architectures, enabling adaptive filtering (high-pass, low-pass or identity) at different nodes. Empirically, this approach shows very promising results, improving the performance of the base GNNs with which it is combined and achieving SOTA results at the cost of a reasonable increase in computation time. As discussed in Sec. 4.2, however, the filterbank method cannot properly handle all cases of harmful heterophily, and alternative ideas should be explored as well in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experimental Results</head><p>A.1 Comparison with SOTA Models on 60%/20%/20% Random Splits</p><p>The main results of the full sets of experiments <ref type="foot" target="#foot_16">16</ref> with statistics of datasets are summarized in Table <ref type="table" target="#tab_3">2</ref>, where we report the mean accuracy (%) and standard deviation. We can see that after applied in ACM or ACMII framework, the performance of baseline models are boosted on almost all tasks and achieve SOTA performance on 9 out of 10 datasets. Especially, ACMII-GCN+ performs the best in terms of average rank (4.40) across all datasets. Overall, It suggests that ACM or ACMII framework can significantly increase the performance of GNNs on node classification tasks on heterophilic graphs and maintain highly competitive performance on homophilic datasets.   <ref type="figure" target="#fig_7">6</ref>) are underlined. Results "*" are reported from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> and results " ? " are from <ref type="bibr" target="#b34">[35]</ref>. NA means the reported results are not available and OOM means out of memory.</p><p>A.2 Comparison with SOTA Models on Fixed 48%/32%/20% Splits See table <ref type="table" target="#tab_5">3</ref> for the results and table <ref type="bibr" target="#b12">13</ref>   The definitions of the similarity matrix, (modified) aggregation similarity score and diversification distinguishability value can be extended to symmetric normalized Laplacian or other aggregation operations. Yet unfortunately, we cannot extend Theorem 1 at this moment, because we need a condition that the row sum of ? is not greater than 1 in the proof. This condition is guaranteed for random walk normalized Laplacian but not for symmetric normalized Laplacian. While in practice, we evaluate our models with symmetric filters and compare them with random walk filters. From table <ref type="table" target="#tab_6">4</ref> we can see that, there are no big differences between these two filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets/Models</head><p>With From table <ref type="table" target="#tab_7">5</ref> we can see that ACM(II) with W mix shows superiority in most datasets, although it is not statistically significant on some of them.</p><p>One possible explanation of the function of W mix is that it could help alleviate the dominance and bias to majority: Suppose in a dataset, most of the nodes need more information from LP channel than HP and identity channels, then W L , W H , W I tend to learn larger ? L than ? H and ? I . For the minority nodes that need more information from HP or identity channels, they are hard to get large ? H or ? I values because W L , W H , W I are biased to the majority. And W mix can help us to learn more diverse alpha values when W L , W H , W I are biased.</p><p>Attention with more complicated design can be found for the node-wise adaptive channel mixing mechanism, but we do not explore this direction deeper in this paper because investigating attention function is not the main contribution of our paper.</p><p>A. </p><formula xml:id="formula_17">?l L = ? H l Comb W l L , ?l H = ? H l Comb W l H , ?l I = ? H l Comb W l I , W l-1 L , W l-1 H , W l-1 I ? R 3F l ?1 ? l L , ? l H , ? l I = Softmax ( ?l L , ?l H , ?l I /T )W l Mix ? R N ?3 , T ? R temperature, W l Mix ? R 3?3 ;</formula><p>The performance comparison can be found in table <ref type="bibr" target="#b5">6</ref>. From the results, we do not find significant difference between the frameworks with combined features and raw features. The reason is that the necessary nonlinear information from each channel is combined in ?l L , ?l H , ?l I and W l Mix is enough to learn to mix the combined weights from different channels. The learning of redundant information in the feature extraction step for each channel will not improve the performance. Meanwhile, A disadvantage of the combined feature is that it increases the computational cost. Thus, we decide to use the raw features.    Unlike other baseline GNN models, GCNII and GCNII* are not able to be applied under ACMII framework and we will make an explanation as follows.</p><p>GCNII:</p><formula xml:id="formula_18">H ( +1) = ? (1 -? ) ?H ( ) + ? H (0) (1 -? ) I n + ? W ( ) GCNII*: H ( +1) = ? (1 -? ) ?H ( ) (1 -? ) I n + ? W ( ) 1 + +? H (0) (1 -? ) I n + ? W ( ) 2</formula><p>From the above formulas of GCNII and GCNII * we cam see that, without major modification, GCNII and GCNII* are hard to be put into ACMII framework. In ACMII framework, before apply ?, we first implement a nonlinear feature extractor ?(H W ( ) ). But in GCNII and GCNII*, before multiplying W (or W 1 , W 2 ) to extract features, we need to add another term including H (0) , which are not filtered by ?. This makes the order of aggregator ? and nonlinear extractor unexchangable and thus, incompatible with ACMII framework. So we did not implement GCNII and GCNII* in ACMII framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation of ACM(II)-GCN+ and ACM(II)-GCN++</head><p>Besides the features extracted by different filters, some recent SOTA models use additional graph structure information explicitly, i.e., MLP ? (A) , to address heterophily problem, e.g., LINKX <ref type="bibr" target="#b24">[25]</ref> and GloGNN <ref type="bibr" target="#b23">[24]</ref> and is found effective on some datasets, e.g., Chameleon, Squirrel. The explicit structure information can be directly incorporated into ACM and ACMII framework, and we have ACM(II)-GCN+ and ACM(II)-GCN++ as follows.</p><p>? ACM-GCN+ and ACMII-GCN+ have an option to include structure information channel (the 4-th channel) in each layer and their differences from ACM-GCN and ACMII-GCN are highlighted in red) as follows,</p><p>Step 1. Feature Extraction for LP, HP, Identity and Structure Information Channel: </p><formula xml:id="formula_19">H l A = ReLU AW l A , W l A ? R N ?F l ,</formula><formula xml:id="formula_20">Hl L = LN(H l L ), Hl H = LN(H l H ), Hl I = LN(H l I ), Hl A = LN(H l A ), ?l L = Sigmoid Hl L W l L , ?l H = Sigmoid Hl H W l H , ?l I = Sigmoid Hl I W l I , ?l A = Sigmoid Hl A W l A , W l-1 L , W l-1 H , W l-1 I , W l A ? R F l ?1</formula><p>Step 3. Node-wise Adaptive Channel Mixing: Option 1: without structure information</p><formula xml:id="formula_21">? l L , ? l H , ? l I = Softmax ( ?l L , ?l H , ?l I /T )W l Mix ? R N ?3 , T = 3 temperature, W l Mix ? R 3?3 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I</formula><p>Option 2: with structure information</p><formula xml:id="formula_22">? l L , ? l H , ? l I , ? l A = Softmax ( ?l L , ?l H , ?l I , ?l A /T )W l Mix ? R N ?4 , T = 4 temperature, W l Mix ? R 4?4 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + diag(? l A )H l A</formula><p>? ACM-GCN++ and ACMII-GCN++ have an option to include structure information channel (the 4-th channel) in each layer and residual connection and their differences from ACM-GCN+ and ACMII-GCN+ are highlighted in red) as follows,</p><p>Step 1. Feature Extraction for LP, HP, Identity and Structure Information Channel, Get H X :</p><formula xml:id="formula_23">H X = ReLU (XW X ) ? R F ?F , H l A = ReLU AW l A , W l A ? R N ?F , get H l</formula><p>L , H l H , H l I with the same step as ACM-GCN and ACMII-GCN.</p><p>Step 2. Row-wise Feature-based Weight Learning with Layer Normalization (LN)</p><formula xml:id="formula_24">Hl L = LN(H l L ), Hl H = LN(H l H ), Hl I = LN(H l I ), Hl A = LN(H l A ), ?l L = Sigmoid Hl L W l L , ?l H = Sigmoid Hl H W l H , ?l I = Sigmoid Hl I W l I , ?l A = Sigmoid Hl A W l A , W l-1 L , W l-1 H , W l-1 I , W l A ? R F ?1</formula><p>Step 3. Node-wise Adaptive Channel Mixing: Option 1: without structure information</p><formula xml:id="formula_25">? l L , ? l H , ? l I = Softmax ( ?l L , ?l H , ?l I /T )W l Mix ? R N ?3 , T = 3 temperature, W l Mix ? R 3?3 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + H X Option 2: with structure information ? l L , ? l H , ? l I , ? l A = Softmax ( ?l L , ?l H , ?l I , ?l A /T )W l Mix ? R N ?4 , T = 4 temperature, W l Mix ? R 4?4 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + diag(? l A )H l A + H X</formula><p>The results of ACM-GCN+, ACMII-GCN+, ACM-GCN++ and ACMII-GCN++ trained on random 60%/20%/20% splits are reported in  </p><formula xml:id="formula_26">g(h) ? E S( ?, Z) v,u1 -E S( ?, Z) v,u2 = (C -1)(hd + 1) -(1 -h)d (C -1)(d + 1) 2<label>(12)</label></formula><p>and the minimum of g(h) is reached at</p><formula xml:id="formula_27">h = d + 1 -C Cd = d intra /h + 1 -C C(d intra /h) ? h = d intra Cd intra + C -1</formula><p>where d intra = dh, which is the expected number of neighbors of a node that have the same label as the node.</p><p>The value of g(h) in equation 12 is the expected differences of the similarity values between nodes in the same class as v and nodes in other classes. g(h) is strongly related to the definition of aggregation homophily and its minimum potentially implies the turning point of performance curves. In the synthetic experiments, we have d intra = 10, C = 5 and the minimum of g(h) is reached at h = 5/27 ? 0.1852, which corresponds to the lowest point in the performance curve in Figure <ref type="figure" target="#fig_13">12</ref>. In other words, the H edge (G) where SGC-1 and GCN perform worst is where g(h) gets the smallest value, instead of the point with the smallest edge homophily value, i.e., H edge (G) = 0. This reveals the advantage of H agg (G) over H edge (G) by taking use of the similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Gradient Calculation in equation 6 E.1 Derivation in Matrix Form</head><p>This derivation is similar to <ref type="bibr" target="#b29">[30]</ref>.</p><p>In output layer, we have</p><formula xml:id="formula_28">Y = softmax( ?XW ) ? softmax(Y ) = exp(Y )1 C 1 T C -1 exp(Y ) &gt; 0 L = -trace(Z T log Y ) where 1 C ? R C?1 , (?) -1 is point-wise inverse function and each element of Y is positive. Then dL = -trace Z T ((Y ) -1 dY ) = -trace Z T (softmax(Y )) -1 d softmax(Y ) 31 Note that d softmax(Y ) = -exp(Y )1 C 1 T C -2 [(exp(Y ) dY )1 C 1 T C ] exp(Y ) + exp(Y )1 C 1 T C -1 (exp(Y ) dY ) = -softmax(Y ) exp(Y )1 C 1 T C -1 [(exp(Y ) dY )1 C 1 T C ] + softmax(Y ) dY = softmax(Y ) -exp(Y )1 C 1 T C -1 (exp(Y ) dY )1 C 1 T C + dY Then, dL = -trace Z T (softmax(Y )) -1 softmax(Y ) -exp(Y )1 C 1 T C -1 (exp(Y ) dY )1 C 1 T C + dY = -trace Z T -exp(Y )1 C 1 T C -1 (exp(Y ) dY )1 C 1 T C + dY = trace Z exp(Y )1 C 1 T C -1 1 C 1 T C T [exp(Y ) dY ] -Z T dY = trace exp(Y ) Z exp(Y )1 C 1 T C -1 1 C 1 T C T dY -Z T dY = trace exp(Y ) exp(Y )1 C 1 T C -1 T dY -Z T dY = trace (softmax(Y ) -Z) T dY</formula><p>where the 4-th equation holds due to Z exp To get dL dW we have,</p><formula xml:id="formula_29">(Y )1 C 1 T C -1 1 C 1 T C = exp(Y )1 C 1 T C -1 .</formula><formula xml:id="formula_30">dL dW = X T ?T dL dY = X T ?T (Y -Z)<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Component-wise Derivation</head><p>Denote X = XW . We rewrite L as follows: </p><formula xml:id="formula_31">L = -trace Z T log (exp(Y )1 C 1 T C ) -1 exp(Y ) = -trace Z T -log(exp(Y )1 C 1 T C ) + Y = -trace Z T Y + trace Z T log exp(Y )1 C 1 T C = -trace Z T ?XW + trace Z T log exp(Y )1 C 1 T C = -trace Z T ?XW + trace 1 T C log (exp(Y )1 C ) Expand L component-</formula><formula xml:id="formula_32">dL d Xj ,c = - N i=1 C c=1 exp( j?Ni ?i,j Xj,c ) exp C c=1 j?Ni ?i,j Z i,c Xj,c ? ? ? ? ? ? ? ? ?i,j Z i,c exp C c=1 j?Ni ?i,j Z i,c Xj,c C c=1 exp( j?Ni ?i,j Xj,c ) C c=1 exp( j?Ni ?i,j Xj,c ) 2 - ?i,j exp C c=1 j?Ni ?i,j Z i,c Xj,c exp( j?Ni ?i,j Xj,c ) C c=1 exp( j?Ni ?i,j Xj,c ) 2 ? ? ? ? ? ? ? = - N i=1 ? ? ? ? ? ? ?i,j Z i,c C c=1 exp( j?Ni ?i,j Xj,c ) -?i,j exp( j?Ni ?i,j Xj,c ) C c=1 exp( j?Ni ?i,j Xj,c ) ? ? ? ? ? ? = - N i=1 ? ? ? ? ? ? ?i,j C c=1,c =c (Z i,c ) exp( j?Ni ?i,j Xj,c ) + (Z i,c -1) exp( j?Ni ?i,j Xj,c ) C c=1 exp( j?Ni ?i,j Xj,c ) ? ? ? ? ? ? = - N i=1 ?i,j Z i,c P (Y i = c ) + (Z i,c -1) P (Y i = c ) = - N i=1 ?i,j Z i,c -P (Y i = c )<label>(14)</label></formula><p>Writing the above in matrix form, we have</p><formula xml:id="formula_33">dL d X = ?(Z -Y ), dL d W = X T ?T (Z -Y ), ?Y ? ?XX T ?T (Z -Y )<label>(15)</label></formula><p>F Proof of Proposition 1</p><p>Proof. According to the given assumptions, for node v, we have ?v,k = 1 d+1 , the expected number of intra-class edges is dh (here the self-loop edge introduced by ? is not counted based on the definition of edge homophily and data generation process) and inter-class edges is (1 -h)d. Suppose there are C ? 2 classes. Consider matrix ?Z,</p><formula xml:id="formula_34">Then, we have E ( ?Z) v,c = E k?V ?v,k 1 {Z k,: =e T c } = k?V E 1 {Z k,: =e T c } d+1</formula><p>, where 1 is the indicator function.</p><p>When v is in class c, we have  Setting g(h) = 0, we obtain the optimal h:</p><formula xml:id="formula_36">h = d + 1 -C Cd (<label>17</label></formula><formula xml:id="formula_37">)</formula><p>For the data generation process in the synthetic experiments, we fix d intra , then d = d intra /h, which is a function of h. We change d in equation 17 to d intra /h, leading to</p><formula xml:id="formula_38">h = d intra /h + 1 -C Cd intra /h<label>(18)</label></formula><p>It is easy to observe that h satisfying equation 18 still makes g(h) = 0, when d in g(h) is replaced by d intra /h. From equation 18 we obtain the optimal h in terms of d intra :    From the black box area of -?, X) in the example in Figure <ref type="figure" target="#fig_20">13</ref> we can see that nodes in class 1 and 4 assign non-negative weights to each other although there is no edge between them; nodes in class 2 and 3 assign non-negative weights to each other as well. This is because the surrounding differences of class 1 are similar as class 4, so are class 2 and 3. In real-world applications, when nodes in several small clusters connect to a large cluster, the surrounding differences of the nodes in the small clusters will become similar. In such case, HP filter are not able to distinguish the nodes from different small clusters.</p><formula xml:id="formula_39">h = d intra Cd intra + C -1</formula><formula xml:id="formula_40">W c v W c u -W cv v -W cv u = C c=1,c =cv W c v W c u + 1 + W cv v W cv u -W cv v -W cv u = C c=1,c =cv W c v W c u + (1 -W cv v )(1 -W cv u ) ? 0</formula><p>I The Similarity, Homophily and DD ?,X (G) Metrics and Their Estimations</p><p>Firstly, we would like to clarify that, for each curve in the synthetic experiments, the node features are fixed and we only change the homophily values. But in real-world tasks, different datasets have different features and aggregated features. Thus, to get more instructive information for different datasets and compare them, we need to consider more metrics, e.g. feature-label consistency and</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of baseline performance under different homophily metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in inter-class block for node 1,3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of how diversification can address harmful heterophily</figDesc><graphic url="image-208.png" coords="5,371.86,91.61,127.88,56.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 2 Figure 4 :</head><label>24</label><figDesc>Figure 4: H v node distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Input Feature (b) GCN Output (c) ACM-GCN Output (d) Low-pass Channel (e) High-pass Channel (f) Identity Channel 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: t-SNE visualization of the output layer of ACM-GCN and GCN trained on Squirrel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of baseline GNNs (red), ACM-GNNs (green), ACMII-GNNs (blue) with SOTA (magenta line) models on 6 selected datasets. The black lines indicate the standard deviation. The symbol "?" shows the range of performance improvement (%) of ACM-GNNs and ACMII-GNNs over baseline GNNs. See Appendix I for a detailed discussion of the relation between H M agg and GNN performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A. 6</head><label>6</label><figDesc>H v node Distributions of Different Datasets See Figure 7 for H v node distributions. We can see that Wisconsin and Texas have high density in low homophily area, Cornell, Chameleon, Squirrel and Film have high density in low and middle homophily area, Cora, CiteSeer and PubMed have high density in high homophily area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: H v node distributions of different datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distributions of the learned ? L , ? H , ? I in the hidden layer of ACM-GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Distributions of the learned ? L , ? H , ? I in the output layer of ACM-GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Synthetic experiments for edge homophily on regular graphs.</figDesc><graphic url="image-368.png" coords="31,179.85,71.99,249.81,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Thus, we have dL dY = softmax(Y ) -Z = Y -Z For Y and W , we have dY = ?XdW and dL = trace dL dY T dY = trace dL dY T ?X dW = trace dL dW T dW</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Z</head><label></label><figDesc>j,c = 1 for any j. Consider the derivation of L over Xj ,c :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>k?V E 1 E 1 2 +</head><label>112</label><figDesc>{Z k,: =e T c } d+1 = hd+1 d+1 (hd + 1 = hd intra-class edges + 1 self-loop introduced by ?).When v is not in class c, we havek?V {Z k,: =e T c } d+1 = (1-h)d (C-1)(d+1) ((1 -h)d inter-class edges uniformly distributed in the other C -1 classes).For nodes v, u, we have ( ?Z) v,: , ( ?Z) u,: ? R C and since elements in ?v,k and ?u,k are independently generated for all k, k ? V, we haveE ( ?Z) v,c ( ?Z) u,c = E ( k?V ?v,k 1 {Z k,: =e T c } )( k ?V ?u,k 1 {Z k ,: =e T c } ) = E ( k?V ?v,k 1 {Z k,: =e T c } ) E ( k ?V ?u,k 1 {Z k ,: =e T c } )Thus,E S( ?, Z) v,u = E &lt; ( ?Z) v,: , ( ?Z) u,: &gt; = c E ( k?V ?v,k 1 {Z k,: =e T c } ) E ( k ?V ?u,k 1 {Z k ,: =e T c ((1-h)d) 2 (C-1)(d+1) 2 ,u, v are in the same class2(hd+1)(1-h)d (C-1)(d+1) 2 + (C-2)(1-h) 2 d 2 (C-1) 2(d+1) 2 , u, v are in different classes For nodes u 1 , u 2 , and v, where Z u1,: = Z v,: and Z u2,: = Z v,: , g(h) ? E S( ?, Z) v,u1 -E S( ?, Z) v,u2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>= (C - 1 ) 2 2 (C - 1 ) 2 (d + 1) 2 = (C - 1 )</head><label>1221221</label><figDesc>(hd + 1) 2 + (C -1) [(1 -h)d] 2 -(C -1) (2(hd + 1)(1 -h)d) -(C -2) [(1 -h)d] (hd + 1) -(1 -h)d (C -1)(d + 1)2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>F. 1 1 Meanuv?V 1 MeanuP( 1 ((</head><label>1111</label><figDesc>An Extension of Proposition 1Base on the definition of aggregation similarity, we haveS agg S( ?, Z) = v Mean u {S( ?, Z) v,u |Z u,: = Z v,: } ? Mean u {S( ?, Z) v,u |Z u,: = Z v,: } |V| = v?V {S( ?,Z)v,u|Zu,:=Zv,:} ?Meanu {S( ?,Z)v,u|Zu,: =Zv,:} |V| Then, E S agg S( ?, Z) = E ? ? {S( ?,Z)v,u|Zu,:=Zv,:} ?Meanu {S( ?,Z)v,u|Zu,: =Zv,:} Mean u {S( ?, Z) v,u |Z u,: = Z v,: } ? Mean u {S( ?, Z) v,u |Z u,: = Z v,: } |V| = P Mean u {S( ?, Z) v,u |Z u,: = Z v,: } -Mean u {S( ?, Z) v,u |Z u,: = Z v,: } ? 0 Consider the random variable RV = Mean u {S( ?, Z) v,u |Z u,: = Z v,: } -Mean u {S( ?, Z) v,u |Z u,: = Z v,: }Since RV is symmetrically distributed and under the conditions in proposition 1, its expectation is E[RV ] = g(h) as showed in equation 16. Since the minimum of g(h) is 0 and RV is symmetrically distributed, we have P(RV ? 0) ? 0.5 and this can explain why H agg (G) is always greater than 0.5 in many real-world tasks.G Proof of Theorem 1Proof. Define W c v = ( ?Z) v,c . Then,W c v = k?V ?v,k 1 {Z k,: =e T c } ? [0, 1], -?, Z) = (I -?)ZZ T (I -?) T = ZZ T + ?ZZ T ?T -?ZZ T -ZZ T ?T(19)For any node v, let the class v belongs to be denoted by c v . For two nodes v, u, if Z v,: = Z u,: , we have(ZZ T ) v,u = 0 ( ?ZZ T ?T ) v,u = ?ZZ T ) v,u = W cu v (ZZ T ?T ) v,u = ( ?ZZ T ) u,v = W cv uThen, from equation 19 it follows that(S(I -?, Z)) v,u = C c=1 W c v W c u -W cu v -W cv u When C = 2, S(I -?, Z) v,u = W cu v (W cu u -1) + W cv u (W cv v -1) ? 0 If Z v,: = Z u,: , i.e., c v = c u , we have (ZZ T ) v,u = ?ZZ T ?T ) v,u = ?ZZ T ) v,u = W cv v (ZZ T ?T ) v,u = ( ?ZZ T ) u,v = W cu u = W cv u Then, from equation 19 it follows that S(I -?, Z) v,u = 1 + C c=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Thus, if C = 2, for any v ? V, if Z u,: = Z v,: , we have S(I -?, Z) v,u ? 0; if Z u,: = Z v,:, we have S(I -?, Z) v,u ? 0. Apparently, the two conditions in equation 10 are satisfied. Thus v is diversification distinguishable and DD ?,X (G) = 1. The theorem is proved. H Discussion of the Limitations of Diversification Operation ' XX T (I -? ' )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Example of the case (the area in black box) that HP filter does not work well for harmful heterophily</figDesc><graphic url="image-425.png" coords="36,330.27,407.71,168.11,77.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation Study on Different Components in ACM-SGC and ACM-GCN (%) Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std ACM-SGC-1 w/ 70.98 ? 8.39 70.38 ? 2.85 83.28 ? 5.43 25.26 ? 1.18 64.86 ? 1.81 47.62 ? 1.27 85.12 ? 1.64 79.66 ? 0.75 85.5 ? 0.76 12.89 83.28 ? 5.81 91.88 ? 1.61 90.98 ? 2.46 36.76 ? 1.01 65.27 ? 1.9 47.27 ? 1.37 86.8 ? 1.08 80.98 ? 1.68 87.21 ? 0.42 10.44 93.93 ? 3.6 95.25 ? 1.84 93.93 ? 2.54 38.38 ? 1.13 63.83 ? 2.07 46.79 ? 0.75 86.73 ? 1.28 80.57 ? 0.99 87.8 ? 0.58 9.44 88.2 ? 4.39 93.5 ? 2.95 92.95 ? 2.94 37.19 ? 0.87 62.82 ? 1.84 44.94 ? 0.93 85.22 ? 1.35 80.75 ? 1.68 88.11 ? 0.21 11.00 93.77 ? 1.91 93.25 ? 2.92 93.61 ? 1.55 39.33 ? 1.25 63.68 ? 1.62 46.4 ? 1.13 86.63 ? 1.13 80.96 ? 0.93 87.75 ? 0.88 10.00 ACM-GCN w/ 82.46 ? 3.11 75.5 ? 2.92 83.11 ? 3.2 35.51 ? 0.99 64.18 ? 2.62 44.76 ? 1.39 87.78 ? 0.96 81.39 ? 1.23 88.9 ? 0.32 11.44 82.13 ? 2.59 86.62 ? 4.61 89.19 ? 3.04 38.06 ? 1.35 69.21 ? 1.68 57.2 ? 1.01 88.93 ? 1.55 81.96 ? 0.91 90.01 ? 0.8 7.22 94.26 ? 2.23 96.13 ? 2.2 94.1 ? 2.95 41.51 ? 0.99 67.44 ? 2.14 53.97 ? 1.39 88.95 ? 0.9 81.72 ? 1.22 90.88 ? 0.55 4.44 91.64 ? 2 95.37 ? 3.31 95.25 ? 2.37 40.47 ? 1.49 68.93 ? 2.04 54.78 ? 1.27 89.13 ? 1.77 81.96 ? 2.03 91.01 ? 0.7 3.11 94.75 ? 2.62 96.75 ? 1.6 95.08 ? 3.2 41.62 ? 1.15 69.04 ? 1.74 58.02 ? 1.86 88.95 ? 1.3 81.80 ? 1.26 90.69 ? 0.53 2.78 ACMII-GCN w/ 82.46 ? 3.03 91.00 ? 1.75 90.33 ? 2.69 38.39 ? 0.75 67.59 ? 2.14 53.67 ? 1.71 89.13 ? 1.14 81.75 ? 0.85 89.87 ? 0.39 7.44 94.26 ? 2.57 96.00 ? 2.15 94.26 ? 2.96 40.96 ? 1.2 66.35 ? 1.76 50.78 ? 2.07 89.06 ? 1.07 81.86 ? 1.22 90.71 ? 0.67 4.67 91.48 ? 1.43 96.25 ? 2.09 93.77 ? 2.91 40.27 ? 1.07 66.52 ? 2.65 52.9 ? 1.64 88.83 ? 1.16 81.54 ? 0.95 90.6 ? 0.47 6.67 95.9 ? 1.83 96.62 ? 2.44 95.25 ? 3.15 41.84 ? 1.15 68.38 ? 1.36 54.53 ? 2.09 89.00 ? 0.72 81.79 ? 0.95 90.74 ? 0.5 2.78</figDesc><table><row><cell>Baseline</cell><cell>Model Components</cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell>Models</cell><cell cols="6">LP HP Identity Mixing Acc ? Comparison of Average Running Time Per Epoch(ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2.53</cell><cell>2.83</cell><cell>2.5</cell><cell>3.18</cell><cell>3.48</cell><cell>4.65</cell><cell>3.47</cell><cell>3.43</cell><cell>4.04</cell></row><row><cell></cell><cell></cell><cell>4.01</cell><cell>4.57</cell><cell>4.24</cell><cell>4.55</cell><cell>4.76</cell><cell>5.09</cell><cell>5.39</cell><cell>4.69</cell><cell>4.75</cell></row><row><cell>ACM-SGC-1 w/</cell><cell></cell><cell>3.88 3.31</cell><cell>4.01 3.49</cell><cell>4.04 3.18</cell><cell>4.43 3.7</cell><cell>4.06 3.53</cell><cell>4.5 4.83</cell><cell>4.38 3.92</cell><cell>3.82 3.87</cell><cell>4.16 4.24</cell></row><row><cell></cell><cell></cell><cell>5.53</cell><cell>5.96</cell><cell>5.43</cell><cell>5.21</cell><cell>5.41</cell><cell>6.96</cell><cell>6</cell><cell>5.9</cell><cell>6.04</cell></row><row><cell></cell><cell></cell><cell>3.67</cell><cell>3.74</cell><cell>3.59</cell><cell>4.86</cell><cell>4.96</cell><cell>6.41</cell><cell>4.24</cell><cell>4.18</cell><cell>5.08</cell></row><row><cell></cell><cell></cell><cell>6.63</cell><cell>8.06</cell><cell>7.89</cell><cell>8.11</cell><cell>7.8</cell><cell>9.39</cell><cell>7.82</cell><cell>7.38</cell><cell>8.74</cell></row><row><cell>ACM-GCN w/</cell><cell></cell><cell>5.73 5.16</cell><cell>5.91 5.25</cell><cell>5.93 5.2</cell><cell>6.86 5.93</cell><cell>6.35 5.64</cell><cell>7.15 8.02</cell><cell>7.34 5.73</cell><cell>6.65 5.65</cell><cell>6.8 6.16</cell></row><row><cell></cell><cell></cell><cell>8.25</cell><cell>8.11</cell><cell>7.89</cell><cell>7.97</cell><cell>8.41</cell><cell>11.9</cell><cell>8.84</cell><cell>8.38</cell><cell>8.63</cell></row><row><cell></cell><cell></cell><cell>6.62</cell><cell>7.35</cell><cell>7.39</cell><cell>7.62</cell><cell>7.33</cell><cell>9.69</cell><cell>7.49</cell><cell>7.58</cell><cell>7.97</cell></row><row><cell></cell><cell></cell><cell>6.3</cell><cell>6.05</cell><cell>6.26</cell><cell>6.87</cell><cell>6.44</cell><cell>6.5</cell><cell>6.14</cell><cell>7.21</cell><cell>6.6</cell></row><row><cell>ACMII-GCN w/</cell><cell></cell><cell>5.24</cell><cell>5.27</cell><cell>5.46</cell><cell>5.72</cell><cell>5.65</cell><cell>7.87</cell><cell>5.48</cell><cell>5.65</cell><cell>6.33</cell></row><row><cell></cell><cell></cell><cell>7.59</cell><cell>8.28</cell><cell>8.06</cell><cell>8.85</cell><cell>8</cell><cell>10</cell><cell>8.27</cell><cell>8.5</cell><cell>8.68</cell></row></table><note><p><p><p><p><p>Ablation study on 9 real-world datasets</p><ref type="bibr" target="#b34">[35]</ref></p>. Cell with means the component is applied to the baseline model. The best test results are highlighted.</p>6.2 Comparison with Baseline and SOTA Models</p>Datasets &amp; Experimental Setup In this section, we evaluate SGC</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 0.70 93.87 ? 3.33 92.26 ? 0.71 38.58 ? 0.25 46.72 ? 0.46 31.28 ? 0.27 66.55 ? 0.72 76.44 ? 0.30 76.25 ? 0.28 86.43 ? 0.13 23.40 Snowball-2 95.08 ? 3.11 96.38 ? 2.59 95.74 ? 2.22 41.4 ? 1.23 68.51 ? 1.7 55.97 ? 2.03 OOM 88.83 ? 1.49 81.58 ? 1.23 90.81 ? 0.52 7.44 ACM-Snowball-3 94.26 ? 2.57 96.62 ? 1.86 94.75 ? 2.41 41.27 ? 0.8 68.4 ? 2.05 55.73 ? 2.39 OOM 89.59 ? 1.58 81.32 ? 0.97 91.44 ? 0.59 7.22 ACMII-GCN 95.9 ? 1.83 96.62 ? 2.44 95.08 ? 2.07 41.84 ? 1.15 68.38 ? 1.36 54.53 ? 2.09 67.15 ? 0.41 89.00 ? 0.72 81.79 ? 0.95 90.74 ? 0.5 5.90 ACMII-Snowball-2 95.25 ? 1.55 96.63 ? 2.24 95.25 ? 1.55 41.1 ? 0.75 67.83 ? 2.63 53.48 ? 0.6 OOM 88.95 ? 1.04 82.07 ? 1.04 90.56 ? 0.39 7.56 ACMII-Snowball-3 93.61 ? 2.79 97.00 ? 2.63 94.75 ? 3.09 40.31 ? 1.6 67.53 ? 2.83 52.31 ? 1.57 OOM 89.36 ? 1.26 81.56 ? 1.15 91.31 ? 0.6 9.00 ACMII-GCN+ 93.93 ? 3.03 96.75 ? 1.79 95.41 ? 2.82 41.5 ? 1.54 75.51 ? 1.58 69.81 ? 1.11 67.44 ? 0.31 89.18 ? 1.11 81.87 ? 1.38 90.96 ? 0.62 4.4 ACMII-GCN++ 92.62 ? 2.57 97.13 ? 1.68 94.75 ? 2.91 41.66 ? 1.42 75.93 ? 1.71 69.98 ? 1.53 67.5 ? 0.53 89.47 ? 1.08 81.76 ? 1.25 90.63 ? 0.56 5.10</figDesc><table><row><cell></cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Deezer-Europe</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell></cell></row><row><cell>#nodes</cell><cell>183</cell><cell>251</cell><cell>183</cell><cell>7,600</cell><cell>2,277</cell><cell>5,201</cell><cell>28,281</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell></cell></row><row><cell>#edges</cell><cell>295</cell><cell>499</cell><cell>309</cell><cell>33,544</cell><cell>36,101</cell><cell>217,073</cell><cell>92,752</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell><cell></cell></row><row><cell>#features</cell><cell>1,703</cell><cell>1,703</cell><cell>1,703</cell><cell>931</cell><cell>2,325</cell><cell>2,089</cell><cell>31,241</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell><cell></cell></row><row><cell>#classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell></cell></row><row><cell>H edge</cell><cell>0.5669</cell><cell>0.4480</cell><cell>0.4106</cell><cell>0.3750</cell><cell>0.2795</cell><cell>0.2416</cell><cell>0.5251</cell><cell>0.8100</cell><cell>0.7362</cell><cell>0.8024</cell><cell></cell></row><row><cell>H node</cell><cell>0.3855</cell><cell>0.1498</cell><cell>0.0968</cell><cell>0.2210</cell><cell>0.2470</cell><cell>0.2156</cell><cell>0.5299</cell><cell>0.8252</cell><cell>0.7175</cell><cell>0.7924</cell><cell></cell></row><row><cell>H class</cell><cell>0.0468</cell><cell>0.0941</cell><cell>0.0013</cell><cell>0.0110</cell><cell>0.0620</cell><cell>0.0254</cell><cell>0.0304</cell><cell>0.7657</cell><cell>0.6270</cell><cell>0.6641</cell><cell></cell></row><row><cell>Data Splits</cell><cell cols="10">60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 50%/25%/25% 60%/20%/20% 60%/20%/20% 60%/20%/20%</cell><cell></cell></row><row><cell>H M agg (G)</cell><cell>0.8032</cell><cell>0.7768</cell><cell>0.694</cell><cell>0.6822</cell><cell>0.61</cell><cell>0.3566</cell><cell>0.5790</cell><cell>0.9904</cell><cell>0.9826</cell><cell>0.9432</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Test Accuracy (%) of State-of-the-art Models, Baseline GNN Models and ACM-GNN models</cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell>MLP-2</cell><cell cols="11">91.30 20</cell></row><row><cell>BernNet</cell><cell>92.13 ? 1.64</cell><cell>NA</cell><cell cols="4">93.12 ? 0.65 41.79 ? 1.01 68.29 ? 1.58 51.35 ? 0.73</cell><cell>NA</cell><cell cols="4">88.52 ? 0.95 80.09 ? 0.79 88.48 ? 0.41 14.75</cell></row><row><cell>GraphSAGE</cell><cell cols="6">71.41 ? 1.24 64.85 ? 5.14 79.03 ? 1.20 36.37 ? 0.21 62.15 ? 0.42 41.26 ? 0.26</cell><cell>OOM</cell><cell cols="4">86.58 ? 0.26 78.24 ? 0.30 86.85 ? 0.11 25.78</cell></row><row><cell>Geom-GCN*</cell><cell>60.81</cell><cell>64.12</cell><cell>67.57</cell><cell>31.63</cell><cell>60.9</cell><cell>38.14</cell><cell>NA</cell><cell>85.27</cell><cell>77.99</cell><cell>90.05</cell><cell>27.44</cell></row><row><cell>SGC-1</cell><cell cols="11">70.98 ? 8.39 70.38 ? 2.85 83.28 ? 5.43 25.26 ? 1.18 64.86 ? 1.81 47.62 ? 1.27 59.73 ? 0.12 85.12 ? 1.64 79.66 ? 0.75 85.5 ? 0.76 24.90</cell></row><row><cell>SGC-2</cell><cell cols="11">72.62 ? 9.92 74.75 ? 2.89 81.31 ? 3.3 28.81 ? 1.11 62.67 ? 2.41 41.25 ? 1.4 61.56 ? 0.51 85.48 ? 1.48 80.75 ? 1.15 85.36 ? 0.52 25.40</cell></row><row><cell>GCNII</cell><cell cols="9">89.18 ? 3.96 83.25 ? 2.69 82.46 ? 4.58 40.82 ? 1.79 60.35 ? 2.7 38.81 ? 1.97 66.38 ? 0.45 88.98 ? 1.33 81.58 ? 1.3</cell><cell cols="2">89.8 ? 0.3 19.30</cell></row><row><cell>GCNII*</cell><cell cols="11">90.49 ? 4.45 89.12 ? 3.06 88.52 ? 3.02 41.54 ? 0.99 62.8 ? 2.87 38.31 ? 1.3 66.42 ? 0.56 88.93 ? 1.37 81.83 ? 1.78 89.98 ? 0.52 16.40</cell></row><row><cell>GCN</cell><cell cols="11">82.46 ? 3.11 75.5 ? 2.92 83.11 ? 3.2 35.51 ? 0.99 64.18 ? 2.62 44.76 ? 1.39 62.23 ? 0.53 87.78 ? 0.96 81.39 ? 1.23 88.9 ? 0.32 20.90</cell></row><row><cell>Snowball-2</cell><cell cols="6">82.62 ? 2.34 74.88 ? 3.42 83.11 ? 3.2 35.97 ? 0.66 64.99 ? 2.39 47.88 ? 1.23</cell><cell>OOM</cell><cell cols="4">88.64 ? 1.15 81.53 ? 1.71 89.04 ? 0.49 19.78</cell></row><row><cell>Snowball-3</cell><cell cols="6">82.95 ? 2.1 69.5 ? 5.01 83.11 ? 3.2 36.00 ? 1.36 65.49 ? 1.64 48.25 ? 0.94</cell><cell>OOM</cell><cell cols="4">89.33 ? 1.3 80.93 ? 1.32 88.8 ? 0.82 19.11</cell></row><row><cell>ACM-SGC-1</cell><cell cols="11">93.77 ? 1.91 93.25 ? 2.92 93.61 ? 1.55 39.33 ? 1.25 63.68 ? 1.62 46.4 ? 1.13 66.67 ? 0.56 86.63 ? 1.13 80.96 ? 0.93 87.75 ? 0.88 17.00</cell></row><row><cell>ACM-SGC-2</cell><cell>93.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>GAT 76.00 ? 1.01 71.01 ? 4.66 78.87 ? 0.86 35.98 ? 0.23 63.9 ? 0.46 42.72 ? 0.33 61.09 ? 0.77 76.70 ? 0.42 67.20 ? 0.46 83.28 ? 0.12 26.20 APPNP 91.80 ? 0.63 92.00 ? 3.59 91.18 ? 0.70 38.86 ? 0.24 51.91 ? 0.56 34.77 ? 0.34 67.21 ? 0.56 79.41 ? 0.38 68.59 ? 0.30 85.02 ? 0.09 22.80 GPRGNN 91.36 ? 0.70 93.75 ? 2.37 92.92 ? 0.61 39.30 ? 0.27 67.48 ? 0.40 49.93 ? 0.53 66.90 ? 0.50 79.51? 0.36 67.63 ? 0.38 85.07 ? 0.09 19.20 H2GCN 86.23 ? 4.71 87.5 ? 1.77 85.90 ? 3.53 38.85 ? 1.17 52.30 ? 0.48 30.39 ? 1.22 67.22 ? 0.90 87.52 ? 0.61 79.97 ? 0.69 87.78 ? 0.28 21.80 MixHop 60.33 ? 28.53 77.25 ? 7.80 76.39 ? 7.66 33.13 ? 2.40 36.28 ? 10.22 24.55 ? 2.60 66.80 ? 0.58 65.65 ? 11.31 49.52 ? 13.35 87.04 ? 4.10 28.30 GCN+JK 66.56 ? 13.82 62.50 ? 15.75 80.66 ? 1.91 32.72 ? 2.62 64.68 ? 2.85 53.40 ? 1.90 60.99 ? 0.14 86.90 ? 1.51 73.77 ? 1.85 90.09 ? 0.68 23.40 GAT+JK 74.43 ? 10.24 69.50 ? 3.12 75.41 ? 7.18 35.41 ? 0.97 68.14 ? 1.18 52.28 ? 3.61 59.66 ? 0.92 89.52 ? 0.43 74.49 ? 2.76 89.15 ? 0.87 20.90 FAGCN 88.03 ? 5.6 89.75 ? 6.37 88.85 ? 4.39 31.59 ? 1.37 49.47 ? 2.84 42.24 ? 1.2 66.86 p, 0.53 88.85 ? 1.36 82.37 ? 1.46 89.98 ? 0.54 18.77 ? 2.17 94.00 ? 2.61 93.44 ? 2.54 40.13 ? 1.21 60.48 ? 1.55 40.91 ? 1.39 66.53 ? 0.57 87.64 ? 0.99 80.93 ? 1.16 88.79 ? 0.5 17.70 ACM-GCNII 92.62 ? 3.13 94.63 ? 2.96 92.46 ? 1.97 41.37 ? 1.37 58.73 ? 2.52 40.9 ? 1.58 66.39 ? 0.56 89.1 ? 1.61 82.28 ? 1.12 90.12 ? 0.4 14.30 ACM-GCNII* 93.44 ? 2.74 94.37 ? 2.81 93.28 ? 2.79 41.27 ? 1.24 61.66 ? 2.29 38.32 ? 1.5 66.6 ? 0.57 89.00 ? 1.35 81.69 ? 1.25 90.18 ? 0.51 14.20 ACM-GCN 94.75 ? 3.8 95.75 ? 2.03 94.92 ? 2.88 41.62 ? 1.15 69.04 ? 1.74 58.02 ? 1.86 67.01 ? 0.38 88.62 ? 1.22 81.68 ? 0.97 90.66 ? 0.47 7.90 ACM-GCN+ 94.92 ? 2.79 96.5 ? 2.08 94.92 ? 2.79 41.79 ? 1.01 76.08 ? 2.13 69.26 ? 1.11 67.4 ? 0.44 89.75 ? 1.16 81.65 ? 1.48 90.46 ? 0.69 4.90 ACM-GCN++ 93.93 ? 1.05 97.5 ? 1.25 96.56 ? 2 41.86 ? 1.48 75.23 ? 1.72 68.56 ? 1.33 67.3 ? 0.48 89.33 ? 0.81 81.83 ? 1.65 90.39 ? 0.33 4.30 ACM-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results: average test accuracy ? standard deviation on 10 real-world benchmark datasets. The best results are highlighted in grey and the best baseline results (SOTA in Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>14 the optimal searched hyperparameters. The results and comparison give us the same conclusion as in Appendix A.1. ? 6.63 86.86 ? 3.29 84.86 ? 4.55 37.54 ? 1.56 71.14 ?1.84 55.17 ? 1.58 87.95 ? 1.05 77.14 ? 1.45 89.15 ? 0.37 10.22 LINKX 77.84 ? 5.81 75.49 ? 5.72 74.60 ? 8.37 36.10 ? 1.55 68.42 ? 1.38 61.81 ? 1.80 84.64 ? 1.13 73.19 ? 0.99 87.86 ? 0.77 18.78 GloGNN 83.51 ? 4.26 87.06 ? 3.53 84.32 ? 4.15 37.35 ? 1.30 69.78 ? 2.42 57.54 ? 1.39 88.31 ? 1.13 77.41 ? 1.65 89.62 ? 0.35 8.78 GloGNN++ 85.95 ? 5.10 88.04 ? 3.22 84.05 ? 4.90 37.70 ? 1.40 71.21 ? 1.84 57.88 ? 1.76 88.33 ? 1.09 77.22 ? 1.78 89.24 ? 0.39 7.33 ACM-SGC-1 82.43 ? 5.44 86.47 ? 3.77 81.89 ? 4.53 35.49 ? 1.06 63.99 ? 1.66 45.00 ? 1.4 86.9 ? 1.38 76.73 ? 1.59 88.49 ? 0.51 17.56 ACM-SGC-2 82.43 ? 5.44 86.47 ? 3.77 81.89 ? 4.53 36.04 ? 0.83 59.21 ? 2.22 40.02 ? 0.96 87.69 ? 1.07 76.59 ? 1.69 89.01 ? 0.6 17.67 Diag-NSD 86.49 ? 7.35 88.63 ? 2.75 85.67 ? 6.95 37.79 ? 1.01 68.68 ? 1.73 54.78 ? 1.81 87.14 ? 1.06 77.14 ? 1.85 89.42 ? 0.43 9.00 O(d)-NSD 84.86 ? 4.71 89.41 ? 4.74 85.95 ? 5.51 37.81 ? 1.15 68.04 ? 1.58 56.34 ? 1.32 86.90 ? 1.13 76.70 ? 1.57 89.49 ? 0.40 10.44 Gen-NSD 85.68 ? 6.51 89.21 ? 3.84 82.97 ? 5.13 37.80 ? 1.22 67.93 ? 1.58 53.17 ? 1.31 87.30 ? 1.15 76.32 ? 1.65 89.33 ? 0.35 ? 6.07 88.43 ? 3.22 87.84 ? 4.4 36.63 ? 0.84 69.14 ? 1.91 55.19 ? 1.49 87.91 ? 0.95 77.32 ? 1.7 90.00 ? 0.52 8.11 ACMII-GCN 85.95 ? 5.64 87.45 ? 3.74 86.76 ? 4.75 36.31 ? 1.2 68.46 ? 1.7 51.8 ? 1.5 88.01 ? 1.08 77.15 ? 1.45 89.89 ? 0.43 9.33 ACM-GCN+ 85.68 ? 4.84 88.43 ? 2.39 88.38 ? 3.64 36.26 ? 1.34 74.47 ? 1.84 66.98 ? 1.71 88.05 ? 0.99 77.67 ? 1.19 89.82 ? 0.41 5.33 ACMII-GCN+ 85.41 ? 5.3 88.04 ? 3.66 88.11 ? 3.24 36.14 ? 1.44 74.56 ? 2.08 67.07 ? 1.65 88.19 ? 1.17 77.2 ? 1.61 89.78 ? 0.49 6.78 ACM-GCN++ 85.68 ? 5.8 88.24 ? 3.16 88.38 ? 3.43 37.31 ? 1.09 74.41 ? 1.49 67.06 ? 1.66 88.11 ? 0.96 77.46 ? 1.65 89.65 ? 0.58 5.33 ACMII-GCN++ 86.49 ? 6.73 88.43 ? 3.66 88.38 ? 3.43 37.09 ? 1.32 74.76 ? 2.2 67.4 ? 2.21 88.25 ? 0.96 77.12 ? 1.58 89.71 ? 0.48 4.78</figDesc><table><row><cell>Datasets/Models</cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell><cell>Average Rank</cell></row><row><cell>Geom-GCN</cell><cell cols="9">60.54 ? 3.67 64.51 ? 3.66 66.76 ? 2.72 31.59 ? 1.15 60.00 ? 2.81 38.15 ? 0.92 85.35 ? 1.57 78.02 ? 1.15 89.95 ? 0.47</cell><cell>18.22</cell></row><row><cell>H2GCN</cell><cell cols="9">82.70 ? 5.28 87.65 ? 4.98 84.86 ? 7.23 35.70 ? 1.00 60.11 ? 2.15 36.48 ? 1.86 87.87 ? 1.20 77.11 ? 1.57 89.49 ? 0.38</cell><cell>15.11</cell></row><row><cell>GPRGCN</cell><cell cols="9">78.11 ? 6.55 82.55 ? 6.23 81.35 ? 5.32 35.16 ? 0.9 62.59 ? 2.04 46.31 ? 2.46 87.95 ? 1.18 77.13 ? 1.67 87.54 ? 0.38</cell><cell>17.67</cell></row><row><cell>FAGCN</cell><cell cols="9">76.76 ? 5.87 79.61 ? 1.58 76.49 ? 2.87 34.82 ? 1.35 46.07 ? 2.11 30.83 ? 0.69 88.05 ? 1.57 77.07 ? 2.05 88.09 ? 1.38</cell><cell>20.00</cell></row><row><cell>GCNII</cell><cell cols="9">77.86 ? 3.79 80.39 ? 3.40 77.57 ? 3.83 37.44 ? 1.30 63.86 ? 3.04 38.47 ? 1.58 88.37 ? 1.25 77.33 ? 1.48 90.15 ? 0.43</cell><cell>12.44</cell></row><row><cell>MixHop</cell><cell cols="9">73.51 ? 6.34 75.88 ? 4.90 77.84 ? 7.73 32.22 ? 2.34 60.50 ? 2.53 43.80 ? 1.48 87.61 ? 0.85 76.26 ?1.33 85.31 ? 0.61</cell><cell>20.78</cell></row><row><cell>WRGAT</cell><cell cols="9">81.62 ?3.90 86.98 ? 3.78 83.62 ? 5.50 36.53 ? 0.77 65.24 ? 0.87 48.85 ? 0.78 88.20 ? 2.26 76.81 ? 1.89 88.52 ? 0.92</cell><cell>14.33</cell></row><row><cell>GGCN</cell><cell cols="10">85.68 11.67</cell></row><row><cell>NLMLP</cell><cell>84.9 ? 5.7</cell><cell>87.3 ? 4.3</cell><cell>85.4 ? 3.8</cell><cell>37.9 ? 1.3</cell><cell>50.7 ? 2.2</cell><cell>33.7 ? 1.5</cell><cell>76.9 ? 1.8</cell><cell>73.4 ? 1.9</cell><cell>88.2 ? 0.5</cell><cell>16.67</cell></row><row><cell>NLGCN</cell><cell>57.6 ? 5.5</cell><cell>60.2 ? 5.3</cell><cell>65.5 ? 6.6</cell><cell>31.6 ? 1.0</cell><cell>70.1 ? 2.9</cell><cell>59.0 ? 1.2</cell><cell>88.1 ? 1.0</cell><cell>75.2 ? 1.4</cell><cell>89.0 ? 0.5</cell><cell>17.44</cell></row><row><cell>NLGAT</cell><cell>54.7 ? 7.6</cell><cell>56.9 ? 7.3</cell><cell>62.6 ? 7.1</cell><cell>29.5 ? 1.3</cell><cell>65.7 ? 1.4</cell><cell>56.8 ? 2.5</cell><cell>88.5 ? 1.8</cell><cell>76.2 ? 1.6</cell><cell>88.2 ? 0.3</cell><cell>18.56</cell></row><row><cell>ACM-GCN</cell><cell>85.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">A.3 Discussion of Random Walk and Symmetric Renormalized Filters</cell><cell></cell></row><row><cell></cell><cell>RW</cell><cell></cell><cell cols="2">Symmetric</cell></row><row><cell>Datasets/Models</cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell cols="3">95.9 ? 1.83 94.92 ? 2.48 94.1 ? 2.56</cell></row><row><cell>Wisconsin</cell><cell cols="4">95.75 ? 2.03 96.62 ? 2.44 95.63 ? 2.81 96.25 ? 2.5</cell></row><row><cell>Texas</cell><cell cols="4">94.92 ? 2.88 95.08 ? 2.07 94.75 ? 2.01 94.59 ? 2.65</cell></row><row><cell>Film</cell><cell cols="3">41.62 ? 1.15 41.84 ? 1.15 41.58 ? 1.3</cell><cell>41.65 ? 0.6</cell></row><row><cell>Chameleon</cell><cell cols="4">69.04 ? 1.74 68.38 ? 1.36 67.9 ? 2.76 68.03 ? 1.68</cell></row><row><cell>Squirrel</cell><cell cols="4">58.02 ? 1.86 54.53 ? 2.09 54.18 ? 1.35 53.68 ? 1.74</cell></row><row><cell>Cora</cell><cell cols="4">88.62 ? 1.22 89.00 ? 0.72 88.65 ? 1.26 88.19 ? 1.38</cell></row><row><cell>Citeseer</cell><cell cols="4">81.68 ? 0.97 81.79 ? 0.95 81.84 ? 1.15 81.81 ? 0.86</cell></row><row><cell>PubMed</cell><cell cols="4">90.66 ? 0.47 90.74 ? 0.5 90.59 ? 0.81 90.54 ? 0.59</cell></row></table><note><p><p><p><p><p>Experimental results on fixed splits provided by</p><ref type="bibr" target="#b34">[35]</ref></p>: average test accuracy ? standard deviation on 9 real-world benchmark datasets. The best results are highlighted. Results of Geom-GCN, H 2 GCN and GPRGNN, LINX, GloGNN, GloGNN++, Diag-NSD, O(d)-NSD, Gen-NSD, NLMLP, NLGCN and NLGAT are from</p><ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref></p>; results on the rest models are run by ourselves and the hyperparameter searching range is the same as table 9.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of random walk and symmetric renormalized filters</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of W mix A.4 Ablation Study of W mix</figDesc><table><row><cell></cell><cell></cell><cell>W mix</cell><cell cols="2">Without W mix</cell></row><row><cell></cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell cols="3">95.9 ? 1.83 93.61 ? 2.37 90.49 ? 2.72</cell></row><row><cell>Wisconsin</cell><cell cols="2">95.75 ? 2.03 96.62 ? 2.44</cell><cell>95 ? 2.5</cell><cell>97.50 ? 1.25</cell></row><row><cell>Texas</cell><cell cols="4">94.92 ? 2.88 95.08 ? 2.07 94.92 ? 2.79 94.92 ? 2.79</cell></row><row><cell>Film</cell><cell cols="4">41.62 ? 1.15 41.84 ? 1.15 40.79 ? 1.01 40.86 ? 1.48</cell></row><row><cell>Chameleon</cell><cell cols="4">69.04 ? 1.74 68.38 ? 1.36 68.16 ? 1.79 66.78 ? 2.79</cell></row><row><cell>Squirrel</cell><cell cols="4">58.02 ? 1.86 54.53 ? 2.09 55.35 ? 1.72 52.98 ? 1.66</cell></row><row><cell>Cora</cell><cell cols="4">88.62 ? 1.22 89.00 ? 0.72 88.41 ? 1.63 88.72 ? 1.5</cell></row><row><cell>Citeseer</cell><cell cols="4">81.68 ? 0.97 81.79 ? 0.95 81.65 ? 1.48 81.72 ? 1.58</cell></row><row><cell>PubMed</cell><cell cols="4">90.66 ? 0.47 90.74 ? 0.5 90.46 ? 0.69 90.39 ? 1.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>5 Learn Weights with Raw Features v.s. Combined Features Performance comparison between raw features and combined features</figDesc><table><row><cell></cell><cell cols="2">With Raw Features</cell><cell cols="2">With Combined Features</cell></row><row><cell>Datasets/Models</cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell>95.9 ? 1.83</cell><cell cols="2">95.08 ? 2.64 93.93 ? 3.52</cell></row><row><cell>Wisconsin</cell><cell cols="3">95.75 ? 2.03 96.62 ? 2.44 96.12 ? 1.31</cell><cell>96 ? 2</cell></row><row><cell>Texas</cell><cell cols="4">94.92 ? 2.88 95.08 ? 2.07 94.92 ? 2.48 94.59 ? 2.94</cell></row><row><cell>Film</cell><cell cols="4">41.62 ? 1.15 41.84 ? 1.15 41.62 ? 1.34 41.44 ? 1.18</cell></row><row><cell>Chameleon</cell><cell cols="4">69.04 ? 1.74 68.38 ? 1.36 68.82 ? 2.18 68.53 ? 3.08</cell></row><row><cell>Squirrel</cell><cell cols="4">58.02 ? 1.86 54.53 ? 2.09 57.48 ? 1.68 53.28 ? 1.08</cell></row><row><cell>Cora</cell><cell cols="4">88.62 ? 1.22 89.00 ? 0.72 88.59 ? 1.04 88.75 ? 0.83</cell></row><row><cell>Citeseer</cell><cell cols="2">81.68 ? 0.97 81.79 ? 0.95</cell><cell>81.9 ? 1.27</cell><cell>81.76 ? 1.05</cell></row><row><cell>PubMed</cell><cell cols="4">90.66 ? 0.47 90.74 ? 0.5 90.75 ? 0.77 90.58 ? 0.64</cell></row><row><cell cols="5">Construct the combined feature H l Comb = [H l L , H l H , H l I ], Replace the first line in Step 2 by the</cell></row><row><cell>following lines:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>get H l L , H l H , H l I with the same step as ACM-GCN and ACMII-GCN. Step 2. Row-wise Feature-based Weight Learning with Layer Normalization (LN)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>table 2 in Appendix A.1. The results on fixed 48%/32%/20% splits are reported in table 3 in Appendix A.2. Hyperparameter searching range for training on real-world datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>lr</cell><cell>weight_decay</cell><cell>dropout</cell><cell>hidden</cell><cell>lambda</cell><cell>alpha_l</cell><cell>head</cell><cell>layers</cell><cell>JK type</cell></row><row><cell cols="2">H2GCN</cell><cell></cell><cell>0.01</cell><cell>0.001</cell><cell>{0, 0.5}</cell><cell>{8, 16, 32, 64}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{1, 2}</cell><cell>-</cell></row><row><cell cols="2">MixHop</cell><cell></cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>{8, 16, 32}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{2, 3}</cell><cell>-</cell></row><row><cell cols="2">GCN+JK</cell><cell></cell><cell>{0.1, 0.01,</cell><cell>0.001</cell><cell>0.5</cell><cell>{4, 8, 16, 32, 64}</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>{max, cat}</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.001}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GAT+JK</cell><cell></cell><cell>{0.1, 0.01,</cell><cell>0.001</cell><cell>0.5</cell><cell>{4, 8, 12, 32}</cell><cell>-</cell><cell>-</cell><cell>{2,4,8}</cell><cell>2</cell><cell>{max, cat}</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.001}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GCNII, GCNII*</cell><cell></cell><cell>0.01</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>0.5</cell><cell>64</cell><cell>{0.5, 1, 1.5}</cell><cell cols="2">{0.1,0.2,0.3,0,4,0.5} -</cell><cell>{4, 8, 16, 32}</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for Deezer-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>{4, 8, 16,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32, 64} for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>others</cell></row><row><cell cols="3">Baselines: {SGC-1, SGC-</cell><cell>{0.002,</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{0, 0.1,</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2,</cell><cell cols="2">GCN, Snowball-2,</cell><cell>0.01,</cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Snowball-3,</cell><cell>FAGCN};</cell><cell>0.05} for</cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ACM-{SGC-1,</cell><cell>SGC-2,</cell><cell>Deezer-</cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">GCN, GCN+, GCN++,</cell><cell>Europe</cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Snowball-2, Snowball-3};</cell><cell>and {0.01,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ACMII-{SGC-1, SGC-2,</cell><cell>0.05, 0.1}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">GCN, GCN+, GCN++,</cell><cell>for others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Snowball-2, Snowball-3}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GraphSAGE</cell><cell></cell><cell>{0.01,0.05,</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{ 0, 0.1,</cell><cell>8 for Deezer-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.1}</cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell>Europe and 64</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell>for others</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ACM-{GCNII, GCNII*}</cell><cell>0.01</cell><cell>{0, 5e-6, 1e-5, 5e-5, 1e-</cell><cell>{ 0, 0.1,</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>{1,2,3,4}</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4, 5e-4, 1e-3} for Deezer-</cell><cell>0.2, 0.3,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Europe and {0, 5e-6, 1e-</cell><cell>0.4, 0.5,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5, 5e-5, 1e-4, 5e-4, 1e-3,</cell><cell>0.6, 0.7,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5e-3, 1e-2} for others</cell><cell>0.8, 0.9}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>T</figDesc><table><row><cell>.50</cell><cell>.50</cell><cell>.25</cell><cell>-.50</cell><cell>-.25</cell><cell>.00</cell><cell>-.25</cell><cell>.25</cell><cell>.13</cell><cell>.00</cell></row><row><cell>.50</cell><cell>.67</cell><cell>.50</cell><cell>-.50</cell><cell>-.17</cell><cell>-.17</cell><cell>-.50</cell><cell>.17</cell><cell>.17</cell><cell>.11</cell></row><row><cell>.25</cell><cell>.50</cell><cell>.50</cell><cell>-.25</cell><cell>.00</cell><cell>-.25</cell><cell>-.50</cell><cell>.00</cell><cell>.13</cell><cell>.17</cell></row><row><cell>-.50</cell><cell>-.50</cell><cell>-.25</cell><cell>.50</cell><cell>.25</cell><cell>.00</cell><cell>.25</cell><cell>-.25</cell><cell>-.13</cell><cell>.00</cell></row><row><cell>-.25</cell><cell>-.17</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.25</cell><cell>.00</cell><cell>-.50</cell><cell>-.38</cell><cell>-.17</cell></row><row><cell>.00</cell><cell>-.17</cell><cell>-.25</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.25</cell><cell>-.25</cell><cell>-.38</cell><cell>-.33</cell></row><row><cell>-.25</cell><cell>-.50</cell><cell>-.50</cell><cell>.25</cell><cell>.00</cell><cell>.25</cell><cell>.50</cell><cell>.00</cell><cell>-.13</cell><cell>-.17</cell></row><row><cell>.25</cell><cell>.17</cell><cell>.00</cell><cell>-.25</cell><cell>-.50</cell><cell>-.25</cell><cell>.00</cell><cell>.50</cell><cell>.38</cell><cell>.17</cell></row><row><cell>.13</cell><cell>.17</cell><cell>.13</cell><cell>-.13</cell><cell>-.38</cell><cell>-.38</cell><cell>-.13</cell><cell>.38</cell><cell>.38</cell><cell>.25</cell></row><row><cell>.00</cell><cell>.11</cell><cell>.17</cell><cell>.00</cell><cell>-.17</cell><cell>-.33</cell><cell>-.17</cell><cell>.17</cell><cell>.25</cell><cell>.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p><ref type="bibr" target="#b25">[26]</ref> did not name this homophily metric. We named it class homophily based on its definition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b31">[32]</ref> use the same example but not to demonstrate the deficiency of homophily metrics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p><ref type="bibr" target="#b7">[8]</ref> also point out the insufficiency of Hnode by examples to show that different graph typologies with the same Hnode(G) can carry different label information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>See Appendix F.1 for an intuitive explanation under certain conditions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In practice, we will only check Hagg(G) when H M agg (G) = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>See Appendix C.1 for a description of the hyperparameter searching range and Appendix D for more a detailed description of the data generation process</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>A similar J-shaped curve for Hedge(G) is found in<ref type="bibr" target="#b44">[45]</ref>, though using different data generation processes. The authors do not mention the insufficiency of edge homophily.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>In graph signal processing, an additional synthesis filter<ref type="bibr" target="#b10">[11]</ref> is required to form the 2-channel filterbank. But a synthesis filter is not needed in our framework.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>See more variants in Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>See Appendix A.4 and A.5 for more discussion of the components in ACM architecture.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>We only test ACM-SGC-1 because SGC-1 does not contain any non-linearity which makes ACM-SGC-1 and ACMII-SGC-1 exactly the same.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>GCNII and GCNII * are hard to implement with the ACMII framework. See Appendix B for explanation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>See Appendix A.3 for the comparison of ?rw and ?sym.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>We choose Deezer-Europe because MLP outperforms GCN on it<ref type="bibr" target="#b25">[26]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14"><p>See table 3 in Appendix A.2 for the performance comparison with several SOTA models, e.g., LINKX<ref type="bibr" target="#b24">[25]</ref> and GloGNN<ref type="bibr" target="#b23">[24]</ref>, on the fixed 48%/32%/20% splits provided by<ref type="bibr" target="#b34">[35]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_15"><p>AcknowledgeThe authors would like to give very special thanks to William L. Hamilton for valuable discussion and advice. The project was partially supported by DeepMind and NSERC.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_16"><p>The splits for all these experiments are random 60%/20%/20% splits for train/valid/test. The open source code we use is from https://github.com/jianhao2016/GPRGNN/blob/ f4aaad6ca28c83d3121338a4c4fe5d162edfa9a2/src/utils.py#L16. See table 3 in Appendix A.2 for the performance comparison with several SOTA models on the fixed 48%/32%/20% splits provided by<ref type="bibr" target="#b34">[35]</ref>.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computing Resources For all experiments on synthetic datasets and real-world datasets, we use NVIDIA V100 GPUs with 16/32GB GPU memory, 8-core CPU, 16G Memory. The software implementation is based on PyTorch and PyTorch Geometric <ref type="bibr" target="#b11">[12]</ref>. 0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} --ACM-SGC-1 0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9} -MLP-2 0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9} 64 GCN 0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9} 64 ACM-GCN 0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9} 64 SGC-LP+HP {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} --SGC-LP+Identity {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} --ACM-SGC-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} -GCN-LP+HP {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 GCN-LP+Identity {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 ACM-GCN-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64  <ref type="table">9</ref> for the hyperparameter seaching range of baseline GNNs, ACM-GNNs, ACMII-GNNs and several SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameter Searching Range &amp; Optimal Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Searched Optimal Hyperparameters for Baselines and ACM(II)-GNNs on Real-world Tasks</head><p>See the reported optimal hyperparameters on random 60%/20%/20% splits for baseline GNNs in table <ref type="table">10</ref>, for ACM-GNNs and ACMII-GNNs in table <ref type="bibr" target="#b10">11</ref> and for ACM(II)-GCN+ and ACM(II)-GCN++ in table <ref type="table">12</ref>.</p><p>See the reported optimal hyperparameters on fixed 48%/32%/20% splits for ACM(II)-GNNs and FAGCN in table <ref type="bibr" target="#b12">13</ref> and for ACM(II)-GCN+ and ACM(II)-GCN++ in table <ref type="table">14</ref>. For each generated graph, we calculate their H node , H class , H M agg . Then, we reorder the value of the metrics in ascend order for x-axis and plot the corresponding test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters for Baseline GNNs</head><note type="other">Datasets</note><p>Here is a simplified example of how we draw Figure <ref type="figure">2</ref>. Suppose we generate 3 graphs with H edge = 0.1, 0.5, 0.9, the test accuracy of GCN on these 3 synthetic graphs are 0.8, 0.5, 0.9. For the generated graphs, we calculate their H M agg , and suppose we get H M agg = 0.7, 0.4, 0.8. Then we will draw the performance of GCN under H M agg with ascend x-axis order [0.4, 0.7, 0.8] and the corresponding reordered y-axis is [0.5, 0.8, 0.9]. Other figures are drawn with the same process.   In order to separate the effects of nonlinearity and graph structure, we compare SGC with 1 hop (sgc-1) with MLP-1 (linear model). For GCN which includes nonlinearity, we use MLP-2 as its corresponding graph-agnostic baseline model. We train the above GNN models, graph-agnostic baseline models and ACM-GNN models on all synthetic datasets and plot the mean test accuracy with standard deviation on each dataset. From Figure <ref type="figure">10</ref> and Figure <ref type="figure">11</ref>, we can see that on each H M agg (G) level, ACM-GNNs will not underperform baseline GNNs and the graph-agnostic models. But when H M agg (G) is small, baseline GNNs will be outperformed by graph-agnostic models by a large margin. This demonstrate that the ACM framework can help GNNs to perform well on harmful graphs while keep competitive on less harmful graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Further Discussion of Aggregation Homophily on Regular Graphs</head><p>We notice that in Figure <ref type="figure">2</ref>(a), the performance of SGC-1 and GCN both have a turning point, i.e., when H edge (G) is smaller than a certain value, the performance will get better instead of getting worse. With some extra restriction on node degree in data generation process, we find that this interesting phenomenon can be theoretically explained by the following proposition 1 based on our proposed similarity matrix which can verify the usefulness of H M agg (G). We first generate regular graphs ,i.e., each node has the same degree, as follows, Generate Synthetic Regular Graphs We first generate 180 graphs in total with 18 edge homophily levels varied from 0.05 to 0.9, each corresponding to 10 graphs. For every generated graph, we have 5 classes with 400 nodes in each class. For each node, we randomly generate 10 intra-class edges and [ 10 Hedge(G) -10] inter-class edges. The features of nodes in each class are sampled from node features in the corresponding class of the base dataset. Nodes are randomly split into 60%/20%/20% for train/validation/test. We train 1-hop SGC (sgc-1) <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on synthetic data (see Appendix C.1 for hyperparameter searching range). For each value of H edge (G), we take the average test accuracy and standard deviation of runs over 10 generated graphs. We plot the performance curves in Figure <ref type="figure">12</ref>.</p><p>From Figure <ref type="figure">12</ref> we can see that the turning point is a bit less than 0.2. We derive the following proposition for d-regular graph to explain and predict it. aggregated-feature-label consistency. With the similarity score of the features S agg (S(I, X)) and aggregated features S agg S( ?, X) listed in Table <ref type="table">15</ref>, our methods open up a new perspective on analyzing and comparing the performance of graph-agnostic models and graph-aware models in real-world tasks. Here are 2 examples.</p><p>Example 1: People observe that GCN (graph-aware model) underperforms MLP-2 (graph-agnostic model) on Cornell, Wisconsin, Texas, Film and people commonly believe that the bad graph structure (low H edge , H node , H class values) is the reason for performance degradation. But based on the high aggregation homophily values, the graph structure inconsistency is not the main cause of the performance degradation. And from Table <ref type="table">15</ref> we can see that the S agg S( ?, X) for those 4 datasets are lower than their corresponding S agg (S(I, X)), which implies that it is the aggregated-feature-label inconsistency that causes the performance degradation, i.e. the aggregation step actually decrease the quality of node features rather than making them more distinguishable.</p><p>For the rest 5 datasets Chameleon, Squirrel, Cora, Citeseer, PubMed, we all have S agg S( ?, X) larger than S agg (S(I, X)) except PubMed, the aggregated features have higher quality than raw features. We can see that the proposed metrics are much more instructive than the existing ones.</p><p>Example 2: According to edge , H node , H class , the value for Chameleon, and Squirrel are extremely low indicating graph structure are bad for GNNs. But on contrary, GCN outperforms MLP-2 on those 2 datasets. Traditional homophily metrics fail to explain such phenomenon but our method can give an explanation from different angles: For Chameleon, its modified aggregation homophily is not low and its S agg S( ?, X) is higher than its S agg (S(I, X)), which means its graph-label consistency together with aggregated-feature-label consistency help the graph-aware model obtain the performance gain; for Squirrel, its modified aggregation homophily is low but its S agg S( ?, X) is higher than its S agg (S(I, X)), which means although its graph-label consistency is bad, the aggregated-feature-label consistency is the key factor to help the graph-aware model perform better.</p><p>We also need to point out that (modified) aggregation similarity score, S agg S( ?, X) and S agg (S(I, X)) are not deciding values because they do not consider the nonlinear structure in the features. In practice, a low score does not tell us the GNN models will definitely perform bad. Analysis From the reported results we can see that the estimations are accurate and the errors are within the acceptable range, which means the proposed metrics and similarity scores can be accurately estimated with a subset of labels and this is important for real-world applications.</p><p>J A Detailed Explanation of the Differences Between ACM(II)-GNNs and GPRGNN, FAGCN Differences with GPRGNN [8]:</p><p>? GPRGNN does not feed distinct node-wise feature transformation to different "multi-scale channels"</p><p>We first rewrite GPRGNN as</p><p>diag(? k , ? k , . . . , ? k )H (k) , where H (k) = ?sym H (k-1) , H</p><p>i: = f ? (X i: ).</p><p>From the above equation we can see that Z = K k=0 ? k ?k sym f ? (X i: ), i.e., the node-wise feature transformation in GPRGNN is only learned by the same ? for all the "multi-scale channels". But in the ACM framework, different channels extract distinct information with different parameters separately.</p><p>? GPRGNN does not have node-wise mixing mechanism.</p><p>There is no node-wise mixing in GPRGNN. The mixing mechanism in GPRGNN is Z = K k=0 diag(? k , ? k , . . . , ? k )H (k) , i.e. for each "multi-scale channel k", all nodes share the same mixing parameter ? k . But in the ACM framework, the node-wise channel mixing can be written as Z = K k=0 diag(? 1 k , ? 2 k , . . . , ? N k )H (k) where K is the number of channels, N is the number of nodes and ? i k , i = 1, . . . , N are the mixing weights that are learned by node i to mix channel k. ACM and ACMII allow GNNs to learn more diverse mixing parameters in diagonal than GPRGNN and thus, have stronger expressive power than GPRGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences with FAGCN [4]:</head><p>? The targets of node-wise operations in ACM (channel mixing) and FAGCN (negative message passing) are different.</p><p>Instead of using a fixed low-pass filter ?, FAGCN tries to learn a more powerful aggregator ? based on ? by allowing negative message passing. The node-wise operation in FAGCN is similar to GAT <ref type="bibr" target="#b2">[3]</ref> which is trying to modify the node-wise filtering (message passing) process, i.e. for each node i, it assigns different weights ? ij ? [-1, 1] to different neighborhood nodes (equation 7 in FAGCN paper). The goal of this node-wise operation in FAGCN is to learn a new filter during the filtering process node-wisely. But in ACM, the nodewise operation is to mix the filtered information from each channel which is processed by different fixed filters. The targets of two the node-wise operations are actually different things.</p><p>? FAGCN does not learn distinct information from different "channels". FAGCN only uses simple addition to mix information instead of node-wise channel mixing mechanism The learned filter ? can be decomposed as ? = ? 1 +(-? 2 ), where ? 1 and -? 2 represent positive and negative edge (propagation) information, respectively. But FAGCN does not feed distinct information to ? 1 and -? 2 . Moreover, the aggregated ? 1 X and "diversified" information (-?</p><p>2 )X are simply added together instead of using any node-wise mixing mechanism. In ACM, we learn distinct information separately in each channel with different parameters and add them adaptively and node-wisely instead of just adding them together. In section 6.1, the ablation study has empirically shown that node-wise adaptive channel mixing is better than simple addition.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<title level="m">Beyond low-frequency information in graph convolutional networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04579</idno>
		<title level="m">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>arXiv, abs/1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>arXiv, abs/1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph structured data viewed through a fourier lens</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Ekambaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arXiv, abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bernnet: Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07308</idno>
		<title level="m">Finding global homophily in graph neural networks when meeting heterophily</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Simple truncated svd based model for node classification on heterophilic graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12807</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training matters: Unlocking potentials of deeper graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08838</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08844</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06134</idno>
		<title level="m">Is homophily a necessity for graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An adaptive filter-bank equalizer for speech enhancement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1214" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv, abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H D</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
