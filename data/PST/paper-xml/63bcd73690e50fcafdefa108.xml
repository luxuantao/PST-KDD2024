<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoAC: Towards Automated Attribute Completion for Heterogeneous Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-01-08">8 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guanghui</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhennan</forename><surname>Zhu</surname></persName>
							<email>zhuzhennan@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
							<email>wenjie.wang@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhuoer</forename><surname>Xu</surname></persName>
							<email>zhuoer.xu@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
							<email>cfyuan@nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yihua</forename><surname>Huang</surname></persName>
							<email>yhuang@nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jackie</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ng</forename><surname>Man-Tat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Chow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Chinese</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Odyssey</forename><forename type="middle">Li</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi-Mou</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Faye</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Tam</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Ruch Hour Police Story</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoAC: Towards Automated Attribute Completion for Heterogeneous Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-08">8 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.03049v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>heterogeneous graph</term>
					<term>graph neural network</term>
					<term>attribute completion</term>
					<term>differentiable search Romance? Thriller? Action? ?? classification node</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real-world data can be modeled as heterogeneous graphs that contain multiple types of nodes and edges. Meanwhile, due to excellent performance, heterogeneous graph neural networks (GNNs) have received more and more attention. However, the existing work mainly focuses on the design of novel GNN models, while ignoring another important issue that also has a large impact on the model performance, namely the missing attributes of some node types. The handcrafted attribute completion requires huge expert experience and domain knowledge. Also, considering the differences in semantic characteristics between nodes, the attribute completion should be fine-grained, i.e., the attribute completion operation should be node-specific. Moreover, to improve the performance of the downstream graph learning task, attribute completion and the training of the heterogeneous GNN should be jointly optimized rather than viewed as two separate processes. To address the above challenges, we propose a differentiable attribute completion framework called AutoAC for automated completion operation search in heterogeneous GNNs. We first propose an expressive completion operation search space, including topology-dependent and topology-independent completion operations. Then, we propose a continuous relaxation schema and further propose a differentiable completion algorithm where the completion operation search is formulated as a bi-level joint optimization problem. To improve the search efficiency, we leverage two optimization techniques: discrete constraints and auxiliary unsupervised graph node clustering. Extensive experimental results on real-world datasets reveal that AutoAC outperforms the SOTA handcrafted heterogeneous GNNs and the existing attribute completion method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph-structured data are ubiquitous, such as social networks <ref type="bibr" target="#b0">[1]</ref>, scholar networks <ref type="bibr" target="#b1">[2]</ref>, biochemical networks <ref type="bibr" target="#b2">[3]</ref>, and knowledge graphs <ref type="bibr" target="#b3">[4]</ref>. Meanwhile, many real-world graph data are heterogeneous <ref type="bibr" target="#b4">[5]</ref>. Unlike the homogeneous graph with only one node type and one edge type, the heterogeneous graph <ref type="bibr" target="#b5">[6]</ref> consists of multiple types of nodes and edges associated with attributes in different feature spaces. For example, the IMDB dataset is a typical heterogeneous graph, which contains three node types (movie, actor, director) and two edge types (movie-actor, movie-director), as shown in Figure <ref type="figure" target="#fig_3">1(a)</ref>. Due to containing rich information and semantics, heterogeneous graphs have drawn more and more attention.</p><p>Recently, graph neural networks (GNNs) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> have demonstrated powerful representation learning ability on graph-structured data <ref type="bibr" target="#b8">[9]</ref>. Meanwhile, many heterogeneous GNNs (HGNNs) have been proposed for heterogeneous graphs <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b12">[13]</ref> [14] <ref type="bibr" target="#b14">[15]</ref> [16] <ref type="bibr" target="#b16">[17]</ref>. However, the existing work on heterogeneous graphs mainly focuses on the construction of novel GNN models, while ignoring another important issue that also has a large impact on the model performance, namely the attributes of some types of nodes are missing <ref type="bibr" target="#b17">[18]</ref>. Missing node attributes is a common problem because collecting the attributes of all nodes is prohibitively expensive or even impossible due to privacy concerns. Since the attributes of all nodes are required in the GNN-based heterogeneous models, some handcrafted ways are employed to deal with the problem of missing attributes. For example, the missing attribute vector can be the sum or the mean of directly connected nodes' attribute vectors. Besides, the onehot representations of a certain node type can also be used to replace the missing attributes. However, the handcrafted ways require huge expert experience and domain knowledge. Also, the topological relationships in the graph are not taken into account. Recently, an attention-based method <ref type="bibr" target="#b17">[18]</ref> was proposed to complete each no-attribute node by weighted aggregation of the attributes from the directly neighboring attributed nodes. Such an attribute completion method only considers the attributes of 1-hop neighbors without exploiting the attributes of higher-order neighbors.</p><p>Moreover, existing attribute completion methods are all coarse-grained. That is, for a specific node type without attributes, they adopt the same attribute completion operation for all nodes without considering the differences in semantic characteristics between nodes. In practice, fine-grained attribute completion is more reasonable. The attribute completion operations for the nodes with different semantics should be different. Take the IMDB dataset as an example. The target type of nodes (i.e., movie nodes) has attributes, and the other types of nodes (i.e., actor nodes and director nodes) have no attributes. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), there exist three attribute completion operations, including 1) For actors (e.g. Jackie Chan) who are involved in movies that mostly belong to the same genre (Kung Fu movies), average attribute aggregation of local (i.e., 1-hop) neighboring nodes should be used. 2) For actors who have strong collaborative relationships with other actors and directors, the message-passing based multihop attribute aggregation is more suitable. 3) For guest actors without representative movies, we can directly use the simple one-hot encoding to complete attributes. For the IMDB dataset, the number of actor nodes that have no attributes is 6124. Manually differentiating the semantic characteristics of all no-attribute nodes and then selecting the most suitable completion operations according to semantic characteristics is infeasible. Thus, an automated attribute completion method that can search the optimal completion operations efficiently is required. Moreover, to improve the performance of the downstream graph learning task, the automated attribute completion and the training of the heterogeneous GNN should be jointly optimized rather than viewed as two separate processes.</p><p>To address the above challenges, we propose a differentiable attribute completion framework called AutoAC 1 for automated completion operation search in heterogeneous GNNs. AutoAC is a generic framework since it can integrate different heterogeneous GNNs flexibly. By revisiting the existing attribute completion methods, we first propose an expressive completion operation search space, including topology-dependent and topology-independent completion operations. Instead of searching over the discrete space (i.e., candidate completion operations for each no-attribute node), we propose a continuous relaxation scheme by placing a weighted mixture of candidate completion choices, which turns the search task into an optimization problem regarding the weights of choices (i.e., completion parameters). Thus, due to the continuous search space, the search process becomes differentiable and we can perform completion operation searching via gradient descent.</p><p>To further improve the search efficiency, we formulate the search of attribute completion operations and the training of GNN as a constrained bi-level joint optimization problem. Specifically, we keep the search space continuous in the optimization process of completion parameters (i.e., upperlevel optimization) but enforce attribute completion choices being discrete in the optimization process of weights in the heterogeneous GNN (i.e., lower-level optimization). In this way, there is only one activated completion operation for each no-attribute node during the training of GNN, removing the need to perform all candidate completion operations. Inspired by NASP <ref type="bibr" target="#b18">[19]</ref>, we employ proximal iteration to solve the constrained optimization problem efficiently.</p><p>Finally, to reduce the dimension of the attribute completion parameters, we further leverage an auxiliary unsupervised 1 AutoAC is available at https://github.com/PasaLab/AutoAC graph node clustering task with the spectral modularity function during the process of GNN training.</p><p>To summarize, the main contributions of this paper can be highlighted as follows:</p><p>? We are the first, to the best of our knowledge, to model the attribute completion problem as an automated search problem for the optimal completion operation of each noattribute node.  <ref type="bibr" target="#b8">[9]</ref> aims to extend neural networks to graphs. Since heterogeneous graphs are more common in the real world <ref type="bibr" target="#b4">[5]</ref>, heterogeneous GNNs have been proposed recently. Part of the work is based on meta-paths. HAN <ref type="bibr" target="#b9">[10]</ref> leverages the semantics of metapaths and uses hierarchical attention to aggregate neighbors. MAGNN <ref type="bibr" target="#b13">[14]</ref> utilizes RotatE <ref type="bibr" target="#b22">[23]</ref> to encode intermediate nodes along each meta-path and mix multiple meta-paths using hierarchical attention. Another part of the work chooses to extract rich semantic information in heterogeneous graphs. GTN <ref type="bibr" target="#b10">[11]</ref> learns a soft selection of edge types and composite relations for generating useful multi-hop connections. HetGNN <ref type="bibr" target="#b12">[13]</ref> uses Bi-LSTM to aggregate node features for each type and among types. As the state-of-the-art model, SimpleHGN <ref type="bibr" target="#b16">[17]</ref> revisits existing methods and proposes a simple framework using learnable edge-type embedding and residual connections for both nodes and edges. Recently, AS-GCN <ref type="bibr" target="#b23">[24]</ref> employs the heterogeneous GNN to mine the semantics for text-rich networks.</p><p>Different from the above methods, HGNN-AC <ref type="bibr" target="#b17">[18]</ref> notices that most of the nodes in the real heterogeneous graph have missing attributes, which could cause great harm to the performance of heterogeneous models, and proposes an attentionbased attribute completion method. However, HGNN-AC needs to get node embeddings based on network topology using metapath2vec <ref type="bibr" target="#b24">[25]</ref>, which is a time-consuming process. Moreover, the attribute completion in HGNN-AC is coarsegrained and supports only one completion operation for all no-attribute nodes. HGCA <ref type="bibr" target="#b25">[26]</ref> unifies attribute completion and representation learning in an unsupervised heterogeneous network. MRAP <ref type="bibr" target="#b26">[27]</ref> performs node attribute competition in knowledge graphs with multi-relational propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Architecture Search (NAS)</head><p>NAS <ref type="bibr" target="#b27">[28]</ref> that designs effective neural architectures automatically has received more attention. The core components of NAS contain search space, search algorithm, and performance estimation strategy. Recently, many works use NAS to design GNN models due to the complexity of GNN <ref type="bibr" target="#b28">[29]</ref>. PolicyGNN <ref type="bibr" target="#b29">[30]</ref> uses reinforcement learning to train metastrategies and then adaptively determines the choice of aggregation layers for each node. SANE <ref type="bibr" target="#b30">[31]</ref> and SNAG <ref type="bibr" target="#b30">[31]</ref> search for aggregation functions using microscope-based and reinforcement learning-based strategies, respectively. The architecture-level approaches such as GraphNAS <ref type="bibr" target="#b31">[32]</ref>, Au-toGNN <ref type="bibr" target="#b32">[33]</ref>, and PSP <ref type="bibr" target="#b33">[34]</ref> aim to search for architectural representations of each layer, including sampling functions, attention computation functions, aggregation functions, and activation functions. The above works are based on homogeneous graphs. Due to the rich semantic and structural information in heterogeneous graphs, applying NAS to heterogeneous graphs is more challenging. Recently, there exist some excellent attempts. GEMS <ref type="bibr" target="#b34">[35]</ref> uses the evolutionary algorithm to search for meta-graphs between source and target nodes. DiffMG <ref type="bibr" target="#b35">[36]</ref> uses differentiable methods to find the best meta-structures in heterogeneous graphs. However, the above works only focus on the GNN model and ignore the heterogeneous graph data itself, which is even more important in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proximal Iteration</head><p>Proximal iteration <ref type="bibr" target="#b36">[37]</ref> is used to handle the optimization problem with a constraint C, i.e., min x f (x), s.t. x ? C, where f is a differentiable objective function. The proximal step is:</p><formula xml:id="formula_0">x (k+1) = prox C x (k) -?f x (k) prox C (x) = arg min z 1 2 ( z -x ) 2 , s.t. z ? C<label>(1)</label></formula><p>where is the learning rate. Due to the excellent theoretical guarantee and good empirical performance, proximal iteration has been applied to many deep learning problems (e.g., architecture search <ref type="bibr" target="#b18">[19]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>Heterogeneous Graph. Given a graph G = V, E where V and E denote the node set and the edge set respectively, G is heterogeneous when the number of node and edge types exceeds 2. Each node v ? V and each edge e ? E are associated with a node type and an edge type respectively.</p><p>Attribute Missing in Heterogeneous Graph. Let x v ? R d denote the original d-dimensional attribute vector in the node v. In practice, the attributes of some types of nodes are not available. Thus, the node set V in G can be divided into two subsets, i.e., V + and V -, which denote the attributed node-set and no-attribute node-set.</p><p>Attribute Completion. Let X = {x v | v ? V + } denote the input attribute set. Attribute completion aims to complete the attribute for each no-attribute node v ? V -by leveraging the available attribute information X and the topological structure of G. Let x C v denote the completed attribute. Thus, after completion, the node attributes for the training of heterogeneous GNN is</p><formula xml:id="formula_1">X new = X ? X C = {x v | v ? V + } ? {x c v | v ? V -}.</formula><p>In this paper, we aim to search for the optimal completion operation for each no-attribute node to improve the prediction performance of GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE PROPOSED METHODOLOGY</head><p>In this section, We first present the proposed completion operation search space and then introduce the differentiable search strategy. Moreover, we introduce the optimization techniques including discrete constraints and the auxiliary unsupervised graph node clustering task for further improving the search efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Search Space of Attribute Completion Operation</head><p>Due to the semantic differences between nodes, using a single attribute completion operation for all no-attribute nodes belonging to the same node type is not reasonable. The available completion operations should be diverse and we can select the most suitable completion operation for each node with missing attributes. Thus, to capture both the node semantics and the topological structure information during the attribute completion process, we first propose an expressive completion operation search space, which consists of topology-dependent and topology-independent operations. Specifically, the topology-dependent operations employ the topology information of the graph to guide the attribute completion. Inspired by the node aggregation operations in typical GNNs (e.g., GraphSage <ref type="bibr" target="#b0">[1]</ref>, GCN <ref type="bibr" target="#b7">[8]</ref>, APPNP <ref type="bibr" target="#b37">[38]</ref>), we design three topology-dependent attribute completion operations, i.e., mean, GCN-based, PPNP-based operations. In contrast, the topology-independent operation directly uses onehot encoding to replace the missing attribute. AutoAC aims to search the optimal operation for each no-attribute node from the general and scalable search space where we can draw on more node aggregation operations in GNNs as attribute completion operations.</p><p>1) Topology-Dependent Completion Operation: Such type of completion operations can be further divided into two categories: local attribute aggregation and global (i.e., multihop) attribute aggregation. Local Attribute Aggregation. Similar to the node aggregation in GraphSage <ref type="bibr" target="#b0">[1]</ref>, we first propose mean attribute aggregation.</p><p>Mean Attribute Aggregation. For the node v ? V -, we calculate the mean of neighbors' attributes to complete the missing attribute. The completed attribute x C v is as follows:</p><formula xml:id="formula_2">x C v = W ? mean x u , ?u ? N + v<label>(2)</label></formula><p>where N + v denotes the local (i.e, 1-hop) neighbors of node v in set V + . W is the trainable transformation matrix.</p><p>GCN-based Attribute Aggregation. Similar to spectral graph convolutions in GCN <ref type="bibr" target="#b7">[8]</ref>, we complete the missing attribute with the following renormalized graph convolution form.</p><formula xml:id="formula_3">x C v = u?N + v (deg(v) ? deg(u)) -1/2 ? x u ? W<label>(3)</label></formula><p>Global Attribute Aggregation. Motivated by the node aggregation in APPNP <ref type="bibr" target="#b37">[38]</ref>, we propose PPNP-based completion operation for global attribute aggregation. PPNP-based Attribute Aggregation. Besides the GCN-based attribute completion, we use another popular node aggregation method PPNP (i.e., Personalized PageRank <ref type="bibr" target="#b37">[38]</ref>) for attribute completion. Specifically, let A ? R n?n denote the adjacency matrix of the graph G. ? = A + I n denotes the adjacency matrix with added self-loops. The form of PPNP-based attribute completion is:</p><formula xml:id="formula_4">X ppnp = ? I n -(1 -? ?) -1 ? X , X = X ? W X C = {X ppnp i | ?i ? V -}<label>(4)</label></formula><p>where ? = D-1/2 ? D-1/2 is the symmetrically normalized adjacency matrix with self-loops, with the diagonal degree matrix D. ? ? (0, 1] is the restart probability. Note that the missing attributes are filled with zeros in X. After PPNP-based attribute aggregation, we complete the attributes of the nodes in V -with X ppnp .</p><p>2) Topology-Independent Completion Operation: For the no-attribute nodes that have few neighbors or are less affected by the neighbor information, we can directly use onehot encoding to replace the missing attributes. The one-hot representation of a specific node type is also a commonly used handcrafted attribute completion method <ref type="bibr" target="#b16">[17]</ref>. For example, there are K distinct actors in IMDB. The one-hot representation for the actor node is a K-dimensional vector. For a specific actor, the element in the corresponding index is 1 and the others are 0. Then, the one-hot representation is transformed linearly for dimension alignment.</p><p>3) Search Space Size Analysis: In summary, the proposed search space O contains a diverse set of attribute completion operations. Let N -denote the total number of nodes with missing attributes. Thus, the space size can be calculated by |O| N - , which is exponential to N -. In practice, the attribute missing of some node types is a common problem, leading to huge search space. Thus, the block-box optimization-based search method (e.g., evolutionary algorithm) over a discrete search space is infeasible. To address this issue, we propose a differentiable search strategy to find the optimal completion operations efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Differentiable Search Strategy</head><p>In this section, we first introduce a continuous relaxation scheme for the completion operation search space to make the search process to be differentiable. Then, we introduce the differentiable search algorithm and two optimization techniques to improve the search efficiency.</p><p>1) Continuous Relaxation and Optimization: Inspired by the success of the differentiable NAS, we first design a continuous search space and then perform differentiable completion operation search via gradient descent.</p><p>As shown in Equation <ref type="formula" target="#formula_5">5</ref>, instead of searching over the discrete space, we view the completion operation as a weighted mixture of candidate choices.</p><formula xml:id="formula_5">x C v = o?O exp ? (v) o o ?O exp ? (v) o o (v)<label>(5)</label></formula><p>where v denotes the node with the missing attribute, o denotes the candidate operation in the search space O, o (v) denotes the completed attribute of node v with o. ? (v) indicates the mixing weight vector of dimension |O| for node v. Furthermore, we refer to</p><formula xml:id="formula_6">? = {? (v) | v ? V -} ? R N -?|O|</formula><p>as the completion parameters.</p><p>After continuous relaxation, the search objective becomes the learning of the completion parameters ?. To this end, we formulate the search problem as an optimization problem that can jointly learn the completion parameters ? and the weights w in the heterogeneous GNN by gradient descent. Let L train and L val denote the training loss and validation loss respectively. Since both losses are determined by the completion parameters ? and the weights w, the search objective is a bilevel optimization problem.</p><formula xml:id="formula_7">min ? L val (? * , ?) s.t. ? * = argmin w L train (?, ?)<label>(6)</label></formula><p>where the upper-level optimization is for the optimal completion parameters ? and the lower-level optimization is for the optimal weights w in the GNN model.</p><p>2) Overview: Figure <ref type="figure" target="#fig_1">2</ref> shows the overall framework of automated attribute completion for heterogeneous graphs. First, we perform a continuous relaxation of the search space by placing a mixture of candidate completion operations. Then, the completion parameters ? are optimized. After determining the attribute completion operations for each no-attribute node, we view the completed attributes together with the raw attributes as the initial embedding for the training of the graph neural network. Why not use the weighted mixture. Although the continuous relaxation allows the search of completion operations to be differentiable, there still exist following limitations when directly using the weighted mixture of all completion operations:</p><p>1) High computational overhead: After continuous relaxation, we need to perform all candidate completion operations for each no-attribute node when training heterogeneous GNNs, leading to huge computational overhead. Also, solving the bi-level optimization problem in Equation 6 incurs significant computational overhead. 2) Performance gap: At the end of the search, continuous parameters ? needs to be discretized, i.e., argmax o?O ?</p><formula xml:id="formula_8">(v)</formula><p>o , resulting in inconsistent performance between searched and final completion operations.</p><p>3) Large dimension of ?: The dimension of completion parameters ? is N -? |O|, which is proportional to the total number of nodes with missing attributes. The large dimension of ? leads to a slow convergence rate and low search efficiency. To address the first two issues (i.e., reducing computational overhead and avoiding performance gap), we first propose an efficient search algorithm with discrete constraints. Specifically, for each no-attribute node v, the completion parameters satisfy the following constraints:</p><formula xml:id="formula_9">? (v) ? C = C 1 ? C 2 , where C 1 = {? (v) | ? (v) 0 = 1}, C 2 = {? (v) | 0 ? ? (v) i ? 1}.</formula><p>The constraint C 2 allows ? to be optimized continuously, and C 1 keeps the choices of completion operation to be discrete when training GNN. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, there is only one activated edge for each choice when training GNN, removing the need to perform all candidate completion operations. The final completion operation is derived from the learned completion parameter ?. For node v, the edge with the maximum completion parameter will be kept. We leverage proximal iteration <ref type="bibr" target="#b36">[37]</ref> to solve the constrained optimization problem. Moreover, proximal iteration can improve the computational efficiency of optimizing ? without second-order derivative.</p><p>Moreover, to address the third issue (i.e., reducing the dimension of ?), we propose an auxiliary unsupervised clustering task. In practice, the no-attribute nodes with similar semantic characteristics may have the same completion operation. Take the actor nodes in the IMDB dataset as an example. For the actors with a large number of representative movies, the average attribute aggregation operation is more suitable. Thus, we can cluster all no-attribute nodes into M clusters, where the nodes in each cluster have the same completion operation. The optimization goal becomes to search for the optimal attribute completion operation for each cluster. In this way, the size of the completion parameters ? is reduced from</p><formula xml:id="formula_10">N -? |O| to M ? |O|, M N -.</formula><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the auxiliary unsupervised clustering loss can be jointly optimized with the node classification loss (i.e., cross-entropy).</p><p>The proposed framework AutoAC is composed of multiple iterations. In each iteration, the completion parameters ? and the weights in the GNN are optimized alternatively. Next, we introduce the search algorithm with discrete constraints and the auxiliary unsupervised clustering task in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Search Algorithm with Discrete Constraints</head><p>Equation 6 implies a bi-level optimization problem with ? as the upper-level variable and w as the lower-level variable. Following the commonly used methods in meta learning <ref type="bibr" target="#b38">[39]</ref> and NAS <ref type="bibr" target="#b39">[40]</ref>, we use a one-step gradient approximation to the optimal internal weight parameters ? * to improve the Get discrete choices of attribute completion operations:</p><formula xml:id="formula_11">?(k) = prox c1 (? (k) ) 4:</formula><p>Update ? for continuous variables:</p><formula xml:id="formula_12">? (k+1) = prox c2 (? (k) -? ?(k) L val (? (k) , ?(k) )) 5:</formula><p>Refine discrete choices after updating: ?(k+1) = prox c1 (? (k+1) ) 6:</p><p>Update ? (k) by ? ? (k) L train (? (k) , ?(k+1) ) 7: end while efficiency. Thus, the gradient of the completion parameters ? is as follows (we omit the step index k for brevity):</p><formula xml:id="formula_13">? ? L val (? * , ?) ?? ? L val (? -?? ? L train (?, ?), ?) =? ? L val (? , ?) -?? 2 ?,? L train (?, ?)? ? L val (? , ?)<label>(7)</label></formula><p>where ? is the weights of the GNN, ? is the learning rate of internal optimization, and ? = ? -?? ? L train (?, ?) indicates the weights for a one-step forward model. we update the completion parameters ? to minimize the validation loss. In Equation <ref type="formula" target="#formula_13">7</ref>, there exists a second-order derivative, which is expensive to compute due to a large number of parameters. Also, the continuous relaxation trick further leads to huge computational overhead since all candidate completion operations need to be performed when training the GNN. Moreover, the overall search process is divided into two stages: search and evaluation. In the evaluation stage, the continuous completion parameters ? need to be discretized for replacing every mixed choice as the most likely operation by taking the argmax, leading to performance gap between the search and evaluation stage.</p><p>To optimize ? efficiently and avoid the performance gap, we propose a search algorithm with discrete constraints when optimizing completion parameters ?. For the no-attribute node v, let the feasible space of ? (v) </p><formula xml:id="formula_14">be C = {? (v) | ? (v) 0 = 1 ? 0 ? ? (v) i ? 1}. We denote it as the intersection of two feasible spaces (i.e., C = C 1 ? C 2 ), where C 1 = {? (v) | ? (v) 0 = 1}, C 2 = {? (v) | 0 ? ? (v) i ? 1}.</formula><p>The optimization problem under constraints can be solved by the proximal iterative algorithm.</p><formula xml:id="formula_15">Proposition 1: prox C (z) = prox C2 (prox C1 (z))</formula><p>Inspired by Proposition 1 <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, in the k-th proximal iteration, we first get discrete variables constrained by C 1 , i.e., ?(k) = prox C1 (? (k) ) (the node notation v is omitted for brevity). Then, we derive gradients w.r.t ?(k) and keep ? to be optimized as continuous variables but constrained by C 2 .</p><formula xml:id="formula_16">? (k+1) = prox c2 (? (k) -? ?(k) L val (? (k) ))<label>(8)</label></formula><p>The detailed search algorithm is described in Algorithm 1. First, we get a discrete representation of ? by proximal step (Line 3). Then, we view ? (k) as constants and optimize ? (k+1)   for continuous variables (Line 4). Since there is no need to compute the second-order derivative, the efficiency of updating ? can be improved significantly. After updating ?, we further refine discrete choices and get ?(k+1) for updating ? (k) on the training dataset, which contributes to reducing the performance gap caused by discretizing completion parameters ? from continuous variables. Moreover, since only one candidate choice is activated for each no-attribute node, the computational overhead can also be reduced. The computational efficiency of updating ? can be significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Auxiliary Unsupervised Clustering Task</head><p>As mentioned before, the dimension of the completion parameters ? is N -? |O| (|O| N -, |O| = 4). Take the DBLP dataset as an example, the number of nodes with missing attributes is about 1.2 ? 10 4 , leading to a large dimension of completion parameters ?. As a result, optimizing ? with a limited size of validation dataset is very difficult.</p><p>Inspired by the observation that the no-attribute nodes with similar explicit topological structure or implicit semantic characteristics, we further propose an auxiliary unsupervised clustering task to divide all no-attribute nodes into M clusters. In each cluster, all nodes share the same completion operation. In this way, the dimension of the completion parameters ? can be reduced to M ?|O|, M N -, and optimizing ? becomes feasible and efficient.</p><p>It is well known that the EM algorithm <ref type="bibr" target="#b40">[41]</ref> is a commonly used method (e.g., K-Means <ref type="bibr" target="#b41">[42]</ref>) to solve the problem of unsupervised clustering. In the scenario of graph node clustering, let h v denote the hidden node representation learned by the heterogeneous GNN. The E-step is responsible for assigning the optimal cluster for each node v by calculating the distances between h v and all cluster centers. The M-step is used to update the centers of all clusters. The E-step and M-step are performed alternately until convergence.</p><p>Although the EM algorithm has a convergence guarantee, it is sensitive to the initial values, making it difficult to apply to the proposed automated completion framework. The main reason is that the bi-level optimization problem defined in Equation 6 is iterative. In the early optimization process, the weights of the GNN have not yet converged and the node representations learned in the GNN are less informative. Such low-quality representations lead to inaccurate clustering, which has a negative impact on the subsequent clustering quality and further leads to a deviation from the overall optimization direction.</p><p>To address this issue, we first formulate the problem of unsupervised node clustering as a form of soft classification, and use the assignment matrix C to record the probability of each node belonging to each cluster. Moreover, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, we embed the clustering process into the bi-level iterative optimization process.</p><p>Motivated by graph pooling and graph module partitioning, we introduce the Spectral Modularity Function Q <ref type="bibr" target="#b42">[43]</ref>  <ref type="bibr" target="#b43">[44]</ref>. From a statistical perspective, this function can reflect the clus-tering quality of graph node modules through the assignment matrix C <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_17">Q = 1 2 |E| ij A ij - d i d j 2 |E| ? (c i , c j ) (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>where |E| is the number of edges in the graph, ?(c i , c j ) = 1 only if nodes i and j are in the same cluster, otherwise 0. d i and d j represent the degrees of node i and node j respectively. It can be known that in a random graph, the probability that node i and node j are connected is didj 2|E| <ref type="bibr" target="#b44">[45]</ref>. Then, the optimization goal is converted into maximizing the spectral modularity function Q, but it is an NP-hard problem. Fortunately, this function can be represented by an approximate spectral domain relaxation form:</p><formula xml:id="formula_19">Q = 1 2 |E| Tr C BC<label>(10)</label></formula><p>where C ij ? [0, 1] denotes the cluster probability. B is the modular matrix B = Add 2|E| . Finding the optimal solution of the assignment matrix C is to maximize Q. To prevent falling into local optimum (i.e., all nodes tend to be in the same cluster), we further add the collapse regularization term. The assignment matrix C should be amortized as adaptively as possible, so as to skip the local optimum.</p><p>Let L GmoC denote the unsupervised clustering loss, which can be expressed as: <ref type="bibr" target="#b10">(11)</ref> where |V | is the number of nodes, M is the number of clusters, ? F represents the Frobenius norm of the matrix. Note that L GmoC can be jointly optimized with the supervised classification loss. Specifically, L GmoC can be used as an auxiliary task for the bi-level optimization problem in Equation <ref type="formula" target="#formula_7">6</ref>. The unsupervised clustering loss is added to L train for joint optimization. Let ? denote the loss-weighted coefficient. The optimization objective is updated as:</p><formula xml:id="formula_20">L GmoC = - 1 2 |E| Tr C BC modularity loss + ? M |V | i C i F collapse regularization</formula><formula xml:id="formula_21">min ? L val (w * , ?) s.t. w * = arg min w (L train (w, ?) + ?L GmoC )<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Complexity Analysis</head><p>In the heterogeneous graph G = V, E , the total number of nodes is N , the total number of nodes with missing attributes is N -, and the embedding dimension is k. In each iteration of 12, we can divide the search process of AutoAC into three phases, i.e., attribute completion phase, upper-level optimization for completion parameters ?, and lower-level optimization for weights ?. We first analyze the computational complexity. Since discrete constraints are performed, only one candidate completion operation is activated for each no-attribute node. The computational complexity of each completion operation </p><formula xml:id="formula_22">(N ? k 2 ) + O(C H + |O| ? M ? b ? ) + |?| ? b ? ) + O(d 2 ? N + |E|).</formula><p>Next, we analyze the space complexity of AutoAC. For the attribute completion phase, the space complexity is O(k 2 ). For the optimization phase, the space complexity is O(N ? k + |O| ? M + |?| + N ? M ), where O(N ? M ) is the space complexity in the unsupervised clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>A. Experimental Setup 1) Experimental Setting: We use the recently proposed Heterogeneous Graph Benchmark (HGB) <ref type="bibr" target="#b16">[17]</ref> to conduct all experiments, which offers a fair way to compare heterogeneous GNN models. HGB gives a set of standard benchmark datasets and unified strategies for feature preprocessing and data split. In the node classification task, all edges are available during training, and node labels are split according to 24% for training, 6% for validation, and 70% for test in each dataset. In the link prediction task, we mask 10% edges of the target link type and the negative edges are randomly sampled The statistics of the four datasets are summarized in Table <ref type="table">I</ref>. More details of datasets can be seen in Appendix A.</p><p>Moreover, the handcrafted attribute completion methods for existing heterogeneous GNNs are provided by HGB. Micro-F1 and Macro-F1 are provided to evaluate the node classification performance, while the MRR and ROC-AUC metrics are used for link prediction. The evaluation metrics are obtained by submitting predictions to the HGB website 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>All experiments are performed in the transductive setting. We employ the Adam optimizer <ref type="bibr" target="#b45">[46]</ref> to optimize both ? and ?. For optimizing ?, the learning rate and the weight decay are 5e-4 and 1e-4 respectively. For optimizing ?, the learning rate and the weight decay are 5e-3 and 1e-5 respectively.</p><p>We implement AutoAC based on the widely-used heterogeneous GNNs, i.e., MAGNN <ref type="bibr" target="#b13">[14]</ref> and SimpleHGN <ref type="bibr" target="#b16">[17]</ref>. The loss weighted coefficient ? and the number of clusters M are two hyperparameters of AutoAC. For MAGNN, we empirically set ? to 0.5 for all datasets, M to 4 for the DBLP and ACM datasets, 16 for the IMDB dataset. For SimpleHGN, ? is 0.4 for all datasets, and M is 8 for the DBLP dataset, 12 for the ACM and IMDB datasets. Moreover, all the GNN models are implemented with PyTorch. All experiments are run on a single GPU (NVIDIA Tesla V100) five times and the average performance and standard deviation are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness of AutoAC</head><p>1) Performance comparison with humancrafted heterogeneous GNNs: Depending on whether or not the meta-path is used, we divide the humancrafted heterogeneous GNNs into two categories:</p><p>? GNNs with meta-path: HAN <ref type="bibr" target="#b12">[13]</ref>, GTN <ref type="bibr" target="#b10">[11]</ref>, Het-SANN <ref type="bibr" target="#b15">[16]</ref>, MAGNN <ref type="bibr" target="#b13">[14]</ref>, HGCA <ref type="bibr" target="#b25">[26]</ref>. ? GNNs without meta-path: HGT <ref type="bibr" target="#b14">[15]</ref>, GATNE <ref type="bibr" target="#b46">[47]</ref>, Het-GNN <ref type="bibr" target="#b12">[13]</ref>, GCN <ref type="bibr" target="#b7">[8]</ref> and GAT <ref type="bibr" target="#b19">[20]</ref> (two commonly used general-purpose GNNs), as well as the current SOTA GNN model SimpleHGN <ref type="bibr" target="#b16">[17]</ref>. The configurations of baselines can be seen in Appendix B. Since AutoAC is general and thus can be integrated into different GNNs. We select two representative GNN models from the two categories (i.e., MAGNN and SimpleHGN) from the perspective of performance and computational efficiency. Then, we combine AutoAC with the two models, denoted by MAGNN-AutoAC and SimpleHGN-AutoAC respectively.</p><p>Table <ref type="table" target="#tab_2">II</ref> shows the performance comparison between Au-toAC and existing heterogeneous GNNs on node classification. AutoAC can improve the performance of MAGNN and SimpleHGN stably on all datasets. The performance gain obtained by AutoAC over MAGNN is around 0.7%-3% and the error rate is reduced by 2.87%-11.69%. Also, SimpleHGN-AutoAC outperforms SimpleHGN by 1%-3% and reduces the error rate by 1.59%-22.09%. By combining with the SOTA model SimpleHGN, SimpleHGN-AutoAC can achieve the best performance in all models.</p><p>2 https://www.biendata.xyz/competition/hgb-1/ Moreover, Table <ref type="table" target="#tab_2">II</ref> shows that AutoAC can bring significant performance improvement on the datasets where the classification target nodes have no raw attributes (e.g., DBLP). Besides, for the datasets where the target nodes already have raw attributes (e.g., ACM and IMDB), completing other nontarget nodes using AutoAC can still promote the classification accuracy of target nodes. Especially, for the IMDB dataset, since there are too many non-target nodes with missing attributes (i.e., 77% of all nodes), the performance improvement with AutoAC is more significant.</p><p>Note that the performance of MAGNN without attribute completion is not as good as other models, such as GTN and GAT. However, MAGNN-AutoAC performs better than GTN on DBLP and ACM, and outperforms GAT on DBLP and IMDB, which indicates that effective attribute completion for heterogeneous graphs can compensate for the performance gap introduced by the GNN model. By unifying attribute completion and representation learning in an unsupervised heterogeneous network, the recently proposed HGCA can also achieve competitive performance on DBLP and ACM. Such experimental results further verify the necessity of AutoAC.</p><p>2) Performance comparison with the existing attribute completion method HGNN-AC: As the current SOTA attribute completion method, HGNN-AC <ref type="bibr" target="#b17">[18]</ref> uses the attention mechanism to aggregate the attributes of the direct neighbors for the nodes with missing attributes. The attention information is calculated by the pre-learning of topological embedding. To be fair, both AutoAC and HGNN-AC are evaluated under the unified HGB benchmark. And, we also combine HGNN-AC with MAGNN and SimpleHGN, denoted by MAGNN-HGNNAC and SimpleHGN-HGNNAC respectively.</p><p>Table <ref type="table" target="#tab_3">III</ref> shows that AutoAC outperforms HGNN-AC on all datasets. Specifically, MAGNN-AutoAC achieves 1%-4% performance improvement over MAGNN-HGNNAC. For the SimpleHGN model, SimpleHGN-AutoAC outperforms SimpleHGN-HGNNAC by 0.4%-2%. Moreover, the performance improvement of HGNN-AC for attribute completion is not stable. As shown in Table <ref type="table" target="#tab_3">III</ref>, after attribute completion with HGNN-AC, MAGNN-HGNNAC is instead inferior to MAGNN on the three datasets, while MAGNN-AutoAC can achieve significant performance improvement with attribute completion. Similarly, there is a degradation in performance on the DBLP dataset compared to SimpleHGN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Performance comparison on link prediction:</head><p>To verify the effectiveness of AutoAC on different downstream tasks, we further conduct link prediction in Table <ref type="table" target="#tab_5">V</ref>. AutoAC can greatly improve the performance of heterogeneous GNNs, especially on IMDB. With AutoAC, MRR and ROC-AUC of SimpleHGN are increased by 9.7% and 28%, respectively. In summary, AutoAC achieves better performance and more stable performance improvement, indicating the effectiveness of searching for the most suitable attribute completion operations for no-attribute nodes from a diverse search space.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Efficiency Study</head><p>Besides the effectiveness, we also evaluate the efficiency of AutoAC in the terms of runtime overhead. Table <ref type="table" target="#tab_2">II</ref> and<ref type="table" target="#tab_5">V</ref> show the runtime of AutoAC and other handcrafted HGNNs on node classification and link prediction tasks. Although the attribute completion and GNN training are jointly optimized in AutoAC, the computational efficiency of AutoAC is still competitive compared to other baselines.</p><p>Also, we compare AutoAC with the existing attribute completion method HGNN-AC. Table <ref type="table" target="#tab_4">IV</ref> shows the efficiency comparison between AutoAC and HGNN-AC. AutoAC contains the search and retraining stages, and HGNN-AC contains the pre-learning and training stages. We can see that AutoAC is much more efficient than HGNN-AC. The end-to-end runtime overhead of AutoAC can be reduced by 15? to 465?. The main reason why HGNN-AC is inefficient is that the preleaning stage that learns a topological embedding for each node is very time-consuming. Especially for the DBLP dataset with a large number of nodes, the pre-learning overhead is up to 9 GPU hours. In contrast, there is no additional pre-leaning stage in AutoAC. Moreover, by introducing the discrete constraints and auxiliary unsupervised clustering task, the search efficiency can be improved significantly.</p><p>In summary, AutoAC can not only achieve better performance but also demonstrate higher computational efficiency.  Table VI and Table <ref type="table" target="#tab_7">VII</ref> show the completion operation ablation study on SimpleHGN and MAGNN. Due to the differences in the data characteristics, there is no single completion operation that can perform well on all datasets. By searching the optimal attribute completion operations AutoAC can achieve the best performance on all datasets.</p><p>Take SimpleHGN shown in Table <ref type="table" target="#tab_6">VI</ref> for example. GCN AC is more effective on DBLP and IMDB, while PPNP AC performs better on ACM. Moreover, for a specific attribute completion operation, the performance is related to the dataset and the chosen GNN model. We take DBLP as an example. GCN AC performs better on SimpleHGN. However, when the GNN model becomes MAGNN, GCN AC is not as good as MEAN AC. Additionally, the performance of the random attribute completion is not stable and can be even worse than the baseline model. Choosing an inappropriate completion operation can have a negative effect on the final performance.</p><p>2) Study on the search algorithm with discrete constraints: When optimizing the attribute completion parameters ?, we enforce discrete constraints on ? and solve the bi-level optimization problem with proximal iteration. To verify the effectiveness of discrete constraints, we further run AutoAC with and without discrete constraints in Table <ref type="table" target="#tab_8">VIII</ref>.</p><p>The search algorithm with discrete constraints can achieve better performance with less search time overhead on all datasets. Additionally, proximal iteration allows removing the need for second-order derivative in solving the bi-level optimization problem. Thus, the memory overhead can also be reduced significantly.As shown in Table VIII, the memory overhead of MAGNN-AutoAC without discrete constraints is huge and the out-of-memory error occurs on DBLP.</p><p>3) Study on the auxiliary unsupervised clustering: To reduce the dimension of the completion parameters ?, we  leverage an auxiliary unsupervised clustering task. Figure <ref type="figure" target="#fig_4">3</ref> shows the performances of different clustering methods.</p><p>? w/o cluster: We directly search the attribute completion operations for each no-attribute node without clustering. ? EM: After each iteration of the optimization process, we adopt the EM algorithm for clustering according to node representation learned by the GNN model. ? EM with warmup: a variant of the EM algorithm, which adds a warm-up process at the beginning of the clustering. In Figure <ref type="figure" target="#fig_4">3</ref>, AutoAC can achieve the best performance on all datasets. Searching completion operations without clustering yields relatively poor performance. Reducing the dimension of ? with unsupervised clustering is very necessary. Moreover,       than GCN AC compared to SimpleHGN-AutoAC. The results further indicate the necessity of searching for suitable attribute completion operations under different datasets and GNNs. Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> show the proportion of searched completion operations for each no-attribute node type on ACM and IMDB. For ACM, multiple different completion operations are selected even for the same node type. Specifically, more than half of the author and subject nodes choose PPNP AC, while the proportions of other three operations are quite similar. Most term nodes are assigned PPNP AC (i.e., 94.74%), indicating that the term type is more likely to capture the global information. The main reason is that the target node type (i.e., paper) with raw attributes in ACM contains only the paper title. The high-order PPNP AC operations are preferred. In contrast, GCN AC accounts for the majority of completion operations on IMDB. This is because that the target node type (i.e., movie) has raw attributes and contains rich features, such as length, country, language, likes of movies, and ratings. Thus, the local completion operation GCN AC is appropriate.</p><p>Next, we analyze the completion operations of concrete actor nodes. In IMDB, node No.10797 is the actor Leonardo DiCaprio, who has starred in 22 movies, and the neighborhood information is very rich. As a result, AutoAC chooses GCN AC for him. In contrast, node No.10799 is the actor Leonie Benesch, who has appeared in only one movie. Thus, one-hot AC is automatically selected by AutoAC.</p><p>G. Hyperparameter Sensitivity 1) Effect of the number of clusters M : Figure <ref type="figure" target="#fig_9">8</ref> shows the performance of AutoAC under different M . Both SimpleHGN-AutoAC and MAGNN-AutoAC can achieve stable performance, showing that AutoAC has sufficient robustness to M .</p><p>2) Effect of the loss weighted coefficient ?: We further evaluate the weighted coefficient ? of the auxiliary unsupervised clustering loss. The available values of ? are set to [0.1, 0.2, 0.3, 0.4, 0.5]. Figure <ref type="figure" target="#fig_10">9</ref> shows the performances of AutoAC under different ?. IMDB is very robust to ?, and the performance change is very insignificant. For DBLP, ? = 0.4 and ? = 0.5 are suitable for SimpleHGN and MAGNN, respectively. For ACM, the choice of ? is slightly sensitive.</p><p>The effects of the learning rate and the weight decay can be seen in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Impacts of Attribute Missing Rates and Masked Edge Rates</head><p>1) Study on the performance of the same dataset with varying attribute missing rates in the node classification task: Table <ref type="table" target="#tab_9">IX</ref> shows the performance of SimpleHGN-AutoAC with varying attribute missing rates. We change attribute missing rates by completing the missing attributes with one-hot   encoding, which is a commonly used handcrafted attribute completion method. A missing rate of 0% means that all missing attributes are completed manually. 45%, 69%, and 76% are inherent attribute missing rates of DBLP, ACM, and IMDB, respectively, i.e., only one node type has raw attributes. From Table <ref type="table" target="#tab_9">IX</ref>, we can see that SimpleHGN-AutoAC performs better with higher missing rates, indicating that AutoAC is capable of searching for the suitable completion operation for each no-attribute node and the searched completion operations are superior to the handcrafted completion method.</p><p>2) Study on the performance of the same dataset with varying masked edge rates in the link prediction task: Table <ref type="table" target="#tab_10">X</ref> shows the performance of SimpleHGN-AutoAC with varying masked edge rates. The edges are masked randomly. We can see that SimpleHGN-AutoAC achieves better performance than SimpleHGN at different masked edge rates, especially on the IMDB dataset. Moreover, the performance of both models decreases as the masked edge rate increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a differentiable attribute completion framework called AutoAC for automated completion operation search in heterogeneous GNNs. First, we introduced an expressive completion operation search space and proposed a continuous relaxation scheme to make the search space differentiable. Second, we formulated the completion operation search as a bi-level joint optimization problem. To improve search efficiency, we enforced discrete constraints on completion parameters and further proposed a proximal iterationbased search algorithm. Moreover, we leveraged an auxiliary unsupervised node clustering task to reduce the dimension of completion parameters. Extensive experimental results reveal that AutoAC is effective to boost the performance of heterogeneous GNNs and outperforms the SOTA attribute completion method in terms of performance and efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of the learning rate and the weight decay</head><p>We further evaluate the effect of the learning rate and weight decay when optimizing the completion parameters ?. The available learning rates are set to [3e-3, 4e-3, 5e-3, 6e-3, 7e-3]. The available weight decay values are set to [5e-6,1e-5, 2e-5, 3e-5, 4e-3]. Figure <ref type="figure" target="#fig_3">10</ref> and Figure <ref type="figure" target="#fig_11">11</ref> show the performances of AutoAC with different learning rates and different weight decay, respectively. The green and blue lines represent SimpleHGN-AutoAC and MAGNN-AutoAC, respectively. From Figure <ref type="figure" target="#fig_3">10</ref> and Figure <ref type="figure" target="#fig_11">11</ref>, we can see that AutoAC is very robust to the learning rate and the weight decay.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Example of heterogeneous graphs with incomplete attributes, i.e., the IMDB dataset. (b) Different attribute completion operations for the actor node, i.e., local attribute aggregation, message-passing based multi-hop attribute aggregation, and one-hot representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The overall workflow of automated attribute completion for the heterogeneous graph neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Search Algorithm in AutoAC 1: Initialize completion parameters ? according to defined search space O; 2: while not converge do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 )</head><label>1</label><figDesc>Study on the necessity of searching attribute completion operations from a diverse search space: We compare AutoAC with the following two methods:? Single-operation attribute completion: We complete all no-attribute nodes with the same single completion operation (i.e., GCN AC, PPNP AC, MEAN AC, and One-hot AC). ? Random attribute completion: For each no-attribute node, we randomly select an attribute completion operation from the search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance comparison between different clustering methods.</figDesc><graphic url="image-48.png" coords="10,317.84,467.45,72.80,50.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Convergence of L GmoC on three datasets.</figDesc><graphic url="image-50.png" coords="10,484.37,467.58,72.80,50.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of searched attribute completion operations.</figDesc><graphic url="image-52.png" coords="11,58.12,583.54,62.76,62.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Detailed distribution of searched completion operations for each noattribute node type on the ACM dataset using SimpleHGN-AutoAC.</figDesc><graphic url="image-54.png" coords="11,212.06,584.09,75.32,62.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>F.Figure 5</head><label>5</label><figDesc>Figure 5 shows the proportion of attribute completion operations searched by SimpleHGN-AutoAC and MAGNN-AutoAC. For different models and datasets, the proportions of searched completion operations are quite different. In SimpleHGN-AutoAC, DBLP tends to select GCN AC, while ACM prefers PPNP AC. For the same dataset, different GNNs also result in different distributions. Take DBLP as an example. MAGNN-AutoAC is more inclined to MEAN AC</figDesc><graphic url="image-51.png" coords="11,48.96,439.10,251.05,125.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Performance comparison under different M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance comparison under different ?</figDesc><graphic url="image-63.png" coords="12,482.63,55.52,72.80,50.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 10. Performance comparison under different learning rates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>AND RUNTIME (CLOCK TIME IN SECONDS) COMPARISON BETWEEN AUTOAC AND SOTA HUMANCRAFTED HETEROGENEOUS GNNS ON NODE CLASSIFICATION. THE BOLD AND THE UNDERLINE INDICATE THE BEST AND THE SECOND BEST IN EACH CATEGORY (I.E., USING AND NOT USING META-PATH). * INDICATES THE GLOBAL BEST IN ALL MODELS. p-VALUE INDICATES THE STATISTICALLY SIGNIFICANT IMPROVEMENT (I.E., T-TEST WITH p &lt; 0.05) OVER THE BEST BASELINE.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell>ACM</cell><cell></cell><cell></cell><cell></cell><cell>IMDB</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell></row><row><cell>HAN</cell><cell>93.17?0.19</cell><cell>93.64?0.17</cell><cell>44</cell><cell>0.23</cell><cell>87.68?1.94</cell><cell>87.73?1.81</cell><cell>31</cell><cell>0.25</cell><cell>59.70?0.90</cell><cell>65.61?0.54</cell><cell>13</cell><cell>0.08</cell></row><row><cell>GTN</cell><cell>93.52?0.55</cell><cell>93.97?0.54</cell><cell>13600</cell><cell>340</cell><cell>91.63?1.27</cell><cell>91.53?1.30</cell><cell>3234</cell><cell>77</cell><cell>59.26?0.84</cell><cell>64.07?0.65</cell><cell>9960</cell><cell>249</cell></row><row><cell>HetSANN</cell><cell>84.08?1.01</cell><cell>84.96?0.88</cell><cell>201</cell><cell>0.93</cell><cell>90.09?1.06</cell><cell>90.00?1.02</cell><cell>470</cell><cell>1.50</cell><cell>49.25?0.57</cell><cell>57.47?1.12</cell><cell>520</cell><cell>1.13</cell></row><row><cell>HGCA</cell><cell>93.05?0.46</cell><cell>93.62?0.41</cell><cell>495</cell><cell>55</cell><cell>91.75?0.54</cell><cell>91.67?0.56</cell><cell>30</cell><cell>1.5</cell><cell>43.54?1.17</cell><cell>53.44?1.00</cell><cell>56</cell><cell>2.8</cell></row><row><cell>MAGNN</cell><cell>93.16?0.38</cell><cell>93.65?0.34</cell><cell>401</cell><cell>19</cell><cell>91.06?1.44</cell><cell>90.95?1.43</cell><cell>230</cell><cell>23</cell><cell>56.92?1.76</cell><cell>65.11?0.59</cell><cell>108</cell><cell>9.8</cell></row><row><cell>MAGNN-AutoAC</cell><cell>93.95?0.30</cell><cell>94.39?0.25</cell><cell>432</cell><cell>21</cell><cell>91.84?0.45</cell><cell>91.77?0.45</cell><cell>684</cell><cell>25</cell><cell>58.96?1.31</cell><cell>66.11?0.53</cell><cell>576</cell><cell>11</cell></row><row><cell>HGT</cell><cell>92.77?0.35</cell><cell>93.44?0.31</cell><cell>131</cell><cell>1.87</cell><cell>90.27?0.55</cell><cell>90.14?0.51</cell><cell>545</cell><cell>7.07</cell><cell>63.02?0.80</cell><cell>67.01?0.36</cell><cell>257</cell><cell>3.38</cell></row><row><cell>HetGNN</cell><cell>92.77?0.24</cell><cell>93.23?0.23</cell><cell>20580</cell><cell>98</cell><cell>84.93?0.78</cell><cell>84.83?0.76</cell><cell>25410</cell><cell>121</cell><cell>47.87?0.33</cell><cell>50.83?0.26</cell><cell>18270</cell><cell>87</cell></row><row><cell>GCN</cell><cell>90.54?0.27</cell><cell>91.18?0.25</cell><cell>29</cell><cell>0.09</cell><cell>92.63?0.23</cell><cell>92.60?0.22</cell><cell>26</cell><cell>0.08</cell><cell>59.95?0.72</cell><cell>65.35?0.35</cell><cell>10</cell><cell>0.11</cell></row><row><cell>GAT</cell><cell>92.96?0.35</cell><cell>93.46?0.35</cell><cell>14</cell><cell>0.14</cell><cell>92.41?0.84</cell><cell>92.39?0.84</cell><cell>29</cell><cell>0.14</cell><cell>56.95?1.55</cell><cell>64.24?0.55</cell><cell>10</cell><cell>0.21</cell></row><row><cell>SimpleHGN</cell><cell>93.83?0.18</cell><cell>94.25?0.19</cell><cell>43</cell><cell>0.39</cell><cell>92.92?0.67</cell><cell>92.85?0.68</cell><cell>42</cell><cell>0.47</cell><cell>62.98?1.66</cell><cell>67.42?0.42</cell><cell>25</cell><cell>0.36</cell></row><row><cell>SimpleHGN-AutoAC</cell><cell>95.15?0.29*</cell><cell>95.52?0.26*</cell><cell>72</cell><cell>0.58</cell><cell cols="2">93.86?0.18* 93.80?0.18*</cell><cell>108</cell><cell>0.62</cell><cell cols="2">64.92?0.58* 67.94?0.41*</cell><cell>72</cell><cell>0.55</cell></row><row><cell>p-value</cell><cell>2.9 ? e -8</cell><cell>3.3 ? e -9</cell><cell>-</cell><cell>-</cell><cell>1.6 ? e -6</cell><cell>2.9 ? e -6</cell><cell>-</cell><cell>-</cell><cell>1.4 ? e -6</cell><cell>9.8 ? e -6</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON BETWEEN AUTOAC AND HGNNAC.THE BOLD AND THE UNDERLINED INDICATE THE BEST AND THE SECOND BEST IN EACH CATEGORY. p-VALUE INDICATES THE STATISTICALLY SIGNIFICANT IMPROVEMENT (I.E., T-TEST WITH p &lt; 0.05) OVER THE BEST BASELINE.</figDesc><table><row><cell>Dataset</cell><cell cols="2">DBLP</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell></row><row><cell>Model \ Metrics</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell></row><row><cell>MAGNN</cell><cell>93.16?0.38</cell><cell>93.65?0.34</cell><cell>91.06?1.44</cell><cell>90.95?1.43</cell><cell>56.92?1.76</cell><cell>65.11?0.59</cell></row><row><cell>MAGNN-HGNNAC</cell><cell cols="2">92.97?0.72 93.43?0.69</cell><cell cols="2">90.89?0.87 90.83?0.87</cell><cell cols="2">56.63?0.81 63.85?0.85</cell></row><row><cell>MAGNN-AutoAC</cell><cell cols="2">93.95?0.30 94.39?0.25</cell><cell cols="2">91.84?0.45 91.77?0.45</cell><cell cols="2">58.96?1.31 66.11?0.53</cell></row><row><cell>SimpleHGN</cell><cell>93.83?0.18</cell><cell>94.25?0.19</cell><cell cols="2">92.92?0.67 92.85?0.68</cell><cell cols="2">62.98?1.66 67.42?0.42</cell></row><row><cell>SimpleHGN-HGNNAC</cell><cell cols="2">93.24?0.49 93.73?0.45</cell><cell>93.16?0.24</cell><cell>93.09?0.23</cell><cell>64.44?1.13</cell><cell>67.67?0.39</cell></row><row><cell>SimpleHGN-AutoAC</cell><cell cols="2">95.15?0.29 95.52?0.26</cell><cell cols="2">93.86?0.18 93.80?0.18</cell><cell cols="2">64.92?0.58 67.94?0.41</cell></row><row><cell>p-value</cell><cell>2.9 ? e -8</cell><cell>3.3 ? e -9</cell><cell>7.3 ? e -7</cell><cell>1.1 ? e -6</cell><cell>4 ? e -3</cell><cell>1 ? e -3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV THE</head><label>IV</label><figDesc>OVERALL RUNTIME OVERHEAD (CLOCK TIME IN SECONDS) OF AUTOAC AND HGNN-AC. / INDICATES THAT THE STAGE IS NOT INVOLVED.</figDesc><table><row><cell>Datasets</cell><cell>Models</cell><cell cols="4">End-to-End Runtime Overhead (Seconds) Pre-learn Search Train/Retrain Total</cell><cell>Speedup</cell></row><row><cell>DBLP</cell><cell>SimpleHGN-HGNNAC SimpleHGN-AutoAC</cell><cell>33048 /</cell><cell>/ 36</cell><cell>432 36</cell><cell>33480 72</cell><cell>465?</cell></row><row><cell></cell><cell>MAGNN-HGNNAC MAGNN-AutoAC</cell><cell>33048 /</cell><cell>/ 72</cell><cell>900 360</cell><cell>33948 432</cell><cell>78?</cell></row><row><cell>ACM</cell><cell>SimpleHGN-HGNNAC SimpleHGN-AutoAC</cell><cell>3888 /</cell><cell>/ 72</cell><cell>432 36</cell><cell>4320 108</cell><cell>40?</cell></row><row><cell></cell><cell>MAGNN-HGNNAC MAGNN-AutoAC</cell><cell>3888 /</cell><cell>/ 432</cell><cell>1260 252</cell><cell>5148 684</cell><cell>7.5?</cell></row><row><cell>IMDB</cell><cell>SimpleHGN-HGNNAC SimpleHGN-AutoAC</cell><cell>8568 /</cell><cell>/ 36</cell><cell>324 36</cell><cell>8892 72</cell><cell>123?</cell></row><row><cell></cell><cell>MAGNN-HGNNAC MAGNN-AutoAC</cell><cell>8568 /</cell><cell>/ 504</cell><cell>180 72</cell><cell>8748 576</cell><cell>15?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>AND RUNTIME (CLOCK TIME IN SECONDS) COMPARISON ON LINK PREDICTION. THE BOLD AND THE UNDERLINE INDICATE THE BEST AND THE SECOND BEST. p-VALUE INDICATES THE STATISTICALLY SIGNIFICANT IMPROVEMENT (I.E., T-TEST WITH p &lt; 0.05) OVER THE BEST BASELINE.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">LastFM</cell><cell></cell><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell>IMDB</cell><cell></cell><cell></cell></row><row><cell>Model \ Metrics</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>Runtime (Total)</cell><cell>Runtime (Per epoch)</cell></row><row><cell>GATNE</cell><cell cols="2">66.87?0.16 85.93?0.63</cell><cell>75960</cell><cell>15435</cell><cell>71.94?2.00</cell><cell>87.23?0.76</cell><cell>92160</cell><cell>16278</cell><cell>47.45?6.48</cell><cell>74.58?3.34</cell><cell>71280</cell><cell>14269</cell></row><row><cell>HetGNN</cell><cell cols="2">62.09?0.01 85.56?0.14</cell><cell>20580</cell><cell>98</cell><cell>88.89?0.40</cell><cell>94.39?0.62</cell><cell>22050</cell><cell>105</cell><cell>56.55?0.83</cell><cell>78.10?0.56</cell><cell>19950</cell><cell>95</cell></row><row><cell>GCN</cell><cell cols="2">59.17?0.31 79.38?0.65</cell><cell>13</cell><cell>0.13</cell><cell>80.48?0.81</cell><cell>90.99?0.56</cell><cell>31</cell><cell>0.12</cell><cell>51.90?1.10</cell><cell>76.99?1.87</cell><cell>28</cell><cell>0.11</cell></row><row><cell>GAT</cell><cell cols="2">58.56?0.66 77.04?2.11</cell><cell>10</cell><cell>0.12</cell><cell>72.89?3.09</cell><cell>82.56?3.35</cell><cell>32</cell><cell>0.15</cell><cell>48.30?1.35</cell><cell>76.74?2.00</cell><cell>12</cell><cell>0.10</cell></row><row><cell>SimpleHGN</cell><cell>67.16?0.37</cell><cell>86.73?0.27</cell><cell>46</cell><cell>0.35</cell><cell>94.61?0.11</cell><cell>97.21 ?0.16</cell><cell>58</cell><cell>0.75</cell><cell>57.92?2.32</cell><cell>79.09 ?1.40</cell><cell>28</cell><cell>0.44</cell></row><row><cell>SimpleHGN-AutoAC</cell><cell cols="2">67.72?0.17 87.10?0.19</cell><cell>42</cell><cell>0.43</cell><cell>95.87?0.66</cell><cell>98.21?0.21</cell><cell>61</cell><cell>0.87</cell><cell>74.14?0.73</cell><cell>86.27?0.45</cell><cell>32</cell><cell>0.49</cell></row><row><cell>p-value</cell><cell>9.3 ? e -4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPLETION</head><label>VI</label><figDesc>OPERATION ABLATION STUDY ON SIMPLEHGN. BOLD INDICATES THE GLOBAL BEST. UNDERLINE INDICATES THE BEST AMONG ALL SINGLE ATTRIBUTE COMPLETION OPERATIONS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">DBLP</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell></row><row><cell>Model \ Metrics</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell></row><row><cell>Baseline (SimpleHGN)</cell><cell cols="2">93.83?0.18 94.25?0.19</cell><cell cols="2">92.92?0.67 92.85?0.68</cell><cell>62.98?1.66</cell><cell>67.42?0.42</cell></row><row><cell>GCN AC</cell><cell>94.23?0.21</cell><cell>94.88?0.23</cell><cell cols="2">93.25?0.45 93.18?0.47</cell><cell>64.67?0.94</cell><cell>67.96?0.53</cell></row><row><cell>PPNP AC</cell><cell cols="2">85.76?2.24 86.58?2.23</cell><cell>93.42?0.46</cell><cell>93.34?0.48</cell><cell cols="2">53.36?19.31 61.68?11.76</cell></row><row><cell>MEAN AC</cell><cell cols="2">90.91?0.72 91.53?0.67</cell><cell cols="2">92.99?0.60 92.90?0.62</cell><cell>63.73?0.94</cell><cell>67.61?0.30</cell></row><row><cell>One-hot AC</cell><cell cols="2">93.80?0.13 94.30?0.14</cell><cell cols="2">93.38?0.16 93.31?0.15</cell><cell>64.17?0.83</cell><cell>67.89?0.24</cell></row><row><cell>Random AC</cell><cell cols="2">91.28?1.63 91.77?1.55</cell><cell cols="2">93.02?0.29 92.95?0.31</cell><cell>64.03?0.68</cell><cell>67.43?0.33</cell></row><row><cell>AutoAC</cell><cell cols="2">95.15?0.29 95.52?0.26</cell><cell cols="2">93.86?0.18 93.80?0.18</cell><cell>64.92?0.58</cell><cell>67.94?0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPLETION</head><label>VII</label><figDesc>OPERATION ABLATION STUDY ON MAGNN. BOLD INDICATES THE GLOBAL BEST. UNDERLINE INDICATES THE BEST AMONG ALL SINGLE ATTRIBUTE COMPLETION OPERATIONS.</figDesc><table><row><cell>Dataset</cell><cell cols="2">DBLP</cell><cell cols="2">ACM</cell><cell cols="2">IMDB</cell></row><row><cell>Model \ Metrics</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell></row><row><cell>Baseline (MAGNN)</cell><cell>93.16?0.38</cell><cell>93.65?0.34</cell><cell cols="2">91.06?1.44 90.95?1.43</cell><cell>56.92?1.76</cell><cell>65.11?0.59</cell></row><row><cell>GCN AC</cell><cell>93.74?0.34</cell><cell>94.16?0.34</cell><cell cols="2">90.96?0.77 90.87?0.76</cell><cell cols="2">57.96?1.11 65.71?0.50</cell></row><row><cell>PPNP AC</cell><cell>93.46?0.32</cell><cell>93.94?0.29</cell><cell cols="2">90.38?0.67 90.28?0.67</cell><cell>58.46?1.17</cell><cell>65.97?0.56</cell></row><row><cell>MEAN AC</cell><cell>93.89?0.12</cell><cell>94.33?0.13</cell><cell cols="2">90.97?0.48 90.86?0.49</cell><cell cols="2">57.60?0.71 65.42?0.38</cell></row><row><cell>One-hot AC</cell><cell>93.73?0.32</cell><cell>94.15?0.28</cell><cell>91.04?0.69</cell><cell>90.92?0.70</cell><cell>58.12?1.71</cell><cell>65.43?0.68</cell></row><row><cell>Random AC</cell><cell>93.38?0.25</cell><cell>93.87?0.19</cell><cell cols="2">91.09?0.61 90.98?0.63</cell><cell cols="2">57.97?1.15 65.57?0.77</cell></row><row><cell>AutoAC</cell><cell>93.95?0.30</cell><cell>94.39?0.25</cell><cell cols="2">91.84?0.45 91.77?0.45</cell><cell cols="2">58.96?1.31 66.11?0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII ABLATION</head><label>VIII</label><figDesc>STUDY ON DISCRETE CONSTRAINTS. / INDICATES MEMORY OVERFLOW.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell>ACM</cell><cell></cell><cell></cell><cell>IMDB</cell><cell></cell></row><row><cell>Model \ Metrics</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Search Time (Seconds)</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Search Time (Seconds)</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Search Time (Seconds)</cell></row><row><cell>SimpleHGN-AutoAC</cell><cell cols="2">95.15?0.29 95.52?0.26</cell><cell>32</cell><cell cols="2">93.86?0.18 93.80?0.18</cell><cell>72</cell><cell cols="2">64.92?0.58 67.94?0.41</cell><cell>36</cell></row><row><cell>w/o Discrete constraints</cell><cell cols="2">95.12?0.27 95.49?0.25</cell><cell>216</cell><cell cols="2">93.43?0.74 93.34?0.76</cell><cell>360</cell><cell cols="2">64.74?0.68 67.85?0.52</cell><cell>180</cell></row><row><cell>MAGNN-AutoAC</cell><cell cols="2">93.95?0.30 94.39?0.25</cell><cell>72</cell><cell cols="2">91.84?0.45 91.77?0.45</cell><cell>432</cell><cell cols="2">58.96?1.31 66.11?0.53</cell><cell>504</cell></row><row><cell>w/o Discrete constraints</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell cols="2">91.24?0.67 91.45?0.68</cell><cell>1800</cell><cell cols="2">58.44?1.12 65.65?0.34</cell><cell>1908</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX PERFORMANCE</head><label>IX</label><figDesc>OF SIMPLEHGN-AUTOAC WITH VARYING ATTRIBUTE MISSING RATES IN THE NODE CLASSIFICATION TASK.</figDesc><table><row><cell>Datasets</cell><cell>Attribute Missing Rates</cell><cell>Node Types with Missing attributes</cell><cell>Macro-F1</cell><cell>Micro-F1</cell></row><row><cell></cell><cell>0%</cell><cell>/</cell><cell>93.83?0.18</cell><cell>94.25?0.19</cell></row><row><cell>DBLP</cell><cell>15%</cell><cell>author</cell><cell>94.35?0.17</cell><cell>94.72?0.16</cell></row><row><cell></cell><cell>30%</cell><cell>term, venue</cell><cell>95.09?0.13</cell><cell>95.47?0.12</cell></row><row><cell></cell><cell>45%</cell><cell>author, term, venue</cell><cell>95.15?0.29</cell><cell>95.52?0.26</cell></row><row><cell></cell><cell>0%</cell><cell>/</cell><cell>92.92?0.67</cell><cell>92.85?0.68</cell></row><row><cell>ACM</cell><cell>17%</cell><cell>subject, term</cell><cell>93.10?0.27</cell><cell>93.14?0.26</cell></row><row><cell></cell><cell>54%</cell><cell>author, subject</cell><cell>93.55?0.20</cell><cell>93.47?0.21</cell></row><row><cell></cell><cell>69%</cell><cell>author, subject, term</cell><cell>93.86?0.18</cell><cell>93.80?0.18</cell></row><row><cell></cell><cell>0%</cell><cell>/</cell><cell>62.98?1.66</cell><cell>67.42?0.42</cell></row><row><cell>IMDB</cell><cell>37%</cell><cell>keyword</cell><cell>63.65?0.57</cell><cell>67.52?0.36</cell></row><row><cell></cell><cell>67%</cell><cell>actor, keyword</cell><cell>64.59?0.53</cell><cell>67.86?0.42</cell></row><row><cell></cell><cell>76%</cell><cell cols="2">director, actor, keyword 64.92?0.58</cell><cell>67.94?0.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X PERFORMANCE</head><label>X</label><figDesc>OF SIMPLEHGN-AUTOAC WITH VARYING MASKED EDGE RATES IN THE LINK PREDICTION TASK.</figDesc><table><row><cell cols="3">Datasets Masked Edge Rates Models</cell><cell>ROC-AUC</cell><cell>MRR</cell></row><row><cell></cell><cell>5%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 97.62?0.36 99.02?0.24 95.92?0.56 97.16?0.44</cell></row><row><cell>DBLP</cell><cell>10% 20%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 95.87?0.66 98.21?0.21 94.61?0.11 97.21?0.16 SimpleHGN 91.34?0.61 95.65?0.41 SimpleHGN-AutoAC 94.08?0.72 97.61?0.33</cell></row><row><cell></cell><cell>30%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 91.11?0.67 97.42?0.44 88.76?0.66 95.39?0.24</cell></row><row><cell></cell><cell>5%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 86.57?1.36 92.75?0.84 64.89?0.58 81.86?0.94</cell></row><row><cell>IMDB</cell><cell>10% 20%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 74.14?0.73 86.27?0.45 57.92?2.32 79.09?1.40 SimpleHGN 58.21?0.39 79.71?0.34 SimpleHGN-AutoAC 73.75?0.82 86.25?0.32</cell></row><row><cell></cell><cell>30%</cell><cell cols="2">SimpleHGN SimpleHGN-AutoAC 65.81?0.31 83.23?0.21 54.13?0.79 77.57?0.67</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://dblp.uni-trier.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>http://dl.acm.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://www.imdb.com</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Details of Datasets DBLP 3 is a computer science bibliography website. The raw attribute of the paper node is the bag-of-words representation of keywords. ACM 4 is a citation network. The raw attribute of the paper node is also the bag-of-words representation of keywords. IMDB 5 is a website about movies. The attributes of movie nodes are originally present, they are represented by the bag-of-words representation of words extracted for key episodes of movies. LastFM is extracted from last.fm with timestamps from January 2015 to June 2015. We use the subset released by <ref type="bibr" target="#b47">[48]</ref>. The target is to predict whether a user likes a certain artist. The raw attribute of the artist node is the onehot encoding. For the DBLP dataset, the attributes of the target nodes are missing. For the ACM and IMDB datasets, the target nodes have raw attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementations and Configurations of Baselines</head><p>We use the HGB benchmark to evaluate the performance of all baselines. In HGB, implementations of baselines are based on their official codes to avoid errors introduced by re-implementation. Next, we present the configurations of baselines in the node classification and link prediction tasks, respectively. For brevity, we denote the dimension of node embedding as d, the dimension of edge embedding as d e , the dimension of attention vector (if exists) as d a , the number of GNN layers as L, the number of attention heads as n h , the negative slope of LeakyReLU as s.</p><p>1) Node Classification: The baselines in the node classification task contain HAN, GTN, HetSANN, MAGNN, HGCA, HGT, HetGNN, GCN, GAT, SimpleHGN, and HGNN-AC. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2017">190i198, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: a structural analysis approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of heterogeneous information network analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Medical entity disambiguation using graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vretinaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>?zcan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3457328</idno>
		<ptr target="https://doi.org/10.1145/3448016.3457328" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Management of Data, ser. SIGMOD &apos;21</title>
		<meeting>the 2021 International Conference on Management of Data, ser. SIGMOD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relation structureaware heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1534" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An attentionbased graph neural network for heterogeneous structural learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4132" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network via attribute completion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021, 2021</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6664" to="6671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entity resolution with hierarchical graph attention networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.1145/3514221.3517872</idno>
		<ptr target="https://doi.org/10.1145/3514221.3517872" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 International Conference on Management of Data, ser. SIGMOD &apos;22</title>
		<meeting>the 2022 International Conference on Management of Data, ser. SIGMOD &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="429" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reliable data distillation on graph convolutional network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3389706</idno>
		<ptr target="https://doi.org/10.1145/3318464.3389706" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD &apos;20</title>
		<meeting>the 2020 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1399" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Asgcn: Adaptive semantic architecture of graph convolutional networks for text-rich networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing heterogeneous networks with missing attributes by unsupervised contrastive learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Node attribute completion in knowledge graphs with multi-relational propagation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3590" to="3594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1997" to="2017" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automated graph machine learning: Approaches, libraries and directions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01288</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Policy-gnn: Aggregation optimization for graph neural networks</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="461" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Simplifying architecture search for graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11652</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Psp: Progressive space pruning for efficient graph neural architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 38th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2168" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Genetic meta-structure search for recommendation on heterogeneous information network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Diffmg: Differentiable meta graph search for heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends? in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Statist. Probability</title>
		<imprint>
			<biblScope unit="page" from="281" to="297" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
	<note>in 5th Berkeley Symp</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance of modularity maximization in practical contexts</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>De Montjoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph clustering with graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16904</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Second workshop on information heterogeneity and fusion in recommender systems (hetrec2011)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuflik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM conference on Recommender systems</title>
		<meeting>the fifth ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="387" to="388" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
