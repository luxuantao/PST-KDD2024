<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELECTION VIA PROXY: EFFICIENT DATA SELECTION FOR DEEP LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-18">18 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Yeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SELECTION VIA PROXY: EFFICIENT DATA SELECTION FOR DEEP LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-18">18 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.11829v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide useful signals for data selection. We evaluate this "selection via proxy" (SVP) approach on several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet, Amazon Review Polarity, and Amazon Review Full. For active learning, applying SVP can give an order of magnitude improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points) without significantly increasing the final error (often within 0.1%). For core-set selection on CIFAR10, proxies that are over 10× faster to train than their larger, more accurate targets can remove up to 50% of the data without harming the final accuracy of the target, leading to a 1.6× end-to-end training time improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data selection methods, such as active learning and core-set selection, improve the data efficiency of machine learning by identifying the most informative training examples. To quantify informativeness, these methods depend on semantically meaningful features or a trained model to calculate uncertainty. Concretely, active learning selects points to label from a large pool of unlabeled data by repeatedly training a model on a small pool of labeled data and selecting additional examples to label based on the model's uncertainty (e.g., the entropy of predicted class probabilities) or other heuristics <ref type="bibr" target="#b30">(Settles, 2011;</ref><ref type="bibr" target="#b31">2012;</ref><ref type="bibr" target="#b21">Lewis &amp; Gale, 1994)</ref>. Conversely, core-set selection techniques start with a large labeled or unlabeled dataset and aim to find a small subset that accurately approximates the full dataset by selecting representative examples <ref type="bibr" target="#b7">(Har-Peled &amp; Kushal, 2007;</ref><ref type="bibr" target="#b39">Tsang et al., 2005;</ref><ref type="bibr" target="#b13">Huggins et al., 2016;</ref><ref type="bibr" target="#b0">Campbell &amp; Broderick, 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018)</ref>.</p><p>Unfortunately, classical data selection methods are often prohibitively expensive to apply in deep learning <ref type="bibr" target="#b33">(Shen et al., 2017;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018;</ref><ref type="bibr" target="#b17">Kirsch et al., 2019)</ref>. Deep learning models learn complex internal semantic representations (hidden layers) from raw inputs (e.g., pixels or characters) that enable them to achieve state-of-the-art performance but result in substantial training times. Many core-set selection and active learning techniques require some feature representation before they can accurately identify informative points either to take diversity into account or as part of a trained model to quantify uncertainty. As a result, new deep active learning methods request labels in large batches to avoid retraining the model too many times <ref type="bibr" target="#b33">(Shen et al., 2017;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018;</ref><ref type="bibr" target="#b17">Kirsch et al., 2019)</ref>. However, batch active learning still requires training a full deep model for every batch, which is costly for large models <ref type="bibr" target="#b9">(He et al., 2016b;</ref><ref type="bibr" target="#b16">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b41">Vaswani et al., 2017)</ref>. Similarly, core-set selection applications mitigate the training time of deep learning models by using bespoke combinations of hand-engineered features and simple models (e.g., hidden Markov models) pretrained on auxiliary tasks <ref type="bibr" target="#b42">(Wei et al., 2013;</ref><ref type="bibr">2014;</ref><ref type="bibr" target="#b40">Tschiatschek et al., 2014;</ref><ref type="bibr">Ni et al., 2015)</ref>.</p><p>In this paper, we propose selection via proxy (SVP) as a way to make existing data selection methods more computationally efficient for deep learning. SVP uses the feature representation from a separate, less computationally intensive proxy model in place of the representation from the much larger and more accurate target model we aim to train. SVP builds on the idea of heterogeneous uncertainty sampling from <ref type="bibr" target="#b20">Lewis &amp; Catlett (1994)</ref>, which showed that an inexpensive classifier (e.g., naïve Bayes) can select points to label for a much more computationally expensive classifier (e.g., decision tree). In our work, we show that small deep learning models can similarly serve as an inexpensive proxy for data selection in deep learning, significantly accelerating both active learning and core-set selection across a range of datasets and selection methods. To create these cheap proxy models, we can scale down deep learning models by removing layers, using smaller model architectures, and training them for fewer epochs. While these scaled-down models achieve significantly lower accuracy than larger models, we surprisingly find that they still provide useful representations to rank and select points. Specifically, we observe high Spearman's and Pearson's correlations between the rankings from small proxy models and the larger, more accurate target models on metrics including uncertainty <ref type="bibr" target="#b31">(Settles, 2012)</ref>, forgetting events <ref type="bibr" target="#b38">(Toneva et al., 2019)</ref>, and submodular algorithms such as greedy k-centers <ref type="bibr" target="#b45">(Wolf, 2011)</ref>. Because these proxy models are quick to train (often 10× faster), we can identify which points to select nearly as well as the larger target model but significantly faster.</p><p>We empirically evaluated SVP for active learning and core-set selection on five datasets: CIFAR10, CIFAR100 <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009)</ref>, ImageNet <ref type="bibr" target="#b27">(Russakovsky et al., 2015)</ref>, Amazon Review Polarity, and Amazon Review Full <ref type="bibr">(Zhang et al., 2015)</ref>. For active learning, we considered both least confidence uncertainty sampling <ref type="bibr" target="#b31">(Settles, 2012;</ref><ref type="bibr" target="#b33">Shen et al., 2017;</ref><ref type="bibr" target="#b5">Gal et al., 2017)</ref> and the greedy k-centers approach from <ref type="bibr" target="#b29">Sener &amp; Savarese (2018)</ref> with a variety of proxies. Across all datasets, we found that SVP matches the accuracy of the traditional approach of using the same large model for both selecting points and the final prediction task. Depending on the proxy, SVP yielded up to a 7× speed-up on CIFAR10 and CIFAR100, 41.9× speed-up on Amazon Review Polarity and Full, and 2.9× speed-up on ImageNet in data selection runtime (i.e., the time it takes to repeatedly train and select points). For core-set selection, we tried three methods to identify a subset of points: max entropy uncertainty sampling <ref type="bibr" target="#b31">(Settles, 2012)</ref>, greedy k-centers as a submodular approach <ref type="bibr" target="#b45">(Wolf, 2011)</ref>, and the recent approach of forgetting events <ref type="bibr" target="#b38">(Toneva et al., 2019)</ref>. For each method, we found that smaller proxy models have high Spearman's rank-order correlations with models that are 10× larger and performed as well as these large models at identifying subsets of points to train on that yield high test accuracy. On CIFAR10, SVP applied to forgetting events removed 50% of the data without impacting the accuracy of ResNet164 with pre-activation <ref type="bibr" target="#b9">(He et al., 2016b)</ref>, using a 10× faster model than ResNet164 to make the selection. This substitution yielded an end-to-end training time improvement of about 1.6× for ResNet164 (including the time to train and use the proxy). Taken together, these results demonstrate that SVP is a promising, yet simple approach to make data selection methods computationally feasible for deep learning. While we focus on active learning and core-set selection, SVP is widely applicable to methods that depend on learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>In this section, we describe SVP and show how it can be incorporated into active learning and core-set selection. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of SVP: in active learning, we retrain a proxy model A P k in place of the target model A T k after each batch is selected, and in core-set selection, we train the proxy A P</p><p>[n] rather than the target A T</p><p>[n] over all the data to learn a feature representation and select points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ACTIVE LEARNING</head><p>Pool-based active learning starts with a large pool of unlabeled data U = {x i } i∈ <ref type="bibr">[n]</ref> where [n] = {1, . . . , n}. Each example is from the space X with an unknown label from the label space Y and is sampled i.i.d. over the space Z = X × Y as (x i , y i ) ∼ p Z . Initially, methods label a small pool of points s 0 = {s 0 j ∈ [n]} j∈[m] chosen uniformly at random. Given U , a loss function , and the labels {y s 0 j } j∈ <ref type="bibr">[m]</ref> for the initial random subset, the goal of active learning is to select up to a budget of b  In active learning, we followed the same iterative procedure of training and selecting points to label as traditional approaches but replaced the target model with a cheaper-to-compute proxy model. For core-set selection, we learned a feature representation over the data using a proxy model and used it to select points to train a larger, more accurate model. In both cases, we found the proxy and target model have high rank-order correlation, leading to similar selections and downstream results.</p><formula xml:id="formula_0">points s = s 0 ∪ {s j ∈ [n] \ s 0 } j∈[b−m] to label that produces a model A s with low error. Traditional Approach (a) Active Learning Select Subset (! " ∪ ! $ ) Initial Subset (! " ) Train Target (%" &amp; ) Train Target (%'($ &amp; ) Select Subset (… ∪ ! ' ) Train Target (%' &amp; ) … Data Selection Select Subset (! " ∪ ! $ ) Initial Subset (! " ) Select Subset (… ∪ ! ' ) Train Target (%' &amp; ) … Data Selection</formula><p>Baseline. In this paper, we apply SVP to least confidence uncertainty sampling <ref type="bibr" target="#b31">(Settles, 2012;</ref><ref type="bibr" target="#b33">Shen et al., 2017;</ref><ref type="bibr" target="#b5">Gal et al., 2017)</ref> and the recent greedy k-centers approach from <ref type="bibr" target="#b29">Sener &amp; Savarese (2018)</ref>. Like recent work for deep active learning <ref type="bibr" target="#b33">(Shen et al., 2017;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018;</ref><ref type="bibr" target="#b17">Kirsch et al., 2019)</ref>, we consider a batch setting with K rounds where we select b K points in every round. Following <ref type="bibr" target="#b5">Gal et al. (2017)</ref>; <ref type="bibr" target="#b29">Sener &amp; Savarese (2018)</ref>; <ref type="bibr" target="#b17">Kirsch et al. (2019)</ref>, we reinitialize the target model and retrain on all of the labeled data from the previous k rounds to avoid any correlation between selections <ref type="bibr" target="#b4">(Frankle &amp; Carbin, 2018;</ref><ref type="bibr" target="#b17">Kirsch et al., 2019)</ref>. We denote this trained model as A T s 0 ∪...∪s k or just A T k for simplicity. Then using A T k , we either calculate the model's confidence as:</p><formula xml:id="formula_1">f confidence (x; A T k ) = 1 − max ŷ P (ŷ|x; A T k )</formula><p>and select the examples with the lowest confidence or extract a feature representation from the model's final hidden layer and compute the distance between examples (i.e., ∆(x i , x j ; A T k )) to select points according to the greedy k-centers method from <ref type="bibr" target="#b45">Wolf (2011)</ref>; <ref type="bibr" target="#b29">Sener &amp; Savarese (2018)</ref> (Algorithm 1). The same model is trained on the final b labeled points to yield the final model, A T K , which is then tested on a held-out set to evaluate error and quantify the quality of the selected data.</p><p>Although other selection approaches exist, least confidence uncertainty sampling and greedy k-centers cover the spectrum of uncertainty-based and representativeness-based approaches for deep active learning. Other uncertainty metrics such as entropy or margin were highly correlated with confidence when using the same trained model (i.e., above a 0.96 Spearman's correlation in our experiments on CIFAR). Query-by-committee <ref type="bibr" target="#b32">(Seung et al., 1992</ref>) can be prohibitively expensive in deep learning, where training a single model is already costly. BALD <ref type="bibr" target="#b11">(Houlsby et al., 2011)</ref> has seen success in deep learning <ref type="bibr" target="#b5">(Gal et al., 2017;</ref><ref type="bibr" target="#b33">Shen et al., 2017)</ref> but is restricted to Bayesian neural networks or networks with dropout <ref type="bibr" target="#b35">(Srivastava et al., 2014)</ref> as an approximation <ref type="bibr" target="#b5">(Gal &amp; Ghahramani, 2016)</ref>. prev_acc i = acc i 10:</p><p>Gradient update classifier on B 11: return forgetting_events 2.2 CORE-SET SELECTION Core-set selection can be broadly defined as techniques that find a subset of data points that maintain a similar level of quality (e.g., generalization error of a trained model or minimum enclosing ball) as the full dataset. Specifically, we start with a labeled dataset L = {x i , y i } i∈[n] sampled i.i.d. from Z with p Z and want to find a subset of m ≤ n points s = {s j ∈ [n]} j∈[m] that achieves comparable quality to the full dataset:</p><formula xml:id="formula_2">min s:|s|=m E x,y∼p Z [ (x, y; A s )] − E x,y∼p Z (x, y; A [n] )</formula><p>Baseline. To find s for a given budget m, we implement three core-set selection techniques: greedy kcenters <ref type="bibr" target="#b45">(Wolf, 2011;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018)</ref>, forgetting events <ref type="bibr" target="#b38">(Toneva et al., 2019)</ref>, and max entropy uncertainty sampling <ref type="bibr" target="#b21">(Lewis &amp; Gale, 1994;</ref><ref type="bibr" target="#b31">Settles, 2012)</ref>. Greedy k-centers is described above and in Algorithm 1. Forgetting events are defined as the number of times an example is incorrectly classified after having been correctly classified earlier during training a model, as described in Algorithm 2. To select points, we follow the same procedure as <ref type="bibr" target="#b38">Toneva et al. (2019)</ref>: we keep the points with the m highest number of forgetting events. Points that are never correctly classified are treated as having an infinite number of forgetting events. Similarly, we rank examples based on the entropy from a trained target A T</p><p>[n] as:</p><formula xml:id="formula_3">f entropy (x; A T [n] ) = − ŷ P (ŷ|x; A T [n] ) log P (ŷ|x; A T [n] )</formula><p>and keep the m examples with the highest entropy. To evaluate core-set quality, we compare the performance of training the large target model on the selected subset A T s to training the target model on the entire dataset A T</p><p>[n] by measuring error on a held-out test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">APPLYING SELECTION VIA PROXY</head><p>In general, SVP can be applied by replacing the models used to compute data selection metrics such as uncertainty with proxy models. In this paper, we applied SVP to the active learning and core-set selection methods described in Sections 2.1 and 2.2 as follows:</p><p>• For active learning, we replaced the model trained at each batch (A T k ) with a proxy (A P k ), but then trained the same final model A T K once the budget b was reached. • For core-set selection, we used a proxy A P</p><p>[n] instead of A T</p><p>[n] to compute metrics and select s.</p><p>We explored two main methods to create our proxy models:</p><p>Scaling down. For deep models with many layers, reducing the dimension or the number of hidden layers is an easy way to trade-off accuracy to reduce training time. For example, deep ResNet models come in a variety of depths <ref type="bibr" target="#b9">(He et al., 2016b;</ref><ref type="bibr">a)</ref> and widths <ref type="bibr" target="#b47">(Zagoruyko &amp; Komodakis, 2016)</ref> that represent many points on the accuracy and training time curve. As shown in Figure <ref type="figure">4a</ref> in the Appendix, a ResNet20 model achieves a top-1 error of 7.6% on CIFAR10 in 26 minutes, while a larger ResNet164 model takes 4 hours and reduces error by 2.5%. Similar results have been shown for many other tasks, including neural machine translation <ref type="bibr" target="#b41">(Vaswani et al., 2017</ref>), text classification <ref type="bibr" target="#b2">(Conneau et al., 2016)</ref>, and recommendation <ref type="bibr" target="#b10">(He et al., 2017)</ref>. Looking across architectures gives even more options to reduce computational complexity. We exploit the limitless model architectures in deep learning to trade-off between accuracy and complexity to scale down to a proxy that can be trained quickly but still provides a good approximation of the target's decision boundary.</p><p>Training for fewer epochs. Similarly, a significant amount of training is spent on a relatively small reduction in error. While training ResNet20, almost half of the training time (i.e., 12 minutes out of 26 minutes) is spent on a 1.4% improvement in test error, as shown in Figure <ref type="figure">4a</ref> in the Appendix.</p><p>Based on this observation, we also explored training proxy models for a smaller number of epochs to get good approximations of the decision boundary of the target model even faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We applied SVP to data selection methods from active learning and core-set selection on five datasets. After a brief description of the datasets and models in Section 3.1, Section 3.2 evaluates SVP's impact on active learning and shows that across labeling budgets SVP achieved similar or higher accuracy and up to a 41.9× improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points). Next, we applied SVP to the core-set selection problem (Section 3.3). For all selection methods, the target model performed nearly as well as or better with SVP than the oracle that trained the target model on all of the data before selecting examples. On CIFAR10, a small proxy model trained for 50 epochs instead of 181 epochs took only 7 minutes compared to the 4 hours for training the target model for all 181 epochs, making SVP feasible for end-to-end training time speed-ups. Finally, Section 3.4 illustrates why proxy models performed so well by evaluating how varying models and methods rank examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EXPERIMENTAL SETUP</head><p>Datasets. We focused on classification as a well-studied task in the active learning literature (see Section A.1 for more detail). Our experiments included three image classification datasets: CIFAR10, CIFAR100 <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009)</ref>, and ImageNet <ref type="bibr" target="#b27">(Russakovsky et al., 2015)</ref>; and two text classification datasets: Amazon Review Polarity and Full <ref type="bibr">(Zhang &amp; LeCun, 2015;</ref><ref type="bibr">Zhang et al., 2015)</ref>. CIFAR10 is a coarse-grained classification task over 10 classes, and CIFAR100 is a fine-grained task with 100 classes. Both datasets contain 50,000 images for training and 10,000 images for testing.</p><p>ImageNet has 1.28 million training images and 50,000 validation images that belong to 1 of 1,000 classes. Amazon Review Polarity has 3.6 million reviews split evenly between positive and negative ratings with an additional 400,000 reviews for testing. Amazon Review Full has 3 million reviews split evenly between the 5 stars with an additional 650,000 reviews for testing.</p><p>Models. For CIFAR10 and CIFAR100, we used ResNet164 with pre-activation from <ref type="bibr" target="#b9">He et al. (2016b)</ref> as our large target model. The smaller, proxy models are also ResNet architectures with preactivation, but they use pairs of 3 × 3 convolutional layers as their residual unit rather than bottlenecks.</p><p>For ImageNet, we used the original ResNet architecture from <ref type="bibr" target="#b8">He et al. (2016a)</ref> implemented in PyTorch<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b25">(Paszke et al., 2017)</ref> with ResNet50 as the target and ResNet18 as the proxy. For Amazon Review Polarity and Amazon Review Full, we used VDCNN <ref type="bibr" target="#b3">(Conneau et al., 2017)</ref> and fastText <ref type="bibr" target="#b15">(Joulin et al., 2016)</ref> with VDCNN29 as the target and fastText and VDCNN9 as proxies. In general, we followed the same training procedure as the original papers (more details in Section A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ACTIVE LEARNING</head><p>We explored the impact of SVP on two active learning techniques: least confidence uncertainty sampling and the greedy k-centers approach from <ref type="bibr" target="#b29">Sener &amp; Savarese (2018)</ref>. Starting with an initial random subset of 2% of the data, we selected 8% of the remaining unlabeled data for the first round and 10% for subsequent rounds until the labeled data reached the budget b and retrained the models from scratch between rounds as described in Section 2.1. Across datasets, SVP sped up data selection without significantly impacting the final predictive performance of the target.</p><p>CIFAR10 and CIFAR100. For least confidence uncertainty sampling and greedy k-centers, SVP sped-up data selection by up to 7× and 3.8× respectively without impacting data efficiency (see Tables <ref type="table">1 and 3</ref>) despite the proxy achieving substantially higher top-1 error than the target ResNet164 model (see Figure <ref type="figure">6</ref> in the Appendix). The speed-ups for least confidence were a direct reflection of the difference in training time between the proxy in the target models. As shown in Figures 4 and 5 in the Appendix, ResNet20 was about 8× faster to train than ResNet164, taking 30 minutes to train rather than 4 hours. Larger budgets required more rounds of selection and, in turn, more training, which led to larger speed-ups as training became a more significant fraction of the total time. Training for fewer epochs provided a significant error reduction compared to random sampling but was not as good as training for the full schedule (see Table <ref type="table" target="#tab_4">4</ref> in the Appendix). For greedy k-centers, the speed-ups increased more slowly because executing the selection algorithm added more overhead.</p><p>ImageNet. For least confidence uncertainty sampling, SVP sped-up data selection by up to 1.6× (Table <ref type="table">1</ref>) despite ResNet18's higher error compared to ResNet50 (Figure <ref type="figure">6g</ref> in the Appendix).</p><p>Training for fewer epochs increased the speed-up to 2.9× without increasing the error rate of ResNet50 (Table <ref type="table" target="#tab_4">4</ref>). Greedy k-centers was too slow on ImageNet due to the quadratic complexity of Algorithm 1.</p><p>Table <ref type="table">1</ref>: SVP performance on active learning. Average (± 1 std.) data selection speed-ups from 3 runs of active learning using least confidence uncertainty sampling with varying proxies and labeling budgets on four datasets. Bold speed-ups indicate settings that either achieve lower error or are within 1 std. of the mean top-1 error for the baseline approach of using the same model for selection and the final predictions. Across datasets, SVP sped up selection without significantly increasing the error of the final target. Additional results and details are in Table <ref type="table">3</ref>.</p><formula xml:id="formula_4">Data Selection Speed-up Budget (b/n) 10% 20% 30% 40% 50% Dataset Selection Model CIFAR10 ResNet164 (Baseline) 1.0× 1.0× 1.0× 1.0× 1.0× ResNet110 1.8× 1.9× 1.9× 1.8× 1.8× ResNet56 2.6× 2.9× 3.0× 3.1× 3.1× ResNet20 3.8× 5.8× 6.7× 7.0× 7.2× CIFAR100 ResNet164 (Baseline) 1.0× 1.0× 1.0× 1.0× 1.0× ResNet110 1.5× 1.6× 1.6× 1.6× 1.6× ResNet56 2.4× 2.7× 3.0× 2.9× 3.1× ResNet20 4.0× 5.8× 6.6× 7.0× 7.2× ImageNet ResNet50 (Baseline) 1.0× 1.0× 1.0× 1.0× 1.0× ResNet18 1.2× 1.3× 1.4× 1.5× 1.6× Amazon VDCNN29 (Baseline) 1.0× 1.0× 1.0× 1.0× 1.0× Review VDCNN9 1.9× 1.8× 1.8× 1.8× 1.8× Polarity fastText 10.6× 20.6× 32.2× 41.9× 51.3×</formula><p>Amazon Review Polarity and Amazon Review Full. On Amazon Review Polarity, SVP with a fastText proxy for VDCNN29 led to up to a relative error reduction of 14% over random sampling for large budgets (Table <ref type="table">3</ref>), while being up to 41.9× faster at data selection than the baseline approach (Table <ref type="table">1</ref>). Despite fastText's architectural simplicity compared to VDCNN29 and higher error (Figure <ref type="figure">6e</ref>), the calculated confidences signaled which examples would be the most informative. For all budgets, VDCNN9 was within 0.1% top-1 error of VDCNN29, giving a consistent 1.8× speed-up. On Amazon Review Full, neither the baseline least confidence uncertainty sampling approach nor the application of SVP outperformed random sampling (see Table <ref type="table">3</ref> in the Appendix), so the data selection speed-ups were uninteresting even though they were similar to Amazon Review Polarity. For both datasets, greedy k-centers was too slow as mentioned above in the ImageNet experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CORE-SET SELECTION</head><p>CIFAR10 and CIFAR100. For all methods on both CIFAR10 and CIFAR100, SVP proxy models performed as well as or better than an oracle where ResNet164 itself is used as the core-set selection model, as shown in Figure <ref type="figure">2</ref> (and Figure <ref type="figure">7</ref> in the Appendix). Using forgetting events on CIFAR10, SVP with ResNet20 as the proxy removed 50% of the data without a significant increase in error from ResNet164.  <ref type="figure" target="#fig_7">11a</ref> and 12a in the Appendix). On CIFAR100, partial training did not work as well for proxies at large subset sizes because the ranking took longer to stabilize and were less correlated (Figure <ref type="figure" target="#fig_7">11b</ref> and Figure <ref type="figure" target="#fig_9">12b</ref> in the Appendix). On small 30% subsets with forgetting events, partial training improved accuracy on CIFAR100.</p><p>ImageNet. Neither the baseline approach nor SVP was able to remove a significant percentage of the data without increasing the final error of ResNet50, as shown in Table <ref type="table">5</ref> in the Appendix. However, the selected subsets from both ResNet18 and ResNet50 outperformed random sampling with up to a 1% drop in top-1 error using forgetting events. Note, due to the quadratic computational complexity of Algorithm 1, we were unable to run greedy k-centers in a reasonable amount of time. Amazon Review Polarity and Amazon Review Full. On Amazon Review Polarity, we were able to remove 20% of the dataset with only a 0.1% increase in VDCNN29's top-1 error using fastText as the proxy (see Table <ref type="table">5</ref>). In comparison to VDCNN29, which took 16 hours and 40 minutes to train over the entire dataset on a Titan V GPU, fastText was two orders of magnitude faster, taking less than 10 minutes on a CPU to train over the same data and compute output probabilities. This difference allowed us to train VDCNN29 to nearly the same error in 13 and a half hours. However, on Amazon Review Full, both the baseline approach and SVP failed to outperform random sampling. Similar to ImageNet, we were unable to run greedy k-centers in a reasonable amount of time, and additionally, Facebook's fastText implementation<ref type="foot" target="#foot_1">2</ref> did not allow us to compute forgetting events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RANKING CORRELATION BETWEEN MODELS</head><p>Models with fewer layers. Figure <ref type="figure" target="#fig_1">3</ref> (and Figure <ref type="figure">9</ref> in the appendix) shows the Spearman's rankorder correlation between ResNets of varying depth for three selection methods on CIFAR10 (and CIFAR100). For greedy k-centers, we started with 1,000 randomly selected points and ranked the remaining points based on the order they are added to set s in Algorithm 1. Across models, there was a positive correlation similar to the correlation between runs of the same model. For forgetting events and entropy, we ranked points in descending order based on the number of forgetting events and the entropy of the output predictions from the trained model, respectively. Both metrics had comparable positive correlations between different models and different runs of the same model. We also looked at the Pearson correlation coefficient for the number of forgetting events and entropy in Figure <ref type="figure" target="#fig_0">15</ref> in the Appendix and found a similar positive correlation. The consistent positive correlation between varying depths illustrates why small models are good proxies for larger models in data selection.</p><p>Published as a conference paper at ICLR 2020 Models with different architectures. We further investigated different model architectures by calculating the Spearman's correlation between pretrained ImageNet models and found that correlations were high across a wide range of models (Figure <ref type="figure" target="#fig_5">8</ref> in the Appendix). For example, MobileNet V2's <ref type="bibr" target="#b28">(Sandler et al., 2018)</ref> entropy-based rankings were highly correlated to ResNet50 (on par with ResNet18), even though the model had far fewer parameters (3.5M vs. 25.6M). In concert with our fastText and VDCNN results, the high correlations between different model architectures suggest that SVP might be widely applicable. While there are likely limits to how different architectures can be, there is a wide range of trade-offs between accuracy and computational complexity, even within a narrow spectrum of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Active learning. There are examples in the active learning literature that address the computational efficiency of active learning methods by using one model to select points for a different, more expensive model. For instance, <ref type="bibr" target="#b20">Lewis &amp; Catlett (1994)</ref> proposed heterogeneous uncertainty sampling and used a Naïve Bayes classifier to select points to label for a more expensive decision tree target model. <ref type="bibr" target="#b37">Tomanek et al. (2007)</ref> uses a committee-based active learning algorithm for an NLP task and notes that the set of selected points are "reusable" across different models (maximum entropy, conditional random field, naive Bayes). In our work, we showed that this can be generalized to deep learning by either using smaller models or fewer training epochs, where it can significantly reduce the running time of uncertainty-based <ref type="bibr" target="#b31">(Settles, 2012;</ref><ref type="bibr" target="#b33">Shen et al., 2017;</ref><ref type="bibr" target="#b5">Gal et al., 2017)</ref> and recent representativeness-based <ref type="bibr" target="#b29">(Sener &amp; Savarese, 2018</ref>) methods.</p><p>Core-set selection. Core-set selection attempts to find a representative subset of points to speed up learning or clustering; such as k-means and k-medians <ref type="bibr" target="#b7">(Har-Peled &amp; Kushal, 2007)</ref>, SVM <ref type="bibr" target="#b39">(Tsang et al., 2005)</ref>, Bayesian logistic regression <ref type="bibr" target="#b13">(Huggins et al., 2016)</ref>, and Bayesian inference <ref type="bibr" target="#b0">(Campbell &amp; Broderick, 2017;</ref><ref type="bibr">2018)</ref>. However, these examples generally require ready-to-use features as input, and do not directly apply to deep neural networks unless a feature representation is first learned, which usually requires training the full target model itself. There is also a body of work on data summarization based on submodular maximization <ref type="bibr" target="#b42">(Wei et al., 2013;</ref><ref type="bibr">2014;</ref><ref type="bibr" target="#b40">Tschiatschek et al., 2014;</ref><ref type="bibr">Ni et al., 2015)</ref>, but these techniques depend on a combination of hand-engineered features and simple models (e.g., hidden Markov models and Gaussian mixture models) pretrained on auxiliary tasks. In comparison, our work demonstrated that we can use the feature representations of smaller, faster-to-train proxy models as an effective way to select core-sets for deep learning tasks.</p><p>Recently, <ref type="bibr" target="#b38">Toneva et al. (2019)</ref> showed that a large number of "unforgettable" examples that are rarely incorrectly classified once learned (i.e., 30% on CIFAR10) could be omitted without impacting generalization, which can be viewed as a core-set selection method. They also provide initial evidence that forgetting events are transferable across models and throughout training by using the forgetting events from ResNet18 to select a subset for WideResNet <ref type="bibr" target="#b47">(Zagoruyko &amp; Komodakis, 2016)</ref> and by computing the Spearman's correlation of forgetting events during training compared to their final values. In our work, we evaluated a similar idea of using proxy models to approximate various properties of a large model, and showed that proxy models closely match the rankings of large models in the entropy, greedy k-centers, and example forgetting metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we introduced selection via proxy (SVP) to improve the computational efficiency of active learning and core-set selection in deep learning by substituting a cheaper proxy model's representation for an expensive model's during data selection. Applied to least confidence uncertainty sampling and Sener &amp; Savarese (2018)'s greedy k-centers approach, SVP achieved up to a 41.9× and 3.8× improvement in runtime respectively with no significant increase in error (often within 0.1%).</p><p>For core-set selection, we found that SVP can remove up to 50% of the data from CIFAR10 in 10× less time than it takes to train the target model, achieving a 1.6× speed-up in end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 CHOICE OF DATASETS For our experimental evaluation in Section 3, we focused on classification because it is a widely studied task in active learning <ref type="bibr" target="#b21">(Lewis &amp; Gale, 1994;</ref><ref type="bibr" target="#b20">Lewis &amp; Catlett, 1994;</ref><ref type="bibr" target="#b31">Settles, 2012;</ref><ref type="bibr" target="#b29">Sener &amp; Savarese, 2018;</ref><ref type="bibr" target="#b17">Kirsch et al., 2019;</ref><ref type="bibr" target="#b23">Mussmann &amp; Liang, 2018;</ref><ref type="bibr" target="#b5">Gal et al., 2017;</ref><ref type="bibr" target="#b11">Houlsby et al., 2011)</ref>. While there are a few bespoke solutions for machine translation <ref type="bibr" target="#b26">(Peris &amp; Casacuberta, 2018)</ref> and named entity recognition <ref type="bibr" target="#b33">(Shen et al., 2017)</ref>, we wanted to compare against the broader body of active learning research. Many popular active learning methods like uncertainty sampling (e.g., entropy, least confidence, and max-margin) assume a single categorical probability distribution for each example, which makes it hard to adapt to other domains. Instead of tackling open challenges for active learning like machine translation, we applied SVP to many classification datasets; however, the simplicity of SVP means that the core ideas of this paper can be applied more broadly in future work.</p><p>Specifically, we performed experiments on three image classification datasets: CIFAR10, CI-FAR100 <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009)</ref>, and ImageNet <ref type="bibr" target="#b27">(Russakovsky et al., 2015)</ref>; and two text classification datasets: Amazon Review Polarity and Full <ref type="bibr">(Zhang &amp; LeCun, 2015;</ref><ref type="bibr">Zhang et al., 2015)</ref>. While multiple tasks on roughly the same data distribution may seem redundant, the data efficiency of active learning depends on error <ref type="bibr" target="#b23">(Mussmann &amp; Liang, 2018)</ref>. We included both CIFAR10 (low-error) and CIFAR100 (high-error) to demonstrate that our approach performs as well as standard active learning at different points on the error and data efficiency curve. The same rationale is also valid for Amazon Review Polarity (low-error) and Full (high-error). However, the Amazon Review dataset adds a medium (text) and a much larger scale (3.6M and 3M examples, respectively). Adding ImageNet further allows us to investigate scale in the number of examples, but also the number of classes and the dimension of the input. To the best of our knowledge, we are the first active learning paper to present results on the full ImageNet classification task.</p><p>A.2 IMPLEMENTATION DETAILS CIFAR10 and CIFAR100. We used ResNet164 with pre-activation from <ref type="bibr" target="#b9">He et al. (2016b)</ref> as our large target model for both CIFAR10 and CIFAR100. Note that as originally proposed in <ref type="bibr" target="#b8">He et al. (2016a)</ref>, the smaller, proxy models are also ResNet architectures with pre-activation, but they use pairs of 3 × 3 convolutional layers as their residual unit rather than bottlenecks and achieve lower accuracy as shown in Figure <ref type="figure">4</ref>. As with <ref type="bibr" target="#b9">He et al. (2016b)</ref>, the ResNets we used were much narrower when applied to CIFAR rather than ImageNet (256 filters rather than 2048 in the final layer of the last bottleneck) and have fewer sections, which means far fewer weights despite the increased depth. For example, ResNet50 on ImageNet has ~25M weights while ResNet164 on CIFAR has ~1.7M (see Table <ref type="table" target="#tab_3">2</ref>). More recent networks such as Wide Residual Networks <ref type="bibr" target="#b47">(Zagoruyko &amp; Komodakis, 2016)</ref>, ResNeXt <ref type="bibr" target="#b46">(Xie et al., 2017)</ref>, and DenseNets <ref type="bibr" target="#b12">(Huang et al., 2017)</ref> use models with more than 25M parameters on CIFAR10, making ResNet164 relatively small in comparison. Core-set selection experiments used a single Nvidia P100 GPU, while the active learning experiments used a Titan V GPU. We followed the same training procedure, initialization, and hyperparameters as <ref type="bibr" target="#b9">He et al. (2016b)</ref> with the exception of weight decay, which was set to 0.0005 and decreased the model's validation error in all conditions.</p><p>ImageNet. we used the original ResNet architecture from <ref type="bibr" target="#b8">He et al. (2016a)</ref> implemented in Py-Torch<ref type="foot" target="#foot_3">3</ref>  <ref type="bibr" target="#b25">(Paszke et al., 2017)</ref> with ResNet50 as the target and ResNet18 as the proxy. For training, we used a custom machine with 4 Nvidia Titan V GPUs and followed Nvidia's optimized implementation<ref type="foot" target="#foot_4">4</ref> with a larger batch size, appropriately scaled learning rate <ref type="bibr" target="#b6">(Goyal et al., 2017)</ref>, a 5-epoch warm-up period, and mixed precision training <ref type="bibr" target="#b22">(Micikevicius et al., 2017)</ref> with the apex<ref type="foot" target="#foot_5">5</ref> library. For active learning, we used the same batch size of 768 images for both ResNet18 and ResNet50 for simplicity, which was the maximum batch size that could fit into memory for ResNet50. However, ResNet18 with a batch size of 768 underutilized the GPU and yielded a lower speed-up. With separate batch sizes for ResNet18 and ResNet50, we would have seen speed-ups closer to 2.7×. Amazon Review Polarity (2-classes) and Full (5-classes). For Amazon Review Polarity and Amazon Review Full, we used VDCNN <ref type="bibr" target="#b3">(Conneau et al., 2017)</ref> and fastText <ref type="bibr" target="#b15">(Joulin et al., 2016)</ref> with VDCNN29 as the target and fastText and VDCNN9 as proxies. For Amazon Review Polarity, core-set selection experiments used a single Nvidia P100 GPU, while the active learning experiments used a Nvidia Titan V GPU to train VDCNN models. For Amazon Review Full, core-set selection and active learning experiments both used a Nvidia Titan V GPU. In all settings, we used the same training procedure from <ref type="bibr" target="#b3">Conneau et al. (2017)</ref> for VDCNN9 and VDCNN29. For fastText, we used Facebook's implementation<ref type="foot" target="#foot_6">6</ref> and followed the same training procedure from <ref type="bibr" target="#b15">Joulin et al. (2016)</ref>.  Figure <ref type="figure">6</ref>: Quality of proxies compared to target models. Average (± 1 std.) top-1 error from 3 runs of active learning with varying proxies, selection methods, and budgets on five classification datasets. Dotted lines show the top-1 error of the proxy models, while solid lines show the top-1 error of the target models. CIFAR10 and CIFAR100 experiments used varying depths of pre-activation ResNet (R) models as proxies and ResNet164 (R164) as the target model (e.g., R20-R164 is ResNet20 selecting for ResNet164). ImageNet used ResNet18 (R18) as the proxy and ResNet50 (R50) as the target. Amazon Review Polarity and Amazon Review Full used VDCNN9 (V9) and fastText (FT) as proxies and VDCNN29 (V29) as the target. Across datasets, proxies, methods, and budgets, smaller proxies had higher top-1 error than the target model, but selecting points that were nearly as good as the points selected by the target that did not harm the final target model's predictive performance.</p><p>Table <ref type="table">3</ref>: SVP performance on active learning. Average (± 1 std.) top-1 error and data selection speed-ups from 3 runs of active learning with varying proxies, methods, and labeling budgets on five datasets. Bold speed-ups indicate settings that either achieve lower error or are within 1 std. of the mean top-1 error for the baseline approach of using the same model for selection and the final predictions. Across datasets and methods, SVP sped up selection without significantly increasing the error of the final target. 1.0×</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence ResNet110</head><p>18.1 ± 0.41 10.5 ± 0.06 7.5 ± 0.11 5.9 ± 0.33 Average (± 1 std.) top-1 error and data selection speed-ups from 3 runs of active learning with varying proxies trained for a varying number of epochs on CIFAR10, CIFAR100, and ImageNet. Bold speed-ups indicate settings that either achieve lower error or are within 1 std. of the mean top-1 error for the baseline approach of using the same model for selection and the final predictions. Training for fewer epochs can provide a significant improvement over random sampling but is not quite as good as training for the full schedule.   Correlations are high across a wide range of model architectures <ref type="bibr" target="#b46">(Xie et al., 2017;</ref><ref type="bibr" target="#b8">He et al., 2016a;</ref><ref type="bibr" target="#b28">Sandler et al., 2018;</ref><ref type="bibr" target="#b12">Huang et al., 2017;</ref><ref type="bibr" target="#b36">Szegedy et al., 2015;</ref><ref type="bibr" target="#b34">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b14">Iandola et al., 2016;</ref><ref type="bibr" target="#b19">Krizhevsky et al., 2012)</ref>. For example, MobileNet V2's entropy-based rankings were highly correlated to ResNet50, even though the model had far fewer parameters (3.5M vs. 25.6M).</p><formula xml:id="formula_5">5.3 ± 0.05 1.8× 1.9× 1.9× 1.8× 1.8×<label>ResNet56</label></formula><p>In concert with our fastText and VDCNN results from Section 3.2, the high correlations between different model architectures suggest that SVP might be widely applicable.        Rankings are calculated with forgetting events (top) and entropy (bottom). Notably, the ranking from forgetting events is much more stable because the model's uncertainty is effectively averaged throughout training rather than a single snapshot at the end like entropy. For t-SNE plots of the entire training run, please see http://bit.ly/svp-cifar10-tsne-entropy and http: //bit.ly/svp-cifar10-tsne-forget for entropy and forgetting events respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: SVP applied to active learning (left) and core-set selection (right). In active learning, we followed the same iterative procedure of training and selecting points to label as traditional approaches but replaced the target model with a cheaper-to-compute proxy model. For core-set selection, we learned a feature representation over the data using a proxy model and used it to select points to train a larger, more accurate model. In both cases, we found the proxy and target model have high rank-order correlation, leading to similar selections and downstream results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure2: SVP performance on core-set selection. Average (± 1 std.) top-1 error of ResNet164 over 5 runs of core-set selection with different selection methods, proxies, and subset sizes on CIFAR10. We found subsets using forgetting events (left), entropy (middle), and greedy k-centers (right) from a proxy model trained over the entire dataset. Across datasets and selection methods, SVP performed as well as an oracle baseline but significantly faster (speed-ups in parentheses).</figDesc><graphic url="image-1.png" coords="7,133.37,273.18,79.24,79.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>Figure 4: Top-1 test error on CIFAR10 for varying model sizes (left) and over the course of training a single model (right), demonstrating a large amount of time is spent on small changes in accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5: Top-1 test error on CIFAR100 for varying model sizes (left) and over the course of training a single model (right), demonstrating a large amount of time is spent on small changes in accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparing selection across model architectures on ImageNet. Spearman's correlation between max entropy rankings from PyTorch (Paszke et al., 2017) pretrained models on ImageNet.Correlations are high across a wide range of model architectures<ref type="bibr" target="#b46">(Xie et al., 2017;</ref><ref type="bibr" target="#b8">He et al., 2016a;</ref><ref type="bibr" target="#b28">Sandler et al., 2018;</ref><ref type="bibr" target="#b12">Huang et al., 2017;</ref><ref type="bibr" target="#b36">Szegedy et al., 2015;</ref><ref type="bibr" target="#b34">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b14">Iandola et al., 2016;</ref><ref type="bibr" target="#b19">Krizhevsky et al., 2012)</ref>. For example, MobileNet V2's entropy-based rankings were highly correlated to ResNet50, even though the model had far fewer parameters (3.5M vs. 25.6M). In concert with our fastText and VDCNN results from Section 3.2, the high correlations between different model architectures suggest that SVP might be widely applicable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure9: Comparing selection across model sizes and methods on CIFAR100. Average Spearman's correlation between different runs of ResNet (R) models and a varying depths. We computed rankings based on forgetting events (left), entropy (middle), and greedy k-centers (right). We saw a similarly high correlation across model architectures (off-diagonal) as between runs of the same architecture (on-diagonal), suggesting that small models are good proxies for data selection.</figDesc><graphic url="image-9.png" coords="22,133.37,86.30,79.24,79.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average (± 1 std.) Spearman's rank-order correlation with ResNet164 during 5 training runs of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings were based on forgetting events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Average (± 1 std.) Spearman's rank-order correlation with ResNet164 during 5 training runs of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings were based on entropy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Average (± 1 std.) Spearman's rank-order correlation between epochs during 5 training runs of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings were based on forgetting events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Average (± 1 std.) Spearman's rank-order correlation between epochs during 5 training runs of varying ResNet architectures on CIFAR10 (left) and CIFAR100 (right), where rankings were based on entropy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure17: 2D t-SNE plots from the final hidden layer of a fully trained ResNet164 model on CIFAR10 and a 30% subset selected (black) with ResNet20 trained after a varying number of epochs. Rankings are calculated with forgetting events (top) and entropy (bottom). Notably, the ranking from forgetting events is much more stable because the model's uncertainty is effectively averaged throughout training rather than a single snapshot at the end like entropy. For t-SNE plots of the entire training run, please see http://bit.ly/svp-cifar10-tsne-entropy and http: //bit.ly/svp-cifar10-tsne-forget for entropy and forgetting events respectively.</figDesc><graphic url="image-37.png" coords="25,112.18,184.37,94.09,85.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>= arg max i∈[n]\s min j∈s ∆ x i , x j ; A T</figDesc><table><row><cell></cell><cell>Algorithm 2 FORGETTING EVENTS</cell></row><row><cell>Algorithm 1 GREEDY K-CENTERS</cell><cell>(TONEVA ET AL., 2019)</cell></row><row><cell>(WOLF, 2011; SENER &amp; SAVARESE, 2018)</cell><cell>1: Initialize prev_acc i = 0, i ∈ [n]</cell></row><row><cell>Input: data x i , existing pool s 0 , trained model A T 0 , and a budget b 1: Initialize s = s 0 2: repeat 3: u 0 4: s = s ∪ {u} 5: until |s| = b + |s 0 | 6: return s \ s 0</cell><cell>2: Initialize forgetting_events i = 0, i ∈ [n] 3: while training is not done do 4: Sample mini-batch B from L 5: for example i ∈ B do 6: Compute acc i 7: if prev_acc i &gt; acc i then 8: forgetting_events i += 1 9:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The entire process of training ResNet20 on all the data, selecting which examples to keep, and training ResNet164 on the subset only took 2 hours and 20 minutes (see Table6in the Appendix), which was a 1.6× speed-up compared to training ResNet164 over all of the data. If we stopped training ResNet56 early and removed 50% of the data based on forgetting events from the first 50 epochs, SVP achieved an end-to-end training time speed-up of 1.8× with only a slightly higher top-1 error from ResNet164 (5.4% vs. 5.1%) as shown in Table7in the Appendix. In general, training the proxy for fewer epochs also maintained the accuracy of the target model on CIFAR10 because the ranking quickly converged (Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of parameters in each model.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Number of Parameters (millions)</cell></row><row><cell>CIFAR10</cell><cell>ResNet164</cell><cell>1.7</cell></row><row><cell></cell><cell>ResNet110</cell><cell>1.73</cell></row><row><cell></cell><cell>ResNet56</cell><cell>0.86</cell></row><row><cell></cell><cell>ResNet20</cell><cell>0.27</cell></row><row><cell></cell><cell>ResNet14</cell><cell>0.18</cell></row><row><cell></cell><cell>ResNet8</cell><cell>0.08</cell></row><row><cell>CIFAR100</cell><cell>ResNet164</cell><cell>1.73</cell></row><row><cell></cell><cell>ResNet110</cell><cell>1.74</cell></row><row><cell></cell><cell>ResNet56</cell><cell>0.86</cell></row><row><cell></cell><cell>ResNet20</cell><cell>0.28</cell></row><row><cell></cell><cell>ResNet14</cell><cell>0.18</cell></row><row><cell></cell><cell>ResNet8</cell><cell>0.08</cell></row><row><cell>ImageNet</cell><cell>ResNet50</cell><cell>25.56</cell></row><row><cell></cell><cell>ResNet18</cell><cell>11.69</cell></row><row><cell cols="2">Amazon Review Polarity VDCNN29</cell><cell>16.64</cell></row><row><cell></cell><cell>VDCNN9</cell><cell>14.17</cell></row><row><cell>Amazon Review Full</cell><cell>VDCNN29</cell><cell>16.64</cell></row><row><cell></cell><cell>VDCNN9</cell><cell>14.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of training for fewer epochs on active learning.</figDesc><table><row><cell>3.1×</cell><cell>7.2×</cell><cell>1.0×</cell><cell>1.6×</cell><cell>2.8×</cell><cell>5.5×</cell><cell>-</cell><cell>1.0×</cell><cell>1.6×</cell><cell>3.1×</cell><cell>7.2×</cell><cell>1.0×</cell><cell>1.6×</cell><cell>3.0×</cell><cell>6.2×</cell><cell>-</cell><cell>1.0×</cell><cell>1.6×</cell><cell>-</cell><cell>1.0×</cell><cell>1.8×</cell><cell>51.3×</cell><cell>-</cell><cell>1.0×</cell><cell>1.8×</cell><cell>43.1×</cell></row><row><cell>2.6× 2.9× 3.0× 3.1×</cell><cell>3.8× 5.8× 6.7× 7.0×</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>2.1× 1.8× 1.7× 1.7×</cell><cell>3.0× 2.9× 2.8× 2.8×</cell><cell>3.8× 4.6× 5.0× 5.3×</cell><cell>----</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>1.5× 1.6× 1.6× 1.6×</cell><cell>2.4× 2.7× 3.0× 2.9×</cell><cell>4.0× 5.8× 6.6× 7.0×</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>2.3× 1.9× 1.8× 1.7×</cell><cell>3.3× 3.2× 3.1× 3.1×</cell><cell>4.5× 5.5× 5.9× 6.1×</cell><cell>----</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>1.2× 1.3× 1.4× 1.5×</cell><cell>----</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>1.9× 1.8× 1.8× 1.8×</cell><cell>10.6× 20.6× 32.2× 41.9×</cell><cell>----</cell><cell>1.0× 1.0× 1.0× 1.0×</cell><cell>2.0× 1.9× 1.8× 1.8×</cell><cell>8.7× 17.7× 26.7× 35.1×</cell></row><row><cell>18.2 ± 0.73 10.3 ± 0.28 7.4 ± 0.10 6.1 ± 0.06 5.5 ± 0.08</cell><cell>ResNet20 18.1 ± 0.28 10.5 ± 0.42 7.4 ± 0.23 5.9 ± 0.19 5.4 ± 0.41</cell><cell>Greedy ResNet164 (Baseline) 20.1 ± 0.39 11.3 ± 0.40 8.1 ± 0.22 6.6 ± 0.24 5.6 ± 0.04</cell><cell>k-Centers ResNet110 19.4 ± 0.55 11.6 ± 0.16 8.1 ± 0.16 6.4 ± 0.10 5.7 ± 0.13</cell><cell>ResNet56 19.8 ± 0.49 11.6 ± 0.16 8.4 ± 0.21 6.3 ± 0.17 5.7 ± 0.19</cell><cell>ResNet20 19.5 ± 0.76 12.1 ± 0.44 8.8 ± 0.31 7.2 ± 0.19 6.1 ± 0.18</cell><cell>CIFAR100 Random -60.7 ± 0.81 42.5 ± 0.55 36.0 ± 0.42 31.9 ± 0.48 29.3 ± 0.16</cell><cell>Least ResNet164 (Baseline) 61.2 ± 1.09 42.2 ± 0.67 33.9 ± 0.33 29.9 ± 0.18 26.9 ± 0.21</cell><cell>Confidence ResNet110 60.2 ± 0.84 42.3 ± 0.95 34.1 ± 0.38 29.7 ± 0.41 27.2 ± 0.25</cell><cell>ResNet56 61.5 ± 0.93 42.0 ± 0.17 33.7 ± 0.33 29.7 ± 0.08 26.4 ± 0.13</cell><cell>ResNet20 62.4 ± 1.07 41.4 ± 0.25 33.8 ± 0.37 29.8 ± 0.10 26.6 ± 0.14</cell><cell>Greedy ResNet164 (Baseline) 60.4 ± 1.30 42.4 ± 0.57 34.5 ± 0.40 30.2 ± 0.33 27.3 ± 0.24</cell><cell>k-Centers ResNet110 59.6 ± 0.78 42.2 ± 0.76 34.9 ± 0.40 30.3 ± 0.46 27.4 ± 0.21</cell><cell>ResNet56 60.9 ± 1.08 42.6 ± 0.47 35.2 ± 0.40 30.8 ± 0.25 27.8 ± 0.23</cell><cell>ResNet20 60.2 ± 1.27 42.9 ± 0.52 35.8 ± 0.45 31.6 ± 0.31 28.5 ± 0.48</cell><cell>ImageNet Random -48.5 ± 0.04 37.5 ± 0.34 32.5 ± 0.12 29.9 ± 0.42 27.8 ± 0.13</cell><cell>Least ResNet50 (Baseline) 48.2 ± 0.37 35.9 ± 0.22 31.0 ± 0.10 28.3 ± 0.32 26.3 ± 0.16</cell><cell>Confidence ResNet18 48.3 ± 0.31 36.1 ± 0.19 31.1 ± 0.12 28.2 ± 0.13 26.4 ± 0.02</cell><cell>Amazon Random -6.5 ± 0.03 5.6 ± 0.07 5.2 ± 0.07 4.9 ± 0.01 4.7 ± 0.03</cell><cell>Review Least VDCNN29 (Baseline) 5.8 ± 0.08 4.8 ± 0.04 4.5 ± 0.01 4.3 ± 0.02 4.2 ± 0.02</cell><cell>Polarity Confidence VDCNN9 5.8 ± 0.11 4.9 ± 0.01 4.5 ± 0.02 4.3 ± 0.04 4.3 ± 0.03</cell><cell>fastText 6.9 ± 0.81 5.2 ± 0.17 4.6 ± 0.01 4.3 ± 0.01 4.3 ± 0.02</cell><cell>Amazon Random -41.7 ± 0.19 39.9 ± 0.05 39.0 ± 0.09 38.4 ± 0.14 37.9 ± 0.10</cell><cell>Review Least VDCNN29 (Baseline) 41.9 ± 0.54 39.7 ± 0.22 38.6 ± 0.10 38.2 ± 0.03 37.6 ± 0.01</cell><cell>Full Confidence VDCNN9 42.0 ± 0.44 39.8 ± 0.23 38.7 ± 0.09 38.1 ± 0.09 37.7 ± 0.10</cell><cell>fastText 42.7 ± 0.77 39.8 ± 0.02 38.7 ± 0.05 38.1 ± 0.06 37.7 ± 0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average (± 1 std.) top-1 error and runtime in minutes from 5 runs of core-set selection with varying proxies, selection methods, and subset sizes on CIFAR10 and CIFAR100.</figDesc><table><row><cell>Top-1 Error of ResNet164 (%) Data Selection Runtime in Minutes Total Runtime in Minutes</cell><cell>30% 50% 70% 30% 50% 70% 30% 50% 70%</cell><cell></cell><cell>8.9 ± 0.29 6.3 ± 0.23 5.4 ± 0.09 265 ± 48.0 286 ± 91.6 260 ± 42.6 342 ± 47.7 406 ± 94.3 425 ± 41.7</cell><cell>9.1 ± 0.33 6.4 ± 0.13 5.5 ± 0.21 27 ± 1.1 28 ± 1.4 30 ± 2.2 104 ± 1.9 147 ± 1.0 193 ± 5.7</cell><cell>8.9 ± 0.09 6.1 ± 0.21 5.3 ± 0.07 65 ± 3.9 67 ± 3.8 68 ± 3.4 142 ± 4.7 187 ± 4.8 230 ± 5.1</cell><cell>7.7 ± 0.19 5.2 ± 0.11 5.0 ± 0.12 218 ± 1.4 218 ± 1.6 219 ± 1.5 296 ± 3.2 340 ± 6.8 382 ± 4.6</cell><cell>7.6 ± 0.18 5.2 ± 0.11 5.1 ± 0.07 24 ± 1.3 24 ± 1.4 25 ± 1.5 101 ± 2.6 142 ± 2.5 185 ± 5.0</cell></row><row><cell></cell><cell>Subset Size</cell><cell>Method Selection Model</cell><cell>Facility Location ResNet164 (Baseline)</cell><cell>ResNet20</cell><cell>ResNet56</cell><cell>Forgetting Events ResNet164 (Baseline)</cell><cell>ResNet20</cell></row><row><cell></cell><cell></cell><cell>Dataset</cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Average top-1 error (± 1 std.) and runtime in minutes from 5 runs of core-set selection with varying selection methods calculated from ResNet20 models trained for a varying number of epochs on CIFAR10 and CIFAR100.</figDesc><table><row><cell>Top-1 Error of ResNet164 (%) Data Selection Runtime in Minutes Total Runtime in Minutes</cell><cell>30% 50% 70% 30% 50% 70% 30% 50% 70%</cell><cell></cell><cell>7.7 ± 0.19 5.2 ± 0.11 5.0 ± 0.12 218 ± 1.4 218 ± 1.6 219 ± 1.5 296 ± 3.2 340 ± 6.8 382 ± 4.6</cell><cell>7.6 ± 0.18 5.2 ± 0.11 5.1 ± 0.07 24 ± 1.3 24 ± 1.4 25 ± 1.5 101 ± 2.6 142 ± 2.5 185 ± 5.0</cell><cell>7.1 ± 0.16 5.4 ± 0.22 5.0 ± 0.17 14 ± 1.0 14 ± 0.7 14 ± 0.7 92 ± 1.5 135 ± 0.7 178 ± 2.5</cell><cell>7.2 ± 0.18 5.4 ± 0.09 5.1 ± 0.15 7 ± 0.9 7 ± 0.4 7 ± 0.4 85 ± 2.0 126 ± 1.4 169 ± 1.0</cell><cell>7.3 ± 0.17 5.4 ± 0.09 5.1 ± 0.12 4 ± 0.4 4 ± 0.2 4 ± 0.2 80 ± 0.8 122 ± 1.5 164 ± 1.5</cell><cell>9.6 ± 0.16 6.4 ± 0.27 5.6 ± 0.19 218 ± 1.4 218 ± 1.7 218 ± 1.6 296 ± 1.5 338 ± 2.2 382 ± 3.1</cell><cell>8.9 ± 0.18 5.7 ± 0.23 5.3 ± 0.09 24 ± 1.3 24 ± 1.5 25 ± 1.5 103 ± 2.2 145 ± 1.3 190 ± 3.7</cell><cell>8.4 ± 0.14 5.6 ± 0.17 5.2 ± 0.14 14 ± 1.1 14 ± 0.7 14 ± 0.7 92 ± 1.6 134 ± 1.2 176 ± 1.3</cell><cell>10.4 ± 1.19 6.3 ± 0.55 5.2 ± 0.23 7 ± 0.8 7 ± 0.4 7 ± 0.4 84 ± 1.5 126 ± 1.6 169 ± 1.9</cell><cell>10.2 ± 1.34 6.2 ± 0.31 5.3 ± 0.15 4 ± 0.3 4 ± 0.2 4 ± 0.2 81 ± 0.9 123 ± 1.2 166 ± 0.7</cell><cell>36.8 ± 0.36 27.1 ± 0.40 23.5 ± 0.19 221 ± 6.1 221 ± 6.1 221 ± 6.1 298 ± 5.7 342 ± 5.5 384 ± 4.7</cell><cell>37.2 ± 0.29 27.1 ± 0.14 23.4 ± 0.16 24 ± 0.7 25 ± 0.7 25 ± 0.7 104 ± 3.3 148 ± 3.6 193 ± 6.1</cell><cell>35.8 ± 0.40 27.7 ± 0.24 24.7 ± 0.33 14 ± 0.3 14 ± 0.3 14 ± 0.3 92 ± 0.7 134 ± 0.6 177 ± 1.0</cell><cell>36.3 ± 0.25 28.2 ± 0.24 24.6 ± 0.28 8 ± 0.4 8 ± 0.5 8 ± 0.5 87 ± 3.6 132 ± 6.2 177 ± 8.5</cell><cell>38.3 ± 0.48 28.4 ± 0.32 25.1 ± 0.31 4 ± 0.2 4 ± 0.1 4 ± 0.1 81 ± 1.1 123 ± 1.2 164 ± 1.0</cell><cell>39.6 ± 0.43 30.1 ± 0.12 25.4 ± 0.39 220 ± 6.4 220 ± 6.4 220 ± 6.4 297 ± 7.3 340 ± 7.3 380 ± 7.1</cell><cell>46.5 ± 0.74 29.7 ± 0.45 24.2 ± 0.21 24 ± 0.6 25 ± 0.7 25 ± 0.7 105 ± 1.7 148 ± 2.6 193 ± 3.6</cell><cell>46.5 ± 0.52 29.7 ± 0.36 24.1 ± 0.48 14 ± 0.4 14 ± 0.3 14 ± 0.3 91 ± 0.5 135 ± 0.7 176 ± 1.5</cell><cell>43.3 ± 1.83 30.0 ± 0.77 24.7 ± 0.41 7 ± 0.2 7 ± 0.2 8 ± 0.2 85 ± 0.1 128 ± 1.3 170 ± 1.6</cell><cell>43.9 ± 2.33 30.8 ± 1.37 25.0 ± 0.50 4 ± 0.1 4 ± 0.1 4 ± 0.1 80 ± 0.4 123 ± 0.9 165 ± 1.3</cell></row><row><cell></cell><cell>Subset Size</cell><cell>Dataset Method Selection Model Epochs</cell><cell>CIFAR10 Forgetting Events ResNet164 (Baseline) 181</cell><cell>ResNet20 181</cell><cell>100</cell><cell>50</cell><cell>25</cell><cell>Entropy ResNet164 (Baseline) 181</cell><cell>ResNet20 181</cell><cell>100</cell><cell>50</cell><cell>25</cell><cell>CIFAR100 Forgetting Events ResNet164 (Baseline) 181</cell><cell>ResNet20 181</cell><cell>100</cell><cell>50</cell><cell>25</cell><cell>Entropy ResNet164 (Baseline) 181</cell><cell>ResNet20 181</cell><cell>100</cell><cell>50</cell><cell>25</cell></row></table><note>VGG-11 w/ BatchNorm (132.9M) VGG-13 w/ BatchNorm (133.1M) VGG-16 w/ BatchNorm (138.4M) VGG-19 w/ BatchNorm (143.7M) SqueezeNet 1.0 (1.2M) SqueezeNet 1.1 (1.2M) 0.65 0.54 0.85 0.81 0.78 0.72 0.70 0.81 0.74 0.71 0.70 0.85 0.76 0.67 0.57 0.85 0.82 0.79 0.74 0.72 0.81 0.75 0.72 0.72 0.85 0.76 0.91 0.72 0.63 0.84 0.83 0.82 0.78 0.77 0.83 0.78 0.76 0.76 0.85 0.75 0.89 0.90 0.74 0.65 0.83 0.84 0.84 0.79 0.79 0.84 0.80 0.77 0.78 0.84 0.74 0.87 0.88 0.90 0.40 0.29 0.71 0.60 0.55 0.47 0.45 0.60 0.50 0.47 0.46 0.69 0.64 0.75 0.73 0.67 0.65 0.41 0.30 0.73 0.62 0.57 0.49 0.47 0.62 0.52 0.49 0.48 0.71 0.66 0.75 0.73 0.68 0.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://pytorch.org/docs/stable/torchvision/models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/facebookresearch/fastText</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Code available at https://github.com/stanford-futuredata/selection-via-proxy</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://pytorch.org/docs/stable/torchvision/models.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://github.com/NVIDIA/DeepLearningExamples</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://github.com/NVIDIA/apex/tree/master/examples/imagenet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://github.com/facebookresearch/fastText</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by affiliate members and other supporters of the Stanford DAWN project-Ant Financial, Facebook, Google, Infosys, NEC, and VMware-as well as Toyota Research Institute, Northrop Grumman, Amazon Web Services, Cisco, and the NSF under CAREER grant CNS-1651570. Jure Leskovec is a Chan Zuckerberg Biohub investigator. We also gratefully acknowledge the support of DARPA under Nos. FA865018C7880 (ASED), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), DGE-1656518 (GRFP), DGE-114747 (GRFP); SNSF under Nos. P2EZP2_172187; Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, JD.com, Amazon, Boeing, Docomo, Huawei, Hitachi, Observe, Siemens, UST Global. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of SNSF, NSF, DARPA, NIH, ARO, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 ADDITIONAL CORE-SET SELECTION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full</head><p>Figure <ref type="figure">7</ref>: SVP performance on core-set selection. Average (± 1 std.) top-1 error of ResNet164 over 5 runs of core-set selection with different selection methods, proxies, and subset sizes on CIFAR100. We found subsets using forgetting events (left), entropy (middle), and greedy k-centers (right) from a proxy model trained over the entire dataset. Across datasets and selection methods, SVP performed as well as an oracle baseline but significantly faster (speed-ups in parentheses).</p><p>Table <ref type="table">5</ref>: Average top-1 error (± 1 std.) from 3 runs of core-set selection with varying selection methods on ImageNet, Amazon Review Polarity, and Amazon Review Full.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automated scalable bayesian inference via hilbert coresets</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05053</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01737</idno>
		<title level="m">Bayesian coreset construction via greedy iterative geodesic ascent</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1104" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
	<note>international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smaller coresets for k-median and k-means clustering</title>
		<author>
			<persName><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Kushal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Máté</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coresets for scalable bayesian logistic regression</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08158</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 17th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the relationship between data efficiency and error for uncertainty sampling</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/mussmann18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3674" to="3682" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised data selection and word-morph mixed language model for tamil low-resource keyword search</title>
		<author>
			<persName><forename type="first">Chongjia</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheung-Chi</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
				<imprint>
			<biblScope unit="page" from="4714" to="4718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning for interactive neural machine translation of data streams</title>
		<author>
			<persName><forename type="first">Álvaro</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1015</idno>
		<ptr target="https://www.aclweb.org/anthology/K18-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
				<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1aIuk-RW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active Learning and Experimental Design workshop In conjunction with AISTATS</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v16/settles11a.html" />
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gavin</forename><surname>Cawley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Lemaire</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Statnikov</surname></persName>
		</editor>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05-16">2010. 16 May 2011</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>From theories to queries: Active learning in practice</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Active learning</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haim</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
				<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep active learning for named entity recognition</title>
		<author>
			<persName><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yakov</forename><surname>Kronrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2630</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-2630" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
				<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="252" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName><forename type="first">Mariya</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlxm30cKm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Core vector machines: Fast svm training on very large data sets</title>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pak-Ming</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular functions for image collection summarization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rishabh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using document summarization techniques for speech data subset selection</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="721" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Submodular subset selection for large-scale speech training data</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m">IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3311" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">Gert</forename><forename type="middle">W</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<idno>48.3 ± 0.31 36.3 ± 0.07 31.3 ± 0.02 28.4 ± 0.17 26.6 ± 0.08</idno>
	</analytic>
	<monogr>
		<title level="m">Xiang Zhang and Yann LeCun. Text understanding from scratch</title>
				<imprint>
			<date type="published" when="2015">2016. 2015. 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<idno>ResNet164 (Baseline) 40.8 ± 0.20 29.5 ± 0.29 24.6 ± 0.42 263 ± 52.2 325 ± 158.7 296 ± 70.2 339 ± 52.7 446 ± 158.1 460 ± 69.1</idno>
		<title level="m">CIFAR100 Facility Location</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Forgetting Events ResNet164 (Baseline)</title>
		<idno>36.8 ± 0.36 27.1 ± 0.40 23.5 ± 0.19</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<idno>ResNet164 (Baseline) 39.6 ± 0.43 30.1 ± 0.12 25.4 ± 0.39</idno>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
