<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-Invariant Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-11-24">24 Nov 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
							<email>xiaotianjun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuyang@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scale-Invariant Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-11-24">24 Nov 2014</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1411.6369v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a model designed to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many classical computer vision tasks have enjoyed a great breakthrough, primarily due to the large amount of training data and the application of deep convolution neural networks (CNN) <ref type="bibr" target="#b7">[8]</ref>. In the most recent ILSVRC 2014 competition <ref type="bibr" target="#b10">[11]</ref>, CNN-based solutions have achieved nearhuman accuracies in image classification, localization and detection tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Accompanying this progress are studies trying to understand what CNN has learnt internally and what contribute to its success <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. By design, layers within the network have progressively larger receptive field sizes, allowing them to learn more complex features. Another key point is the shift-invariance property, that a pattern in the input can be recognized regardless of its position <ref type="bibr" target="#b8">[9]</ref>. Pooling layers contribute resilience to slight deformation as well small scale change <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, it is evident that CNN deals with shift-variance far better than scale-invariance <ref type="bibr" target="#b2">[3]</ref>. Not dealing with scaleinvariance well poses a direct conflict to the design philosophy of CNN, in that higher layers may see and thus captures features of certain plain patterns simply because they are larger at the input, not because they are more complex. In other words, there is no alignments between in the position of a filter and the complexity it captures. What is more, there are other invariance that CNN does not deal with internally. Examples include rotations and flips (since features of natural objects are mostly symmetric).</p><p>A brutal force solution would be to make the network larger by introducing more filters to cope with scale variations of the same feature, accompanied by scale-jittering the input images, often by order of magnitude. This is, in fact, the popular practice today <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. It is true even for proposals that directly deal with this problem. For example, <ref type="bibr" target="#b2">[3]</ref> drives the CNN with crops of different size and positions with three differnt scales, and then uses VLAD pooling to produce a feature summary of the patches.</p><p>We explore a radically different approach that is also simple. Observing that filters that detect the same pattern but with different scales bear strong relationship, we adopt a multi-column design and designate each column to specialize on certain scales. We call our system SiCNN (Scaleinvariant CNN). Unlike a conventional multi-column CNN, filters in SiCNN are strongly regulated among columns. The goal is to make the network resilient to scale variance without blowing up number of free parameters, and thus reduce the need of jittering the input.</p><p>We performed detailed analysis and verified that SiCNN exhibits the desired behavior. For example, the column that deals with larger scale is indeed activated by input patterns with the larger scaling factor, and the system as a whole becomes less sensitive to scale variance. On unaugmented CIFAR-10 dataset <ref type="bibr" target="#b6">[7]</ref>, our method produces the best re-sult among all previous works using a single CNN and a simple softmax classifier, and is complementary to other techniques that improve the performance. Our model increases training cost linear to number of columns, but we find that incremental refinement can dramatically reduce the cost without compromising the performance significantly.</p><p>The rest of the paper is organized as follows. Section 2 presents SiCNN, covering the high-level intuition, the mathematical foundation and the architecture. Detailed analysis and results are presented in Section 3, and we conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>Consider the case of classifying objects that have only one canonical scale and the only free parameter is their positions. A stack of convolution filters can progressively build more complex hidden representations. These hidden representations are all invariant by shift, meaning that the activations preserve the same pattern except that they are shifted. In other words, conv(Shift(I), f ) ‚â° Shift(conv(I, f )), for arbitrary image I and filter f , and this relationship is upheld layer to layer. This makes the job of the classifier easy.</p><p>In the existing CNN architecture, dealing with multiple scales is jointly achieved by the pooling layers and the convolution layers. The convolution layer not only needs to learn different features but also their scaled variants into multiple feature maps. Units in the pairing pooling layer generate scale-invariance within their receptive fields, which help to save feature maps. This multi-scale solution leads to bigger model and, since filters are independently learned, the need of more training data. The popular practice is scale-jittering <ref type="bibr" target="#b7">[8]</ref>.</p><p>Our idea is simple, and is inspired by the invariance-byshift property of the existing convolution layer. Just like CNN convolve a filter on different positions, we also "convolve" the filter on different scales. This is done by adding independent columns, each is a conventional CNN but "specialized" at detecting one scale. Crucially, the columns are strongly regularized such that the number of free parameters in the convolution layers stay the same. Thus, we inject scale-invariance into the model, requiring neither additional data augmentation nor increasing the model size.</p><p>In the followings, we first introduce our architecture, present the intuition and then give the concrete mathematical definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scale-Invariance Architecture</head><p>SiCNN uses multiple columns of convolutional stack with varying filter size to capture objects with unknown scales in input images. The architecture is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. From bottom up, the input image is fed into all the columns. Each column has several convolutional layers  with max-pooling. The key difference from conventional multi-column CNN is that, although these columns use different filter size, they still share a set of common parameters among their filters. A canonical column (Column 1 in Figure <ref type="figure" target="#fig_1">1</ref>) keeps canonical filters in each layer. Other columns, which we call scale columns, transform these canonical filters into their own filter. Collectively, a canonical filter and its transformed filters detect its pattern at different scales in multiple columns simultaneously. Therefore, a single pattern with different scales trigger one or more columns.</p><p>In our architecture, we simply concatenate the top-layer feature maps from all the columns into a feature vector. The final classification layers (a softmax layer in the simplest case) take this feature vector as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Filters in Multiple Scales</head><p>Filters that are transformed into different columns from the canonical filter capture the same pattern at different scales. We will discuss this transformation from canonical filters to other columns.</p><p>Consider a canonical filter f , which detects a pattern in image I by convolution conv(I, f ) (Figure <ref type="figure" target="#fig_2">2</ref>). When the image is scaled by a scaling operation S to S(I), we expect another column k with transformed filter T (f ) to capture the same pattern instead. Thus, the column k generates another convolution result conv(S(I), T (f )). Just as invariance-by-shift, we require this convolution be equivalent with scaling from the convolution result in the canonical column. That is conv(S(I), T (f )) = S(conv(I, f )).</p><p>( We call this property of filter as invariance-by-scaling.</p><p>Given a scaling S, we want to find the T that satisfies Equation 1 for any image I and filter f . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùëÜ(ùëã 2 )</head><p>ReLU + pooling The above discussion is for the first layer. However, it is easy to see that if the filter transformation T in each layer satisfies Equation <ref type="formula" target="#formula_0">1</ref>, then invariance-by-scaling property is preserved layer by layer till reaching the classification layer. In Figure <ref type="figure" target="#fig_4">3</ref>, when input images I and S(I) with different scale are fed into the canonical column and the column k separately, they generate X 1 and S(X 1 ) respectively. The following ReLU nonlinear activation function and maxpooling keep this scale relationship and result in Y 1 and S(Y 1 ). By recursively applying Equation 1, we know the top layers of these two columns also keep the same scale relationship: if canonical column generates Y M on input image I, the column k generates S(Y M ) on image S(I).</p><formula xml:id="formula_1">ùëÜ(ùëå 2 ) conv(ùëå 1 , ùëì 2 ) conv(ùëÜ ùëå 1 , ùëá(ùëì 2 )) Canonical column Column ùëò ùëå ùëÄ ‚Ä¶ ùëÜ(ùëå ùëÄ ) ‚Ä¶</formula><p>If the object scale fits exactly one of the columns, then there is a perfect matching with the column outputting the highest responses. Otherwise, if the object scale just falls in between the scales of two neighboring columns, both the two columns will have relatively high responses. The concatenated feature vector at the end makes it possible for the classifier to do a linear combinations of responses from multiple columns to eliminate above variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Filter Transformation</head><p>By a vector representation of the image (concatenating all the rows or columns in matrix), scaling and convolution are all linear transformations. Given a canonical filter f c represented by a vector, we can solve the following equation derived from Equation <ref type="formula" target="#formula_0">1</ref>to get the transformed filter f t , ‚àÄI, conv(S(I), f t ) = S(conv(I, f c )).</p><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Equation 2 is a system of linear equations for f t . However, such a system doesn't always have a valid solution because it has too many constraints (linear equations). To address this problem, we reduce I to be of the same size as f c , which makes the conv(I, f c ) produce only a single number. Then, Equation 2 turns into</p><formula xml:id="formula_3">‚àÄI, (S ‚Ä¢ I) T ‚Ä¢ f t = I T ‚Ä¢ f c ,<label>(3)</label></formula><p>where S is the scaling matrix, I is the vector representation of the image, and similar are f c and f t . It's easy to prove that Equation 3 is equivalent to</p><formula xml:id="formula_4">S T ‚Ä¢ f t = f c ,<label>(4)</label></formula><p>We can solve Equation 4 to obtain f t . However, in practice we can't always obtain an exact or unique f t because S is not a square and invertible matrix. When S is a scaling-up matrix (#rows &gt; #columns), the equation have infinite number of solutions; when S is a scaling-down matrix (#rows &lt; #columns), the equation have no exact solutions. For the first case with infinite solutions, we choose the solution with the minimum L2 norm,</p><formula xml:id="formula_5">f t = arg min ft f t 2 2 subject to S T ‚Ä¢ f t = f c .<label>(5)</label></formula><p>The reason that we choose a minimum-norm solution is similar of applying weight decay to the weights, i.e., to reduce over-fitting. A flat filter is likely to have more generalization to various cases. It is easy to get the solution of ( <ref type="formula" target="#formula_5">5</ref>) by the generalized inverse of S T ,</p><formula xml:id="formula_6">f t = S(S T S) ‚àí1 ‚Ä¢ f c .<label>(6)</label></formula><p>For the second case with no exact solution, we see the problem from a different angle: we take the scaled image ƒ® = S‚Ä¢I as an input image, and proximate the original I with a scaling of ƒ®, I ‚âà Sƒ® . Here the S is a scaling-up matrix in the reverted direction of S. We turn the Equation 3 into</p><formula xml:id="formula_7">‚àÄ ƒ®, ƒ®T ‚Ä¢ f t = ( S ‚Ä¢ ƒ®) T ‚Ä¢ f c .<label>(7)</label></formula><p>Similar to Equation <ref type="formula" target="#formula_4">4</ref>, we get</p><formula xml:id="formula_8">f t = ST ‚Ä¢ f c .<label>(8)</label></formula><p>In our implementation, we use bicubic interpolation <ref type="bibr" target="#b5">[6]</ref> as the scaling method to transform filters. This method can produce nice scaling results without losing too much information of the original image.</p><p>In our model, we also consider a special scaling operation: horizontal flipping. We add some columns with flipped filters to capture the flipped patterns in input. The scaling matrix for flipping is a symmetric invertible matrix. It is very easy to solve Equation <ref type="formula" target="#formula_4">4</ref>,</p><formula xml:id="formula_9">f t = S ‚Ä¢ f c .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training Multiple Columns</head><p>We integrate all the columns with tied filters into a single model and train them together with back-propagation algorithm. Observing Equation 6, 8 and 9 in above cases, we find the transformation from canonical filter to any scale is always a linear transformation. That means, the filters in all the columns are tied to the canonical filter by a matrix multiplication,</p><formula xml:id="formula_10">f t = Q ‚Ä¢ f c ,</formula><p>where Q is some transformation matrix. Particularly, Q is an identity matrix for canonical column. This property makes back-propagation very convenient. Suppose we have n columns, and the corresponding filters are</p><formula xml:id="formula_11">f i t = Q i ‚Ä¢ f c .</formula><p>Define the cost function as E, which is a function of all the f i t . By the chain rule of derivatives, we get</p><formula xml:id="formula_12">‚àÇE ‚àÇf c = n i=1 ‚àÇf i t ‚àÇf c T ‚Ä¢ ‚àÇE ‚àÇf i t = n i=1 Q T i ‚Ä¢ ‚àÇE ‚àÇf i t .</formula><p>In training, we first do the back-propagation in each column independently. Then, derivatives of the filters distributed on all columns are transformed and gathered as the canonical filters' derivatives. When the canonical filters are updated by these aggregated derivatives, all the filters on the scaled columns are recomputed by filter transformation from the new canonical filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Results</head><p>This section presents our experimental results. We begin with a detailed analysis on the scale invariance achieved within the network, followed by end-to-end performance on CIFAR-10 dataset <ref type="bibr" target="#b6">[7]</ref>. The baseline CNN is close to the Alex network <ref type="bibr" target="#b4">[5]</ref>, with 3 layers of convolution. Each layer uses 5 √ó 5 receptive size and a stride of 1, pooling of receptive size 3 √ó 3 and a stride of 2, followed by local normalization. The first convolution is paired with max pooling whereas the latter two is followed by average pooling. SiCNN extends it to 6 columns. The first three columns use filter size of 3, 5 and 7, with the last three columns being the flipped versions of the first three. All the weights are regularized and tied to the column with the 5 √ó 5 non-flipped column. We train these models on standard CIFAR-10 with the training method similar to that in <ref type="bibr" target="#b4">[5]</ref>. We use the same hyper-parameters (learning rate, momentum, weight decay) as in <ref type="bibr" target="#b4">[5]</ref>. We first train the whole net for 240 epochs, then reduce the learning rate by a factor of ten. We train for another 20 epochs, tune the learning rate again, and train for another 20 epochs to get the final result.</p><p>To exploit the invariance property of the model, we need to generate a new test dataset with a mixture of different scales. We crop the central 24 √ó 24 and 28 √ó 28 of the CIFAR-10 images and resize them to 32 √ó 32. This mixed dataset has 3 different scales: small, middle and large. We refer to this dataset as scaled CIFAR-10 later in this section.</p><p>Our experiment results are best viewed in electronic form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Filter Transformation for Scale-Invariance</head><p>Consider an arbitrary image I and its scaled version S(I). After applying a filter f and its transformation T (f ), their corresponding activations become conv(I, f ) and conv(S(I), T (f )), respectively. As described in Section 2.3, to achieve scale-invariant pattern matching, we expect the former after scaling is indistinguishable with the latter, i.e., S(conv(I, f )) ‚â° conv(S(I), T (f )).</p><p>In section Section 2.3, we achieve this for small image patches; in this section, we verify this property for larger images.</p><p>Note that the left side of Equation <ref type="formula" target="#formula_13">10</ref>, S(conv(I, f )) is the scaled activation of the canonical image, and is the design target of our transformation function. So we quantify with relative error using diff(x, y) =</p><p>x ‚àí y 2 x 2 .</p><p>where x = S(conv(I, f )) and y = conv(S(I), T (f )).</p><p>We compare three different kinds of filter transformation methods. trans I is an identity transformation, with which we apply the original filter f onto the scaled image S(I). trans T is the filter transformation described in Section 2.3. trans S is a comparison method, where we directly use simple image sampling to scale the filter. We also normalize the transformed filter to the same L1 norm as the original filter; we find this normalization performs the best compared to other alternatives. trans S takes such a transformation on filter f , We report the filter invariance-by-scaling by measuring diff(S(conv(I, f )), conv(S(I), T (f )))</p><formula xml:id="formula_14">T (f ) = f 1 S(f ) 1 S(f ).<label>(</label></formula><p>in Table <ref type="table" target="#tab_0">1</ref>. We take 100 random images from the test set of CIFAR-10 and take their averaged result. Three canonical filters with size of 5 √ó 5 are considered: random filter (the first row), filters learnt in the baseline CNN model (the second row), and filters learnt in SiCNN (the third row). As comparison, non-overlapped 2 √ó 2 max pooling, which is usually considered powerful for scale-invariance, is taken as a non-parametric filter applied to image I and scaled S(I).</p><p>In Table <ref type="table" target="#tab_0">1</ref>(a), we use a scaling-up S that doubles the image size from 32 √ó 32 to 64 √ó 64. Accordingly, trans T and trans S scale the filter size from 5 √ó 5 to 9 √ó 9. In Table <ref type="table" target="#tab_0">1</ref>(b), S is a scaling-down. Image size is halved from 32 √ó 32 to 16 √ó 16, and the filter size is scaled down from 5 √ó 5 to 3 √ó 3.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, it is clear that convolution with the same filter without any transformation is very sensitive to the image scale (column trans I ). Our filter transformation method (column trans T ) and the sampling-based method (column trans S ) are much more robust to the image scale. trans T is almost always better than the simple-minded trans S , especially when the image is scaled down and we need a more precise filter with a very small size. Considering the fact that trans S is hard for back-propagation because of the normalization, our method becomes an apparent choice for transforming filters. Also, when the image is scaled up, our filter transformation is even better than  pooling. Considering pooling doesn't need to detect any patterns, it's interesting to see that our method achieves robust invariance-by-scaling. When the image is scaled down, our method is still comparable against pooling. From random filter to SiCNN trained one, these filters adapt to some specific scale more and more. Consequently, the invarianceby-scaling of these filters also gets worse (from first row to the last row) as expected.</p><p>To give a more concrete feeling of our approach, we inspect the feature maps generated by images of different scales. Fig. <ref type="figure" target="#fig_6">4</ref> shows two particular examples, visualizing a feature map in each of the three convolution layers and the final result after pooling and normalization. In each example, the left column is the activations from the original image of 32 √ó 32, and the other two columns are from scaled image of 48 √ó 48. The left and middle column are results of using filter of the size 5 √ó 5 and its transformed 7 √ó 7 filter. All the feature maps have been scaled to the same size for ease of comparison. From the relatively small difference in each layer, it is clear that applying the transformed filters on the scaled image preserves the essential characteristics of the original. The rightmost column is the result of applying the original 5 √ó 5 filter to the scaled image. It is clear that the fixed filter generates the activations that diverge from that of the original image (the leftmost column) significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Column Features</head><p>To give an idea of what features the different columns learn, we scan over the activations on the last pooling layer caused by 30,000 test images from the scaled CIFAR-10. We randomly pick up a filter in the last layer and visualize the top 16 images that cause the largest outputs of this filter, in each of the 6 columns individually (Fig. <ref type="figure" target="#fig_8">5</ref>). This method is similar to that used in <ref type="bibr" target="#b16">[17]</ref>. It can be seen that each column in our model focuses on a particular scale and orientation: the images which causes largest activations get larger from left to right, and the automobiles in the two rows face opposite directions.</p><p>In addition to the visual inspection, we try to quantify how sensitive filters of different columns are to the scales. We take the top 100 images that this feature of a given column gets activated the most, then break them down according to which scale they belong to in the data set: small, middle or large. This statics is reported at the bottom of the images in Fig. <ref type="figure" target="#fig_8">5</ref>. It is clear that columns with small filters "picks" the small-scale images more, whereas the columns with larger filters does the opposite.</p><p>When an object for recognition is scaled from small to large, the columns in SiCNN will also work in turn to capture this object. We illustrate that in Figure <ref type="figure" target="#fig_9">7</ref>. Using method similar as in Fig. <ref type="figure" target="#fig_8">5</ref>, we first select a feature map that detects dogs in the last layer. Then we pick a dog image from CIFAR-10 and scale the object into different sizes (2x larger at most). The max activation value in the feature map are plotted as a function of the object size for each scale column. In Figure <ref type="figure" target="#fig_9">7</ref>, it is clear that when the object is small, the column with 3 √ó 3 filter first captures it and gives a big response. When the object gets larger, activations on this 3 √ó 3-filter column drop. The 5 √ó 5 and 7 √ó 7 columns gradually reach their peak responses in turn. The peaks of the three columns locate with the same interval along the object size, because of the equal-interval filter sizes of 3, 5 and 7. Comparing activation values among different columns is meaningless, because they are eventually summed up with different weights for classification. However, this study clearly shows that by tracing which column is activated more, we can detect a object as well as its scale. Table <ref type="table" target="#tab_1">2</ref> compare the results of the baseline CNN and SiCNN, both of which are trained on standard CIFAR-10 dataset. SiCNN achieves statistically significant gain on standard CIFAR-10. Its full advantage is more apparent with the scaled CIFAR-10, where CNN has a performance drop of more than 43.22%, and SiCNN drops by 32.42%. We manually examine the error cases, and find that the simple central-crop-resize has cut off many significant features in the scaled CIFAR-10. We speculate SiCNN will work better on higher quality multi-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scale-Invariant Classification</head><p>To verify the above hypothesis, we pick 5 random images in which the object is at the center, and scale them to different sizes; the largest one is the central 16 √ó 16 area of the image resized to 32 √ó 32. We put these images into both CNN and SiCNN, and compare the probability of the correct class. The results are shown in Fig. <ref type="figure" target="#fig_9">7</ref>. It can be seen that as the scale of the image goes up, the performance of CNN drops whereas SiCNN is stable. The only exception among these samples is the one of horse; its scaled up versions start to lose vital features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training results on CIFAR-10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Testing error CNN <ref type="bibr" target="#b4">[5]</ref> 16.6% CNN+ dropout <ref type="bibr" target="#b4">[5]</ref> 15.6% CNN+Spearmint <ref type="bibr" target="#b14">[15]</ref> 14.98% SiCNN 14.22% CNN+Maxout <ref type="bibr" target="#b3">[4]</ref> 11.68% CNN+Maxout <ref type="bibr" target="#b3">[4]</ref> + SiCNN (voting)</p><p>11.35% Maxout-SiCNN (2-column)</p><p>11.33% Network in Network <ref type="bibr" target="#b9">[10]</ref> 10.41% Table <ref type="table">3</ref>. Comparison of error rate.</p><p>In Table <ref type="table">3</ref>, we compare the classification error rate of SiCNN with other previous approaches on CIFAR <ref type="bibr">-10 [7]</ref>. We achieve an error rate of 14.22% on unaugmented data, an improvement of more than 2% absolute gain over the baseline CNN in <ref type="bibr" target="#b4">[5]</ref>. SiCNN also exceeds other improvement on CNN, such as dropout <ref type="bibr" target="#b4">[5]</ref> and Spearmint <ref type="bibr" target="#b14">[15]</ref>, but is insufficient to catch up with the maxout <ref type="bibr" target="#b3">[4]</ref> and network-  in-network <ref type="bibr" target="#b9">[10]</ref>, which are the current state of the art. Nevertheless, our method can be combined with these techniques as SiCNN is addressing scale-invariance problem, which is a different goal from others. For example, by using an average voting with SiCNN, we drop the error rate of maxout model from 11.68% to 11.35%. Moreover, by simply adding a extra flipped column to maxout model, we reach an error rate of 11.33% with a single 2-column maxout-SiCNN model. We find these results encouraging, and expect that SiCNN will work better on benchmarks that exhibit higher scale variations, as the results in Section 3.3 suggest. Replicating SICNN on larger and more complex dataset such as ImageNet is ongoing work.</p><p>SiCNN takes the form of multi-column CNN without blowing up number of free parameters. As a more direct comparison, we train a 6-column CNN where the filters are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Incremental Training</head><p>Improving scale invariance does not come for free. In the current configuration, training costs increase linearly with number of columns. Clearly, we can first train one single column, transform its filters to other columns and finally refine the model. In this ideal setting, it is reasonable to expect that the additional training cost is insignificant.</p><p>We explored two incremental training methods. In the first choice (named inc-1), we first train a baseline CNN with about half the epochs of a full training, and we build a 6-column SiCNN based on the current filters. Then we begin to refine the entire model with the left half of the epochs. In the second choice (named inc-2), we continue from a fully trained baseline CNN, and use its filters to build the 6column SiCNN. Then, with all the filter parameters frozen, we only refine the parameters in classifier. As we use a single softmax layer as classifier, the inc-2 method has a very small extra cost.</p><p>Results for incremental learning are summarized in Table 4. Compared with SiCNN trained from scratch (the second row), inc-1 training (the third row) takes nearly half cost, with the model achieving a comparable performance. With the inc-2 training (the fourth row), although the extra training cost is very small (1.5%), we still get a model with better performance than baseline CNN. Also by combination with the maxout units <ref type="bibr" target="#b3">[4]</ref>, we're able to reach an error rate of 11.33% which is higher than the previous result. Incremental learning does help to balance the performance gain and training efficiency in SiCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we propose a new generalization of CNN, SiCNN, where we incorporate scale and flip invariance into the model. This model improves the results of traditional CNN, and complements other optimization techniques. Our results clearly indicate that the model learns the feature in different scales in different columns. The idea is generaliz-able, can be applied in all aspects where CNN is employed, including supervised and unsupervised learning, recognition, detection, and localization tasks. Our preliminary study also implies a nice trade-off between performance and training cost.</p><p>Several open problems remain. For example, we can use a different way of summarizing all the columns (instead of concatenation), and different connectivity structure among column (e.g., pair-wise between columns instead of all-to-one against the canonical column). We plan to apply SiCNN to larger and more complex datasets such as Imagenet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Architecture of SiCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The transformation from a canonical filter to another column. Best viewed in electronic form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Columns with multiple layers to capture patterns in different scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of activations in each layer. In each of the two examples, the left column shows the original image of size 32 √ó 32 and its activations in each layer using a 5 √ó 5 filter; the middle and right columns show enlarged image of size 48√ó48 and its activations in each layer using the 7 √ó 7 transformed filter and the same 5 √ó 5 filter, respectively. The filter is chosen randomly from the SiCNN model we trained.</figDesc><graphic url="image-8.png" coords="5,466.49,91.82,73.85,123.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Activation versus object size in three columns. Peaks of each curve are illustrated by dash lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visualization a filter with respect to the 6 columns. In each column, the top 16 images that causes the largest activation of the filter are shown. The breakdown analysis of top 100 images are shown below each column scale (we combine each two flipped columns together).</figDesc><graphic url="image-16.png" coords="7,68.18,445.83,124.50,104.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Probability of the correct class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>a) Scaling up</cell><cell></cell><cell></cell></row><row><cell>Filter</cell><cell cols="4">trans I trans S trans T pooling</cell></row><row><cell cols="2">random 11.64%</cell><cell>1.71%</cell><cell>1.67%</cell><cell></cell></row><row><cell>CNN</cell><cell>59.63%</cell><cell>9.10%</cell><cell>9.19%</cell><cell>11.20%</cell></row><row><cell cols="2">SiCNN 66.11%</cell><cell>12.88%</cell><cell>10.94%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(b) Scaling down</cell><cell></cell><cell></cell></row><row><cell>Filter</cell><cell cols="4">trans I trans S trans T pooling</cell></row><row><cell cols="2">random 14.14%</cell><cell>1.81%</cell><cell>1.56%</cell><cell></cell></row><row><cell>CNN</cell><cell>85.13%</cell><cell>20.50%</cell><cell>15.85%</cell><cell>9.22%</cell></row><row><cell cols="3">SiCNN 107.19% 29.23%</cell><cell>17.10%</cell><cell></cell></row></table><note>Invariance-by-scaling regarding different filter transformation methods. The smaller the values listed in the table, the better Invariance-by-scaling.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification error rate, tested on standard CIFAR-10 and scaled CIFAR-10. The last row shows the classification improvement by SiCNN. The last column shows the performance drop due to test dataset with more scales.</figDesc><table><row><cell>Model</cell><cell>Standard CIFAR-10</cell><cell>Scaled CIFAR-10</cell><cell>Performance drop</cell></row><row><cell>CNN</cell><cell>17.33%</cell><cell>24.82%</cell><cell>43.22%</cell></row><row><cell>SiCNN</cell><cell>14.22%</cell><cell>18.83%</cell><cell>32.42%</cell></row><row><cell>Improvement</cell><cell>17.94%</cell><cell>24.13%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Classification error rate and cost for incremental training. The last two rows, inc-1 and inc-2, correspond to the two incremental training methods. All the training costs are normalized to CNN's cost. is a very small value. Here = 0.015.</figDesc><table><row><cell>Model</cell><cell>Error on CIFAR-10</cell><cell>Error on scaled CIFAR-10</cell><cell>cost</cell></row><row><cell>CNN</cell><cell>17.33%</cell><cell>24.82%</cell><cell>1</cell></row><row><cell>SiCNN</cell><cell>14.22%</cell><cell>18.83%</cell><cell>6</cell></row><row><cell>SiCNN, inc-1</cell><cell>14.71%</cell><cell>20.10%</cell><cell>3.5</cell></row><row><cell>SiCNN, inc-2</cell><cell>16.06%</cell><cell>23.24%</cell><cell>1 +</cell></row><row><cell cols="4">independent. Under the same training condition this net-</cell></row><row><cell cols="4">work suffers severe overfit, the testing error hovers around</cell></row><row><cell cols="3">19% while training error already reaches zero.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. IRO</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Universit√© de Montr√©al, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1840</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2009. 1, 4, 7</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>CoRR, abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2010</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
