<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-17">17 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoguo</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuhan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
							<email>bozheng@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-17">17 May 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462979</idno>
					<idno type="arXiv">arXiv:2105.07706v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pre-ranking</term>
					<term>effectiveness</term>
					<term>efficiency</term>
					<term>feature selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In real-world search, recommendation, and advertising systems, the multi-stage ranking architecture is commonly adopted. Such architecture usually consists of matching, pre-ranking, ranking, and re-ranking stages. In the pre-ranking stage, vector-product based models with representation-focused architecture are commonly adopted to account for system efficiency. However, it brings a significant loss to the effectiveness of the system. In this paper, a novel pre-ranking approach is proposed which supports complicated models with interaction-focused architecture. It achieves a better tradeoff between effectiveness and efficiency by utilizing the proposed learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD). Evaluations in a realworld e-commerce sponsored search system for a search engine demonstrate that utilizing the proposed pre-ranking, the effectiveness of the system is significantly improved. Moreover, compared to the systems with conventional pre-ranking models, an identical amount of computational resource is consumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Learning to rank.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As important internet services, large-scale search engine and recommendation systems play important roles in information retrieval and item recommendation, where a ranking system selects only a few items from tens of millions of candidates. Under the constraint of extremely low system latency, a single complicated ranking model cannot rank the entire candidate set. Therefore, multistage ranking architecture is commonly adopted <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> (shown in Figure <ref type="figure" target="#fig_2">1 (a)</ref>). Large-scale deep neural networks (DNNs) with interaction-focused architecture <ref type="bibr" target="#b2">[3]</ref> are usually employed for the ranking model to maintain good system performance, while only less complicated models with representation-focused architecture <ref type="bibr" target="#b2">[3]</ref> are adopted in pre-ranking to ensure efficiency.</p><p>However, simple pre-ranking models with representation-focused architecture will inevitably diminish the model expression ability. Vector-product based model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, which is classified as representationfocused architecture, is often employed in pre-ranking (see Figure <ref type="figure" target="#fig_0">1</ref> (b)). The limitation is that neither explicit interactive features nor implicit interactive semantics, which are less efficient for computation but very effective for the expression ability <ref type="bibr" target="#b10">[11]</ref>, can be used. The pre-ranking models with representation-focused architecture focus excessively on the efficiency optimization and remain a large effectiveness gap to models with interaction-focused architecture. Moreover, as the online feature generation process consume as much resource as that needed for the online inference of the model, the utilization for the pre-ranking with interaction-focused architecture is possible by using feature selection considering both effectiveness and efficiency. In this context, the pre-ranking with an interaction-focused architecture is studied in this paper.</p><p>In this paper, a pre-ranking model with tradeoff of both effectiveness and efficiency is proposed for a real-world e-commerce application, where the effectiveness improves significantly with slight decrease of efficiency shown in Figure <ref type="figure" target="#fig_2">1 (b)</ref>. The contributions of this paper are summarized as follows. 1) A pre-ranking model with interaction-focused architecture is proposed by inheriting the architecture of ranking model to solve the problem of performance loss of the model with representation-focused architecture. 2) A learnable Feature Selection method based on feature Complexity and variational Dropout (FSCD) is proposed to search for a set of effective and efficient feature fields for the pre-ranking model. 3) Extensive experiments are carried out which show the proposed approach achieves great improvement in effectiveness of the system compared to the conventional baselines, while the efficiency of the system is also maintained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED APPROACH</head><p>The proposed pre-ranking is derived from the ranking model. Both models utilize interaction-focused architectures, which shares an identical feature set ğ‘† = {ğ‘“ 1 , ğ‘“ 2 , ..., ğ‘“ ğ‘€ }, where ğ‘“ ğ‘— is the ğ‘—-th feature field in ğ‘†, and ğ‘€ is the number of feature fields. In the system, the ranking model employs all feature fields in ğ‘† to maintain effectiveness, while the pre-ranking model utilizes only a subset to reduce computational complexity and scores more items. In this context, offline processes such as sample generation can be reused for both models, which saves offline computational resources. The interaction-focused architecture of pre-ranking is inherited from that of ranking model, where both explicit interactive features and implicit interactive semantics can be utilized, which reduces the gap in the models' optimization objectives and achieves significant improvement of the model effectiveness for pre-ranking compared to the model with representation-focused architecture.</p><p>For introducing interaction-focused architecture into pre-ranking, a great challenge is the model efficiency. The overall efficiency of the pre-ranking model, whose learnable variables can be divided into feature embeddings ğ‘£ ğ‘£ ğ‘£ and dense weights ğ‘¤ ğ‘¤ ğ‘¤, is strongly influenced by the feature embeddings ğ‘£ ğ‘£ ğ‘£. For example, in real-world scenarios, storage for feature embeddings exceeds over 95% of that for the whole model, while feature generation process for embeddings consumes as much resources as that needed for the online inference of the model. Therefore, the feature selection for features with both high effectiveness and efficiency is important for the utilization of pre-ranking with interaction-focused architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FSCD for Pre-Ranking Model</head><p>Inspired by the Dropout FR method based on the variational dropout layer <ref type="bibr" target="#b0">[1]</ref> where the efficiency of the model is ignored, FSCD is proposed that considers both effectiveness and efficiency in a learnable process as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. In the proposed FSCD, the effectiveness is optimized by the cross entropy based loss function, while the efficiency is optimized by the feature-wise regularization term in Eq. ( <ref type="formula" target="#formula_2">3</ref>). Both effective and efficient features can be selected by FSCD in one single training process, while the expression ability of pre-ranking model is improved utilizing these features compared to the vector-product based model. The details are as follows. To select feature fields with both high effectiveness and high efficiency, each feature field ğ‘“ ğ‘— is expected to learn a dropout parameter ğ‘§ ğ‘— âˆˆ {0, 1} to indicate whether the feature field is dropped (ğ‘§ ğ‘— = 0) or preserved (ğ‘§ ğ‘— = 1). ğ‘“ ğ‘— 's embeddings ğ‘£ ğ‘— are multiplied by the dropout ğ‘§ ğ‘— to form the embedding layer, and ğ‘§ ğ‘— is subject to a Bernoulli distribution parameterized by ğœƒ ğ‘— , i.e.,</p><formula xml:id="formula_0">ğ‘§ ğ‘— âˆ¼ Bern(ğœƒ ğ‘— ),<label>(1)</label></formula><p>where the hyperparameter ğœƒ ğ‘— is the priori probability for the preservation of feature field ğ‘“ ğ‘— and is configured as function of feature complexity ğ‘ ğ‘— , i.e.,</p><formula xml:id="formula_1">ğœƒ ğ‘— = H (ğ‘ ğ‘— ) = 1 âˆ’ ğœ (ğ‘ ğ‘— ), ğ‘ ğ‘— = G(ğ‘œ ğ‘— , ğ‘’ ğ‘— , ğ‘› ğ‘— )<label>(2)</label></formula><p>where ğœ (â€¢) is the sigmoid function. ğœƒ ğ‘— = 1 âˆ’ ğœ (ğ‘ ğ‘— ) is one of alternatives to relate ğœƒ ğ‘— and ğ‘ ğ‘— which works well in practice. The feature complexity ğ‘ ğ‘— measures the computational and storage complexity of the ğ‘—-th feature field including but not limited to the online computational complexity ğ‘œ ğ‘— , the embedding dimension ğ‘’ ğ‘— , and the number of keys ğ‘› ğ‘— for one feature field, where ğ‘œ ğ‘— is configured according to the feature type specified in Section 3. </p><formula xml:id="formula_2">(||ğ‘¤ ğ‘¤ ğ‘¤ || 2 +||ğ‘£ ğ‘£ ğ‘£ || 2 ) + ğ‘€ âˆ‘ï¸ ğ‘—=1 ğ›¼ ğ‘— ğ‘§ ğ‘— ğ‘ ,<label>(3)</label></formula><p>where ğ›¼ ğ‘— is the regularization weight for ğ‘§ ğ‘— and can be derived as (See derivation details in Appendix A)</p><formula xml:id="formula_3">ğ›¼ ğ‘— = log(1 âˆ’ ğœƒ ğ‘— ) âˆ’ log(ğœƒ ğ‘— ). (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>ğ›¼ ğ‘— is a function that decreases with ğœƒ ğ‘— and increases with ğ‘ ğ‘— . Therefore, a feature with larger complexity ğ‘ ğ‘— is penalized with a larger value of ğ›¼ ğ‘— , and more likely to be dropped. In this way, the feature complexity is included in the proposed FSCD, which previous works do not address to the best of the authors' knowledge. Subject to Bernoulli distribution, ğ‘§ ğ‘— is discrete and not differentiable, it is relaxed to a differentiable function as <ref type="bibr" target="#b4">[5]</ref> </p><formula xml:id="formula_5">ğ‘§ ğ‘— = F (ğ›¿ ğ‘— ) =ğœ ( 1 ğ‘¡ (log(ğ›¿ ğ‘— ) âˆ’ log(1 âˆ’ ğ›¿ ğ‘— ) + log(ğ‘¢ ğ‘— ) âˆ’ log(1 âˆ’ ğ‘¢ ğ‘— ))),<label>(5)</label></formula><p>where ğ‘¢ ğ‘— âˆ¼ Uniform(0, 1) is subject to a uniform distribution and changes during the training process, while ğ‘¡ = 0.1 is a constant that works well in the experiments. F (ğ›¿ ğ‘— ) is close to 0 or 1 for most ğ›¿ ğ‘— values, which approximates the discrete Bernoulli distribution. In contrast to the dropout ğ‘§ ğ‘— , ğ›¿ ğ‘— âˆˆ (0, 1) is a differentiable parameter. Moreover, it acts as the posterior probability for feature preservation, which is influenced by the priori probability for feature preservation ğœƒ ğ‘— , and can be learned as the feature importance. In this context, the entire process of training for feature selection is built in Eq. ( <ref type="formula" target="#formula_2">3</ref>) and <ref type="bibr" target="#b4">(5)</ref>. Note that, the learnable variables are ğ‘£ ğ‘£ ğ‘£, ğ‘¤ ğ‘¤ ğ‘¤ and ğ›¿ ğ›¿ ğ›¿, which are trained simultaneously. Through a fast convergence of ğ›¿ ğ›¿ ğ›¿, the feature set of the pre-ranking model can then be obtained by selecting the feature fields with top-ğ¾ of ğ›¿ ğ›¿ ğ›¿ values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Tuning the Pre-Ranking Model</head><p>After feature selection, the feature fields not in the pre-ranking set acquired by FSCD are masked, and the models are fine-tuned using weights ğ‘£ ğ‘£ ğ‘£ and ğ‘¤ ğ‘¤ ğ‘¤ as initialization parameters. Concretely, the model can be trained with the following loss function</p><formula xml:id="formula_6">ğ¿(ğ‘£ ğ‘£ ğ‘£ â€² ,ğ‘¤ ğ‘¤ ğ‘¤ â€² ) = âˆ’ ğ‘ âˆ‘ï¸ ğ‘–=1 logğ‘ (ğ‘¦ ğ‘– |ğ‘“ (ğ‘¥ â€² ğ‘– , ğ‘£ ğ‘£ ğ‘£ â€² ,ğ‘¤ ğ‘¤ ğ‘¤ â€² )),<label>(6)</label></formula><p>where ğ‘¥ â€² ğ‘– , ğ‘£ ğ‘£ ğ‘£ â€² , and ğ‘¤ ğ‘¤ ğ‘¤ â€² are the samples with the selected feature fields, the remaining feature embeddings, and dense weights for the preranking model, respectively. ğ‘£ ğ‘£ ğ‘£ â€² and ğ‘¤ ğ‘¤ ğ‘¤ â€² are initialized by ğ‘£ ğ‘£ ğ‘£ and ğ‘¤ ğ‘¤ ğ‘¤, which accelerates the training. The Bernoulli dropout ğ‘§ ğ‘§ ğ‘§ is omitted in the fine-tuning process.</p><p>In this way, the pre-ranking model is obtained. As the model is in an interaction-focused architecture and adopts both effective and efficient feature fields, its expression ability significantly improves. Therefore, the effectiveness of the system can be improved with high efficiency. The entire method is illustrated in Algorithm 1. 1: Calculate ğ‘ ğ‘— , ğœƒ ğ‘— and ğ›¼ ğ‘— for each feature field using Eq. ( <ref type="formula" target="#formula_1">2</ref>) and (4).</p><p>2: Build the model with interaction-focused architecture where the feature embeddings are multiplied by ğ‘§ ğ‘— parameterized by ğ›¿ ğ‘— in Eq. ( <ref type="formula" target="#formula_5">5</ref>). 3: Train using Eq. ( <ref type="formula" target="#formula_2">3</ref>), and then select the feature fields with top-ğ¾ of ğ›¿ ğ‘— . 4: Fine-tune according to Eq. ( <ref type="formula" target="#formula_6">6</ref>) and obtain M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Experiment Configurations</head><p>Evaluations are mainly based on the online sponsored search system for a real-world e-commerce app named Taobao, where both the click-through rate (CTR) and response per mile (RPM), which evaluates platform revenue, are optimized. The proposed effective and efficient pre-ranking are compared with the baselines of the vector-product based model <ref type="bibr" target="#b3">[4]</ref> and COLD <ref type="bibr" target="#b11">[12]</ref> based pre-ranking.</p><p>Both the pre-ranking and ranking models are based on an interactionfocused architecture. After the feature fields are transformed into embeddings, hidden layers with sizes of 1024, 512, and 256 are adopted for the ranking, while only 2 hidden layers with sizes of 1024 and 256 are used for the pre-ranking to ensure high efficiency. Finally, the sigmoid function is utilized to predict the final CTRs for both models.</p><p>The feature set for the pre-ranking model is a subset of that for the ranking model, which is selected by the proposed FSCD. All feature sets, consisting of 246 feature fields in their entirety,  <ref type="bibr" target="#b0">[1]</ref>, the DFS method <ref type="bibr" target="#b5">[6]</ref>, and the proposed FSCD. include different types, e.g., simple features that directly look up embeddings and complex features that require complicated computations for online embeddings. Each type can be divided into either user/query features or item correlated features which include item features and interactive features between item and user/query. The user/query features are computed only once for online inference regardless of the number of candidate advertisements and require less computational consumption, while the item correlated features should be computed as many times as the number of advertisements to be scored, thus consuming much more computational resources.</p><p>The ğ‘œ ğ‘— values are determined by the above feature types and the detailed configurations are listed in Table <ref type="table" target="#tab_1">1</ref>. For other hyperparameters, ğ›¾ 1 = 1, ğ›¾ 2 = 10 âˆ’2 , and ğ›¾ 3 = 10 âˆ’7 are configured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of FSCD</head><p>After the proposed FSCD process, the feature fields are ranked using the output parameter ğ›¿ ğ›¿ ğ›¿. Table <ref type="table" target="#tab_2">2</ref> lists the minimum, median, and maximum ranking indices for different types of features using Dropout FR <ref type="bibr" target="#b0">[1]</ref> and deep feature selection (DFS) method <ref type="bibr" target="#b5">[6]</ref>. For the proposed FSCD in this paper, the ranking indices for the simple features are much smaller than those for the complex feature types (see median and maximum). The simple features tend to have a more forward ranking, while the complicated features are backward, which emphasizes the efficiency of the features. However, the minimum ranking index of type IV features is only 8, which means that a feature with large complexity can still rank forward if it is truly effective for the training task. For the conventional methods, the rankings of the features are less dependent on the feature type. The pre-ranking models with different feature field number ğ¾ are well trained based on Eq. ( <ref type="formula" target="#formula_6">6</ref>) with 2 billion samples. The area under the curve (AUC), latency, and CPU consumption for online inference of the different models are illustrated in Table <ref type="table" target="#tab_4">3</ref>. The results of the conventional feature selection methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> are also provided for comparison. Specifically, when ğ¾ = 246, the feature set of pre-ranking model becomes exactly the same as that of the ranking model. The results show that the proposed FSCD has slightly smaller AUC than the other methods when identical ğ¾ values are considered. However, the complexity is extremely   reduced. When ğ¾ = 100, the AUC difference relative to that of the model for the other methods is only 0.0026, while the complexity is approximately 30% lower than that of the other methods for both latency and CPU cost. When ğ¾ &gt; 100, the AUC increases slowly, while the complexity increases significantly. Therefore, the pre-ranking utilizes the feature fields that have top-100 of ğ›¿ values as the final feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Comparisons</head><p>Table <ref type="table" target="#tab_5">4</ref> and Table <ref type="table" target="#tab_6">5</ref> show the offline and online experimental results using the proposed effectiveness and efficiency based pre-ranking, while the original vector-product and COLD based pre-rankings are configured as benchmarks. Each model is trained with more than 200 billion real-world e-commerce samples from a log system to achieve the best online performance. Therefore, the offline AUCs are much larger than those shown in Table <ref type="table" target="#tab_4">3</ref>. The recall rate is defined as the preserved probability of pre-ranking model for the top-5 items ranked by the ranking model. The offline results in Table <ref type="table" target="#tab_5">4</ref> show that the proposed model is much more effective than the vector-product based model, while it is slightly less effective than COLD due to efficiency considerations. For the online effect, 30 continuous days of mobile real-world requests are evaluated and the results are shown in Table <ref type="table" target="#tab_6">5</ref>. It shows that the proposed pre-ranking model obtains a good balance for online CTR and RPM performance and gains significant improvement in both CTR and RPM compared with its counterparts.</p><p>Finally, the efficiency is analyzed. Although a complicated preranking with an interaction-focused architecture is adopted in the proposed approach, the feature selection method based on effectiveness and efficiency optimizes the computational complexity to a low degree. Moreover, the number of input item is reduced from 6000 to 800, which further reduces the overall complexity of the system. In this context, the online CPU consumption of the system is almost the same as that of the vector-product based model, while the response time is slightly increased. In regard to the COLD model, the response time and CPU consumption are both greater than those of the proposed approach, which means the proposed pre-ranking model is more efficient. The key metrics for efficiency are listed in Table <ref type="table" target="#tab_6">5</ref> at a peak number of queries per second (QPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>In this paper, to solve the problem of performance loss caused by the pre-ranking model with representation-focused architecture, a pre-ranking model based on feature selection with joint optimization for both effectiveness and efficiency is proposed in an interaction-focused architecture. The pre-ranking model with interaction-focused architecture, which is inherited from that of ranking model, utilizes the feature subset selected by the proposed learnable FSCD, which includes not only simple features but also interactive features with significant effectiveness. The experiments on offline training demonstrate the validity of the proposed FSCD method in both effectiveness and efficiency. Moreover, the offline and online effects in the real-world sponsored search system illustrate the performance improvements in the proposed pre-ranking model compared to the conventional benchmarks. The proposed pre-ranking has been utilized as an online running model for a real-world sponsored search system and has generated substantial revenue for the company.</p><p>A DERIVATION FOR EQ. (3) AND <ref type="bibr" target="#b3">(4)</ref> Assuming that ğ‘£ ğ‘£ ğ‘£ and ğ‘¤ ğ‘¤ ğ‘¤ are subject to a joint Gaussian distribution <ref type="bibr" target="#b7">[8]</ref>, the joint distribution for ğ‘£ ğ‘£ ğ‘£, ğ‘¤ ğ‘¤ ğ‘¤ and ğ‘§ ğ‘§ ğ‘§ is ğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§) = N (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤ |0 0 0, Î£)</p><formula xml:id="formula_7">ğ‘€ ğ‘—=1 Bern(ğ‘§ ğ‘— |ğœƒ ğ‘— ),<label>(7)</label></formula><p>where ğ‘£ ğ‘£ ğ‘£, ğ‘¤ ğ‘¤ ğ‘¤ and ğ‘§ ğ‘§ ğ‘§ are all learnable variables. To optimize these variables, the loss function can be concluded by maximizing the posterior probability of ğ‘£ ğ‘£ ğ‘£, ğ‘¤ ğ‘¤ ğ‘¤ and ğ‘§ ğ‘§ ğ‘§ given the training samples D. In this context, by applying Bayesian rule <ref type="bibr" target="#b9">[10]</ref> </p><p>The first part is the cross entropy, while the second part âˆ’logğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§) can be rewritten according to Eq. ( <ref type="formula" target="#formula_7">7</ref> </p><p>where ğœ†, ğ›¼ ğ‘— and ğ¶ are all constants derived by Eq. <ref type="bibr" target="#b6">(7)</ref>. ğœ†(||ğ‘¤ || 2 + ||ğ‘£ || 2 ) is the common ğ‘™ 2 regularization term with weight ğœ†. The nontrivial term ğ‘€ ğ‘—=1 ğ›¼ ğ‘— ğ‘§ ğ‘— is a new regularization term derived by the Bernoulli distribution, where the regularization factor ğ›¼ ğ‘— is derived by Eq. ( <ref type="formula" target="#formula_0">1</ref>) and <ref type="bibr" target="#b6">(7)</ref> as</p><formula xml:id="formula_10">ğ›¼ ğ‘— = log(1 âˆ’ ğœƒ ğ‘— ) âˆ’ log(ğœƒ ğ‘— ).<label>(10)</label></formula><p>Then the total loss function can be written as Eq. ( <ref type="formula" target="#formula_2">3</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Real-world multi-stage ranking architecture. (a) Multi-stage architecture including matching, pre-ranking, ranking, and re-ranking with scoring item numbers. (b) Intuitive view for effectiveness and efficiency of pre-ranking and ranking, where the pre-ranking is derived from ranking and optimized to an interaction-focused architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed FSCD for pre-ranking model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>The Proposed Pre-ranking based on FSCD Input: Feature set ğ‘† = {ğ‘“ 1 , ğ‘“ 2 , ..., ğ‘“ ğ‘€ }, feature complexity metrics {ğ‘œ ğ‘— , ğ‘’ ğ‘— , ğ‘› ğ‘— }, training samples D, hyperparameters ğ›¾ 1,2,3 . Output: Pre-ranking model M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, maximizing ğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§|D) âˆ ğ‘ƒ (D|ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§)ğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§) is equivalent to: ğ‘£ ğ‘£ ğ‘£ * ,ğ‘¤ ğ‘¤ ğ‘¤ * ,ğ‘§ ğ‘§ ğ‘§ * = arg min ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§ âˆ’logğ‘ƒ (D|ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§) âˆ’ logğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) as âˆ’logğ‘ƒ (ğ‘£ ğ‘£ ğ‘£,ğ‘¤ ğ‘¤ ğ‘¤,ğ‘§ ğ‘§ ğ‘§) = ğœ†(||ğ‘¤ || 2 + ||ğ‘£ || 2 ) + ğ‘€ âˆ‘ï¸ ğ‘—=1 ğ›¼ ğ‘— ğ‘§ ğ‘— + ğ¶,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Different types of features for the entire feature set with the values of ğ‘œ ğ‘— .</figDesc><table><row><cell cols="2">feature type index</cell><cell cols="2">feature type</cell><cell>ğ‘œ ğ‘—</cell></row><row><cell>I</cell><cell cols="3">Simple query/user features that directly look up embeddings</cell><cell>0.4</cell></row><row><cell>II</cell><cell cols="3">Simple item correlated features that require complicated computations</cell><cell>1.5</cell></row><row><cell>III</cell><cell cols="3">Complex query/user features that directly look up embeddings</cell><cell>1.0</cell></row><row><cell>IV</cell><cell cols="3">Complex item correlated features that require complicated computations</cell><cell>3.0</cell></row><row><cell>feature type</cell><cell>number of</cell><cell>minimum</cell><cell>median</cell><cell>maximum</cell></row><row><cell>index</cell><cell>feature fields</cell><cell>ranking</cell><cell>ranking</cell><cell>ranking</cell></row><row><cell>I</cell><cell>141</cell><cell>1/2/1</cell><cell>129/137/85</cell><cell>244/246/156</cell></row><row><cell>II</cell><cell>5</cell><cell>15/31/18</cell><cell>36/44/42</cell><cell>99/52/159</cell></row><row><cell>III</cell><cell>68</cell><cell>3/1/4</cell><cell cols="2">137/131/185 246/243/231</cell></row><row><cell>IV</cell><cell>32</cell><cell>7/3/8</cell><cell>108/58/229</cell><cell>178/239/246</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Feature ranking and distributions for different types of features, where the ranking results for different methods are provided and divided by slashes in order, including the Dropout FR method</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>AUC results, latency, and CPU consumption for the first ğ¾ feature fields of the proposed FSCD and conventional methods. Each model is trained with 2 billion samples.</figDesc><table><row><cell>Methods</cell><cell cols="2">recall rate offline AUC</cell></row><row><cell>Vector-product based model</cell><cell>88%</cell><cell>0.695</cell></row><row><cell>COLD model</cell><cell>96%</cell><cell>0.738</cell></row><row><cell>The proposed model</cell><cell>95%</cell><cell>0.737</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Offline results of the proposed pre-ranking model and the conventional models. Each model is trained with more than 200 billion samples.</figDesc><table><row><cell>Methods</cell><cell>input item number</cell><cell>CTR</cell><cell>RPM</cell><cell>response time</cell><cell>CPU consumption</cell></row><row><cell>vector-product based model</cell><cell>6000</cell><cell>/</cell><cell>/</cell><cell>58.4 ms</cell><cell>79%</cell></row><row><cell>COLD model</cell><cell>600</cell><cell cols="2">+1.11% +1.04%</cell><cell>62.3 ms</cell><cell>85%</cell></row><row><cell>The proposed model</cell><cell>800</cell><cell cols="2">+1.54% +2.76%</cell><cell>59.9 ms</cell><cell>79%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Online effectiveness and efficiency of the proposed pre-ranking model and the conventional models on realworld mobile online queries.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dropout feature ranking for deep learning models</title>
		<author>
			<persName><forename type="first">Chun-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ladislav</forename><surname>Rampasek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08645</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint optimization of cascade ranking models</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Culpepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep look into neural ranking models for information retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102067</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
				<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep feature selection: theory and application to identify enhancers and promoters</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wyeth</forename><forename type="middle">W</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="322" to="336" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascade ranking for operational e-commerce search</title>
		<author>
			<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1557" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pbodl: Parallel bayesian online deep learning for click-through rate prediction in tencent advertising system</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00802</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designing efficient cascaded classifiers: tradeoff between accuracy and cost</title>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Vikas C Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<author>
			<persName><surname>Bpr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cold: Towards the next generation of pre-ranking system</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16122</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Eenmf: An end-to-end neural matching framework for e-commerce sponsored search</title>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daorui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01190</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
