<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-based Editing of Talking-head Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-04">4 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eli</forename><surname>Shecht- Man</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics MICHAEL ZOLLHÖFER</orgName>
								<orgName type="institution" key="instit1">AYUSH TEWARI</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">ADAM FINKELSTEIN</orgName>
								<orgName type="institution">Princeton University ELI SHECHTMAN</orgName>
								<address>
									<settlement>Adobe DAN B GOLDMAN KYLE GENOVA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Max Planck Institute for Informatics MANEESH AGRAWALA</orgName>
								<orgName type="institution">Princeton University ZEYU JIN</orgName>
								<address>
									<settlement>Adobe CHRISTIAN THEOBALT</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Ayush Tewari</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Max Planck Insti-tute for Informatics</orgName>
								<orgName type="institution" key="instit3">Michael Zollhöfer</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
								<address>
									<addrLine>Adam Finkelstein</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Max Planck Institute for Informatics; Maneesh Agrawala</orgName>
								<orgName type="institution">Prince-ton University</orgName>
								<address>
									<addrLine>Princeton Uni-versity; Zeyu Jin, Christian Theobalt</addrLine>
									<settlement>Eli Shechtman, Adobe, Dan B Goldman, Kyle Genova, Adobe</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Text-based Editing of Talking-head Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-04">4 Jun 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3306346.3323028</idno>
					<idno type="arXiv">arXiv:1906.01524v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>as well as convincing language translation and full sentence synthesis. CCS Concepts:</term>
					<term>Information systems → Video search</term>
					<term>Speech / audio search</term>
					<term>• Computing methodologies → Computational photography</term>
					<term>Reconstruction</term>
					<term>Motion processing</term>
					<term>Graphics systems and interfaces Text-based video editing, talking heads, visemes, dubbing, face tracking, face parameterization, neural rendering</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. We propose a novel text-based editing approach for talking-head video. Given an edited transcript, our approach produces a realistic output video in which the dialogue of the speaker has been modified and the resulting video maintains a seamless audio-visual flow (i.e. no jump cuts).</p><p>Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Talking-head video -framed to focus on the face and upper body of a speaker -is ubiquitous in movies, TV shows, commercials, YouTube video logs, and online lectures. Editing such pre-recorded video is challenging, but can be needed to emphasize particular content, remove filler words, correct mistakes, or more generally match the editor's intent. Using current video editing tools, like Adobe Premiere, skilled editors typically scrub through raw video footage to find relevant segments and assemble them into the desired story. They must carefully consider where to place cuts so as to minimize disruptions to the overall audio-visual flow. <ref type="bibr" target="#b2">Berthouzoz et al. [2012]</ref> introduce a text-based approach for editing such videos. Given an input video, they obtain a time-aligned transcript and allow editors to cut and paste the text to assemble it into the desired story. Their approach can move or delete segments, while generating visually seamless transitions at cut boundaries. However, this method only produces artifact-free results when these boundaries are constrained to certain well-behaved segments of the video (e.g. where the person sits still between phrases or sentences).</p><p>Neither conventional editing tools nor the text-based approach allow synthesis of new audio-visual speech content. Thus, some modifications require either re-shooting the footage or overdubbing existing footage with new wording. Both methods are expensive as they require new performances, and overdubbing generally produces mismatches between the visible lip motion and the audio.</p><p>This paper presents a method that completes the suite of operations necessary for transcript-based editing of talking-head video. Specifically, based only on text edits, it can synthesize convincing new video of a person speaking, and produce seamless transitions even at challenging cut points such as the middle of an utterance.</p><p>Our approach builds on a thread of research for synthesizing realistic talking-head video. The seminal Video Rewrite system of <ref type="bibr" target="#b6">Bregler et al. [1997]</ref> and the recent Synthesizing Obama project of <ref type="bibr" target="#b58">Suwajanakorn et al. [2017]</ref> take new speech recordings as input, and superimpose the corresponding lip motion over talking-head video. While the latter state-of-the art approach can synthesize fairly accurate lip sync, it has been shown to work for exactly one talking head because it requires huge training data (14 hours). This method also relies on input audio from the same voice on which it was trained -from either Obama or a voice impersonator. In contrast our approach works from text and therefore supports applications that require a different voice, such as translation.</p><p>Performance-driven puppeteering and dubbing methods, such as VDub <ref type="bibr" target="#b17">[Garrido et al. 2015]</ref>, Face2Face <ref type="bibr" target="#b63">[Thies et al. 2016]</ref> and Deep Video Portraits <ref type="bibr" target="#b32">[Kim et al. 2018b]</ref>, take a new talking-head performance (usually from a different performer) as input and transfer the lip and head motion to the original talking-head video. Because these methods have access to video as input they can often produce higher-quality synthesis results than the audio-only methods. Nevertheless, capturing new video for this purpose is obviously more onerous than typing new text.</p><p>Our method accepts text only as input for synthesis, yet builds on the Deep Video Portraits approach of <ref type="bibr" target="#b32">Kim et al. [2018b]</ref> to craft synthetic video. Our approach drives a 3D model by seamlessly stitching different snippets of motion tracked from the original footage. The snippets are selected based on a dynamic programming optimization that searches for sequences of sounds in the transcript that should look like the words we want to synthesize, using a novel visemebased similarity measure. These snippets can be re-timed to match the target viseme sequence, and are blended to create a seamless mouth motion. To synthesize output video, we first create a synthetic composite video in which the lower face region is masked out. In cases of inserting new text, we retime the rest of the face and background from the boundaries. The masked out region is composited with a synthetic 3D face model rendering using the mouth motion found earlier by optimization (Figure <ref type="figure">5</ref>). The composite exhibits the desired motion, but lacks realism due to the incompleteness and imperfections of the 3D face model. For example, facial appearance does not perfectly match, dynamic high-frequency detail is missing, and the mouth interior is absent. Nonetheless, these data are sufficient cues for a new learned recurrent video generation network to be able to convert them to realistic imagery. The new composite representation and the recurrent network formulation significantly extend the neural face translation approach of <ref type="bibr" target="#b32">Kim et al. [2018b]</ref> to text-based editing of existing videos.</p><p>We show a variety of text-based editing results and favorable comparisons to previous techniques. In a crowd-sourced user study, our edits were rated to be real in 59.6% of cases. The main technical contributions of our approach are:</p><p>• A text-based editing tool for talking-head video that lets editors insert new text, in addition to cutting and copy-pasting in an existing transcript. • A dynamic programming based strategy tailored to video synthesis that assembles new words based on snippets containing sequences of observed visemes in the input video. • A parameter blending scheme that, when combined with our synthesis pipeline, produces seamless talking heads, even when combining snippets with different pose and expression. • A recurrent video generation network that converts a composite of real background video and synthetically rendered lower face into a photorealistic video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Ethical Considerations</head><p>Our text-based editing approach lays the foundation for better editing tools for movie post production. Filmed dialogue scenes often require re-timing or editing based on small script changes, which currently requires tedious manual work. Our editing technique also enables easy adaptation of audio-visual video content to specific target audiences: e.g., instruction videos can be fine-tuned to audiences of different backgrounds, or a storyteller video can be adapted to children of different age groups purely based on textual script edits. In short, our work was developed for storytelling purposes. However, the availability of such technology -at a quality that some might find indistinguishable from source material -also raises important and valid concerns about the potential for misuse. Although methods for image and video manipulation are as old as the media themselves, the risks of abuse are heightened when applied to a mode of communication that is sometimes considered to be authoritative evidence of thoughts and intents. We acknowledge that bad actors might use such technologies to falsify personal statements and slander prominent individuals. We are concerned about such deception and misuse.</p><p>Therefore, we believe it is critical that video synthesized using our tool clearly presents itself as synthetic. The fact that the video is synthesized may be obvious by context (e.g. if the audience understands they are watching a fictional movie), directly stated in the video or signaled via watermarking. We also believe that it is essential to obtain permission from the performers for any alteration before sharing a resulting video with a broad audience. Finally, it is important that we as a community continue to develop forensics, fingerprinting and verification techniques (digital and non-digital) to identify manipulated video. Such safeguarding measures would reduce the potential for misuse while allowing creative uses of video editing technologies like ours.</p><p>We hope that publication of the technical details of such systems can spread awareness and knowledge regarding their inner workings, sparking and enabling associated research into the aforementioned forgery detection, watermarking and verification systems. Finally, we believe that a robust public conversation is necessary to create a set of appropriate regulations and laws that would balance the risks of misuse of these tools against the importance of creative, consensual use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Facial Reenactment. Facial video reenactment has been an active area of research [Averbuch- <ref type="bibr" target="#b0">Elor et al. 2017;</ref><ref type="bibr" target="#b16">Garrido et al. 2014;</ref><ref type="bibr" target="#b30">Kemelmacher-Shlizerman et al. 2010;</ref><ref type="bibr" target="#b35">Li et al. 2014;</ref><ref type="bibr" target="#b38">Liu et al. 2001;</ref><ref type="bibr" target="#b58">Suwajanakorn et al. 2017;</ref><ref type="bibr" target="#b67">Vlasic et al. 2005]</ref>. <ref type="bibr" target="#b63">Thies et al. [2016]</ref> recently demonstrated real-time video reenactment. Deep video portraits <ref type="bibr" target="#b32">[Kim et al. 2018b</ref>] enables full control of the head pose, expression, and eye gaze of a target actor based on recent advances in learning-based image-to-image translation <ref type="bibr" target="#b26">[Isola et al. 2017]</ref>. Some recent approaches enable the synthesis of controllable facial animations from single images [Averbuch- <ref type="bibr" target="#b0">Elor et al. 2017;</ref><ref type="bibr" target="#b19">Geng et al. 2018;</ref><ref type="bibr" target="#b70">Wiles et al. 2018]</ref>. <ref type="bibr" target="#b42">Nagano et al. [2018]</ref> recently showed how to estimate a controllable avatar of a person from a single image. We employ a facial reenactment approach for visualizing our text-based editing results and show how facial reenactment can be tackled by neural face rendering.</p><p>Visual Dubbing. Facial reenactment is the basis for visual dubbing, since it allows to alter the expression of a target actor to match the motion of a dubbing actor that speaks in a different language. Some dubbing approaches are speech-driven <ref type="bibr" target="#b6">[Bregler et al. 1997;</ref><ref type="bibr" target="#b9">Chang and Ezzat 2005;</ref><ref type="bibr" target="#b13">Ezzat et al. 2002;</ref><ref type="bibr" target="#b36">Liu and Ostermann 2011]</ref> others are performance-driven <ref type="bibr" target="#b17">[Garrido et al. 2015</ref>]. Speechdriven approaches have been shown to produce accurate lip-synced video <ref type="bibr" target="#b58">[Suwajanakorn et al. 2017]</ref>. While this approach can synthesize fairly accurate lip-synced video, it requires the new audio to sound similar to the original speaker, while we enable synthesis of new video using text-based edits. <ref type="bibr" target="#b40">Mattheyses et al. [2010]</ref> show results with no head motion, in a controlled setup with uniform background. In contrast, our 3D based approach and neural renderer can produce subtle phenomena such as lip rolling, and works in a more general setting.</p><p>Speech animation for rigged models. Several related methods produce animation curves for speech <ref type="bibr" target="#b12">[Edwards et al. 2016;</ref><ref type="bibr" target="#b59">Taylor et al. 2017;</ref><ref type="bibr" target="#b73">Zhou et al. 2018]</ref>. They are specifically designed for animated 3D models and not for photorealistic video, requiring a character rig and artist supplied rig correspondence. In contrast, our approach "animates" a real person speaking, based just on text and a monocular recording of the subject.</p><p>Text-Based Video and Audio Editing. Researchers have developed a variety of audio and video editing tools based on time-aligned transcripts. These tools allow editors to shorten and rearrange speech for audio podcasts <ref type="bibr" target="#b52">[Rubin et al. 2013;</ref><ref type="bibr" target="#b56">Shin et al. 2016</ref>], annotate video with review feedback <ref type="bibr">[Pavel et al. 2016]</ref>, provide audio descriptions of the video content for segmentation of B-roll footage <ref type="bibr" target="#b65">[Truong et al. 2016]</ref> and generate structured summaries of lecture videos <ref type="bibr" target="#b46">[Pavel et al. 2014]</ref>. <ref type="bibr" target="#b33">Leake et al. [2017]</ref> use the structure imposed by timealigned transcripts to automatically edit together multiple takes of a scripted scene based on higher-level cinematic idioms specified by the editor. <ref type="bibr">Berthouzoz et al.'s [2012]</ref> tool for editing interviewstyle talking-head video by cutting, copying and pasting transcript text is closest to our work. While we similarly enable rearranging video by cutting, copying and pasting text, unlike all of the previous text-based editing tools, we allow synthesis of new video by simply typing the new text into the transcript.</p><p>Audio Synthesis. In transcript-based video editing, synthesizing new video clips would often naturally be accompanied by audio synthesis. Our approach to video is independent of the audio, and therefore a variety of text to speech (TTS) methods can be used. Traditional TTS has explored two general approaches: parametric methods (e.g. <ref type="bibr" target="#b72">[Zen et al. 2009]</ref>) generate acoustic features based on text, and then synthesize a waveform from these features. Due to oversimplified acoustic models, they tend to sound robotic. In contrast, unit selection is a data driven approach that constructs new waveforms by stitching together small pieces of audio (or units) found elsewhere in the transcript <ref type="bibr" target="#b23">[Hunt and Black 1996]</ref>. Inspired by the latter, the VoCo project of <ref type="bibr" target="#b27">Jin et al. [2017]</ref> performs a search in the existing recording to find short ranges of audio that can be stitched together such that they blend seamlessly in the context around an insertion point. Section 4 and the accompanying video present a few examples of using our method to synthesize new words in video, coupled with the use of VoCo to synthesize corresponding audio. Current state-of-the-art TTS approaches rely on deep learning <ref type="bibr" target="#b54">[Shen et al. 2018;</ref><ref type="bibr" target="#b66">Van Den Oord et al. 2016</ref>]. However, these methods require a huge (tens of hours) training corpus for the target speaker.</p><p>Deep Generative Models. Very recently, researchers have proposed Deep Generative Adversarial Networks (GANs) for the synthesis of images and videos. Approaches create new images from scratch <ref type="bibr" target="#b10">[Chen and Koltun 2017;</ref><ref type="bibr" target="#b21">Goodfellow et al. 2014;</ref><ref type="bibr" target="#b28">Karras et al. 2018;</ref><ref type="bibr" target="#b47">Radford et al. 2016;</ref><ref type="bibr" target="#b69">Wang et al. 2018b</ref>] or condition the synthesis on an input image <ref type="bibr" target="#b26">[Isola et al. 2017;</ref><ref type="bibr" target="#b41">Mirza and Osindero 2014]</ref>. High-resolution conditional video synthesis <ref type="bibr" target="#b68">[Wang et al. 2018a]</ref> has recently been demonstrated. Besides approaches that require a paired training corpus, unpaired video-to-video translation techniques <ref type="bibr" target="#b1">[Bansal et al. 2018</ref>] only require two training videos. Video-tovideo translation has been used in many applications. For example, impressive results have been shown for the reenactment of the human head <ref type="bibr" target="#b44">[Olszewski et al. 2017]</ref>, head and upper body <ref type="bibr" target="#b32">[Kim et al. 2018b]</ref>, and the whole human body <ref type="bibr" target="#b8">[Chan et al. 2018;</ref><ref type="bibr" target="#b37">Liu et al. 2018</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular 3D Face Reconstruction.</head><p>There is a large body of work on reconstructing facial geometry and appearance from a single image using optimization methods <ref type="bibr" target="#b14">[Fyffe et al. 2014;</ref><ref type="bibr" target="#b18">Garrido et al. 2016;</ref><ref type="bibr" target="#b25">Ichim et al. 2015;</ref><ref type="bibr" target="#b29">Kemelmacher-Shlizerman 2013;</ref><ref type="bibr" target="#b51">Roth et al. 2017;</ref><ref type="bibr" target="#b55">Shi et al. 2014;</ref><ref type="bibr" target="#b58">Suwajanakorn et al. 2017;</ref><ref type="bibr" target="#b63">Thies et al. 2016</ref>]. Many of these techniques employ a parametric face model <ref type="bibr" target="#b3">[Blanz et al. 2004;</ref><ref type="bibr" target="#b4">Blanz and Vetter 1999;</ref><ref type="bibr" target="#b5">Booth et al. 2018</ref>] as a prior to better constrain the reconstruction problem. Recently, deep learning-based approaches have been proposed that train a convolutional network We first align phonemes to the input audio and track each input frame to construct a parametric head model. Then, for a given edit operation (changing spider to fox), we find segments of the input video that have similar visemes to the new word. In the above case we use viper and ox to construct fox. We use blended head parameters from the corresponding video frames, together with a retimed background sequence, to generate a composite image, which is used to generate a photorealistic frame using our neural face rendering method. In the resulting video, the actress appears to be saying fox, even though that word was never spoken by her in the original recording.</p><p>to directly regress the model parameters <ref type="bibr" target="#b11">[Dou et al. 2017;</ref><ref type="bibr" target="#b20">Genova et al. 2018;</ref><ref type="bibr" target="#b48">Richardson et al. 2016;</ref><ref type="bibr" target="#b60">Tewari et al. 2018a</ref><ref type="bibr" target="#b62">Tewari et al. , 2017;;</ref><ref type="bibr" target="#b64">Tran et al. 2017]</ref>. Besides model parameters, other approaches regress detailed depth maps <ref type="bibr" target="#b49">[Richardson et al. 2017;</ref><ref type="bibr" target="#b53">Sela et al. 2017]</ref>, or 3D displacements <ref type="bibr" target="#b7">[Cao et al. 2015;</ref><ref type="bibr" target="#b22">Guo et al. 2018;</ref><ref type="bibr" target="#b61">Tewari et al. 2018b</ref>].</p><p>Face reconstruction is the basis for a large variety of applications, such as facial reenactment and visual dubbing. For more details on monocular 3D face reconstruction, we refer to <ref type="bibr" target="#b74">Zollhöfer et al. [2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Our system takes as input a video recording of a talking head with a transcript of the speech and any number of edit operations specified on the transcript. Our tool supports three types of edit operations;</p><p>• Add new words: the edit adds one or more consecutive words at a point in the video (e.g. because the actor skipped a word or the producer wants to insert a phrase). • Rearrange existing words: the edit moves one or more consecutive words that exist in the video (e.g. for better word ordering without introducing jump cuts). • Delete existing words: the edit removes one or more consecutive words from the video (e.g. for simplification of wording and removing filler such as "um" or "uh").</p><p>We represent editing operations by the sequence of words W in the edited region as well as the correspondence between those words and the original transcript. For example, deleting the word "wonderful" in the sequence "hello wonderful world" is specified as ('hello', 'world') and adding the word "big" is specified as ('hello', 'big', 'world').</p><p>Our system processes these inputs in five main stages (Figure <ref type="figure" target="#fig_0">2</ref>). In the phoneme alignment stage (Section 3.1) we align the transcript to the video at the level of phonemes and then in the tracking and reconstruction stage (Section 3.2) we register a 3D parametric head model with the video. These are pre-processing steps performed once per input video. Then for each edit operation W we first perform a viseme search (Section 3.3) to find the best visual match between the subsequences of phonemes in the edit and subsequences of phonemes in the input video. We also extract a region around the edit location to act as a background sequence, from which we will extract background pixels and pose data. For each subsequence we blend the parameters of the tracked 3D head model (Section 3.4) and then use the resulting parameter blended animation of the 3D head, together with the background pixels, to render a realistic full-frame video (Section 3.5) in which the subject appears to say the edited sequence of words. Our viseme search and approach for combining shorter subsequences with parameter blending is motivated by the phoneme/viseme distribution of the English language (Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phoneme Alignment</head><p>Phonemes are perceptually distinct units that distinguish one word from another in a specific language. Our method relies on phonemes to find snippets in the video that we later combine to produce new content. Thus, our first step is to compute the identity and timing of phonemes in the input video. To segment the video's speech audio into phones (audible realizations of phonemes), we assume we have an accurate text transcript and align it to the audio using P2FA <ref type="bibr" target="#b52">[Rubin et al. 2013;</ref><ref type="bibr" target="#b71">Yuan and Liberman 2008]</ref>, a phoneme-based alignment tool. This gives us an ordered sequence V = (v 1 , . . . , v n ) of phonemes, each with a label denoting the phoneme name, start time, and end time v</p><formula xml:id="formula_0">i = (v lbl i , v in i , v out i ).</formula><p>Note that if a transcript is not given as part of the input, we can use automatic speech transcription tools <ref type="bibr">[IBM 2016;</ref><ref type="bibr" target="#b43">Ochshorn and Hawkins 2016]</ref> or crowdsourcing transcription services like rev.com to obtain it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Face Tracking and Reconstruction</head><p>We register a 3D parametric face model with each frame of the input talking-head video. The parameters of the model (e.g. expression, head pose, etc.) will later allow us to selectively blend different aspects of the face (e.g. take the expression from one frame and pose from another). Specifically, we apply recent work on monocular model-based face reconstruction <ref type="bibr" target="#b18">[Garrido et al. 2016;</ref><ref type="bibr" target="#b63">Thies et al. 2016</ref>]. These techniques parameterize the rigid head pose T ∈ SE(3), the facial geometry α ∈ R 80 , facial reflectance β ∈ R 80 , facial expression δ ∈ R 64 , and scene illumination γ ∈ R 27 . Model fitting is based on the minimization of a non-linear reconstruction energy. For more details on the minimization, please see the papers of <ref type="bibr" target="#b18">Garrido et al. [2016]</ref> and <ref type="bibr" target="#b63">Thies et al. [2016]</ref>. In total, we obtain a 257 parameter vector p ∈ R 257 for each frame of the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Viseme Search</head><p>Given an edit operation specified as a sequence of words W, our goal is to find matching sequences of phonemes in the video that can be combined to produce W. In the matching procedure we use the fact that identical phonemes are expected to be, on average, more visually similar to each other than non-identical phonemes (despite co-articulation effects). We similarly consider visemes, groups of aurally distinct phonemes that appear visually similar to one another (Section 3.3), as good potential matches. Importantly, the matching procedure cannot expect to find a good coherent viseme sequence in the video for long words or sequences in the edit operation. Instead, we must find several matching subsequences and a way to best combine them.</p><p>We first convert the edit operation W to a phoneme sequence W = (w 1 , . . . , w m ) where each w i is defined as (w lbl i , w in i , w out i ) similar to our definition of phonemes in the video v i . We can convert the text W to phoneme labels w lbl i using a word to phoneme map, but text does not contain timing information w in i , w out i . To obtain timings we use a text-to-speech synthesizer to convert the edit into speech. For all results in this paper we use either the built-in speech synthesizer in Mac OS X, or Voco <ref type="bibr" target="#b27">[Jin et al. 2017</ref>]. Note however that our video synthesis pipeline does not use the audio signal, but only its timing. So, e.g., manually specified phone lengths could be used as an alternative. The video generated in the rendering stage of our pipeline (Section 3.5) is mute and we discuss how we can add audio at the end of that section. Given the audio of W, we produce phoneme labels and timing using P2FA, in a manner similar to the one we used in Section 3.1.</p><p>Given an edit W and the video phonemes V, we are looking for the optimal partition of W into sequential subsequences W 1 , . . . , W k , such that each subsequence has a good match in V, while encouraging subsequences to be long (Figure <ref type="figure" target="#fig_2">4</ref>). We are looking for long subsequences because each transition between subsequences may cause artifacts in later stages. We first describe matching one subsequence W i = (w j , . . . , w j+k ) to the recording V, and then explain how we match the full query W.</p><p>Matching one subsequence. We define C match (W i , V ⋆ ) between a subsequence of the query W i and some subsequence of the video V ⋆ as a modified Levenshtein edit distance <ref type="bibr" target="#b34">[Levenshtein 1966</ref>] between phoneme sequences that takes phoneme length into account. The edit distance requires pre-defined costs for insertion, deletion and swap. We define our insertion cost C insert = 1 and deletion cost C delete = 1 and consider viseme and phoneme labels as well as Table <ref type="table">1</ref>. Grouping phonemes (listed as ARPABET codes) into visemes. We use the viseme grouping of Annosoft's lipsync tool <ref type="bibr">[Annosoft 2008</ref>]. More viseme groups may lead to better visual matches (each group is more specific in its appearance), but require more data because the chance to find a viseme match decreases. We did not perform an extensive evaluation of different viseme groupings, of which there are many.</p><formula xml:id="formula_1">v01 AA0, AA1, AA2 v09 Y, IY0, IY1, IY2 v02 AH0, AH1, AH2, HH v10 R, ER0, ER1, ER2 v03 AO0, AO1, AO2 v11 L v04 AW0, AW1, AW2, OW0, v12 W OW1, OW2 v13 M, P, B v05 OY0, OY1, OY2, UH0, UH1, v14 N, NG, DH, D, G, UH2, UW0, UW1, UW2 T, Z, ZH, TH, K, S v06 EH0, EH1, EH2, AE0, AE1, AE2 v15 CH, JH, SH v07 IH0, IH1, IH2, AY0, AY1, AY2 v16 F, V v08 EY0, EY1, EY2 v17 sp phoneme lengths in our swap cost C swap (v i , w j ) = C vis (v i , w j )(| v i | + | w j |) + χ | v i | − | w j | (1)</formula><p>where |a| denotes the length of phoneme a, C vis (v i , w j ) is 0 if v i and w j are the same phoneme, 0.5 if they are different phonemes but the same viseme (Section 3.3), and 1 if they are different visemes.</p><p>The parameter χ controls the influence of length difference on the cost, and we set it to 10 −4 in all our examples. Equation (1) penalized for different phonemes and visemes, weighted by the sum of the phoneme length. Thus longer non-matching phonemes will incur a larger penalty, as they are more likely to be noticed. We minimize C match (W i , V) over all possible V ⋆ using dynamic programming <ref type="bibr" target="#b34">[Levenshtein 1966</ref>] to find the best suffix of any prefix of V and its matching cost to W i . We brute-force all possible prefixes of V to find the best match V i to the query W i .</p><p>Matching the full query. We define our full matching cost C between the query W and the video V as</p><formula xml:id="formula_2">C(W, V) = min (W 1 , ...,W k )∈split(W) (V 1 , ...,V k ) k i=1 C match (W i , V i ) + C len (W i ) (2)</formula><p>where split(W) denotes the set of all possible ways of splitting W into subsequences, and V i is the best match for W i according to C match . The cost C len (W i ) penalizes short subsequences and is defined as</p><formula xml:id="formula_3">C len (W i ) = ϕ |W i |<label>(3)</label></formula><p>where |W i | denotes the number of phonemes in subsequence W i and ϕ is a weight parameter empirically set to 0.001 for all our examples. To minimize Equation ( <ref type="formula">2</ref>) we generate all splits (W 1 , . . . , W k ) ∈ split(W) of the query (which is typically short), and for each W i we find the best subsequence V i of V with respect to C match . Since the same subsequence W i can appear in multiple partitions, we memoize computations to make sure each match cost is computed only once. The viseme search procedure produces subsequences (V 1 , . . . , V k ) of the input video that, when combined, should produce W.  shown. Each subsequence is matched to the input video V, producing a correspondance between query phonemes w i and input video phonemes v i . We retime in parameter space to match the lengths of each v i to w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Retiming &amp; Blending</head><p>The sequence (V 1 , . . . , V k ) of video subsequences describes sections of the video for us to combine in order to create W. However, we cannot directly use the video frames that correspond to (V 1 , . . . , V k ) for two reasons: (1) A sequence V i corresponds to part of W in viseme identity, but not in viseme length, which will produce unnatural videos when combined with the speech audio, and (2) Consecutive sequences V i and V i+1 can be from sections that are far apart in the original video. The subject might look different in these parts due to pose and posture changes, movement of hair, or camera motion. Taken as-is, the transition between consecutive sequences will look unnatural (Figure <ref type="figure" target="#fig_1">3</ref> top).</p><p>To solve these issues, we use our parametric face model in order to mix different properties (pose, expression, etc.) from different input frames, and blend them in parameter space. We also select a background sequence B and use it for pose data and background pixels. The background sequence allows us to edit challenging videos with hair movement and slight camera motion.</p><p>Background retiming and pose extraction. An edit operation W will often change the length of the original video. We take a video sequence (from the input video) B ′ around the location of the edit operation, and retime it to account for the change in length the operation will produce, resulting in a retimed background sequence B. We use nearest-neighbor sampling of frames, and select a large enough region around the edit operation so that retiming artifacts are negligible. All edits in this paper use the length of one sentence as background. The retimed sequence B does not match the original nor the new audio, but can provide realistic background pixels and pose parameters that seamlessly blend into the rest of the video. In a later step we synthesize frames based on the retimed background and expression parameters that do match the audio.</p><p>Subsequence retiming. The phonemes in each sequence v j ∈ V i approximately match the length of corresponding query phonemes, but an exact match is required so that the audio and video will be properly synchronized. We set a desired frame rate F for our synthesized video, which often matches the input frame-rate, but does not have to (e.g. to produce slow-mo video from standard video). Given the frame rate F , we sample model parameters p ∈ R 257 by linearly interpolating adjacent frame parameters described in Section 3.2. For each v j ∈ V i we sample F | w j | frame parameters in [v in j , v out j ] so that the length of the generated video matches the length of the query | w j |. This produces a sequence that matches W in timing, but with visible jump cuts between sequences if rendered as-is (Figure <ref type="figure" target="#fig_2">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bottom).</head><p>Parameter blending. To avoid jump cuts, we use different strategies for different parameters, as follows. Identity geometry α ∈ R 80 and reflectance β ∈ R 80 are kept constant throughout the sequence (it's always the same person), so they do not require blending. Scene illumination γ ∈ R 27 typically changes slowly or is kept constant, thus we linearly interpolate illumination parameters between the last frame prior to the inserted sequence and the first frame after the sequence, disregarding the original illumination parameters of V i . This produces a realistic result while avoiding light flickering for input videos with changing lights. Rigid head pose T ∈ SE( <ref type="formula" target="#formula_3">3</ref>) is taken directly from the retimed background sequence B. This ensures that the pose of the parameterized head model matches the background pixels in each frame. Facial expressions δ ∈ R 64 are the most important parameters for our task, as they hold information about mouth and face movement -the visemes we aim to reproduce. Our goal is to preserve the retrieved expression parameters as much as possible, while smoothing out the transition between them. Our approach is to smooth out each transition from V i to V i+1 by linearly interpolating a region of 67 milliseconds around the transition. We found this length to be short enough so that individual visemes are not lost, and long enough to produce convincing transitions between visemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Neural Face Rendering</head><p>We employ a novel neural face rendering approach for synthesizing photo-realistic talking-head video that matches the modified parameter sequence (Section 3.4). The output of the previous processing step is an edited parameter sequence that describes the new desired facial motion and a corresponding retimed background video clip. The goal of this synthesis step is to change the facial motion of the retimed background video to match the parameter sequence. To this end, we first mask out the lower face region, including parts of the neck (for the mask see Figure <ref type="figure">5b</ref>), in the retimed background video and render a new synthetic lower face with the desired facial expression on top. This results in a video of composites r i (Figure <ref type="figure">5d</ref>). Finally, we bridge the domain gap between r i and real video footage of the person using our neural face rendering approach, which is based on recent advances in learning-based image-to-image translation <ref type="bibr" target="#b26">[Isola et al. 2017;</ref><ref type="bibr" target="#b57">Sun et al. 2018</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Training the Neural Face</head><p>Renderer. To train our neural face rendering approach to bridge the domain gap we start from a paired Fig. <ref type="figure">6</ref>. We assume the video has been generated by a sequential process, which we model by a recurrent network with shared generator G. In practice, we unroll the loop three times. training corpus T = (f i , r i ) N i=1 that consists of the N original video frames f i and corresponding synthetic composites r i . The r i are generated as described in the last paragraph, but using the ground truth tracking information of the corresponding frame (Figure <ref type="figure">5</ref>), instead of the edited sequence, to render the lower face region. The goal is to learn a temporally stable video-to-video mapping (from r i to f i ) using a recurrent neural network (RNN) that is trained in an adversarial manner. We train one person-specific network per input video. Inspired by the video-to-video synthesis work of <ref type="bibr" target="#b68">Wang et al. [2018a]</ref>, our approach assumes that the video frames have been generated by a sequential process, i.e., the generation of a video frame depends only on the history of previous frames (Figure <ref type="figure">6</ref>). In practice, we use a temporal history of size L = 2 in all experiments, so the face rendering RNN looks at L + 1 = 3 frames at the same time. The best face renderer G * is found by solving the following optimization problem:</p><formula xml:id="formula_4">G * = arg min G max D s , D t L(G, D s , D t ) . (4)</formula><p>Here, D s is a per-frame spatial patch-based discriminator <ref type="bibr" target="#b26">[Isola et al. 2017]</ref>, and D t is a temporal patch-based discriminator. We train the recurrent generator and the spatial and temporal discriminator of our GAN in an adversarial manner, see Figure <ref type="figure" target="#fig_4">7</ref>. In the following, we describe our training objective L and the network components in more detail.</p><p>Training Objective. For training our recurrent neural face rendering network, we employ stochastic gradient decent to optimize the following training objective:</p><formula xml:id="formula_5">L(G, D s , D t ) = E (f i ,r i ) L r (G) + λ s L s (G, D s ) + λ t L t (G, D t ) .</formula><p>(5)</p><p>Here, L r is a photometric reconstruction loss, L s is a per-frame spatial adversarial loss, and L t is our novel adversarial temporal consistency loss that is based on difference images. Let f i i−L denote the tensor of video frames from frame f i−L to the current frame f i . The corresponding tensor of synthetic composites r i i−L is defined in a similar way. For each of the L + 1 time steps, we employ an ℓ 1 -loss to enforce the photometric reconstruction of the ground truth:</p><formula xml:id="formula_6">L r (G) = L l =0 m i−L+l ⊗ f i−L+l − G(c i,l ) 1 , with c i,l = r i−L+l i−L , o i−L+l −1 i−L . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Here, the c i,l are the generator inputs for the current frame i and time step l, with</p><formula xml:id="formula_8">o i−L+l −1 i−L</formula><p>being the tensor of output frames for the previous time steps. ⊗ is the Hadamard product and m i−L+l is a mouth re-weighting mask that gives a higher weight to photometric errors in the mouth region (Figure <ref type="figure">5</ref>). The mask is 1 away from the mouth, 10 for the mouth region, and has a smooth transition in between. Note the same generator G is shared across all time steps. For each time step, missing outputs of non existent previous frames (we only unroll 3 steps) and network inputs that are in the future are replaced by zeros (Figure <ref type="figure">6</ref>). In addition to the reconstruction loss, we also enforce a separate patch-based adversarial loss for each frame:</p><formula xml:id="formula_9">L s (G, D s ) = L l =0 log(D s (r i−L+l , f i−L+l )) + log(1 − D s (r i−L+l , G(c i,l ))) .<label>(7)</label></formula><p>Note there exists only one discriminator network D s , which is shared across all time steps. We also employ an adversarial temporal consistency loss based on difference images <ref type="bibr" target="#b39">[Martin-Brualla et al. 2018]</ref>:</p><formula xml:id="formula_10">L t (G, D t ) = log(D t (r i i−L , ∆ i,l (f))) + log(1 − D t (r i i−L , ∆ i,l (o))) .</formula><p>(8) Here, ∆ i,l (f) is the ground truth tensor and ∆ i,l (o) the tensor of synthesized difference images. The operator ∆(•) takes the difference of subsequent frames in the sequence:</p><formula xml:id="formula_11">∆ i,l (x) = x i i−L+1 − x i−1 i−L . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>Network Architecture. For the neural face rendering network, we employ an encoder-decoder network with skip connections that is based on U-Net <ref type="bibr" target="#b50">[Ronneberger et al. 2015]</ref>. Our spatial and temporal discriminators are inspired by <ref type="bibr" target="#b26">Isola et al. [2017]</ref> and <ref type="bibr" target="#b68">Wang et al. [2018a]</ref>. Our network has 75 million trainable parameters. All subnetworks (G, D s , D t ) are trained from scratch, i.e., starting from random initialization. We alternate between the minimization to train G and the maximization to train D s as well as D t . In each iteration step, we perform both the minimization as well as the maximization on the same data, i.e., the gradients with respect to the generator and discriminators are computed on the same batch of images. We do not add any additional weighting between the gradients with respect to the generator and discriminators as done in <ref type="bibr" target="#b26">Isola et al. [2017]</ref>. The rest of the training procedure follows <ref type="bibr" target="#b26">Isola et al. [2017]</ref>. For more architecture details, see Supplemental W13.</p><p>Table <ref type="table">2</ref>. Input sequences. We recorded three sequences, each about 1 hour long. The sequences contain ground truth sentences and test sentences we edit, and also the first 500 sentences from the TIMIT dataset. We also downloaded a 1.5 hour long interview from YouTube that contains camera and hand motion, and an erroneous transcript. Seq2 and Seq3 are both 60fps. Seq1 was recorded at 240fps, but since our method produces reasonable results with lower frame rates, we discarded frames and effectively used 60fps. Seq4 is 25fps, and still produces good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Transcript The rendering procedure produces photo-realistic video frames of the subject, appearing to speak the new phrase W . These localized edits seamlessly blend into the original video, producing an edited result, all derived from text.</p><p>Adding Audio. The video produced by our pipeline is mute. To add audio we use audio synthesized either by the built in speech synthesizer in Mac OS X, or by VoCo <ref type="bibr" target="#b27">[Jin et al. 2017</ref>]. An alternative is to obtain an actual recording of the performer's voice. In this scenario, we retime the resulting video to match the recording at the level of phones. Unless noted otherwise, all of our synthesis results presented in the performer's own voice are generated using this latter method. Note that for move and delete edits we use the performer's voice from the original video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>We show results for our full approach on a variety of videos, both recorded by ourselves and downloaded from YouTube (Section 4). We encourage the reader to view video results (with audio) in the supplemental video and website, since our results are hard to evaluate from static frames.</p><p>Runtime Performance. 3D face reconstruction takes 110ms per frame. Phoneme alignment takes 20 minutes for a 1 hour speech video. Network training takes 42 hours. We train for 600K iteration steps with a batch size of 1. Viseme search depends on the size of the input video and the new edit. For a 1 hour recording with continuous speech, viseme search takes between 10 minutes and 2 hours for all word insertion operations in this paper. Neural face rendering takes 132ms per frame. All other steps of our pipeline incur a negligible time penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Editing</head><p>Our main application is text-based editing of talking-head video. We support moving and deleting phrases, and the more challenging task of adding new unspoken words. A few examples of replacing one or more words by unspoken word(s) are shown in Figure <ref type="figure">1</ref> and Figure <ref type="figure">9</ref>. Our approach produces photo-realistic results with good audio to video alignment and a photo-realistic mouth interior including highly detailed teeth (Figure <ref type="figure">10</ref>). For more examples of adding new words, and results for moves and deletes we refer to the supplemental video and Supplemental W1-W4. We compare the output of our approach with a baseline that is trained based on input data as proposed in Deep Video Portraits (DVP) <ref type="bibr" target="#b32">[Kim et al. 2018b]</ref>. DVP does not condition on the background and thus cannot handle dynamic background. In addition, this alternative approach fails if parts of the foreground move independently of the head, e.g., the hands. Our approach explicitly conditions on the background and can thus handle these challenging cases with ease. In addition, our approach only has to spend capacity in the mouth region (we also re-weight the reconstruction loss based on a mouth mask), thus our approach gives much sharper higher quality results. Video credit (middle): The Mind of The Universe. Fig. <ref type="figure">9</ref>. Our approach enables a large variety of text-based edits, such as deleting, rearranging, and adding new words. Here, we show examples of the most challenging of the three scenarios, adding one or more unspoken words. As can be seen, our approach obtains high quality reenactments of the new words based on our neural face rendering approach that converts synthetic composites into photo-real imagery. For video results we refer to the supplemental.</p><p>Our approach enables us to seamlessly re-compose the modified video segments into the original full frame video footage, and to seamlessly blend new segments into the original (longer) video. Thus our approach can handle arbitrarily framed footage, and is agnostic to the resolution and aspect ratio of the input video. It also enables localized edits (i.e. using less computation) that do not alter most of the original video and can be incorporated into a standard editing pipeline. Seamless composition is made possible by our neural face rendering strategy that conditions video generation on the original background video. This approach allows us to accurately reproduce the body motion and scene background (Figure <ref type="figure">11</ref>). Other neural rendering approaches, such as Deep Video Portraits <ref type="bibr" target="#b32">[Kim et al. 2018b]</ref> do not condition on the background, and thus cannot guarantee that the body is synthesized at the right location in the frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation</head><p>Besides text-based edits, such as adding, rearranging, and deleting words, our approach can also be used for video translation, as long as the source material contains similar visemes to the target language. Our viseme search pipeline is language agnostic. In order to support a new language, we only require a way to convert words into individual phonemes, which is already available for many languages. We show results in which an English speaker appears to speak German (Supplemental W5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Full Sentence Synthesis Using Synthetic Voice</head><p>With the rise of voice assistants like Alexa, Siri and the Google Assistant, consumers have been getting comfortable with voice-based interactions. We can use our approach to deliver corresponding video. Given a video recording of an actor who wishes to serve as the face of the assistant, our tool could be used to produce the video for any utterance such an assistant might make. We show results of full sentence synthesis using the native Mac OS voice synthesizer Fig. <ref type="figure">10</ref>. Our approach synthesizes the non-rigid motion of the lips at high quality (even lip rolling is captured) given only a coarse computer graphics rendering as input. In addition, our approach synthesizes a photorealistic mouth interior including highly detailed teeth. The synthesis results are temporally coherent, as can be seen in the supplemental video. Fig. <ref type="figure">11</ref>. Our approach enables us to seamlessly compose the modified segments back into the original full frame input video sequence, both spatially as well as temporally. We do this by explicitly conditioning video generation on the re-timed background video.</p><p>(Supplemental W7). Our system could also be used to easily create instruction videos with more fine-grained content adaptation for different target audiences, or to create variants of storytelling videos that are tailored to specific age groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION, ANALYSIS &amp; COMPARISONS</head><p>To evaluate our approach we have analyzed the content and size of the input video data needed to produce good results and we have compared our approach to alternative talking-head video synthesis techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Size of Input Video</head><p>We performed a qualitative study on the amount of data required for phoneme retrieval. To this end, we iteratively reduced the size of the used training video. We tested our retrieval approach with 5%, 10%, 50%, and 100% of the training data (Supplemental W8). More data leads in general to better performance and visually more pleasing results, but the quality of the results degrade gracefully with the amount of used data. Best results are obtained with the full dataset.</p><p>We also evaluate the amount of training data required for our neural face renderer. Using seq4 (our most challenging sequence), we test a self-reenactment scenario in which we compare the input frames to our result with varying training data size. We obtain errors (mean RMSE per-image) of 0.018 using 100%, 0.019 using 50% and 0.021 using only 5% of the data (R,G,B ∈ [0, 1]). This result suggests that our neural renderer requires less data than our viseme retrieval pipeline, allowing us to perform certain edits (e.g., deletion) on shorter videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Size of Edit</head><p>We tested our system with various synthesized phrases. We randomly select from a list of "things that smell" and synthesize the phrases into the sentence "I love the smell of X in the morning" (Supplemental W11). We found that phrase length does not directly correlate with result quality. Other factors, such as the visemes that comprise the phrase and phoneme alignment quality influence the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of Parameter Space Blending</head><p>We evaluate the necessity of our parameter blending strategy by comparing our approach to a version without the parameter blending (Figure <ref type="figure" target="#fig_6">12</ref> and Supplemental W12). Without our parameter space blending strategy the results are temporally unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to MorphCut</head><p>MorphCut is a tool in Adobe Premiere Pro that is designed to remove jump cuts in talking-head videos, such as those introduced by moving or deleting words. It is based on the approach of <ref type="bibr" target="#b2">Berthouzoz et al. [2012]</ref>, requires the performer to be relatively still in the video and cannot synthesize new words. In Figure <ref type="figure" target="#fig_7">13</ref>, we compare our approach to MorphCut in the word deletion scenario and find that our approach is able to successfully remove the jump cut, while MorphCut fails due to the motion of the head.</p><p>We also tried to apply MorphCut to the problem of word addition. To this end, we first applied our phoneme/viseme retrieval pipeline to select suitable frames to compose a new word. Afterwards, we tried to remove the jump cuts between the different phoneme subsequences with MorphCut (Figure <ref type="figure" target="#fig_8">14</ref>). While our approach with  parameter space blending is able to generate seamless transitions, MorphCut produces big jumps and can not smooth them out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison to Facial Reenactment Techniques</head><p>We compare our facial reenactment backend with a baseline approach that is trained based on the input data as proposed in Deep Video Portraits <ref type="bibr" target="#b32">[Kim et al. 2018b</ref>] (Figure <ref type="figure" target="#fig_5">8</ref>). For a fair comparison, we trained our recurrent generator network (including the temporal GAN loss, but without our mouth re-weighting mask) with Deep Video Portraits style input data (diffuse rendering, uv-map, and eye conditioning) and try to regress a realistic output video. Compared to Deep Video Portraits <ref type="bibr" target="#b32">[Kim et al. 2018b]</ref>, our approach synthesizes a more detailed mouth region, handles dynamic foregrounds well, such as for example moving hands and arms, and can better handle dynamic background. We attribute this to our mouth re-weighting mask and explicitly conditioning on the original background and body, which simplifies the learning task, and frees up capacity in the network. Deep Video Portraits struggles with any form of motion that is not directly correlated to the head, since the head motion is the only input in their technique. We refer to the supplemental video for more results.</p><p>We also compare our approach to Face2Face <ref type="bibr" target="#b63">[Thies et al. 2016]</ref>, see Figure <ref type="figure" target="#fig_9">15</ref>. Our neural face rendering approach can better handle the  <ref type="bibr" target="#b63">[Thies et al. 2016</ref>] facial reenactment approach. Our approach produces high quality results, while the retrievalbased Face2Face exhibits ghosting artifacts and is temporally unstable. We refer to the supplemental video for more results.</p><p>complex articulated motion of lips, e.g., lip rolling, and synthesizes a more realistic mouth interior. The Face2Face results show ghosting artifacts and are temporally unstable, while our approach produces temporally coherent output. We refer to the supplemental video for more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>We also performed an ablation study to evaluate the new components of our approach (see Figure <ref type="figure">16</ref>). We perform the study in a self-reenactment scenario in which we compare our result to the input frames. To this end we compare our complete approach (Full) with two simplified approaches. The first simplification removes both the mouth mask and background conditioning (w\o bg &amp; mask) from our complete approach, while the second simplification only removes the mouth mask (w\o mask). As shown in Figure <ref type="figure">16</ref>, all components positively contribute to the quality of the results. This is especially noticeable in the mouth region, where the quality and level of detail of the teeth is drastically improved. In addition, we also show the result obtained with the Deep Video Portraits (DVP) of <ref type="bibr" target="#b31">Kim et al. [2018a]</ref>. We do not investigate alternatives to the RNN in our ablation study, as <ref type="bibr" target="#b68">Wang et al. [2018a]</ref> have already demonstrated that RNNs outperform independent per-frame synthesis networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">User Study</head><p>To quantitatively evaluate the quality of videos generated by our text-based editing system, we performed a web-based user study with N = 138 participants and collected 2993 individual responses, see Table <ref type="table">3</ref>. The study includes videos of two different talking heads, Set 1 and Set 2, where each set contains 6 different base sentences. For each of the base sentences, we recorded a corresponding target sentence in which one or more words are different. We use both the base and target sentences as ground truth in our user study. Next, we employed our pipeline to artificially change the base into the target sentences. In total, we obtain 2 × 3 × 6 = 36 video clips.</p><p>Table <ref type="table">3</ref>. We performed a user study with N = 138 participants and collected in total 2993 responses to evaluate the quality of our approach. Participants were asked to respond to the statement "This video clip looks real to me" on a 5-point Likert scale from 1 (strongly disagree) to 5 (strongly agree). We give the percentage for each score, the average score, and the percentage of cases the video was rated as 'real' (a score of 4 or higher). The difference between conditions is statistically significant (Kruskal-Wallis test, p &lt; 10 −30 ). Our results are different from both GT-base and from GT-target (Tukey's honest significant difference procedure, p &lt; 10 −9 for both tests). This suggests that while our results are often rated as real, they are still not on par with real video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Base Videos</head><p>GT Target Videos Our Modified Videos Scores Scores Scores</p><formula xml:id="formula_13">5 4 3 2 1 Σ 'real' 5 4 3 2 1 Σ 'real' 5 4 3 2 1 Σ 'real'</formula><p>Set 1 45.3 36.3 7.9 10.0 0.5 4.1 81.6% 47.0 31.9 9.7 10.1 1.4 4.1 78.9% 31.9 25.2 10.9 23.9 8.2 3.5 57.1% Set 2 41.6 38.1 9.9 9.2 1.2 4.1 79.7% 45.7 39.8 8.7 5.4 0.4 4.3 85.6% 29.3 32.8 9.4 22.9 5.7 3.9 62.1%</p><p>Mean 43.5 37.2 8.9 9.6 0.9 4.1 80.6% 46.4 35.9 9.2 7.7 0.9 4.2 82.2% 30.6 29.0 10.1 23.4 7.0 3.7 59.6%</p><p>Fig. <ref type="figure">16</ref>. Ablation study comparing ground truth with several versions of our approach: a simplified version without providing mask and the background conditioning (w/o bg &amp; a simplified version that provides the background but not mouth mask (w/o mask); and our complete approach with all new components (Full). In addition, we show a result from the Deep Video Portraits (DVP) of <ref type="bibr" target="#b31">Kim et al. [2018a]</ref>. All components of our approach positively contribute to the quality of the results, and our full method outperforms DVP. This is especially noticeable in the hair and mouth regions.</p><p>In the study, the video clips were shown one video at a time to participants N = 138 in randomized order and they were asked to respond to the statement "This video clip looks real to me" on a 5-point Likert scale (5-strongly agree, 4-agree, 3-neither agree nor disagree, 2-disagree, 1-strongly disagree). As shown in Table <ref type="table">3</ref>, the real ground truth base videos were only rated to be 'real' 80.6% of the cases and the real ground truth target videos were only rated to be 'real' 82.2% of the cases (score of 4 or 5). This shows that the participants were already highly alert, given they were told it was a study on the topic of 'Video Editing'. Our pipeline generated edits were rated to be 'real' 59.6% of the cases, which means that more than half of the participants found those clips convincingly real. Table <ref type="table">3</ref> also reports the percentage of times each score was given and the average score per video set. Given the fact that synthesizing convincing audio/video content is very challenging, since humans are highly tuned to the slightest audio-visual misalignments (especially for faces), this evaluation shows that our approach already achieves compelling results in many cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS &amp; FUTURE WORK</head><p>While we have demonstrated compelling results in many challenging scenarios, there is room for further improvement and follow-up work: (1) Our synthesis approach requires a re-timed background video as input. Re-timing changes the speed of motion, thus eye blinks and gestures might not perfectly align with the speech anymore. To reduce this effect, we employ a re-timing region that is longer than the actual edit, thus modifying more of the original video footage, with a smaller re-timing factor. For the insertion of words, this could be tackled by a generative model that is able to synthesize realistic complete frames that also include new body motion and a potentially dynamic background. (2) Currently our phoneme retrieval is agnostic to the mood in which the phoneme was spoken. This might for example lead to the combination of happy and sad segments in the blending. Blending such segments to create a new word can lead to an uncanny result. (3) Our current viseme search aims for quality but not speed. We would like to explore approximate solutions to the viseme search problem, which we believe can allow interactive edit operations. (4) We require about 1 hour of video to produce the best quality results. To make our method even more widely applicable, we are investigating ways to produce better results with less data. Specifically, we are investigating ways to transfer expression parameters across individuals, which will allow us to use one pre-processed dataset for all editing operations. (5) Occlusions of the lower face region, for example by a moving hand, interfere with our neural face renderer and lead to synthesis artifacts, since the hand can not be reliably re-rendered. Tackling this would require to also track and synthesize hand motions. Nevertheless, we believe that we demonstrated a large variety of compelling text-based editing and synthesis results. In the future, end-to-end learning could be used to learn a direct mapping from text to audio-visual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We presented the first approach that enables text-based editing of talking-head video by modifying the corresponding transcript. As demonstrated, our approach enables a large variety of edits, such as addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis. We believe our approach is a first important step towards the goal of fully text-based editing and synthesis of general audio-visual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PHONEME &amp; VISEME CONTENT</head><p>Our matching algorithm (Section 3.3) is designed to find the longest match between subsequences of phonemes/visemes in the edit and the input video. Suppose our input video consists of all the sentences in the TIMIT corpus <ref type="bibr" target="#b15">[Garofolo et al. 1993</ref>], a set that has been designed to be phonetically rich by acoustic-phonetic reseachers. Figure <ref type="figure" target="#fig_10">17</ref> plots the probability of finding an exact match anywhere in TIMIT to a phoneme/viseme subsequence of length K ∈ [1, 10]. Exact matches of more than 4-6 visemes or 3-5 phonemes are rare. This result suggests that even with phonetically rich input video we cannot expect to find edits consisting of long sequences of phonemes/visemes (e.g. multiword insertions) in the input video and that our approach of combining shorter subsequences with parameter blending is necessary.</p><p>Figure <ref type="figure" target="#fig_10">17</ref> also examines the variation in individual viseme instances across the set of 2388 sentences in the TIMIT corpus. We see that there is variation both between different visemes and within a class of visemes. These observations led us to incorporate viseme distance and length in our search procedure (Section 3.3) and informed our blending strategy (Section 3.4).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Method overview. Given an input talking-head video and a transcript, we perform text-based editing. We first align phonemes to the input audio and track each input frame to construct a parametric head model. Then, for a given edit operation (changing spider to fox), we find segments of the input video that have similar visemes to the new word. In the above case we use viper and ox to construct fox. We use blended head parameters from the corresponding video frames, together with a retimed background sequence, to generate a composite image, which is used to generate a photorealistic frame using our neural face rendering method. In the resulting video, the actress appears to be saying fox, even though that word was never spoken by her in the original recording.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Our parameter blending strategy produces a seamless synthesized result from choppy original sequences. Above, we insert the expression "french toast" instead of "napalm" in the sentence "I like the smell of napalm in the morning." The new sequence was taken from different parts of the original video: F R EH1 taken from "fresh", N CH T taken from "drenched", and OW1 S T taken from "roast". Notice how original frames from different sub-sequences are different in head size and posture, while our synthesized result is a smooth sequence. On the right we show the pixel difference between blue and red frames; notice how blue frames are very different. Videos in supplemental material.</figDesc><graphic url="image-21.png" coords="6,90.35,194.71,50.84,50.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Viseme search and retiming. Given a query sequence W, We split it into all possible subsequences, of which one (W 1 , W 2 ) ∈ split(W) is shown. Each subsequence is matched to the input video V, producing a correspondance between query phonemes w i and input video phonemes v i . We retime in parameter space to match the lengths of each v i to w i .</figDesc><graphic url="image-29.png" coords="6,63.92,314.75,218.03,125.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig.5. Training Corpus: For each ground truth frame f i (a), we obtain a 3D face reconstruction. The reconstructed geometry proxy is used to mask out the lower face region (b, left) and render a mouth mask m i (b, right), which is used in our training reconstruction loss. We superimpose the lower face region from the parametric face model to obtain a synthetic composite r i (c). The goal of our expression-guided neural renderer is to learn a mapping from the synthetic composite r i back to the ground truth frame f i .</figDesc><graphic url="image-35.png" coords="7,317.96,200.60,242.24,114.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. We employ a spatial discriminator D s , a temporal discriminator D t , and an adversarial patch-based discriminator loss to train our neural face rendering network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of different neural face rendering backends:We compare the output of our approach with a baseline that is trained based on input data as proposed in Deep Video Portraits (DVP)<ref type="bibr" target="#b32">[Kim et al. 2018b]</ref>. DVP does not condition on the background and thus cannot handle dynamic background. In addition, this alternative approach fails if parts of the foreground move independently of the head, e.g., the hands. Our approach explicitly conditions on the background and can thus handle these challenging cases with ease. In addition, our approach only has to spend capacity in the mouth region (we also re-weight the reconstruction loss based on a mouth mask), thus our approach gives much sharper higher quality results. Video credit (middle): The Mind of The Universe.</figDesc><graphic url="image-36.png" coords="9,51.81,78.70,508.40,96.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Evaluation of Parameter Blending: Without our parameter blending strategy, the editing results are temporally unstable. In this example, the mouth unnaturally closes instantly between two frames without blending, while it closes smoothly with our blending approach.</figDesc><graphic url="image-40.png" coords="10,317.96,78.71,242.24,124.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.13. We compare our approach in the word deletion scenario to Mor-phCut. MorphCut fails on the second, third, and forth frames shown here while our approach is able to successfully remove the jump cut. Video credit: The Mind of The Universe.</figDesc><graphic url="image-41.png" coords="11,51.81,78.71,242.24,113.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.14. We tried to stitch retrieved viseme sequences with MorphCut to generate a new word. While our approach with the parameter space blending strategy is able to generate a seamless transition, MorphCut produces a big jump of the head between the two frames.</figDesc><graphic url="image-42.png" coords="11,51.81,254.24,242.24,127.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Comparison to the Face2Face<ref type="bibr" target="#b63">[Thies et al. 2016</ref>] facial reenactment approach. Our approach produces high quality results, while the retrievalbased Face2Face exhibits ghosting artifacts and is temporally unstable. We refer to the supplemental video for more results.</figDesc><graphic url="image-43.png" coords="11,317.96,78.71,242.24,161.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. Left: probability of matching phoneme/viseme subsequences of length K ∈ [1, 10] in the TIMIT corpus. To ensure that the query subsequences reflect the distribution of such sequences in English we employ leave-one-out strategy: we choose a random TIMIT sequence of length K, and look for an exact match anywhere in the rest of the dataset. Exact matches of more than 4-6 vismes and 2-3 phonemes are uncommon. Right: variation in viseme duration in TIMIT. Different instances of a single viseme vary by up to an order of magnitude. Between different visemes, the median instance length varies by a factor of five.</figDesc><graphic url="image-45.png" coords="14,317.96,502.59,242.24,92.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig. 17. Left: probability of matching phoneme/viseme subsequences of length K ∈ [1, 10] in the TIMIT corpus. To ensure that the query subsequences reflect the distribution of such sequences in English we employ leave-one-out strategy: we choose a random TIMIT sequence of length K, and look for an exact match anywhere in the rest of the dataset. Exact matches of more than 4-6 vismes and 2-3 phonemes are uncommon. Right: variation in viseme duration in TIMIT. Different instances of a single viseme vary by up to an order of magnitude. Between different visemes, the median instance length varies by a factor of five.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,51.81,260.46,508.40,107.22" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Trans. Graph., Vol. 38, No. 4, Article 68. Publication date: July 2019.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Brown Institute for Media Innovation, the Max Planck Center for Visual Computing and Communications, ERC Consolidator Grant 4DRepLy (770784), Adobe Systems, and the Office of the Dean for Research at Princeton University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bringing Portraits to Life</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130800.3130818</idno>
		<ptr target="https://doi.org/10.1145/3130800.3130818" />
	</analytic>
	<monogr>
		<title level="m">Lipsync Tool</title>
				<imprint>
			<publisher>Annosoft</publisher>
			<date type="published" when="2008">2008. 2017. November 2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recycle-GAN: Unsupervised Video Retargeting</title>
		<author>
			<persName><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tools for Placing Cuts and Transitions in Interview Video</title>
		<author>
			<persName><forename type="first">Floraine</forename><surname>Berthouzoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrawala</forename></persName>
		</author>
		<idno type="DOI">10.1145/2185520.2185563</idno>
		<ptr target="https://doi.org/10.1145/2185520.2185563" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2012-07">2012. July 2012</date>
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exchanging Faces in Images</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Volker Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><surname>Seidel</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2004.00799.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-8659.2004.00799.x" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2004-09">2004. September 2004</date>
		</imprint>
	</monogr>
	<note>Eurographics)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Morphable Model for the Synthesis of 3D Faces</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1145/311535.311556</idno>
		<ptr target="https://doi.org/10.1145/311535.311556" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large Scale 3D Morphable Models</title>
		<author>
			<persName><forename type="first">James</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1009-7</idno>
		<ptr target="https://doi.org/10.1007/s11263-017-1009-7" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2018-04">2018. April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video Rewrite: Driving Visual Speech with Audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<idno type="DOI">10.1145/258734.258880</idno>
		<ptr target="https://doi.org/10.1145/258734.258880" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97)</title>
				<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH &apos;97)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time High-fidelity Facial Performance Capture</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766943</idno>
		<ptr target="https://doi.org/10.1145/2766943" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015-07">2015. July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07371</idno>
		<title level="m">Everybody Dance Now. arXiv e-prints</title>
				<imprint>
			<date type="published" when="2018-08">2018. August 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferable Videorealistic Speech Animation</title>
		<author>
			<persName><forename type="first">Yao-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Ezzat</surname></persName>
		</author>
		<idno type="DOI">10.1145/1073368.1073388</idno>
		<ptr target="https://doi.org/10.1145/1073368.1073388" />
	</analytic>
	<monogr>
		<title level="m">Symposium on Computer Animation (SCA)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="143" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Photographic Image Synthesis with Cascaded Refinement Networks</title>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.168</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.168" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1520" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-To-End 3D Face Reconstruction With Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">JALI: An Animatorcentric Viseme Model for Expressive Lip Synchronization</title>
		<author>
			<persName><forename type="first">Pif</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925984</idno>
		<ptr target="https://doi.org/10.1145/2897824.2925984" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">127</biblScope>
			<date type="published" when="2016-07">2016. July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trainable Videorealistic Speech Animation</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gadi</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="DOI">10.1145/566654.566594</idno>
		<ptr target="https://doi.org/10.1145/566654.566594" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="388" to="398" />
			<date type="published" when="2002-07">2002. July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Driving High-Resolution Facial Scans with Video Performance Capture</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryosuke</forename><surname>Ichikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014-12">2014. December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Dahlgren</surname></persName>
		</author>
		<ptr target="http://www.ldc.upenn.edu/Catalog/LDC93S1.html" />
		<title level="m">DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic Face Reenactment</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Rehmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Thormaehlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.537</idno>
		<idno>CVPR. 4217-4224</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.537" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12552</idno>
		<ptr target="https://doi.org/10.1111/cgf.12552" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="2015-05">2015. May 2015</date>
		</imprint>
	</monogr>
	<note>Eurographics)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reconstruction of Personalized 3D Face Rigs from Monocular Video</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levi</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/2890493</idno>
		<ptr target="https://doi.org/10.1145/2890493" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016-06">2016. June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Warpguided GANs for Single-photo Facial Animation</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3272127.3275043</idno>
		<ptr target="http://doi.acm.org/10.1145/3272127.3275043" />
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers (SIGGRAPH Asia &apos;18)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="1" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised Training for 3D Morphable Model Regression</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2837742</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2837742" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unit selection in a concatenative speech synthesis system using a large speech database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="https://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/speech-to-text/" />
		<title level="m">IBM Speech to Text Service</title>
				<imprint>
			<date type="published" when="2016-12-17">2016. 2016-12-17</date>
		</imprint>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic 3D Avatar Creation from Hand-held Video Input</title>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofien</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><surname>Pauly</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766974</idno>
		<ptr target="https://doi.org/10.1145/2766974" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015-07">2015. July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.632" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VoCo: text-based insertion and replacement in audio narration</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gautham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Internet-Based Morphable Model</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3256" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Being John Malkovich</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15549-9_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15549-9_25" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Video Portraits</title>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">163</biblScope>
			<date type="published" when="2018">2018a. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Video Portraits</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018">2018b. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>TOG)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Computational Video Editing for Dialogue-driven Scenes</title>
		<author>
			<persName><forename type="first">Mackenzie</forename><surname>Leake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073653</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073653" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">130</biblScope>
			<date type="published" when="2017-07">2017. July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
				<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Data-Driven Approach for Facial Expression Retargeting in Video</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="310" />
			<date type="published" when="2014-02">2014. February 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Realistic facial expression synthesis for an imagebased talking head</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern</forename><surname>Ostermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2011.6011835</idno>
		<ptr target="https://doi.org/10.1109/ICME.2011.6011835" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03658</idno>
		<title level="m">Neural Animation and Reenactment of Human Actor Videos</title>
				<imprint>
			<date type="published" when="2018-09">2018. September 2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expressive Expression Mapping with Ratio Images</title>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/383259.383289</idno>
		<ptr target="https://doi.org/10.1145/383259.383289" />
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="271" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LookinGood: Enhancing Performance Capture with Real-time Neural Re-rendering</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pidlypenskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">255</biblScope>
			<date type="published" when="2018-12">2018. December 2018</date>
		</imprint>
	</monogr>
	<note>14 pages</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimized photorealistic audiovisual speech synthesis using active appearance modeling</title>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Mattheyses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Latacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Auditory-Visual Speech Processing</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Conditional Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<ptr target="https://arxiv.org/abs/1411.1784" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">paGAN: Real-time Avatars Using Dynamic Textures</title>
		<author>
			<persName><forename type="first">Koki</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3272127.3275075</idno>
		<ptr target="https://doi.org/10.1145/3272127.3275075" />
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Papers (SIGGRAPH Asia &apos;18)</title>
				<meeting><address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ochshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hawkins</surname></persName>
		</author>
		<ptr target="https://lowerquality.com/gentle/" />
		<title level="m">Gentle: A Forced Aligner</title>
				<imprint>
			<date type="published" when="2016">2016. 2018-09-25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Realistic Dynamic Facial Textures from a Single Image using GANs</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.580</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.580" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5439" to="5448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VidCrit: Video-based Asynchronous Video Review</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<editor>
			<persName><forename type="first">O</forename></persName>
		</editor>
		<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">July 2019. 2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="517" to="528" />
		</imprint>
	</monogr>
	<note>Publication date</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video Digests: A Browsable, Skimmable Format for Informational Lecture Videos</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colorado</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3D Face Reconstruction by Learning from Synthetic Data</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2016.56</idno>
		<ptr target="https://doi.org/10.1109/3DV.2016.56" />
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Detailed Face Reconstruction from a Single Image</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.589</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.589" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive 3D Face Reconstruction from Unconstrained Photo Collections</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Tong Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2636829</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2636829" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2127" to="2141" />
			<date type="published" when="2017-11">2017. November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Content-based tools for editing audio stories</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floraine</forename><surname>Berthouzoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gautham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual ACM symposium on User interface software and technology</title>
				<meeting>the 26th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.175</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.175" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1585" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic Acquisition of High-fidelity Facial Performances Using Monocular Videos</title>
		<author>
			<persName><forename type="first">Fuhao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661229.2661290</idno>
		<ptr target="https://doi.org/10.1145/2661229.2661290" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014-11">2014. November 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic Authoring of Audio with Linked Scripts</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Hijung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Hybrid Model for Identity Obfuscation by Face Replacement</title>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: Learning Lip Sync from Audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073640</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073640" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">95</biblScope>
			<date type="published" when="2017-07">2017. July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach for Generalized Speech Animation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasio</forename><surname>Garcia Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073699</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073699" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">93</biblScope>
			<date type="published" when="2017-07">2017. July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">High-Fidelity Monocular Face Reconstruction based on an Unsupervised Model-based Face Autoencoder</title>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2876842</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2876842" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018a. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</title>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</title>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.401</idno>
		<idno>ICCV. 3735-3744</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.401" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Face2Face: Real-Time Face Capture and Reenactment of RGB Videos</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.262</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.262" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.163</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.163" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1493" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Quickcut: An interactive tool for editing narrated video</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floraine</forename><surname>Berthouzoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">WaveNet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>SSW. 125</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Face Transfer with Multilinear Models</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jovan</forename><surname>Popović</surname></persName>
		</author>
		<idno type="DOI">10.1145/1073204.1073209</idno>
		<ptr target="https://doi.org/10.1145/1073204.1073209" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="433" />
			<date type="published" when="2005-07">2005. July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Video-to-Video Synthesis</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">X2Face: A network for controlling face generation by using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Speaker identification on the SCOTUS corpus</title>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2935783</idno>
		<ptr target="https://doi.org/10.1121/1.2935783" />
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="3878" to="3878" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
		</imprint>
	</monogr>
	<note>speech communication</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visemenet: Audio-driven Animator-centric Speech Animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">161</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018-07">2018. July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<title level="m">State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications. Computer Graphics Forum (Eurographics State of the Art Reports</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
