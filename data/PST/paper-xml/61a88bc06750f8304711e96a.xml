<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning</title>
				<funder ref="#_ereWgpW #_mcc64Vn">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_4nsaGp5 #_uhAaDGw #_p3vjVas #_zNq2Qtq">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<email>xinzhang@iastate.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhuqing</forename><surname>Liu</surname></persName>
							<email>liu@ece.osu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengyuan</forename><surname>Zhu</surname></persName>
							<email>zhuz@iastate.edu</email>
						</author>
						<author>
							<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
							<email>songtao@ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Statistics</orgName>
								<orgName type="institution">Iowa State University Ames</orgName>
								<address>
									<postCode>50010</postCode>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Statistics</orgName>
								<orgName type="institution">Iowa State University Ames</orgName>
								<address>
									<postCode>50010</postCode>
									<region>IA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">IBM Thomas J. Watson Research Center Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cooperative multi-agent reinforcement learning (MARL) has received increasing attention in recent years and has found many scientific and engineering applications. However, a key challenge arising from many cooperative MARL algorithm designs (e.g., the actor-critic framework) is the policy evaluation problem, which can only be conducted in a decentralized fashion. In this paper, we focus on decentralized MARL policy evaluation with nonlinear function approximation, which is often seen in deep MARL. We first show that the empirical decentralized MARL policy evaluation problem can be reformulated as a decentralized nonconvex-strongly-concave minimax saddle point problem. We then develop a decentralized gradient-based descent ascent algorithm called GT-GDA that enjoys a convergence rate of O(1/T ). To further reduce the sample complexity, we propose two decentralized stochastic optimization algorithms called GT-SRVR and GT-SRVRI, which enhance GT-GDA by variance reduction techniques. We show that all algorithms all enjoy an O(1/T ) convergence rate to a stationary point of the reformulated minimax problem. Moreover, the fast convergence rates of GT-SRVR and GT-SRVRI imply O( -2 ) communication complexity and O(m ? n -2 ) sample complexity, where m is the number of agents and n is the length of trajectories. To our knowledge, this paper is the first work that achieves O( -2 ) in both sample and communication complexities in decentralized policy evaluation for cooperative MARL. Our extensive experiments also corroborate the theoretical results of our proposed decentralized policy evaluation algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, multi-agent reinforcement learning (MARL) has found important applications in many scientific and engineering fields, such as robotic network <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref>, sensor network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>, and power network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, just to name a few. In MARL, multiple agents observe the current joint state over a network, perform their own actions based on the current state, and transition to the next joint state. Each agent can only observe its local reward, which is a function of the joint state and actions. One important paradigm in MARL is the cooperative MARL, where the agents share a common goal to find an optimal global policy to achieve the maximum global accumulative reward <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>. As in classical reinforcement learning (RL) problems, a key component in cooperative MARL algorithms is the policy evaluation (PE) problem, whose goal is to evaluate the expected long-term accumulative reward for a given global policy. In cooperative MARL, agents share a common value function based on the joint states. As a result, the parameterization of the value function is the same across all agents. Examples of cooperative MARL include but are not limited to traffic light control <ref type="bibr" target="#b54">[55]</ref>, autonomous driving <ref type="bibr" target="#b20">[21]</ref>, financial trading <ref type="bibr" target="#b25">[26]</ref>), etc. In cooperative MARL, the PE problem emerges as a key step for the agents to find an optimal global policy in MARL tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. For example, in the actor-critic algorithmic framework for MARL, the actors conduct the policy improvement step, while the critics perform the policy evaluation step and estimates the value function. The overall actor-critic algorithm tries to find an optimal policy by iterating between the policy evaluation and improvement steps. Thus, developing efficient policy evaluation algorithms is critical to the success of RL algorithms based on the actor-critic framework.</p><p>However, developing efficient PE algorithms for MARL is highly non-trivial. On one hand, the global accumulative reward is not directly observable in an MARL system. As a result, the PE problem of MARL can only be solved in a decentralized fahsion. On the other hand, modern MARL tasks have been increasingly complex and often not directly computable. As a result, MARL often use highly nonlinear parametric models (e.g., deep neural network (DNN)) for policy approximation. In this paper, we focus on PE based on nonlinear function approximations due to the following reasons: 1) linear approximation schemes are based on their pre-defined basis space, which may not be able to approximate the non-linear value function with high accuracy; 2) non-linear neural network approximation can handle the cases where the states space that is mixed with continuous and (infinite) discrete state values; and 3) nonlinear neural network approximation usually have a better generalization performance than linear approximation <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b58">[59]</ref>. However, it has been shown that the convergence performance of RL algorithms with nonlinear function approximations is not guaranteed <ref type="bibr" target="#b47">[48]</ref>.</p><p>In light of the growing importance of MARL, in this paper, we focus on addressing the above challenges. The key contributions of this paper are summarized as follows: ? To our knowledge, this work is the first to investigate the decentralized PE (DPE) problem for MARL with nonlinear function approximations. Via the Fenchel's duality and local reward decomposition, we first reformulate the DPE problem of MARL as a decentralized non-convexstrongly-concave minimax saddle point problem. To solve the minimax problem in a decentralized fashion, we propose a gradient-tracking-based gradient descent-ascent (GT-GDA) algorithm. We show that GT-GDA enjoys a convergence rate of O(1/T ), which leads to an O(mn -2 ) sample complexity and an O( -2 ) communication complexity, where m is the number of agents, n is the data size, and is the convergence accuracy.</p><p>? To further reduce the sample complexity, we develop two variance-reduced algorithms, namely gradient-tracking stochastic recursive variance reduction (GT-SRVR) algorithm and its variant with incremental batch size (GT-SRVRI). We show that both algorithms achieve the same communication complexity O( -2 ) as GT-GDA, but requiring a lower sample complexity O(m ? n -2 ).</p><p>? It is worth noting that, in our theoretical analysis, we relax the commonly-used compactness conditions of the feasible set with some mild assumptions on objectives. Thus, the solutions found by our algorithms are exactly the stationary points for the original policy evaluation problem. This result may be of independent interest for general RL problems. The rest of the paper is organized as follows. In Section 2, we first provide the preliminaries of the DPE problem of MARL and discuss related works. In Section 3, we first introduce the GT-GDA algorithm, and then propose two stochastic variance reduced algorithms, namely GT-SRVR and GT-SRVRI. We present their theoretical properties in Section 4. Section 5 provides numerical results to verify our theoretical findings, and Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem formulation and related work</head><p>In this section, we first introduce the DPE problem formulation in in Section 2.1. Then in Section 2.2, we review the recent developments of PE algorithms and compare them with our work. In Sec-tion 2.3, we highlight the challenges in designing efficient DPE algorithms with nonlinear function approximation and the significance of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem formulation of decentralized policy evaluation for MARL</head><p>Consider a multi-agent network system G = (N , L), where N and L denote the sets of agents and edges, respectively, with |N | = m. In the system, the agents cooperatively perform a learning task. The agents can communicate with each other through edges in L. An MARL problem is formulated based on the multi-agent Markov decision process (MDP) framework, which is characterized by a quintuple (S, A, P a ss , {R i (s, a)} m i=1 , ?), where S and A are the state and action spaces, respectively; s ? S and a ? A are joint state and action; P a ss is the transition probability from state s to state s after taking action a; R i (s, a) is the local reward received by agent i after taking action a in state s; and ? ? (0, 1) is a discount factor. Both the joint state s and action a are available to all agents, while the local reward R i is private to agent i. In a multi-agent system, the global reward function is defined as the average of the local rewards 1 m m i=1 R i (s, a). Moreover, a joint policy ? specifies sequential decision rules for all agents. Policy ?(a|s) is the conditional probability of taking joint action a given state s. The goal of PE is to estimate the value function of a given policy ?, which is defined as the long-term discounted accumulative reward:</p><formula xml:id="formula_0">V ? (s 0 ) = E 1 m ? t=0 ? t m</formula><p>i=1 R i (s t , a t )|s 0 , ? , where the expectation is taken over all possible state-action trajectories and initial states.</p><p>To determine V ? (?), one of the most effective methods is the temporal-difference (TD) learning algorithm, which focuses on solving the Bellman equation for V ? (?):</p><formula xml:id="formula_1">V(s) = T ? V(s) 1 m m i=1 R ? i (s)+ ? s ?S P ? ss V(s ), where T ? denotes the Bellman operator, R ? i (s) = E a??(?|s) R(s, a) andP ? ss = E a??(?|s) P a</formula><p>ss However, P ? ss is unknown in MARL and the size of the state space S could be infinite. To address this challenge, a widely adopted approach is to approximate V ? (?) by a function V ? (?) parameterized by ? ? R p . According to the formulation in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>, the Bellman equation can be solved by minimizing the following mean-squared projected bellman error (MSPBE): MSPBE(?)</p><formula xml:id="formula_2">1 2 E s?d ? T ? V ? (s) -V ? ? ? V ? (s) 2 K -1 ? , where K ? = E s?d ? [? ? V ? (s)? ? V ? (s) ] ? R p?p</formula><p>and d ? is the stationary distribution of the MDP under policy ?. From the Fenchel's duality x 2</p><p>A -1 = max y?R p 2 x, yy Ay, we can reformulate the MSPBE minimization problem as the following primal-dual minimax problem:</p><formula xml:id="formula_3">min ??R p max ??R p L(?, ?) E ? ? ? ? V ? (s), ? - 1 2 ? [? ? V ? (s)? ? V ? (s) ]? ,<label>(1)</label></formula><p>where the expectation is taken over s ? d ? (?), a ? ?(?|s), s ? P a s? , and ? = 1 m m i=1 R i (s, a) + ?V ? (s ) -V ? (s). In practice, we only have access to a finite dataset with n-step trajectories D = (s t , a t , {R i (s t , a t )} n i=1 , s t+1 ) n t=0 . By replacing the unknown expectation with the finite sample average, we have the following empirical minimax problem:</p><formula xml:id="formula_4">min ??R p max ??R p F (?, ?) = 1 n n t=1 ? t ? ? ? V ? (s t ), ? - 1 2 ? K ? ?,<label>(2)</label></formula><p>where ? t</p><formula xml:id="formula_5">1 m m i=1 R i (s t , a t ) + ?V ? (s t+1 ) -V ? (s t ) and K ? 1 n n t=1 ? ? V ? (s t )? ? V ? (s t ) .</formula><p>In this paper, we assume that both K ? and its empirical estimate K? are positive definite for all ?. Define J(?) F (?, ? * ) = max ??R p F (?, ?), where ? * = arg max ??R p F (?, ?). J(?) can be viewed as the finite empirical version of MSPBE. Here, we aim to minimize J(?) by finding a stationary point of F (?, ?). Recall that in MARL, the local reward is only observable for each individual agent. Thus, it is hard to obtain the global reward 1 m m i=1 R i (s t , a t ) and ? t in a multi-agent network. To address this challenge, we define ? i,t = R i (s t , a t ) + ?V ? (s t+1 ) -V ? (s t ) and decompose the minimax problem in (2) as follows:</p><formula xml:id="formula_6">min ??R p max ??R p F (?, ?) = 1 m m i=1 F i (?, ?) = 1 mn m i=1 n t=1 f it (?, ?), where f it (?, ?) ? i,t ?? ? V ? (s t ), ? -1 2 ? [? ? V ? (s t ) ? ? V ? (s t ) ]?.</formula><p>We call this step as local reward decomposition. In cooperative MARL, a key challenge is that the PE problem in (2) has to be solved in a decentralized fashion, which is due to the fact that i) the locally observed rewards are private and cannot be shared with the other agents/central server; ii) it is difficult to set up a central sever in many MARL applications while decentralized setting is more flexible (e.g.,wireless network <ref type="bibr" target="#b57">[58]</ref>, UAV network <ref type="bibr" target="#b6">[7]</ref>); and iii) the central server is vulnerable to cyber-attacks and would be a significant communication bottleneck <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b26">[27]</ref>. To solve Problem <ref type="bibr" target="#b1">(2)</ref> in a decentralized fashion, we can rewrite it in the following equivalent form:</p><formula xml:id="formula_7">min {?i} m i=1 max {?i} m i=1 1 m m i=1 F i (? i , ? i ) = 1 mn m i=1 n t=1 f it (? i , ? i ), subject to ? i = ? j , ? i = ? j , ?(i, j) ? L,<label>(3)</label></formula><p>where ? i and ? i are the local copies of the original primal-dual parameters at agent i. In (3), the equality constraint ensures that the local copies at all nodes are equal to each other, so the formulation is also referred to as the "consensus form."</p><p>Clearly, Problems (2) and ( <ref type="formula" target="#formula_7">3</ref>) are equivalent. For a fixed ?, each local function</p><formula xml:id="formula_8">F i (? i , ?) is a strongly concave function of ?. For a fixed ?, F i (?, ?) is a non-convex function of ?. Thus, Problem<label>(3)</label></formula><p>is a decentralized non-convex-strongly-concave minimax consensus optimization problem. In this paper, we adopt two complexity metrics that are widely used in the decentralized optimization literature (e.g., <ref type="bibr" target="#b45">[46]</ref>) to measure the efficiency of an algorithm: Definition 1 (Sample Complexity). The sample complexity is defined as the total number of incremental first-order oracle (IFO) calls required across all nodes until algorithm converges, where one IFO call evaluates a pair of (f it (?, ?), ?f it (?, ?)) at node i. Definition 2 (Communication Complexity). The communication complexity is defined as the total rounds of communications required until algorithm converges, where each node can send and receive a p-dimensional vector with its neighboring nodes in one communication round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related work on policy evaluation</head><p>1) The tabular approach: The study of MARL under the MDP formulation traces its roots to the seminal work by <ref type="bibr" target="#b28">[29]</ref>. Motivated by this formalization, several methods have been developed to solve and analyze MARL problems, including <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref>, etc. However, most of these early works approximate the value function in a tabular form, which only works for cases where the state and action spaces are relatively small. For complex MARL tasks where the state space is large or even infinite, the tabular approach becomes intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Policy evaluation with linear function approximation:</head><p>To address limitation in tabular approaches for MARL, the work in <ref type="bibr" target="#b24">[25]</ref> proposed to estimate the value function with a linear approximation (i.e., V(s) ? ?(s) ?, ? ? R p , where ? : S ? R p is a feature mapping) and developed a distributed gradient temporal-difference (DGTD) algorithm. However, this work only considered asymptotic convergence analysis and required diminishing step-sizes to ensure convergence. In <ref type="bibr" target="#b11">[12]</ref>, the authors proposed a distributed homotopy primal-dual algorithm (DHPD) for the PE problem in MARL. They also cast MSPBE minimization as a stochastic primal-dual optimization problem, where the objective is convex in primal and strongly-concave in dual. By using an adaptive restarting scheme, DHPD achieves an O(1/T ) convergence rate in finding stationary points. The work in <ref type="bibr" target="#b12">[13]</ref> developed a distributed consensus-based TD(0) algorithm, which integrates the network consensus step and local TD(0) updates. They provided a finite-time analysis and showed a convergence rate of O(1/T ). To further improve convergence, the work in <ref type="bibr" target="#b49">[50]</ref> proposed a primal-dual distributed incremental aggregated gradient (PD-DistIAG) method to integrate gradient-tracking and incremental aggregated gradient methods to achieve linear convergence. However, a major limitation of the linear approximation approach is that it is not applicable for nonlinear MARL models (e.g., DNN).</p><p>3) Policy evaluation with nonlinear function approximation (single-agent): In the literature, PE with nonlinear approximation is by far only limited to single-agent RL. For policy evaluation, linear and nonlinear approximation approaches differ fundamentally in the following aspects. Under linear approximation for policy evaluation, the problem boils down to finding a solution for a linear equation system, which is in essence similar to solving a relatively easy strongly-convex optimization problem <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b44">[45]</ref>. In stark contrast, under non-linear approximation for policy evaluation, the problem possesses a non-convex-strongly-concave structure, and it is far more challenging to find a saddle point solution. To our knowledge, the work in <ref type="bibr" target="#b3">[4]</ref> was the first to study the PE problem with nonlinear approximations and developed a nonlinear TD algorithm. However, the proposed algorithms adopted two-timescale step-sizes<ref type="foot" target="#foot_0">2</ref> , resulting in a slow convergence performance. Recently, the work </p><formula xml:id="formula_9">O( -4 ) - ASTSG O( -3 ) - DHPD [12] O(mn -2 ) O( -2 ) PD-DistIAG [50] O(m log -2 ) O(log -2 ) APP-SAG [43] O(m log -2 ) O(log -2 ) DTDT [52] O(m log -2 ) O(log -2 ) GT-GDA Theorem 1 O(mn -2 ) O( -2 ) GT-SRVR Theorem 3 O(m ? n -2 ) O( -2</formula><p>) GT-SRVRI Theorem 4 1 The feasible parameter spaces are required to be closed convex sets.</p><p>in <ref type="bibr" target="#b48">[49]</ref> showed that PE with nonlinear approximation in RL is equivalent to a non-convex-stronglyconcave minimax optimization problem. To find a stationary point for such minimax problem, they proposed a non-convex primal-dual gradient with variance reduced (nPD-VR) algorithm. However, their algorithm requires an O(1/m) step-size, where m is the size of the dataset. This is problematic in cases with a large transition dataset. More recently, the authors of <ref type="bibr" target="#b40">[41]</ref> proposed two single-time scale first-order stochastic algorithms for the nonconvex-strongly-concave minimax optimization. These two algorithms utilized stochastic gradient with momentum and variance-reduced momentum <ref type="bibr" target="#b7">[8]</ref>, and achieved O(1/ ? T ) and O(1/T 2/3 ) convergence rates, respectively. However, diminishing step-sizes are required in these two algorithms, which do not work well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Relations with decentralized nonconvex-strongly-concave minimax optimization: As mentioned earlier, the PE problem of cooperative MARL can be reformulated as a non-convex-stronglyconcave optimization (NCSC) problem (see details in Problem (2). Thus, our work is also closely related to the area of decentralized NCSC minimax optimization. To efficiently solve the decentralized minimax problem in Problem (2), our proposed algorithms are primal-dual-based algorithms, where we update the two variables simultaneously rather than alternatively. Thus, our proposed algorithms are much simpler and significantly different from existing related works that require to solve maximization subproblem for dual variable in each iteration <ref type="bibr" target="#b34">[35]</ref>. Further, we propose a "hybrid" scheme that non-trivially integrates variance reduction and gradient tracking techniques for both primal and dual variables. Compared with algorithms in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, with simple stochastic gradient updates and variable mixing, our proposed scheme enjoys much improved theoretical and numerical performances. Also, we note that the theoretical analysis of the more sophisticated hybrid scheme in the PE problem of cooperative MARL is more involved compared to existing works and necessitates new proof techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Significance of our work and challenges of DPE with nonlinear function approximation</head><p>To our knowledge, our work is the first to solve the DPE problem with nonlinear function approximation for MARL. However, such nonlinear approximation imposes several significant challenges on algorithm development and analysis. As discussed in Section 2.1, we propose to reformulate the DPE problem as a decentralized nonconvex-strongly-concave minimax optimization problem. In the literature, although a few works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> have studied similar decentralized minimax problem, their variable updates rely on either stochastic or full gradients, which are not sample/communicationefficient for MARL. To address these limitations, we first propose two decentralized variance-reduced algorithms for solving the nonconvex-strongly-concave DPE minimax problem, for which establishing the convergence rates is highly challenging. Second, we adopt the gradient tracking (GT) technique to reduce network consensus error. Under nonlinear function approximation, our algorithms need to track the gradients for both primal and dual variables. However, such "double gradient tracking" introduces new constraints on algorithm design. Third, many of the existing nonconvex-strongly-concave minimax optimization methods (e.g. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49]</ref>) require prior knowledge of the compact domain of model variables, which cannot be assumed in DPE for MARL. Such unboundedness in DPE creates new challenges and necessitates new proof techniques in our algorithm analysis. To conclude this section, we summarize all related work in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Algorithm 1 GT-GDA Algorithm at Agent i. . 1: Set prime-dual parameter pair (? i,0 , ? i,0 ) = (? 0 , ? 0 ).</p><p>2: Calculate local gradients as p i,0 = ? ? F i (? i,0 , ? i,0 ), and d i,0 = ? ? F i (? i,0 , ? i,0 );</p><formula xml:id="formula_10">3: for t = 1, ? ? ? , T do 4:</formula><p>Update local parameters (? i,t+1 , ? i,t+1 ) as in (6); Track global gradients (p i,t+1 , d i,t+1 ) as in (5); 7: end for 3 Gradient-tracking gradient descent ascent algorithm.</p><p>In this section, we first present a gradient-tracking gradient descent ascent (GT-GDA) method for solving the DPE problem in (3) for MARL, and then provide the its theoretical results.</p><p>1) The Algorithm: For the consensus problem in (3), a popular approach is to let agents aggregate their neighbor information through a consensus weight matrix M ? R m?m <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b49">50]</ref>. Let [M] ij denote the element in the i-th row and the j-th column in M. M satisfies the following properties: (a) Doubly stochastic:</p><formula xml:id="formula_11">m i=1 [M] ij = m j=1 [M] ij = 1; (b) Symmetric: [M] ij = [M] ji , ?i, j ? N ; and (c) Network-Defined Sparsity: [M] ij &gt; 0 if (i, j) ? L; otherwise [M] ij = 0, ?i, j ? N .</formula><p>The above properties imply that the eigenvalues of M are real and can be sorted as</p><formula xml:id="formula_12">-1 &lt; ? m (M) ? ? ? ? ? ? 2 (M) &lt; ? 1 (M) = 1.</formula><p>We define the second-largest eigenvalue in magnitude of M as ? max{|? 2 (M)|, |? m (M)|}. Our GT-GDA algorithm for each agent i is illustrated in Algorithm 1. Specifically, at the t-th iteration, agent i first calculates the local full gradients as follows:</p><formula xml:id="formula_13">v i,t = ? ? F i (? i,t , ? i,t ), u i,t = ? ? F i (? i,t , ? i,t ).<label>(4)</label></formula><p>Note that v i,t and u i,t only contain the gradient information of the local objective function F i (?, ?). Thus, merely updating with v i,t and u i,t cannot guarantee the convergence of the global objective function F (?, ?). To address this challenge, we introduce two auxiliary variables, p i,t and d i,t . The agent updates the two variables by performing the following local weighted aggregation:</p><formula xml:id="formula_14">p i,t = j?Ni [M] ij p j,t-1 + v i,t -v i,t-1 , d i,t = j?Ni [M] ij d j,t-1 + u i,t -u i,t-1 ,<label>(5)</label></formula><p>where N i {j ? N , : (i, j) ? L} denotes the set of agent i's neighbors. Technically, p i,t and d i,t track the directions of global gradients. With some derivations, it can be shown that i?N p i,t = i?N ? ? F i (? i,t , ? i,t ) and i?N d i,t = i?N ? ? F i (? i,t , ? i,t ). Lastly, each agent updates local parameters following the conventional decentralized gradient descent and ascent steps:</p><formula xml:id="formula_15">? i,t+1 = j?Ni [M] ij ? j,t -?p i,t , ? i,t+1 = j?Ni [M] ij ? j,t + ?d i,t ,<label>(6)</label></formula><p>where the constants ? and ? are the step-sizes for individual primal and dual variables, respectively.</p><p>2) Theoretical Results of GT-GDA: In this section, we will establish the convergence behaviors of the proposed GT-GDA algorithm. Toward this end, we first state several assumptions as follows:</p><formula xml:id="formula_16">Assumption 1. The function F (?, ?) = 1 m m i=1 F i (?, ?</formula><p>) and J(?) = max ??R p F (?, ?) satisfy: (a) (Boundness from Below): There exists a finite lower bound</p><formula xml:id="formula_17">J * = inf ? J(?) &gt; -?; (b) (Lipschitz Smoothness): Local objective function F i (?, ?) is L F -Lipschitz smooth, i.e., there exists a positive constant L F such that the gradient ?F i (?, ?) = [? ? F i (?, ?) , ? ? F i (?, ?) ] satisfies ?F i (?, ?)-?F i (? , ? ) 2 ? L 2 F ?-? 2 +L 2 F ?-? 2 , ? ?, ? , ?, ? ? R p , i ? [m]; (c) (Strong Concavity in Dual): Local objective function F i (?, ?) is ?-strongly concave for fixed ? ? R p , i.e.</formula><p>, there exists a positive constant ? such that</p><formula xml:id="formula_18">? ? F i (?, ?) -? ? F i (?, ? ) ? ? ?-? , ? ?, ?, ? ? R p , i ? [m]</formula><p>(d) (Bounded Dual Maximizer): For any primal variable ? ? R p , its associated dual maximizer ? * (?) arg max ??R p F (?, ?) is bounded, i.e., ? * (?) &lt; ?;</p><p>(e) (Bounded Gradient at Maximum): The partial derivative at maximum point</p><formula xml:id="formula_19">? ? F (?, ? * (?)) is bounded, i.e., ? ? F (?, ? * (?)) &lt; ?, ? ? ? R p .</formula><p>In these assumptions, (a) and (b) are standard in literature. It can be verified that (c) holds when K ? is positive definite; (d)-(e) guarantee that ?J(?) = ? ? F (?, ? * (?)) (see Lemma 10 in the supplementary material). Note that most of the existing works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41]</ref> adopt the compactness assumption to ensure such gradient equivalence. The compactness assumption restricts the feasible parameter space as a closed convex set. Although we also make these boundedness assumptions, the convergence performance of our algorithm is independent of the upper bound of ? * (?) and ? ? F (?, ? * (?)) . To quantify the convergence rate, we propose to use the following new metric, which is the key to the success of establishing all convergence results in this paper:</p><formula xml:id="formula_20">M t ?J( ?t ) 2 +2 ? * t -?t 2 + 1 m m i=1 ? i,t -?t 2 + ? i,t -?t 2 ,<label>(7)</label></formula><p>where ? * t denotes ? * ( ?t ) = arg max ??R p F ( ?t , ?). The first term in <ref type="bibr" target="#b6">(7)</ref> measures the convergence of primal variable ?: ?J( ?t ) 2 = 0 indicates that ?t is a first-order stationary point for J(?). The second term in ( <ref type="formula" target="#formula_20">7</ref>) measures ?t 's convergence to the unique maximizer ? * t for F ( ?t , ?). The last term in <ref type="bibr" target="#b6">(7)</ref> is the average consensus error of local copies. Thus, as M t ? 0, we have that the algorithm reaches a consensus first-order stationary point of the original MSPBE problem. In comparison, the single-agent PE problem <ref type="bibr" target="#b40">[41]</ref> does not have the last four consensus error terms over multi-agents, and so it is dramatically different from our DPE problem. Also, for the linear approximation PE problem in <ref type="bibr" target="#b49">[50]</ref>, the first term is replaced with ?t -? * 2 as it can be viewed as a strongly convex optimization, while the other terms are the same. Based on the metric in <ref type="bibr" target="#b6">(7)</ref>, we have the following:</p><p>Theorem 1 (Convergence of GT-GDA). Under Assumption 1, if the step-sizes satisfy that ? ?/? ? ? 2 /13L 2 F and ? ? min{k 1 , k 2 , k 3 , k 4 }, then GT-GDA has the following convergence result:</p><formula xml:id="formula_21">1 T + 1 T t=0 E[M t ] ? 2E[P 0 -J * ] min{1, L 2 F }(T +1)?</formula><p>, where P t is a potential function defined as:</p><formula xml:id="formula_22">P t J( ?t )+ 8?L 2 F ?? ?t -? * t 2 + 1 m m i=1 ? i,t -?t 2 + ? i,t -?t 2 +? p i,t -pt 2 +? d i,t -dt 2 ,</formula><p>and the constants in the step-size ? are as follows:</p><formula xml:id="formula_23">k 1 = 13L 2 F 2? 2 L F + L 2 F ? + (1 -?) , k 2 = 13L 2 F ? 2 1/2 + 1/(1 -?) 2 , k 3 = (1 -?) 6?(1 + 1/?) , k 4 = 26(1 -?)L 2 F (? 2 + 144L 4 F + 4L 2 F ? 2 + 48? 2 L 2 F (1+1/?) 1-? ) .</formula><p>Remark 1. In Theorem 1, the step-sizes and convergence rate depend on by the network topology. For a sparse network, ? is close to (but not exactly) one (recall that ? = max{|? 2 |, |? m |} &lt; 1). Therefore, k 2 and k 4 are close to zero in this case. Also, the ratio of the step-sizes ? ?/? is required to be a non-zero constant. Either a too small or a too large value of ? might affect the primal or dual convergence of the algorithm. This restriction is due to the consensus error in the decentralized training. In practice, one can first determine ? and then select ? and ?.</p><p>From Theorem 1, we immediately have the following complexity results for GT-GDA: Corollary 2. Under the same conditions in Theorem 1, to achieve an 2 -stationary solution, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">T+1</head><p>T t=0 E[M t ] ? 2 , the total communication rounds are on the order of O( -2 ) and the total samples evaluated across the network system is on the order of O(mn -2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gradient-tracking stochastic variance reduction algorithms</head><p>In the GT-GDA algorithm, agents need to evaluate local full gradients in each iteration, which may result in a high sample complexity when the trajectory length n is large in MARL. This limitation motivates us to leverage the stochastic recursive variance-reduced approach (e.g., <ref type="bibr" target="#b53">[54]</ref>) to achieve low sample complexity in MARL decentralized policy evaluation.</p><formula xml:id="formula_24">Algorithm 2 GT-SRVR/GT-SRVRI Algorithm at Agent i. . If GT-SRVR: |R i,t | = n, |S i,t | = q; If GT-SRVRI: |R i,t | = min{(t/q+1) ? q, c -2 , n}, |S i,t | = q.</formula><p>1: Set prime-dual parameter pair (? i,0 , ? i,0 ) = (? 0 , ? 0 ). 2: Draw R i,0 samples without replacement and calculate local stochastic gradient estimators as</p><formula xml:id="formula_25">p i,0 = v i,0 = 1 |Ri,0| j?Ri,0 ? ? f ij (? i,0 , ? i,0 ), and d i,0 = u i,0 = 1 |Ri,0| j?Ri,0 ? ? f ij (? i,0 , ? i,0 ); 3: for t = 1, ? ? ? , T do 4:</formula><p>Update local parameters (? i,t+1 , ? i,t+1 ) as in (6); Track global gradients (p i,t+1 , d i,t+1 ) as in (5); 7: end for 1) The Algorithms: We first propose an algorithm called gradient-tracking stochastic recursive variance reduction (GT-SRVR) algorithm. Different from GT-GDA, in iteration t and at agent i, GT-SRVR estimates the local gradient with the following estimators:</p><formula xml:id="formula_26">v i,t = ? ? F i (? i,t , ? i,t ), if mod(t, q) = 0, v i,t-1 + 1 |Si,t| j?Si,t ? ? f ij (? i,t , ? i,t )-? ? f ij (? i,t-1 , ? i,t-1 ) , otherwise,<label>(8a)</label></formula><formula xml:id="formula_27">u i,t = ? ? F i (? i,t , ? i,t ), if mod(t, q) = 0, u i,t-1 + 1 |Si,t| j?Si,t ? ? f ij (? i,t , ? i,t )-? ? f ij (? i,t-1 , ? i,t-1 ) , otherwise,<label>(8b)</label></formula><p>where S i,t is a local subsample at the tth iteration for agent i. In (8), the algorithm evaluates full gradients ?F i (? i,t , ? i,t ) only every q steps. For other iterations with mod(t, q) = 0, the algorithm estimates the local gradients with a mini-batch of gradients</p><formula xml:id="formula_28">1 |Si,t| j?Si,t ? ? f ij (? i,t , ? i,t ) and a recursive correction term u i,t-1 -1 |Si,t| j?Si,t ? ? f ij (? i,t-1 , ? i,t-1</formula><p>). It will be shown later that, thanks to the periodic full gradient (when mod(t, q) = 0) and recursive correction term, GT-SRVR is able to achieve the same convergence rate and communication complexity as GT-GDA. Moreover, because of the S i,t subsampling, GT-SRVR has a lower sample complexity than GT-GDA. The full description of GT-SRVR is shown in Algorithm 2.</p><p>Note that in GT-SRVR, full gradients are still required for every q steps, which may still incur a high computational cost. Also, in the initialization phase (before the main loop), agents need to evaluate full gradients, which could be time-consuming. To address these limitations, we propose an enhanced version of GT-SRVR called GT-SRVR with Incremental batch size (GT-SRVRI). Specifically, we modify the gradient estimators in (8a) and (8b) for the tth iteration with mod(t, q) = 0 as follows :</p><formula xml:id="formula_29">v i,t = 1 |R i,t | j?Ri,t ? ? f ij (? i,t , ? i,t ), u i,t = 1 |R i,t | j?Ri,t ? ? f ij (? i,t , ? i,t ),<label>(9)</label></formula><p>where R i,t is a subsample set (sampling without replacement), whose size is chosen as |R i,t | = min{(t/q + 1) ? q, c -2 , n}. Here, ? &gt; 0 is a constant, is a desired convergence error, and c &gt; 0 is a constant that depends on . Our design of |R i,t | is motivated by the fact that the periodic full gradient evaluation only plays an important role in the later stage of the convergence process for achieving high accuracy. Later, we will see that under some mild assumptions and parameter settings, GT-SRVRI has similar convergence performance as GT-SRVR. The full description of GT-SRVR/GT-SRVRI is shown in Algorithm 2. Remark 2. In GT-SRVRI, we increase the batch-size as the number of iterations increases. We note that the work in <ref type="bibr" target="#b17">[18]</ref> also proposed a batch-size adaptation scheme based on the historical gradient information. Although similar idea can be also adopted in our algorithms, it requires the exact value of the stochastic gradient variance ? 2 for batch-size selection as well as extra memory cost to store the history-gradient information, which is less practical compared to our approach.</p><p>2) Theoretical Results of GT-SRVR/GT-SRVRI: Now, we establish the convergence performance of GT-SRVR/GT-SRVRI. First, we replace Assumption 1(b) with the following individual Lipschitz smoothness assumption: Assumption 2 (Lipschitz smoothness). The function f ij (?, ?) is L f -Lipschitz smooth, i.e., there exists a constant L f &gt; 0, such that the gradient</p><formula xml:id="formula_30">?f ij (?, ?) = [? ? f ij (?, ?] , ? ? f ij (?, ?) ) satisfies ?f ij (?, ?)-?f ij (? , ? ) 2 ? L 2 f ?-? 2 +L 2 F ?-? 2 , ? ?, ? , ?, ? ? R p , i ? [m], j ? [n].</formula><p>We note that Assumption 2 is a common assumption for stochastic variance reduced methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41]</ref>. Further, we make the following assumption only for GT-SRVRI algorithm: Assumption 3 (Bounded Variance). There exists a constant</p><formula xml:id="formula_31">? 2 &gt; 0, such that E ?f ij (?, ?) - ?F i (?, ?) 2 ? ? 2 , ? ?, ?, ? R p , i ? [m], j ? [n].</formula><p>With the metric in <ref type="bibr" target="#b6">(7)</ref>, the convergence of GT-SRVR/GT-SRVRI can be characterized as follows: Theorem 3 (Convergence of GT-SRVR). Under Assumption 1 (a)&amp;(c)-(e) and Assumption 2, if the step-sizes satisfy ? ?/? ? ? 2 /13L 2 f and ? ? min{k 5 , k 6 , k 7 , k 8 }, then we have the following convergence result for GT-SRVR:</p><formula xml:id="formula_32">1 T + 1 T t=0 E[M t ] ? 2E[p 0 -J * ] min{1, L 2 f }(T + 1)?</formula><p>, where p t is the potential function defined as:</p><formula xml:id="formula_33">p t J( ?t )+ 8?L 2 f ?? ?t -? * t 2 + 1 m m i=1 ? i,t -?t 2 + ? i,t -?t 2 +? p i,t -pt 2 +? d i,t -dt 2 ,</formula><p>and the constants in the step-size ? are:</p><formula xml:id="formula_34">C 0 = 1 1 -? (1 + 1 ? ) + 1 2 + 18L 2 f ? 2 , k 5 = 13L 2 f ? 2 1/2 + 1/(1 -?) 2 , k 6 = 1 8?C 0 , k 7 = 26(1 -?)L 2 f (? 2 + 144L 4 f + 4L 2 f ? 2 + 64C 0 L 2 f ? 2 ) , k 8 = 13L f 2? 2 L f + L 2 f ? + (1 -?) .</formula><p>Theorem 4 (Convergence of GT-SRVRI). Under Assumption 1 (a)&amp;(c)-(e), Assumption 2, and the same parameter settings and potential function as stated in Theorem 3, we have the following convergence result for GT-SRVRI:</p><formula xml:id="formula_35">1 T +1 T t=0 E[M t ] ? 2E[p 0 -p T+1 ] min{1, L 2 f }(T +1)? + 1 min{1, L 2 f } ? 12 ? (1+ 1 ? )+4+ 144L 2 f ? 2 ? 2 2 c + ? 2 C(n, q, ?) T +1 ,<label>(10)</label></formula><p>where the constant C(n, q, ?) is defined as:</p><formula xml:id="formula_36">C(n, q, ?) 1 1-? ( n q ) ( 1 ? -1) -? 1-? , if ? &gt; 0 and ? = 1 log( n q )+1, if ? = 1. Remark 3.</formula><p>In Theorems 3 and 4, it can be seen that the step-sizes and convergence rate depend on the network topology and the ratio of the step-sizes ?. Additionally, the convergence performance of GT-SRVRI is affected by the constant C(n, q, ?), which depends on the inexact gradient estimation in tth iteration with mod(t, q) = 0.</p><p>Following from Theorems 3 and 4, we immediately have the sample and communication complexity results for GT-SRVR/GT-SRVRI: Corollary 5. Under the conditions in Theorems 3 and 4, and with q = ? n, to achieve an 2stationary solution (i.e., 1</p><formula xml:id="formula_37">T+1 T t=0 E[M t ] ? 2 ) with ? n 2 ? 1,</formula><p>we have:</p><p>? for GT-SRVR, the total communication rounds are O( -2 ) and the total samples evaluated across the network are O(m ? n -2 ));</p><p>? GT-SRVRI with ? ? 1, the total communication rounds are bounded by O(log( ? n) -2 ) and the total samples evaluated across the network are bounded by O(m log( ? n) ? n -2 )). Remark 4. From Corollary 5, we can see that GT-SRVR has the same communication complexity as GT-GDA, but the sample complexity is lower than GT-GDA. For GT-SRVRI with ? ? 1, the upper bounds of both the complexities have an additional factor log( ? n) factor compared with GT-SRVR. Although the theoretical complexity bounds for GT-SRVRI is weaker than GT-SRVR (due to abandoning full gradients completely), we show through experiments in Section 5 that GT-SRVRI empirically outperforms than GT-SRVR in practice.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we demonstrate the performance of our proposed GT-GDA and GT-SRVR/GT-SRVRI algorithms for cooperative MARL decentralized policy evaluation. We adopt the environment of Cooperative Navigation task in <ref type="bibr" target="#b31">[32]</ref>, which consists of m agents inhabiting a two-dimensional world with continuous space and discrete time. In the system, agents cooperate with each other to reach their own landmarks. Due to space limitation, the detailed experimental settings and different network topologies are relegated to the supplementary material.</p><p>Since there is no existing work in the literature on solving decentralized policy evaluation with nonlinear function approximation for MARL, we compare our algorithms with two stochastic algorithms as simple baselines in our experiments: 1) Decentralized Stochastic Gradient Descent Ascent (DSGDA) and 2) Gradient-Tracking-Based Stochastic Gradient Descent Ascent (GT-SGDA) (see the supplementary material for their detailed definitions). We initialize the parameters from the normal distribution for all the algorithms. The learning rates is fixed as ? = 10 -1 , ? = 10 -1 .</p><p>From Figure <ref type="figure" target="#fig_6">1</ref>(a) and 1(b), it can be seen that GT-SRVRI converges much faster than other algorithms (GT-GDA, GT-SRVR, GT-SGD and DSGD) in terms of the total number of gradient evaluations. We can also observe that both GT-SRVR and GT-SRVRI have better sample efficiency in attaining high accuracy (error smaller than 10 -9 ) than the other three algorithms thanks to the variancereduced techniques. As is shown in Figure <ref type="figure" target="#fig_6">1</ref>(c) and 1(d), GT-SRVR and GT-SRVRI have the same communication cost as GT-GDA, which is much lower than those of DSGDA and GT-SGDA. Our experimental results confirm our theoretical analysis that GT-SRVR/GT-SRVRI enjoy low sample and communication complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we studied the decentralized MARL policy evaluation problem with nonlinear function approximation. We first reformulated the problem as a decentralized non-convex-strongly-concave minimax problem and developed a gradient tracking based algorithm called GT-GDA. We showed that GT-GDA algorithm has the communication complexity of O( -2 ) and sample complexity of O(mn -2 ). To further reduce the sample complexity while maintaining the communication complexity, we proposed two stochastic variance-reduced methods called GT-SRVR and GT-SRVRI, both of which can achieve the same communication complexity as GT-GDA but improve the sample complexity to O(m ? n -2 ). We have also conducted extensive numerical studies to verify the performance of our proposed algorithms. We note that our work opens up several interesting direction for future research. First, It is interesting to adopt communication-efficient mechanisms to further reduce the communication cost, especially when the parameters are high-dimensional. Second, it is also interesting to study MARL problems with partially observable information. Lastly, decentralized MARL policy evaluation with non-linear approximation with Markovian online sampling remains an important open problem, which is worth further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 :</head><label>5</label><figDesc>Calculate local gradients (v i,t+1 , u i,t+1 ) as in (4); 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 :</head><label>5</label><figDesc>Calculate local gradient estimators (v i,t+1 , u i,t+1 ) as in (8); 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>MSPBE value vs. sample complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>metric M vs. sample complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>metric M vs. comm. complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MARL decentralized policy evaluation performance comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons among existing policy evaluation algorithms, where m is the number of agents; n is the size of dataset; 2 is the convergence error. Our proposed algorithms are marked in bold.</figDesc><table><row><cell>Algorithm</cell><cell>Reference</cell><cell>Decentralized Nonlinear Convex Multi-Agent Approxi. Sets 1</cell><cell>Sample Complex.</cell><cell>Commun. Complex.</cell></row><row><cell>STSG</cell><cell>[41]</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>A primal-dual algorithm is a two-timescale if ?t/?t ? 0 or ?t/?t ? ? as t ? ?, where ?t and ?t denote the primal and dual step-sizes at time t, respectively.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work has been supported in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">CAREER CNS-2110259</rs>, <rs type="grantNumber">CNS-2112471</rs>, <rs type="grantNumber">CNS-2102233</rs>, <rs type="grantNumber">CCF-2110252</rs>, <rs type="grantNumber">ECCS-2140277</rs>, <rs type="grantNumber">NSF-CCF-1934884</rs> and a <rs type="funder">Google Faculty Research Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ereWgpW">
					<idno type="grant-number">CAREER CNS-2110259</idno>
				</org>
				<org type="funding" xml:id="_4nsaGp5">
					<idno type="grant-number">CNS-2112471</idno>
				</org>
				<org type="funding" xml:id="_uhAaDGw">
					<idno type="grant-number">CNS-2102233</idno>
				</org>
				<org type="funding" xml:id="_p3vjVas">
					<idno type="grant-number">CCF-2110252</idno>
				</org>
				<org type="funding" xml:id="_zNq2Qtq">
					<idno type="grant-number">ECCS-2140277</idno>
				</org>
				<org type="funding" xml:id="_mcc64Vn">
					<idno type="grant-number">NSF-CCF-1934884</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decentralized q-learning for stochastic teams and games</title>
		<author>
			<persName><forename type="first">G?rdal</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serdar</forename><surname>Y?ksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1545" to="1558" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On a theorem of danskin with an application to a theorem of von neumann-sion</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Rapaport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Analysis: Theory, Methods &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1163" to="1181" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neuro-dynamic programming: an overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1995 34th IEEE Conference on Decision and Control</title>
		<meeting>1995 34th IEEE Conference on Decision and Control</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="560" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName><forename type="first">Shalabh</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1204" to="1212" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Achieving controllability of electric loads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName><surname>Hiskens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coverage control for mobile sensing networks</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Karatas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learningbased resource allocation for uav networks</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arumugam</forename><surname>Nallanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="729" to="743" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum-based variance reduction in non-convex sgd</title>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Orabona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15236" to="15245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning from conditional distributions via dual embeddings</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1458" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sbeed: Convergent reinforcement learning with nonlinear function approximation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed optimal power flow for smart microgrids</title>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>Dall'anese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1464" to="1475" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast multi-agent temporal-difference learning via homotopy stochastic primal-dual method</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihailo</forename><forename type="middle">R</forename><surname>Jovanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Foundations for Reinforcement Learning Workshop, 33rd Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finite-time analysis of distributed TD(0) with linear function approximation on multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Thinh</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Maguluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1626" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07944</idno>
		<title level="m">Stochastic variance reduction methods for policy evaluation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Power systems stability control: reinforcement learning framework</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mevludin</forename><surname>Glavic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="427" to="435" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning for electric power system decision and control: Past considerations and perspectives</title>
		<author>
			<persName><forename type="first">Mevludin</forename><surname>Glavic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rapha?l</forename><surname>Fonteneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6918" to="6927" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">Junling</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Michael P Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">History-gradient aided batch size adaptation for variance reduced algorithms</title>
		<author>
			<persName><forename type="first">Kaiyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4762" to="4772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative deep learning in fixed topology networks</title>
		<author>
			<persName><forename type="first">Zhanhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Balu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinmay</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumik</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5904" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for autonomous driving: A survey</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Ravi Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Talpaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad A Al</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcement learning in robotics: A survey</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1238" to="1274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Least-squares policy iteration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An algorithm for distributed reinforcement learning in cooperative multi-agent systems</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Primal-dual algorithm for distributed reinforcement learning: distributed GTD</title>
		<author>
			<persName><forename type="first">Donghwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naira</forename><surname>Hovakimyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1967" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stock trading system using reinforcement learning with cooperative agents</title>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">Won</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning</title>
		<meeting>the Nineteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5336" to="5346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On gradient descent ascent for nonconvex-concave minimax problems</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6083" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><surname>Michael L Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decentralized parallel algorithm for training generative adversarial nets</title>
		<author>
			<persName><forename type="first">Mingrui</forename><surname>Liu Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A decentralized proximal point-type method for saddle point problems</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Pattathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zebang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenggan</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14380</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02275</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decentralized policy gradient descent ascent for safe multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Horesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GNSD: A gradient-tracking based nonconvex stochastic algorithm for decentralized optimization</title>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Data Science Workshop (DSW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="315" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems</title>
		<author>
			<persName><forename type="first">Luo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haishan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03724</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed policy evaluation under multiple behavior strategies</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Valcarcel Macua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">H</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1260" to="1274" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalabh</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1204" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed subgradient methods for multi-agent optimization</title>
		<author>
			<persName><forename type="first">Angelia</forename><surname>Nedic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cooperative control of mobile sensor networks: Adaptive gradient climbing in a distributed environment</title>
		<author>
			<persName><forename type="first">Petter</forename><surname>Ogren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Fiorelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">Ehrich</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1292" to="1302" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Survey of model-based reinforcement learning: Applications on robotics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athanasios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Polydoros</surname></persName>
		</author>
		<author>
			<persName><surname>Nalpantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="173" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Single-timescale stochastic nonconvex-concave optimization for smooth nonlinear TD learning</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10103</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed optimization in sensor networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Iternational Symposium on Information Processing in Sensor Networks</title>
		<meeting>the 3rd Iternational Symposium on Information Processing in Sensor Networks</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fully asynchronous policy evaluation in distributed reinforcement learning over networks</title>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyou</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00433</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Effective reinforcement learning for mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaelbling</forename><surname>Pack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2002 IEEE International Conference on Robotics and Automation</title>
		<meeting>2002 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3404" to="3410" />
		</imprint>
	</monogr>
	<note>Cat. No. 02CH37292</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Finite-time error bounds for linear stochastic approximation andtd learning</title>
		<author>
			<persName><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2803" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9217" to="9228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finite-time analysis of decentralized temporal-difference learning with linear function approximation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyue</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4485" to="4495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variance reduced policy evaluation with smooth function approximation</title>
		<author>
			<persName><forename type="first">Hoi-To</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5784" to="5795" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning via double averaging primal-dual optimization</title>
		<author>
			<persName><forename type="first">Hoi-To</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9649" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Finite-time error bounds for biased stochastic approximation with applications to q-learning</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3015" to="3024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Decentralized td tracking with linear function approximation and its finite-time analysis</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reinforcement learning to play an optimal nash equilibrium in team markov games</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1603" to="1610" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spiderboost and momentum: Faster variance reduction algorithms</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2406" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning for traffic light control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Seventeenth International Conference (ICML&apos;2000)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1151" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Byzantine-resilient decentralized td learning with linear function approximation</title>
		<author>
			<persName><forename type="first">Zhaoxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">An improved convergence analysis for decentralized online stochastic non-convex optimization</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usman</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soummya</forename><surname>Kar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04195</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A collaborative multi-agent reinforcement learning anti-jamming algorithm in wireless networks</title>
		<author>
			<persName><forename type="first">Fuqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luliang</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Communications Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1024" to="1027" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A dissection of overfitting and generalization in continuous reinforcement learning</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07937</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06893</idno>
		<title level="m">Remi Munos, and Samy Bengio. A study on overfitting in deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control</title>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="321" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fully decentralized multi-agent reinforcement learning with networked agents</title>
		<author>
			<persName><forename type="first">Kaiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Basar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5872" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
