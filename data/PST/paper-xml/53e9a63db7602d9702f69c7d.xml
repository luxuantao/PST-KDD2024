<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global stability of complex-valued neural networks with both leakage time delay and discrete time delay on time scales</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-06-22">22 June 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Chongqing Jiaotong University</orgName>
								<address>
									<postCode>400074</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qiankun</forename><surname>Song</surname></persName>
							<email>qiankunsong@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Chongqing Jiaotong University</orgName>
								<address>
									<postCode>400074</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global stability of complex-valued neural networks with both leakage time delay and discrete time delay on time scales</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-06-22">22 June 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">EB45F414AE5AEDCEC2A9055AF84E980B</idno>
					<idno type="DOI">10.1016/j.neucom.2013.04.040</idno>
					<note type="submission">Received 9 December 2012 Received in revised form 5 March 2013 Accepted 27 April 2013 Communicated by H. Zhang</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Complex-valued neural networks Global stability Leakage time delay Discrete time delay Time scales Linear matrix inequality</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, the complex-valued neural networks with both leakage time delay and discrete time delay as well as two types of activation functions on time scales are considered. By using the fixed point theory, a criterion for checking the existence, uniqueness of the equilibrium point for the considered complexvalued neural networks is presented. By constructing appropriate Lyapunov-Krasovskii functionals, and employing the free weighting matrix method, several delay-dependent criteria for checking the global stability of the addressed complex-valued neural networks are established in linear matrix inequality (LMI), which can be checked numerically using the effective LMI toolbox in MATLAB. Three examples with simulations are given to show the effectiveness and less conservatism of the proposed criteria.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, there have been increasing research interests in analyzing the dynamic behaviors of complex-valued neural networks due to their widespread applications, see, <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> and the references therein. In these applications, the activation function plays a very important role.</p><p>In real-valued neural networks, their activation function is usually chosen to be smooth and bounded. However, in the complex domain, according to Liouville's theorem <ref type="bibr" target="#b7">[8]</ref>, every bounded entire function must be constant. Thus, if the activation function is entire and bounded in the complex domain, then it is a constant. This is not suitable. Therefore, activation functions are the main challenge for complex-valued neural networks <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, a class of continuous-time recurrent neural networks with two types of activation functions was considered. Sufficient conditions for existence, uniqueness and global stability of the equilibrium point are given. In <ref type="bibr" target="#b9">[10]</ref>, a class of generalized discrete complex-valued neural networks was studied. The existence of a unique equilibrium pattern is discussed and a sufficient condition of global exponential stability is given. In <ref type="bibr" target="#b10">[11]</ref>, the discretetime delayed neural networks with complex-valued linear threshold neurons were investigated, and several criteria on boundedness, and global exponential stability are obtained. In <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, the discrete-time delayed neural networks with complex-valued linear threshold neurons were investigated, and several criteria on boundedness, global attractivity and complete stability are obtained.</p><p>On the other hand, stability of real-valued neural networks has been widely studied, for example, see <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. It should be mentioned that the most of the studies of the stability of real-valued neural networks are on continuous-time and discrete-time. In fact, both continuous-time and discrete-time neural networks are very important in implementations and applications. But it is troublesome to study the existence, uniqueness and global stability of continuoustime and discrete-time neural networks. It is necessary to unify the study of continuous-time and discrete-time neural networks under the same framework.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, the author proposed the basic ideal of time scales which is useful to unify the differential and difference equations. In <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, the authors summarized and generalized the theory of time scales. In <ref type="bibr" target="#b27">[28]</ref>, the authors introduced the time scales into the study of neural networks. They investigated the existence and stability of a periodic solution to a BAM neural network on time scales. Soon afterwards, the study of stability of neural networks on time scales has become a hot topic. In <ref type="bibr" target="#b28">[29]</ref>, the authors studied the period solutions of delayed neural networks on time scales and obtained some results. In <ref type="bibr" target="#b29">[30]</ref>, the global exponential stability of neural networks with mixed delay and general activation functions on time scales was considered, several LMI criteria for checking the global exponential stability were obtained. In <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>, the stability of some classes of neural networks on time scales was investigated, some sufficient conditions to ensure the stability were given. In <ref type="bibr" target="#b33">[34]</ref>, the authors studied a complex-valued neural network model on time scales and obtained a sufficient condition to guarantee global exponential stability.</p><p>As pointed out in <ref type="bibr" target="#b34">[35]</ref>, neural networks with leakage delay is a class of important neural networks: time delay in the leakage term Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom also has great impact on the dynamics of neural networks since time delay in the stabilizing negative feedback term has a tendency to destabilize a system <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. Therefore, it is necessary to investigate the stability problem for neural networks with leakage delay <ref type="bibr" target="#b34">[35]</ref>. To the best of the authors knowledge, there is no results on the problem of stability for complex-valued neural networks with leakage delays.</p><p>Motivated by the above discussions, the objective of this paper is to study the existence, uniqueness and global stability of the equilibrium point of complex-valued neural networks with both leakage delay and time delay as well as two types of activation functions on time scales by employing a combination of fixed point theory, Lyapunov-Krasovskii functionals and the free weighting matrix method. The obtained sufficient conditions do not require the existence on the partial derivatives of the activation functions, which is needed in <ref type="bibr" target="#b8">[9]</ref>, and are expressed in linear matrix inequality (LMI), which can be checked numerically using the effective LMI toolbox in MATLAB. Three examples are given to show the effectiveness and less conservatism of the proposed criteria.</p><p>Notations: The notations are quite standard. Throughout this paper, C n , R mÂn and C mÂn denote, respectively, the n-dimensional Euclidean space and the set of all m Â n real and complex matrices. The subscript n and T denote matrix complex conjugation and transposition and matrix transposition, respectively. The notation X≥Y (respectively, X 4Y) means that X-Y is positive semi-definite (respectively, positive definite). ρ max ðPÞ and ρ min ðPÞ are defined as the largest and the smallest eigenvalue of matrix P, respectively. T is an arbitrary nonempty closed subset (time scale) of R. Set ½a; b T is defined as ½a; b T ≔ft∈T; a ≤t ≤bg. Sometimes, the arguments of a function or a matrix will be omitted in the analysis when no confusion can arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problems formulation and preliminaries</head><p>In this section, we will recall some definitions and lemmas which will be used in the proofs of our main results.</p><p>The forward and backward jump operators s; ρ : T-R and the graininess μ : T-R þ , respectively, by sðtÞ≔inf fs∈T : s 4 tg, ρðtÞ≔ supfs∈T : s o tg and μðtÞ ¼ sðtÞ-t.</p><p>Throughout this paper, we always assume that time scale T has a bounded graininess μðtÞ ≤μ o ∞.</p><p>For a point t∈T, t is called right-dense if sðtÞ ¼ t, right-scattered if sðtÞ 4 t, left-dense if ρðtÞ ¼ t, left-scattered if ρðtÞ o t.</p><p>A function f : T-R is called rd-continuous provided it is continuous at right-dense points on T and its left sided limits exist at left-dense points on T. The set of rd-continuous functions</p><formula xml:id="formula_0">f : T-R is denoted by C rd ¼ C rd ðTÞ ¼ C rd ðT; RÞ. A function f : T-R is called regressive, if 1 þ μðtÞf ðtÞ≠0, for all t∈T.</formula><p>For f : T-R and t∈T, we define the delta derivative of f(t), f Δ ðtÞ, to be the number (if it exists) with the property that, for an ϵ 4 0, there exists a neighborhood U of t such that j½f ðsðtÞÞ-f ðsÞ-f Δ ðtÞ½sðtÞ-sj o ϵ½sðtÞ-s; for all s∈U.</p><p>We call f Δ ðtÞ the delta derivative of f at t. Moreover, we say that f is delta differentiable on T provided that f Δ ðtÞ exists for all t∈T. The function f Δ ðtÞ : T-R is then called the delta derivative of f on T.</p><p>When f is right-dense at t, then</p><formula xml:id="formula_1">f Δ ðtÞ ¼ lim s-t þ</formula><p>f ðsÞ-f ðtÞ s-t :</p><p>When f is right-scattered at t, then f Δ ðtÞ ¼ f ðsðtÞÞ-f ðtÞ sðtÞ-t :</p><p>Lemma 1 (Agarwal et al. <ref type="bibr" target="#b24">[25]</ref>, Bohner and Peterson <ref type="bibr" target="#b25">[26]</ref>). If f and g are two delta differentiable functions on time scale T. Then</p><formula xml:id="formula_2">ðfgÞ Δ ðtÞ ¼ f Δ ðtÞgðtÞ þ f ðsðtÞÞg Δ ðtÞ ¼ g Δ ðtÞf ðtÞ þ gðsðtÞÞf Δ ðtÞ:</formula><p>Let f be right-dense continuous, if F Δ ðtÞ ¼ f ðtÞ, we define the delta integral by Z t a f ðsÞ Δs ¼ FðtÞ-FðaÞ:</p><p>It is easy to check that the following formula holds: Z sðtÞ t f ðsÞ Δs ¼ μðtÞf ðtÞ:</p><p>In this paper, we consider the following complex-valued neural networks with both leakage delay and time delay on time scale T:</p><formula xml:id="formula_3">z Δ ðtÞ ¼ -Czðt-δÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ þ H;<label>ð1Þ</label></formula><p>for t∈T, where zðtÞ ¼ ðz The initial condition associated with neural network (1) is given by z p ðsÞ ¼ ϕ p ðsÞ; s∈½-ζ; 0 T ; p ¼ 1; 2; …; N; where ζ ¼ maxfδ; τg and ϕ p ðsÞ are continuous in ½-ζ; 0 T for all p ¼ 1; 2; …; N.</p><p>As we know, the activation functions play a very important role in the study of global exponential stability of neural networks. However, in the complex domain, the activation functions cannot both be entire and bounded. In this paper, we will consider the following two types of activation functions.</p><p>(H1) Let ReðzÞ and ImðzÞ be the real and imaginary parts of a complex number z, respectively. f p ðzÞ can be expressed by</p><formula xml:id="formula_4">f p ðzÞ ¼ f R p ðReðzÞÞ þ if I p ðImðzÞÞ; where f R p ð Á Þ; f I p ð Á Þ : R-R for all p ¼ 1; 2; …; N. Then, for w 1 ∈R, w 2 ∈R and w 1 ≠w 2 , ξ R- p ≤ f R p ðw 1 Þ-f R p ðw 2 Þ w 1 -w 2 ≤ξ Rþ p ; ξ I- p ≤ f I p ðw 1 Þ-f I p ðw 2 Þ w 1 -w 2 ≤ξ Iþ p ;</formula><p>for all p ¼ 1; 2; …; N. (H2) The real and imaginary parts of f p ð Á Þ cannot be separated and it is bounded and satisfies the following condition in the complex domain for all p ¼ 1; 2; …; N, for u; v∈C:</p><formula xml:id="formula_5">jf p ðuÞ-f p ðvÞj ≤F p ju-vj</formula><p>where F p is a constant.</p><p>Next we introduce some definitions and lemmas to be used in the stability analysis. Definition 1. Vector z is called an equilibrium point of neural network (1) if it satisfies </p><formula xml:id="formula_6">-C z þ ðD þ EÞf ðzÞ þ H ¼ 0: Definition 2. Neural</formula><formula xml:id="formula_7">-z∥ ¼ ð∑ N p ¼ 1 jReðz p ðtÞÞ-Reðz p ÞÞj 2 þ ∑ N p ¼ 1 jImðz p ðtÞÞ-Imðz p ÞÞj 2 Þ 1=2 and ∥ϕðsÞ∥ ¼ ð∑ N p ¼ 1 jReðϕðsÞÞj 2 þ ∑ N p ¼ 1 jImðϕðsÞÞj 2 Þ 1=2 . Definition 3 (Song [17]). A map H: R n -R n is a homeomorphism of R n onto itself, if H∈C 0 , H is one-to-one, H is onto and the inverse map H -1 ∈C 0 .</formula><p>To prove our results, the following lemmas are necessary.</p><p>Lemma 2 (Song <ref type="bibr" target="#b16">[17]</ref>). If H∈C 0 satisfies the following conditions:</p><formula xml:id="formula_8">(i) H is injective on R 2n , (ii) ∥Hðx; yÞ∥-þ ∞ as ∥ðx; yÞ T ∥-þ ∞, then H is homeomorphism of R 2n onto itself.</formula><p>Lemma 3 (Xu and Lam <ref type="bibr" target="#b13">[14]</ref>). Let P be a positive definite matrix and a; b∈R n , then 2a</p><formula xml:id="formula_9">T b ≤a T P -1 a þ b T Pb.</formula><p>Lemma 4 (Schur complement). Given constant matrices P, Q and R, where</p><formula xml:id="formula_10">P T ¼ P, Q T ¼ Q , then P R R T -Q " # o 0; is equivalent to the following conditions Q 4 0 and P þ RQ -1 R T o0.</formula><p>Lemma 5 <ref type="bibr">(Gu [40]</ref>). For any constant matrix W∈R nÂn  In the following lemma, we will extend Lemma 5 to the complex domain. </p><formula xml:id="formula_11">Proof. Let W ¼ W R þ iW I and ωðsÞ ¼ ω R ðsÞ þ iω I ðsÞ, where W T R ¼ W R , W T I ¼ -W I . Then, by Lemma 5, we have Z b a ωðsÞ Δs ! n W Z b a ωðsÞ Δs ! ¼ Z b a ω R ðsÞ ω I ðsÞ ! Δs ! T W R -W I W I W R ! Z b a ω R ðsÞ ω I ðsÞ ! Δs ! ≤ðb-aÞ Z b a ω R ðsÞ ω I ðsÞ ! T W R -W I W I W R ! ω R<label>ðsÞ</label></formula><formula xml:id="formula_12">ω I ðsÞ ! Δs ¼ ðb-aÞ Z b a ω n ðsÞWωðsÞ Δs:</formula><p>The proof is completed. □ Lemma 7 (Gahinet et al. <ref type="bibr" target="#b40">[41]</ref>). Given a Hermitian matrix Q, then</p><formula xml:id="formula_13">Q o0 is equivalent to Q R -Q I Q I Q R " # o 0; where Q R ¼ ReðQ Þ and Q I ¼ ImðQ Þ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main results</head><p>First, let us consider separable activation functions f p ð Á Þ satisfying (H1) for all p ¼ 1; 2; …; N. Since the real parts and imaginary parts of the activation functions have individual properties, we separate the neural network (1) on time scale T into its real and imaginary parts. By separating the state vector, connection weight matrix, vector-valued activation function, and the external input vector into its real and imaginary part, we have that</p><formula xml:id="formula_14">x Δ ðtÞ ¼ -Cxðt-δÞ þ D R f R ðxðtÞÞ-D I f I ðyðtÞÞ þE R f R ðxðt-τÞÞ-E I f I ðyðt-τÞÞ þ H R y Δ ðtÞ ¼ -Cyðt-δÞ þ D I f R ðxðtÞÞ þ D R f I<label>ðyðtÞÞ</label></formula><formula xml:id="formula_15">þE I f R ðxðt-τÞÞ þ E R f I ðyðt-τÞÞ þ H I ; 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; :<label>ð2Þ</label></formula><p>for t∈T, where xðtÞ ¼ ReðzðtÞÞ,</p><formula xml:id="formula_16">yðtÞ ¼ ImðzðtÞÞ, f R ð Á Þ ¼ Reðf ð Á ÞÞ, f I ð Á Þ ¼ Imðf ð Á ÞÞ, D R ¼ ReðDÞ, D I ¼ ImðDÞ, E R ¼ ReðEÞ, E I ¼ ImðEÞ, H R ¼ ReðHÞ and H I ¼ ImðHÞ.</formula><p>In order to get the main result of this case, the proof is divided into two steps. The first step, we shall prove the existence and uniqueness of the equilibrium point for neural network (1) under Condition (H1).</p><p>For presentation convenience, in the following, we denote</p><formula xml:id="formula_17">W 1 ¼ diagðξ R 1 ; ξ R 2 …; ξ R N Þ; W 2 ¼ diagðξ I 1 ; ξ I 2 ; …; ξ I N Þ;</formula><p>with ξ R p ¼ maxfjξ R- p j; jξ Rþ p jg and ξ I p ¼ maxfjξ I- p j; jξ Iþ p jg.</p><p>Theorem 1. Under Condition (H1), neural network (1) has a unique equilibrium point on time scale T, if there exist six positive diagonal matrices U 1 ; U 2 ; …; U 6 , such that the following LMI holds:</p><formula xml:id="formula_18">Π 1;1 Π 1;2 n Π 2;2 ! o0;<label>ð3Þ</label></formula><p>where</p><formula xml:id="formula_19">Π 1;1 ¼ -U 1 C-CU 1 þ W T 1 U 5 W 1 0 0 -U 2 C-CU 2 þ W T 2 U 6 W 2 ! ; Π 1;2 ¼ U 1 D R þ U 1 E R -CU 3 -U 1 D I -U 1 E I U 2 D I þ U 2 E I U 2 D R þ U 2 E R -CU 4 ! ; Proof. Let Hðx; yÞ ¼ -C 0 0 -C x y ! þ D R þ E R -D I -E I D I þ E I D R þ E R ! Â f R<label>ðxÞ</label></formula><formula xml:id="formula_20">f I ðyÞ ! þ H R H I ! :<label>ð4Þ</label></formula><p>In the following, we will prove that H(x) is a homeomorphism of R n onto itself. First, we prove that H(x) is an injective map on R n .</p><formula xml:id="formula_21">Π 2;2 ¼ -U 5 þ U 3 D R þ U 3 E R þ ðD R Þ T U 3 þ ðE R Þ T U 3 -U 3 D I -U 3 E I þ ðD I Þ T U 4 þ ðE I Þ T U 4 -ðD I Þ T U 3 -ðE I Þ T U 3 þ U 4 D I þ U 4 E I -U 6 þ U 4 D R þ U 4 E R þ ðD R Þ T U 4 þ ðE R Þ T U 4 ! :</formula><p>In fact, if there exist ðu; vÞ T ∈R 2N and ðu; vÞ T ≠ðx; yÞ T such that Hðu; vÞ ¼ Hðx; yÞ, then</p><formula xml:id="formula_22">-C 0 0 -C x-u y-v ! þ D R þ E R -D I -E I D I þ E I D R þ E R ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_23">f I ðyÞ-f I ðvÞ ! ¼ 0:</formula><p>Multiply both sides above by</p><formula xml:id="formula_24">2 x-u y-v ! T U 1 0 0 U 2 ! þ 2 f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_25">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_26">! T U 3 0 0 U 4 ! ; we get 0 ¼ 2 x-u y-v ! T -U 1 C 0 0 -U 2 C ! x-u y-v ! þ2 x-u y-v ! T U 1 ðD R þ E R Þ -U 1 ðD I þ E I Þ U 2 ðD I þ E I Þ U 2 ðD R þ E R Þ ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_27">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_28">! þ2 f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_29">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_30">! T -U 3 C 0 0 -U 4 C ! x-u y-v ! þ2 f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_31">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_32">! T U 3 ðD R þ E R Þ -U 3 ðD I þ E I Þ U 4 ðD I þ E I Þ U 4 ðD R þ E R Þ ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_33">f I ðyÞ-f I ðvÞ ! ¼ 2 x-u y-v ! T -U 1 C 0 0 -U 2 C ! x-u y-v ! þ2 x-u y-v ! T U 1 ðD R þ E R Þ-CU 3 -U 1 ðD I þ E I Þ U 2 ðD I þ E I Þ U 2 ðD R þ E R Þ-CU 4 ! Â f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_34">f I ðyÞ-f I ðvÞ ! þ2 f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_35">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_36">! T U 3 ðD R þ E R Þ -U 3 ðD I þ E I Þ U 4 ðD I þ E I Þ U 4 ðD R þ E R Þ ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_37">f I ðyÞ-f I ðvÞ !<label>ð5Þ</label></formula><p>Let</p><formula xml:id="formula_38">Σ 1 ¼ U 5 0 0 U 6 ! - U 3 0 0 U 4 ! D R þ E R -D I -E I D I þ E I D R þ E R ! - D R þ E R -D I -E I D I þ E I D R þ E R ! T U 3 0 0 U 4 ! ;</formula><p>then an application of Lemma 4 yields Σ 1 4 0 and</p><formula xml:id="formula_39">Σ 2 ¼ U 1 0 0 U 2 ! -C 0 0 -C þ -C 0 0 -C U 1 0 0 U 2 ! þ W 1 0 0 W 2 ! T U 5 0 0 U 6 ! W 1 0 0 W 2 ! þ U 1 0 0 U 2 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ -C 0 0 -C U 3 0 0 U 4 !! Σ -1 1 Â U 1 0 0 U 2 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ -C 0 0 -C U 3 0 0 U 4 !! T o 0:<label>ð6Þ</label></formula><p>For notation simplicity, let</p><formula xml:id="formula_40">Σ 3 ¼ U 1 0 0 U 2 ! -C 0 0 -C þ -C 0 0 -C U 1 0 0 U 2 ! þ U 1 0 0 U 2 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ -C 0 0 -C U 3 0 0 U 4 !! Σ -1 1 Â U 1 0 0 U 2 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ -C 0 0 -C U 3 0 0 U 4 !! T : Σ 4 ¼ Σ 1 þ U 3 0 0 U 4 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ D R þ E R -D I -E I D I þ E I D R þ E R ! T U 3 0 0 U 4 ! :</formula><p>By ( <ref type="formula" target="#formula_37">5</ref>) and Lemma 4, we have</p><formula xml:id="formula_41">0 ≤ x-u y-v ! T Σ 3 x-u y-v ! þ f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_42">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_43">! T Σ 4 f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_44">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_45">! ; that is 0 ≤ x-u y-v ! T Σ 3 x-u y-v ! þ f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_46">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_47">! T Â U 5 0 0 U 6 ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_48">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_49">! :<label>ð7Þ</label></formula><p>Since U 5 and U 6 are positive diagonal matrices, from Condition (H1), we can get</p><formula xml:id="formula_50">f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_51">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_52">! T U 5 0 0 U 6 ! f R ðxÞ-f R<label>ðuÞ</label></formula><formula xml:id="formula_53">f I ðyÞ-f I<label>ðvÞ</label></formula><formula xml:id="formula_54">! T ≤ x-u y-v ! T W 1 0 0 W 2 ! T U 5 0 0 U 6 ! Â W 1 0 0 W 2 ! x-u y-v ! :<label>ð8Þ</label></formula><p>It follows from ( <ref type="formula" target="#formula_49">7</ref>) and ( <ref type="formula" target="#formula_54">8</ref>) that</p><formula xml:id="formula_55">0 ≤ x-u y-v ! T Σ 2 x-u y-v ! :<label>ð9Þ</label></formula><p>From ( <ref type="formula" target="#formula_39">6</ref>) and ( <ref type="formula" target="#formula_55">9</ref>), we can get ðx; yÞ T ¼ ðu; vÞ T . Therefore, Hðx; yÞ is an injective map on R 2N . Second, we prove that ∥Hðx; yÞ∥-þ ∞ as ∥ðx;</p><formula xml:id="formula_56">yÞ T ∥-þ ∞. The vectorial norm ∥ðx; yÞ T ∥ is defined as ∥ðx; yÞ T ∥ ¼ ð∑ N m ¼ 1 jx m j 2 þ∑ N m ¼ 1 jy m j 2 Þ 1=2 .</formula><p>Let Hðx; yÞ ¼ Hðx; yÞ-Hð0; 0Þ. By Lemma 3, we have</p><formula xml:id="formula_57">2 x y ! T U 1 0 0 U 2 ! þ 2 f R<label>ðxÞ</label></formula><formula xml:id="formula_58">f I<label>ðyÞ</label></formula><formula xml:id="formula_59">! T - f R<label>ð0Þ</label></formula><formula xml:id="formula_60">f I<label>ð0Þ</label></formula><formula xml:id="formula_61">! T 0 @ 1 A 0 @ U 3 0 0 U 4 !! Hðx; yÞ ¼ 2 x y ! T U 1 0 0 U 2 ! -C 0 0 -C x y ! þ 2 x y ! T U 1 0 0 U 2 ! D R þ E R -D I -E I D I þ E I D R þ E R ! þ 2 -C 0 0 -C U 3 0 0 U 4 !! f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_62">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_63">! þ f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_64">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_65">! T U 3 0 0 U 4 ! D R þ E R -D I -E I D I þ E I D R þ E R ! Â f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_66">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_67">! ≤ x y ! T Σ 3 x y ! þ 2 f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_68">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_69">! T Â U 5 0 0 U 6 ! f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_70">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_71">! :<label>ð10Þ</label></formula><p>It follows from Condition (H1) that</p><formula xml:id="formula_72">f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_73">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_74">! T U 5 0 0 U 6 ! f R ðxÞ-f R<label>ð0Þ</label></formula><formula xml:id="formula_75">f I ðyÞ-f I<label>ð0Þ</label></formula><formula xml:id="formula_76">! ≤ x y ! T W 1 0 0 W 2 ! T U 5 0 0 U 6 ! W 1 0 0 W 2 ! x y ! :<label>ð11Þ</label></formula><p>From ( <ref type="formula" target="#formula_39">6</ref>), we know that Σ 2 o 0. Then from ( <ref type="formula" target="#formula_71">10</ref>) and ( <ref type="formula" target="#formula_76">11</ref>), we obtain</p><formula xml:id="formula_77">2 x y ! T U 1 0 0 U 2 ! þ 2 f R<label>ðxÞ</label></formula><formula xml:id="formula_78">f I<label>ðyÞ</label></formula><formula xml:id="formula_79">! T 0 @ 0 @ - f R<label>ð0Þ</label></formula><formula xml:id="formula_80">f I<label>ð0Þ</label></formula><formula xml:id="formula_81">! T 1 A U 3 0 0 U 4 !! Hðx; yÞ ≤ x y ! T Σ 2 x y ! ≤-ρ min ð-Σ 2 Þ∥ðx; yÞ T ∥ 2 ;<label>ð12Þ</label></formula><p>which is equivalent to</p><formula xml:id="formula_82">ρ min ð-Σ 2 Þ∥ðx; yÞ T ∥ 2 ≤2 x y ! T U 1 0 0 U 2 ! þ f R<label>ðxÞ</label></formula><formula xml:id="formula_83">f I<label>ðyÞ</label></formula><formula xml:id="formula_84">! T 0 @ 0 @ - f R<label>ð0Þ</label></formula><formula xml:id="formula_85">f I<label>ð0Þ</label></formula><formula xml:id="formula_86">! T 1 A U 3 0 0 U 4 !! Á : Hðx; yÞ∥ ≤2 U 1 0 0 U 2 ! þ W 1 0 0 W 2 ! T U 3 0 0 U 4 ! 0 @ 1 A ∥ðx; yÞ T ∥ Á ∥ Hðx; yÞ∥;<label>ð13Þ</label></formula><p>when ∥ðx; yÞ T ∥≠0, we have</p><formula xml:id="formula_87">∥ Hðx; yÞ∥≥ ρ min ð-Σ 2 Þ 2 U 1 0 0 U 2 ! þ W 1 0 0 W 2 ! T U 5 0 0 U 6 ! W 1 0 0 W 2 ! 0 @ 1 A ∥ðx; yÞ T ∥: Therefore, ∥ Hðx; yÞ∥-þ ∞ as ∥ðx; yÞ T ∥-þ ∞ which implies ∥Hðx; yÞ∥-þ ∞ as ∥ðx; yÞ T ∥-þ ∞.</formula><p>From Lemma 2, we know that Hðx; yÞ is a homeomorphism of R 2N . Thus, neural network (2) has a unique equilibrium point. □</p><p>The second step, we are going to prove the global stability of the equilibrium point for neural network (1) on time scale T.</p><p>If the activation function satisfies Condition (H1), we suppose that the conditions of Theorem 1 hold and denote that γ R p ¼ maxfjξ R- p j; jξ </p><formula xml:id="formula_88">¼ P R 1 þ iP I 1 , P 2 ¼ P R 2 þ iP I 2 , P 3 ¼ P R 3 þ iP I 3 , two positive diagonal matrices R 1 ¼ diagðr 1 ; r 2 ; …; r N Þ, R 2 ¼ diagðr′ 1 ; r′ 2 ; …; r′ N Þ, two any appropriately dimensioned matrices Q 1 ¼ Q R 1 þ iQ I 1 , Q 2 ¼ Q R 2 þ iQ I 2 ,</formula><p>such that the following LMI holds:</p><formula xml:id="formula_89">Ω R -Ω I Ω I Ω R ! o 0:<label>ð14Þ</label></formula><p>where</p><formula xml:id="formula_90">Ω R ¼ Ω R 1;1 Ω R 1;2 Ω R 1;3 0 Ω R 1;5 0 0 Ω R 2;1 Ω R 2;2 Ω R 2;3 0 0 Ω R 2;6 Ω R 2;7 Ω R 3;1 Ω R 3;2 Ω R 3;3 0 -CP R 1 C Ω R 3;6 Ω R 3;7 0 0 0 Ω R 4;4 0 0 0 -CP R 1 C 0 -CP R 1 C 0 -1 δ P R 2 0 0 0 Ω R 6;2 Ω R 6;3 0 0 -R 1 0 0 Ω R 7;2 Ω R 7;3 0 0 0 -R 2 0 B B B B B B B B B B B B B B @ 1 C C C C C C C C C C C C C C A ;</formula><p>and</p><formula xml:id="formula_91">Ω I ¼ Ω I 1;1 Ω I 1;2 Ω I 1;3 0 Ω I 1;5 0 0 Ω I 2;1 Ω I 2;2 Ω I 2;3 0 0 Ω I 2;6 Ω I 2;7 Ω I 3;1 Ω I 3;2 Ω I 3;3 0 -CP I 1 C Ω I 3;6 Ω I 3;7 0 0 0 -P I 3 0 0 0 -CP I 1 C 0 -CP I 1 C 0 -1 δ P I 2 0 0 0 Ω I 6;2 Ω I 6;3 0 0 0 0 0 Ω I 7;2 Ω I 7;3 0 0 0 0 0 B B B B B B B B B B B B B B @ 1 C C C C C C C C C C C C C C A ; where Ω R 1;1 ¼ -CP R 1 -P R 1 C þ μCP R 1 C þ δP R 2 þ P R 3 þ R 1 Γ, Ω R 1;2 ¼ P R 1 -μCP R 1 , Ω R 1;3 ¼ P R 1 C-μCP R 1 C, Ω R 1;5 ¼ -CP R 1 C, Ω R 2;1 ¼ P R 1 -μP R 1 C, Ω R 2;2 ¼ μP R 1 þ ðQ R 1 Þ T þ Q R 1 , Ω R 2;3 ¼ -μP R 1 C-ðQ R 1 Þ T C-Q R 2 , Ω R 2;6 ¼ ðQ R 1 Þ T D R þ ðQ I 1 Þ T D I , Ω R 2;7 ¼ ðQ R 1 Þ T E R þ ðQ I 1 Þ T E I , Ω R 3;1 ¼ CP R 1 -μCP R 1 C, Ω R 3;2 ¼ -μCP R 1 -CQ R 1 - ðQ R 2 Þ T , Ω R 3;3 ¼ μCP R 1 C-ðQ R 2 Þ T C-CQ R 2 , Ω R 3;6 ¼ ðQ R 2 Þ T D R þ ðQ I 2 Þ T D I , Ω R 3;7 ¼ ðQ R 2 Þ T E R þ ðQ I 2 Þ T E I , Ω R 4;4 ¼ -P R 3 þ R 2 Γ, Ω R 6;2 ¼ ðD R Þ T Q R 1 þ ðD I Þ T Q I 1 , Ω R 6;3 ¼ ðD R Þ T Q R 2 þ ðD I Þ T Q I 2 , Ω R 7;2 ¼ ðE R Þ T Q R 1 þ ðE I Þ T Q I 1 , Ω R 7;3 ¼ ðE R Þ T Q R 2 þ ðE I Þ T Q I 2 , Ω I 1;1 ¼ -CP I 1 -P I 1 C þ μCP I 1 C þ δP I 2 þ P I 3 , Ω I 1;2 ¼ P I 1 -μCP I 1 , Ω I 1;3 ¼ P I 1 C-μCP I 1 C, Ω I 1;5 ¼ -CP I 1 C, Ω I 2;1 ¼ P I 1 -μP I 1 C, Ω I 2;2 ¼ μP I 1 , Ω I 2;3 ¼ -μP I 1 C þ ðQ I 1 Þ T C-Q I 2 , Ω I 2;6 ¼ ðQ R 1 Þ T D I -ðQ I 1 Þ T D R , Ω I 2;7 ¼ ðQ R 1 Þ T E I - ðQ I 1 Þ T E R , Ω I 3;1 ¼ CP I 1 -μCP I 1 C, Ω I 3;2 ¼ -μCP I 1 -CQ I 1 þ ðQ I 2 Þ T , Ω I 3;3 ¼ μCP I 1 C, - Ω I 3;6 ¼ ðQ R 2 Þ T D I -ðQ I 2 Þ T D R , Ω I 3;7 ¼ ðQ R 2 Þ T E I -ðQ I 2 Þ T E R Ω I 6;2 ¼ ðD R Þ T Q I 1 - ðD I Þ T Q R 1 , Ω I 6;3 ¼ ðD R Þ T Q I 2 -ðD I Þ T Q R 2 , Ω I 7;2 ¼ ðE R Þ T Q I 1 -ðE I Þ T Q R 1 , Ω I 7;3 ¼ ðE R Þ T Q I 2 -ðE I Þ T Q R 2 .</formula><p>Proof. Under the condition of Theorem 1, neural network (1) has a unique equilibrium point ẑ. Then we shift the equilibrium point of ( <ref type="formula" target="#formula_15">2</ref>) to the origin by the translation zðtÞ ¼ zðtÞ-ẑ and obtain</p><formula xml:id="formula_92">zΔ ðtÞ ¼ -C zðt-δÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ<label>ð15Þ</label></formula><p>where f ðzðtÞÞ ¼ f ðzðtÞÞ-f ðẑÞ, f ð zðt-τÞÞ ¼ f ðzðt-τÞÞ-f ðẑÞ.</p><p>Consider the following Lyapunov-Krasovskii functional candidates</p><formula xml:id="formula_93">VðtÞ ¼ V 1 ðtÞ þ V 2 ðtÞ; where V 1 ðtÞ ¼ zðtÞ-C Z t t-δ zðsÞ Δs n P 1 zðtÞ-C Z t t-δ zðsÞ Δs V 2 ðtÞ ¼ Z 0 -δ Z t tþθ zn ðsÞP 2 zðsÞ Δs Δθ þ Z t t-τ</formula><p>zn ðsÞP 3 zðsÞ Δs;</p><p>Calculate the delta derivative of V(t) along the trajectories of neural network (1) under condition (H1), we obtain </p><formula xml:id="formula_94">V Δ 1<label>ðtÞ</label></formula><p>In deriving inequalities ( <ref type="formula">16</ref>), we have made use of Lemma 6.</p><p>If </p><formula xml:id="formula_96">f p ð Á Þ satisfies Condition (H1), then ξ R- p ≤ f R p ð</formula><p>where r p is a positive constant for all p ¼ 1; 2; …; N.</p><p>The vector forms of ( <ref type="formula">19</ref>) and ( <ref type="formula" target="#formula_97">20</ref>) are that</p><formula xml:id="formula_98">f n ðzðtÞÞR 1 f ðzðtÞÞ ≤z n ðtÞR 1 Γ zðtÞ;<label>ð21Þ</label></formula><p>where R 1 ¼ diagfr 1 ; r 2 ; …; r N g. Also, we have</p><formula xml:id="formula_99">f n ðzðt-τÞÞR 2 f ðzðt-τÞÞ ≤z n ðt-τÞR 2 Γ zðt-τÞ;<label>ð22Þ</label></formula><p>where R 2 ¼ diagfr′ 1 ; r′ 2 ; …; r′ N g. It follows from ( <ref type="formula" target="#formula_92">15</ref>) that</p><formula xml:id="formula_100">0 ¼ ð-z Δ ðtÞ-C zðt-δÞ þ Df ðzðtÞÞ þ Ef ð zðt-τÞÞÞ n ðQ 1 zΔ ðtÞ þ Q 2 zðt-δÞÞ þðQ 1 zΔ ðtÞ þ Q 2 zðt-δÞÞ n ð-z Δ ðtÞ-C zðt-δÞ þDf ðzðtÞÞ þ Ef ðzðt-τÞÞÞ<label>ð23Þ</label></formula><p>It follows from ( <ref type="formula">16</ref>), ( <ref type="formula" target="#formula_95">17</ref>) and ( <ref type="formula" target="#formula_98">21</ref>)-( <ref type="formula" target="#formula_100">23</ref>) that</p><formula xml:id="formula_101">V Δ ðtÞ ≤α n 1 ðtÞΩα 1 ðtÞ:<label>ð24Þ</label></formula><p>where α 1 ðtÞ ¼ ðzðtÞ; zΔ ðtÞ; zðt-δÞ; zðt-τÞ; R t t-δ zðsÞ Δs; f ðzðtÞÞ; f ðzðt-τÞÞÞ T and with</p><formula xml:id="formula_102">Ω 1;1 ¼ -CP 1 -P 1 C þ μCP 1 C þ δP 2 þ P 3 þ R 1 Γ.</formula><p>Separate the real and imaginary parts of Ω, then utilizing Lemma 7, Ω o 0 is equivalent to</p><formula xml:id="formula_103">Ω R -Ω I Ω I Ω R ! o 0:</formula><p>Thus, it follows from (24) that</p><formula xml:id="formula_104">V Δ ðtÞ ≤0:<label>ð25Þ</label></formula><p>From (25), we have VðtÞ ≤Vð0Þ</p><formula xml:id="formula_105">¼ zð0Þ-C Z 0 -δ zðsÞ Δs ! n P 1 zð0Þ-C Z 0 -δ zðsÞ Δs ! þ Z 0 -δ Z 0 θ zn ðsÞP 2 zðsÞ Δs Δθ þ Z 0 -τ zn ðsÞP 3 zðsÞ Δs ≤ð∥P 1 ∥ þ 2δ∥P 1 C∥ þ δ 2 ∥C Ã P 1 C∥ þ δ 2 ∥P 2 ∥ þ τ∥P 3 ∥Þ sup s∈½-ζ;0 T ∥ϕðsÞ∥ ! 2 ≜M 1 sup s∈½-ζ;0 T ∥ϕðsÞ∥ ! 2 :<label>ð26Þ</label></formula><p>From definition of V (t), we have</p><formula xml:id="formula_106">VðtÞ≥ρ max ðP 1 Þ zðtÞ-C Z t t-δ zðsÞΔs 2<label>ð27Þ</label></formula><p>It follows from ( <ref type="formula" target="#formula_105">26</ref>) and ( <ref type="formula" target="#formula_106">27</ref>) that ∥zðtÞ∥ ≤∥C∥</p><formula xml:id="formula_107">Z t t-δ ∥zðsÞ∥Δs þ ffiffiffiffiffiffiffi M 1 p ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ρ max ðP 1 Þ p sup s∈½-ζ;0 T ∥ϕðsÞ∥:</formula><p>From the well-known Gronwall inequality, one obtains where</p><formula xml:id="formula_108">∥zðtÞ∥ ≤ ffiffiffiffiffiffiffi M 1 p ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ρ max ðP 1 Þ p sup s∈½-ζ;0 T</formula><formula xml:id="formula_109">M ¼ ffiffiffiffiffiffiffi M 1 p e δ∥C∥ = ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ρ max ðP 1 Þ p .</formula><p>By Definition 2, we obtain that neural network (1) under Condition (H1) is globally stable on time scale T. The proof is completed.</p><formula xml:id="formula_110">□ Ω ¼ Ω 1;1 CP 1 C-μCP 1 P 1 C-μCP 1 C 0 -CP 1 C 0 0 CP 1 C-μP 1 C μCP 1 C þ Q n 1 þ Q 1 -μP 1 C-Q n 1 C-Q 2 0 0 Q n 1 D Q n 1 E CP 1 -μCP 1 C -μCP 1 -CQ 1 -Q n 2 μCP 1 C-Q n 2 C-CQ 2 0 -CP 1 C Q n 2 D Q n 2 E 0 0 0 -P 3 þ R 2 Γ 0 0 0 -CP 1 C 0 -CP 1 C 0 -1 δ P 2 0 0 0 D n Q 1 D n Q 2 0 0 -R 1 0 0 E n Q 1 E n Q 2 0 0 0 -R 2 0 B B B B B B B B B B B @ 1 C C C C C C C C C C C A ;</formula><p>When there is no leakage delay and time scale T ¼ C, neural network (1) turns into the following model which is considered in <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_111">_ zðtÞ ¼ -CzðtÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ þ H:<label>ð28Þ</label></formula><p>For neural network <ref type="bibr" target="#b27">(28)</ref>, we can also get the following result.</p><p>Corollary 1. Neural network <ref type="bibr" target="#b27">(28)</ref> is globally stable at the equilibrium point, if there exist two positive definite Hermitian matrices</p><formula xml:id="formula_112">P 1 ¼ P R 1 þ iP I 1 , P 2 ¼ P R 2 þ iP I 2 , two positive diagonal matrices R 1 ¼ diagðr 1 ; r 2 ; …; r N Þ, R 2 ¼ diagðr′ 1 ; r′ 2 ; …; r′ N Þ, and any appropri- ately dimensioned matrix Q 1 ¼ Q R 1 þ iQ I 1 , such that the following LMI holds: Ω R -Ω I Ω I Ω R ! o0:<label>ð29Þ</label></formula><p>where</p><formula xml:id="formula_113">Ω R ¼ Ω R 1;1 Ω R 1;2 0 0 0 Ω R 2;1 Ω R 2;2 0 Ω R 2;4 Ω R 2;5 0 0 Ω R 3;3 0 0 0 Ω R 4;2 0 -R 1 0 0 Ω R 5;2 0 0 -R 2 0 B B B B B B B B @ 1 C C C C C C C C A ;</formula><p>and</p><formula xml:id="formula_114">Ω I ¼ Ω I 1;1 Ω I 1;2 0 0 0 Ω I 2;1 0 0 Ω I 2;4 Ω I 2;5 0 0 -P I 2 0 0 0 Ω I 4;2 0 0 0 0 Ω I 5;2 0 0 0 0 B B B B B B B B @ 1 C C C C C C C C A ; where Ω R 1;1 ¼ -CP R 1 -P R 1 C þ P R 2 þ R 1 Γ, Ω R 1;2 ¼ P R 1 , Ω R 2;1 ¼ P R 1 , Ω R 2;2 ¼ ðQ R 1 Þ T þ Q R 1 , Ω R 2;4 ¼ ðQ R 1 Þ T D R þ ðQ I 1 Þ T D I ,0Ω R 2;5 ¼ ðQ R 1 Þ T E R þ ðQ I 1 Þ T E I , Ω R 3;3 ¼ -P R 2 þ R 2 Γ, Ω R 4;2 ¼ ðD R Þ T Q R 1 þ ðD I Þ T Q I 1 , Ω R 5;2 ¼ ðE R Þ T Q R 1 þ ðE I Þ T Q I 1 , Ω I 1;1 ¼ -CP I 1 -P I 1 C þ P I 2 , Ω I 1;2 ¼ P I 1 , Ω I 2;1 ¼ P I 1 , Ω I 2;4 ¼ ðQ R 1 Þ T D I -ðQ I 1 Þ T D R , Ω I 2;5 ¼ðQ R 1 Þ T E I -ðQ I 1 Þ T E R , Ω I 4;2 ¼ðD R Þ T Q I 1 -ðD I Þ T Q R 1 , Ω I 5;2 ¼ðE R Þ T Q I 1 -ðE I Þ T Q R 1 .</formula><p>Remark 1. In Assumption 1 in <ref type="bibr" target="#b8">[9]</ref>, the activation function is supposed to be</p><formula xml:id="formula_115">f p ðzÞ ¼ f R ðx; yÞ þ if I ðx; yÞ;</formula><p>where z ¼ x þ iy and f R p ðÁ; ÁÞ, f I p ðÁ; ÁÞ are real-valued for all p ¼ 1; 2; …; N. The partial derivatives of f R p ðÁ; ÁÞ, f I p ðÁ; ÁÞ with respect to x and y exist and are continuous. In this paper, both the real parts and the imaginary parts of the activation functions are no longer assumed to be derivable. Remark 2. In <ref type="bibr" target="#b10">[11]</ref>, the authors studied the global stability of complex-valued neural networks with the activation function</p><formula xml:id="formula_116">f p ðzÞ ¼ maxf0; ReðzÞg þ i maxf0; ImðzÞg;</formula><p>which is a class of activation function satisfies Condition (H1) in this paper. Also, the authors did not provide a feasible way to solve the complex-valued LMIs. In this paper, Lemma 7 is presented to solve the complex-valued LMIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Example</head><p>Example 1. Consider a two-neuron neural network on a special time scale</p><formula xml:id="formula_117">T ¼ ⋃ þ∞ k ¼ -1 ½2k; 2k þ 1 z Δ ðtÞ ¼ -Czðt-δÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ þ H;<label>ð30Þ</label></formula><p>where</p><formula xml:id="formula_118">C ¼ 1 0 0 1 ; D ¼ 2 þ i -3-i -2 þ i -1 " # ; E ¼ -2 i -1 þ i -2 " # ; H ¼ 2-i -2 þ i " # ; τ ¼ 2; f R p ðReðz p ÞÞ ¼ 1 20 ð Reðz p Þ þ 1 þ Reðz p Þ-1 Þ , f I p ðImðz p ÞÞ ¼ 1 10 ð Imðz p Þ þ 1 þ Imðz p Þ-1 Þ</formula><p>for all p ¼1,2. When δ ¼ 2, Figs. 1 and 2 depict the real and imaginary parts of states of the considered neural network <ref type="bibr" target="#b29">(30)</ref> with δ ¼ 2, where initial condition is z 1 ðtÞ ¼ 0:01; z 2 ðtÞ ¼ 0:05i. It is easy to check that the unique equilibrium point of neural network ( <ref type="formula" target="#formula_117">30</ref>) is globally stable.</p><p>When δ ¼ 4, Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref> depict the real and imaginary parts of states of the considered neural network <ref type="bibr" target="#b29">(30)</ref> with δ ¼ 4, where initial condition is z 1 ðtÞ ¼ 0:05; z 2 ðtÞ ¼ 0:05i. It is easy to check that the unique equilibrium point of neural network <ref type="bibr" target="#b29">(30)</ref> is not stable, this implies that the effect of leakage delay on the dynamics of neural networks on time scales cannot be ignored. In the following, we will consider the existence, uniqueness and global stability of the equilibrium point on time scale T for neural network <ref type="bibr" target="#b29">(30)</ref> with δ ¼ 2.</p><p>It can be verified that the activation functions satisfy Condition (H1), and</p><formula xml:id="formula_119">W 1 ¼ 0:1 0 0 0:1 ; W 2 ¼ 0:2 0 0 0:2 ; D R ¼ 2 -3 -2 -1 ; D I ¼ 1 -1<label>1</label></formula><formula xml:id="formula_120">0 ; E R ¼ -2 0 -1 -2 ; E I ¼ 0 1 1 0 ; Also, we have Γ ¼ ð 0:04 0 0 0:04 Þ.</formula><p>It is easy to check that the graininess μðtÞ of T satisfies 0 ≤μðtÞ ≤1. Thus, μ ¼ 1.</p><p>By employing Matlab LMI Toolbox, we can find the solutions to LMIs in (3) and ( <ref type="formula" target="#formula_89">14</ref>) as follows, which guarantee the existence, uniqueness and global stability of the equilibrium point on time scale T: </p><formula xml:id="formula_121">U 1 ¼ 10</formula><formula xml:id="formula_122">Q 1 ¼ 10 -9 -1:129 þ 0:075i 0:114-0:1159i 0:104 þ 0:0328i -1:523-0:1253i ! ; Q 2 ¼ 10 -10 0:4397 þ 0:0753i -0:0441 þ 0:0154i -0:0492-0:0937i 0:6531-0:1127i : Example 2. Consider a two-neuron neural network on a special time scale T ¼ ⋃ þ∞ k ¼ -1 ½2k; 2k þ 1 z Δ ðtÞ ¼ -Czðt-δÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ þ H;<label>ð31Þ</label></formula><p>where</p><formula xml:id="formula_123">C ¼ 1 0 0 1 ; D ¼ 2 þ i -3-i -2 þ i -1 " # ; E ¼ -2 i -1 þ i -2 " # ; H ¼ 2-i -2 þ i " # ; τ ¼ 2; δ ¼ 2; f 1 ðz 1 Þ ¼ 1 20 ð z 1 þ 1 -z 1 -1 Þ , f 2 ðz 2 Þ ¼ 1 20 ð z 2 þ 1 -z 2 -1 Þ .</formula><p>It can be verified that the activation functions satisfy Condition (H2), and</p><formula xml:id="formula_124">Γ ¼ 0:01 0 0 0:01 ; D R ¼ 2 -3 -2 -1 ; D I ¼ 1 -1<label>1</label></formula><formula xml:id="formula_125">0 ; E R ¼ -2 0 -1 -2 ; E I ¼ 0 1 1 0 :</formula><p>It is easy to check that the graininess μðtÞ of T satisfies 0 ≤μðtÞ ≤1. Thus, μ ¼ 1. By employing Matlab LMI Toolbox, we can find the solutions to LMIs in <ref type="bibr" target="#b13">(14)</ref> as follows, which guarantee the global stability of the equilibrium point on time scale T: Q 2 ¼ 10 -11 2:261 þ 0:7013i -0:257-0:2942 -0:267-0:3859 3:08-0:9109i :</p><formula xml:id="formula_126">P 1 ¼ 10 -</formula><p>Figs. 5 and 6 depict the real and imaginary parts of states of the considered network <ref type="bibr" target="#b30">(31)</ref>, where initial condition is x 1 ðtÞ ¼ 0:1 cos ð0:1tÞi, x 2 ðtÞ ¼ 0:5 sin ð0:3tÞ, t∈½-2; 0 T .</p><p>Example 3. Consider a two-neuron neural network</p><formula xml:id="formula_127">_ zðtÞ ¼ -CzðtÞ þ Df ðzðtÞÞ þ Ef ðzðt-τÞÞ þ H;<label>ð32Þ</label></formula><p>where</p><formula xml:id="formula_128">C ¼ 1 0 0 1 ; D ¼ i -3-i -1 þ i -1 " # ; E ¼ -1 1 -1 þ i -2-i " # ; H ¼ 2-i -2 þ i " # ; τ ¼ 2; f 1 ðz 1 Þ ¼ 1 20 ð z 1 þ 1 -z 1 -1 Þ , f 2 ðz 2 Þ ¼ 1 20 ð z 2 þ 1 -z 2 -1 Þ .</formula><p>It can be verified that the activation functions satisfy Condition (H2), and Γ ¼ 0:01 0 0 0:01</p><formula xml:id="formula_129">; D R ¼ 0 -3 -1 -1 ; D I ¼ 1 -1 1 0 ; E R ¼ -1 0 -1 -2 ; E I ¼ 0 0 1 -1 :</formula><p>By employing Matlab LMI Toolbox, we can find the solutions to LMIs in <ref type="bibr" target="#b13">(14)</ref> as follows, which guarantee global stability of the equilibrium point. On the other hand, we cannot find the solution to the LMIs in Theorem 4 of <ref type="bibr" target="#b8">[9]</ref>, which means that the conclusions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Theorem 4]</ref> are not applicable to ascertain stability of the model. Therefore, for this example, the global stability conditions (Corollary 1) are less conservative than that in Theorem 4 of <ref type="bibr" target="#b8">[9]</ref>.  <ref type="bibr" target="#b31">(32)</ref>, where initial condition is x 1 ðtÞ ¼ 0:1 cos ð0:1tÞi, x 2 ðtÞ ¼ 0:5 sin ð0:3tÞ, t∈½-2; 0.</p><formula xml:id="formula_130">P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, the existence, uniqueness and global stability of the equilibrium point of complex-valued neural networks with both leakage delay and time delay as well as two types of activation functions have been investigated by employing a combination of fixed point theory, Lyapunov-Krasovskii functionals and the free weighting matrix method. Two sufficient conditions for checking the existence, uniqueness and global stability of the equilibrium point for the considered complexvalued neural networks have been given in linear matrix inequality (LMI), which can be checked numerically using the effective LMI toolbox in Three examples have been provided to demonstrate the effectiveness and less conservatism of the proposed criteria.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and W 4 0, a scalar function ωðsÞ : ½a; b T -R n with scalars a ob such that the integrations concerned are well defined, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 6 .</head><label>6</label><figDesc>For any constant Hermitian matrix W∈C nÂn and W 4 0, a scalar function ωðsÞ : ½a; b T -C n with scalars a ob such that the integrations concerned are well defined, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>∥ϕðsÞ∥e δ∥C∥ : which means that ∥zðtÞ-ẑ∥ ≤M sup s∈½-ζ;0 T ∥ϕðsÞ∥;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. State trajectories of model (30) with δ ¼ 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Q 1 ¼Fig. 4 .Fig. 3 .</head><label>143</label><figDesc>Fig. 4. State trajectories of model (30) with δ ¼ 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. State trajectories of neural network (31).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. State trajectories of neural network (32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>XiaofengFig. 8 .</head><label>8</label><figDesc>Fig. 8. State trajectories of neural network (32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 ðtÞ; z 2 ðtÞ; …; z N ðtÞÞ T ∈C N is the state vector of ; c 2 ; …; c N g∈R NÂN is the self-feedback connection weight matrix. D ¼ ðd pq Þ NÂN ∈C NÂN is the connection weight matrix, E ¼ ðe pq Þ NÂN ∈C NÂN is the delayed connection weight matrix. f ðzðtÞÞ ¼ ðf ðz 1 ðtÞÞ; f ðz 2 ðtÞÞ; …; f ðz N ðtÞÞÞ T ∈C N denotes the neuron activation at time t. H ¼ ðh 1 ; h 2 ; …; h N Þ T ∈C N is the external input vector. δ≥0, τ≥0 is the leakage delay, the transmission delay, respectively, which satisfy that t-τ; t-δ∈T for all t∈T.</figDesc><table><row><cell>the</cell><cell>neural</cell><cell>network</cell><cell>with</cell><cell>N</cell><cell>neurons</cell><cell>at</cell><cell>time</cell><cell>t.</cell></row><row><cell cols="2">C ¼ diagfc 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>network (1) is globally stable at equilibrium point z on time scale T, if there exists a positive constant M such that the solution zðtÞ ¼ ðz 1 ðtÞ; z 2 ðtÞ; …; z n ðtÞÞ of neural network (1) satisfies ‖zðtÞ-z‖ ≤M sup</figDesc><table><row><cell>∥ϕðsÞ∥;</cell></row><row><cell>s∈½-ζ;0 T</cell></row><row><cell>where</cell></row><row><cell>∥zðtÞ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>If f p ð Á Þ satisfies Condition (H2), then jf p ðz p ðtÞÞj ≤F p jz p ðtÞj.</figDesc><table><row><cell></cell><cell>xp ðtÞÞ xp ðtÞ</cell><cell cols="2">≤ξ Rþ p ; ξ I-p ≤</cell><cell>f</cell><cell>I p ð ỹp ðtÞÞ ỹp ðtÞ</cell><cell>≤ξ Iþ p ;</cell></row><row><cell cols="3">for all p ¼ 1; 2; …; N.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Thus,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>jf</cell><cell cols="2">R p ð xp ðtÞÞj ≤γ R p j xp ðtÞj; jf</cell><cell cols="3">I p ð ỹp ðtÞÞj ≤γ I p j ỹp ðtÞj;</cell><cell>ð18Þ</cell></row><row><cell cols="6">where γ R p ¼ maxfjξ R-p j; jξ Rþ p jg, γ I p ¼ maxfjξ I-p j; jξ Iþ p jg for all p ¼</cell></row><row><cell cols="2">1; 2; …; N.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">By (18), we have</cell><cell></cell><cell></cell></row><row><cell cols="2">r p f</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Thus,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">r p f</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>n p ðz p ðtÞÞf p ðz p ðtÞÞ ≤r p γ 2 p zn p zp ;</p>ð19Þ</p>where γ p ¼ maxfγ R p ; γ I p g and r p is a positive constant for all p ¼ 1; 2; …; N. n p ðz p ðtÞÞf p ðz p ðtÞÞ ≤r p F 2 p zn p zp ;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Figs. 7 and 8depict the real and imaginary parts of states of the considered network</figDesc><table><row><cell>1 ¼</cell><cell cols="2">19:1081 -2:1714-2:791i</cell><cell cols="3">-2:1714 þ 2:791i 21:5900</cell><cell>;</cell></row><row><cell>P 2 ¼</cell><cell cols="2">11:9955 -2:8682-1:0651i</cell><cell cols="4">-2:8682 þ 1:0651i 13:0691</cell><cell>;</cell></row><row><cell>R 1 ¼</cell><cell>201:9240 0</cell><cell cols="2">0 255:7542</cell><cell>; R 2 ¼</cell><cell cols="3">152:8010 0</cell><cell>0 267:9551</cell><cell>;</cell></row><row><cell>Q 1 ¼</cell><cell cols="6">-18:2607-1:1813i 3:5359 þ 6:7807i 5:2928-3:0007i -21:4852-2:9271i</cell><cell>:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Chen, Q. Song / Neurocomputing 121 (2013) 254-264</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the reviewers and the editor for their valuable suggestions and comments which have led to a much improved paper. The work is supported by the National Natural Science Foundation of China under Grants 61273021, 60974132, 11172247 and 51208538.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Complex-valued Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hirose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complex-valued multistate associative memory with nonlinear multilevel functions for gray-level image reconstruction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1463" to="1473" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Symmetric complex-valued RBF receiver for multipleantenna-aided wireless systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lajos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Channel equalization using adaptive complex radial basis function networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Orthogonality of decision boundaries of complex-valued neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-layered complex-valued neural network for realvalued classification problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="945" to="955" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On efficient learning machine with root power mean neuron in complex domain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="727" to="738" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Howell</surname></persName>
		</author>
		<title level="m">Complex Analysis for Mathematics and Engineering</title>
		<imprint>
			<publisher>Jones and Bartlett Pub. Inc</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>3rd edn.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global stability of complex-valued recurrent neural networks with time-delays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="853" to="865" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global dynamics of a class of complex valued neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boundedness and stability for discrete-time delayed neural network with complex-valued linear threshold neurons</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Dyn. Nat. Soc</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
	<note>Article ID 368379, 19 pp.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discrete-time recurrent neural networks with complexvalued linear threshold neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="669" to="673" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A class of discrete time recurrent neural networks with multivalued neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="3782" to="3788" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new approach to exponential stability analysis of neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="83" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust stability analysis of a class of neural networks with discrete time delays</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faydasicok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">29-30</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamical stability analysis of multiple equilibrium points in time-varying delayed recurrent neural networks with discontinuous activation functions</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exponential stability of recurrent neural networks with both timevarying delays and general activation functions via LMI approach</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="2823" to="2830" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel weighting-delay-based stability criteria for recurrent neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymptotic stability for neural networks with mixed time delays: the discrete-time case</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global exponential stability of BAM neural networks with time-varying delays: the discrete-time case</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Anthoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Nonlinear Sci. Numer. Simulation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="613" to="622" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multistability of recurrent neural networks with time-varying delays and the piecewise linear activation function</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1371" to="1377" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust stability for stochastic Hopfield neural networks with time delays</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal.: Real World Appl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1119" to="1128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global asymptotic stability of delayed cellular neural networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="947" to="950" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Analysis on measure chains-a unified approach to continuous and discrete calculus, Results Math</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hilger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="18" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic equations on time scales: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bohner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dynamic Equations on Time Scales an Introduction with Applications, Birkhäuser</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bohner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new analytical method for the linearization of dynamic equation on measure chains</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Differential Equations</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="543" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global exponential stability of delayed BAM network on time scale</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="3582" to="3588" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Periodic solution to BAM neural network with delays on time scales</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="274" to="282" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global exponential stability of neural networks with discrete and distributed delays and general activation functions on time scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3142" to="3150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global exponential stability for impulsive BAM neural networks with distributed delays on time scales</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="91" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global exponential stability of fuzzy interval delayed neural networks with impulse on time scales</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust stability of delayed reaction-diffusion recurrent neural networks with Dirichlet boundary conditions on time scales</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1632" to="1637" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global stability of complex-valued neural networks on time scales</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bohner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Differential Equations Dyn. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leakage delays in BAM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalsamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1117" to="1132" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the stability of nonlinear systems with leakage delay</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Inst</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="377" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Passivity of uncertain neural networks with both leakage delay and time-varying delay</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Dyn</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1695" to="1707" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Global attractive periodic solutions of BAM neural networks with continuously distributed delays in the leakage terms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal.: Real World Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2141" to="2151" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Delay-dependent stability of neural networks of neutral type with time delay in the leakage term</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinearity</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1709" to="1726" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An integral inequality in the stability problem of time-delay systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CDC Australia</title>
		<meeting>the IEEE CDC Australia</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="2805" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gahinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Laub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chilali</surname></persName>
		</author>
		<title level="m">LMI Control Toolbox User&apos;s Guide (1st Version)</title>
		<meeting><address><addrLine>Natick, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MathWorks, Inc</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
