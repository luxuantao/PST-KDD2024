<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theoretical Computer Science</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Matthieu</forename><surname>Latapy</surname></persName>
							<email>matthieu.latapy@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Pierre et Marie Curie</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Theoretical Computer Science</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A1297DB8EFB7D1048E7BDAA9C42AC2C</idno>
					<idno type="DOI">10.1016/j.tcs.2008.07.017</idno>
					<note type="submission">Received 7 December 2006 Received in revised form 1 February 2008 Accepted 23 July 2008 Communicated by G. Italiano</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graphs Algorithms Triangles Complex networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding, counting and/or listing triangles (three vertices with three edges) in massive graphs are natural fundamental problems, which have recently received much attention because of their importance in complex network analysis. Here we provide a detailed survey of proposed main-memory solutions to these problems, in a unified way.</p><p>We note that previous authors have paid surprisingly little attention to space complexity of main-memory solutions, despite its both fundamental and practical interest. We therefore detail space complexities of known algorithms and discuss their implications. We also present new algorithms which are time optimal for triangle listing and beats previous algorithms concerning space needs. They have the additional advantage of performing better on power-law graphs, which we also detail. We finally show with an experimental study that these two algorithms perform very well in practice, allowing us to handle cases which were previously out of reach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A triangle in an undirected graph is a set of three vertices such that each possible edge between them is present in the graph. Following classical conventions, we call finding, counting and listing the problems of deciding if a given graph contains any triangle, counting the number of triangles in the graph, and listing all of them, respectively. We moreover call node-counting the problem of counting for each vertex the number of triangles to which it belongs. We refer to all these problems as a whole by triangle problems.</p><p>Triangle problems may be considered as classical, natural and fundamental algorithmic questions, and have been studied as such <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Moreover, they have recently gained much practical importance since they are central in the so-called complex network analysis, see for instance <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. First, they are involved in the computation of one of the main statistical property used to describe large graphs met in practice, namely the clustering coefficient <ref type="bibr" target="#b35">[36]</ref>. The clustering coefficient of a vertex v (of degree at least 2) is the probability that any two randomly chosen neighbors of v are linked together. It is computed by dividing the number of triangles containing v by the number of possible edges between its neighbors, i.e. d(v)</p><formula xml:id="formula_0">2 if d(v)</formula><p>denotes the number of neighbors of v. One may then define the clustering coefficient of the whole graph as the average of this value for all the vertices (of degree at least 2). Likewise, the transitivity ratio 1 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> is defined as</p><formula xml:id="formula_1">3N ∆ N ∨</formula><p>where N ∆ denotes the number of triangles in the graph and N ∨ denotes the number of connected triples, i.e. pairs of edges with one common extremity, in the graph.</p><p>In the context of complex network analysis, triangles also play a key role in the study of motif occurrences, i.e. the presence of special (small) subgraphs in given (large) graphs. This has been studied in particular in protein interaction networks, where some motifs may correspond to biological functions, see for instance <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref>. Triangles often are key parts of such motifs.</p><p>In summary, triangle finding, counting, node-counting and/or listing appear as key issues both from a fundamental point of view and for practical purpose. The aim of this contribution is to review the algorithms proposed until now for solving these problems with both a fundamental perspective (we discuss asymptotic complexities and give detailed proofs) and a practical one (we discuss space requirements and graph encoding, and we evaluate algorithms with some experiments).</p><p>We note that, until now, authors have paid surprisingly little attention to space requirements of main-memory algorithms for triangle problems; this however is an important limitation in practice, and this also induces interesting theoretical questions. We therefore discuss this (all space complexity results stated in this paper are new, though very simple in most cases), and we present space-efficient algorithms.</p><p>Approaches relying on streaming algorithms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref>, approximate results <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, or compressed graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> also exist. They are of high interest in cases where the graphs do not fit in main-memory. Otherwise, they are much slower and more intricate than main-memory algorithms, on which we focus here. We will see that, as long as the graph fits in main-memory, such approaches are sufficient.</p><p>The paper is organised as follows. After a few preliminaries (Section 2), we begin with results on finding, counting and node-counting problems, among which basically no difference in complexity is known (Section 3). Then we turn to the harder problem of triangle listing, in Section 4. In these parts of the paper, we deal with both the general case (no assumption is made on the graph) and on the important case where the graph is sparse. Many very large graphs met in practice also have heterogeneous degrees; we focus on this case in Section 5. Finally, we present experimental evaluations in Section 6. We summarise the current state-of-the-art and we point out the main perspectives in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Throughout the paper, we consider an undirected <ref type="foot" target="#foot_0">2</ref> graph G = (V , E) with n = |V | vertices and m = |E| edges. We suppose that G is simple ((v, v) ∈ E for all v, and there is no multiple edge). We also assume that m ∈ Ω(n); this is a classical convention which plays no role in our algorithms but makes complexity formulae simpler. We denote by N(v) = {u ∈ V , (v, u) ∈ E} the neighborhood of v ∈ V and by d(v) = |N(v)| its degree. We also denote by d max the maximal degree in G: d max = max v {d(v)}.</p><p>Before entering in the core of this paper, we need to discuss a few issues that play an important role in the following. They are necessary to make the discussion all along the paper precise and rigorous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation for precise space complexity</head><p>In the context of complex network studies, the difference between an algorithm with a given time complexity and an algorithm twice as fast generally is not crucial. Space limitations are much stronger and dividing space complexity by a constant is a significant improvement: it often makes the difference between tractable and untractable computations in practice. We will give an illustration of this in Section 6.</p><p>In order to capture this situation, we will use a notation in addition to the usual O() and Θ() ones. We will say that a space complexity is in Θ (f (n, m)) if the space cost of the algorithm is exactly σ f (n, m) + c where c is any constant and σ is the space needed to encode a vertex, an integer between 0 and n, or a pointer. Though it actually is in Θ(log(n)), we will follow the classical convention assuming that σ is a constant; taking this into account would make the text unclear, and would bring little information, if any.</p><p>With this notation, the adjacency matrix of G needs Θ ( n 2 σ ) ⊆ Θ(n 2 ) space, because the matrix needs n 2 bits (and an integer and a pointer). An adjacency list representation of G (array of n linked lists) needs Θ (4m + n) ⊆ Θ(m) space (a vertex and a pointer for each edge in both directions plus n pointers), and an adjacency array representation (array of n arrays) needs only Θ (2m + n) ⊆ Θ(m) space. This is why this last representation generally is preferred when dealing with huge graphs.</p><p>Throughout this paper, we will assume that graphs are given by their adjacency matrix and/or adjacency array representation. Results on such representations may easily be converted into results on adjacency list representations (only the space complexity in terms of Θ () is affected), as well as more subtle adjacency representations (hashtables or balanced trees for instance).</p><p>Each adjacency array in an adjacency array representation of G may moreover be sorted. This can be done in place in Θ(m log(n)) time and Θ (2m + n) ⊆ Θ(m) space (only a constant space is needed in addition to the representation of G). <ref type="foot" target="#foot_1">3</ref>Finally, notice that, in several cases, we will not give the space needs in terms of Θ () because the algorithm complexity is prohibitive; the precise space requirements then are of little interest, and they would make the text intricate. We will enter in these details only in cases where the time and space complexities are small enough to make the precise space cost interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worst case complexity, and graph families</head><p>All the complexities we discuss in this paper are worst case complexities, in the sense that they are bounds for the time and space needs of the algorithms, on any input. In most cases, these bounds are tight (leading to the use of the Θ() notation, see for instance <ref type="bibr" target="#b16">[17]</ref> for definitions). In other words, we say that an algorithm is in Θ(f (n)) if for all possible instances of the input the algorithm runs within this complexity, and there is at lease one instance for which it is reached. In several cases, however, the worst case complexity is actually the complexity for any input (in the case of Theorem 4, for instance, and for most space complexities). It would also be of high interest to study the expected behavior of triangle algorithms, in addition to the worst case one. This has been done in some cases; for instance, it is proved in <ref type="bibr" target="#b23">[24]</ref> that vertex-iterator (see Section 4.1) has expected time complexity in O(n 5 3 ). Obtaining such results however is often very difficult, and their relevance for practical purposes is not always clear: the choice of a model for the average input is a difficult task (in our context, random graphs would be an unsatisfactory choice <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36]</ref>). We therefore focus on worst case analysis, which has the advantage of giving guarantees on the behaviors of algorithms, on any input.</p><p>Another interesting approach is to study (worst case) complexities on given graph families. This has already been done on various cases, the most important ones probably being the sparse graphs, i.e. graphs in which m is in o(n 2 ). This is motivated by the fact that most real-world complex networks lead to such graphs, see for instance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36]</ref>. Often, it is even assumed that m is in O(n). Recent studies however show that, despite the fact that m is small compared to n 2 , it may be in ω(n) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27]</ref>. Other classes of graphs have been considered, like for instance planar graphs: it is shown in <ref type="bibr" target="#b23">[24]</ref> that one may decide if any planar graph contains a triangle in O(n) time.</p><p>We do not detail all these results here. Since we are particularly interested in real-world complex networks, we present in detail the results concerning sparse graphs all along the paper. We also introduce new results on power-law graphs (Section 5), which capture an important property met in practice. A survey on available results on specific classes of graphs remains to be done, and is out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The fastest algorithms for finding, counting, and node-counting</head><p>The fastest algorithm known for node-counting relies on fast matrix product <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref>. Indeed, if one considers the adjacency matrix A of G then the value A 3 vv on the diagonal of A 3 is nothing but twice the number of triangles to which v belongs, for any v. Finding, counting and node-counting triangle problems can therefore be solved in O(n ω ) time, where ω &lt; 2.376 is the fast matrix product exponent <ref type="bibr" target="#b15">[16]</ref>. This was first noticed in 1978 <ref type="bibr" target="#b23">[24]</ref>, and currently no faster algorithm is known for any of these problems in the general case, even for triangle finding (but this is no longer true when the graph is sparse, see Theorem 2).</p><p>This approach naturally needs the graph to be given by its adjacency matrix representation. Moreover, it makes it necessary to compute and store the matrix A 2 , leading to a Θ(n 2 ) space complexity in addition to the adjacency matrix storage.</p><p>Theorem 1 <ref type="bibr">([24,16]</ref>). Given the adjacency matrix representation of G, it is possible to solve triangle finding, counting and nodecounting in O(n ω ) ⊂ O(n 2.376 ) time and Θ(n 2 ) space on G using fast matrix product.</p><p>This time complexity is the current state of our knowledge, as long as one makes no assumption on G. Note that no non-trivial lower bound is known for this complexity; therefore faster algorithms may be designed.</p><p>As we will see, there exist (slower) algorithms with lower space complexity for these problems. Some of these algorithms only need an adjacency array representation of G. They are derived from listing algorithms, which we present in Section 4.</p><p>One can design faster algorithms if G is sparse. In <ref type="bibr" target="#b23">[24]</ref>, it was first proved that triangle finding, counting, node-counting and listing<ref type="foot" target="#foot_2">4</ref> can be solved in Θ(m</p><p>2 ) time and Θ(m) space. This result has been improved in <ref type="bibr" target="#b13">[14]</ref> using a property of the graph (namely arboricity) but the worst case complexities were unchanged. No better result was known until 1995 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, where the authors prove Theorem 2,<ref type="foot" target="#foot_3">5</ref> which constitutes a significant improvement although it relies on very simple ideas. We detail the proof and give a slightly different version, which will be useful in the following (similar ideas are used in Section 4.3, and this proof permits a straightforward extension of this theorem in Section 5).</p><p>Algorithm 1ayz-node-counting. Counts for all v the triangles in G containing v <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Input: the adjacency array representation of G, its adjacency matrix A, and an integer K Output: T such that T <ref type="bibr">[v]</ref> is the number of triangles in G containing v ).</p><p>Proof. Let us first show that Algorithm 1 (ayz-node-counting) solves node-counting (and thus counting and finding). Consider a triangle in G that contains a vertex with degree at most K ; then it is discovered in lines 2a and 2aa. Lines 2aaa to 2aad ensure that it is counted exactly once for each vertex it contains. Consider now the triangles in which all the vertices have degree larger than K . Each of them induces a triangle in G , and G contains no other triangle. These triangles are counted using the matrix product approach (lines 5, 6 and 6a), and finally all the triangles in G are counted for each vertex.</p><p>Let us now study the time complexity of Algorithm 1 (ayz-node-counting) in function of K . For each vertex v with d(v) ≤ K , one counts the number of triangles containing v in Θ(d(v) 2 ) ⊆ O(d(v)K ) thanks to the adjacency array representation of G. If we sum over all the vertices in the graph this leads to a time complexity in O(mK ) for lines 2 to 2aad. Now notice that there cannot be more than 2m In order to minimize this, one has to search for a value of K such that mK ∈ Θ(( m K ) ω ). This leads to K ∈ Θ(m ω-1 ω+1 ), which gives the announced time complexity.</p><p>The space complexity comes directly from the need of the adjacency matrix. All other space costs are lower; in particular, the graph G may contain 2m</p><formula xml:id="formula_3">K vertices, which leads to a Θ m K 2 = Θ m 2 1-ω-1 ω+1 = Θ m 4 ω+1</formula><p>⊂ O(m 1.185 ) space cost for A , A 2 and A 3 .</p><p>Note that one may also use sparse matrix product algorithms, see for instance <ref type="bibr" target="#b38">[39]</ref>. However, the matrix A 2 may not be sparse (in particular if there are vertices with large degrees, which is often the case in practice as discussed in Section 5). But algorithms may take benefit from the fact that one of the two matrices involved in a product is sparse, and there also exists algorithms for products of more than two sparse matrices. These approaches lead to situations where it depends on the precise relation between n and m which algorithm is the fastest. Discussing this further therefore is quite complex, and it is out of the scope of this paper.</p><p>In conclusion, despite the fact that the algorithms presented in this section are asymptotically very fast, they have two important limitations. First, they have a prohibitive space cost, since the matrices involved in the computation (in addition to the adjacency matrix, but it is considered as the encoding of G itself) may need Θ(n 2 ) space. Moreover, the fast matrix product algorithms are quite intricate, which leads to difficult implementations with high risks of errors. This also leads to large constant factors in the complexities, which have no importance at the asymptotic limit but may play a significant role in practice.</p><p>For these reasons, and despite the fact that they clearly are of prime theoretical importance, these algorithms have limited practical impact. Instead, one generally uses one of the listing algorithms (adapted accordingly) which we detail now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Time-optimal listing algorithms</head><p>First notice that there may be n 3 ∈ Θ(n 3 ) triangles in G. Likewise, there may be Θ(m Lemma 3 <ref type="bibr">([24,33,34]</ref>). Listing all triangles in G is in Ω(n 3 ) and Ω(m 3 2 ) time.</p><p>In this section, we first observe that the time complexity Θ(n 3 ) can easily be reached (Section 4.1). However, Θ(m</p><formula xml:id="formula_4">3 2 )</formula><p>is much better in the case of sparse graphs. We present more subtle algorithms that reach this bound (Section 4.2). Again, space complexity is a key issue, and we discuss this for each algorithm. We will see that algorithms proposed until now rely on the use of adjacency matrices and/or need Ω(m) space in addition to the graph encoding. We improve this by proposing algorithms with lower space needs, using only the adjacency array representation of G, and still in Θ(m 3 2 ) time (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Basic algorithms</head><p>One may trivially obtain a listing algorithm in Θ(n 3 ) (optimal) time with the matrix representation of G by testing in Θ(1) time any possible triple of vertices. Moreover, this algorithm needs only a constant (and very small) amount of space in addition to the graph representation.</p><p>Theorem 4 <ref type="bibr">([33,34]</ref> and Folklore). Given the adjacency matrix representation of G, it is possible to solve triangle listing in Θ(n 3 ) time and Θ(n 2 ) space using the direct testing of every triple of vertices.</p><p>This approach however has severe drawbacks. First, it needs the adjacency matrix of G. More importantly, its complexity does not depend on the actual properties of G; it always needs Θ(n 3 ) computation steps even if the graph contains very few edges. It must however be clear that, if almost all triples of vertices form a triangle, no better asymptotic bound can be attained, and the simplicity of this algorithm makes it very efficient in these cases.</p><p>In order to obtain faster algorithms on sparse graphs, while keeping the implementation very simple, one often uses the following algorithms. The first one, introduced in <ref type="bibr" target="#b23">[24]</ref> and called vertex-iterator in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, consists in iterating Algorithm 2 (vertex-listing) on each vertex of G. The second one, which seems to be the most widely used algorithm <ref type="foot" target="#foot_4">6</ref> , consists in iterating Algorithm 3 (edge-listing) over each edge in G. It was first introduced in <ref type="bibr" target="#b23">[24]</ref>, and discussed in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> where the authors call it edge-iterator.</p><p>Algorithm 2vertex-listing. Lists all the triangles containing a given vertex <ref type="bibr" target="#b23">[24]</ref>.</p><p>Input: the adjacency array representation of G, its adjacency matrix A, and a vertex v Output: all the triangles to which v belongs 1. for each pair {u, w} of neighbors of v:</p><p>1a. if A uw = 1 then output triangle {u, v, w} Algorithm 3edge-listing. Lists all the triangles containing a given edge <ref type="bibr" target="#b23">[24]</ref>.</p><p>Input: the sorted adjacency array representation of G, and an edge (u, v) of G Output: all the triangles in G containing (u, v)</p><formula xml:id="formula_5">1. for each w in N(u) ∩ N(v): 1a. output triangle {u, v, w}</formula><p>Theorem 5 <ref type="bibr">([24,33,34]</ref>). Given the adjacency array representation of G and its adjacency matrix, it is possible to list all its triangles in Θ v d(v) 2 , Θ(md max ), Θ(mn), and Θ(n 3 ) time and Θ(n 2 ) space; vertex-iterator achieves this. Proof. The fact that Algorithm 2 (vertex-listing) lists all the triangles to which a vertex v belongs is straightforward. Then, iterating over all vertices gives three times each triangle; if one wants each triangle only once it is sufficient to restrict the output of triangles to the ones for which η(w) &gt; η(v) &gt; η(u), for any injective numbering η() of the vertices.</p><p>Thanks to the adjacency array representation of G, the pairs of neighbors of v may be computed in Θ(d(v) 2 ) time and Θ(1) space (this would be impossible with the adjacency matrix only). Thanks to the adjacency matrix, the test in line 1a may be processed in Θ(1) time and space (this would be impossible with the adjacency array representation only). The time complexity of Algorithm 2 (vertex-listing) therefore is in Θ(d(v) 2 ) time and Θ(1) space. The Θ( v d(v) 2 ) time and Θ(n 2 ) space complexity of the overall algorithm follows. Moreover, we have 3 ), and all these complexity may be attained in the worst case (clique of n vertices), hence the results.</p><formula xml:id="formula_6">Θ( v d(v) 2 ) ⊆ O( v d(v)d max ) = O(md max ) ⊆ O(mn) ⊆ O(n</formula><p>Theorem 6 <ref type="bibr">([24,33,34]</ref> and Folklore). Given the sorted adjacency array representation of G, it is possible to list all its triangles in Θ(md max ), Θ(mn) and Θ(n 3 ) time and Θ (2m + n) ⊆ Θ(m) space; The edge-iterator algorithm achieves this.</p><p>Proof. The correctness of the algorithm is immediate. One may proceed like in the proof of Theorem 5 to obtain each triangle only once.</p><formula xml:id="formula_7">Each edge (u, v) is treated in time Θ(d(u) + d(v)) (because N(u) and N(v) are sorted) and Θ(1) space. We have d(u) + d(v) ∈ Θ(d max ), therefore the overall time complexity is in O(md max ) ⊆ O(mn) ⊆ O(n 3</formula><p>). In the worst case (clique of n vertices) all these complexity are tight.</p><p>The space complexity is nothing but the one of the graph representation, as the algorithms needs only a constant additional space. First note 7 that these algorithms are optimal in the worst case, just like the direct method (Lemma 3 and Theorem 4). However, they are much more efficient on sparse graphs, in particular if the maximal degree is low <ref type="bibr" target="#b6">[7]</ref>, since they both are in Θ(md max ) time. If the maximal degree is a constant, vertex-iterator even is in Θ(n) time (like edge-iterator). Moreover, both algorithms only need a constant amount of space in addition to the graph encoding, which makes them very interesting from this perspective.</p><p>However, vertex-iterator has a severe drawback: it needs the adjacency matrix of G and the adjacency array representation. Instead, edge-iterator only needs the sorted adjacency array representation, which is often available in practice. <ref type="bibr" target="#b7">8</ref> Moreover, edge-iterator runs in Θ (2m + n) ⊆ Θ(m) space, which makes it very compact. Because of these two reasons, and because of its simplicity, it is widely used in practice.</p><p>The performance of these algorithms however are quite poor when the maximal degree is unbounded, and in particular if it grows like a power of n. They may even be asymptotically sub-optimal on sparse graphs and/or on graphs with some vertices of high degree, which often appear in practice (we discuss this further in Section 5). It is however possible to design time-optimal listing algorithms for sparse graphs, which we detail now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Time-optimal listing algorithms for sparse graphs</head><p>Several algorithms have been proposed that reach the Θ(m 3 2 ) bound of Lemma 3, and thus are time optimal on sparse graphs (note that this is also optimal for dense graphs, but we have seen in Section 4.1 much simpler algorithms for these cases). In 1978, an algorithm was proposed to find a triangle in Θ(m 3 2 ) time and Θ(n 2 ) space <ref type="bibr" target="#b23">[24]</ref>. Therefore, it is slower than the ones discussed in Section 3 for finding, but it may be extended to obtain a listing algorithm with the same complexity. We first present this below. Then, we detail two simpler solutions with this complexity, proposed recently in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. The first one consists in a simple extension of Algorithm 1 (ayz-node-counting); the other one, named forward, has the advantage of being very efficient in practice <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Moreoever, it has a much lower space complexity. In addition, we will slightly modify it in Section 4.3 to reach a Θ (2m + 2n) ⊆ Θ(m) space cost, which makes it very compact.</p><p>An approach based on covering trees <ref type="bibr" target="#b23">[24]</ref> We use here the classical notions of covering trees and connected components, as defined for instance in <ref type="bibr" target="#b16">[17]</ref>. Since they are very classical, we do not recall them. We just note that a covering tree of each connected component of any graph may be computed in time linear in the number of edges of this graph, and space linear in its number of vertices (typically using a breadth-first search). One then has access to the father of any vertex in Θ(1) time and space.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, the authors propose a triangle finding algorithm in Θ(m</p><p>2 ) time and Θ(n 2 ) space. We present here a simple extension of this algorithm to solve triangle listing with the same complexity. To achieve this, we need the following lemma, which is a simple extension of Lemma 4 in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 7 ([24]). Let us consider a covering tree for each connected component of G, and a triangle t in G having an edge in one</head><p>of these trees. Then there exists an edge (u, v) in E but in none of these trees, such that t = {u, v, father(v)}.</p><p>Proof. Let t = {x, y, z} be a triangle in G, and let T be the tree that contains an edge of t. We can suppose without loss of generality that this edge is (x, y = father(x)). Two cases have to be considered. First, if (x, z) ∈ T then it is in none of the trees, and taking v = x and u = z satisfies the claim. Second, if (x, z) ∈ T then we have father(z) = x (because father(x) = y = z). Moreover, (y, z) ∈ T (else T would contain a cycle, namely t). Therefore taking v = z and u = y satisfies the claim.</p><p>This lemma shows that, given a covering tree of each connected component of G, one may find triangles by checking for each edge (u, v) that belongs to none of these trees if {u, v, father(v)} is a triangle. Then, all the triangles containing (v, father(v)) are discovered. This leads to Algorithm 4 (tree-listing), and to the following result (which is a direct extension of the one concerning triangle finding described in <ref type="bibr" target="#b23">[24]</ref>). <ref type="bibr" target="#b6">7</ref> We also note that another O(mn) time algorithm was proposed in <ref type="bibr" target="#b29">[30]</ref> for a more general problem. In the case of triangles, it does not improve vertexiterator and edge-iterator, which are much simpler, therefore we do not detail it here. 8 Recall that, if needed, one may sort the adjacency array representation of G in Θ(m log(n)) time and using only a constant amount of space in addition to the one needed for the graph representation.</p><p>Algorithm 4tree-listing. Lists all the triangles in a graph <ref type="bibr" target="#b23">[24]</ref>.</p><p>Input: the adjacency array representation of G, and its adjacency matrix A Output: all the triangles in G 1. while there remains an edge in E: 1a. compute a covering tree for each connected component of G 1b. for each edge (u, v) in none of these trees: 1ba. if (father(u), v) ∈ E then output triangle {u, v, father(u)} 1bb. else if (father(v), u) ∈ E then output triangle {u, v, father(v)} 1c. remove from E all the edges in these trees Theorem 8 <ref type="bibr">([24]</ref>). Given the adjacency array representation of G and its adjacency matrix, it is possible to list all its triangles in</p><formula xml:id="formula_9">Θ(m<label>3</label></formula><p>2 ) time and Θ(n 2 ) space; Algorithm 4 (tree-listing) achieves this.</p><p>Proof. Let us first prove that the algorithm is correct. It is clear that the algorithm may only output triangles. Suppose that one is missing. But all its edges have been removed when the computation stops, and so (at least) one of its edges was in a tree at some step. Let us consider the first such step (therefore the three edges of the triangle are present). Lemma 7 says that there exists an edge satisfying the condition tested in lines 1b and 1ba, and thus the triangle was discovered at this step. Finally, we reach a contradiction, and thus all triangles have been discovered. Now let us focus on the time complexity. Following <ref type="bibr" target="#b23">[24]</ref>, let c denote the number of connected components at the Finally, the space complexity comes from the need of the adjacency matrix in input.</p><formula xml:id="formula_10">current</formula><p>The performances of this algorithm rely on the fact that the graph is given both in its adjacency matrix representation and its adjacency array one. This reduces significantly the practical relevance of this approach concerning reduced space complexity. We will see in the next section algorithms that have the same time complexity but need much less space. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> The fastest known algorithm for finding, counting, and node-counting triangles, namely Algorithm 1 (ayz-node-counting), was proposed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> and we detailed it in Section 3. As proposed first in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, it is easy to modify it to obtain a listing algorithm, namely Algorithm 5 (ayz-listing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An extension of Algorithm 1 (ayz-node-counting)</head><p>Algorithm 5ayz-listing. Lists all the triangles in a graph <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Input: the adjacency array representation of G, its adjacency matrix A, and an integer K Output: all the triangles in G 1. for each vertex v with d(v) ≤ K :</p><p>1a. output all triangles containing v with Algorithm 2 (vertex-listing), without duplicates 2. let G be the subgraph of G induced by {v, d(v) &gt; K } 3. compute the sorted adjacency array representation of G 4. list all triangles in G using Algorithm 3 (edge-listing) Theorem 9 <ref type="bibr">([33,34,3,2]</ref>). Given the adjacency array representation of G and its adjacency matrix, it is possible to list all its triangles in Θ(m Proof. First recall that one may sort the adjacency array representation of G in O(m log(n)) time and Θ(1) space. This has no impact on the overall complexity of Algorithm 5 (ayz-listing), thus we suppose in this proof that the representation is sorted.</p><p>In a way similar to the proof of Theorem 2, let us first express the complexity of Algorithm 5 (ayz-listing) in terms of K . Using the Θ(d(v) 2 ) complexity of Algorithm 2 (vertex-listing) we obtain that lines 1 and 1a have a cost in</p><formula xml:id="formula_11">O( v,d(v)≤K d(v) 2 ) ⊆ O( v,d(v)≤K d(v)K ⊆ O(mK ) time and in Θ(1) space.</formula><p>Since we may suppose that the adjacency array representation of G is sorted, line 3 can be achieved in O(m) time. The number of vertices in G is in Θ( m K ) and it may be a clique, thus the space needed for G is in Θ(( m K ) 2 ).</p><p>Finally, the overall time complexity is in O mK + m m K . The optimum is attained with K in Θ( √ m), leading to the announced time complexity (which is tight from Lemma 3). The space needed for G then is Θ(( m K ) 2 ) = Θ(m), and thus the algorithm has Θ(n 2 ) space complexity due to the adjacency array needed in input.</p><p>Again, this result has a significant space cost: it needs the adjacency matrix of G, and, even then, it needs Θ(m) additional space. Moreover, it relies on the use of a parameter, K , which may be difficult to choose in practice: though Theorem 9 says that it must be in Θ( √ m), this makes little sense when one considers a given graph. We discuss further this issue in Section 6.</p><p>The forward fast algorithm <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> In <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, the authors propose another algorithm with optimal time complexity and a Θ(m) space cost (it needs the adjacency array representation of G only). We now present it in detail. We give a new proof of the correctness and complexity of this algorithm, in order to be able to extend it in the next sections (in particular in Section 5).</p><p>Algorithm 6forward. Lists all the triangles in a graph <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Input: the the adjacency array representation of G Output: all the triangles in G</p><p>1. number the vertices with an injective function η()</p><formula xml:id="formula_12">such that d(u) &gt; d(v) implies η(u) &lt; η(v)</formula><p>for all u and v 2. let A be an array of n arrays initially empty 3. for each vertex v taken in increasing order of η():</p><formula xml:id="formula_13">3a. for each u ∈ N(v) with η(u) &gt; η(v): 3aa. for each w in A[u] ∩ A[v]: output triangle {u, v, w} 3ab. add v to A[u]</formula><p>Theorem 10 ( <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>). Given the adjacency array representation of G, it is possible to list all its triangles in Θ(m</p><p>2 ) time and Θ (3m + 3n) ⊆ Θ(m) space; Algorithm 6 (forward) achieves this. Proof. For each vertex x, let us denote by A(x) the set {y ∈ N(x), η(y) &lt; η(x)}; this set contains only neighbors of x with degree larger than or equal to the one of x itself. For any triangle t = {a, b, c} one can suppose without loss of generality that η(c) &lt; η(b) &lt; η(a). One may then discover t by discovering that c is in</p><formula xml:id="formula_15">A(a) ∩ A(b).</formula><p>This is what the algorithm does. To show this, it suffices to show that</p><formula xml:id="formula_16">A[u] ∩ A[v] = A(u) ∩ A(v) when computed in line 3aa.</formula><p>First notice that when one enters in the main loop (line 3), the set A[v] contains all the vertices in A(v). Indeed, u was previously treated by the main loop since η(u) &lt; η(v), and, during this, lines 3 and 3ab ensure that it has been added to A[v] (just replace u by v and v by u in the pseudocode). Moreover, A[v] contains no other element, and thus it is exactly A(v)</p><p>when one enters the main loop.</p><p>When entering the main loop for v, A[u] is not equal to A(u) but it contains all the vertices w in A(u) such that η(w) &lt; η(v). Therefore, the intersections are equal:</p><formula xml:id="formula_17">A[u] ∩ A[v] = A(u) ∩ A(v),</formula><p>and thus the algorithm is correct.</p><p>Let us turn to complexity analysis. First notice that line 1 can be achieved in Θ(n log(n)) time and Θ (n) space. Now, note that lines 3 and 3a are nothing but a loop over all edges, thus in Θ(m). triangles.</p><p>The space complexity is obtained when one notices that each edge induces the storage of exactly one vertex (line 3ab), leading to a space requirement in Θ (3m + 3n): Θ (2m + n) for the graph representation plus Θ (m + n) for A and Θ (n) for η.</p><p>Compared to the two other time optimal algorithms we have presented, this algorithm is significantly more compact. Moreover, it is very simple and easy to implement, which also implies, as shown in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, that it is very efficient in practice. In addition, it does not have the drawback of depending on a parameter K , central in Algorithm 5 (ayz-listing). Finally, we show in the next sections that it may be slightly modified to reduce further its space complexity (Section 4.3), and that even better performances can be proved if one considers power-law graphs (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Time-optimal compact algorithms for sparse graphs</head><p>This section is devoted to time-optimal listing algorithms that have very low space requirements, both regarding the representation of G and the additional space needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A compact version of Algorithm 6 (forward)</head><p>Thanks to the proof we gave of Theorem 10, it is now easy to modify Algorithm 6 (forward) in order to improve significantly its space complexity. This leads to the following result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 7compact-forward. Lists all the triangles in a graph.</head><p>Input: the adjacency array representation of G Output: all the triangles in G 1. number the vertices with an injective function η() such that d(u) &gt; d(v) implies η(u) &lt; η(v) for all u and v 2. sort the adjacency array representation according to η() 3. for each vertex v taken in increasing order of η():</p><p>3a. for each u ∈ N(v) with η(u) &gt; η(v): 3aa. let u be the first neighbor of u, and v the one of v 3ab. while there remain untreated neighbors of u and v and η(u ) &lt; η(v) and η(v ) &lt; η(v): </p><formula xml:id="formula_18">3aba. if η(u ) &lt; η(v )</formula><p>2 ) time and Θ (2m+2n) ⊆ Θ(m) space; Algorithm 7 (compact-forward) achieves this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Recall that, as explained in the proof of Theorem 10, when one computes the intersection of A[v] and A[u] (line 3aa</head><p>of Algorithm 6 (forward)), A[v] is the set of neighbors of v with number lower than η(v), and A[u] is the set of neighbors of u with number lower than η(v). If the adjacency structures encoding the neighborhoods are sorted according to η(), we then have that A[v] is nothing but the beginning of N(v), truncated when we reach a vertex v with η(v ) &gt; η(v). Likewise, A[u] is N(u) truncated at u such that η(u ) &gt; η(v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 7 (compact-forward) uses this: lines 3ab to 3abcc are nothing but the computation of the intersection of A[v]</head><p>and A[u], which are supposed to be stored at the beginning of the adjacency structures, which is done in line 2. All this has no impact on the asymptotic time cost, and the A structure does not have to be explicitly stored.</p><p>Notice now that line 1 has a O(n log(n)) time and Θ (n) space cost. Moreover, sorting the simple compact representation of G (line 2) is in O(m log(n)) time and Θ(1) space. These time complexities play no role in the overall complexity, but the space complexities induce a Θ (n) additional space cost for the overall algorithm.</p><p>Also note that we do not need to store the whole adjacency arrays representing G in order to list the triangles using Algorithm 7 (compact-forward): if the adjacency array of each vertex v contains only its neighbors u such that η(u) &gt; η(v) then the algorithm still works. We obtain the following result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 12. If the input adjacency arrays representing G are already sorted according to η(), then it is possible to list all the triangles in G in</head><formula xml:id="formula_20">time Θ(m 3 2 ) and space Θ (m + n) ⊆ Θ(m).</formula><p>This last method is very compact (it does not even need to store the whole graph), and moreover all the preprocessing needed to reach these performances (computing the degree of each vertex, sorting them according to their degree, translating the adjacency arrays, and sorting them) can be done in Θ(m log(n)) time and Θ (2n) ⊆ Θ(n) space only. In cases where the available memory is too limited to store the whole graph, this makes this method very appealing.</p><p>In practice, these results mean that one could encode vertices by integers, with the property that this numbering goes from highest degree vertices to lowest ones, then store the graph in the adjacency array representation, sort it, and compute the triangles using Algorithm 7 (compact-forward). In such a framework, and using the trick pointed out in the corollary above, the algorithm runs in Θ (m) space, since line 1, responsible for the Θ (n) cost, is unnecessary. On the other hand, if one wants to keep the original numbering of vertices, then one has to store the function η() and renumber the vertices back after the triangle computation. This has an additional Θ (n) space cost (and no significant time cost).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A new algorithm</head><p>The algorithms discussed above basically rely on the fact that they avoid considering each pair of neighbors of high degree vertices, which would have a prohibitive cost. They do so by managing low degree vertices first, which has the consequence that most edges involved in the highest degrees have already been treated when the algorithm comes to these vertices. Here we take a quite different approach. First we design an algorithm able to efficiently list the triangles of high degree vertices. Then, we use it in an algorithm similar to Algorithm 5 (ayz-listing), but that both avoids adjacency matrix representation, and reaches a Θ(m) space cost.</p><p>First note that we already have an algorithm listing all the triangles containing a given vertex v, namely Algorithm 2 (vertex-listing) <ref type="bibr" target="#b23">[24]</ref>. This algorithm is in Θ(1) space (when the adjacency matrix is given), but it is inefficient on high degree vertices, since it needs Θ(d(v) 2 ) time. Our improved listing algorithm relies on an equivalent to Algorithm 2 (vertex-listing) that avoids this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 8new-vertex-listing. Lists all the triangles containing a given vertex.</head><p>Input: the adjacency array representation of G, and a vertex v Output: all the triangles to which v belongs Proof. One may see Algorithm 8 (new-vertex-listing) as a way to use the adjacency matrix of G without explicitly storing it: the array A is nothing but the v-th line of the adjacency-matrix. It is constructed in Θ ( n σ ) time and space (lines 1 and 2). 9 Then one can test for any edge (v, u) in Θ( <ref type="formula">1</ref>) time and space. The loop starting at line 3 takes any edge containing one neighbor u of v and tests if its other end (w in the algorithm) is linked to v using A, in Θ(1) time and space. This is sufficient to find all the triangles containing v. Since this number of edges is bounded by 2m (one may actually obtain an equivalent algorithm by replacing lines 3a and 3aa by a loop over all the edges), we obtain that the algorithm is in O(m) time and Θ ( n σ )</p><p>space.</p><p>The obtained time complexity is optimal since v may belong to Θ(m) triangles. Proof. Let us first study the complexity of the algorithm as a function of K . For each vertex v with d(v) &gt; K , one lists the number of triangles containing v in Θ(m) time and Θ ( n σ ) ⊆ Θ(n) space (Lemma 13) (the conditions in lines 1aa to 1ac, as well as the one in line 2aa, only serve to ensure that each triangle is listed exactly once). Then, one lists the triangles containing edges whose extremities are of degree at most K ; this is done by line 2aa in Θ(K ) time and Θ(1) space for each edge, thus a total in O(mK ) time and Θ(1) space. Finally, the space needs of the whole algorithm are independent of K and it is only in Θ ( n σ ) ⊆ Θ(n) in addition to the Θ (2m+n) ⊆ Θ(m) space representation of G. Its time complexity is in O( m K m+mK ) time, since there are O( m K ) vertices with degree larger than K . In order to minimize this, we take K in Θ( √ m), which leads to the announced time complexity.</p><p>Theorems 11 and 14 improve Theorems 9 and 10 since they show that the same (optimal) time-complexity may be achieved in significantly less space.</p><p>Note however that it is still unknown whether there exist algorithms with time complexity in Θ(m 3 2 ) but with lower space requirements. We saw that edge-iterator achieves Θ(md max ) ⊆ O(mn) time with only a constant space requirement in addition to the adjacency array, and thus in Θ (2m + n) ⊆ Θ(m) space. In this direction, one may use the adjacency arrays to obtain the following stronger (if 9 Recall that σ is the space needed to store an integer between 0 and n; here only one bit is necessary.</p><formula xml:id="formula_21">d max ∈ Ω( √ m log(n))) result.</formula><p>Corollary 15. Given the adjacency array representation of G, it is possible to list all its triangles in O(m</p><formula xml:id="formula_22">3 2 √ log(n)) time and Θ (2m + n) ⊆ Θ(m) space; Algorithm 9 (new-listing) achieves this if one takes K ∈ Θ( √ m log(n)).</formula><p>Proof. Let us first sort the arrays in O(m log(n)) time and Θ (1) space. Then, we change Algorithm 8 (new-vertex-listing) by removing the use of A and replace line 3aa by a dichotomic search for w in N(u), which has a cost in O(log(n)) time and Θ (1) space. Now if Algorithm 9 (new-listing) uses this modified version of Algorithm 8 (new-vertex-listing), then it is in Θ (1) space and O( m K m log(n) + mK ) time. The optimal value for K is then in Θ( √ m log(n)), leading to the announced complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The case of power-law graphs</head><p>Until now, we presented several results which take advantage of the fact that most large graphs met in practice are sparse; designing algorithms with complexities expressed in term of m rather than n then leads to significant improvements.</p><p>Going further, it has been observed since several years that most large graphs met in practice also have another important characteristic in common: their degrees are very heterogeneous. More precisely, in most cases, the vast majority of vertices have a very low degree while some have a huge degree. This is often captured by the fact that the degree distribution, i.e. the proportion p k for each k of vertices of degree k, is well fitted by a power-law: p k ∼ k -α for an exponent α generally between 2 and 3. See <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref> for extensive lists of cases in which this property was observed. <ref type="foot" target="#foot_5">10</ref>We will see that several algorithms proposed in previous section have provable better performances on such graphs than on general (sparse) graphs.</p><p>Let us first note that there are several ways to model real-world power-law distributions; see for instance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15]</ref>. We use here one of the most simple and classical ones, namely continuous power-laws; choosing one of the others would lead to similar results. In such a distribution, p k is taken to be equal to k+1 k Cx -α dx, where C is the normalization constant. 11 This ensures that p k is proportional to k -α in the limit where k is large. We must moreover ensure that the sum of the p k is equal to 1:</p><formula xml:id="formula_23">∞ k=1 p k = ∞ 1 C x -α dx = C 1 α-1 = 1. We obtain C = α -1, and finally p k = 1 α-1 k+1 k x -α dx = k -α+1 -(k + 1) -α+1 .</formula><p>Finally, when we talk about power-law graphs in the following, we refer to graphs in which the proportion of vertices of</p><formula xml:id="formula_24">degree k is p k = k -α+1 -(k + 1) -α+1</formula><p>. Theorem 16. Given an adjacency array representation of a power-law graph G with exponent α, Algorithm 7 (compact-forward) and Algorithm 9 (new-listing) with K ∈ Θ(n 1 α ) list all its triangles in O(mn 1 α ) time and the same space complexities as above.</p><p>Proof. Let us denote by n K the number of vertices of degree larger than or equal to K . In a power-law graph with exponent α, this number is given by: n K n = ∞ k=K p k . We have</p><formula xml:id="formula_25">∞ k=K p k = 1 -K -1 k=1 p k = 1 -(1 -K -α+1 ) = K -α+1 . Therefore n K = nK -α+1 .</formula><p>Let us first prove the result concerning Algorithm 9 (new-listing). As already noticed in the proof of Theorem 14, its space complexity does not depend on K . Moreover, its time complexity is in O(n K m + mK ). The value of K that minimizes this is in Θ(n 1 α ), and the result for Algorithm 9 (new-listing) follows.</p><p>Let us now consider the case of Algorithm 7 (compact-forward). The space complexity was already proved for Theorem 11. The time complexity is the same as the one for Algorithm 6 (forward), and we use here the same notations as in the proof of Theorem 10. Recall that the vertices are numbered by decreasing order of their degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us study the complexity of the intersection computation (line 3aa in</head><formula xml:id="formula_26">Algorithm 6 (forward)). It is in Θ(|A[u]| + |A[v]|).</formula><p>Recall that, at this point of the algorithm, A[v] is nothing but the set of neighbors of v with number lower than the one of v </p><formula xml:id="formula_27">(d(v) + n d(v) ).</formula><p>Like above, let us compute the value K of d(v) such that these two bounds are equal. We obtain</p><formula xml:id="formula_28">K = n 1 α . Then, the computation of the intersection is in O(K + n K ) = O(n 1 α</formula><p>), and since the number of such computations is bounded by the number of edges (lines 3 and 3a of Algorithm 6 (forward)), we obtain the announced complexity.</p><p>Let us underline the fact that this results is not an average case complexity: it is indeed a worst case complexity guaranteed as long as one considers power-law graphs. This result improves significantly the known bounds, as soon as α is large enough, and even if m ∈ Θ(n). This holds in particular for typical cases met in practice, where α often is between 2 and 3 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1]</ref>. It may be seen as an explanation of the</p><formula xml:id="formula_29">k+ 1 2 k-1 2</formula><p>x -α dx. Choosing any of this kind of solutions has little impact on the obtained results, see <ref type="bibr" target="#b14">[15]</ref> and the proofs we present in this section.</p><p>fact that Algorithm 6 (forward) has very good performances on graphs with heterogeneous degree distributions, as shown experimentally in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>One may use the same kind of approach to prove better performances for Algorithm 1 (ayz-node-counting) and Algorithm 5 (ayz-listing) in the case of power-law graphs as follows.</p><p>Corollary 17. Given the adjacency array representation of a power-law graph G with exponent α and its adjacency matrix, it is possible to solve node-counting, counting and finding on G in O(n ωα+ω ωα-ω+2 ) time and Θ(n 2α+2 ωα-ω+2 ) space; Algorithm 1 (ayz-node- counting) achieves this if one takes K in Θ(n ω-1 ωα-ω+2 ).</p><p>Proof. With the same reasoning as the one in the proof of Theorem 2, one obtains that the algorithm runs in O(nK 2</p><formula xml:id="formula_30">+ (n K ) ω )</formula><p>where n K denotes the number of vertices of degree larger than K . As explained in the proof of Theorem 16, this is n K = nK -α+1 . Therefore, the best K is such that nK 2 is in Θ(n ω K ω(1-α) ). Finally, K must be in n 1-ω ω(1-α)-2 . One then obtains the announced time complexity. The space complexity is bounded by the space needed to construct the adjacency matrix between the vertices of degree at most K , thus it is (n K ) 2 , and the result follows.</p><p>If the degree distribution of G follows a power law with exponent α = 2.5 (typical for internet graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1]</ref>) then this result says that Algorithm 1 (ayz-node-counting) reaches a O(n 1.5 ) time and O(n 1.26 ) space complexity. If the exponent is larger, then the complexity is even better. Note that one may also obtain tighter bounds in terms of m and n, for instance using the fact that Algorithm 1 (ayz-node-counting) has running time in Θ(mK + (n K ) ω ) rather than Θ(nK 2 + (n K ) ω ) (see the proofs of Theorem 2 and Corollary 17). We do not detail this here because the obtained results are quite technical and follow immediately from the ones we detailed.</p><p>Corollary 18. Given the adjacency array representation of a power-law graph G with exponent α and its adjacency matrix, it is possible to list all its triangles in Θ(mn 1 α ) time and Θ(n 2 ) space; Algorithm 5 (ayz-listing) achieves this if one takes K in Θ(n 1 α ).</p><p>Proof. The time complexity of Algorithm 5 (ayz-listing) is in Θ(mK + mn K ). The K minimizing this is such that K ∈ Θ(n K ), which is the same condition as the one in the proof of Theorem 16; therefore we reach the same time complexity. The space complexity is bounded by the size of the adjacency matrix of G (with the same notation as in the proof of Theorem 9), i.e.</p><p>Θ((n K ) 2 ). It is bounded by the size of the adjacency of G itself, which leads to the announced complexity.</p><p>Notice that this result implies that, for some reasonable values of α (namely α &gt; 2) the space needed in addition to the graph representation is in o(n). This however is of theoretical interest only: it relies on the use of both the adjacency matrix and the adjacency array representation of G, which is unfeasible in practice for large graphs.</p><p>Finally, the results presented in this section show that one may use properties of most large graphs met in practice (here, their heterogeneous degree distribution), to improve results known on the general case (or on the sparse graph case).</p><p>We note however that we have no lower bound for the complexity of triangle listing with the assumption that the graph is a power-law one (which we had for general and sparse graphs); actually, we do not even have a proof of the fact that the given bound is tight for the presented algorithms. One may therefore prove that they have even better performance (or that the bound is tight), and algorithms faster than the ones presented here may exist (for power-law graphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental evaluation</head><p>In <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, the authors present a wide set of experiments on both real-world complex networks and some generated using various models, to evaluate experimentally the known algorithms. They focus on vertex-iterator, edge-iterator, Algorithm 6 (forward), and Algorithm 5 (ayz-listing), together with their counting and node-counting variants (they compute clustering coefficients). They also study variants of these algorithms using, for instance, hashtables and balanced trees. These variants have the same worst case asymptotic complexities but one may guess that they would run faster than the original algorithms, for several reasons we do not detail here. Matrix approaches are considered as too intricate to be used in practice.</p><p>The overall conclusion of their extensive experiments is that Algorithm 6 (forward) performs best on real-world (sparse and power-law) graphs: its asymptotic time is optimal and the constants involved in its implementation are very small. Variants, which need more subtle data structures, actually fail in performing better in most cases (because of the overhead induced by the management of these structures).</p><p>In order to integrate our contribution in this context and have a precise idea of the behavior of the discussed algorithms in practice, we also performed a wide set of experiments. We first discuss typical instances below, and detail a case for which previously known fast algorithms cannot be used because of space limitations.</p><p>We provide implementations at <ref type="bibr" target="#b25">[26]</ref> which make it easy for anyone to compare the algorithms on his/her own instances (and to use them in applications). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typical real-world cases</head><p>Table <ref type="table" target="#tab_3">1</ref> shows the performances of the main algorithms discussed above in a variety of real-world cases. We do not give a detailed description of these graphs, which would be of little interest here; the key points are that these graphs span well the variety of cases met in practice, and they all have heterogeneous degree distributions well fitted by power-laws. Likewise, we do not give a detailed description of the machine running the experiments (a typical high performance workstation with a Dual Core amd Opteron(tm) Processor 275 at 2.2 GHz and 8 gb of main memory), which would provide little useful information, if any; the running times are provided to help in comparing the different algorithms, not to predict the precise running time on a given computer.</p><p>Experimental results are in full accordance with theoretical predictions. Algorithm 3 (edge-listing) is very compact but may need much time to terminate, in particular in case of graphs with high-degree nodes. Algorithm 6 (forward) and Algorithm 7 (compact-forward) are very fast, but Algorithm 6 (forward) has a prohibitive space cost in some cases. Algorithm 7 (compact-forward) and Algorithm 9 (new-listing) are very compact (almost as much as Algorithm 3 (edge-listing), as predicted by the analysis), while being very fast. This makes them the most appealing solutions in practice.</p><p>One may notice that the computation times of all algorithms are strongly related to (though not a direct consequence of) the presence of high-degree nodes; this was a key point in all our formal analysis, which is confirmed by these experiments.</p><p>Note that Algorithm 9 (new-listing), just like Algorithm 1 (ayz-node-counting) and Algorithm 5 (ayz-listing), suffers from a serious drawback: it relies on the choice of a relevant value for K , the maximal degree above which vertices are considered as having a high degree. Though in theory this is not a problem, in practice it may be quite difficult to determine the best value for K , i.e. the one that minimizes the execution time. It depends both on the machine running the program and on the graph under concern.</p><p>We took here the best value, i.e. the one leading to the lowest execution time, in each case. We will discuss this in more details below. With this best value given, the time performances of Algorithm 9 (new-listing) are similar to, but lower than, the ones of Algorithm 6 (forward). Its space requirements are much lower, as predicted by Theorem 14. Likewise, Algorithm 9 (new-listing) speed is close to the one of Algorithm 7 (compact-forward) and it has slightly lower space requirements, in particular when the number of nodes is very large, as predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A case previously our of reach</head><p>It is important to note that the use of compact algorithms, namely Algorithm 7 (compact-forward) and Algorithm 9 (newlisting), makes it possible to manage graphs that were previously out of reach because of space requirements. To illustrate this, we detail now the experiment labelled WebGraph in Table 1 (last line), which previous algorithms were unable to manage in our 8 GigaBytes memory machine. This experiment also has the advantage of being representative of what we observed on a wide variety of instances.</p><p>The graph we consider here is a web graph provided by the WebGraph project <ref type="bibr" target="#b9">[10]</ref>. It contains all the web pages in the .uk domain discovered during a crawl conducted from the 11-th of july, 2005, at 00:51, to the 30-th at 10:56 using UbiCrawler <ref type="bibr" target="#b10">[11]</ref>. It has n = 39 459 925 vertices and m = 783 027 125 (undirected) edges, leading to more than 6 GB of memory usage if stored in (sorted) (uncompressed) adjacency arrays, each vertex being encoded in 4 bytes as an integer between 0 and n -1. Its degree distribution is plotted in Fig. <ref type="figure" target="#fig_7">1</ref>, showing that the degrees are very heterogeneous (its maximal degree is d max = 1 776 858) and reasonably well fitted by a power-law of exponent α = 2.5. It contains 304 529 576 triangles. Let us emphasise that Algorithm 6 (forward), as well as the ones based on adjacency matrices, are unable to manage this graph on our 8 GB memory machine. Instead, and despite the fact that it is quite slow, edge-iterator, with its Θ (2m + n) ⊆ Θ(m) space complexity, can handle this. It took approximately 41 h to solve node-counting on this graph with this algorithm on our machine.</p><p>Algorithm 7 (compact-forward) achieves much better results: it took approximately 20 min. Likewise, Algorithm 9 (newlisting) took around 45 min (depending on the value of K ). This is probably close to what Algorithm 6 (forward) would achieve if more main memory was available.</p><p>In order to use Algorithm 9 (new-listing) in such cases, one may evaluate the best K in a preprocessing step at running time (by measuring the time needed to perform the key steps of the algorithm for various K ). This can be done without changing the asymptotic complexity. However, there is a much simpler way to choose K , with negligible loss in performance, which we discuss now.</p><p>We plot in Fig. <ref type="figure" target="#fig_7">1</ref> (right) the running time of Algorithm 9 (new-listing) as a function of the number of vertices with degree larger than K , for varying values of K . Surprisingly enough, this plot shows clearly that the time performance increases drastically as soon as a few vertices are considered as high degree ones. This may be seen as a consequence of the fact that edge-iterator is very efficient when the maximal degree is bounded; managing high degree vertices efficiently with Algorithm 8 (new-vertex-listing) and then the low degree ones with edge-iterator therefore leads to good performances. In other words, the few high degree vertices (which may be observed on the degree distribution plotted in Fig. <ref type="figure" target="#fig_7">1</ref>) are responsible for the low performance of edge-iterator.</p><p>When K decreases, the number of vertices with degree larger than K increases, and the performances continue to be better and better for a while. They reach a minimal running time, and then the running time grows again. The other important point here is that this growth is very slow, and thus the performance of the algorithm remains close to its best for a wide range of values of K . This implies that, with any reasonable guess for K , the algorithm performs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this contribution, we gave a detailed survey of existing results on triangle problems, and we completed them in two directions. First, we gave the space complexity of each previously known algorithm. Second, we proposed new algorithms that achieve both optimal time complexity and low space needs. Taking space requirements into account is a key issue in this context, since this currently is the bottleneck for triangle problems when the considered graphs are very large. This is discussed on a practical case in Section 6, where we show that our compact algorithms make it possible to handle cases that were previously out of reach.</p><p>Another significant contribution of this paper is the analysis of algorithm performances on power-law graphs (Section 5), which model a wide variety of very large graphs met in practice. We were able to show that, on such graphs, several algorithms have better performance than in the general (sparse) case. Finally, the current state of the art concerning triangle problems, including our new results, may be summarized as follows:</p><p>• except the fact that node-counting may have a Θ(n) space overhead (depending on the underlying algorithm), there is no known difference in time and space complexities between finding, counting, and node-counting;</p><p>• the fastest known algorithms for these three problems rely on matrix product and are in O(n 2.376 ) or O(m 1.41 ) time and Θ(n 2 ) space ( Theorems 1 and 2); however, no lower bound better than the trivial Ω(m) one is known for the time complexity of these problems;</p><p>• the other known algorithms rely on solutions to the listing problem and have the same performances as on this problem; they are slower than matrix approaches but need less space;</p><p>• listing can be solved in Θ(n 3 ) or Θ(nm) (optimal in the general case) time and Θ(n 2 ) or Θ(m) (optimal) space ( Theorems 4-6) ; this can be achieved from the sorted adjacency array representation of the graph;</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>K</head><label></label><figDesc>vertices v with d(v) &gt; K . Line 4 constructs (in O m + ( m K ) 2 time, which plays no role in the global complexity) the adjacency matrix of the subgraph G of G induced by these vertices. Using fast matrix product, line 5 computes the number of triangles for each vertex in G in time O m K ω . Finally, we obtain the overall time complexity of the algorithm: O mK + m K ω .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 2 ) 3 ∈ Θ(m 3 2</head><label>233</label><figDesc>triangles, since G may be a clique of √ m vertices (thus containing √ m ) triangles). This gives the following lower bounds for the time complexity of any triangle listing algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 )</head><label>2</label><figDesc>step of the algorithm. The value of c increases during the computation, until it reaches c = n. Two cases have to be considered. First suppose that c ≤ n -√ m. During this step of the algorithm, nc ≥ n -(n -√ m) = √ m edges are removed. And thus there can be no more than m √ m = √ m such steps. Consider now the other case, c &gt; n -√ m. The maximal degree then is at most nc &lt; n -(n -√ m) = √ m, and, since the degree of each vertex (of non-null degree) decreases at each step, there can be no more than √ m such steps. Finally, the total number of steps is bounded by 2 √ m. Moreover, each step costs O(m) time: the test in line 1ba is in Θ(1) time thanks to the adjacency matrix, and line 1b finds the O(m) edges on which it is ran in O(m) time thanks to the father() relation which is in Θ(1) time. This leads to the O(m 3 time complexity, and, from Lemma 3, this bound is tight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 2 )</head><label>2</label><figDesc>time and Θ(n 2 ) space; Algorithm 5 (ayz-listing) achieves this if one takes K ∈ Θ( √ m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 2)</head><label>3</label><figDesc>Inside the loop, the expensive operation is the intersection computation. To obtain the claimed complexity, it suffices to show that both A[u] and A[v] contain O( √ m) vertices: since each structure A[x] is trivially sorted by construction, this is sufficient to ensure that the intersection computation is in O( √ m). For any vertex x, by definition of A(x) and η(), A(x) is included in the set of neighbors of x with degree at least d(x). Suppose x has ω( √ m) such neighbors: |A(x)| ∈ ω( √ m). But all these vertices have degree at least equal to the one of x, with d(x) ≥ |A(x)|, and thus they have all together ω(m) edges, which is impossible. Therefore one must have |A(x)| ∈ O( √ m), and since A[x] ⊆ A(x) this proves the O(m time complexity. This bound is tight since the graph may contain Θ(m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .Lemma 13 .</head><label>113</label><figDesc>create an array A of n booleans and set them to false 2. for each vertex u in N(v), set A[u] to true 3. for each vertex u in N(v): 3a. for each vertex w in N(u): 3aa. if A[w] then output {v, u, w} Given the adjacency array representation of G, it is possible to list all its triangles containing a given vertex v in Θ(m) (optimal) time and Θ ( n σ ) ⊆ Θ(n) space; Algorithm 8 (new-vertex-listing) achieves this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(and thus of degree at least equal to d(v)). Therefore, |A[v]| is bounded both by d(v) and the number of vertices of degree at least d(v), i.e.n d(v) . Likewise, |A[u]| is bounded by d(u) and by n d(v) , since A[u] is the set of neighbors of u with degree at least equal to d(v). Moreover, we have η(u) &gt; η(v) (line 3a of Algorithm 6 (forward)), and so |A[u]| ≤ d(u) ≤ d(v). Finally, both |A[u]| and |A[v]| are bounded by both d(v) and n d(v) , and the intersection computation is in O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left: the degree distribution of our graph. Right: the execution time (in min) as a function of the number of vertices considered as high degree ones.</figDesc><graphic coords="14,45.12,52.13,448.20,137.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>then set u to the next neighbor of u 3abb. else if η(u ) &gt; η(v ) then set v to the next neighbor of v</figDesc><table><row><cell>3abc. else:</cell></row><row><cell>3abca. output triangle {u, v, u }</cell></row></table><note><p>3abcb. set u to the next neighbor of u 3abcc. set v to the next neighbor of v Theorem 11. Given the adjacency array representation of G, it is possible to list all its triangles in Θ(m</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Performances of the main algorithms on typical real-world examplesFrom top to bottom: a cooccurrence graph (nodes are the words in a book, and a link indicates that the two words appear in a same sentence); Flickr group graph (the nodes are flickr groups and a link indicates that the two groups have at least one member in common); an actor graph (the nodes are movie actors found in imdb, with a link between two actors if they appear in a same movie); an exchange graph at ip level (nodes are ip addresses with a link between two addresses if the router on which the measurement is conducted routed a packet from one of them to the other); the Notre-Dame web graph (a graph in which nodes are the web pages at the university of Notre-Dame, with links between them); Flickr contact graph (nodes are flickr users, and there is a link between two users if one of them is a contact of the other); a phone call graph (nodes are phone numbers, with a link between two numbers if a call from one to the other occurred during the observation); a p2p exchange graph (nodes are the users of an eDonkey server, and two nodes are linked if they exchanged a file during the measurement); a web graph provided by the WebGraph project (pages in the .uk domain, with links between them,</figDesc><table><row><cell>Graph</cell><cell>n</cell><cell>m</cell><cell>d max</cell><cell>Time cost</cell><cell></cell><cell></cell><cell></cell><cell>Space cost</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>el</cell><cell>f</cell><cell>cf</cell><cell>nl</cell><cell>el</cell><cell>f</cell><cell>cf</cell><cell>nl</cell></row><row><cell>Cooccurrences</cell><cell>9 264</cell><cell>392 066</cell><cell>7 053</cell><cell>2 s</cell><cell>0.25 s</cell><cell>0.25 s</cell><cell>0.25 s</cell><cell>4 mb</cell><cell>6 mb</cell><cell>4 mb</cell><cell>4 mb</cell></row><row><cell>Flickr groups</cell><cell>75 121</cell><cell>88 650 430</cell><cell>43 720</cell><cell>2 h 26 min</cell><cell>31 min</cell><cell>31 min</cell><cell>31 min</cell><cell>700 mb</cell><cell>1100 mb</cell><cell>700 mb</cell><cell>700 mb</cell></row><row><cell>Actor</cell><cell>383 640</cell><cell>15 038 083</cell><cell>3 956</cell><cell>1 min 12 s</cell><cell>20 s</cell><cell>20 s</cell><cell>28 s</cell><cell>120 mb</cell><cell>180 mb</cell><cell>120 mb</cell><cell>120 mb</cell></row><row><cell>ip exchanges</cell><cell>467 273</cell><cell>1 744 214</cell><cell>81 756</cell><cell>6 s</cell><cell>1 s</cell><cell>1 s</cell><cell>1 s</cell><cell>16 mb</cell><cell>26 mb</cell><cell>17 mb</cell><cell>16 mb</cell></row><row><cell>Notre-Dame</cell><cell>701 654</cell><cell>1 935 518</cell><cell>5 331</cell><cell>1 s</cell><cell>0.5 s</cell><cell>0.5 s</cell><cell>0.5 s</cell><cell>18 mb</cell><cell>31 mb</cell><cell>21 mb</cell><cell>18 mb</cell></row><row><cell>Flickr contacts</cell><cell>1 920 914</cell><cell>10 097 185</cell><cell>30 167</cell><cell>68 s</cell><cell>14 s</cell><cell>14 s</cell><cell>15 s</cell><cell>85 mb</cell><cell>138 mb</cell><cell>92 mb</cell><cell>85 mb</cell></row><row><cell>Phone calls</cell><cell>2 527 730</cell><cell>6 340 925</cell><cell>1 230</cell><cell>3 s</cell><cell>2 s</cell><cell>2 s</cell><cell>3 s</cell><cell>59 mb</cell><cell>102 mb</cell><cell>68 mb</cell><cell>61 mb</cell></row><row><cell>p2p exchanges</cell><cell>6 235 399</cell><cell>159 870 973</cell><cell>15 420</cell><cell>29 min</cell><cell>7 min</cell><cell>7 min</cell><cell>10 min</cell><cell>1.3 gb</cell><cell>1.9 gb</cell><cell>1.3 gb</cell><cell>1.3 gb</cell></row><row><cell>Webgraph</cell><cell>39 459 925</cell><cell>783 027 125</cell><cell>1 776 858</cell><cell>41 h</cell><cell>-</cell><cell>20 min</cell><cell>45 min</cell><cell>6 gb</cell><cell>9.2 gb</cell><cell>6.2 gb</cell><cell>6.1 gb</cell></row></table><note><p>detailed below). For each graph, we give, from left to right: its number of nodes n, its number of links m, its maximal degree d max , the running times of Algorithm 3 (edge-listing) (el), Algorithm 6 (forward) (f), Algorithm 7 (compact-forward) (cf), and Algorithm 9 (new-listing) (nl) (with the best value for K , see below), and their respective space needs.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>i.e. we make no distinction between (u, v) and (v, u) in V × V .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Even better performance may be obtained using (compact) radix sorting, see for instance<ref type="bibr" target="#b19">[20]</ref>. The improvement however plays no significant role in our context, therefore we do not discuss this further.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The original results actually concern triangle finding but they can easily be extended to counting, node-counting and listing at no cost; we present such an extension in Section 4, Algorithm 4 (tree-listing).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Again, the original results concerned triangle finding, but may easily be extended to node-counting, see Algorithm 1 (ayz-node-counting), and listing, see Algorithm 5 (ayz-listing). This was first proposed in<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. These algorithms have also been generalized to longer cycles in<ref type="bibr" target="#b37">[38]</ref> but this is out of the scope of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>It is for instance implemented in the widely used complex network analysis software Pajek<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>Note that if α is a constant then m is in Θ(n). It may however depend on n, and should be denoted by α(n). In order to keep the notations simple, we do not use this notation, but one must keep this in mind.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I warmly thank Frédéric Aidouni, Michel Habib, Vincent Limouzy, Clémence Magnien, Thomas Schank and Pascal Pons for helpful comments and references. I also thank Paolo Boldi from the WebGraph project <ref type="bibr" target="#b9">[10]</ref>, who provided the data used in Section 6. This work was partly funded by the MetroSec (Metrology of the Internet for Security) <ref type="bibr" target="#b39">[40]</ref> and PERSI (Programme d'Étude des Réseaux Sociaux de l'Internet) <ref type="bibr" target="#b40">[41]</ref> projects.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• listing may also be solved in Θ(m 3 2 ) (optimal in the general and sparse cases) time and Θ(m) space ( Theorems 11 and 14), still from the adjacency array representation of the graph; this is much better for sparse graphs;</p><p>• if main memory is very limited, one may use Corollary 12 to solve triangle listing in Θ (m + n) ⊆ Θ(m), while keeping the optimal Θ(m</p><p>2 ) time complexity; using external memory, this may even be reduced to Θ (m) ⊆ Θ(m) main memory needs, as discussed at the end of Section 4.3;</p><p>• in the case of power-law graphs, it is possible to prove better complexities, leading to O(mn 1 α ) time and compact solutions (where α is the exponent of the power-law) (Theorem 16);</p><p>• in practice, it is possible to obtain very good performances (both concerning time and space needs) using Algorithm 7 (compact-forward) and Algorithm 9 (new-listing).</p><p>We detailed several other results, but they are weaker (they need the adjacency matrix of the graph in input and/or have higher complexities) than these ones. This contribution also opens several questions for further research, most of them related to the tradeoff between space and time efficiency. Let us cite for instance:</p><p>• can matrix approaches be modified in order to induce lower space complexity?</p><p>• is listing feasible in less space, while still in optimal time Θ(m 3 2 )?</p><p>• is it possible to design a listing algorithm with complexity o(mn 1 α ) time and o(m) space for power-law graphs with exponent α? what is the optimal time complexity in this case?</p><p>It is also important to notice that other approaches exist, based for instance on streaming algorithmics (avoiding to store the graph in main memory) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref> and/or approximate algorithms <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, and/or various methods to compress the graph <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. These approaches are very promising for graphs even larger than the ones considered here, in particular the ones that do not fit in main memory.</p><p>Another interesting approach would be to express the complexity of triangle algorithms in terms of the number of triangles in the graph (and of its size). Indeed, it may be possible to achieve much better performance for listing algorithms if the graph contains few triangles. Likewise, it is reasonable to expect that triangle listing, but also node-counting and counting, may perform poorly if there are many triangles in the graph. The finding problem, on the contrary, may be easier on graphs having many triangles. To our knowledge, this direction has not yet been explored.</p><p>Finally, the results we present in Section 5 take advantage of the fact that most very large graphs considered in practice may be approximed by power-law graphs. It is not the first time that algorithms for triangle problems use underlying graph properties to get improved performance. For instance, results on planar graphs are provided in <ref type="bibr" target="#b23">[24]</ref>, and results using arboricity in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3]</ref>. It however appeared quite recently that many large graphs met in practice have some nontrivial (statistical) properties in common, and using these properties in the design of efficient algorithms still is at its very beginning. We consider this as a key direction for further research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Modern Phys</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">47</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding and counting given length cycles</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium Algorithms</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding and counting given length cycles</title>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reduction in streaming algorithms with an application of counting triangles in graphs</title>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Bar-Yossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/SIAM Symposium On Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Personnal communication</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pajek: A program for large network analysis</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Mrvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connections</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A subquadratic triad census algorithm for large sparse networks with small maximum degree</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Mrvar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="237" to="243" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The webgraph framework i: Compression techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The webgraph framework ii: Codes for the world-wide web</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>DCC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<ptr target="http://webgraph.dsi.unimi.it/" />
		<title level="m">WebGraph project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ubicrawler: A scalable fully distributed web crawler</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Codenotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Softw., Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="711" to="726" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mathematical results on scale-free random graphs, in: Handbook of Graphs and Networks: From the Genome to the Internet</title>
		<author>
			<persName><forename type="first">Béla</forename><surname>Bollobás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><surname>Riordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Wiley-VCH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Network Analysis: Methodological Foundations</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Brandes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Erlebach</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Arboricity and subgraph listing algorithms</title>
		<author>
			<persName><forename type="first">Norishige</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takao</forename><surname>Nishizeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reply to the comment on &apos;breakdown of the internet under intentional attack</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ben Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Havlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix multiplication via arithmetic progressions</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Symb. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="280" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Introduction to Algorithms, second edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comment on &apos;breakdown of the internet under intentional attack</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Dorogovtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F F</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural and algorithmic aspects of massive social networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Stephen Eubank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhav</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Radix sorting with no extra space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gianni Franceschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Pătraşcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Algorithms, ESA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix measures for transitivity and balance</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><forename type="middle">J</forename><surname>Kommel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Sociology</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward a general calculus of phonemic distribution</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">H</forename><surname>Paper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language: J. Linguistic Soc. America</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="143" to="169" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computing on data streams</title>
		<author>
			<persName><forename type="first">Monika</forename><surname>Rauch Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridar</forename><surname>Rajagopalan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>DEC Systems Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding a minimum circuit in a graph</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Itai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rodeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">New streaming algorithms for counting triangles in graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jowhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodsi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>COCOON</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Latapy</surname></persName>
		</author>
		<ptr target="http://www.liafa.jussieu.fr/~latapy/Triangles/" />
		<title level="m">Triangle computation web page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Complex network measurements: Estimating the relevance of observed properties</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Latapy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clémence</forename><surname>Magnien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Infocom&apos;08</title>
		<meeting>Infocom&apos;08<address><addrLine>Phoenix, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graphs over time: Densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network motifs: Simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to find long paths efficiently</title>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Monien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Discrete Math</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structure and time-evolution of an internet dating community</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Edling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liljeros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximating clustering coefficient and transitivity</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Graph Algorithms Appl. (JGAA)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="275" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Finding, counting and listing all triangles in large graphs</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Universität Karlsruhe, Fakultät für Informatik</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Finding, counting and listing all triangles in large graphs, an experimental study</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Experimental and Efficient Algorithms</title>
		<meeting><address><addrLine>WEA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Homomorphisms in graph property testing -a survey</title>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Colloquium on Computational Complexity</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>ECCC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collective dynamics of smallworld networks</title>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Network motifs in integrated cellular networks of transcription-regulation and protein-protein interaction</title>
		<author>
			<persName><forename type="first">Esti</forename><surname>Yeger-Lotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Sattath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">Y</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanah</forename><surname>Margalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5934" to="5939" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting short directed cycles using rectangular matrix multiplication and dynamic programming</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="254" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast sparse matrix multiplication</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Algorithms</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="604" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="http://www2.laas.fr/METROSEC/" />
		<title level="m">Metrosec project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="http://www.liafa.jussieu.fr/~persi/" />
		<title level="m">Persi project</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
