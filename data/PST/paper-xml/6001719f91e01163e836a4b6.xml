<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 STRUCTURED PREDICTION AS TRANSLATION BETWEEN AUGMENTED NATURAL LANGUAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan</orgName>
								<orgName type="institution" key="instit2">The relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 STRUCTURED PREDICTION AS TRANSLATION BETWEEN AUGMENTED NATURAL LANGUAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.</p><p>2. We apply our framework to (1) joint entity and relation extraction; (2) named entity recognition;</p><p>(3) relation classification; (4) semantic role labeling; (5) coreference resolution; (6) event extraction; (7) dialogue state tracking (Sections 4 and 5). In all cases we achieve at least comparable results to the current state-of-the-art, and we achieve new state-of-the-art performance on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012).</p><p>3. We also train a single model simultaneously on all tasks (multi-task learning), obtaining comparable or better results as compared with single-task models (Section 5.1).</p><p>4. We show that, thanks to the improved transfer of knowledge about label semantics, we can significantly improve the performance in the few-shot regime over previous approaches (Section 5.2).</p><p>5. We show that, while our model is purely generative (it outputs a sentence, not class labels), it can be evaluated discriminatively by using the output token likelihood as a proxy for the class score, resulting in more accurate predictions (Section 3 and Appendix A.3). Input: Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice teacher [</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Structured prediction refers to inference tasks where the output space consists of structured objects, for instance graphs representing entities and relations between them. In the context of natural language processing (NLP), structured prediction covers a wide range of problems such as entity and relation extraction, semantic role labeling, and coreference resolution. For example, given the input sentence "Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed" we might seek to extract the following graphs (respectively in a joint entity and relation extraction, and a coreference resolution task):</p><p>Figure <ref type="figure">1</ref>: Our TANL model translates between input and output text in augmented natural language, and the output is then decoded into structured objects.</p><p>example is handled within our framework, in the case of three different structured prediction tasks. The augmented languages are designed in a way that makes it easy to encode structured information (such as relevant entities) in the input, and to decode the output text into structured information.</p><p>We show that out-of-the-box transformer models can easily learn this augmented language translation task. In fact, we successfully apply our framework to a wide range of structured prediction problems, obtaining new state-of-the-art results on many datasets, and highly competitive results on all other datasets. We achieve this by using the same architecture and hyperparameters on all tasks, the only difference among tasks being the augmented natural language formats. This is in contrast with previous approaches that use task-specific discriminative models. The choice of the input and output format is crucial: by using annotations in a format that is as close as possible to natural language, we allow transfer of latent knowledge that the pre-trained model has about the task, improving performance especially in a low-data regime. Nested entities and an arbitrary number of relations are neatly handled by our models, while being typical sources of complications for previous approaches. We implement an alignment algorithm to robustly match the structural information extracted from the output sentence with the corresponding tokens in the input sentence.</p><p>We also leverage our framework to train a single model to solve all tasks at the same time, and show that it achieves comparable or better results with respect to training separately on each task. To the best of our knowledge, this is the first model to handle such a variety of structured prediction tasks without any additional task-specific modules.</p><p>To summarize, our key contributions are the following.</p><p>1. We introduce TANL, a framework to solve several structure prediction tasks in a unified way, with a common architecture and without the need for task-specific modules. We cast structured prediction tasks as translation tasks, by designing augmented natural languages that allow us to encode structured information as part of the input or output. Robust alignment ensures that extracted structure is matched with the correct parts of the original sentence (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many classical methods for structured prediction (SP) in NLP are generalizations of traditional classification algorithms and include, among others, Conditional Random Fields <ref type="bibr" target="#b24">(Lafferty et al., 2001)</ref>, Structured Perceptron <ref type="bibr" target="#b6">(Collins, 2002)</ref>, and Structured Support Vector Machines <ref type="bibr" target="#b55">(Tsochantaridis et al., 2004)</ref>. More recently, multiple efforts to integrate SP into deep learning methods have been proposed. Common approaches include placing an SP layer as the final layer of a neural net <ref type="bibr" target="#b7">(Collobert et al., 2011)</ref> and incorporating SP directly into DL models <ref type="bibr" target="#b9">(Dyer et al., 2015)</ref>.</p><p>Current state-of-the-art approaches for SP in NLP train a task-specific classifier on top of the features learned by a pre-trained language model, such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. In this line of work, BERT MRC <ref type="bibr" target="#b30">(Li et al., 2019a)</ref> performs NER using two classification modules to predict respectively the first and the last tokens corresponding to an entity for a given input sentence. For joint entity and relation extraction, SpERT <ref type="bibr" target="#b10">(Eberts &amp; Ulges, 2019)</ref> uses a similar approach to detect token spans corresponding to entities, followed by a relation classification module. In the case of coreference resolution, many approaches employ a higher-order coreference model <ref type="bibr" target="#b26">(Lee et al., 2018)</ref> which learns a probability distribution over all possible antecedent entity token spans.</p><p>Also related to this work are papers on sequence-to-sequence models for multi-task learning and SP. <ref type="bibr" target="#b45">Raffel et al. (2019)</ref> describe a framework to cast problems such as translation and summarization as text-to-text tasks in natural language, leveraging the transfer learning power of a transformer-based language model. Other sequence-to-sequence approaches solve specific structured prediction tasks by generating the desired output directly: see for example WDec <ref type="bibr" target="#b37">(Nayak &amp; Ng, 2020)</ref> for entity and relation extraction, and SimpleTOD <ref type="bibr" target="#b17">(Hosseini-Asl et al., 2020)</ref> and SOLOIST <ref type="bibr" target="#b41">(Peng et al., 2020)</ref> for dialogue state tracking. Closer to us, GSL <ref type="bibr" target="#b0">(Athiwaratkun et al., 2020)</ref>, which introduced the term augmented natural language, showed early applications of the generative approach in sequence labeling tasks such as slot labeling, intent classification, and named entity recognition without nested entities. In some cases (e.g., relation classification), our output format resembles that of a question answering task <ref type="bibr" target="#b36">(McCann et al., 2018)</ref>. This paradigm has recently proved to be effective for some structured prediction tasks, such as entity and relation extraction and coreference resolution <ref type="bibr" target="#b32">(Li et al., 2019c;</ref><ref type="bibr">Zhao et al., 2020;</ref><ref type="bibr" target="#b63">Wu et al., 2020)</ref>. Additional task-specific prior work is discussed in Appendix A.</p><p>Finally, TANL enables easy multi-task structured prediction (Section 5.1). Recent work has highlighted benefits of multi-task learning <ref type="bibr" target="#b4">(Changpinyo et al., 2018)</ref> and transfer learning <ref type="bibr" target="#b57">(Vu et al., 2020)</ref> in NLP, especially in low-resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We frame structured prediction tasks as text-to-text translation problems. Input and output follow specific augmented natural languages that are appropriate for a given task, as shown in Figure <ref type="figure">1</ref>. In this section, we describe the format design concept and the decoding procedure we use for inference.</p><p>Augmented natural languages. We use the joint entity and relation extraction task as our guiding example for augmented natural language formats. Given a sentence, this task aims to extract a set of entities (one or more consecutive tokens) and a set of relations between pairs of entities. Each predicted entity and relation has to be assigned to an entity or a relation type. In all the datasets considered, the relations are asymmetric; i.e., it is important which entity comes first in the relation (the head entity) and which comes second (the tail entity). Below is the augmented natural language designed for this task (also shown in Figure <ref type="figure">1</ref>): Specifically, the desired output replicates the input sentence and augments it with patterns that can be decoded into structured objects. For this task, each group consisting of an entity and possibly some relations is enclosed by the special tokens [ ]. A sequence of |-separated tags describes the entity type and a list of relations in the format "X = Y", where X is the relation type, and Y is another entity (the tail of the relation). Note that the objects of interest are all within the enclosed patterns "[ . . . | . . . ]". However, we replicate all words in the input sentence, as it helps reduce ambiguity when the sentence contains more than one occurrence of the same entity. It also improves learning, as shown by our ablation studies (Section 5.3 and Appendix B). In the target output sentence, entity and relation types are described in natural words (e.g. person, location) -not abbreviations such as PER, LOC -to take full advantage of the latent knowledge that a pre-trained model has about those words.</p><p>For certain tasks, additional information can be provided as part of the input, such as the span of relevant entities in semantic role labeling or coreference resolution (see Figure <ref type="figure">1</ref>). We detail the input/output formats for all structured prediction tasks in Section 4.</p><p>Nested entities and multiple relations. Nested patterns allow us to represent hierarchies of entities. In the following example from the ADE dataset, the entity "lithium toxicity" is of type disease, and has a sub-entity "lithium" of type drug. The entity "lithium toxicity" is involved in multiple relations: one of type effect with the entity "acyclovir", and another of type effect with the entity "lithium". In general, the relations in the output can occur in any order. Decoding structured objects. Once the model generates an output sentence in an augmented natural language format, we decode the sentence to obtain the predicted structured objects, as follows.</p><p>1. We remove all special tokens and extract entity types and relations, to produce a cleaned output.</p><p>If part of the generated sentence has an invalid format, that part is discarded. 2. We match the input sentence and the cleaned output sentence at the token levels using the dynamic programming (DP) based Needleman-Wunsch alignment algorithm <ref type="bibr" target="#b38">(Needleman &amp; Wunsch, 1970)</ref>. We then use this alignment to identify the tokens corresponding to entities in the original input sentence. This process improves the robustness against potentially imperfect generation by the model, as shown by our ablation studies (Section 5.3 and Appendix B). 3. For each relation proposed in the output, we search for the closest entity that exactly matches the predicted tail entity. If such an entity does not exist, the relation is discarded. 4. We discard entities or relations whose predicted type does not belong to the dataset-dependent list of types.</p><p>To better explain the DP alignment in step 2, consider the example below where the output contains a misspelled entity word, "Aciclovir" (instead of "acyclovir"). The cleaned output containing the word "Aciclovir", tokenized as "A-cicl-o-vir", is matched to "a-cycl-o-vir" in the input, from which we deduce that it refers to "acyclovir". Multi-task learning. Our method naturally allows us to train a single model on multiple datasets that can cover many structured prediction tasks. In this setting, we add the dataset name followed by the task separator : (for example, "ade :") as a prefix to each input sentence.</p><p>Categorical prediction tasks. For tasks such as relation prediction, where there is a limited number of valid outputs, an alternative way to perform classification is to compute class scores of all possible outputs and predict the class with the highest score. We demonstrate that we can use the output sequence likelihood as a proxy for such score. This method offers a more robust way to perform the evaluation in low resource scenarios where generation can be imperfect (see Appendix A.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STRUCTURED PREDICTION TASKS</head><p>Joint entity and relation extraction. Format and details for this task are provided in Section 3.</p><p>Named entity recognition (NER). This is an entity-only particular case of the previous task.</p><p>Relation classification. For this task, we are given an input sentence with head and tail entities and seek to classify the type of relation between them, choosing from a predefined set of relations. Since the head entity does not necessarily precede the tail entity in the input sentence, we add a phrase "The relationship between [ head ] and [ tail ] is" after the original input sentence. The output repeats this phrase, followed by the relation type. In the following example, the head and tail entities are "Carmen Melis" and "soprano" which have a voice type relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic role labeling (SRL).</head><p>Here we are given an input sentence along with a predicate, and seek to predict a list of arguments and their types. Every argument corresponds to a span of tokens that correlates with the predicate in a specific manner (e.g. subject, location, or time). The predicate is marked in the input, whereas arguments are marked in the output and are assigned an argument type. In the following example, "sold" is the predicate of interest. Event extraction. This task requires extracting (1) event triggers, each indicating the occurrence of a real-world event and (2) trigger arguments indicating the attributes associated with each trigger. In the following example, there are two event triggers, "attacked" of type attack and "injured" of type injury. We perform trigger detection using the same format as in NER, as shown below. To perform argument extraction, we consider a single trigger as input at a time. We mark the trigger (with its type) in the input, and we use an output format similar to joint entity and relation extraction. Below, we show an argument extraction example for the trigger "attacked", where two arguments need to be extracted, namely, "Two soldiers" of type target and "yesterday" of type attack time. Coreference resolution. This is the task of grouping individual text spans (mentions) referring to the same real-world entity. For each mention that is not the first occurrence of a group, we reference with the first mention. In the following example, "his" refers to "Barack Obama" and is marked as Dialogue state tracking (DST). Here we are given as input a history of dialogue turns, typically between a user (trying to accomplish a goal) and an agent (trying to help the user). The desired output is the dialogue state, consisting of a value for each key (or slot name) from a predefined list. In the input dialogue history, we add the prefixes "[ user ] :" and "[ agent ] :" to delineate user and agent turns, respectively. Our output format consists of a list of all slot names with their predicted values. We add "[ belief ]" delimiters to help the model know when to stop generating the output sequence. We tag slots that are not mentioned in the dialogue history with the value "not given" (we do not show them in the example below, for brevity). </p><formula xml:id="formula_0">[</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we show that our TANL framework, with the augmented natural languages outlined in Section 4, can effectively solve the structured prediction tasks considered and exceeds the previous state of the art on multiple datasets.</p><p>All our experiments start from a pre-trained T5-base model <ref type="bibr" target="#b45">(Raffel et al., 2019)</ref>. To keep our framework as simple as possible, hyperparameters are the same across all experiments, except for some  <ref type="bibr" target="#b35">(Luan et al., 2019)</ref> 88.4 63.2 MRC4ERE <ref type="bibr">(Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN <ref type="bibr" target="#b63">(Yuan et al., 2020)</ref> 84  <ref type="bibr" target="#b31">(Li et al., 2019b)</ref> 93.3 92.1 Cloze-CNN <ref type="bibr" target="#b1">(Baevski et al., 2019)</ref> 93.5 GSL <ref type="bibr" target="#b0">(Athiwaratkun et al., 2020)</ref> 90  <ref type="bibr" target="#b21">(Joshi et al., 2020)</ref> 85.3 78.1 75.3 79.6 BERT+c2r-coref <ref type="bibr" target="#b20">(Joshi et al., 2019)</ref> 81.4 83.5 71.7 75.3 68.8 71.9 73.9 76.9 CorefQA+SpanBERT <ref type="bibr" target="#b63">(Wu et al., 2020)</ref> 86  dataset-specific ones, such as the maximum sequence length. Details about the experimental setup, datasets, and baselines are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SINGLE-TASK AND MULTI-TASK EXPERIMENTS</head><p>We use three data settings in our experiments: (1) single dataset, (2) multiple datasets for the same task (multi-dataset), and (3) all datasets across all tasks (multi-task). Table <ref type="table" target="#tab_6">1</ref> shows the results.</p><p>With the single-task setup, we achieve state-of-the-art performance on the following datasets: ADE, NYT, and ACE2005 (joint entity and relation extraction), FewRel and TACRED (relation classification), CoNLL-2005 and CoNLL-2012 (semantic role labeling). For example, we obtain a +6.2 absolute improvement in F1 score on the NYT dataset over the previous state of the art. Interestingly, this result is higher than the performance of models that use ground-truth entities to perform relation extraction, such as REDN <ref type="bibr" target="#b28">(Li &amp; Tian, 2020)</ref>, which achieves a relation F1 score of 89.8. In coreference resolution, TANL performs similarly to previous approaches that employ a BERT-base model, except for CorefQA <ref type="bibr" target="#b63">(Wu et al., 2020)</ref>. To the best of our knowledge, ours is the first endto-end approach to coreference resolution not requiring a separate mention proposal module and not enforcing a maximum mention length.</p><p>For other datasets, we obtain a competitive performance within a few points of the best baselines. We highlight that our approach uses a single model architecture that can be trained to perform any of the tasks without model modification. This is in stark contrast with typical discriminative models, which tend to be task-specific, as can be seen from Table <ref type="table" target="#tab_6">1</ref>.</p><p>In fact, under this unified framework, a single model can be trained to perform multiple or all tasks at once, with the performance being on par or even better than the single-task setting. In particular, when the dataset sizes are small such as in ADE or CoNLL04, we obtain sizable improvements and become the new state of the art (from 80.6 to 83.7 for ADE relation F1, and from 89.4 to 90.6 for CoNLL04 entity F1). The only case where our multi-task model has notably lower scores is coreference resolution, where the input documents are much longer than in the other tasks. Since the maximum sequence length in the multi-task experiment (512 tokens) is smaller than in the singledataset coreference experiment (1,536 tokens for input and 2,048 for output), the input documents need to be split into smaller chunks, and this hurts the model's ability to connect multiple mentions of the same entity across different chunks. From the multi-task experiment, we leave out all datasets based on ACE2005 except for event extraction due to overlap between train and test splits for different tasks. We discuss our experiments in more detail in Appendix A.</p><p>All results presented in this paper are obtained from a pre-trained T5-base model. In principle, any pre-trained generative language model can be used, such as BART <ref type="bibr" target="#b27">(Lewis et al., 2020)</ref> or GPT-2 <ref type="bibr" target="#b44">(Radford et al., 2019)</ref>. It would be interesting to check whether these models are as capable as T5 (or even better) at learning to translate between our augmented languages. We leave this as a direction for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LOW-RESOURCE SETTINGS</head><p>Multiple experiments suggest that TANL is data-efficient compared to other baselines. On the FewRel dataset, a benchmark for few-shot relation classification, our model outperforms the best baselines BERT EM and BERT EM +MTB <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b53">Soares et al., 2019)</ref>, where the MTB version uses a large entity-linked text corpus for pre-training. On the TACRED relation classification dataset, our model also significantly improves upon the best baselines (from 71.5 to 88.4). While TACRED is not specifically a few-shot dataset, we observe that there are many label types that rarely appear in the training set, some of them having less than 30 appearances out of approximately 70,000 training label instances. We show the occurrence statistics for all label types in the appendix (Table <ref type="table" target="#tab_16">3</ref>), demonstrating that the dataset is highly imbalanced. Nonetheless, we find that our model performs well, even on instances involving scarce label types. This ability distinguishes our models from other few-shot approaches such as prototypical networks <ref type="bibr" target="#b52">(Snell et al., 2017)</ref> or matching networks <ref type="bibr" target="#b56">(Vinyals et al., 2016)</ref>, which are designed only for few-shot scenarios but do not scale well on real-world data which often contains a mix of high and low-resource label types.</p><p>Our low-resource study on the joint entity and relation extraction task also confirms that our approach is more data-efficient compared to other methods. We experiment on the CoNLL04 dataset, using only 0.8% (9 sentences) to 6% (72 sentences) of the training data. Our approach outperforms SpERT (a state-of-the-art discriminative model for joint entity and relation extraction) in this low-resource regime, whereas the performance is similar when using the full training set.</p><p>Thanks to the unified framework, we can easily train on a task, potentially with larger resources, and adapt to other low-resource end tasks (transfer learning). To show this, we train a model with a large dataset from joint entity and relation extraction (NYT) and fine-tune it on a limited portion of the CoNLL04 dataset (Figure <ref type="figure" target="#fig_1">2</ref>), obtaining a significant increase in performance (up to +9 relation F1).</p><p>Finally, in Appendix C we analyze how the size of the training dataset affects the number of generation errors of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ABLATION STUDIES</head><p>We conduct ablation studies to demonstrate that label semantics, augmented natural language format, and optimal alignment all contribute to the effectiveness of TANL (Figure <ref type="figure" target="#fig_1">2b</ref>). Further details on these ablation studies can be found in Appendix B.</p><p>Numeric labels: To prevent the model from understanding the task through label semantics, we use numeric labels. This substantially hurts the performance, especially in a low-resource setting where transfer learning is more important. Abridged output: Second, to determine the impact of the augmented natural language format outlined in Section 4, we experiment with a format which does not repeat the entire input sentence. We find that this abridged format consistently hurts model performance, especially in low-resource scenarios. In other tasks, we generally find that a more natural-looking format usually performs better (see Appendix A.3). No DP alignment: We use exact word matching instead of the dynamic programming alignment described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>We have demonstrated that our unified text-to-text approach to structured prediction can handle all the considered tasks within a simple framework and offers additional benefits in low-resource settings. Unlike discriminative models common in the literature, TANL is generative as it translates from an input to an output in augmented natural languages. These augmented languages are flexible and can be designed to handle a variety of tasks, some of which are complex and previously required sophisticated prediction modules. By streamlining all tasks to be compatible with a single model, multi-task learning becomes seamless and yields state-of-the-art performance for many tasks.</p><p>Generative models, and in particular sequence-to-sequence models, have been used successfully in many NLP problems such as machine translation, text summarization, etc. These tasks involve mappings from one natural language input to another natural language output. However, the use of sequence modeling for structured prediction has received little consideration. This is perhaps due to the perception that the generative approach is too unconstrained and that it would not be a robust way to generate a precise output format that corresponds to structured objects, or that it may add an unnecessary layer of complexity with respect to discriminative models. We demonstrate that this is quite the opposite. The generative approach can easily handle disparate tasks, even at the same time, by outputting specific structures appropriate for each task with little, if any, format error.</p><p>We note that one drawback of the current generative approach is that the time complexity for each token generation is O(L 2 ) where L is the sentence length. However, there have been recent advances in the attention mechanism that reduce the complexity to O(L log L) as in Reformer <ref type="bibr" target="#b23">(Kitaev et al., 2020)</ref>, or to O(L) as in Linformer <ref type="bibr" target="#b60">(Wang et al., 2020)</ref>. Incorporating these techniques in the future can significantly reduce computation time and allow us to tackle more complex tasks, as well as improve on datasets with long input sequences such as in coreference resolution.</p><p>Based on our findings, we believe that generative modeling is highly promising but has been an understudied topic in structured prediction. Our findings corroborate a recent trend where tasks typically treated with discriminative methods have been successfully solved using generative approaches <ref type="bibr" target="#b2">(Brown et al., 2020;</ref><ref type="bibr" target="#b18">Izacard &amp; Grave, 2020;</ref><ref type="bibr" target="#b49">Schick &amp; Schütze, 2020)</ref>. We hope our results will foster further research in the generative direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SETUP, DATASETS, AND BASELINES</head><p>In all experiments, we fine-tune a pre-trained T5-base model <ref type="bibr" target="#b45">(Raffel et al., 2019)</ref>, to exploit prior knowledge of the natural language. The family of T5 models was specially designed for downstream text-to-text tasks, making them suitable for our needs. The T5-base model has about 220 million parameters. For comparison, both encoder and decoder are similar in size to BERT-base <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. We use the implementation of HuggingFace's Transformers library <ref type="bibr" target="#b61">(Wolf et al., 2019)</ref>.</p><p>To keep our framework as simple as possible, hyperparameters are the same across the majority of our experiments. We use: 8 V100  <ref type="bibr">)</ref>. At inference time, we employ beam search with 8 beams, and we adjust the maximum sequence length depending on the length of the sentences in each dataset. Note that beam search is not an essential part of our framework, as we find that greedy decoding gives almost identical results.</p><p>In the rest of this section, we describe datasets and baselines for each structured prediction task, as well as additional insights on particular experiments. Results of all experiments are given in Table <ref type="table" target="#tab_6">1</ref>.</p><p>Unless otherwise specified, micro-F1 scores are reported. Most experiments are run more than once, as described below, and the average result is reported. Table <ref type="table" target="#tab_18">5</ref> shows input-output examples from different datasets.</p><p>For the multi-task experiment, we train for 50 epochs on 80 GPUs, with a batch size of 3 per GPU.</p><p>The maximum input/output sequence length is set to 512 for all tasks.</p><p>A.1 JOINT ENTITY-RELATION EXTRACTION Datasets. We experiment on the following datasets: CoNLL04 <ref type="bibr" target="#b47">(Roth &amp; Yih, 2004)</ref>, ADE <ref type="bibr" target="#b15">(Gurulingappa et al., 2012)</ref>, NYT <ref type="bibr" target="#b46">(Riedel et al., 2010)</ref>, and ACE2005 <ref type="bibr" target="#b59">(Walker et al., 2006)</ref>.</p><p>• The CoNLL04 dataset consists of sentences extracted from news articles, with four entity types (location, organization, person, other) and five relation types (work for, kill, organization based in, live in, located in). As in previous work, we use the training (922 sentences), validation (231 sentences), and test (288 sentences) split by <ref type="bibr" target="#b14">Gupta et al. (2016)</ref>. We train for 200 epochs and report our test results averaged over 10 runs.</p><p>• The ADE dataset consists of 4, 272 sentences extracted from medical reports, with two entity types (drug, disease) and a single relation type (effect). This dataset has sentences with nested entities. As in previous work, we conduct a 10-fold cross-validation and report the average macro-F1 results across all 10 splits (except for the multi-task experiment, which is carried out once and uses the first split of the ADE dataset). We train for 200 epochs.</p><p>• The NYT dataset <ref type="bibr" target="#b66">(Zeng et al., 2018)</ref> is based on the New York Times corpus and was automatically labeled with distant supervision by <ref type="bibr" target="#b46">Riedel et al. (2010)</ref>. We use the preprocessed version of <ref type="bibr" target="#b64">Yu et al. (2019)</ref>. This dataset has three entity types (location, organization, person) and 24 relation types (such as place of birth, nationality, company). It consists of 56,195 sentences for training, 5,000 for validation, and 5,000 for testing. We train for 50 epochs and report our test results averaged over 5 runs.</p><p>• The ACE2005 dataset is derived from the ACE2005 corpus <ref type="bibr" target="#b59">(Walker et al., 2006)</ref> and consists of sentences from a variety of domains, including news and online forums. We use the processing code of <ref type="bibr" target="#b35">Luan et al. (2019)</ref>   The natural labels we use for the relation types are: physical, artifact, employer, affiliation, social, part of. We train for 100 epochs and report our test results averaged over 10 runs.</p><p>For all single-dataset experiments, Table <ref type="table" target="#tab_12">2</ref> shows the number of training epochs, the number of runs, and the standard deviations, in addition to the average results, which are already reported in Table <ref type="table" target="#tab_6">1</ref>.</p><p>Baselines. SpERT (Eberts &amp; Ulges, 2019) is a BERT-based model which performs span classification and then relation classification. Multi-turn QA <ref type="bibr" target="#b32">(Li et al., 2019c)</ref> casts the problem as a multi-turn question answering task. ETL-Span <ref type="bibr" target="#b64">(Yu et al., 2019)</ref> uses BiLSTM and decomposes the problem into two tagging sub-problems: head entity extraction, and tail entity and relation extraction. WDec <ref type="bibr" target="#b37">(Nayak &amp; Ng, 2020</ref>) uses an encoder-decoder architecture to directly generate a list of relation tuples. MRC4ERE <ref type="bibr">(Zhao et al., 2020)</ref> improves on the question answering approach by leveraging a diverse set of questions. RSAN <ref type="bibr" target="#b63">(Yuan et al., 2020</ref>) is a sequence labeling approach which utilizes a relation-aware attention mechanism.</p><p>Low-resource experiments. As outlined in Section 5.2, we experiment on the CoNLL04 dataset with only a limited portion of the training set available and plot our results in Figure <ref type="figure" target="#fig_2">3</ref>. Comparison is made with SpERT (Eberts &amp; Ulges, 2019), a state-of-the-art discriminative model. TANL performs better than SpERT with fewer data, especially on the more complex task of relation extraction (right plot). We also show our method's performance with preliminary fine-tuning on the NYT dataset for one epoch, which significantly improves the performance on both entity and relation extraction. To account for the small dataset size, we fine-tune on CoNLL04 for 2,000 epochs (10× the number of epochs we use to train on the full CoNLL04 dataset). For a fair comparison, we train SpERT for 20, 200, and 2000 epochs (respectively 1×, 10×, and 100× the number of epochs suggested in the paper), and report the best result among the three, which is always obtained with 200 epochs. We plot mean and standard deviation over 10 runs (each model being fine-tuned on the same 10 subsets of the training set and evaluated on the entire test set). For reference, the smallest training set has only 9 sentences (0.8% of the total), effectively consisting in a few-shot learning scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-dataset experiments.</head><p>We train a single model on all four datasets for 20 epochs and report the average over 10 runs. We use a different split of the ADE dataset in each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 NAMED ENTITY RECOGNITION</head><p>Datasets. We experiment on two flat NER datasets, CoNLL03 <ref type="bibr" target="#b48">(Sang &amp; Meulder, 2003)</ref> and OntoNotes <ref type="bibr" target="#b43">(Pradhan et al., 2013)</ref>, and two nested NER datasets, GENIA <ref type="bibr" target="#b40">(Ohta et al., 2002)</ref> and ACE2005 <ref type="bibr" target="#b59">(Walker et al., 2006)</ref>.</p><p>• For the CoNLL03 dataset <ref type="bibr" target="#b48">(Sang &amp; Meulder, 2003)</ref> we use the same processing and splits as <ref type="bibr" target="#b30">Li et al. (2019a)</ref>, resulting in 14,041 sentences for training, 3,250 for validation, and 3,453 for testing. This dataset has four entity types (location, organization, person, miscellaneous). We train for 50 epochs and report our test results averaged over 10 runs.  <ref type="bibr" target="#b40">(Ohta et al., 2002)</ref> consists of sentences from the molecular biology domain. As in previous work, we use the processing and splits of Finkel &amp; <ref type="bibr" target="#b12">Manning (2009)</ref> resulting in 14,824 sentences for training, 1,855 for validation, and 1,854 for testing. There are five entity types (protein, DNA, RNA, cell line, cell type). We train for 50 epochs and report our test results averaged over 10 runs. • The ACE2005 dataset for nested NER is based on the ACE2005 corpus <ref type="bibr" target="#b59">(Walker et al., 2006)</ref>, but is different from the one used for joint entity-relation extraction. We use the same processing and splits of <ref type="bibr" target="#b30">Li et al. (2019a)</ref>, resulting in 7,299 sentences for training, 971 for validation, and 1,060 for testing. It has the same seven entity types as the ACE2005 dataset used for joint entity-relation extraction. We train for 50 epochs and report our test results averaged over 10 runs.</p><p>As for joint entity-relation extraction, Table <ref type="table" target="#tab_12">2</ref> summarizes our setup and results (with standard deviations) for the single-dataset experiments.</p><p>Baselines. State-of-the-art results on popular NER datasets are mostly detained by BERT-MRC <ref type="bibr" target="#b30">(Li et al., 2019a)</ref> and BERT-MRC + DSC <ref type="bibr" target="#b31">(Li et al., 2019b)</ref>, which formulate the problem as a machine reading comprehension task, by asking multiple questions. ClozeCNN <ref type="bibr" target="#b1">(Baevski et al., 2019)</ref> leverages a cloze-driven pre-training. Seq2seq-BERT <ref type="bibr" target="#b54">(Straková et al., 2019</ref>) uses a seq2seq model to output the list of entity types. Second-best learning and decoding <ref type="bibr" target="#b51">(Shibuya &amp; Hovy, 2019)</ref> iteratively decodes nested entities starting from the outermost ones, using the Viterbi algorithm. For flat NER, our approach is similar to GSL <ref type="bibr" target="#b0">(Athiwaratkun et al., 2020)</ref>.</p><p>Multi-dataset experiments. We train a single model on all four datasets for 10 epochs and report our results averaged over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 RELATION CLASSIFICATION</head><p>Datasets. We experiment on FewRel <ref type="bibr" target="#b16">(Han et al., 2018)</ref> and TACRED <ref type="bibr" target="#b67">(Zhang et al., 2017)</ref>.</p><p>• FewRel consists of 100 relations with 7 instances for each relation. The standard evaluation for this benchmark uses few-shot N -way K-shot settings, which we follow. The entire dataset is split into train (64 relations), validation (16 relations) and test set (20 relations). We train our model on the meta-training set, which has no overlapping classes with the evaluation set. At evaluation time, given a support set and a query set on a new task, we fine-tune the model on the support set to learn the new task and evaluate on the query set. Baselines. We compare our approach with the following two models in the literature. The first is BERT-pair <ref type="bibr" target="#b13">(Gao et al., 2019)</ref>, a sequence classification model based on BERT, which learns to optimize the scores indicating the relation between a query instance and other supporting instances for the same relation. The second is BERT EM + Matching the Blanks (MTB) <ref type="bibr" target="#b53">(Soares et al., 2019)</ref>. BERT EM uses entity markers indicating the start and the end of the head and tail entities in the input sentence. MTB is a pre-training based on an additional large corpus of relation data. Nevertheless, our model is able to outperform BERT EM +MTB in certain cases, such as the 5-way 1-shot setting on FewRel.</p><p>Augmented natural language formats. We experiment with many augmented natural language formats, as shown below: TACRED results and label sparsity. We obtain an F1 score of 88.39 on the TACRED dataset, exceeding the previous state of the art by +16.9. A major factor for such a big improvement is the shared semantics across different labels, which is particularly beneficial in the case of sparse labels.</p><formula xml:id="formula_1">Input (</formula><p>In Table <ref type="table" target="#tab_16">3</ref> we show the relation types in natural words, the number of training examples, which can be quite small, and the test recall (i.e., out of all ground truth relations for a given type, how many we predict correctly). We can see that even though some relation types such as date of birth have as little as 64 labels in the training set (less than 0.1% of the entire set), our model is able to correctly predict this relation type with recall 77.8%.</p><p>The ability to handle few-shot cases despite the label scarcity allows our approach to perform well in real-world data such as TACRED, where the labels can be highly imbalanced. As seen in Table <ref type="table" target="#tab_16">3</ref>, only a few instances such as employee of, top members employees, title, and no relation dominate the majority of the training set (approximately 60,000 out of 68,000), where the rest can be considered scarce. Our model is different from other approaches specifically designed for few-shot scenarios in that it scales across different levels of data.</p><p>Few-shot experiments. For the FewRel dataset, we perform meta-training by training the model on the training set of FewRel for 1 epoch. During evaluation, we fine-tune the model on the support set for each episode for 2,500 epochs in the 1-shot cases, and for 500 epochs in the 5-shot cases.</p><p>Likelihood-based prediction. In relation classification, we aim to predict one class out of a pre-defined set of classes, so we can perform prediction by using sequence likelihoods as class scores. This helps improve the performance particularly in the case of few-shot scenarios, where the generation of label types can be imperfect since the model has seen only one or few instances of each type. With the likelihood evaluation, we obtain a slight improvement across the board. For instance, we improve from an F1 score of 95.6 ± 4.8 to 96.4 ± 4.2 for the 5-way 5-shot case. All our reported numbers on the FewRel dataset are obtained by using this evaluation. For TACRED, using the likelihood approach does not yield an improvement, possibly due to the fact that the model can generate exact label types given enough training resources, unlike in the few-shot case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 SEMANTIC ROLE LABELING</head><p>Datasets. We use <ref type="bibr">CoNLL-2005</ref><ref type="bibr" target="#b3">(Carreras &amp; Màrquez, 2005)</ref> and the CoNLL-2012 English subset of OntoNotes 5.0 <ref type="bibr" target="#b43">(Pradhan et al., 2013)</ref> in our experiments. See also <ref type="bibr" target="#b3">Carreras &amp; Màrquez (2005)</ref>;  <ref type="formula">2012</ref>). These tasks have highly specific label types, and their natural words might be cumbersome for training. Therefore, we use the raw label types from the original datasets as presented below.</p><p>• CoNLL-2005 focuses on the semantic roles given verb predicates. The argument notation is the following. V: verb; A0: acceptor; A1: thing accepted; A2: accepted from; A3: attribute; AM-MOD: modal; AM-NEG: negation. • CoNLL-2012. The argument notation, taken from <ref type="bibr" target="#b42">Pradhan et al. (2012)</ref>, is as follows.</p><p>Numbered arguments (A0-A5, AA): Arguments defining verb-specific roles. Their semantics depends on the verb and the verb usage in a sentence, or verb sense. The most frequent roles are A0 and A1. Commonly, A0 stands for the agent, and A1 corresponds to the patient or theme of the proposition. However, no consistent generalization can be made across different verbs or different senses of the same verb. PropBank takes the definition of verb senses from VerbNet, and for each verb and each sense defines the set of possible roles for that verb usage, called the roleset. The definition of rolesets is provided in the PropBank Frames files, made available for the shared task as an official resource to develop systems. Adjuncts (AM-): General arguments that any verb may take optionally. The following are the 13 types of adjuncts. AM-ADV: general-purpose; AM-CAU: cause; AM-DIR: direction; AM-DIS: discourse marker; AM-EXT: extent; AM-LOC: location; AM-MNR: manner; AM-MOD: modal verb; AM-NEG: negation marker; AM-PNC: purpose; AM-PRD: predication; AM-REC: reciprocal; AM-TMP: temporal. References (R-): Arguments representing arguments realized in other parts of the sentence. The role of a reference is the same as the role of the referenced argument. The label is an R-tag prefixed to the label of the referent, e.g., R-A1.</p><p>Baselines. We compare our results with Dependency and Span SRL <ref type="bibr" target="#b33">(Li et al., 2019d)</ref>, which uses a Bi-LSTM with highway connection and biaffine scorers, and BERT-SRL <ref type="bibr" target="#b50">(Shi &amp; Lin, 2019)</ref>, BERT-based model which predicts the spans based on the contextual and positional embeddings.</p><p>Multi-dataset experiments. We train a single model on all datasets for 50 epochs and report our results averaged over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EVENT EXTRACTION</head><p>Datasets. We use the ACE2005 English event data <ref type="bibr" target="#b59">(Walker et al., 2006)</ref> in our experiments, following standard event extraction literature. We use the same split as previous work <ref type="bibr" target="#b19">(Ji &amp; Grishman, 2008;</ref><ref type="bibr" target="#b29">Li et al., 2013)</ref> with 529 documents for training, 30 for validation, and 40 for testing. Since the majority of event triggers and their corresponding arguments are within the same sentence, we perform the event extraction task only at the sentence level. We fine-tune our model for 50 epochs on this dataset.</p><p>Baselines. We compare our method with the following two baseline models in the literature. The first is J3EE (Nguyen &amp; Nguyen, 2019), a Bi-GRU based model that jointly performs event trigger detection, event mention detection, and event argument classification. J3EE performs event trigger detection and event mention detection as sequence tagging problems, and event argument classification as a classification problem, given any trigger and candidate argument pair. The second baseline is DyGIE++ <ref type="bibr" target="#b58">(Wadden et al., 2019)</ref>, a BERT based multi-task learning framework for the tasks of coreference resolution, relation extraction, named entity recognition, and event extraction. DyGIE++ enumerates all possible phrases within a sentence and predicts the best entity type and trigger type for each of these phrases. Argument roles are then predicted for each trigger and entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 COREFERENCE RESOLUTION</head><p>Datasets. We use the standard OntoNotes benchmark defined in the CoNLL-2012 shared task <ref type="bibr" target="#b42">(Pradhan et al., 2012)</ref>. It consists of 2,802 documents for training, 343 for validation, and 348 for testing, for a total of about one million words. Since documents can be large (up to 4,000 words), we split each document into (partially overlapping) chunks up to 1,024 words long (and 128 words for the multi-task experiment). At test time, we merge groups from different chunks if they have at least one mention in common in order to obtain document-level predictions. As in prior work, evaluation is done by computing the average F1 score of the three standard metrics for coreference resolution: MUC, B 3 , CEAF φ4 . We train for 100 epochs, with a maximum sequence length equal to 1,536 tokens for input and 2,048 for output, and a batch size of 1 per GPU.</p><p>Baselines. The e2e-coref model <ref type="bibr" target="#b25">(Lee et al., 2017)</ref> is among the first end-to-end approaches to coreference resolution. It considers all spans as potential mentions and learns a distribution over possible antecedents for each span. Higher-order c2f-coref <ref type="bibr" target="#b26">(Lee et al., 2018)</ref> iteratively refines span representations taking into account higher-order relations between mentions. BERT + c2f-coref <ref type="bibr" target="#b20">(Joshi et al., 2019)</ref> combines the previous approach with BERT. SpanBERT <ref type="bibr" target="#b21">(Joshi et al., 2020)</ref> introduces a new pretraining method which is designed to better represent and predict spans of text.</p><p>CorefQA <ref type="bibr" target="#b63">(Wu et al., 2020)</ref> generate queries for each mention from a mention proposal network and uses a question answering framework to extract text spans of coreferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 DIALOGUE STATE TRACKING</head><p>Datasets. We use the MultiWOZ 2.1 <ref type="bibr" target="#b11">(Eric et al., 2020)</ref> task oriented dialogue dataset in our experiments. It consists of 8,420 conversations for training, 1,000 for validation, and 999 for testing.</p><p>We follow the pre-processing procedure put forward in <ref type="bibr" target="#b62">(Wu et al., 2019)</ref> for dialogue state tracking.</p><p>In addition, we remove the "police" and "hospital" domains from the training set since they are not present in the test set. Removing these two domains reduces the training set size from 8,420 to 7,904. We fine-tune for 100 epochs, with maximum sequence length set to 512 tokens. We train a single generative model that predicts the dialogue state for the entire dialogue history up to the current turn. Following prior work, we report the joint accuracy.</p><p>Baselines. We compare our performance on MultiWOZ 2.1 against SimpleTOD <ref type="bibr" target="#b17">(Hosseini-Asl et al., 2020)</ref>, the current state of the art for MultiWOZ dialogue state tracking. SimpleTOD uses a sequence to sequence approach based on the GPT-2 <ref type="bibr" target="#b44">(Radford et al., 2019)</ref> language model. Unlike our approach, SimpleTOD is trained to jointly generate actions and responses as well as dialogue states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ABLATION STUDIES</head><p>As outlined in Section 5.3, we conduct ablation studies on the CoNLL04 dataset (joint entity and relation extraction) to demonstrate the importance of label semantics, natural output format, and optimal alignment. We compare TANL with the following three variations.</p><p>• Numeric labels: we use numbers (1, 2, 3, . . . ) to indicate entity and relation types in the output sentences, as in the following example. • Abridged output: here, the output consists of a list of entities, enclosed between [ ] tokens, without text between them. • No alignment: we process output sentences without the alignment module. For each predicted entity or relation, we look for the first exact match in the input sentence (the entity or relation is discarded if no exact match is found).</p><p>The outcomes of these experiments are shown in Figures <ref type="figure" target="#fig_5">2 and 4</ref>, and Table <ref type="table" target="#tab_17">4</ref>. We run all experiments using a variable amount of training data, from 100% (1,153 sentences) down to 0.8% (9 sentences), and always evaluate on the entire test set (288 sentences). To account for the variable size of the training dataset, we adjust the number of training epochs as follows: 200 epochs when using all training data; 400 epochs for 50% of the training data; 800 epochs for 25%; 1,600 epochs for 12.5%; 2,000 epochs for all remaining cases (6.3%, 3.1%, 1.6%, 0.8%).</p><p>Results show that all three components (label semantics, natural output format, and alignment) positively contribute to the effectiveness of TANL. The impact of label semantics is not noticeable when  using the full CoNLL04 training dataset (natural and numeric labels give similar F1 scores), but it becomes statistically relevant when using 50% of the training data, or less. On the other hand, the impact of alignment is higher when the training dataset is larger. Interestingly, for entity extraction (left plot of Figure <ref type="figure" target="#fig_5">4</ref>), repeating the input sentence is more important than using natural labels, whereas the opposite is true for relation extraction (right plot).</p><p>From these experiments, we deduce that: (1) the model indeed uses latent knowledge about label semantics, especially when the amount of training data is low; (2) using a "natural" output format (which replicates the input sentence as much as possible) allows the model to make more accurate predictions, likely by encouraging the use of the entire input as context;</p><p>(3) alignment helps in locating the correct entity spans in the input sentence, and in correcting mistakes made by the model when replicating the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANALYSIS OF GENERATION ERRORS</head><p>The performance of TANL crucially depends on the quality of the generated output sentences. Figure <ref type="figure" target="#fig_6">5</ref> shows how often the following kinds of generation errors occur on the CoNLL04 dataset.</p><p>• Reconstruction errors: the output sentence does not exactly replicate the input sentence.</p><p>• Format errors: the augmented natural language format is invalid.</p><p>• Entity errors: there is at least one relation whose predicted tail entity does not match any predicted entity. • Label errors: there is at least one predicted entity or relation type that does not exactly match any of the dataset's possible types.</p><p>Reconstruction errors are by far the most common, but they are mitigated by our alignment step. When using the full CoNLL04 training dataset, other errors appear very infrequently; therefore, it is not necessary to add further post-processing steps to mitigate them. We perform this generation error analysis on the CoNLL04 because it is the smallest of the benchmarks we consider, and as a result, the generation errors on CoNLL04 are likely to be the most significant. Yet when training on only a limited portion of the training data, format, and entity errors do occur. In this low-resource setting, TANL would benefit from additional post-processing. We leave the investigation of such post-processing strategies aimed at low-resource scenarios for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Input Output</head><p>CoNLL04 Boston University's Michael D. Papagiannis said he believes the crater was created 100 million years ago when a 50-mile-wide meteorite slammed into the Earth. CD4 positive T cell lines CD4 positive T cell lines CD4 positive T cell lines CD4 positive T cell lines CD4 positive T cell lines CD4 positive T cell lines | cell line ] and [ purified purified purified purified purified purified purified purified purified purified purified purified purified purified purified purified purified [ T cells transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells | cell type</head><formula xml:id="formula_2">1 | protein ] ([ AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 | protein ]). ACE2005<label>(</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Input:</head><label></label><figDesc>Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed. Tolkien Tolkien | person ]'s epic novel [ The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings | book | author = Tolkien ] was published in 1954-1955, years after the book was completed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experiments on the CoNLL04 dataset. (a) Our model outperforms the previous state-of-the-art model SpERT, in low-resource scenarios. (b) Ablation studies where we remove label semantics (numeric labels), augmented natural language format (abridged output) or dynamic programming alignment (no DP alignment), and plot the score difference with the non-ablated TANL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Low-resource experiments on the CoNLL04 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Output: [ Boston University | 2 ]'s [ Michael D. Papagiannis | 3 | 1 = Boston University ] said he believes the crater was created [ 100 million years | 4 ] ago when a 50-mile-wide meteorite slammed into the [ Earth | 1 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Output: [</head><label>[</label><figDesc>Boston University | organization ] [ Michael D. Papagiannis | person | works for = Boston University ] [ 100 million years | other ] [ Earth | location ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation studies on CoNLL04, using different portions of the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Percentage of output sentences presenting different kinds of errors, when training with a variable portion of the CoNLL04 training dataset.</figDesc><graphic url="image-4.png" coords="25,177.30,128.11,257.40,128.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Six days after starting acyclovir she exhibited signs of lithium toxicity.</figDesc><table><row><cell>Output: Six days after starting [ acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir | drug ] she exhibited signs of [ [ lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium | drug ] toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity |</cell></row></table><note>Input: disease | effect = acyclovir | effect = lithium ].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Input: [ user ] : I am looking for a cheap place to stay [ agent ] : How long? [ user ] : Two Output: [ belief ] hotel price range cheap cheap cheap cheap cheap</figDesc><table><row><cell>cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap, hotel type hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel, duration two two two two two two two two two two two two two two two two two [ belief ]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Results on all tasks. All numbers indicate F1 scores except noted otherwise. Datasets marked with an asterisk (*) have nested entities.</figDesc><table><row><cell></cell><cell></cell><cell>CoNLL04</cell><cell>ADE*</cell><cell>NYT</cell><cell>ACE2005</cell></row><row><cell>Entity Relation Extr.</cell><cell>SpERT (Eberts &amp; Ulges, 2019) DyGIE</cell><cell cols="3">Entity Rel. Entity Rel. Entity Rel. Entity Rel. 88.9 71.5 89.3 78.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>GPUs with a batch size of 8 per GPU; the AdamW optimizer<ref type="bibr" target="#b22">(Kingma &amp; Ba, 2015;</ref><ref type="bibr" target="#b34">Loshchilov &amp; Hutter, 2019)</ref>; linear learning rate decay starting from 0.0005; maximum input/output sequence length equal to 256 tokens at training time (longer sequences are truncated), except for coreference resolution and dialogue state tracking (see below). The number of fine-tuning epochs is adjusted depending on the size of the dataset, as described later. With these settings, one fine-tuning step takes approximately 0.8 seconds. This translates into 15 seconds per epoch for the (relatively small) CoNLL04 dataset (joint entity-relation extraction) and 16 minutes per epoch for the (much larger) OntoNotes dataset (NER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 :</head><label>2</label><figDesc>Details about the single-dataset experiments in joint entity-relation extraction and named entity recognition.</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell cols="2"># Epochs</cell><cell></cell><cell># Runs</cell><cell>Results</cell></row><row><cell cols="4">Joint entity-relation extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity F1</cell><cell>Relation F1</cell></row><row><cell cols="2">CoNLL04</cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>10</cell><cell>89.4 ± 0.3</cell><cell>71.4 ± 1.1</cell></row><row><cell cols="2">ADE</cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>10</cell><cell>90.2 ± 0.7</cell><cell>80.6 ± 1.5</cell></row><row><cell cols="2">NYT</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>5</cell><cell>94.9 ± 0.1</cell><cell>90.8 ± 0.1</cell></row><row><cell cols="2">ACE2005</cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell>10</cell><cell>88.9 ± 0.1</cell><cell>63.7 ± 0.7</cell></row><row><cell cols="4">Named entity recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity F1</cell></row><row><cell cols="2">CoNLL03</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>91.7 ± 0.1</cell></row><row><cell cols="2">OntoNotes</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell>10</cell><cell>89.8 ± 0.1</cell></row><row><cell cols="2">GENIA</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>76.4 ± 0.4</cell></row><row><cell cols="2">ACE2005</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>84.9 ± 0.2</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell></row><row><cell>Entity F1</cell><cell>40 50 60</cell><cell></cell><cell></cell><cell>TANL TANL (with NYT)</cell><cell>Relation F1</cell><cell>20 30 10</cell><cell></cell><cell>TANL TANL (with NYT)</cell></row><row><cell></cell><cell>30</cell><cell>1%</cell><cell>2% Percentage of training data</cell><cell>4% SpERT</cell><cell></cell><cell>0</cell><cell>1%</cell><cell>2% Percentage of training data</cell><cell>4% SpERT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>• The English OntoNotes dataset (Pradhan et al., 2013) consists of 59,924 sentences for training, 8,528 for validation, and 8,262 for testing. It has 18 entity types (such as person, organization, date, percent). We train for 20 epochs and report our test results averaged over 10 runs. • The GENIA dataset</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>• TACRED is a large-scale relation classification dataset with 106,264 examples, covering 41 relation types.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>chosen): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano</figDesc><table><row><cell>soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice</cell></row><row><cell>teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan. The relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] is</cell></row><row><cell>Output (chosen) : relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] = voice type</cell></row><row><cell>Input (alternative 1): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice</cell></row><row><cell>teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan. The relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] is</cell></row><row><cell>Output (alternative 1): voice type</cell></row><row><cell>Input (alternative 2): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano | tail ] and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 3 :</head><label>3</label><figDesc>TACRED recall by relation type, with number of train and test examples.</figDesc><table><row><cell>Relation type</cell><cell># Train</cell><cell cols="2"># Test Test recall</cell></row><row><cell>country of death</cell><cell>7</cell><cell>9</cell><cell>33.3</cell></row><row><cell>dissolved</cell><cell>24</cell><cell>2</cell><cell>50.0</cell></row><row><cell>country of birth</cell><cell>29</cell><cell>5</cell><cell>20.0</cell></row><row><cell>state or province of birth</cell><cell>39</cell><cell>8</cell><cell>50.0</cell></row><row><cell>state or province of death</cell><cell>50</cell><cell>14</cell><cell>35.7</cell></row><row><cell>religion</cell><cell>54</cell><cell>47</cell><cell>55.3</cell></row><row><cell>date of birth</cell><cell>64</cell><cell>9</cell><cell>77.8</cell></row><row><cell>city of birth</cell><cell>66</cell><cell>5</cell><cell>40.0</cell></row><row><cell>charges</cell><cell>73</cell><cell>103</cell><cell>83.5</cell></row><row><cell>number of employees members</cell><cell>76</cell><cell>19</cell><cell>57.9</cell></row><row><cell>shareholders</cell><cell>77</cell><cell>13</cell><cell>7.7</cell></row><row><cell>city of death</cell><cell>82</cell><cell>28</cell><cell>32.1</cell></row><row><cell>founded</cell><cell>92</cell><cell>37</cell><cell>89.2</cell></row><row><cell>political religious affiliation</cell><cell>106</cell><cell>10</cell><cell>40.0</cell></row><row><cell>website</cell><cell>112</cell><cell>26</cell><cell>73.1</cell></row><row><cell>cause of death</cell><cell>118</cell><cell>52</cell><cell>38.5</cell></row><row><cell>member of</cell><cell>123</cell><cell>18</cell><cell>0.0</cell></row><row><cell>founded by</cell><cell>125</cell><cell>68</cell><cell>86.8</cell></row><row><cell>date of death</cell><cell>135</cell><cell>54</cell><cell>64.8</cell></row><row><cell>schools attended</cell><cell>150</cell><cell>30</cell><cell>46.7</cell></row><row><cell>siblings</cell><cell>166</cell><cell>55</cell><cell>63.6</cell></row><row><cell>members</cell><cell>171</cell><cell>31</cell><cell>35.5</cell></row><row><cell>other family</cell><cell>180</cell><cell>60</cell><cell>40.0</cell></row><row><cell>children</cell><cell>212</cell><cell>37</cell><cell>70.3</cell></row><row><cell>state or province of headquarters</cell><cell>230</cell><cell>51</cell><cell>82.4</cell></row><row><cell>spouse</cell><cell>259</cell><cell>66</cell><cell>66.7</cell></row><row><cell>subsidiaries</cell><cell>297</cell><cell>44</cell><cell>31.8</cell></row><row><cell>origin</cell><cell>326</cell><cell>132</cell><cell>57.6</cell></row><row><cell>state or provinces of residence</cell><cell>332</cell><cell>81</cell><cell>49.4</cell></row><row><cell>cities of residence</cell><cell>375</cell><cell>189</cell><cell>46.0</cell></row><row><cell>city of headquarters</cell><cell>383</cell><cell>82</cell><cell>72.0</cell></row><row><cell>age</cell><cell>391</cell><cell>200</cell><cell>95.0</cell></row><row><cell>parents</cell><cell>439</cell><cell>150</cell><cell>64.7</cell></row><row><cell>countries of residence</cell><cell>446</cell><cell>148</cell><cell>42.6</cell></row><row><cell>country of headquarters</cell><cell>469</cell><cell>108</cell><cell>48.1</cell></row><row><cell>alternate names</cell><cell>913</cell><cell>224</cell><cell>84.4</cell></row><row><cell>employee of</cell><cell>1,525</cell><cell>264</cell><cell>67.0</cell></row><row><cell>top members employees</cell><cell>1,891</cell><cell>346</cell><cell>77.7</cell></row><row><cell>title</cell><cell>2,444</cell><cell>500</cell><cell>84.6</cell></row><row><cell>no relation</cell><cell cols="2">55,113 12,184</cell><cell>93.9</cell></row><row><cell>Total</cell><cell cols="2">68,164 15,509</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the CoNLL04 dataset (using the full training set, and using only 50% of the training sentences). We report mean and standard deviation over 10 runs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CoNLL04</cell><cell>CoNLL04 (50%)</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>Entity F1</cell><cell cols="2">Relation F1</cell><cell>Entity F1</cell><cell>Relation F1</cell></row><row><cell></cell><cell>TANL</cell><cell></cell><cell cols="4">89.44 ± 0.30 71.44 ± 1.15 87.15 ± 1.08 68.30 ± 1.47</cell></row><row><cell></cell><cell cols="2">TANL (numeric labels)</cell><cell cols="4">89.13 ± 0.45 71.57 ± 0.89 86.59 ± 0.94 66.12 ± 1.31</cell></row><row><cell></cell><cell cols="2">TANL (abridged output)</cell><cell cols="4">88.42 ± 0.67 70.98 ± 1.12 86.11 ± 0.55 67.18 ± 1.18</cell></row><row><cell></cell><cell cols="2">TANL (no alignment)</cell><cell cols="4">87.88 ± 0.31 69.72 ± 1.31 85.56 ± 1.01 66.64 ± 1.54</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell></row><row><cell>Entity F1</cell><cell>50 60 70</cell><cell></cell><cell cols="2">TANL TANL (numeric labels)</cell><cell>Relation F1</cell><cell>20 30 40 50</cell><cell>TANL TANL (numeric labels)</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell cols="2">TANL (abridged output) TANL (no DP alignment)</cell><cell></cell><cell>10</cell><cell>TANL (abridged output) TANL (no DP alignment)</cell></row><row><cell></cell><cell>1%</cell><cell cols="2">3% Percentage of training data 10% 30%</cell><cell>100%</cell><cell></cell><cell>1%</cell><cell>3% Percentage of training data 10% 30%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 5 :</head><label>5</label><figDesc>Input-output examples for all structured prediction datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>] | cell type ] from normal individuals, we have demonstrated that [</figDesc><table><row><cell>native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins native envelope glycoproteins | protein ] of HIV, [ gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 gp 160 | protein ], can</cell></row><row><cell>induce activation of [ transcription factor, activated protein -1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>What's your new TV series coming up? Oh it's a new show for FX called Beef. Beef? Beef. You start shooting? Um I'm going to Shriport tomorrow. Shriport Louisiana. it it | your new TV series coming up ]'s a new show for FX called Beef. [ Beef? Beef? Beef? Beef? Beef? MultiWOZ [ user ] : am looking for a place to to stay that has cheap price range it should be in a type of hotel [ agent ] : okay, do you have a specific area you want to stay in? [ user ] : no, i just need to make sure it s cheap. oh, and i need parking</figDesc><table><row><cell>Dataset</cell><cell>Input</cell><cell>Output</cell></row><row><cell>CoNLL2005</cell><cell>Still, one federal appeals court has signaled</cell><cell></cell></row><row><cell>(SRL)</cell><cell>it's willing to entertain the notion, and the</cell><cell></cell></row><row><cell></cell><cell>lawyers have renewed their arguments in</cell><cell></cell></row><row><cell></cell><cell>Texas and eight other states where the defense</cell><cell></cell></row><row><cell></cell><cell>is [ permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted permitted ] under state law.</cell><cell></cell></row><row><cell></cell><cell>Chairman Jack Welch is seeking work-related</cell><cell>Chairman Jack Welch is seeking work-related documents of [</cell></row><row><cell>argument cl.)</cell><cell>documents of his estranged wife in his high-</cell><cell>his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife his estranged wife | individual | person = divorce ] in [ his his his his his his his his his his his his his his his his his |</cell></row><row><cell></cell><cell>stakes [ divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce divorce | divorce ] case.</cell><cell>individual | person = divorce ] high-stakes divorce case.</cell></row><row><cell>CoNLL2012 (coreference</cell><cell></cell><cell>What's [ [ your your your your your your your your your your your your your your your your your ] new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up new TV series coming up ]? Oh [ it it it it it it it it it it it it it it it Beef? Beef? Beef? Beef? Beef? Beef? Beef? Beef? Beef? Beef? Beef? Beef? |</cell></row><row><cell>res.)</cell><cell></cell><cell>it ] [ Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. Beef. | Beef? ] [ You You You You You You You You You You You You You You You You You | your ] start shooting? Um [ I I I I I I I I I I I I I I I I I | You ]'m</cell></row><row><cell></cell><cell></cell><cell>going to [ Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport Shriport ] tomorrow. [ Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. Shriport Louisiana. | Shriport ]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">voice teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis | head ] in Milan. Output (alternative 2): relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] = voice type The alternative 1 version has a shorted output which only produces the keyword such as voice type corresponding to the predicted relation. However, we find that it does not perform as well as the chosen format. We hypothesize that it is due to the rich semantics of the sentence "relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ]", and possibly softer gradient information on the longer sequence which improves training. The alternative 2 version annotates the head vs. tail information for the entities directly in the input, instead of using a phrase such as "relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ]" to specify that "Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen MelisCarmen Melis" is the head entity. However, this format also does not perform as well, possibly because the meaning of the words head and tail are not fully understood in this context. Overall, the chosen format sounds the most natural out of all options and is closer to natural language, which we use as our guiding principle to design our augmented natural language formats.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">on [ Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence ] would be held on 1 October 2017. The relationship between [ referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum ] and [ Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence ] is relationship between [ referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum ] and [ Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence Catalan independence ] = main subject</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>[ Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University Boston University | organization ]'s [ Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Michael D. Papagiannis Still, one federal appeals court has signaled it's willing to entertain the notion, and the lawyers have renewed their arguments in [ Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states Texas and eight other states | AM-LOC ] [ where where where where where where where under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law under state law | AM-LOC ]. ACE2005 (event</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] = voice type [ belief ] hotel area not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book day not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book people not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book stay not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel internet not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel name not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel parking yes yes yes yes yes yes</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL03</head><p>Charlton, 61, and his wife, Peggy, became citizens of Ireland when they formally received Irish passports from deputy Prime Minister Dick Spring who said the honour had been made in recognition of Charlton's achievements as the national soccer manager.</p><p>[  OntoNotes The eventual court decision could become a landmark in Dutch corporate law because the lawsuit ASKO plans to file would be the first to challenge the entire principle and practice of companies issuing voting preferred shares to management -controlled trusts to dilute voting power of common stockholders.   was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there since I was there | ARGM-TMP ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells | cell type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE2005 (event trigger id.)</head><p>Hoon said Saddam's regime was crumbling under the pressure of a huge air assault. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Augmented natural language for generative sequence labeling</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno>CoRR, abs/2009.13272</idno>
		<ptr target="http://arxiv.org/abs/2009.13272" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1539</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1539" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="5359" to="5368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2005 shared task: Semantic role labeling</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W05-0620/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
				<editor>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</editor>
		<meeting>the Ninth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-29">CoNLL 2005. June 29-30, 2005. 2005</date>
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-task learning for sequence tagging: An empirical study</title>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1251/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
				<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre</forename><surname>Isabelle</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">August 20-26, 2018. 2018</date>
			<biblScope unit="page" from="2965" to="2977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient long-distance relation extraction with dg-spanbert</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2004.03636</idno>
		<ptr target="https://arxiv.org/abs/2004.03636" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118694</idno>
		<ptr target="https://www.aclweb.org/anthology/W02-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07">2002. July 2002</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1033</idno>
		<ptr target="https://www.aclweb.org/anthology/P15-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno>CoRR, abs/1909.07755</idno>
		<ptr target="http://arxiv.org/abs/1909.07755" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MultiWOZ 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.lrec-1.53" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
				<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName><forename type="first">Jenny</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D09-1015/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08-07">2009, 6-7 August 2009. 2009</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fewrel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1649</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1649" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="6249" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C16-1239/" />
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
				<editor>
			<persName><forename type="first">Nicoletta</forename><surname>Calzolari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</editor>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 11-16, 2016. 2016</date>
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2012.04.008" />
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1514</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1514" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple language model for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00796</idno>
		<ptr target="https://arxiv.org/abs/2005.00796" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P08-1030" />
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
				<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-08: HLT</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1588" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="5802" to="5807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1853" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgNKkHtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
				<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>ISBN 1558607781</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1018</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarseto-fine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2108</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-2108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Downstream model design of pre-trained language model for relation extraction task</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<idno>CoRR, abs/2004.03786</idno>
		<ptr target="https://arxiv.org/abs/2004.03786" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P13-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1910.11476</idno>
		<ptr target="http://arxiv.org/abs/1910.11476" />
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dice loss for dataimbalanced NLP tasks</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02855</idno>
		<ptr target="http://arxiv.org/abs/1911.02855" />
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1129</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1129" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019c</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dependency or span, end-to-end uniform semantic role labeling</title>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016730</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016730" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019d</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1308</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1308" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1806.08730</idno>
		<ptr target="http://arxiv.org/abs/1806.08730" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6374" />
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">D</forename><surname>Saul B Needleman</surname></persName>
		</author>
		<author>
			<persName><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">One for all: Neural joint modeling of entities and events</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Huu Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016851</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016851" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01">January 27 -February 1, 2019. 2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="6851" to="6858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The genia corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Mima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
				<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">SOLOIST: few-shot task-oriented dialog with A single pre-trained auto-regressive model</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Shayandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>CoRR, abs/2005.05298</idno>
		<ptr target="https://arxiv.org/abs/2005.05298" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W12-4501/" />
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning -Proceedings of the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes, EMNLP-CoNLL</title>
				<editor>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><surname>Xue</surname></persName>
		</editor>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-13">2012. July 13, 2012. 2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W13-3516/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<editor>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-08">2013. August 8-9, 2013. 2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15939-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-15939-8_" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>José</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Balcázar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aristides</forename><surname>Bonchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michèle</forename><surname>Gionis</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sebag</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">September 20-24, 2010. 2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15939-8_10</idno>
		<ptr target="https://www.aclweb.org/anthology/W04-2401/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
				<editor>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><surname>Riloff</surname></persName>
		</editor>
		<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-06">2004. May 6-7, 2004. 2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Held in cooperation with HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W03-0419/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003</title>
				<editor>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</editor>
		<meeting>the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06-01">May 31 -June 1, 2003. 2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>CoRR, abs/2009.07118</idno>
		<ptr target="https://arxiv.org/abs/2009.07118" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Simple BERT models for relation extraction and semantic role labeling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1904.05255</idno>
		<ptr target="http://arxiv.org/abs/1904.05255" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Nested named entity recognition via second-best sequence learning and decoding</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>CoRR, abs/1909.02250</idno>
		<ptr target="http://arxiv.org/abs/1909.02250" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>CoRR, abs/1703.05175</idno>
		<ptr target="http://arxiv.org/abs/1703.05175" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1279</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1279" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.1145/1015330.1015341</idno>
		<ptr target="https://doi.org/10.1145/1015330.1015341" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Machine Learning, ICML &apos;04</title>
				<meeting>the Twenty-First International Conference on Machine Learning, ICML &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Exploring and predicting transferability across NLP tasks</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00770</idno>
		<ptr target="https://arxiv.org/abs/2005" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1585" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="5783" to="5788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2006.04768</idno>
		<ptr target="https://arxiv.org/abs/2006.04768" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>CoRR, abs/1910.03771</idno>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Corefqa: Coreference resolution as querybased span prediction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.622/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1909.04273</idno>
		<ptr target="http://arxiv.org/abs/1909.04273" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A relation-specific attention network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/561</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/561" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1047/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">July 15-20. 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1004</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Asking effective and diverse questions: A machine reading comprehension based framework for joint entity-relation extraction</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/546</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/546" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
