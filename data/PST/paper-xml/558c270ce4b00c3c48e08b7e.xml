<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Knowledge Transfer and Zero-Shot Learning in a Large-Scale Setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bernt Schiele MPI Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bernt Schiele MPI Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Knowledge Transfer and Zero-Shot Learning in a Large-Scale Setting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D6028A845074BE941365E48FC8CC722D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While knowledge transfer (KT) between object classes has been accepted as a promising route towards scalable recognition, most experimental KT studies are surprisingly limited in the number of object classes considered. To support claims of KT w.r.t. scalability we thus advocate to evaluate KT in a large-scale setting. To this end, we provide an extensive evaluation of three popular approaches to KT on a recently proposed large-scale data set, the ImageNet Large Scale Visual Recognition Competition 2010 data set. In a first setting they are directly compared to one-vs-all classification often neglected in KT papers and in a second setting we evaluate their ability to enable zero-shot learning. While none of the KT methods can improve over one-vs-all classification they prove valuable for zero-shot learning, especially hierarchical and direct similarity based KT. We also propose and describe several extensions of the evaluated approaches that are necessary for this large-scale study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inspired by the success of recent object class recognition on individual classes, the simultaneous recognition of many classes has become an active research area. Scaling recognition to larger numbers of classes poses challenges with respect to the expressiveness and learnability of object models as well as the need for increasing amounts of training data. Knowledge transfer between object classes has been advertised as a promising route towards scalable recognition, by efficiently re-using acquired knowledge in the context of newly posed, but related recognition tasks. While experimental studies connected to knowledge transfer have shown promising results they are often limited w.r.t. the size of employed data sets.</p><p>As a consequence, it remains unclear whether the benefits demonstrated in small-scale experiments considering only a few classes really take effect in large-scale settings. In fact, Deng et al. <ref type="bibr" target="#b5">[6]</ref> found that the relative performance of different recognition methods can change when increasing test database size by an order of magnitude. The major contribution of this paper is therefore to revisit three recently proposed knowledge transfer approaches and to evaluate them in a truly large-scale setting, effectively starting where previous evaluations have left off. We evaluate knowledge transfer on the recently proposed ImageNet data set <ref type="bibr" target="#b6">[7]</ref>, specifically, on the associated ImageNet Large Scale Visual Recognition Competition 2010 (ILSVRC10) subset <ref type="bibr" target="#b1">[2]</ref>. It consists of over 1.2 million images of 1000 object classes, providing a currently unparalleled test bed for vision algorithms in terms of both scale and diversity. Being based on WordNet <ref type="bibr" target="#b17">[18]</ref> synonym sets, ImageNet offers the additional advantage of providing a hierarchical organization of object classes according to hypernym/hyponym relations, lending itself to knowledge transfer using object class hierarchies.</p><p>Our experimental study follows three prominent directions in knowledge transfer, which have proven effective for comparatively small numbers of object classes. The first direction imposes a hierarchical structure on the space of object classes, according to the general-to-specific ordering defined by the data set <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>. The second direction is based on representing object classes relative to an inventory of generic visual attributes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, where classes are characterized by distinct patterns of attribute activations. The third direction is based on direct similarities to related classes effectively using the classifiers of most similar classes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. For all three directions we go far beyond previous studies in terms of data set size, and evaluate knowledge transfer in the context of both traditional multiclass classification and zero-shot recognition.</p><p>Our paper makes the following contributions: First, to the best of our knowledge, we are the first to provide an in-depth study of knowledge transfer in a truly large-scale setting. Second, we compare three different approaches to knowledge transfer: one based on an object class hierarchy, one based on attributes, and one based on direct similarity. Third, we contrast knowledge transfer with the traditional approach of one-versus-all classification <ref type="bibr" target="#b20">[21]</ref>, which is often neglected in previous knowledge transfer work. Fourth, we challenge fully unsupervised transfer in a zeroshot recognition task aiming to recognize 200 unseen test classes. Fifth, we propose technical modifications to several approaches making them applicable to large-scale data 1 .</p><p>Sec. 1 discusses related work. Sec. 2 introduces the different knowledge transfer approaches. Sec. 3 motivates our setup for the experiments in Sec. 4 and 5. Related work Knowledge transfer for object class recognition comes in different flavors, such as joint learning of multiple classes <ref type="bibr" target="#b27">[28]</ref> or transferring object class priors <ref type="bibr" target="#b8">[9]</ref>. Recently, three lines of research have gained particular popularity due to their potential scalability.</p><p>A first line of research exploits the hierarchical structure of the object class space imposed by a general-to-specific ordering, either based on an existing hierarchy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> or learned from visual features <ref type="bibr" target="#b11">[12]</ref>. Scalability is achieved by associating classifiers to each hierarchy node, allowing for classification in a divide-and-conquer fashion. Our hierarchical classification is closest to <ref type="bibr" target="#b6">[7]</ref>, combining classifier scores of distinct subgraphs to yield final classification scores. <ref type="bibr" target="#b5">[6]</ref> follows a different route by forming a weighted average of all classifiers in a hierarchy for classification. While the latter two approaches report multiclass classification results on (subsets of) the ImageNet data set, our study additionally considers zero-shot recognition.</p><p>A second line of research uses an intermediate layer of descriptive attributes to represent object classes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>, encoding high-level visual properties that can be shared among object classes, hence promoting scalability. Our attribute-based object class model is inspired by <ref type="bibr" target="#b13">[14]</ref>, and uses linguistic knowledge bases to determine both an attribute inventory and the associations between object classes and attributes fully automatically <ref type="bibr" target="#b21">[22]</ref>.</p><p>A third line of research uses direct similarities between object classes. <ref type="bibr" target="#b0">[1]</ref> encodes instances of previously unknown classes as collections of "familiar" classifier responses, i.e., similarities to known classes, and applying a nearest-neighbor scheme for classification. While most work based on similarity between classes [1, 10] require a few training samples for new classes, we employ our unsupervised approach <ref type="bibr" target="#b21">[22]</ref> where class similarities are mined automatically using semantic relatedness measures with linguistic knowledge bases like Wikipedia or web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Knowledge transfer approaches</head><p>In this paper we explore two distinct settings for knowledge transfer. In a first experiment (Sec. 4) we assume that training data is available for all classes. In this setting knowledge can be transferred (or shared) among all classes and thus may lead to better classification performance. This setting is called knowledge sharing in the following. In the second experiment we assume that training data is available for a subset of known classes and that no training data is available for the remaining unseen classes. This setting is called zero-shot recognition and described in Sec. 5. We have chosen these two distinct settings as they represent two extreme cases for knowledge transfer.</p><p>The following gives an overview of the different knowledge transfer approaches explored in our study. Sec. 2.4 then describes how semantic relatedness is used to enable unsupervised attribute-and direct similarity-based knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hierarchy-based knowledge transfer</head><p>We exploit the hierarchical structure of the ILSVRC10 to train two types of classifiers (see for a small sample subgraph Fig. <ref type="figure" target="#fig_0">1</ref>). We train classifiers for leaf nodes z l by using training images of that node as positive samples and all other images as negative samples. Additionally we train classifiers for inner nodes y i using all images associated to hyponyms of y i as positive and all images outside the subtree rooted at y i as negative examples. Fig. <ref type="figure" target="#fig_0">1</ref> shows an example, where a classifier for solanaceous vegetable uses French fries, mashed potato, bell pepper, pimento, and jalapeno images as positives as well as parsnip and turnip images as negative examples. We exclude the root and any trivial nodes (with only a single hyponym), as they do not provide additional information, resulting in a total of 370 inner node classifiers.</p><p>We distinguish three approaches. First, for scoring image x according to a leaf class z l , we average over all classifier scores s(y i |x) of hypernyms H z l of z l (for a bell pepper classifier we thus use the pepper and solanaceous vegetable classifiers), which we denote the inner WordNet nodes model:</p><formula xml:id="formula_0">s inn (z l |x) = yi∈Hz l s(y i |x) |H z l |<label>(1)</label></formula><p>Second, since this model is not capable to distinguish among leaf classes z l that share the same hypernyms, such as French fries and mashed potato, we also include leaf node classifiers in the all WordNet nodes model:</p><formula xml:id="formula_1">s all (z l |x) = s(z l |x) + yi∈Hz l s(y i |x) 1 + |H z l |<label>(2)</label></formula><p>The third approach is based on the hierarchical cost sensitive classifier proposed by <ref type="bibr" target="#b5">[6]</ref>. This formulation ties to optimize for the hierarchical error, defined in Sec. 3.2. To estimate the score of a certain class z l we use cost-weighted classifier probabilities of all leaf nodes Z l , cost sensitive to the cost c z l zi between nodes z i and z l which is equivalent to the hierarchical error:</p><formula xml:id="formula_2">s cost (z l |x) = - zi∈Z l c z l zi p(z i |x)<label>(3)</label></formula><p>The hierarchy-based model allows for a flexible combination of leaf and inner node classifiers. In the knowledge sharing case the inner and leaf node classifiers are trained on training data from all classes. In the zero-shot case only those leaf node classifiers can be trained for which training data is available and the inner node classifiers are trained on the known classes only. Fig. <ref type="figure" target="#fig_5">4</ref> gives an example for transferring knowledge using leaf, inner, and all WordNet nodes models accordingly for the zero-shot case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attribute-based knowledge transfer</head><p>We adopt the probabilistic direct attribute prediction model (DAP) introduced by Lampert et al. <ref type="bibr" target="#b13">[14]</ref>. The DAP represents object classes z l relative to an inventory of descriptive attributes a m , realized as probabilistic attribute classifiers p(a m |x). In the knowledge sharing case these are trained on all classes whereas in the zero-shot case these are trained on known classes only. Once trained, the attribute classifiers can be flexibly combined to recognize previously unseen classes in the zero-shot setting or to recognize known classes in the knowledge sharing case. The association between object classes z l and attributes a m (see Fig. <ref type="figure" target="#fig_1">2</ref> for an example) is controlled by a matrix of indicator variables a z l m . Assuming mutual independence of attributes and uniform priors p(a m ) = 0.5 yields the following probability estimate of class z l being present in image x <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_3">p attr (z l |x) ∝ M m=1 (2 * p(a m |x)) a z l m (4)</formula><p>For efficiency reasons, we propose the following nonprobabilistic sum formulation, which replaces calibrated attribute probabilities p(a m |x) by zero-boundary attribute decision scores s(a m |x):</p><formula xml:id="formula_4">s attr (z l |x) = M m=1 s(a m |x) a z l m M m=1 a z l m ,<label>(5)</label></formula><p>Although this formulation does not require calibrated probabilities, it does require normalized scores. We found empirically that a simple z-score is sufficient.</p><p>In order to validate the sum formulation, we compare its performance to the probabilistic formulation in Tab. 1 for both error measures (see Sec. 3.2 for details). The important observation is that the sum formulation outperforms the probabilistic formulation.We thus use the sum formulation in the following. Table <ref type="table">1</ref>: Evaluation of the probabilistic product model suggested by <ref type="bibr" target="#b13">[14]</ref> vs. our sum model, see Sec. 2.2. Error in %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Direct similarity-based knowledge transfer</head><p>Motivated by its superior classification performance <ref type="bibr" target="#b21">[22]</ref>, we also include a direct similarity based approach. This can be defined as a modification of the attribute-based model that represents object classes relative to a set of K semantically related reference classes z k , implemented by classifiers s(z k |x):</p><formula xml:id="formula_5">s dir (z l |x) = K k=1 s(z k |x) K ,<label>(6)</label></formula><p>Direct similarity is used only in zero-shot experiments as the most related known class in the knowledge sharing setting is always the class itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Semantic relatedness for attribute-and direct similarity-based approaches</head><p>The attribute-based approach relies on an association matrix between a set of attributes and the object classes. The ILSVRC10, however, is neither provided with a set of attributes nor with manual class-attribute associations. Therefore we rely on part attributes mined from WordNet to generate an inventory of attributes for all classes <ref type="bibr" target="#b21">[22]</ref>. In total we mine 811 part attributes. An alternative to mine attributes would be to use WordNet's synset definitions <ref type="bibr" target="#b23">[24]</ref>.</p><p>For these mined attributes we use semantic relatedness measures in connection with linguistic knowledge bases to automatically determine associations between the attributes and object classes. While in <ref type="bibr" target="#b21">[22]</ref> each class and attribute is associated with one term, the classes and attributes in this work refer to WordNet concepts, called synsets, which are represented by several terms. As the semantic relatedness measures are based on terms rather than semantic concepts we take the median over all possible term combinations for a specific association.</p><p>For mining class-attribute associations we choose the best performing measures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> which are applicable to large scale: (1) the explicit semantic analysis based on Wikipedia <ref type="bibr" target="#b25">[26]</ref>; (2) Yahoo Holynyms which is based on hitcounts and uses specific part queries such as "the wheel of the car"; (3) Yahoo Image which is based on image-search hitcounts; and (4) Yahoo Snippets which is based on web page summaries returned by the search engine. For the direct similarity based approach we replace Yahoo Holonyms with simple Yahoo Web queries as it is not applicable for direct similarity. For improved robustness of the attributes we also compute a class level fusion over all attributes.  <ref type="bibr" target="#b14">[15]</ref>, LLC: locality-constrained linear coding <ref type="bibr" target="#b29">[30]</ref>,</p><p>Fisher vector <ref type="bibr" target="#b18">[19]</ref>, SVC: Super-Vector Coding <ref type="bibr" target="#b30">[31]</ref>, Lbp: local binary patterns, SGD: stochastic gradient decent <ref type="bibr" target="#b2">[3]</ref>, ASGD: averaging SGD <ref type="bibr" target="#b15">[16]</ref>.</p><p>Robust associations for large scale. In contrast to prior work we have a significantly larger amount of potential classes associated to each attribute. To learn precise attribute classifiers we use only the most likely classes as positives and least likely as negatives, leaving out the potentially noisy middle part. For the attribute backrest in Fig. <ref type="figure" target="#fig_1">2</ref> we would thus use wheelchair and armchair as positives, bike and husky as negatives, and not use the classes shopping cart and passenger car which are uncertain in respect to the attribute backrest.</p><p>Parameter selection. For attribute-and direct similaritybased knowledge transfer, continuous semantic relatedness measures have to be discretized to yield binary associations between attributes and object classes and in between object classes, respectively, by thresholding. Since we found large performance differences depending on thresholding <ref type="bibr" target="#b22">[23]</ref>,</p><p>we determine threshold values on the validation set, and fix them for the rest of the experiments. In particular, for attribute-based knowledge transfer, we set the threshold such that, on average, 3% of all attributes are active for a given object class. For the direct similarity based approach, we set the threshold such that the K = 5 most related object class models are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head><p>Evaluating and comparing the different knowledge transfer approaches of Sec. 2 in a large scale setting requires careful design of the experimental setup. The following details and argues for our choices concerning data set, image representation, and learning methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The number of available datasets containing more than a few hundred object classes with sufficiently many images per class is still limited. Caltech256 <ref type="bibr" target="#b10">[11]</ref> is frequently used, however, it consists only of 256 classes and 30k images. NUS-WIDE <ref type="bibr" target="#b4">[5]</ref> is significantly larger with 270k images and over 5k unique tags but contains ground truth for only 81 categories. The tiny image data set <ref type="bibr" target="#b26">[27]</ref> (80 million images, loosely labeled with 75,062 WordNet nouns) provides a significantly larger number of images but is mostly restricted to 32x32 pixel images.</p><p>Recently, Deng et al. proposed ImageNet <ref type="bibr" target="#b6">[7]</ref> (3.2 million images of 5247 WordNet synonym sets) as a resource for truly large-scale experimentation. Based on this dataset the ImageNet Large Scale Visual Recognition Challenge 2010 (ILSVRC10, <ref type="bibr" target="#b1">[2]</ref>) has been introduced. We have chosen this subset for large-scale experiments as it is a well-defined subset of 1,000 object classes (1.2 million images, divided into distinct portions for training, validation, and test) for classification experiments, suggesting this benchmark to be the de-facto choice for large-scale experiments in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance measures</head><p>ILSVRC10 <ref type="bibr" target="#b1">[2]</ref> introduced and defined the following performance measures used throughout the paper. Performance is measured as the top-n error rate (the n most confident classification hypotheses are considered as potentially correct) and distinguishes two error measures. The first is a flat measure which equals 0 if the test class is predicted correctly within the n most confident hypotheses, and 1 otherwise. The second is a hierarchical measure, which equals the minimum height of the lowest common ancestors between true and hypothesized classes. As suggested in <ref type="bibr" target="#b1">[2]</ref> we report top-n errors for n = 5 and n = 1, which corresponds to 1-accuracy. In order to avoid fitting the test data, we use the provided validation set for preliminary experimentation and parameter selection (Fig. <ref type="figure" target="#fig_4">3a</ref> and 3b, Tab. 1 and 2). The final results (Sec. 4 and 5, Fig. <ref type="figure" target="#fig_4">3c</ref>, Tab. <ref type="figure" target="#fig_4">3</ref> and<ref type="figure" target="#fig_5">4</ref>) are obtained on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Image representation</head><p>In order to allow for a sufficient range of experiments on the ILSVRC10 dataset, we require an image representation that is both powerful enough to achieve good performance and reasonably sized to support efficient learning. We thus base our choice on the outcome of the ILSVRC10 competition, which we recapitulate in part in Tab. 2, and seek to find a compromise between performance and manageable runtimes.</p><p>We observe that the performance ranges from 80% top-5 error rate for a BoW Sift baseline (Tab. 2, first row) to an impressive performance of as low as 34% and 28% top-5 error of the best performing approaches (Tab. 2, last two rows). In an attempt to regulate the performance-runtime tradeoff, we explore different combinations of techniques used by the best performing approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> such as spatial pyramid matching (SPM <ref type="bibr" target="#b14">[15]</ref>), locality-constrained  linear coding (LCC <ref type="bibr" target="#b29">[30]</ref>), and the Fisher vector <ref type="bibr" target="#b18">[19]</ref> (we adapted the implementation of <ref type="bibr" target="#b12">[13]</ref>), in connection with the color sift variant rgSift <ref type="bibr" target="#b28">[29]</ref> (Tab. 2, rows 2 to 6).</p><p>As can be seen from Tab. 2 and Figure <ref type="figure" target="#fig_4">3b</ref> (blue dots) the performance increases monotonically with descriptor dimensionality. While the last two approaches perform best they use feature vectors of several 100k and over 1Mio dimensions, resulting in prohibitive runtimes for our purposes. For this paper, we opted for the Fisher vector and LLC+SPM representation as a sensible compromise between performance (38% top-5 error rate, Tab. 2, row 6) and runtime. For combining the two representations we simply average their scores. We fix this representation for all remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning method</head><p>Motivated by the potential of stochastic gradient-based optimization for rapid convergence, and in line with the two best performing ILSVRC10 approaches, we use linear SVM classifiers, trained using stochastic gradient descent (SGD) <ref type="bibr" target="#b2">[3]</ref>. Similar in spirit to averaging SGD (ASGD) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16]</ref>, we average the SVM's weight and bias. However, in contrast to <ref type="bibr" target="#b15">[16]</ref> we do not average after each step, but take the mean of the results after each epoch (one pass over the data). More specifically, we save the weight vector w i and bias b i after each epoch i (the data is randomly reordered before each epoch). While the score of the normal SGD after n epochs only depends on the weights and bias after the final epoch</p><formula xml:id="formula_6">f SGD (x) = w n , x + b n ,<label>(7)</label></formula><p>we compute the mean over all epochs in MeanSGD:</p><formula xml:id="formula_7">f M eanSGD (x) = n i=1 w i , x + b i n<label>(8)</label></formula><p>(where w, x is the scaler product of w an x).</p><p>As can be seen in Figure <ref type="figure" target="#fig_4">3a</ref>, using MeanSGD (solid lines) instead of SGD (dashed lines) significantly speeds up convergence and improves performance. We use hinge loss and fix, according to Figure <ref type="figure" target="#fig_4">3a</ref>, the step size λ to 10 -7 and the number of epochs n to 20 epochs.</p><p>In order to benefit from modern multi-core hardware, we further implemented a parallelized version of MeanSGD based on Bouttou's SGD <ref type="bibr" target="#b3">[4]</ref>, exploiting data parallelism. It requires about 20 hours (including file and network I/O) for training all 1,000 one-vs-all classifiers with 20 epochs using the 53,768 dimensional Fisher vector on a 32-core machine. The code including a Matlab wrapper is available on our webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Large scale knowledge sharing</head><p>As motivated in Sec. 2, in a first set of experiments we consider the knowledge sharing case where we assume to have training samples for all classes.</p><p>Tab. 3 gives results for classifying all test images of the ILSVRC10 data set into 1000 classes, using the provided training set for training. Performance is measured in terms of the corresponding flat and hierarchical (in brackets) variants of top-5 and top-1 error (see Sec. 3.2). The table compares the performance of standard one-vs-all classification (part 1 of Tab. 3, using leaf node z l classifiers only), hierarchical models (part 2), and attribute-based models (part 3).</p><p>We proceed by examining Tab. 3 from top to bottom. First, we observe that the standard one-vs-all approach (Tab. 3 part 1) achieves a remarkable top-5 error rate of 37.6% with a hierarchical error rate of 2.91.</p><p>In contrast, the hierarchical model using only inner nodes (Tab. 3 part 2) performs relatively poorly (top-5 error of 71.3%, hierarchical error 7.31). This drop is understandable, considering the much smaller number of available inner node classifiers (370 compared to 1,000 leaf node classifiers). Adding the leaf nodes boosts the performance of the hierarchical model by more than 20% w.r.t. the flat top-5 error rate (50.4%, hierarchical error rate 5.49). Surprisingly, the resulting performance is still slightly worse than one-vs-all -the effect of adding confusion by adding more uniformly weighted classifiers is apparently more pronounced than added discriminative power. When examining the results more closely we find that the performance of the inner leaf node classifier does not correlate with the level of abstraction in the hierarchy. However, we find that it strongly depends on the semantic grouping, e.g. the category flower which is associated with 87 leaf nodes can be very well separated from other nodes in contrast to the class node described with the synset {fastener, fastening, holdfast, fixing}, which has 10 visually diverse and difficult child nodes such as button, hair slide, knot, and screw.</p><p>The hierarchical approach based on <ref type="bibr" target="#b5">[6]</ref> uses one-vs-all leaf nodes, but makes them sensitive to the hierarchical cost (see Sec. 2.1). With 48.6% top-5 error (Tab. 3 part 2) it clearly outperforms the hierarchical approach using only inner WordNet nodes (by 23%) and slightly all WordNet nodes (by 2%). However, compared to plain one-vs-all the flat top-5 error increases by 11% and even the hierarchical error by 1.8. The main reason for this less discriminant hierarchical classifier seems to be that this approach uses all classifiers but the one trained for the specific class to be detected.</p><p>The last line of Tab. 3 part 2 gives the results for a stacking-based combination of inner and leaf node classifiers. We use a SVM (MeanSGD) stacked on top of the scores of all nodes and both features to learn the relative importance of the nodes, i.e. we learn one-vs-all classifiers which use the classifier scores as feature vectors. In contrast to the the previous hierarchical approaches the trained SVM now correctly attenuates the influence of weak (inner) nodes and achieves a top-5 error of 36.8% which is even slightly better than one-vs-all.</p><p>Tab. 3 part 3 gives results for attribute-based models using different semantic relatedness measures for determining object class-attribute associations. On average, using single measures (Wikipedia, Yahoo Holonyms, Image, or Snippets) performs in the same order of magnitude as inner WordNet nodes. When combining all attribute-classifiers from the different measures we improve performance by more than 10% to 56.4% top-5 error (15% lower than inner WordNet nodes). However, this cannot compete with the hierarchical approaches including the discriminative leaf nodes.</p><p>In the same fashion as for all WordNet nodes we can also stack a SVM on top of the different attribute classifiers to learn an optimal weighting between them. This results in a significant reduction in error by 13% to 43.8% top-5 error, which is, however, still 6% higher than one-vs-all or 7% higher than the stacked hierarchical approach.</p><p>Influence of feature representation and amount of training data. In this experiment we further analyze the dependency with respect to the number of feature dimensions and the amount of available training data. In addition to one-vsall we pick the best approach for both knowledge transfer settings which is not based on one-vs-all leaf nodes: inner WordNet nodes for hierarchical setting and all attributes. In Figure <ref type="figure" target="#fig_4">3b</ref> we plot the error versus the feature dimensionality of the approaches listed in Tab. 2. We observe that for all approaches the performance increases logarithmically with increased feature dimension. From the SIFT representation (1,000 dimensional) to the combined LLC and Fisher vector (53,768 dimensional) the error decreases the most for one-vs-all by 34%, but still strongly by 29% for attributes and 21% for inner WordNet nodes. The relative performance difference between the approaches remains mainly stable across the different features representations which indicates that relative results of the approaches are independent of a specific feature representation.</p><p>In Figure <ref type="figure" target="#fig_4">3c</ref> we show results for a reduced amount of training data per class to 10, 25, and 100 samples. The first observation is that the hierarchical and the attribute-based knowledge transfer schemes degrade less (17% and 25%, respectively) than the one-vs-all (46%) scheme. However, the relative ordering remains the same for 100 and 25 samples per class. Only for the rather extreme case of only 10 training samples the attribute-based approach slighly outperforms one-vs-all classification by 1.7%.</p><p>Summary. We conclude that the benefit of knowledge transfer is in fact limited for this knowledge sharing and standard multiclass classification setting and becomes apparent only in the stacking-based approaches. In case of limited feature representation or reduced training data the absolute performance differences between the approaches decrease, but one-vs-all remains among the best. The hierarchical based approaches only show reasonable performance when leaf nodes are included. As concerns attributebased approaches, we observe that using all attributeclassifiers based on multiple semantic relatedness measures significantly improves performance. Results. Tab. 4 gives results for zero-shot recognition, comparing hierarchical (part 1), attribute-based (part 2), and direct similarity-based (part 3) models. In analogy to Tab. 3, the table further distinguishes among hierarchical models using leaf, inner, and all hierarchy nodes, as well as among different semantic relatedness measures for attribute-based and direct similarity-based models. As the relative ranking of the methods is nearly identical between the different error measures (top-5, top-1, flat and hierarchical error) we use the flat top-5 error as the basis for our discussion.</p><p>On average, we observe a significant amount of error across the compared approaches. We stress that this can be expected, since the zero-shot recognition task is of considerable difficulty, and cannot be solved without transferring knowledge between potentially unrelated object classes. to attribute-based models is consistent with our previous findings <ref type="bibr" target="#b21">[22]</ref>. It can be explained by both the limited quality of the automatically mined part attribute inventory and by having one vs. two potential sources of introducing label noise into the system by means of semantic relatedness (mined object class-attribute associations).</p><p>The strong performance of hierarchical models can be attributed to the increased amount of supervision given by the hierarchy, while the attribute-and direct similarity-based models are fully unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper explored knowledge transfer (KT) in a truly large-scale setting, going far beyond experimental studies of prior work in KT w.r.t. data set scale, diversity, and range of tested methods. Our evaluation is based on a recently proposed large-scale data set (ILSVRC10, <ref type="bibr" target="#b1">[2]</ref>) and includes three prominent approaches to knowledge transfer 1 .</p><p>For the fully supervised knowledge sharing experiment, the hierarchical approach using the inner or all node classifiers obtained inferior performance to the leaf nodes only, corresponding to the one-vs-all classifiers. Only when learning a stacked one-vs-all SVM on top, the hierarchical approach could slightly surpass performance of the one-vs-all classifiers. In the zero-shot recognition setting however, the hierarchical approaches obtained overall best performance of the explored KT methods.</p><p>The attribute based KT methods, in their fully unsupervised incarnation as explored in this paper, consistently produced higher error rates than the hierarchy and direct similarity-based KT methods. As pointed out before this reduced performance can be -at least partly -explained by the limited nature of attributes used here that were restricted to automatically mined part attributes. It remains an open research question how to obtain an inventory of representative and descriptive attributes for this kind of approach.</p><p>The direct similarity based KT method performed on a similar level as the hierarchical methods. This is remarkable as this approach is fully unsupervised using semantic relatedness to automatically find the most related known classes. This is in contrast to the hierarchical methods that require additional information given as a hierarchy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ISVLRC10 subgraph. Leaf (blue), inner nodes (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example part attributes (orange), object classes (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Convergence of SGD and MeanSGD for different step sizes λ on ILSVRC10 (one-vs-all, Fisher vector, rgSift).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance vs. (a) number of epochs, (b) feature dimensionality, and (c) number of training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Zero-shot recognition using hierarchies. Unseen object classes (red) mashed potato / jalapeno can be recognized using neighboring leaf node (French fries / bell pepper, pimento), inner node (potato / pepper), or all (the respective unions) classifiers.5. Large-scale zero-shot recognitionIn this section, we apply the knowledge transfer approaches of Sec. 2 to a zero-shot recognition setting, in which the sets of object classes of training and test are disjoint. We hence denote training object classes as known, and test classes as unseen. In order to solve the zero-shot recognition task, knowledge obviously has to be transferred between training and test classes. Lampert et al.<ref type="bibr" target="#b13">[14]</ref> provided a first benchmark for zero-shot recognition in the form of the Animals-with-Attributes (AwA) data set, consisting of approximately 30,000 images, divided into 40 known animal classes for training and 10 unseen animal classes for testing. In the present experimental study, we lift zero-shot recognition to another level both in terms of data set scale and diversity, by applying it to almost two orders of magnitude more images. In particular, we divide the ILSVRC10 data set randomly into two disjoint sets of object classes, one assumed known (800 classes), and one assumed unseen (200 classes)1 . In all experiments, we further maintain the original split into training and test data defined by the ILSVRC10 data set, meaning that we train on the known (800 class) fraction of the original training set (1,005,761 images), and test on the unseen (200 class) fraction of the original test set (30,000 images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>One-vs-all performance of different methods on ILSVRC10. BoW: bag of visual words, SPM: spatial pyramid matching</figDesc><table><row><cell></cell><cell></cell><cell>Learning</cell><cell>Total</cell><cell cols="2">Err. top</cell></row><row><cell>Model</cell><cell cols="2">Descriptor method</cell><cell>dim.</cell><cell>5</cell><cell>1</cell></row><row><cell>BoW [2]</cell><cell>Sift</cell><cell>LibLinear</cell><cell cols="3">1,000 80 91</cell></row><row><cell>BoW</cell><cell>Sift</cell><cell>MeanSGD</cell><cell cols="3">1,000 72 86</cell></row><row><cell>BoW + SPM</cell><cell>rgSift</cell><cell>MeanSGD</cell><cell cols="3">8,000 59 76</cell></row><row><cell>LLC + SPM</cell><cell>rgSift</cell><cell>MeanSGD</cell><cell cols="3">21,000 50 69</cell></row><row><cell>Fisher vector</cell><cell>rgSift</cell><cell>MeanSGD</cell><cell cols="3">32,768 43 61</cell></row><row><cell cols="2">LLC+SPM, Fisher rgSift</cell><cell cols="4">MeanSGD 53,768 38 57</cell></row><row><cell>Fisher+SPM [25]</cell><cell cols="2">Sift, Color SGD</cell><cell cols="2">262,144 34</cell><cell>-</cell></row><row><cell cols="2">LLC,SVC+SPM [16] Hog, Lbp</cell><cell cols="4">ASGD 1,179,648 28 47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Large scale knowledge sharing results. Shown is flat error in % (hierarchical error)</figDesc><table><row><cell>Approach</cell><cell>Top 5 Error</cell><cell>Top 1 Error</cell></row><row><cell>1. One-vs-all</cell><cell></cell><cell></cell></row><row><cell>(=leaf WordNet nodes)</cell><cell>37.6 (2.91)</cell><cell>57.2 (5.77)</cell></row><row><cell>2. Hierarchical</cell><cell></cell><cell></cell></row><row><cell>inner WordNet nodes</cell><cell>71.3 (7.31)</cell><cell>90.7 (8.69)</cell></row><row><cell>all WordNet nodes</cell><cell>50.4 (5.49)</cell><cell>67.9 (7.54)</cell></row><row><cell>leaf nodes, cost sensitive</cell><cell>48.6 (4.71)</cell><cell>60.2 (5.66)</cell></row><row><cell>SVM stacking, all nodes</cell><cell>36.8 (2.84)</cell><cell>56.3 (5.59)</cell></row><row><cell>3. Attributes</cell><cell></cell><cell></cell></row><row><cell>Wikipedia</cell><cell>63.7 (5.21)</cell><cell>81.5 (8.52)</cell></row><row><cell>Yahoo Holonyms</cell><cell>68.7 (5.61)</cell><cell>87.1 (9.24)</cell></row><row><cell>Yahoo Image</cell><cell>74.0 (5.80)</cell><cell>90.6 (10.28)</cell></row><row><cell>Yahoo Snippets</cell><cell>67.2 (5.33)</cell><cell>84.6 (8.55)</cell></row><row><cell>all attributes</cell><cell>56.4 (4.63)</cell><cell>75.9 (7.32)</cell></row><row><cell>SVM stacking, all attributes</cell><cell>43.8 (3.38)</cell><cell>63.5 (6.34)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We provide code, settings, and intermediate results on our web pages to facilitate further research and comparison on large-scale knowledge transfer.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Examining the performance of the hierarchical methods (Tab. 4 part 1) we observe a top-5 error of 72.8% using leaf WordNet nodes only. This is the closest setting examined here to one-vs-all classification. It uses the WordNet hierarchy to identify the most similar known leaf node classes for an unseen test class (see Fig. <ref type="figure">4</ref>). Using the inner Word-Net nodes only, the performance improves to a top-5 error of 66.7%. This is remarkable, since, in comparison to leaf node classifiers, only far fewer and less specific inner node classifiers are used. Furthermore it is in contrast to results in the knowledge sharing experiment (using all classes for training) where performance drops for inner nodes (see Tab. 3): while we benefit from knowledge transfer through the inner nodes for zero-shot recognition, we are loosing precision compared to one-vs-all when sharing knowledge in the inner nodes. The error can slightly be reduced to 65.2% using all WordNet nodes, effectively combining the two previous settings.</p><p>Part 2 of Tab. 4 shows the results for attributed-based models using the fully unsupervised mining of both attribute inventory and object class-attribute associations. Overall the obtained error rates for the individual relatedness measures are not competitive to the ones obtained by the hierarchical models. Yahoo Snippets performs best with 76.2% top-5 error. However, when combining all attribute measures we achieve a top-5 error of 70.3% which lies between the performance of leaf and inner WordNet nodes.</p><p>On the other hand, the direct similarity-based models reported in part 3 of Tab. 4 obtain as low as 69.3% top-5 error for Yahoo Web and competitive 66.6% when combining the classifiers of all measures, which is only slightly worse than the best performance obtained by a hierarchical method (all WordNet nodes with 65.2%).</p><p>The slightly favorable role of direct similarity compared</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single-example learning of novel classes using representation by similarity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large Scale Visual Recognition Challenge 2010</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<ptr target="www.image-net.org/challenges/LSVRC/2010/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sgd-qn: Careful quasi-newton stochastic gradient descent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://leon.bottou.org/projects/sgd" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nuswide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What does classifying more than 10,000 image categories tell us?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>ECCV&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance pseudo-metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>7694</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Caltech</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and using taxonomies for fast visual categorization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale image classification: fast feature extraction and SVM training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic hierarchies for visual object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<idno>ECCV&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
		<idno>SICON&apos;92</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">In defense of one-vs-all classication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klautau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What Helps Where -And Why? Semantic Relatedness for Knowledge Transfer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combining language sources and robust semantic relatedness for attribute-based knowledge transfer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-PnA&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-PnA&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improved Fisher Vector for Large Scale Image Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánches</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<ptr target="www.image-net.org/challenges/LSVRC/2010/ILSVRC2010XRCE.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining heterogeneous knowledge resources for improved distributional semantic models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PCICLing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="289" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">80 million tiny images: a large dataset for non-parametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sharing visual features for multiclass and multiview object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Xi Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting object hierarchy: Combining models from different category levels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
