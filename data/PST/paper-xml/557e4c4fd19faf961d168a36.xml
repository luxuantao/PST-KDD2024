<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5F72B3EA26DF757BE207B806E9AD6CF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Software Project Control: An Experimental Investigation of Judgment with Fallible Information Tarek K. Abdel-Hamid, Member, IEEE, Kishore Sengupta, and Daniel Ronan Abstruct-Software project management is becoming an increasingly critical task in many organizations. While the macrolevel aspects of project planning and control have been addressed extensively, there is a serious lack of research on the microempirical analysis of individual decision making behavior. In this study we investigate the heuristics deployed to cope with the problems of poor estimation and poor visibility that hamper software project planning and control, and present the implications for software project management. The paper presents a laboratory experiment in which subjects managed a simulated software development project. The subjects were given project status information at different stages of the lifecycle, and had to assess software productivity in order to dynamically readjust project plans. A conservative anchoring and adjustment heuristic is shown to explain the subjects' decisions quite well. Implications for software project planning and control are presented. Index Terms-Anchoring, experimentation, project control, software productivity, software project management. I. <ref type="bibr">INTRODUCTION</ref> OW are software projects controlled? This question H speaks to the often frustrating task of delivering software systems on budget and on schedule. Yet, it is a task that is becoming increasingly critical in many organizations as computer-based technology continues to play an ever larger role in determining how organizations operate, how they create their products, and indeed in reshaping the products themselves.</p><p>While the software engineering literature has been quite extensive in its coverage of the macro-level aspects of project planning and control, there is a serious lack of research on the micro-empirical analysis of individual decision making behavior. Our objective in this paper is to investigate the heuristics deployed to cope with the problems of poor estimation and poor visibility that hamper software project planning and control, and present the implications for software project management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">A MODEL OF PROJECT CONTROL</head><p>Fundamentally, control is any process that guides activity toward some predetermined goal. The essence of the concept is in determining whether the activity is achieving the desired results. Fig. <ref type="figure" target="#fig_0">1</ref> depicts the "textbook" model of project control that has been adopted in one form or another for many years in the literature <ref type="bibr" target="#b2">[3]</ref>. A control system is shown to have four basic elements: a measuring device which detects what is happening; a mechanism for comparing what is actually happening with some standard or expectation of what should be happening; a procedure for altering behavior if the need for doing so is indicated; and a means of transmitting feedback information to the control device <ref type="bibr" target="#b1">[2]</ref>.</p><p>Two information inputs are absolutely vital to effective project control. The first is planning information, such as schedule and resource estimates, which provide the standards used for assessing the significance of what is happening. Indeed, it has been stated that the degree of control over a project can be no greater than the extent to which adequate plans have been made for the project [19]. The second is accurate and timely status information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">PROJECT CONTROL IN THE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOFWARE DEVELOPMENT ENVIRONMENT</head><p>Throughout the short history of the software industry, both researchers and practitioners alike have agonized over the industry's continuing failure to adequately control software development <ref type="bibr" target="#b17">[18]</ref>. That the problem is both serious and very much alive is highlighted in a recent congressional report, in which eight major software projects in the U.S. Department of Defense experienced a combined cost overrun of more than one billion dollars. The report highlighted a wide range of "common" and "chronic" problems: "These problem areas include lack of cost visibility (i.e., knowing the real extent of costs at a particular point in time) and control; schedule delays; and lack of capability at the time promised" <ref type="bibr" target="#b6">[7]</ref>.</p><p>The reason why controlling software projects has been problematic is no mystery. It has proven to be quite difficult to, on the one hand, accurately estimate development costs and schedules and, on the other, to measure a project's progress rate; the two critical information inputs that are absolutely vital to effective project control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U.S. Government work not protected by U.S. Copyright</head><p>To address the estimation difficulty, a large number of algorithmic estimation models have been developed over the last decade. However, the utility of such models has proven to be quite poor [ l l ] . As a result, many software development organizations do not yet rely on these tools [26].</p><p>On the measurement side, managers continue to complain that "it is difficult to measure performance in programming. It is difficult to diagnose trouble in time to prevent it. It is difficult to evaluate the status of intermediate work such as undebugged programs or design specification and their potential value to the complete project" [17]. Such measurement difficulties are caused both by people-type factors as well as product-type factors.</p><p>First, controlling software projects (like most human systems) suffers from the following phenomenon: people are inclined to report favorable information (this is relatively unimportant to management since no corrections are needed) and to withhold unfavorable information (this is the type needed if corrections are to be made). Moreover, the very temperament of the typical programmer, who is highly motivated by technical problems but generally less motivated by management ones, further degrades the measurement process Second, the software product is relatively intangible during most of the development process, and for which there are no visible milestones to measure progress and quality like a physical product would <ref type="bibr" target="#b5">[6]</ref>. "This invisibility is compounded for large software, for which logical complexity cannot be maintained in one person's mind, and for which development must be partitioned into a number of tasks assigned to different people" [27].</p><p>Having to cope with such impediments to project planning and control is indeed a formidable challenge that has frustrated both software practitioners and researchers alike. Lehman articulated well the common sentiment:</p><p>The process of controlling a software engineering project may well be the most talked about and least understood of all the project managers' functions. Many projects have failed because of the managers' inability to adequately define where the project was in the development cycle. Other development efforts have been cancelled not for inadequate programming, or for lack of technology, but from the sheer frustration of all concerned in attempting to determine when, if ever, they would be completed <ref type="bibr" target="#b14">[15]</ref>. Sadly, Lehman's observations are as true today as they were more than a decade ago [7].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE RESEARCH QUESTION</head><p>The research question addressed in this paper is the following: How do software project managers cope with poor initial estimates and unreliable measurements in controlling software development throughout the lifecycle? And what are the implications of whatever (compensatory) strategies they opt to use on project dynamics?</p><p>More specifically, we will focus on the heuristics that managers use in assessing their teams' software development productivity. To assess progress on a software project, a manager needs to track work accomplished as well as the resources utilized. Combining both "measures" (by dividing the former by the latter) yields an estimate of the team's productivity. The amount of effort still needed to complete the project may then be estimated as a function of the tasks that remain to be accomplished (total project size minus work already completed) and the estimated productivity. As such, productivity assessment plays a central role in the planning and control process [14].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETTING</head><p>The research question was explored in the context of a role-playing project simulation game. Specifically, 33 graduate students at a U.S. university participated as part of a software engineering management course requirement. The subjects were fifth and sixth quarter masters students studying in the computer systems management curriculum (a six-quarter M.B.A. program with a concentration in information technol-The simulation game mimics the life of a real software project from the start of the design phase until the end of testing. (A description of the model's structure is provided in the Appendix.) The subject's role was to track the project's progress using status reports on resources used, work accomplished, etc., generated at different stages of the project (by the "simulated" project team), and to estimate the team's average productivity in taskdperson-day. A task is a unit of work, e.g., a software module 50 lines of code long. Using a task as the unit of work for reporting progress and estimating future resources is a common practice [5, p. 6151. For example, Cooper <ref type="bibr" target="#b7">[8]</ref> explains how projects are decomposed into "smaller, more comprehensive subtasks until the level is reached where the tasks cannot be subdivided any further. You will end up at the 50/100 lines of code level ... These small tasks also permit the allocation of resources, the loading of manpower, and the subsequent costing (of the project)."</p><p>The subjects were told that their productivity estimates would then be used to make the necessary adjustments to the project's staffing level and its schedule. In reality, however, the model's gaming interface was designed to disregard the subjects' estimates. That is, the subjects' individual estimates had no influence upon the simulated project. This ensured that the model provided an identical project lifecycle profile to all subjects. It was possible to maintain such an illusion because, as in real life, productivity in the model is affected by a large number of interacting factors (e.g., changes in staff mix, changes in staff size, learning, changes in lifecycle phases, etc.) that change dynamically throughout the lifecycle. (Postexperimental interviews of the subjects indicated that none of the subjects were at any time in doubt that they were in fact "calling the shots" throughout the project's lifecycle.)</p><p>The simulated project used in the experiment was a real project developed at NASA in the early 1980's. The initial estimates of the project's size and cost were 396 tasks and 11 11 person-days, respectively. The project ended up growing to 610 tasks in size and it cost 2064 person-days to complete. ogy).  The Project will start with a full tip. equivalent core tsam of 1.5 aenior designers, and staff up quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Initial estimates report.</head><p>The actual overall staff productivity was, thus, 0.30 tasks per person-day .</p><p>The 33 subjects were randomly assigned to three experimental groups of eleven subjects each. The only difference between the three groups was in the initial project estimates provided to each group. As part of the game's documentation, each subject received an "Initial Estimates Report" like the one shown in Fig. <ref type="figure">2</ref>. Group 2 received an initial productivity estimate of 0.3 taskdperson-day (the actual for the project). Group 1's initial productivity estimate was 0.20 tasks/person-day, and Group 3's initial productivity estimate was 0.45 tasks/person-day. The Table <ref type="table" target="#tab_2">I</ref> summarizes the classroom abilities and work experience of the subjects. The groups exhibited no significant difference with respect to classroom ability (F(2,25) = 0 . 0 1 , ~ &gt; 0.9907) or work experience ( F ( 2 , 2 5 ) = 0 . 2 5 , ~ &gt; 0.7792).</p><p>Each subject was required to provide three productivity estimates during the life of the project:</p><p>1) At the end of the design phase (end of the fifth month) 2) At the middle of the coding phase (end of tenth month) 3) At the end of coding (approximately the end of the fifteenth month) subjects were told that the initial productivity estimate was derived from an extensive database of historical project statistics that the simulated (although real) organization has developed and maintained in the past five years. The differences between the groups are summarized below:</p><p>The project required 19 months to complete. At each one of these three decision points, the subjects were provided with a progress report on the project,s status. An example status report is shown in Fig. <ref type="figure">3</ref>. The subjects were advised to give whatever weight they saw fit to these reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GROUP GROUP GROUP</head><p>As was mentioned above, the subjects were in their final stages of a masters program in computer systems management. As such they appreciated the real life complexities of the control task. For example, they understood the reasons why "percent reported complete" may not be totally reliable, especially in the  early phases of the project. The students also appreciated the dynamic impacts of factors such as work force mix, learning, schedule pressures, etc., on productivity. Each revised productivity estimate had to be entered into the model through a menu screen as well as hand written on a special estimation sheet which was submitted to a lab attendant before the subject was allowed to continue the simulation. Upon completion of the project, each subject was required to briefly specify the method(s) hefshe used to generate the estimates.</p><p>The simulation was played over approximately a one hour session, with each subject working alone. Ten percent of each student's grade in the course was based on their performance on this exercise. Good performance was defined in terms of "closure to the actual productivity," i.e., on providing the most accurate productivity estimate. As noted by DeMarco <ref type="bibr">[9, p.</ref> 211 "Success for the estimator must be defined as a function of convergence of the estimate to the actual, and of nothing else."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS AND STATISTICAL ANALYSIS</head><p>The experiment elicited three productivity estimates per subject from 33 subjects. Five subjects made significant errors, and were omitted from the analysis, resulting in a sample size of 28. (The five subjects misapplied the definition of productivity as person-daysltask instead of tasksfperson-day. While the estimates provided by them can still be used in the analysis, the fact that they made a significant conceptual mistake limits the validity of their estimates.)' Table <ref type="table" target="#tab_6">I1</ref> shows summary statistics for the productivity estimates. Plots of the same statistics are given in Fig. <ref type="figure">4</ref>.</p><p>Following the approach suggested by Winer <ref type="bibr" target="#b25">[25]</ref>, a multivariate analysis of variance for repeated measures was performed. Since the numbers of subjects in the three groups</p><p>We conducted a sensitivity analysis in order to determine how the results would be affected had these subjects been included in the original data set. The productivity estimates reported by these subjects were transformed to their reciprocals and added to the estimates of the 28 subjects. The results remain unchanged. were not equal (after the discarded subjects the sizes were 9, 9, and lo), the analysis was conducted with the General Linear Models procedure in SAS <ref type="bibr" target="#b21">[22]</ref>. Table <ref type="table" target="#tab_10">111</ref> contains the MANOVA results. The results show that the productivity estimates of subjects across the three different groups were significantly different (' p &lt; 0.0003). We, therefore, reject the null hypothesis of no significant differences among groups. In other words, the subjects' productivity estimates were indeed influenced by the initial estimates (anchors) provided to them.</p><p>There was no within-subjects effect of time (p &gt; 0.81),</p><p>or any interaction between the initial estimates and time (p &gt; 0.1). The lack of a significant time effect indicates that on the whole, subjects' estimates did not change significantly over time, i.e., did not show a (statistically) significant upward or downward trend. The absence of an interaction effect indicates that subjects in different groups did not behave differently over time, i.e., subjects with different initial estimates did not converge to a common point over time. Collectively, therefore, the two results show that a) subjects adhered to their respective anchors in making productivity estimates, and b) did not converge to the actual estimates over time.2 Conservative Anchor Dragging: According to Kahneman and Tversky [24], anchoring is quite common in static decision tasks involving judgement under uncertainty:</p><p>In many situations, people make estimates by starting from an initial value that is adjusted to yield the final answer... (The) adjustments are typically insufficient. That is, different starting points yield different estimates, which are biased toward the initial values. We call this phenomenon anchoring.</p><p>On the other hand, there has been very little research specifically examining anchoring in a dynamic environment. Hogarth [12], reports an experiment in which subjects used anchoring and adjustment effectively in making repeated forecasts in an exponential series. Mackinnon and Wearing <ref type="bibr" target="#b28">[28]</ref> report similar results on a similar experimental task. However, the two studies employed relatively simple tasks, and subjects received accurate feedback. No previous study has examined anchoring in complex, dynamic environments characterized by fallible information.</p><p>A rationale for why decision makers might resort to anchoring over multiple time periods is provided by Hogarth ... in continuous environments, the adjustment and anchoring heuristic essentially provides the basic mode of judgment. Consider, for instance, how one forms impressions of strangers through interaction. That is, in discrete incidents a single (possibly inaccurate) judgment is made. In continuous processing, however, a series of adjustment and anchoring responses, all of which may be relatively inaccurate, takes one progressively to the target. (pp. 206-207)</p><p>The results of Fig. <ref type="figure">4</ref>, however, seem to indicate otherwise. While a series of adjustments were indeed made, they did not necessarily take the subjects (e.g., Group 1) "progressively to the target." Rather, the subjects opted for a more conservative route. This conservative anchoring strategy can be expressed as follows: when confronted with two candidate (but tentative) productivity estimates (the anchor and the one implicit in the status report data), the subjects tended to lean toward the lower of the two. In a project situation, a lower (i.e., more conservative) productivity estimate is "safer" since it justifies a higher level of resource requirements and/or time to complete   the project, thereby providing a cushion of safety for the project manager. Such conservatism is consistent with findings in the behavioral decision theory field, where "an abundance of research has shown that human beings are conservative processors of fallible information" ( <ref type="bibr">[lo]</ref>, emphasis added). We will label this decision strategy "conservative anchor dragging," i.e., continuous anchoring moderated by conservatism.</p><p>In order to test the strength of the conservative anchor dragging phenomenon, we conducted an additional test to examine the decision strategies adopted by individual subjects. The decision strategies were inferred from the following test: If the estimate chosen was lower than the midpoint between the anchor and the reported estimate, the subject was considered to have used conservative anchor dragging. Thus, for a subject in Group 1, the candidate estimates at time 100 were the anchor (0.2) and the value implicit in the status report data (0.33). If the subject's estimate was lower than 0.265 (the midpoint between 0.33 and 0.2), the subject was considered to have used anchor dragging.</p><p>Table <ref type="table" target="#tab_11">IV</ref> shows the results aggregated over three time periods for Groups 1 and 3, i.e., those with initial estimates of 0.2 and 0.45 (Group 2, which was given the perfect estimate of 0.3 was excluded from the analysis). Table IV confirms that subjects in both groups converged toward the lower of the anchor and reported estimate, particularly in time periods 200 and 300. The conservative anchor dragging strategy was exhibited in almost two-thirds of the instances, and significantly more often than otherwise (t = -2.1032; d.f. = 36; p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>Since any conclusions are contingent on the limitations present, the study's limitations are discussed prior to the implications of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations</head><p>The main limitations of the study center on the experimental setting and the participants. First, the experimental setting was a computer-based laboratory experiment. Laboratory research entails giving up the richness of context to obtain control. For example, to control for information feedback to the experimental subjects, feedback information was limited to status reports. In reality project managers may rely on more than status reports in determining project status (including conversations with programmers, interviews, etc.). The question is, what price did we pay for such a simplification?</p><p>As noted earlier, the evidence indicates that software project visibility continues to be rather poor. This suggests that even with such additional cues status information remains unreliable (especially early in the lifecycle). Therefore, the inclusion of such additional cues would not have altered the fundamental experimental task, namely, weighing two sets of unreliable information (initial estimates and status information) in estimating the project team's software productivity. However, while such additional cues may not necessarily eliminate status uncertainty, they may or may not affect the weights that managers give to the different information sources. Such an issue would be an interesting one to investigate in a future study.</p><p>Second, the generality of the current results is tempered by the use of graduate students as managerial surrogates. Surveying over 250 recent studies on the dynamics of small social groups, Bettenhausen <ref type="bibr" target="#b3">[4]</ref> concluded that " ... findings using student groups in manipulated settings often generalize to organizational settings as well or better than findings from intact groups that are frequently confounded by unique, unmeasured contextual factors." In one particularly relevant study, Remus <ref type="bibr" target="#b19">[20]</ref> found no significant differences between students and managers in making production scheduling decisions. Although software project management decisions are somewhat different from production scheduling decisions, they are similar enough to apply his findings and assume that software engineering graduate students are acceptable surrogates in this experimental investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implications for Project Planning and Control</head><p>Having shown that conservative anchor dragging can lead to significantly different productivity-assessment profiles, our objective here is to explore the impact such differences may or may not have on project behavior. Conceptually, it is not difficult to see why such impacts may be significant. For example, Fig. <ref type="figure" target="#fig_3">5</ref>'s causal loop diagram demonstrates how estimated productivity directly influences work force hiring and firing. The work force level acquired determines the communication and training overheads incurred on the project which, in turn, affect the progress rate and the ultimate cost of the project. In addition, estimated productivity can influence project behavior through more subtle, and less direct means. For example, if a project is perceived to be behind schedule (e.g., due to an underestimate of productivity), software developers may feel pressured to work harder in order to bring the project back on schedule.</p><p>Going a step beyond a conceptual appreciation of the impacts of estimated productivity on project behavior, we further explored the impacts of the different productivityassessment profiles on project dynamics. In Fig. <ref type="figure">6</ref> we show the results of two simulation runs that incorporate the mean productivity estimates of Group 1 (initial anchor = 0.2) and Group 3 (initial anchor = 0.45). Recall that in the role-playing experiment, the model interface was designed to disregard the subjects' productivity estimates. In the simulation runs of Fig. <ref type="figure">6</ref>, on the other hand, the groups' mean productivity estimates were incorporated into the simulated project, i.e., they were the values deployed by the simulated project team in managing the project.</p><p>Because Group 1's productivity estimate starts low and remains low, a larger person-day estimate is made for the project. As a result, a larger work force is assembled early on, and remains relatively stable until the end-of-project "windingdown" phase. Contrast this with Group 3, where the work force level is significantly lower throughout the first half of the lifecycle. This is caused by the overestimated productivity value which, in effect, underestimates the project's cost in person-days. This (not uncommon) underestimation problem inevitably catches up with management; causing the project to fall behind schedule. This, in turn, induces the sharp upward correction in the work force level around day 200.</p><p>As was mentioned earlier, the actual average productivity for the simulated project was 0.30 taskdperson-day. The simulated project costs and completion times for Groups 1 and 3 are summarized below: Notice that overestimating productivity (Group 3) does not lead to any significant increase over the project's nominal productivity level (0.3 tasks/person-day). On the other hand, underestimation does lead to a significant drop in the team's average productivity (from 0.3 to 0.21). This, we feel, is quite an interesting finding, and of particular relevance to the practicing software manager.</p><formula xml:id="formula_0">~_ _ _</formula><p>We can offer two reasons to explain this result. Underestimating productivity translates directly into an overestimated project cost in person-days. When this happens, work typically expands to consume the overestimated resources. For example, work expansion could take the form of goldplating, or it could be in the form of an increase in people's slack time activities (such as catching up on the mail, coffee breaks, etc.) [5]. Thus, low productivity estimates can lead to more "wasteful" processes which, in turn, "fulfills the prophecy" of lower development productivity.</p><p>Second, since a lower productivity estimate translates into an overestimated project cost, it can also lead to hiring a larger work force. This, in turn, can cause higher communication and training overheads on the project, which eventually leads to a decrease in team productivity, as is captured by the feedback loop of Fig. <ref type="figure" target="#fig_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Past studies have demonstrated anchoring as a pervasive phenomenon in how individuals make decisions in static situations. This paper contributes to research in decision making and software engineering by demonstrating the dynamic nature of anchoring in the software project environment. The software project control task is somewhat unique in that the anchor (initial project estimates) as well as the feedback information (status reports) are both unreliable. (Although, as the project</p><formula xml:id="formula_1">I I 0.</formula><p>loo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>300.</head><p>Fig. <ref type="figure">6</ref>. Simulation results.</p><p>proceeds the feedback information becomes relatively more reliable.) Our results suggest that anchoring persists over time and is not necessarily mitigated by feedback provided in the intervening periods. Specifically, the experimental subjects adopted a "conservative anchor-dragging strategy," i.e., when faced with two estimates of productivity, they accorded greater weight to the lower (i.e., more conservative) figure. The "advantage" to the project manager of adopting a more conservative estimate is that it justifies a higher level of resource requirements and/or time to complete the project, thereby providing a cushion of safety. The potential pitfall to the organization, however, is that underestimating productivity can become self-fulfilling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>OVERVIEW OF MODEL STRUCTURE Fig. <ref type="figure">7</ref> shows a high-level view of the model's four subsystems: human-resource management, software production, control, and planning, and some of the relations between them. The actual model is very detailed and contains more than 100 causal links; a full description of the model's structure and its mathematical formulation is published in <ref type="bibr">[l]</ref>.</p><p>Human-Resource Management: This subsystem captures the hiring, assimilation, and transfer of people. We segregate the project's workforce into employee types (newly hired and experienced, for example). We make this distinction because new team members are usually less productive than veterans.</p><p>This segregation also lets us capture the training process to assimilate new members. The veterans usually train the newcomers, both technically and socially. This is important, because this training can significantly affect a project's progress by reducing the veteran's productivity.</p><p>In deciding how big a work force they need, project managers typically consider several factors. One, of course, is the project's scheduled completion date. Another is the work force's stability, so managers try to predict project employment time for new members before they are hired. In general, the relative weight managers give to stability versus completion date changes as the project progresses.</p><p>Software Production: This subsystem models development; it does not include the operation and maintenance phases. The development phases included are designing, coding, and testing.</p><p>As software is developed, it is reviewed to detect any errors, e.g., using quality assurance activities such as structured walkthroughs. Errors detected through such activities are reworked. Not all software errors are detected during development, however, some escape detection until the testing phase.</p><p>The software-production subsystem models productivity and its determinants in great detail. Productivity is defined as potential productivity minus the loss from faulty processes. Potential productivity is the level of productivity that can occur when an individual or group makes the best possible use of its resources, and is a function of the nature of the task and the group's resources <ref type="bibr" target="#b23">[23]</ref>. Loss from faulty processes are losses in productivity from things like communication and coordination overhead and low motivation.</p><p>Control Subsystem: As progress is made, it is reported. A comparison of the degree of project progress to the planned schedule is captured within the control subsystem.</p><p>In all organizations, decisions are based on the information available to the decision maker. Often, this information is inaccurate. Apparent conditions may be far removed from those actually encountered, depending on information flow, time lag, and distortion.</p><p>Progress rate is a good example of a variable that is difficult to assess during the project. Because software is basically an intangible product during most of the development, it is difficult to measure things like programming performance and intermediate work. In the earlier phases of development, progress is typically measured by the rate of resource expenditure rather than accomplishments. But as the project advances towards its final stages, though, work accomplishments become relatively more visible and project members better perceive how productive the work force has actually been.</p><p>Planning subsystem: In the Planning Subsystem, project estimates are made and revised as the project progresses. For  By dividing the value of person-days remaining at any point in the project by the time remaining, a manager can determine the indicated work force level, which is the work force needed to complete the project on time. However, hiring decisions are not made solely on the basis of scheduling requirements. Managers also consider the training requirements and the work force's stability. Thus, before adding new project members, management assesses the project employment time for the new members. In general, the relative weighting between the desire for work force stability and the desire to complete the project on time is not static; it changes throughout the project's life.</p><p>Although management determines the work-force level needed to complete the project, this level does not necessarily translate into the actual hiring goal. The hiring goal is constrained by the ceiling on new hires. This ceiling represents the highest work force-level management believes can be adequately handled by its experienced project members.</p><p>Thus, three factors-scheduled completion time, workforce stability, and training requirements-affect the workforce level.</p><p>Model Validation: The model was developed on the basis of field interviews of software project managers in five organi- zations, complemented by an extensive database of empirical findings from the literature. The following set of tests were conducted to validate the model:</p><p>-Face validity test. To test the fit between the rate/level/feedback structure of the model and the essential characteristics of real project environments. This fit was confirmed by the software project managers involved in the study.</p><p>-Replication of reference modes. To test whether the model can endogenously reproduce the various reference behavior modes characterizing real environments. Reference modes reproduced by the model included a diverse set of behavior patterns both observed in the organizations studied as well as reported in the literature (e.g., the "90% syndrome," diminishing returns of QA effort, the deadline effect, etc.).</p><p>-Case studies. Five case studies (two by the first author, and three conducted independently by three separate organizations) were conducted after the model was completely developed. All case studies were conducted in organizations other than the five organizations studied during model development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Project control model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a two month safety factor applied to the proyect duration estimate. (i.e.. while any schedule alippaqe is undesirable, a slippage of more than SS day. i s untolerable.) Haximum Tolerable Project Duration: 375 Days In thia organlration people are typically asslgned to more than one project. They may spend anywhere from 21 to 815 of thsir ti-on a particular project. FOR CLARITY, TBS AVERAGE STAT? SIZE AND SIMULATION OUTPUT WILL BE GIVEN IN PULL-TIMS EQUIVALENT EMPLOYEES. One full time equivalent employee is cqul to one person rho apends l1l5 of his tim on the project or two pIoPlC that spend 515 of their t m e on the project.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Productivity impact on project behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc>Fig</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SUBJECT CHARACTERISTICS-MEANS AND STANDARD DEVIATIONS (n = 28)</head><label>I</label><figDesc></figDesc><table><row><cell>Group</cell><cell></cell><cell>Classroom</cell><cell>Work</cell></row><row><cell></cell><cell></cell><cell>Ability</cell><cell>Experience</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell>(S.D)</cell><cell>(S.D)</cell></row><row><cell>1</cell><cell>Initial estimate</cell><cell>3.4222</cell><cell>3.3333</cell></row><row><cell></cell><cell>= 0.2 (n = 9)</cell><cell>(0.4381)</cell><cell>(0.7071)</cell></row><row><cell>2</cell><cell>Initial estimate</cell><cell>3.3444</cell><cell>3.1111</cell></row><row><cell></cell><cell>= 0.3</cell><cell>(0.4746)</cell><cell>(0.7817)</cell></row><row><cell></cell><cell>(n = 9)</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>Initial estimate</cell><cell>3.4200</cell><cell>3.1000</cell></row><row><cell></cell><cell>= 0.45</cell><cell>(0.3615)</cell><cell>(0.8756)</cell></row><row><cell></cell><cell>(n = 10)</cell><cell></cell><cell></cell></row><row><cell cols="2">Notes:</cell><cell></cell><cell></cell></row><row><cell>1.</cell><cell cols="2">Classroom ability indicates</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>the grade received by the subject on a scale of 1-4. The grade received for the experiment was excluded in the calculation of the overall grade. 2. Data on work experience was collected on the following</head><label></label><figDesc></figDesc><table><row><cell>scale:</cell><cell cols="3">1: 0-4 years' work experience.</cell></row><row><cell></cell><cell cols="3">2: 5 -8 years' work experience.</cell></row><row><cell></cell><cell cols="3">3: 9-12 years' work experience.</cell></row><row><cell></cell><cell cols="3">4: 13-16 years' work experience.</cell></row><row><cell cols="2">CURRENT INTERVAL STATISTICS:</cell><cell>Elapsed Time =</cell><cell>100</cell></row><row><cell cols="4">INITIAL ESTIMATES: (These will not change throughout the project)</cell></row><row><cell>Project Size</cell><cell></cell><cell></cell><cell>396</cell><cell>Tasks</cell></row><row><cell>Project Cost</cell><cell></cell><cell></cell><cell>1.980.00</cell><cell>Person-Days</cell></row><row><cell>Project Duration</cell><cell></cell><cell></cell><cell>320</cell><cell>Days</cell></row><row><cell cols="2">REPORTED STATIGTICS AT TIMB</cell><cell></cell></row></table><note><p>= -------------S Project</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Reported Complete Updated Size of Project Total Number of Full-time Equiv. Staff Effort Expenditures to Date:</head><label></label><figDesc></figDesc><table><row><cell>-&gt;</cell><cell>100</cell><cell>Days</cell></row><row><cell></cell><cell>22.92</cell><cell>Percent</cell></row><row><cell></cell><cell>421</cell><cell>Tasks</cell></row><row><cell></cell><cell>3.5</cell><cell>Staff</cell></row><row><cell>Development Activities</cell><cell>292.3</cell><cell>Person-Days</cell></row><row><cell>Design and Coding</cell><cell>152.32</cell><cell>Person-Days</cell></row><row><cell>Rework (i.e., fixing errors)</cell><cell>50.69</cell><cell>Person-Days</cell></row><row><cell>Quality Assurance</cell><cell>89.29</cell><cell>Person-days</cell></row><row><cell>Testing</cell><cell>0.00</cell><cell>Person-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>days Total Person-Days Expended 292.30 Person-Days Days Days New Est. of Project Duration (Start-End) 320 Max Tolerable Project Duration 375 Fig. 3. Status report.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE I1 MEANS AND STANOABD mYl"S OF PRODUCTIVITY &amp;W&amp;WES</head><label>I1</label><figDesc>QF THREE T R E " r GROUPS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>ACROSS THREE TIME PERIODS (n = 28) Time 300 Group Time 1 0 0 Time 200</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(S.D)</cell><cell>(S.D)</cell><cell>(S.D)</cell></row><row><cell></cell><cell>1</cell><cell cols="2">Initial estimate</cell><cell>0.2649</cell><cell>0.2559</cell><cell>0.2390</cell></row><row><cell></cell><cell></cell><cell>= 0.2 (n = 9)</cell><cell></cell><cell>(0.0836)</cell><cell>(0.0893)</cell><cell>(0.0944)</cell></row><row><cell></cell><cell>2</cell><cell cols="2">Initial estimate</cell><cell>0.3109</cell><cell>0.3552</cell><cell>0.3564</cell></row><row><cell></cell><cell></cell><cell>= 0.3 (n = 9)</cell><cell></cell><cell>(0.0305)</cell><cell>(0.0420)</cell><cell>(0.0651)</cell></row><row><cell></cell><cell>3</cell><cell cols="2">Initial estimate</cell><cell>0.4260</cell><cell>0.3830</cell><cell>0.3790</cell></row><row><cell></cell><cell></cell><cell>= 0 . 4 5 (n = 10)</cell><cell></cell><cell>(0.0853)</cell><cell>(0.0910)</cell><cell>(0.0795)</cell></row><row><cell>0</cell><cell>100</cell><cell>MO</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">PROJECTLIFECYCLE (Daya)</cell><cell></cell><cell></cell></row><row><cell>+GROUP1</cell><cell>+GROUF'2</cell><cell>, GROUP3</cell><cell>+REPORTED</cell><cell></cell></row><row><cell></cell><cell cols="3">Fig. 4. Group productivity estimates.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>04). However, univariate methods for repeated measures can present difficulties, and frequently result in Type I errors [16]. Moreover, the sphericity test was strongly rejected</head><label></label><figDesc></figDesc><table /><note><p>'A univariate test yielded a significant time*initial estimates interaction (p &lt; 0.(p &lt; 0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>001) for our sample. It is, therefore, inappropriate in this case to use the results of the univariate test.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 111 MULTIVARIATE ANALYSIS OF VARIANCE-(TZ = 28) Source of Degrees of Significance Variation s.s freedom F of F</head><label>111</label><figDesc></figDesc><table><row><cell>Between Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Initial Estimate Subjects -</cell><cell>0.292</cell><cell>2</cell><cell>11.37</cell><cell>0.0003</cell></row><row><cell>within-cells</cell><cell>0.321</cell><cell>25</cell><cell></cell><cell></cell></row><row><cell>Within Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time</cell><cell>0.983</cell><cell>2, 24</cell><cell>0.20</cell><cell>0.8161</cell></row><row><cell>Time'Initial</cell><cell>0.732</cell><cell>4, 48</cell><cell>2.02</cell><cell>0.1052</cell></row><row><cell>Estimate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IV EVIDENCE OF CONSERVATIVE ANCHOR DRAGGING ( n = 28)</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Anchoring Strategy</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Anchor</cell><cell>No Anchor</cell><cell></cell></row><row><cell>Group</cell><cell></cell><cell>Dragging</cell><cell>Dragging</cell><cell>Total</cell></row><row><cell>1</cell><cell>Initial estimate</cell><cell>17</cell><cell>10</cell><cell>27</cell></row><row><cell></cell><cell>= 0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Initial estimate</cell><cell>20</cell><cell>10</cell><cell>30</cell></row><row><cell></cell><cell>= 0 . 4 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Total:</cell><cell>37</cell><cell>20</cell><cell></cell></row><row><cell>Note:</cell><cell>t = -2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>.1032; degrees of freedom = 36.0; p= 0.0425</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sofiware Project Dynamics: An Integrated Approach</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Madnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Management Control Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dearden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Chicago, I L Richard D. Irwin, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">A. 0. Awani, Data Processing Project Management</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Petrocelli</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Five years of group research: What we have learned and what needs to be addressed</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Bettenhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Management</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="345" to="381" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><surname>Boehm</surname></persName>
		</author>
		<title level="m">Software Engineering Economics</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Silver Bullet: Essence and accidents of software engineering</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Mag</title>
		<imprint>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="1987-04">Apr. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DoD automated information systems experience runaway costs and years of schedule delays while providing little capability</title>
		<imprint>
			<date type="published" when="1989-11-20">Nov. 20, 1989</date>
			<publisher>U S . House of Representatives</publisher>
		</imprint>
	</monogr>
	<note>Rep. 101-382</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software development management planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="26" />
			<date type="published" when="1984-01">Jan. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Demarco</surname></persName>
		</author>
		<title level="m">ControllingSoftware Projects</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Yourdon</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conservatism in human information processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Representation of Human Judgment</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Kleinmpntz</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the use of software cost models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Genuchten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koolen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. and Management</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prior to joining NPS, he spent two and a half years at the Stanford Research Institute as a senior consultant in the information technology area. He has been an advisor to NASA&apos;s Jet Propulsion Lab., since 1986,-on the development of computer-based tools for software project management</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hogarth</surname></persName>
			<affiliation>
				<orgName type="collaboration">IEEE TRANSACTIONS ON SOFIWARE ENGINEERING</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests focus on software project management, system dynamics, expert simulators, and management information systems. He is the co-author of Software Project Dynamics: An Integrated Approach</title>
		<meeting><address><addrLine>Monterey, CA; Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">90</biblScope>
		</imprint>
	</monogr>
	<note>MIS Quarterly, IEEE SOFTWARE. Dr. Abdel-Hamid is a member of the Association for Computing Machinery, SIM, the IEEE Computer Society, and the Systems Dynamics Society. . -pi. 197-2i7, -1981</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Software project development cost estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Syst and Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="7" to="278" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Managing software development projects for maximum productivity</title>
		<author>
			<persName><forename type="first">N R</forename><surname>Howes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Software E n g</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="1984-01">Jan 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How software projects are really managed</title>
		<author>
			<persName><forename type="first">J H</forename><surname>Lehman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Datamation</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1979-01">Jan 1979</date>
			<biblScope unit="page" from="119" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The misuse of repeated measures analysis in marketing research</title>
		<author>
			<persName><forename type="first">S A</forename><surname>Latour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P W</forename><surname>Miniard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Marketing R e s</title>
		<imprint>
			<biblScope unit="volume">X</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="1983-02">Feb 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H D</forename><surname>Mills</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>Software Productivity Canada Little, Brown &amp; CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Software engineering Retrospect and prospect</title>
		<author>
			<persName><forename type="first">H D</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Twelvth Annu. Int Comput Software and Appl Con (COMPSAC)</title>
		<imprint>
			<biblScope unit="page" from="89" to="96" />
			<date type="published" when="1988">Oct 5-7, 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward a management philosophy for software development</title>
		<author>
			<persName><forename type="first">U W</forename><surname>Pooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P F</forename><surname>Gehring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Programming Management, T A Rullo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graduate students as surrogates for managers in experiments on business decision making</title>
		<author>
			<persName><forename type="first">Remus</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Business Re</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="19" to="25" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Controlling software projects</title>
		<author>
			<persName><surname>Rook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Eng J , Jdn</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">He is currently Assistant Professor of Information Systems at the Department of Administrative Services, Naval Postgraduate School, Monterey, CA Previously, he was with AT&amp;T&apos;s Network Software Center and Arthur Young (now Ernst and Young) His research interests are in decision support for dynamic tasks, computer supported cooperative work, learning, and intelligent tutoring His research appears in journals such as Management Science and MIS Quarterly, conference proceedings, and book chapters He is currently working on a book on cognitive aspects of decision support in distributed environments Dr Sengupta is a member of the American Association for</title>
		<author>
			<persName><surname>Sasistat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and the Institute of Management Sciences Philadelphia</title>
		<imprint>
			<date type="published" when="1980">1990. 1980</date>
			<publisher>PA Heyden</publisher>
			<pubPlace>Cleveland, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Case Western Reserve University</orgName>
		</respStmt>
	</monogr>
	<note>Guide for Personal Computers, Version 6 Kishore Sengupta received the P h D degree in information systems form</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Cary</surname></persName>
		</author>
		<author>
			<persName><surname>Sas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Steiner</surname></persName>
		</author>
		<title level="m">Group Process and Productiviq</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Judgement under uncertainty: Heuristics and biases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974-09">Sept. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Winer</surname></persName>
		</author>
		<title level="m">Statistical Principles in Experimental Design</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Software development estimation models: A review and critique</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wrigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dexter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASAC Conf</title>
		<meeting>ASAC Conf</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Management of large software development efforts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Zmud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="1980-06">June 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">He is currently managing three independent software development projects dealing with financial management, electronics inventory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mackinnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wearing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and the M.S. degree in computer systems management from the Naval Postgraduate School</title>
		<meeting><address><addrLine>New London, CT; Monterey, CA; Wildwood, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="177" to="191" />
		</imprint>
	</monogr>
	<note>He is Chief of the Business Systems Section at the Coast Guard&apos;s Electronics Engineering Center. and message management that will be distributed throughout the Coast Guard</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
