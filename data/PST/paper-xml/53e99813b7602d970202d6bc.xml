<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disappearing Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Ni</surname></persName>
							<email>nitao@cs.vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute Potsdam</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Baudisch</surname></persName>
							<email>patrick.baudisch@hp.i.uni-potsdam.de</email>
							<affiliation key="aff0">
								<orgName type="department">Hasso Plattner Institute Potsdam</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UIST&apos;09</orgName>
								<address>
									<addrLine>October 4-7</addrLine>
									<postCode>2009</postCode>
									<region>Victoria, British Columbia</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disappearing Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9EF3C8AE0B1221ABB05A86134C97EB8A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ACM Classification: H5.2 [Information interfaces and presentation]: User Interfaces: Input Devices and Strategies</term>
					<term>Interaction Styles Design</term>
					<term>Human Factors Miniaturization</term>
					<term>mobile device</term>
					<term>input device</term>
					<term>sensor</term>
					<term>wearable</term>
					<term>ubicomp</term>
					<term>interaction technique</term>
					<term>gesture. blutwurst</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we extrapolate the evolution of mobile devices in one specific direction, namely miniaturization. While we maintain the concept of a device that people are aware of and interact with intentionally, we envision that this concept can become small enough to allow invisible integration into arbitrary surfaces or human skin, and thus truly ubiquitous use. This outcome assumed, we investigate what technology would be most likely to provide the basis for these devices, what abilities such devices can be expected to have, and whether or not devices that size can still allow for meaningful interaction. We survey candidate technologies, drill down on gesture-based interaction, and demonstrate how it can be adapted to the desired form factors. While the resulting devices offer only the bare minimum in feedback and only the most basic interactions, we demonstrate that simple applications remain possible. We complete our exploration with two studies in which we investigate the affordance of these devices more concretely, namely marking and text entry using a gesture alphabet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In this paper, we investigate the limits of the miniaturization of mobile devices, researching what the smallest future devices might be, and how users would interact with them.</p><p>In the past, the miniaturization of mobile devices has progressed from notebook computers and PDAs to increasingly smaller devices, such as interactive watches <ref type="bibr" target="#b0">[1]</ref> and rings <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8]</ref>. At first sight, it might therefore appear that a continuing miniaturization process is destined to let devices shrink past any specific size limit. This is not the case, however. What constraints miniaturization is the devices' user interface hardware, the size of which is linked to human constraints. Screens, for example, have to be large enough to be seen, keyboards large enough to be typed on.</p><p>Since human constraints are largely constant (e.g., fat finger problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref>), arbitrary hardware miniaturization does not directly lead to arbitrarily small devices, but to devices the sizes and shapes of which are determined by their user interface hardware. For larger devices that has already happened: the size of current notebook computers is already largely determined by screen diagonal and keyboard size. As non-user interface hardware continues to shrink, this observation will apply to increasingly smaller devices.</p><p>Figure <ref type="figure">1</ref>: Disappearing mobile devices are too small to be held, so they have to be mounted, e.g. on the user's wrist. Here the user is entering a '2' by scanning two fingers. Depending on the desired degree of miniaturization, scanning causes it to perceive a sequence of gestures, marks, or just Morse code.</p><p>Exploring the miniaturization of future mobile devices therefore means to explore the miniaturization of user interface hardware, more specifically to examine how to strip devices of any user interface hardware that requires physical extent.</p><p>As a first step, this requires removing all interface hardware that is linked to the user's finger size, eyesight, or other human constraints. Instead, alternative approaches have to be used, such as gesture-based interaction, as previously explored by projects such as FlowMouse <ref type="bibr" target="#b45">[46]</ref> or Gesture Pendant <ref type="bibr" target="#b40">[41]</ref>. As a second step, we also need to remove all interface hardware the physical properties of which resist miniaturization. The miniaturization of gesture pendant, for example, is limited by the size of the optical path inside the camera and the necessity to illuminate a large space (here 36 infrared LEDs).</p><p>In this paper, we explore gesture interaction on the surface of the device as one path to ultimate miniaturization. Compared to other gesture-based approaches surface-based interaction requires only minimal illumination and can be accomplished with particularly small sensors. Based on this design, we reconsider the basics of gesture interaction.</p><p>The resulting "ultimately miniaturized" mobile devices adopt properties from wearable and ubiquitous computing: They can be worn and are always in reach; they offer only simple interactive capabilities and blend invisibly into any surface, almost like the sensors/actuators in smart rooms <ref type="bibr" target="#b1">[2]</ref>. However, the "infrastructure" the device blends into is the user's clothing or skin rather than a room (Figure <ref type="figure" target="#fig_0">2</ref>). Nonetheless, we envision these devices to still be "traditional" mobile devices in that they are operated intentionally (unlike ubicomp <ref type="bibr" target="#b42">[43]</ref>) and afford manual interaction.</p><p>Based on these properties and inspired by disappearing computers <ref type="bibr" target="#b35">[36]</ref> in ubicomp, we refer to these future devices as disappearing mobile devices. Today's technology does not allow us to create such devices "to scale". Neither can we anticipate in which particular order hardware components will shrink or what particular sequence of devices will result from the evolution of hardware. We therefore instead focus on analyzing candidate technologies with respect to their theoretical limits.</p><p>We then explore interaction techniques for such devices. While we recognize that output is equally important and challenging, we focus our exploration almost exclusively on input. We complete our exploration with two studies in which we investigate the affordance of these devices more concretely, namely interaction based on marking and text entry using a gesture alphabet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>This work builds on mobile and wearable computing as well as interaction with very small devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile and Wearable Computing</head><p>While mobile computing initially focused on hand-held devices, such as PDAs and phones (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>), continuous miniaturization has allowed researchers to start exploring devices small enough that they can be worn. The IBM Linux Watch <ref type="bibr" target="#b30">[31]</ref> runs Linux. IBM's digital jewelry consists of earrings, a pendant and a Trackpoint ring <ref type="bibr" target="#b27">[28]</ref>.</p><p>Telebeads <ref type="bibr" target="#b18">[19]</ref> are mobile mnemonic artifacts that allow teenagers to link individuals or groups with wearable objects such as handmade jewelry. Fukumoto's finger-ring shaped bone conduction HANDset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> allows users to issue fingertip tapping commands. So does Whisper <ref type="bibr" target="#b8">[9]</ref>. GestureWrist <ref type="bibr" target="#b35">[36]</ref>, worn like a wristband, supports forearm gestures. GesturePad hides under clothes <ref type="bibr" target="#b35">[36]</ref>.</p><p>Early work in wearable computing focused on augmented reality, in an attempt to reproduce desktop functionalities in a mobile form (e.g., smart clothing <ref type="bibr" target="#b25">[26]</ref>, augmented reality systems <ref type="bibr" target="#b5">[6]</ref>). Later research directed attention to low-level interaction techniques <ref type="bibr" target="#b26">[27]</ref>, and application scenarios of wearable computing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction with Very Small Devices</head><p>Miniaturization has resulted in many challenges for user interface research. With decreasing device size visual output becomes increasingly limited. Screens have to be small to fit the device, which eventually conflicts with the visual abilities of users <ref type="bibr" target="#b37">[38]</ref>. Consequently, some very small devices have abandoned screens entirely in favor of auditory (e.g., earPod <ref type="bibr" target="#b51">[52]</ref>) or tactile feedback <ref type="bibr" target="#b22">[23]</ref>. Mounting the device close to the user's ear (Figure <ref type="figure" target="#fig_0">2c</ref>) allows for effective auditory feedback from a small form factor and low power consumption.</p><p>Audio is an interesting alternative as well for input, e.g., in the form of speech recognition <ref type="bibr" target="#b39">[40]</ref>. The inherent volume of speech input can limit its applicability in situations where others are around <ref type="bibr" target="#b39">[40]</ref>.</p><p>Manual input faces similarly hard challenges. Text entry: Keypads with very few keys have been studied in research <ref type="bibr" target="#b24">[25]</ref>. Our work in this paper borrows from pen-based techniques, such as Unistroke <ref type="bibr" target="#b10">[11]</ref>, the more mnemonic Graffiti <ref type="bibr" target="#b23">[24]</ref>, and EdgeWrite <ref type="bibr" target="#b48">[49]</ref>, a unistroke-like language that offers extra supports for people with motor impairments. A trackball version uses target crossing <ref type="bibr" target="#b46">[47]</ref> instead.</p><p>Pointing: Tiny rate-controlled joysticks allow for compact form factors <ref type="bibr" target="#b27">[28]</ref>. Touchscreens, in contrast, offer position input, which is generally considered a more convenient and efficient way to point. Touch input, however, is complicated by the fat finger problem <ref type="bibr" target="#b41">[42]</ref>, which results from the softness of the user's fingertip and the occlusion of screen contents by the user's finger. The smaller the device the more severe the impact of the fat finger problem <ref type="bibr" target="#b0">[1]</ref>.</p><p>Styli have been used to alleviate this problem <ref type="bibr" target="#b49">[50]</ref>, as well as techniques that offset the finger with respect to the target (offset cursor <ref type="bibr" target="#b33">[34]</ref>) or the target with respect to the finger (shift <ref type="bibr" target="#b41">[42]</ref>). Even with shift, front-side touch fails for screen diagonals below 1" <ref type="bibr" target="#b0">[1]</ref>. Alternatives are pointing input next to the screen (SideSight <ref type="bibr" target="#b2">[3]</ref>) or on the device backside (e.g., LucidTouch <ref type="bibr" target="#b44">[45]</ref>, nanoTouch <ref type="bibr" target="#b0">[1]</ref>).</p><p>As mentioned in the introduction, freehand gesture input alleviates some of the size limitations by moving the interaction away from the surface into the space surrounding the device. Liess et al. present a 2D displacement motion sensor based on laser self-mixing <ref type="bibr" target="#b21">[22]</ref>. Gesture pendant <ref type="bibr" target="#b40">[41]</ref> uses a small infrared camera worn as a part of a necklace or pin to recognize various finger and hand gestures. Flow-Mouse <ref type="bibr" target="#b45">[46]</ref> uses a webcam to enable pointing, rotation, panning, and marking. Unlike earlier techniques, it increases reliability by using optical flow, rather than hand recognition. Gesture input in mid air faces three limitations that complicate miniaturization. First, it involves macroscopic cam-eras. Second, it requires substantial illumination <ref type="bibr" target="#b40">[41]</ref>. And third, it can make it hard for users to trigger discreet events, which are important for delimiting gesture languages <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HARDWARE FOR DISAPPEARING MOBILE DEVICES</head><p>As discussed earlier, camera-based gesture interaction is the starting point for our exploration of arbitrarily small devices, but it has limitations. In this section, we survey existing interface devices, discuss how they might fit into a disappearing device, and what modifications are necessary.</p><p>If a technology depends on human factors, such as finger size or eyesight, we scale it to the size of a disappearing mobile device, i.e., size zero. At this point, users cannot resolve the spatial properties of the hardware anymore. All interface elements that are usually spatially distinct, such as pixels, collapse into the same location. A screen of any resolution, for example, thereby becomes indistinguishable from a single pixel. A keyboard becomes functionally equivalent to a single button, as any number of buttons will always be pressed at the same time. We call the result, i.e., hardware with no inherent spatial properties monolithic.</p><p>In addition, we investigate technological constraints. How far a specific technology will be able to scale depends at least in part on industrial efforts. In some case, however, there are limits resulting from physical principles that can already be predicted today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Several output modalities make interesting candidates. A key requirement is that the device produces a signal strong enough to be perceivable by human senses. Mounting the device closer to the user's perceptual organs, however, generally reduces the required power and allows for further miniaturization (Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>When miniaturized, screens turn into a single pixel or LED (Figure <ref type="figure" target="#fig_0">2a</ref>). Depending on ambient lighting conditions, visual output can be more perceivable than audio or tactile. LEDs can also be manufactured at a very small scale. For example, the 1.0 x 0.8 x 0.2mm PicoLED built by Rohm (www.rohm.com) is claimed to be the world's tiniest diode. The 1.7 x 1.5 x 0.5mm LXCL-PWT1 from Luxeon (www.luxeon.com) is one of the smallest LED flashlights, which emits 26 lumens of light at 350mA.</p><p>Audio on a disappearing mobile device can offer only a single channel. Higher sound frequencies and lower volume allow for smaller membranes, which are required for miniaturization. Power requirements can be reduced further by mounting the device close to the user's ears or a bone connected to it (Figure <ref type="figure" target="#fig_0">2c</ref>) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Tactile feedback is similar in nature to auditory feedback in that lower frequency and stronger signal require a larger vibrating mass. Humans are generally less sensitive to touch than to sound, but tactile feedback may be viable if mounted directly onto a sensitive skin area (Figure <ref type="figure" target="#fig_0">2b</ref>).</p><p>In theory, other modalities could be used as well. That said, other skin perceptions, such as heat perception tend to be low in bandwidth. Smell and taste involve chemicals, making them difficult to handle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gesture and Touch Input</head><p>Since camera-based interaction is limited in terms of power requirements for the illuminant and physical size of the camera, we reduce power requirements by moving towards the surface. We primarily look at interactions that take place in direct physical contact with the device, i.e., variations of touch. At size zero, touch techniques collapse into the following three interaction styles.</p><p>Touch: In its simplest form, a device can tell whether it is being touched on not. Implementations include capacitive sensors and thresholded light sensors. Hudson <ref type="bibr" target="#b16">[17]</ref> demonstrated how to turn unmodified LED arrays into touch sensitive input devices. Other mechanisms are possible: microphones, for example, respond to touch as long as the hand stays in motion (scratching <ref type="bibr" target="#b12">[13]</ref>).</p><p>Pressure: Humans are able to operate physical buttons of about 1mm in diagonal. An interaction similar to physical buttons can be implemented using pressure sensors. Ramos et al. show that users can control up to six levels of pressure <ref type="bibr" target="#b34">[35]</ref>. Sensing pressure gets harder with decreasing device size, as the force that can be sensed is the product of pressure and surface. Another potential limitation is the fact that sensing pressure requires the device to be mounted on a hard surface in order to allow users to build up pressure.</p><p>Motion: Not all devices that sense motion continue to work when scaled to size zero. Capacitive touch pads, for example, turn into touch sensors and cannot track motion anymore. Friction imposes lower size limits on mechanical solutions such as trackballs. A wide range of devices can track optical flow. While they still use a camera, limiting the input to the immediate surface of the device reduces the need for illumination substantially. The Xybernaut Poma (www.xybernaut.com), for example, is basically an upside-down optical mouse (Figure <ref type="figure" target="#fig_6">7b</ref>). Even more power and space-efficient, laser mice use an infrared laser diode instead of an LED. By analyzing the laser speckle pattern they allow for simpler structure, higher accuracy, and capability of working on a wide range of surfaces <ref type="bibr" target="#b32">[33]</ref>. Thus, laser technology is a particularly interesting candidate for disappearing mobile devices.</p><p>Self-motion sensing using accelerometers (e.g., <ref type="bibr" target="#b15">[16]</ref>) is another applicable approach if the device can be mounted such that it can be moved freely (e.g., Figure <ref type="figure" target="#fig_0">2a</ref>, but not Figure <ref type="figure" target="#fig_0">2c</ref>) and if inadvertent activation can be avoided. Many other touch-based techniques, such as bending <ref type="bibr" target="#b38">[39]</ref> and multi-touch <ref type="bibr" target="#b44">[45]</ref> do not transfer to miniaturized hardware, because they require more than one contact point, and thus physical extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Because of the limitations of pressure, we focus on touch and motion. The more powerful choice is clearly motion sensing, but even with the reduced requirement for illumination, optical solutions do have elements that cannot be miniaturized arbitrarily, such as an array of optical sensors and optics that require a certain focal length (a Misumi MO-R803, for example, measures 4.4mm in diameter, www.misumi.com.tw).</p><p>Overall, devices that sense optical flow clearly have to be considered. However, for the ultimate limits of miniaturization, such as when designing devices to be implanted into human skin, we might have to rely on even smaller input devices, such as touch sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three Classes of Devices</head><p>Based on this discussion, we initially defined two specific models of monolithic input hardware.</p><p>A motion scanner consists of a touch sensor combined with a motion sensor. It can perceive gestures as well as a distinct out-of-range state.</p><p>A touch scanner consists of only a single touch sensor (of any underlying technology). It senses two states: touch and out-of-range.</p><p>Due to the limitations of the motion scanner, we introduced a third device of intermediate size and functionality.</p><p>A direction scanner consists of three very closely collocated touch sensors mounted in a non-collinear layout.</p><p>While the touch sensors are too close to be operated individually, directional gestures across the device result in run-time differences between the touch sensors, allowing it to sense the direction of gestures.</p><p>The reason we called them scanners is because of the particular interaction style they support. We discuss this interaction style in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT ON MONOLITHIC DEVICES: SCANNING</head><p>In order to allow users to interact with these three device types we need to offer a command language, i.e., a set of code words (commands and parameters) and delimiters <ref type="bibr" target="#b13">[14]</ref> that allow the device to parse the input stream into individual command and parameter tokens.</p><p>For each device type, we begin by discussing delimiters, a crucial element of any gesture language <ref type="bibr" target="#b13">[14]</ref>. Then we define other data types: Boolean, integer, float, and character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Input Language of Touch Scanners is Morse</head><p>On touch scanners, the only way to define a delimiter is by means of a timeout. Example for such timeout delimiters can be found in Morse code <ref type="bibr" target="#b28">[29]</ref>. Morse uses pauses of different lengths to delimit parts of a character (pause 1 unit), characters (3 units), and words (7 units). The tapping actions on Fukumoto's handset implement Morse <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Moving from there, a simple and coherent way to define characters/text on a touch scanner is to adopt Morse code as a whole; it defines characters a sequence of short and long signals. For other applications, we can redefine the meaning of individual codes, but the nature of the interaction remains equivalent to Morse code. a b Figure <ref type="figure">3</ref>: (a) Enter a "1" and (b) a "2" into a touch scanner using a "scanning" interaction.</p><p>Figure <ref type="figure">3</ref> illustrates an alternative of entering certain parameters that makes use of the specific properties of the device. Rather than tapping codes into the device, users can enter certain codes, such as the numbers from 1 to 4 by scanning a hand with the respective number of fingers across the device (Figure <ref type="figure">3</ref>). Note that this approach may allow scanning more complex objects, such as simplified punch cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Input Language of Direction Scanners is Marking</head><p>Direction scanners can do everything that touch scanners can. Any language defined for a touch scanner will therefore also run on a direction scanner. In addition, direction scanners support directional gestures, which multiplies the possibilities and enables higher bandwidth than Morse.</p><p>The choice of delimiter defines the language. If we define our delimiter to be lift-off (transition from touch to out-ofrange), we obtain 8 code words (N, NE, E…) and our language is marking <ref type="bibr" target="#b17">[18]</ref>. If we use a specific mark or a timeout as a delimiter, code words can be sequences of marks and our resulting language is simple marks <ref type="bibr" target="#b50">[51]</ref>. Distinguishing marks performed with different numbers of fingers (Figure <ref type="figure">3</ref>) offers a particularly effective way of entering simple mark words based on repetitive strokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Input Language of Motion Scanners is Unistroke</head><p>Motion scanners can do everything that directional scanners can. In addition, they recognize the path of a gesture.</p><p>The language of motion scanners is similar to direction scanners, yet the individual gestures are more powerful. Individual gestures can now, for example, be compound marks <ref type="bibr" target="#b50">[51]</ref> or character gestures from a unistroke-based alphabet, such as Jot <ref type="bibr" target="#b10">[11]</ref> or Graffiti <ref type="bibr" target="#b11">[12]</ref> (Figure <ref type="figure" target="#fig_1">4a</ref>). In theory, any gesture that can be completed on a touchscreen with a stylus or finger can also be completed on a motion scanner including continuous gestures, such as rubbing <ref type="bibr" target="#b31">[32]</ref> or dialing <ref type="bibr" target="#b29">[30]</ref> (Figure <ref type="figure" target="#fig_1">4b</ref>). However, there are several unusual properties that arise when applying these techniques to the hand-over-device concept of disappearing mobile devices. We discuss some of these particularities in the following, others in the user study sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Some of) the Particularities of Scanning</head><p>Scanning inherits from touch and gestures, and as a result, has several interesting properties.</p><p>Unlike gestures in mid-air, scanning allows getting out-ofrange very easily. The reason is that scanning takes place near the device surface, which means to get out-of-range, users only need to extend their gesture beyond the field of view of the sensor. As a result, out-of-range makes a great delimiter that mid-air gesturing does not offer.</p><p>On the other hand, the proximity of the user's fingers to the device also limits the amplitude of gestures. If we define out-of-range to function as a delimiter, moving the hand too far (even without lifting it) will accidentally uncover the sensor and thus commit the current gesture prematurely, generally resulting in an error.</p><p>Another way of looking at this is to consider scanning as "upside down" mouse input: during scanning, fingers do not act as the pointing device/mouse anymore, but as the mouse pad. In this new role, fingers have to provide enough surface area for the interaction to take place. Small fingers now limits the amplitude of the gestures, the same way that a small mouse pad limits the range of mouse input.</p><p>This effect is amplified by the fact that users cannot see the device while scanning, because it is occluded by the user's hand. This increases the risk of accidentally leaving the field of view of the sensor and delimiting prematurely.</p><p>In homage to the similarly motivated fat finger problem, we termed this problem small finger problem. One way of alleviating it is to scan with a larger object, such as an entire hand (Figure <ref type="figure">5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROTOTYPE DEVICES</head><p>In order to validate the concept of monolithic devices and scanning we have created simple prototypes of touch scanners (Figure <ref type="figure" target="#fig_4">6</ref>) and motion scanners (Figure <ref type="figure" target="#fig_6">7</ref>).  All prototypes use a PC for data processing. Input from the motion scanners comes in as pointing input. A mouse hook program intercepts the mouse move events, extracts relative mouse movements as dx and dy, and delivers them to the application. For the mouse version shown in Figure <ref type="figure" target="#fig_6">7a</ref>, input is mirrored left-right to account for the fact that the device is operated up-side-down.</p><p>Since none of these devices offers an explicit out-of-range state we used a timeout for that purpose, i.e., any period of at least 750ms without MouseMove events is considered out-of-range.</p><p>The laser track point in Figure <ref type="figure" target="#fig_6">7c</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Demo Applications</head><p>We have used these prototypes to implement a series of simple interactive demo applications running off a PC that illustrate the interaction model. Air condition control uses on a motion scanner with a single multi-color LED. Double-tapping unlocks the device. The LED responds by displaying its target temperature, such as a shade of blue for cold. Entering "h" turns it to heating and makes the LED turn light red. Marking "up" increases the temperature one notch at a time; marking "down" a few times or entering "c" sets it back to cold.</p><p>Audio player offers audio output and emulates some of the functionality of the iPod click wheel. Marking triggers main functions, including play, pause, and next track. Dialing as shown in Figure <ref type="figure" target="#fig_1">4b</ref> browses songs in a playlist. The Graffiti commands "a" adds a song to the play-now playlist. Entering "v" and dialing adjusts volume. Entering "s" and dialing scrubs within the current track. Modes return on timeout, confirmed with a sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDIES</head><p>The interaction techniques we adopted for disappearing mobile devices, such as marking and gesturing, were initially developed for a very different type of user interface hardware. When ported to a disappearing mobile device, these techniques undergo substantial changes. First, as discussed above, users now perform the techniques with the surface of the fingertip or hand, rather pen or mouse. Second, due to the occlusion of the device users are likely to commit prematurely as they accidentally gesture over the edge of the device. And third, there is no visual feedback while performing the gesture due to occlusion.</p><p>As a result, existing knowledge and performance data about these techniques is unlikely to translate to the new hardware interface and needs to be reestablished in the new context. To begin this process, we conducted a usability test on marking adapted to a disappearing mobile device, as well as a user study comparing two types of text entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USABILITY STUDY: MARKING ON A DISAPPEARING D.</head><p>The purpose of this study was to validate the effectiveness of the marking technique on direction/motion scanner devices and to identify problems and limitations. During each trial, participants entered one mark. Our goal was to determine the reliability of the technique and to identify potential usability issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interface and apparatus</head><p>The device used in the study was a simulated motion scanner. Because of its high tracking accuracy, we used the simplest of our three prototypes: the optical mouse (Logitech G5 high precision, 2000dpi). Participants held the device in their left hand, and entered marks with their right hand's index finger. The device was connected to a PC running Windows XP. The software was written in Visual C#.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>During each trial, participants performed a selection from an 8-item marking menu. At the beginning of each trial, a screen showed participants which item to select. To simulate expert performance, we used the descriptive terms "up", "up-right", "right", etc. for menu names. Participants selected the respective item by swiping their index finger in the corresponding direction over the device.</p><p>As described above, marking gestures were delimited by a 750ms timeout. Each trial was timed from the moment an item was presented to the participant until the timeout completed. If an incorrect item was selected, an error was recorded. Error trials were not repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Before the study, participants were educated about the marking concept. For that purpose, a marking menu with the traditional 8-octant visuals and a cursor trail was shown on a connected computer screen. Users performed 20 practice trials using the device under this visual feedback. After completion of the practice trials, visual feedback was turned off, so all timed trials were performed without feed-back. Overall, each participant performed 80 timed trials: 10 for each menu item in randomized order. Overall the study took approximately 15 minutes per participant. Participants 12 participants (7 male) aged between 26 and 38 were recruited from our local institute. All were right-handed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>On average, trials took 1736ms including the 750ms timeout (SD = 289.55ms) to complete. The average error rate across all marking directions was 4.8% (46 errors out of 960 trials combined across all users).  Broken down by direction, error rates ranged from 2.5% for "up" to 6.7% for "left" (Figure <ref type="figure" target="#fig_8">8</ref>). A one-way Repeated-Measures ANOVA did not find a significant effect of marking direction on error rate (F 7, 77 =.516, p = .82).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observations about error rates</head><p>Observation of participants revealed two factors that caused erroneous selections.</p><p>First, errors resulted from a biased orientation of the device. As illustrated by Figure <ref type="figure" target="#fig_9">9a</ref>, users would occasionally rotate the device as they were holding it in their nondominant hands. A correctly aimed mark thereby ended up in a neighboring sector. Second, errors resulted when participants inadvertently stayed in the range of the sensor after the gesture was presumably complete (Figure <ref type="figure" target="#fig_9">9b</ref>). The fact that fingers are thinner than long played a role in this. When held as shown in Figure <ref type="figure" target="#fig_9">9b</ref>, participants' fingers cleared the sensor quickly and reliably for "down" and "up" movements. When marking "left" (reversed for left-handed users), in contrast , participants either had to twist their hand in order to get It out of the way or they had to lift their hand off the device in order to get out of the range of the sensor. When participants did not lift their hand far enough off the device, the device continued to track and an error resulted. This effect that "up" and "down" gestures are easier to perform than others is reinforced by the fact that it is easier to pivot around the elbow than orthogonal to it (see discussion around bottom-left placement of the Lagoon in Alias Sketchbook <ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our usability study provided the following two insights:</p><p>Reference system: Traditional marking devices, such as tablet computers, offer a visible reference system; the frame of the screen itself keeps users updated on what physical direction corresponds to "up". The mouse-based prototype used in the study offered only a weak frame of reference; a true disappearing mobile devices will lack such a frame of reference altogether. Our observations, however, suggest that adding a frame of reference can reduce error. One approach is to mount devices at places that are surrounded by features that can serve as landmarks, e.g., the users arm in the case of a wrist-mounted device.</p><p>Asymmetries of the finger: not all gestures are equally reliable to perform. When designing a gesture language, easy gestures such as "down" should be used preferably, while hard gestures, such as "left" (mirrored in the case of lefthanded use) should be avoided. However, triggering the out-of-range state explicitly using an additional touch sensor can most likely reduce this problem substantially.</p><p>Despite these two usability issues, error rates below 5% suggest that marking is a sound input technique for disappearing mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDY: UNISTROKE ON DISAPPEARING DEVICE</head><p>The purpose of this user study was to investigate text entry on motion scanners. As discussed earlier, motion scanners afford the entry of sequence of gestures and thus unistrokebased alphabets. In this study, we compared two unistroke alphabets, Graffiti and EdgeWrite, and evaluated their suitability for use with disappearing mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interfaces</head><p>There were two interface conditions.</p><p>The Graffiti condition was implemented using an open source Graffiti recognizer <ref type="bibr" target="#b14">[15]</ref>. Its alphabet differs slightly from the original version by Palm Inc. in that the letters 'B' and 'E' use a lowercase mnemonic, and 'D', 'G', 'P', and 'Q' each use one of the alternate writings also supported by Palm's Graffiti <ref type="bibr" target="#b11">[12]</ref>. The alphabet used in the study is reproduced in Figure <ref type="figure" target="#fig_10">10</ref>. The EdgeWrite condition was implemented using the original EdgeWrite program for mouse and trackballs <ref type="bibr" target="#b46">[47]</ref>. Figure <ref type="figure" target="#fig_11">11</ref> shows the supported characters. We configured EdgeWrite with 45-pixel radius, 60° diagonals, and 750ms timeout. All interface conditions were run on the same mouse-based device used in the marking study. All input was mirrored left-right to account for the fact that the device that was operated on its back. Neither of the alphabets distinguished between upper and lower case characters.</p><p>Unlike the marking study, participants operated the device with their entire hand. In a pilot study, we had asked participants to enter text with a single finger only and found very high (&gt; 40%) error rates. This had clearly resulted from the small-finger problem discussed earlier. Switching to the full hand eliminated the problem.</p><p>In addition, participants wore a glove as shown in Figure <ref type="figure" target="#fig_13">12</ref>. Piloting had revealed that the ridges in participants' palms impact tracking. This is caused by the fact that the focal range of commercial mice is deliberately cropped in order to minimize tracking errors during clutching. The correct solution is to use an optical sensor without the respective firmware modification. As a fast workaround, the glove solved the problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>We used the same apparatus as in the Marking study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>A different set of 24 volunteers (19 males) aged between 21 and 41 participated. Three had previous experience with Graffiti on PDAs, but none had used EdgeWrite before. All were right-handed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Participants entered one character per trial. Participants were presented with the character and then entered it using the respective interface. Graffiti gestures were delimited by a 1500ms timeout; EdgeWrite used a 750ms timeout. Both were tuned carefully in a pilot study to ensure an optimal recognition rate. If the character was not recognized or recognized as a different character, an error was recorded. Error trials were not repeated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>We used a between-subjects design, i.e., 12 randomly assigned participants used the Graffiti interface, while the other 12 used the EdgeWrite interface. The betweensubjects design allowed us to keep the study duration reasonably low (40min) by requiring each participant to learn only one of the two alphabets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Before the study, participants received a demonstration and then practiced each character at least 10 times. As in the previous study, visual feedback was provided during training, but not during timed trials. To keep training time within reasonable bounds, a printed copy of the two alphabets (Figure <ref type="figure" target="#fig_10">10</ref>  We recorded error rate, but not task time. Piloting had revealed substantial differences in learning rates and prior experience and as a result some participants required the sheet while others managed to perform the task without. This impacted task time to a point where we chose not to consider task time in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis</head><p>Based on the results of comparable studies with trackball <ref type="bibr" target="#b46">[47]</ref> and stylus devices <ref type="bibr" target="#b48">[49]</ref>, we expected the error rate of the EdgeWrite condition to be lower than for the Graffiti condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A total of 4992 data points were collected (2496 per technique).</p><p>An independent t-test found a significant difference in error rate (t 22 = 2.283, p = .032). The error rate of the EdgeWrite condition (5.2%, 131 errors, SD 1.35%) was lower than that of the Graffiti condition (6.9%, 172 errors, SD 1.92%). This supported our hypothesis.</p><p>Subjective ratings of learnability showed the same trend. On a five-item Likert scale (with higher values being better), the learnability of the EdgeWrite condition was rated 3.08 (SD = .793), while Graffiti's learnability was rated only 2.42 (SD = 1.165). EdgeWrite's usability was rated 3.67 (SD = .888), which is comparable to Graffiti's usability 3.58 (SD = .996). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results by character</head><p>Figure <ref type="figure" target="#fig_15">13</ref> shows the breakdown of error rate by character. For both interfaces, error rates differed widely between characters. In the graffiti condition error rates were particularly unbalanced, with 'D', 'G', 'O', 'Q', and 'R' together accounting for 66.3% of all errors (114 out of 172).</p><p>Figure <ref type="figure" target="#fig_16">14</ref> shows examples of misrecognized characters of these five characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Following the study, we analyzed our logs to investigate why these Graffiti characters had performed poorly during the study. When the Graffiti recognizer evaluates a stroke, if uses two types of features: (a) the presence of local features, such as corners and (b) the relative position between stroke segments, such as whether two strokes intersect or not. For the top-five most error-prone characters, recognition relies heavily on such relative position features (Figure <ref type="figure" target="#fig_15">13</ref>). Most of the characters with low error rates, in contrast, do not rely on relative position features.</p><p>On closer inspection, this is not surprising. Both gesture languages were designed for devices that visualize the stroke as it is drawn, which makes it easy to align later stroke segments with segments already on the screen. This type of feedback, however, is missing on disappearing mobile devices, which resulted in increased error rated for gestures relying on this type of feature.</p><p>A possible approach to overcoming the lack of visual feedback is to offer a haptic reference. On stylus-based devices, the user's palm can serve as such a reference; on a disappearing mobile device, however, the palm is in motion. We found that complementing the device with a small tactile "dot", similar to the tactile dots on the F and J keys of a QWERTY keyboard can serve as such a haptic reference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The error rate of EdgeWrite was consistent with results reported in <ref type="bibr" target="#b47">[48]</ref>, indicating that text entry with EdgeWrite on a disappearing mobile device is not only possible, but competitive with larger devices.</p><p>The error rate of Graffiti, in contrast, was roughly twice as high as Graffiti run on a pen-based computer (e.g., 3% error rate in <ref type="bibr" target="#b23">[24]</ref>). The characters responsible for this performance loss did poorly because they relied on relative position features, which are hard to control on a disappearing mobile device. These findings suggest that gesture languages on disappearing mobile devices need to be (re)designed carefully, so as to be either more forgiving with misaligned features or to instead use gestures that do not rely on relative positions.</p><p>The characters from the EdgeWrite condition, in contrast, do not rely on relative positions the way Graffiti characters do. This seems to be a likely explanation for why the EdgeWrite condition performed well on our prototype device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLICATIONS FOR DESIGN</head><p>Based on the two studies presented above we have gained first insights into the characteristics of disappearing mobile devices. Here is a summary of the design implications:</p><p>1. Use the entire hand for input to allow for larger motion amplitude. It reduces error with complex gestures.</p><p>2. Preferably use marks that do not suffer from the incomplete lift-off problem, such as "down" and "up". Ideally, use a dedicated a touch sensor to implement an explicit out-of-range state.</p><p>3. Preferably use unistroke gestures that do not rely on the correct recognition of relative position features.</p><p>4. If orientation matters, as is the case for marking, provide a clear reference frame, such as physical features located close to the device. 5. To avoid premature commit, provide a tactile feature that tells users when the device is getting near the edge of their hand, e.g., a small tactile dot.</p><p>6. Design devices, such that they glance over irregularities and gaps between fingers. However, limit the focal range to prevent motion past lift-off from being recognized as a gesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION AND FUTURE WORK</head><p>In this paper, we investigated the evolution of mobile devices in one specific direction, namely miniaturization. We investigated what technology are most likely to provide the basis for such devices, what abilities they can be expected to have, and what interaction models can be ported to these devices.</p><p>Our main research question was to determine what abilities such devices can be expected to have and whether devices that size can still allow for meaningful interaction. We found that the input capabilities of such disappearing mobile devices are surprisingly comprehensive, allowing for comparably efficient and reliable selection of commands, as well as text entry. Despite their size, the proposed devices do offer an input vocabulary sufficing for a whole range of current consumer devices, such as audio players or potentially even more complex communication devices. The smallness of these devices and their ability to blend into the users' clothing or maybe even their skin will allow for truly ubiquitous use.</p><p>As future work, we plan on exploring visual output on disappearing mobile devices, such as single-pixel displays.</p><p>Based on this, we plan on creating self-contained prototypes and studying them in real-world tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of disappearing mobile devices that are worn or implanted (a) in the wrist, providing visual feedback, (b) on the finger, providing tactile feedback, and (c) in the earlobe, providing auditory feedback.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Entering the Graffiti character "A" on a motion scanner. (b) Entering a real-value parameter using dialing. Using the hand instead of a finger allows for larger gesture amplitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>small finger problem: (a) Scanning with a single finger offers only limited range for gestures. (b) This range can be increased by scanning with the entire hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Touch scanner prototype based on a Phidgets light sensor board connected to a PC. To simulate the notion of a disappearing mobile device, we hide the device under the sleeve of a sweatshirt; a 1mm hole exposes the sensor.</figDesc><graphic coords="5,66.06,507.78,215.82,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is a research prototype from the Hardware group at Microsoft, courtesy of John Lutian. It is particularly optimized for tracking human skin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Three prototypes we used to simulate a motion scanner device: (a) Optical mouse on its back, (b) Xybernaut Poma, and (c) laser track point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Errors for marks of the respective direction (y-axis is overall number of errors across all users).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Errors resulted from (a) inadvertent rotation of the device and (b) incomplete lift-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The gesture alphabet supported by the graffiti condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The primary letter forms of EdgeWrite [49, 47]. Alternative forms exist for most characters (not shown).</figDesc><graphic coords="7,328.14,115.50,219.72,157.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: A glove served as a work around to overcome the limited focal range of our device.</figDesc><graphic coords="7,56.76,628.08,234.36,80.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>and Figure 11) was visible during timed trials. Training took about 20 minutes on average. Each participant then performed 8 blocks, during each of which the complete set of 26 alphabetic characters was presented in random order. Finally, participants filled in a questionnaire. Overall, the study took approximately 40 minutes per participants, including training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Number of errors (out of 96 entries per technique) recorded for each Roman letter.</figDesc><graphic coords="8,319.14,138.48,237.78,153.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Examples of gestures from the study that were misrecognized because participants (a) undershot or (b) overshot. DP means "D" was presented and the participant's stroke was recognized as P. Green/red overdrawn strokes highlight the missing/superfluous part that caused the error.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank John Lutian for providing us with his remarkable laser track point prototype, Jake Wobbrock for sharing the original EdgeWrite code with us, and Ken Hinckley and Gerry Chu for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Back-of-device interaction allows creating very small touch devices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;09</title>
		<meeting>CHI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1923" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The New EasyLiving Project at Microsoft Research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brumitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint DARPA/NIST Smart Spaces Workshop &apos;98</title>
		<meeting>Joint DARPA/NIST Smart Spaces Workshop &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SideSight: multi-&quot;touch&quot; interaction around small devices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;08</title>
		<meeting>UIST &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ShapeTouch: Leveraging Contact Shape on Interactive Surfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Tabletop&apos;08</title>
		<meeting>Tabletop&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Killer App&quot; of wearable computing: wireless force sensing body protectors for martial arts</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;04</title>
		<meeting>UIST &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="277" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge-based augmented reality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seligmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="1993-07">1993. Jul. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieké</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurtenbach</surname></persName>
		</author>
		<title level="m">Tracking menus. Proc. UIST&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A finger-ring shaped wearable handset based on bone-conduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fukumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISWC &apos;05</title>
		<meeting>ISWC &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Whisper: a wristwatch style wearable handset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tonomura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;99</title>
		<meeting>CHI&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Body-coupled Fin-gerRing: wireless wearable keyboard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tonomura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Touch-typing with a stylus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERCHI &apos;93</title>
		<meeting>INTERCHI &apos;93</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Graffiti_(Palm_OS)" />
		<title level="m">Graffiti</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scratch input: creating large, inexpensive unpowered and mobile finger input surfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;08</title>
		<meeting>UIST &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design and analysis of delimiters for selection-action pen gesture phrases in scriboli</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guimbretiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;05</title>
		<meeting>CHI &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Julien Couvreur&apos;s programming blog</title>
		<ptr target="http://blog.monstuff.com/archives/000012.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synchronous gestures for multiple persons and computers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;03</title>
		<meeting>UIST &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using light emitting diode arrays as touchsensitive input and output devices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST&apos;04</title>
		<meeting>UIST&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="287" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User learning and performance with marking menus</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurtenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;94</title>
		<meeting>CHI &apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Telebeads: social network mnemonics for teenagers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Labrune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Interaction design and children&apos;06</title>
		<meeting>Conference on Interaction design and children&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lawo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Witt</surname></persName>
		</author>
		<title level="m">Using wearable computing in real-world applications. CHI &apos;08 Extended Abstracts</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3687" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blindsight: eyes-free access to mobile phones</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;08</title>
		<meeting>CHI &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A miniaturized multidirectional optical motion sensor and input device based on laser self-mixing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weijers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heinks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Horst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duijve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mimnagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2001" to="2006" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A role for haptics in mobile interaction: initial design using a handheld tactile display prototype</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pasquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hayward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;06</title>
		<meeting>CHI &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The immediate usability of graffiti</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Graphics Interface &apos;97</title>
		<meeting>Graphics Interface &apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text entry with a small number of buttons</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text entry systems: Mobility, accessibility, universality</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Tanaka-Ishii</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<title level="m">Smart clothing&quot;: wearable multimedia computing and &quot;personal imaging&quot; to restore the balance between people and their intelligent environments. Proc. Multimedia &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A wearable computer for use in microgravity space and other nondesktop environments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;96</title>
		<meeting>CHI &apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="69" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Digital jewelry: wearable technology for everyday life. CHI &apos;01 Extended Abstracts</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="45" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/Morse_code" />
		<title level="m">Morse code</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Navigating documents with the virtual scroll ring</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moscovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;04</title>
		<meeting>UIST &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IBM&apos;s linux watch: the challenge of miniaturization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kamijoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkiteswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guniguntala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2002-01">2002. January, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rubbing and tapping for precise and rapid selection on touch-screen displays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Olwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;08</title>
		<meeting>CHI &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A laser speckle pattern technique for designing an optical computer mouse</title>
		<author>
			<persName><forename type="first">P</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pulov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pulov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt Laser Eng</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="21" to="26" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving the accuracy of touch screens: an experimental evaluation of three strategies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weldon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;88</title>
		<meeting>CHI &apos;88</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<title level="m">Pressure widgets. Proc. CHI &apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISWC &apos;01</title>
		<meeting>ISWC &apos;01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building disappearing computers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Streitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="2005-03">2005. Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A survey of display device properties and visual acuity for visualization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
		<idno>TR-2005- 32</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>North Carolina State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gummi: a bendable computer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schwesig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;04</title>
		<meeting>CHI &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The role of speech input in wearable computing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="89" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The gesture pendant: a self-illuminating, wearable, infrared computer vision system for home automation control and medical monitoring</title>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Auxier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ashbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Wearable Computing &apos;00</title>
		<meeting>International Symposium on Wearable Computing &apos;00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shift: A Technique for Operating Pen-Based Interfaces Using Touch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;07</title>
		<meeting>CHI&apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Computer for the 21st Century</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American Special Issue on Communications, Computers, and Networks</title>
		<imprint>
			<date type="published" when="1991-09">1991. September, 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unitedpulse: feeling your partner&apos;s pulse</title>
		<author>
			<persName><forename type="first">J</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wettach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hornecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MobileHCI &apos;08</title>
		<meeting>MobileHCI &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="535" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LucidTouch: a see-through mobile device</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Forlines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barnwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;07</title>
		<meeting>UIST &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">FlowMouse: a computer vision-based pointing and gesture input device</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERACT&apos;05</title>
		<meeting>INTERACT&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="565" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Trackball text entry for people with motor impairments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;06</title>
		<meeting>CHI &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<title level="m">Gestural text entry on multiple devices. Proc. Assets &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="184" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">EdgeWrite: a stylus-based text entry method designed for high-accuracy and stability of motion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kembel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;03</title>
		<meeting>UIST &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An evaluation of stylusbased text entry methods on handheld devices in stationary and mobile settings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MobileHCI &apos;07</title>
		<meeting>MobileHCI &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Simple vs. compound mark hierarchical marking menus</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;04</title>
		<meeting>UIST &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Earpod: eyes-free menu selection using touch input and reactive audio feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chignell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI &apos;07</title>
		<meeting>CHI &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1395" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
