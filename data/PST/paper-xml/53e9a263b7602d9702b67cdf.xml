<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ranking Robustness: A Novel Framework to Predict Query Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yun</forename><surname>Zhou</surname></persName>
							<email>yzhou@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
							<email>croft@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ranking Robustness: A Novel Framework to Predict Query Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F05E55E005E65A3482DABD4502C5ADBC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Query formulation Algorithms</term>
					<term>Experimentation</term>
					<term>Theory Ranking robustness</term>
					<term>query performance prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. We demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of TREC test collections including the GOV2 collection. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that the robustness score performs better than or at least as good as the clarity score. We find that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also notice that a combination of the two usually results in more prediction power.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In a typical retrieval system, a user forms a query according to his information need and a number of documents (usually in the form of a ranked list) are presented to the user by the retrieval system in response to the query. Query performance prediction refers to the process of estimating the quality of the output of a retrieval system in response to a user's query without any relevance information. Compared to the long history of developing sophisticated retrieval models for improving performance in IR, research on predicting query performance is still in its early stage. However, researchers have started to realize the importance of this problem and a number of new methods have been proposed for prediction recently <ref type="bibr" target="#b1">[1]</ref>. The ability to predict query performance has the potential of a fundamental impact both on the user and the retrieval system.</p><p>From the perspective of a user, performance prediction provides valuable feedback that can be used to direct a search. For example, when the retrieved documents are estimated to be of low quality, the user may rephrase his query or be more willing to cooperate with the system to improve retrieval effectiveness, such as providing relevance feedback. With the help of prediction, the user can quickly form a good query to acquire satisfying results for his information need. Otherwise, the user must spend time reading the returned documents to rewrite the query when the results for the initial query are not satisfactory.</p><p>On the other hand, from the perspective of a retrieval system, performance prediction is the first step at solving the crucial problem of retrieval consistency. Current retrieval systems are evaluated by the average effectiveness on a fixed set of queries. Although failures on a small number of queries may not have a significant effect on average performance, users who are interested in these queries are unlikely to be tolerant of this kind of deficiency. A reliable system that always produces acceptable retrieval performance is more preferred by users than another system that works extremely well on a number of queries but occasionally makes terrible mistakes. To improve the consistency of retrieval systems, we first need to distinguish poorlyperforming queries by performance prediction techniques. The important role of performance prediction in improving retrieval consistency has been recognized by the IR community. For example, in 2003, the Robust Track <ref type="bibr" target="#b2">[2]</ref> was proposed by TREC which addresses the problem of enhancing the retrieval of poorlyperforming queries. As the first footprint in finding a solution to this problem, the Robust Track requires systems to rank the queries by predicted effectiveness to investigate the capabilities of systems to detect hard queries <ref type="bibr" target="#b1">[1]</ref>.</p><p>In this paper, we develop a method for predicting query performance by computing ranking robustness which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. The idea of predicting retrieval performance by measuring ranking robustness is inspired by a general observation in noisy data retrieval that the degree of ranking robustness against noise is positively correlated with retrieval performance. Regular documents also contain "noise" if we interpret noise as uncertainty. We propose a statistical measure called the robustness score to quantify the notion of ranking robustness. We demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of TREC test collections including the GOV2 collection. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CIKM'06, <ref type="bibr">November 5-11, 2006</ref>, Arlington, Virginia, USA. Copyright 2006 ACM 1-59593-433-2/06/0011…$5.00.</p><p>Our experimental results show that the robustness score performs better than or at least as good as the clarity score.</p><p>The rest of this paper is organized as follows. Section 2 describes related work. In section 3, we propose a statistical measure called the robustness score to quantify the notion of ranking robustness. In section 4, we present our evaluations that show the effectiveness of our approach. In section 5, we summarize the main conclusions of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK 2.1 Query Performance Prediction</head><p>Prediction of query performance has long been of interest in information retrieval and has been investigated under different names such as query-difficulty or query-ambiguity. Query prediction is a challenging task as shown in <ref type="bibr" target="#b1">[1]</ref> and <ref type="bibr" target="#b3">[3]</ref>. Some of the first success at addressing this task was demonstrated by the clarity score method proposed in <ref type="bibr" target="#b4">[4]</ref>. Since then, the clarity measure has been the state-of-the-art technique. At the time of writing this paper, we know of no published work that has claimed to achieve the prediction accuracy comparable to or better than the clarity score across a variety of test collections.</p><p>Recently, a number of prediction methods have been tried since the introduction of the TREC Robust Track in 2003. In the Robust Track systems are required to rank the queries by predicted performance, with the goal of utilizing the prediction capability to do query-specific processing. Generally speaking, these methods extract features of retrieval and compute the performance score for each query by using the features to estimate the query performance. One way to measure the quality of the performance prediction methods is to compare the rankings of queries based on their actual precision (such as MAP) with the rankings of the same queries ranked by their performance scores (that is, predicted precision). Based on whether training data are needed when computing the performance scores, these methods can be classified into two groups: one that does not need training data and one that does. Our approach that will be introduced in Section 3 is in the first group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category I: Does Not Need Training Data</head><p>In this category, no training data are required when predicting query performance. Our method that will be introduced in section 3 belongs to this category. Some researchers have used IDF-related (inverse document frequency) features as predictors. For example, Tomlinson et al. <ref type="bibr" target="#b5">[5]</ref> adopted the weighted average IDF of the query terms for predicting. He and Ounis <ref type="bibr" target="#b6">[6]</ref> proposed a predictor based on the standard deviation of the IDF of the query terms. Plachouras <ref type="bibr">[7]</ref> represented the quality of a query term by Kwok's inverse collection term frequency. The above IDF-based predictors showed some moderate correlation with query performance. These predictors are easy to compute but they do not take the retrieval algorithms into account and thus are unlikely to predict query performance well.</p><p>Inspired by the success of the clarity score, some researcher have proposed methods that are related to the ideas in the clarity score technique. Amati <ref type="bibr" target="#b8">[8]</ref> proposed to use the KL-divergence between a query term's frequency in the top retrieved documents and the frequency in the whole collection, which is very similar to the definition of the clarity score. He and Ounis <ref type="bibr" target="#b6">[6]</ref> proposed a simplified version of the clarity score where the query model is estimated by the term frequency in the query. Motivated by the observation that the clarity score indicates the specificity of a query, they <ref type="bibr" target="#b6">[6]</ref> also proposed the notion of the query scope, which is quantified as the percentage of documents that contain at least one query term in the collection. Diaz and Jones <ref type="bibr" target="#b9">[9]</ref> extended clarity scores to include time features. They showed that using these time features together with clarity scores improves prediction.</p><p>Kwok et al. <ref type="bibr" target="#b10">[10]</ref> suggests predicting query performance by retrieved document similarity. The basic idea is that when relevant documents occupy the top ranking positions, the similarity between top retrieved documents should be high, based on the assumption that relevant documents are similar to each other. While this idea is interesting, preliminary results are not very promising.</p><p>Bernstein et al. <ref type="bibr" target="#b11">[11]</ref> estimate the prior probability of each document that will be retrieved by the retrieval system. For a given query, they compare the ranking of documents based on the prior probabilities to the ranking of documents returned from the retrieval system. They hypothesize that if the two ranking are similar, the query will be difficult since the query does not have strong discriminating power. Their results show some limited indication of query performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category II: Needs Training Data</head><p>Elad Yom-Tov et al. <ref type="bibr" target="#b12">[12]</ref> proposed a histogram-based predictor and a decision tree based predictor. The features used in their models were the document frequency of query terms and the overlap of top retrieval results between using the full query and the individual query terms. Their idea was that well-performing queries tend to agree on most of the retrieved documents. They reported promising prediction results and showed that their methods were more precise than those used in <ref type="bibr" target="#b13">[13]</ref>[7] <ref type="bibr" target="#b5">[5]</ref>.</p><p>Kwok et al. <ref type="bibr" target="#b13">[13]</ref> built a query predictor using support vector regression. For features, they chose the best three terms in each query and used their log document frequency and their corresponding frequencies in the query. They also included the number of top retrieved documents that contain some or all query terms as a feature. They observed a small correlation between predicted and actual query performance.</p><p>Using visual features, such as titles and snippets, from a surrogate document representation of retrieved documents, Jensen et al. <ref type="bibr" target="#b14">[14]</ref> trained a regression model with manually labeled queries to predict precision at the top 10 documents (P@10) in the Web search. They reported moderate correlation with P@10.</p><p>We point out that there kinds of predictors may highly depend on the amount and characteristics of available training data if the prediction methods do not generalize well and have to be retrained often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Retrieval on Noisy Data</head><p>With regard to text document collections in information retrieval, it is often convenient to assume that the contents of the collections are clean and free of errors. With the advent of large collections of multimedia documents (such as audio or image document), techniques such as OCR (optical character recognition) or ASR (automatic speech recognition) have been widely used to extract text from multimedia archives. In the following description, the text output of a recognition process applied to multimedia documents is noisy data or corrupted data since the recognition process is error prone and brings significant levels of noise to the data. The recognition process that produces corrupted data is data corruption.</p><p>One of the core problems in the field of information retrieval on corrupted data is to explore the impact of data corruption on retrieval effectiveness in order to design a ranking function that is robust to unexpected errors in corrupted data. Here a robust retrieval model means that some changes in document or collection statistics caused by data corruption do not alter the retrieval results much compared to retrieval on perfect documents (that is, the results of a recognition process with 100% accuracy).</p><p>A general observation about experiments on investigating the effects of data corruption is that as retrieval effectiveness improves, the ranking function becomes more robust against data corruption. For example, Lopresti and Zhou <ref type="bibr" target="#b15">[15]</ref> explored the effectiveness of three retrieval functions on simulated OCR noisy data. They found that the ranking of the three functions with respect to retrieval effectiveness is the same as their ranking with respect to their ability to deal with simulated noise. Another example is that Singhal, Salton and Buckley <ref type="bibr" target="#b16">[16]</ref> proposed a new robust length normalization method to alleviate the problem that the regular cosine normalization is sensitive to OCR errors. Although the original motivation for this technique was to deal with OCR data corruption, surprisingly they found that the new normalization scheme also brought significant improvements on correct text collections in comparison to the original cosine normalization. Moreover, Mittendorf <ref type="bibr" target="#b17">[17]</ref> studied data corruption effects on retrieval and presented a theorem on ranking robustness that partially explained the phenomenon that retrieval performance on corrupted data is often correlated with the degree of resilience against noise.</p><p>The above work reveals the interesting relationship between ranking robustness and retrieval performance. Although this work was done in the context of retrieval on noisy data, clean documents in regular retrieval also contain "noise" if we interpret noise as uncertainty. In the remaining of this paper, we will propose a framework to quantify ranking robustness and show its correlation with query performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MEASURE RANKING ROBUSTNESS</head><p>The notion of ranking robustness originates in the field of noisy data retrieval, where retrieval is performed on the output of a recognition process that exacts text from multimedia archives. Ranking robustness in noisy data retrieval refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of noise brought by the recognition process. Note that clean documents also contain "noise" if we generalize the notion of noise from recognition errors to uncertainty in text documents. For example, the meaning of a document may remain the same even after adding or deleting some words. Synonymy and homonymy are another two popular examples that can bring uncertainty to clean text documents. Therefore, we can extend the notion of ranking robustness to regular ad hoc document retrieval. In essence, ranking robustness reflects the ability of a retrieval system to handle uncertainty. The idea of predicting retrieval performance by measuring ranking robustness is inspired by a general observation in noisy data retrieval that the degree of ranking robustness against noise is positively correlated with retrieval performance. We hypothesize that when it comes to regular retrieval, the correlation between robustness and performance still holds. Our hypothesis will be thoroughly examined in the next section.</p><p>Next we describe our way of measuring ranking robustness in regular retrieval. We begin by considering how to calculate ranking robustness in noisy data retrieval. If we can acquire a clean version of the corrupted data, one straightforward way is to compare a ranked document list from the corrupted collection to the corresponding ranked list from the perfect collection using the same query and ranking function. With regard to regular document retrieval, usually documents are assumed to be free of corruption. To simulate data corruption, we assume that there exists a noisy channel which is analogous to the recognition process in noisy data retrieval. Documents are corrupted after going thought the channel. One way to implement the noisy channel is to design a document model for each document (Document models are distributions over words or other linguistic units). One corrupted version of the original document is one random sample from the corresponding document model.</p><p>Specifically, suppose we have query Q, ranking function G and collection C. We generate corrupted collection C' by sampling from the document models of the documents in C. Then we perform retrieval on both C and C' and two ranked list L and L' are returned respectively. Finally we compute the similarity between the two rankings. Note that L is a fixed ranked list while L' is a random variable. We call the expected similarity between L and L' the robustness score and use it to measure ranking robustness. This process is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Let us formally define the robustness score. Consider query Q and a document collection of M documents C=(D 1 ,D 2 ,…D M ). Let V denote the size of vocabulary, both query Q and the documents are represented as vectors of indexed term counts, that is,</p><formula xml:id="formula_0">Q=(q 1 ,q 2 ,…q V )∈ N V D k =(D k,1 ,D k,2 ,…D k,V ) ∈N V</formula><p>where D k,i is the number of times that term i appears in document D k and q j is the number of times that term j appears in query Q. N denotes nonnegative integer and N V denotes a V-dimension vector space of nonnegative integer. Under our representation, collection C is a M×V matrix with nonnegative integer entries, that is, C∈ S(M×V), where S(M×V) denotes the set of a M×V matrix with nonnegative integer entries . The rows of matrix C can be viewed as a set of documents represented by V-dimension vectors.</p><p>We introduce a few definitions before we show the computation of the robustness score. We assume that document D k , k∈ <ref type="bibr">[1,M]</ref>, corresponds to document model X k which is a V-dimension multivariate distribution and can be represented by a random vector ,</p><formula xml:id="formula_1">Definition 1: Retrieval Function G(D,Q) retrieval function G(D,Q) maps query Q and document D into a real number, that is , G(D,Q)∈ R,D∈ N V ,Q∈ N V Definition 2: Ranked List L(Q,G,C) Let S M denote the set of permutation of {1,2..M}. Ranked list L(Q,G,C)∈ S M</formula><p>. )</p><formula xml:id="formula_3">V k k k k i k V X X X X X N = ∈</formula><p>, where random variable X k,i denotes the number of times term i occurs. The joint pmf of X k is the function defined by</p><formula xml:id="formula_4">1 ,<label>1 1 , ( ) ( ,</label></formula><p>..., ) Pr( ,..., )</p><formula xml:id="formula_5">k k X X V k k V V f x f x x X x X x = = = = where 1 ( ,..., ) V V x x x N = ∈ . Definition 4: Ranking Similarity SimRank(L 1 ,L 2 )</formula><p>Given two ranked list L 1 (Q,G,C 1 ) and L 2 (Q,G,C 2 ), function SimRank(L 1 ,L 2 ) returns a real number that measures the similarity between the two ranked lists.(we assume that the documents in C 1 have one-to-one correspondence to the documents in C 1 ). Moreover, SimRank(L 1 ,L 2 ) should be bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5: Random Collection X</head><p>Given document model X 1 ,…X M , where X k (k∈ [1,M]) is a Vdimension random vector, we define random collection X=(X 1 ,X 2 ,…X M ) ,that is, X is a M×V matrix whose entries consist of random nonnegative integers from some distributions. The pmf of X is the function defined by</p><formula xml:id="formula_6">1 1<label>1</label></formula><p>( ) ( ,..., ) Pr( ,..., )</p><formula xml:id="formula_7">X X M M M f T f t t X t X t = = = =</formula><p>, where X k denotes the k-th row of X and</p><formula xml:id="formula_8">t k ∈ N V , k∈ [1,M].</formula><p>With the above definitions, we give the definition of the robustness score.</p><p>Given query Q∈ N V , retrieval function G, collection C=(D 1 ,D 2 ,…D M )∈ S(M×V) and random collection X=(X 1 ,X 2 ,…X M ), the robustness score is defined as the expected value of random variable SimRank(L(Q,G,C),L(Q,G,X)):</p><formula xml:id="formula_9">( ) ( , , , ) { ( ( , , ), ( , , ))} ( ( , , ), ( , , )) ( )<label>(1)</label></formula><formula xml:id="formula_10">X T S M V Robustness Score Q G C X E SimRank L Q G C L Q G X SimRank L Q G C L Q G T f T ∈ × = = ∑</formula><p>To make Equation 1 feasible to calculate, we further make the following five assumptions:</p><p>(1) We assume independence between any two document models X i and X j , that is,</p><formula xml:id="formula_11">1 2 1 1 ( ) ( , ,... ) Pr( ) ( ) (2) k M M X X M k k X k k k f T f t t t X t f t = = = = = = ∏ ∏</formula><p>(2) Instead of the whole collection, only the top J retrieved documents in L(Q,G,C) and the corresponding J documents in L(Q,G,X) are used to compute the similarity between the two ranked lists. For the purpose of rank comparison, the corresponding J documents in L(Q,G,X) will shift up in rank and form a new ranked list of length J.</p><p>(3) The Spearman rank correlation coefficient <ref type="bibr" target="#b18">[18]</ref> is adopted to compute the value of function SimRank(L 1 ,L 2 ) in Equation <ref type="formula" target="#formula_2">1</ref>. The coefficient ranges from -1 to 1. A value close to 1 means a perfect positive correlation between the two rankings and a value close to -1 means a perfect negative correlation. If the two rankings have almost no correlation, the correlation coefficient will be close to zero.</p><p>(4) For each document model, we assume independence between any terms. We also assume the term frequencies in the sampled document follow Poisson distributions with the means equal to the corresponding term frequencies in the original document.</p><p>Modeling term frequencies by Poisson distributions has been widely adopted by other researchers <ref type="bibr">[19] [20]</ref>. Furthermore, many retrieval models, such as the query likelihood model, only take query terms into account when ranking documents. In this case, we can simplify Equation 2 by assuming that the frequencies of non-query terms are constant in the sampled document. Formally speaking, given document</p><formula xml:id="formula_12">D k =(D k,1 ,D k,2 ,…D k,V</formula><p>) and query Q=(q 1 ,q 2 ,…q V ), probability mass function</p><formula xml:id="formula_13">k X f of document model X k =(X k,1 ,X k,2 ,…X k,V ) is estimated as follows: , 1<label>2 1</label></formula><p>( , ,... ) ( ) (3)</p><formula xml:id="formula_14">k k j V X V X j j f x x x f x = = ∏</formula><p>where , ( )</p><formula xml:id="formula_15">k j X f</formula><p>x is given by : ,</p><formula xml:id="formula_16">( ) P r( ) , , ! 1, ( ) P r( ) 0 , k j k j j k j x X k j k j k j X k j i f q A N D D e f x X x x N D x e l s e i f x D f x X x e ls e λ λ λ - &gt; &gt; = = = ∈ = = ⎧ = = = ⎨ ⎩<label>, , , , , , ( 0) ( 0)</label></formula><p>For better understanding, we give a toy example to show how to generate a simulated document given the original document based on the above assumptions. (5) The expectation in Equation 1 is very hard to evaluate directly. Instead, we independently draw K samples T(1),T(2),..T(K) from f X (T) to approximate the expectation, that is, Equation 1 is estimated as: 1 ( , , , ) 1 ( ( , , ), ( , , ( ))) ( <ref type="formula">4</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given vocabulary</head><formula xml:id="formula_17">K i Robustness Score Q G C X SimRank L Q G C L Q G T i K = ≅ ∑</formula><p>where T(i) is a sample independently drawn from f X (T) which is determined by Equation 2 and 3.</p><p>The error of this estimation is proportional to the reciprocal of the square root of K <ref type="bibr" target="#b21">[21]</ref>. According to our experiments, we find that a relatively small value of K is good and stable enough for query performance prediction.</p><p>In summary, evaluating robustness takes the following steps. First, we perform retrieval with query Q and retrieval function G. Then we generate J simulated documents using the document models of the top J documents retrieved and rank the simulated documents with the same query and retrieval function. The similarity between the two ranked lists is computed using the Spearman rank correlation coefficient. We repeat this K times and the average of the Spearman correlation coefficient is the robustness score.</p><p>We briefly explain why the robustness score defined above gives us useful information on retrieval performance. A low robustness score means the ranking function provides a very different ranking in the presence of simulated noise compared to the perfect ranking(The ranking on the corresponding clean documents without any simulated noise). We assume that the perfect ranking is optimal, that is, performance of any ranking in the presence of simulated noise can not exceed that of the ranking without simulated noise. Under this assumption, a large deviation between the noisy ranking and the perfect ranking indicates that the noisy ranking is ineffective. If the retrieval performance on the documents with simulated noise is low, we have reasons to believe that the performance on the actual collection may also be low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>In this section, we present the results of predicting query performance by the robustness score. We adopt the clarity method as our baseline. Query performance is measured by average precision.</p><p>First, we study the correlation with average precision. Our results show that robustness scores have statistically significant correlation with average precision across a variety of TREC collections. We note that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also observe that a combination of the two usually performs better than either one when used in isolation.</p><p>Second, we perform a linear regression analysis to evaluate the ability to directly predict the value of average precision. This analysis reveals that the robustness score predicts the value of average precision better than the clarity score. Again, we observe further improvements with a combination of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Our experiments use a variety of TREC collections and the web collection GOV2. All queries used in our experiments are titles of TREC topics. Table <ref type="table" target="#tab_0">1</ref> gives the summary of these test collections. With regard to the calculation of the robustness score, we use the query likelihood model <ref type="bibr" target="#b22">[22]</ref> with Dirichlet smoothing as the ranking function (Dirichlet prior μ is set to 1000). We set parameter K in Equation <ref type="formula">4</ref>to 100 and choose top 50 documents to compute the rank similarity in Equation <ref type="formula">4</ref>. We tried different values of K ranging from 10 to 500000 and found that the results change very little starting from 100. This means we do not have to require a large number of samples to compute robustness scores.</p><p>For computing the clarity score, we use the equations defined in <ref type="bibr" target="#b4">[4]</ref> .The document model is estimated by using Dirichlet smoothing with Dirichlet prior μ =1000. Relevance models are mixed from Jelinek-Mercer smoothed document models with λ =0.6.</p><p>To obtain average precision, all document retrieval is done by using the query-likelihood model and the results are evaluated by the trec_eval program. Again, Dirichlet smoothing with Dirichlet prior μ =1000 is used for smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlation with Average Precision</head><p>We measure the correlation with average precision by both the Kendall's rank correlation test <ref type="bibr" target="#b18">[18]</ref> and the Pearson's correlation test <ref type="bibr" target="#b23">[23]</ref>. Kendall's rank correlation is a non-parametric test since it does not assume any distributions of both variables. In our experiments, Kendall's rank correlation is used to compare the ranking of queries by average precision to the ranking by the clarity scores or the robustness scores of these queries. Pearson's correlation reflects the degree of linear relationship between the two variables 2 .The values of both kinds of correlation range between -1.0 and 1.0 where -1.0 means perfect negative correlation and 1.0 means perfect positive correlation. 1 Topic 672 is removed because of no relevant documents. 2 Here the two variables refer to the actual query performance (measured by average precision) and the predictor. The results for correlation with average precision are presented in table 2 and 3. When we combine the clarity score and the robustness score, we adopt a simple linear combination, that is, (1-α)×clarity score+α×robustness score. For the collections other than TREC 123, we use the α that yields the highest value of Pearson's coefficient on TREC123. For TREC123, we use the best α on Robust 2004. In fact, we find that the optimal linear combination weight changes little across our test collections. Note that when using linear regression to combine the two, we essentially apply learning to our method. But we have only one parameter and we find the regression generalizes well.</p><p>From these results, we first observe statistically significant correlation between the robustness scores and the average precision over all test collections no matter which metric is adopted. The extent of the correlation in the Robust 2004 Track is visible in Figure <ref type="figure" target="#fig_3">2</ref> as a linear trend for average precision of queries to increase as their robustness score increases.</p><p>Second, we see that the linear combination of the two features usually performs better than either one when used in isolation. This is within our expectation since clarity scores and robustness scores measure two different properties of a ranked document list. 3 Note that the only exception occurs in TREC 4 because the 3 We also examine the correlation between the clarity score and the robustness score. We observe the correlations measured by robustness scores correlate with the average precision much better than the clarity scores.</p><p>Third, the robustness score shows a stronger linear relationship with average precision compared to the clarity score. The linear regression analysis performed in the next section will further confirm this observation.</p><p>We observe that the performance of the clarity score drops greatly on the GOV2 collection. We speculate that this is due to the fact that there are a relatively large number of low quality documents in this collection. Moreover, it seems that this characteristic has a more negative impact on clarity scores than on robustness scores.</p><p>To understand this, let us recall that the clarity score measure the degree of dissimilarity between the language usage associated with the query and the generic language of the collection. The ability of clarity scores to predict query performance is based on the following assumption: a query whose highly ranked documents contain many relevant documents (high query performance) is likely to receive a high clarity score because these highly ranked documents tend to be about a single topic and therefore have unusual word usage. However, when it comes to large web collections, the low quality documents retrieved in respond to a query are likely to have unusual word distributions <ref type="bibr" target="#b24">[24]</ref>, resulting in high clarity scores. In other words, the clarity score method can not distinguish whether a high clarity score is caused by a small number of topic terms in the query language model or by the noise from the low quality documents retrieved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Linear Regression Analysis</head><p>Both Kendall's rank correlation and Pearson's correlation are not capable of directly predicting average precision scores. To address this problem, we adopt the linear regression technique which yields an equation that predicts the values of average Pearson's coefficient range from 0.27 to 0.63 on the four TREC collections. We find almost no correlation on the two Web collections. We see that there are relations between the two measures, but they are not very similar to each other. Otherwise, a combination of the two would not lead to further improvement.</p><p>precision from predictors. Although there are fancier non-linear models, linear regression models often perform better in situations with sparse data or highly noisy data <ref type="bibr" target="#b25">[25]</ref>. Moreover, the linear regression analysis provides an adequate and interpretable description of how the predictors affect the dependent variable.</p><p>In this section, we first evaluate the linear prediction quality of the clarity score and the robustness score. Then we investigate the relative importance of each predictor in terms of prediction power. One common way to measure how well a linear regression model fits data is the so-called coefficient of determination or R-square. The range of R-square is between 0 and 1 and a high value means fitting well. Here we perform simple linear regression and the predictor is either the robustness score or the clarity score or the linear combination of the two. Table <ref type="table" target="#tab_3">4</ref> shows the results which are consistent to what we have observed in Table <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref>. For example, we see that the robustness scores fit the average precision much better than the clarity scores on all collections. The goodness-of-fit is low on the GOV2 collection. Again, we observe that the linear combination of the two predictors often boost the quality of linear regression. The effect of linear regression between average precision and robustness score for the 50 title queries from the TREC4 collection is shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>To identify the predictor that bestows the greatest impact on the dependent variable, we compare the regression coefficients of the two predictors. However, the values of the original regression coefficients depend on both the importance of each predictor and the variance of that predictor. To make a fair comparison, we adopt the standardized regression coefficient called Beta that eliminates the influence of variance. The standardized coefficient is what the regression coefficient would be if the model were fitted to standardized data, that is, if from each observation we subtracted the sample mean and then divided by the sample deviation. Hence, the magnitudes of these Beta values represent the importance of each predictor. Table <ref type="table" target="#tab_4">5</ref> shows the results for standardized regression coefficients. We used the SPSS software to compute the standardized regression coefficients. We observe the similar trends as in Table <ref type="table" target="#tab_3">4</ref>. Based on the results from table 4 and 5, our results suggest that when using linear regression robustness scores predict average precision better than clarity scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we introduce the notion of ranking robustness and propose a statistical measure called the robustness score to quantify ranking robustness. We demonstrate that there is a strong correlation between the robustness score of a test query and the performance of that query. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that the robustness score performs better than or at least as good as the clarity score. We observe that the robustness score shows a stronger linear relationship with query performance compared to the clarity score. Therefore, the robustness score can predict the values of average precision more accurately than the clarity score when using a linear regression model. We find that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also notice that a combination of the two usually results in more prediction power. These results give fresh insight into our understanding of principles underlying retrieval and opens up possibilities for developing new techniques in the direction of ranking robustness for predicting or improving retrieval effectiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression Line Average Precision</head><note type="other">Robustness Score</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Robustness Score Calculation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 3 :</head><label>3</label><figDesc>is a permutation of the documents in collection C that describes the ordering of documents by decreasing G(D,Q) where D∈ C Document Model X k and Probability Mass Function (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>V={a,b,c}, query Q={a} and document D 1 ={a,a,b,b,b} , Q and D 1 are represented by 3-dimension vector [1,0,0] and<ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref>0]  respectively. Let N(D 1 ) denotes a simulated document generated from X 1 ,that is, the document mode of D 1 . Since term c does not occur in D 1 , it will not occur in N(D 1 ). Since term b is a non-query term and it occurs three times in D 1 , it will occur exactly three times in N(D 1 ). The occurrence frequency of term a in N(D 1 ) is a random number determined by Poisson distribution P(λ) with λ=2 because term a occurs twice in D 1 . For example, {a,a,a,b,b,b} and {a,b,b,b} are two possibilities of N(D 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average precision versus robustness score for the 249 title queries from the Robust 2004 Track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Linear regression between average precision and robustness score for the 50 title queries from the TREC4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Summary of test collections</head><label>1</label><figDesc></figDesc><table><row><cell>TREC</cell><cell cols="2">Collection</cell><cell>Topic</cell><cell>Number</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Number</cell><cell>Document</cell></row><row><cell>1+2+3</cell><cell cols="2">Disk 1+2+3</cell><cell>51-150</cell><cell>1,078,166</cell></row><row><cell>4</cell><cell cols="2">Disk 2+3</cell><cell>201-250</cell><cell>567,529</cell></row><row><cell>5</cell><cell cols="2">Disk 2+4</cell><cell>251-300</cell><cell>524,929</cell></row><row><cell>Robust 2004</cell><cell>Disk</cell><cell>4+5</cell><cell>301-450;</cell><cell>528,155</cell></row><row><cell></cell><cell cols="2">minus CR</cell><cell>601-700 1</cell><cell></cell></row><row><cell>Terabyte 2004</cell><cell>GOV2</cell><cell></cell><cell>701-750</cell><cell>25,205,197</cell></row><row><cell>Terabyte 2005</cell><cell>GOV2</cell><cell></cell><cell>751-800</cell><cell>25,205,197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Pearson's correlation coefficient for correlation with average precision, for robustness score, clarity score and a linear combination of the two features. Bold cases mean the results are statistically significant at the 0.05 level.</head><label>2</label><figDesc></figDesc><table><row><cell>TREC</cell><cell>Robustness</cell><cell>Clarity</cell><cell>Robustness</cell></row><row><cell></cell><cell>Score</cell><cell>Score</cell><cell>+Clarity</cell></row><row><cell>TREC123</cell><cell>0.434</cell><cell>0.335</cell><cell>0.469</cell></row><row><cell>TREC4</cell><cell>0.613</cell><cell>0.430</cell><cell>0.582</cell></row><row><cell>TREC5</cell><cell>0.454</cell><cell>0.366</cell><cell>0.507</cell></row><row><cell>Robust 04</cell><cell>0.550</cell><cell>0.507</cell><cell>0.613</cell></row><row><cell>Terabyte04</cell><cell>0.341</cell><cell>0.305</cell><cell>0.374</cell></row><row><cell>Terabyte05</cell><cell>0.301</cell><cell>0.206</cell><cell>0.362</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Kendall's rank correlation coefficient for correlation with average precision, for robustness score, clarity score and a linear combination of the two features. Bold cases mean the results are statistically significant at the 0.05 level.</head><label>3</label><figDesc></figDesc><table><row><cell>TREC</cell><cell>Robustness</cell><cell>Clarity</cell><cell>Robustness</cell></row><row><cell></cell><cell>Score</cell><cell>Score</cell><cell>+Clarity</cell></row><row><cell>TREC123</cell><cell>0.329</cell><cell>0.331</cell><cell>0.370</cell></row><row><cell>TREC4</cell><cell>0.548</cell><cell>0.353</cell><cell>0.499</cell></row><row><cell>TREC5</cell><cell>0.328</cell><cell>0.311</cell><cell>0.345</cell></row><row><cell>Robust 04</cell><cell>0.392</cell><cell>0.412</cell><cell>0.460</cell></row><row><cell>Terabyte04</cell><cell>0.213</cell><cell>0.134</cell><cell>0.226</cell></row><row><cell>Terabyte05</cell><cell>0.208</cell><cell>0.171</cell><cell>0.252</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 Coefficient of determination (R-square) from linear regression: the dependent variable is average precision. The predictor (independent variable) is either the robustness score or the clarity score or a combination of the two.</head><label>4</label><figDesc></figDesc><table><row><cell>TREC</cell><cell>Robustness</cell><cell>Clarity</cell><cell>Robustness</cell></row><row><cell></cell><cell>Score only</cell><cell>Score only</cell><cell>+Clarity</cell></row><row><cell>TREC123</cell><cell>0.188</cell><cell>0.112</cell><cell>0.220</cell></row><row><cell>TREC4</cell><cell>0.376</cell><cell>0.185</cell><cell>0.339</cell></row><row><cell>TREC5</cell><cell>0.206</cell><cell>0.134</cell><cell>0.257</cell></row><row><cell>Robust 04</cell><cell>0.302</cell><cell>0.257</cell><cell>0.376</cell></row><row><cell>Terabyte04</cell><cell>0.116</cell><cell>0.093</cell><cell>0.140</cell></row><row><cell>Terabyte05</cell><cell>0.091</cell><cell>0.042</cell><cell>0.131</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 standardized regression coefficients (Beta) from multiple linear regression: the dependent variable is precision. The two predictors are the robustness score and the clarity score.</head><label>5</label><figDesc></figDesc><table><row><cell>Collection</cell><cell>Robustness Score</cell><cell>Clarity Score</cell></row><row><cell>TREC123</cell><cell>0.357</cell><cell>0.195</cell></row><row><cell>TREC4</cell><cell>0.568</cell><cell>0.071</cell></row><row><cell>TREC5</cell><cell>0.376</cell><cell>0.246</cell></row><row><cell>Robust 04</cell><cell>0.396</cell><cell>0.311</cell></row><row><cell>Terabyte 04</cell><cell>0.270</cell><cell>0.216</cell></row><row><cell>Terabyte 05</cell><cell>0.314</cell><cell>0.224</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEGEMENTS</head><p>We thank Vanessa Murdock, Ben Carterette and Jiwoon Jeon for their helpful comments on this work. This work was supported by the Center for Intelligent Information Retrieval.</p><p>Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2004 Robust Track</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://trec.nist.gov/tracks.html" />
		<title level="m">Robust Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://www.haifa.ibm.com/sigir05-qp/index.html" />
		<title level="m">Predicting Query Difficulty</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>SIGIR workshop</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting query performance</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inferring query performance using preretrieval predictors</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the SPIRE 2004</title>
		<meeting>the SPIRE 2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">University of Glasgow at TREC2004: Experiments in Web, Robust, and Terabyte Tracks with Terrier</title>
		<author>
			<persName><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Query difficulty ,robustness and selective application of query expansion</title>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR 2004</title>
		<imprint>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using temporal profiles of queries for precision prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TREC 2005 Robust Track Experiments Using PIRCS</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dinstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2005 Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RMIT University at TREC 2005: Terabyte and Robust Track</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Billerbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2005 Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2005">TREC 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to Estimate Query Difficulty with Applications to Missing Content Detection and Distributed Information Retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005)</title>
		<meeting><address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="512" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TREC 2004 Robust Track Experiments Using PIRCS</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Online Proceedings of 2004 Text REtrieval Conference</title>
		<imprint/>
	</monogr>
	<note>TREC 2004</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting Query Difficulty on the Web by Learning Visual Clues</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM Conference on Research and Development in Information Retrieval (SIGIR 2005)</title>
		<meeting>the 2005 ACM Conference on Research and Development in Information Retrieval (SIGIR 2005)</meeting>
		<imprint>
			<biblScope unit="page">615</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrieval Strategy for Noisy Text</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">symposium on document analysis and information retrieval</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Length normalization in degraded text collections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">symposium on document analysis and information retrieval</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data corruption and information retrieval</title>
		<author>
			<persName><forename type="first">Elke</forename><surname>Mittendorf</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Department of Computer Science, the Katholieke Universiteit Leuven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nonparametric statistical inference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic models for automatic indexing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bookstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="312" to="319" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A probabilistic approach to automatic keyword indexing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Harter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="280" to="289" />
			<date type="published" when="1975">1975</date>
			<pubPlace>Part I</pubPlace>
		</imprint>
	</monogr>
	<note>Part II</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Whitlock</surname></persName>
		</author>
		<title level="m">Monte carlo methods</title>
		<imprint>
			<publisher>John Wiley &amp; Sons,Inc</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A general language model for information retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><surname>Kreyszig</surname></persName>
		</author>
		<idno>INC. 1997</idno>
	</analytic>
	<monogr>
		<title level="m">Advanced Enineering Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>chapter 23</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Document Quality Models for Web Ad Hoc Retrieval, a poster presentation</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proceedings of CIKM 2005</title>
		<imprint>
			<biblScope unit="page" from="331" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
