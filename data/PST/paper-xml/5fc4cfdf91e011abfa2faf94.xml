<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-26">26 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<email>pooleb@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-26">26 Nov 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2011.13456v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024 ˆ1024 images for the first time from a score-based generative model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Two successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. Score matching with Langevin dynamics (SMLD) <ref type="bibr">(Song &amp; Ermon, 2019)</ref> estimates the score (i.e., the gradient of the log probability density) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) <ref type="bibr" target="#b39">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Ho et al., 2020)</ref> trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse distributions to make training tractable. For continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale. We therefore refer to these two model classes together as score-based generative models.</p><p>Score-based generative models, and related techniques <ref type="bibr" target="#b3">(Bordes et al., 2017;</ref><ref type="bibr" target="#b13">Goyal et al., 2017)</ref>, have proven effective at generation of images <ref type="bibr">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b17">Ho et al., 2020)</ref>, audio <ref type="bibr" target="#b6">(Chen et al., 2020;</ref><ref type="bibr" target="#b26">Kong et al., 2020)</ref>, graphs <ref type="bibr" target="#b31">(Niu et al., 2020)</ref>, and shapes <ref type="bibr" target="#b5">(Cai et al., 2020)</ref>. However, the ˚Work done during an internship at Google Brain. practical performance of these two model classes is often quite different for reasons that are not fully understood. More generally, despite obvious surface similarities, the relationship between SMLD and DDPM approaches is largely unexplored.</p><p>We address these open questions by unifying and generalizing both approaches through the lens of stochastic differential equations (SDEs). Instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE <ref type="bibr" target="#b1">(Anderson, 1982)</ref>, which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Our proposed framework has several theoretical and practical contributions:</p><p>Flexible sampling: We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC <ref type="bibr" target="#b34">(Parisi, 1981;</ref><ref type="bibr" target="#b15">Grenander &amp; Miller, 1994)</ref>; and (ii) deterministic samplers based on the probability flow ordinary differential equation (ODE). The former unifies and improves over existing sampling methods for score-based models. The latter allows for exact likelihood computation, efficient and adaptive sampling via black-box ODE solvers, flexible data manipulation via latent codes, and a unique encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controllable generation:</head><p>We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently computed from unconditional scores. This enables applications such as class-conditional generation, image inpainting, and colorization using a single unconditional score-based model without re-training.</p><p>Unified picture: The methods of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although <ref type="bibr" target="#b17">Ho et al. (2020)</ref> has reported higher sample quality than <ref type="bibr">Song &amp; Ermon (2019;</ref><ref type="bibr">2020)</ref>, we show that with better architectures and our new sampling algorithms, the latter can achieve new state-of-the-art Inception and FID scores on CIFAR-10, as well as high-fidelity generation of 1024 ˆ1024 samples. This indicates that either SDE could be advantageous, and should be tuned jointly with other design choices for score-based generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 DENOISING SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)</head><p>Let p σ px | xq fi N px; x, σ 2 Iq be a perturbation kernel, and p σ pxq fi ş p data pxqp σ px | xqdx, where p data pxq denotes the data distribution. Consider a sequence of positive noise scales σ min " σ 1 ă σ 2 ă ¨¨¨ă σ N " σ max . Typically, σ min is small enough such that p σmin pxq « p data pxq, and σ max is large enough such that p σmax pxq « N px; 0, σ 2 max Iq. <ref type="bibr">Song &amp; Ermon (2019)</ref> propose to train a Noise Conditional Score Network (NCSN), denoted by s θ px, σq, with a weighted sum of denoising score matching <ref type="bibr" target="#b45">(Vincent, 2011)</ref> objectives:</p><formula xml:id="formula_0">θ ˚" arg min θ N ÿ i"1 σ 2 i E p data pxq E pσ i px|xq " s θ px, σ i q ´∇x log p σi px | xq 2 2 ‰ .<label>(1)</label></formula><p>Given sufficient data and model capacity, the optimal score-based model s θ ˚px, σq matches ∇ x log p σ pxq almost everywhere for σ P tσ i u N i"1 . For sampling, <ref type="bibr">Song &amp; Ermon (2019)</ref> run M steps of Langevin MCMC to get a sample for each p σi pxq sequentially:</p><formula xml:id="formula_1">x m i " x m´1 i ` i s θ ˚px m´1 i , σ i q `?2 i z m i , m " 1, 2, ¨¨¨, M,<label>(2)</label></formula><p>where i ą 0 is the step size, and z m i is standard normal. The above is repeated for i " N, N 1, ¨¨¨, 1 in turn with x 0 N " N px | 0, σ 2 max Iq and x 0 i " x M i`1 when i ă N . As M Ñ 8 and i Ñ 0 for all i, x N 1 becomes an exact sample from p σmin pxq « p data pxq under some regularity conditions. </p><formula xml:id="formula_2">0 ă β 1 , β 2 , ¨¨¨, β N ă 1. For each training data point x 0 " p data pxq, a discrete Markov chain tx 0 , x 1 , ¨¨¨, x N u is constructed such that ppx i | x i´1 q " N px i ; ? 1 ´βi x i´1</formula><p>, β i Iq, and therefore p αi px i | x 0 q " N px i ;</p><p>? α i x 0 , p1 ´αi qIq, where α i fi ś i j"1 p1 ´βj q. Similar to SMLD, we can denote the perturbed data distribution as p αi pxq fi ş p data pxqp αi px | xqdx. The noise scales are prescribed such that x N is an approximate sample from N p0, Iq. A variational Markov chain in the reverse direction is parameterized with p θ px i´1 |x i q " N px i´1 ; 1 ? 1´βi px i `βi s θ px i , iqq, β i Iq, and trained with a re-weighted variant of the evidence lower bound (ELBO):</p><formula xml:id="formula_3">θ ˚" arg min θ N ÿ i"1 p1 ´αi qE p data pxq E pα i px|xq r s θ px, iq ´∇x log p αi px | xq 2 2 s.<label>(3)</label></formula><p>After solving Eq. ( <ref type="formula" target="#formula_3">3</ref>) to get the optimal model s θ ˚px, iq, samples can be generated by starting from x N " N p0, Iq and following the estimated reverse Markov chain as below</p><formula xml:id="formula_4">x i´1 " 1 ? 1 ´βi px i `βi s θ ˚px i , iqq `aβ i z i , i " N, N ´1, ¨¨¨, 1.<label>(4)</label></formula><p>We call this method ancestral sampling, since it amounts to performing ancestral sampling from the graphical model ś N i"1 p θ px i´1 | x i q. The objective Eq. ( <ref type="formula" target="#formula_3">3</ref>) described here is equivalent to L simple in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, but we re-write it in a slightly different form to expose more similarity to Eq. (1). Like Eq. (1), Eq. ( <ref type="formula" target="#formula_3">3</ref>) is also a weighted sum of denoising score matching objectives, which implies that the optimal model, s θ ˚p x, iq, matches the score of the perturbed data distribution, ∇ x log p αi pxq. Moreover, we observe that the weights of the i-th summand in Eq. (1) and Eq. (3), namely σ 2 i and p1 ´αi q, are related to corresponding perturbation kernels in the same functional form:</p><formula xml:id="formula_5">σ 2 i 91{Er ∇ x log p σi px | xq 2 2 s and p1 ´αi q91{Er ∇ x log p αi px | xq 2 2 s.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCORE-BASED GENERATIVE MODELING WITH SDES</head><p>Perturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PERTURBING DATA WITH SDES</head><p>Our goal is to construct a diffusion process txptqu T t"0 indexed by a continuous time variable t P r0, T s, such that xp0q " p 0 , for which we have a dataset of i.i.d. samples, and xpT q " p T , for which we have a tractable form to generate samples efficiently. In other words, p 0 is the data distribution and p T is the prior distribution. This diffusion process can be modeled as the solution to an Itô SDE: We can map data to a noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling (Section 3.2). We can also reverse the associated probability flow ODE (Section 4.3), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score ∇ x log p t pxq (Section 3.3).</p><formula xml:id="formula_6">dx " f px, tqdt `gptqdw,<label>(5)</label></formula><p>where w is the standard Wiener process (a.k.a., Brownian motion), f p¨, tq : R d Ñ R d is a vectorvalued function called the drift coefficient of xptq, and gp¨q : R Ñ R is a scalar function known as the diffusion coefficient of xptq. For ease of presentation we assume the diffusion coefficient is a scalar (instead of a d ˆd matrix) and does not depend on x, but our theory can be generalized to hold in those cases (see Appendix A). The SDE has a unique strong solution when the coefficients are globally Lipschitz in both state and time <ref type="bibr">(Øksendal, 2003)</ref>. We hereafter denote by p t pxq the probability density of xptq, and use p st pxptq | xpsqq to denote the transition kernel from xpsq to xptq, where 0 ď s ă t ď T .</p><p>Typically, p T is an unstructured prior distribution that contains no information of p 0 , such as a Gaussian distribution with simple mean and variance. There are various ways of designing the SDE in Eq. ( <ref type="formula" target="#formula_6">5</ref>) such that it diffuses the data distribution into a fixed prior distribution. We provide two examples later in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GENERATING SAMPLES BY REVERSING THE SDE</head><p>By starting from samples of xpT q " p T and reversing the process, we can obtain samples xp0q " p 0 . A remarkable result from <ref type="bibr" target="#b1">Anderson (1982)</ref> states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:</p><formula xml:id="formula_7">dx " rf px, tq ´gptq 2 ∇ x log p t pxqsdt `gptqd w,<label>(6)</label></formula><p>where w is a standard Wiener process when time flows backwards from T to 0, and dt is an infinitesimal negative timestep. Once the score of each marginal distribution, ∇ x log p t pxq, is known for all t, we can derive the reverse diffusion process from Eq. ( <ref type="formula" target="#formula_7">6</ref>) and simulate it to sample from p 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ESTIMATING SCORES FOR THE SDE</head><p>The score of a distribution can be estimated by training a score-based model on samples with score matching <ref type="bibr">(Hyvärinen, 2005;</ref><ref type="bibr" target="#b42">Song et al., 2019a</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‰</head><p>. Note that Eq. ( <ref type="formula">7</ref>) uses denoising score matching, but other score matching objectives, such as sliced score matching <ref type="bibr" target="#b42">(Song et al., 2019a)</ref> and finite-difference score matching <ref type="bibr" target="#b33">(Pang et al., 2020)</ref> are also applicable here.</p><p>We need to know the transition kernel p 0t pxptq | xp0qq to efficiently solve Eq. ( <ref type="formula">7</ref>). When f p¨, tq is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in <ref type="bibr" target="#b37">Särkkä &amp; Solin (2019)</ref>). For more general SDEs, we may solve Kolmogorov's forward equation <ref type="bibr">(Øksendal, 2003)</ref>  When using a total of N noise scales, each perturbation kernel p σi px | x 0 q of SMLD corresponds to the distribution of x i in the following Markov chain:</p><formula xml:id="formula_8">x i " x i´1 `bσ 2 i ´σ2 i´1 z i´1 , i " 1, ¨¨¨, N,<label>(8)</label></formula><p>where z i´1 " N p0, Iq, and we have introduced σ 0 " 0 to simplify the notation. In the limit of N Ñ 8, tσ i u N i"1 becomes a function σptq, z i becomes zptq, and the Markov chain tx i u N i"1 becomes a continuous stochastic process txptqu 1 t"0 , where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i. The process txptqu 1 t"0 is given by the following SDE dx "</p><formula xml:id="formula_9">c d rσ 2 ptqs dt dw.<label>(9)</label></formula><p>Likewise for the perturbation kernels tp αi px | x 0 qu N i"1 of DDPM, the discrete Markov chain is</p><formula xml:id="formula_10">x i " a 1 ´βi x i´1 `aβ i z i´1 , i " 1, ¨¨¨, N.<label>(10)</label></formula><p>As N Ñ 8, Eq. ( <ref type="formula" target="#formula_10">10</ref>) converges to the following SDE, dx " ´1 2 βptqx dt `aβptq dw.</p><p>Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs Eq. ( <ref type="formula" target="#formula_9">9</ref>) and Eq. ( <ref type="formula" target="#formula_11">11</ref>) respectively. Interestingly, the SDE of Eq. ( <ref type="formula" target="#formula_9">9</ref>) always gives a process with exploding variance when t Ñ 8, whilst the SDE of Eq. ( <ref type="formula" target="#formula_11">11</ref>) yields a process with bounded variance (proof in Appendix B). Due to this difference, we hereafter refer to Eq. ( <ref type="formula" target="#formula_9">9</ref> Ancestral sampling-the sampling method of DDPM (Eq. ( <ref type="formula" target="#formula_4">4</ref>))-actually corresponds to one special discretization of the reverse-time VP SDE (Eq. ( <ref type="formula" target="#formula_11">11</ref>)) (see Appendix E). In theory, other discretizations Algorithm 1 PC sampling (VE SDE)</p><p>1: xN " N p0, σ 2 max Iq 2: for i " N ´1 to 0 do 3:</p><formula xml:id="formula_12">x 1 i Ð xi`1 `pσ 2 i`1 ´σ2 i qs θ ˚pxi`1, σi`1q 4: z " N p0, Iq 5: xi Ð x 1 i `bσ 2 i`1</formula><p>´σ2 i z 6: for j " 1 to M do 7:</p><p>z " N p0, Iq 8:</p><p>xi Ð xi ` isθ˚pxi, σiq `?2 iz 9: return x0 Algorithm 2 PC sampling (VP SDE) 1: xN " N p0, Iq 2: for i " N ´1 to 0 do 3: x 1 i Ð p2 ´?1 ´βi`1qxi`1 `βi`1sθ˚pxi`1, i `1q 4: z " N p0, Iq 5: xi Ð x 1 i `?βi`1z 6: for j " 1 to M do 7:</p><p>z " N p0, Iq 8:</p><p>xi Ð xi ` isθ˚pxi, iq `?2 iz 9: return x0 should be able to perform comparably or better. To verify this, we propose to apply the same discretization as the forward SDE to the reverse-time SDE (details in Appendix E). The resulting samplers for SMLD and DDPM are given as the "predictor" part in Algs. 1 and 2, and we call this family of methods the reverse diffusion sampler. As shown in Tab. 1, these sampling methods perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling can be applied to SMLD models, see Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PREDICTOR-CORRECTOR SAMPLERS</head><p>Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model s θ ˚px, tq « ∇ x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC <ref type="bibr" target="#b34">(Parisi, 1981;</ref><ref type="bibr" target="#b15">Grenander &amp; Miller, 1994)</ref> or HMC <ref type="bibr" target="#b30">(Neal et al., 2011)</ref> to sample from p t directly, and correct the solution of a numerical SDE solver. At each time step, the numerical SDE solver gives an estimate of the sample at the next time step, playing the role of a "predictor". The score-based MCMC approach corrects the marginal distribution of the estimated sample, playing the role of a "corrector". The idea is analogous to Predictor-Corrector methods for solving systems of equations (Allgower &amp; Georg, 2012), and we similarly name our hybrid sampling algorithms Predictor-Corrector (PC) samplers.</p><p>When using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed Langevin dynamics <ref type="bibr">(Song &amp; Ermon, 2019)</ref> as the corrector, we have Algs. 1 and 2 for VE and VP SDEs respectively, where t i u N ´1 i"0 are step sizes for Langevin dynamics (specified in Appendix G). PC samplers generalize the original sampling methods of SMLD and DDPM: We can recover annealed Langevin dynamics for SMLD by removing the predictor part, or regain the ancestral sampling method for DDPM when using it as the predictor and removing the corrector part.</p><p>We test PC samplers on SMLD/DDPM models trained with Eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_3">3</ref>), instead of our new continuous objective Eq. ( <ref type="formula">7</ref>). This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in Tab. 1, where probability flow is a predictor to be discussed in Section 4.3. Detailed experimental Table 1: Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. "P1000" or "P2000": predictor-only samplers using 1000 or 2000 steps. "C2000": corrector-only samplers using 2000 steps. "PC1000": Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps.  settings are given in Appendix G. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. In Fig. <ref type="figure" target="#fig_3">3</ref>, we additionally provide qualitative comparison for models trained with the continuous objective Eq. ( <ref type="formula">7</ref>), where PC samplers clearly surpass predictoronly samplers under comparable computation, when using a proper number of corrector steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PROBABILITY FLOW AND EQUIVALENCE TO NEURAL ODES</head><p>Score-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding deterministic process whose trajectories share the same marginal probability densities tp t pxqu T t"0 . This deterministic process satisfies an ODE (more details in Appendix D.1):</p><formula xml:id="formula_13">dx " " f px, tq ´1 2 gptq 2 ∇ x log p t pxq ı dt,<label>(12)</label></formula><p>which can be determined from the SDE once scores are known. We name the ODE in Eq. ( <ref type="formula" target="#formula_13">12</ref>) the probability flow ODE. As the score functions are typically parameterized by a neural network, this is an example of a neural ODE <ref type="bibr">(Chen et al., 2018)</ref>.</p><p>Efficient sampling As with neural ODEs, we can sample xp0q " p 0 by solving Eq. ( <ref type="formula" target="#formula_13">12</ref>) from different final conditions xpT q " p T . Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (Tab. 1, "probability flow sampler", details in Appendix D.3). Using a black-box ODE solver <ref type="bibr" target="#b10">(Dormand &amp; Prince, 1980)</ref> allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over 90% without affecting the visual quality of samples (Fig. <ref type="figure" target="#fig_4">4</ref>). RealNVP <ref type="bibr" target="#b9">(Dinh et al., 2016)</ref> 3.49 -iResNet <ref type="bibr" target="#b2">(Behrmann et al., 2019)</ref> 3.45 -Glow <ref type="bibr" target="#b24">(Kingma &amp; Dhariwal, 2018)</ref> 3.35 -MintNet <ref type="bibr" target="#b43">(Song et al., 2019b)</ref> 3.32 -Residual Flow <ref type="bibr">(Chen et al., 2019)</ref> 3.28 46.37 FFJORD <ref type="bibr" target="#b14">(Grathwohl et al., 2018)</ref> 3.40 -Flow++ <ref type="bibr" target="#b16">(Ho et al., 2019)</ref> 3.29 -DDPM (L) <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> ď 3.70 13.51 DDPM (Lsimple) <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> ď 3.75 3.17   <ref type="bibr">(Song &amp; Ermon, 2019)</ref> 25.32 8.87 ˘.12 NCSNv2 <ref type="bibr">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 ˘.07 DDPM <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> 3.17 9.46 ˘.11 Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq. ( <ref type="formula" target="#formula_13">12</ref>) via the instantaneous change of variables formula <ref type="bibr">(Chen et al., 2018)</ref>. This allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in Tab. 2. We compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (excluding models evaluated with variational dequantization <ref type="bibr" target="#b16">(Ho et al., 2019)</ref> or discrete data). Main results: (i) For the same DDPM model in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we obtain better bits/dim compared to the upper bound given by ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with our continuous denoising score matching objective in Eq. ( <ref type="formula">7</ref>) on uniformly dequantized data. This continuously-trained model (DDPM cont. in Tab. 2) has much lower bits/dim compared to the original one, setting a new record bits/dim on uniformly dequantized CIFAR-10 even without maximum likelihood training.</p><formula xml:id="formula_14">NCSN++</formula><p>Manipulating latent representations By integrating Eq. ( <ref type="formula" target="#formula_13">12</ref>), we can encode any datapoint xp0q into a latent space xpT q. Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows <ref type="bibr" target="#b9">(Dinh et al., 2016;</ref><ref type="bibr" target="#b43">Song et al., 2019b)</ref>, we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see Fig. <ref type="figure" target="#fig_4">4</ref> and Appendix D.4).</p><p>Uniquely identifiable encoding Unlike all current invertible models, our encoding is uniquely identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution <ref type="bibr" target="#b36">(Roeder et al., 2020)</ref>. This is because our forward SDE, Eq. ( <ref type="formula" target="#formula_6">5</ref>), has no trainable parameters, and its associated probability flow ODE, Eq. ( <ref type="formula" target="#formula_13">12</ref>), provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in Appendix D.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ARCHITECTURE CHOICES</head><p>Although SMLD and DDPM can be viewed as different instantiations of a unified framework, there is a performance gap reported in previous papers. The best FID values of SMLD models on CIFAR-10 is 10.23 <ref type="bibr">(Song &amp; Ermon, 2020)</ref>, whereas for DDPM it is 3.17 <ref type="bibr" target="#b17">(Ho et al., 2020)</ref>. With PC samplers and the same model architecture in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, the gaps can be reduced, but score-based models trained with the VE SDE still perform slightly worse (see Tab. 1). This raises the question of whether the variance preserving property of the VP SDE is always preferable or whether there is an interaction with model architecture.</p><p>To answer this question, we performed an extensive architecture search (see Appendix H). Our optimal architecture for the VE SDE, dubbed NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, whereas the optimal architecture for the VP SDE over the same architecture sweep obtains an FID of 2.88. This indicates that the VE SDE can be advantageous, and practitioners likely need to experiment with different SDEs for new domains and architectures.</p><p>Based on these improved architectures, we were able to further improve FID with our continuous training objective in Eq. ( <ref type="formula">7</ref>) (FID 2.38), and increasing the number of residual blocks (FID 2.2, see Tab. 3 and Appendix H.2). This model sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previous </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONTROLLABLE GENERATION</head><p>The continuous structure of our framework allows us to not only produce data samples from p 0 , but also from p 0 pxp0q | yq if p t py | xptqq is known. Given a forward SDE as in Eq. ( <ref type="formula" target="#formula_6">5</ref>), we can sample from p t pxptq | yq by starting from p T pxpT q | yq and solving a conditional reverse-time SDE: Imputation is a special case of conditional sampling. Suppose we have an incomplete data point y where only some subset, Ωpyq is known. Imputation amounts to sampling from ppxp0q | Ωpyqq, which we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in Appendix I.3). Fig. <ref type="figure" target="#fig_6">5</ref> (right) shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.</p><formula xml:id="formula_15">dx " tf</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches, new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.</p><p>While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyperparameters, as well as more extensive investigation on the merits and limitations of various samplers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We include several appendices with additional details, derivations, and results. Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in Appendix A. We give a full derivation of VE and VP SDEs in Appendix B, and discuss how to use them from a practitioner's perspective in Appendix C. We elaborate on the probability flow formulation of our framework in Appendix D, including a derivation of the probability flow ODE (Appendix D.1), exact likelihood computation (Appendix D.2), probability flow sampling (Appendices D.3 and D.4) and experimental verification on uniquely identifiable encoding (Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, and the DDPM-type ancestral sampler for SMLD models in Appendix F. We provide full experimental details on Predictor-Corrector samplers in Appendix G. We explain our model architectures and detailed experimental settings in Appendix H, with 1024 ˆ1024 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in Appendix I, and include extended results for class-conditional generation (Appendix I.1), image inpainting (Appendix I.2) and colorization (Appendix I.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THE FRAMEWORK FOR MORE GENERAL SDES</head><p>In the main text, we introduced our framework based on a simplified SDE Eq. ( <ref type="formula" target="#formula_6">5</ref>) where the diffusion coefficient is independent of xptq. It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:</p><formula xml:id="formula_16">dx " f px, tqdt `Gpx, tqdw,<label>(14)</label></formula><p>where f p¨, tq : R d Ñ R d and Gp¨, tq : R d Ñ R dˆd . We follow the Itô interpretation of SDEs throughout this paper.</p><p>According to <ref type="bibr" target="#b1">(Anderson, 1982)</ref>, the reverse-time SDE is given by (cf ., Eq. ( <ref type="formula" target="#formula_7">6</ref>))</p><p>dx " tf px, tq ´∇ ¨rGpx, tqGpx, tq T s ´Gpx, tqGpx, tq T ∇ x log p t pxqudt `Gpx, tqd w, (15)</p><p>where we define ∇ ¨Fpxq :" p∇ ¨f 1 pxq, ∇ ¨f 2 pxq, ¨¨¨, ∇ ¨f d pxqq T for a matrix-valued function Fpxq :" pf 1 pxq, f 2 pxq, ¨¨¨, f d pxqq T throughout the paper.</p><p>The probability flow ODE corresponding to Eq. ( <ref type="formula" target="#formula_16">14</ref>) has the following form (cf ., Eq. ( <ref type="formula" target="#formula_13">12</ref>), see a detailed derivation in Appendix D.1):</p><p>dx "</p><formula xml:id="formula_17">" f px, tq ´1 2 ∇ ¨rGpx, tqGpx, tq T s ´1 2 Gpx, tqGpx, tq T ∇ x log p t pxq * dt.<label>(16)</label></formula><p>Finally for conditional generation with the general SDE Eq. ( <ref type="formula" target="#formula_16">14</ref>), we can solve the conditional reverse-time SDE below (cf ., Eq. ( <ref type="formula">13</ref>), details in Appendix I):</p><formula xml:id="formula_18">dx " tf px, tq ´∇ ¨rGpx, tqGpx, tq T s ´Gpx, tqGpx, tq T ∇ x log p t pxq ´Gpx, tqGpx, tq T ∇ x log p t py | xqudt `Gpx, tqd w. (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel p 0t pxptq | xp0qq in closed form. This hinders the training of score-based models, because Eq. ( <ref type="formula">7</ref>) requires knowing ∇ xptq log p 0t pxptq | xp0qq. To overcome this difficulty, we can replace denoising score matching in Eq. ( <ref type="formula">7</ref>) with other efficient variants of score matching that do not require computing ∇ xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching <ref type="bibr" target="#b42">(Song et al., 2019a)</ref>, our training objective Eq. ( <ref type="formula">7</ref>) becomes</p><formula xml:id="formula_20">θ ˚" arg min θ E t " λptqE xp0q E xptq E v"pv " 1 2 s θ pxptq, tq 2 2 `vT s θ pxptq, tqv * ,<label>(18)</label></formula><p>where λ : r0, T s Ñ R `is a positive weighting function, t " Up0, T q, Ervs " 0, and Covrvs " I.</p><p>We can always simulate the SDE to sample from p 0t pxptq | xp0qq, and solve Eq. ( <ref type="formula" target="#formula_20">18</ref>) to train the time-dependent score-based model s θ px, tq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DERIVATIONS OF VE AND VP SDES</head><p>Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively.</p><p>First, when using a total of N noise scales, each perturbation kernel p σi px | x 0 q of SMLD can be derived from the following Markov chain:</p><formula xml:id="formula_21">x i " x i´1 `bσ 2 i ´σ2 i´1 z i´1 , i " 1, ¨¨¨, N,<label>(19)</label></formula><p>where z i´1 " N p0, Iq, x 0 " p data , and we have introduced σ 0 " 0 to simplify the notation. In the limit of N Ñ 8, the Markov chain tx i u N i"1 becomes a continuous stochastic process txptqu 1 t"0 , tσ i u N i"1 becomes a function σptq, and z i becomes zptq, where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i P t1, 2, ¨¨¨, N u. where the approximate equality holds when ∆t ! 1. In the limit of ∆t Ñ 0, this converges to</p><formula xml:id="formula_22">Let ∆t " 1 N ´1 , x ´i´1 N ´1 ¯" x i , σ ´i´1 N ´1 ¯" σ i ,</formula><formula xml:id="formula_23">dx " c d rσ 2 ptqs dt dw,<label>(20)</label></formula><p>which is the VE SDE. Note that xp0q " x 1 " x 0 `σ1 z 0 " x 0 `σ2 p0qz 0 and x 0 " p data , so we have p 0 pxp0qq " ş p data pxqN pxp0q; x, σ 2 1 Iqdx. In other words, p 0 pxp0qq for the VE SDE is a perturbed data distribution with a small Gaussian noise.</p><p>For the perturbation kernels tp αi px | x 0 qu N i"1 used in DDPM, the discrete Markov chain is</p><formula xml:id="formula_24">x i " a 1 ´βi x i´1 `aβ i z i´1 , i " 1, ¨¨¨, N,<label>(21)</label></formula><p>where z i´1 " N p0, Iq. To obtain the limit of this Markov chain when N Ñ 8, we define an auxiliary set of noise scales t βi " pN ´1qβ i u N i"1 , and re-write Eq. ( <ref type="formula" target="#formula_24">21</ref>) as below</p><formula xml:id="formula_25">x i " d 1 ´β i N ´1 x i´1 `d βi N ´1 z i´1 , i " 1, ¨¨¨, N.<label>(22)</label></formula><p>In the limit of N Ñ 8, t βi u N i"1 becomes a function βptq indexed by t P r0, 1s. Let β ´i´1 N ´1 ¯" βi , xp i´1 N ´1 q " x i , zp i´1 N ´1 q " z i and ∆t " 1 N ´1 . The Markov chain Eq. ( <ref type="formula" target="#formula_25">22</ref>) becomes the following at</p><formula xml:id="formula_26">t P t0, 1, ¨¨¨, N ´2 N ´1 u xpt `∆tq " a 1 ´βptq∆t xptq `aβptq∆t zptq « ´1 2 βptq∆t xptq `aβptq∆t zptq, (<label>23</label></formula><formula xml:id="formula_27">)</formula><p>where the approximate equality holds when ∆t ! 1. Therefore, in the limit of ∆t Ñ 0, Eq. ( <ref type="formula" target="#formula_26">23</ref>) converges to the following VP SDE:</p><formula xml:id="formula_28">dx " ´1 2 βptqx dt `aβptq dw. (<label>24</label></formula><formula xml:id="formula_29">) Because xp0q " lim N Ñ8 ? 1 ´β1 x 0 `?β 1 z 0 " lim N Ñ8 b 1 ´βp0q N ´1</formula><p>x 0 `b βp0q N ´1 z 0 " x 0 and x 0 " p data , we have p 0 pxp0qq " p data pxq.</p><p>So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when t Ñ 8. In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all t P r0, 8q when ppxp0qq has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq. (5.51) in <ref type="bibr" target="#b37">Särkkä &amp; Solin (2019)</ref> to obtain an ODE that governs the evolution of variance dΣptq dt " βptqpI ´Σptqq,</p><p>where Σptq " Covrxptqs. Solving this ODE, we obtain</p><p>Σptq " I `eş T 0 ´βptqdt pΣp0q ´Iq, from which it is clear that the variance Σptq is always bounded given Σp0q. Moreover, Σptq " I if Σp0q " I. Due to this difference, we name Eq. ( <ref type="formula" target="#formula_9">9</ref>) as the Variance Exploding (VE) SDE, and Eq. ( <ref type="formula" target="#formula_11">11</ref>) the Variance Preserving (VP) SDE.</p><p>Since both VE and VP SDEs have affine drift coefficients, their perturbation kernels p 0t pxptq | xp0qq are both Gaussian and can be computed with Eqs. (5.50) and (5.51) in <ref type="bibr" target="#b37">Särkkä &amp; Solin (2019)</ref>:</p><formula xml:id="formula_30">p 0t pxptq | xp0qq " # N `xptq; xp0q, rσ 2 ptq ´σ2 p0qsI ˘, (VE SDE) N `xptq; xp0qe ´1 2 ş t 0 βpsqds , I ´Ie ´şt 0 βpsqds ˘(VP SDE) . (<label>25</label></formula><formula xml:id="formula_31">)</formula><p>Therefore, both SDEs allow efficient training with the objective in Eq. ( <ref type="formula">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SDES IN THE WILD</head><p>Below we discuss SDEs that directly correspond to implementations for practical SMLD and DDPM models. In SMLD, the noise scales tσ i u N i"1 is typically a geometric sequence where σ min is fixed to 0.01 and σ max is chosen according to Technique 1 in <ref type="bibr">Song &amp; Ermon (2020)</ref>. Usually, SMLD models normalize image inputs to the range r0, 1s. Since tσ i u N i"1 is a geometric sequence, we have σ i " σ min ´σmax σmin ¯i´1</p><p>N ´1 and therefore σptq " σ min ´σmax σmin ¯t. The VE SDE in this case is</p><formula xml:id="formula_32">dx " σ min ˆσmax σ min ˙tc 2 log σ max σ min dw,<label>(26)</label></formula><p>where xp0q " ş p data pxqN pxp0q; x, σ 2 min Iqdx. The perturbation kernel can be computed via Eq. ( <ref type="formula" target="#formula_30">25</ref>):</p><formula xml:id="formula_33">p 0t pxptq | xp0qq " N ˆxptq; xp0q, " σ 2 min ´σmax σ min ¯2t ´σ2 min ı I ˙. (<label>27</label></formula><formula xml:id="formula_34">)</formula><p>For DDPM models, tβ i u N i"1 is typically an arithmetic sequence where β i " βmin N ´1 `i´1 pN ´1q 2 p βmax βmin q. Therefore, βptq " βmin `tp βmax ´β min q. This corresponds to the following instantiation of the VP SDE:</p><formula xml:id="formula_35">dx " ´1 2 p βmin `tp βmax ´β min qqxdt `b βmin `tp βmax ´β min qdw,<label>(28)</label></formula><p>where xp0q " p data pxq. In our experiments, we let βmin " 0.1 and βmax " 20, which correspond to the settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. The perturbation kernel is given by</p><formula xml:id="formula_36">p 0t pxptq | xp0qq " N ´xptq; e ´1 4 t 2 p βmax´βminq´1 2 t βmin xp0q, I ´Ie ´1 2 t 2 p βmax´βminq´t βmin ¯. (<label>29</label></formula><formula xml:id="formula_37">)</formula><p>As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in Fig. <ref type="figure">6</ref>. The SMLD and DDPM models both use N " 1000 noise scales. For SMLD, we only need to compare the variances of perturbation kernels. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in Fig. <ref type="figure">6</ref>, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROBABILITY FLOW ODE D.1 DERIVATION</head><p>The idea of probability flow ODE is inspired by <ref type="bibr" target="#b29">Maoutsa et al. (2020)</ref>, and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in Eq. ( <ref type="formula" target="#formula_17">16</ref>). We consider the SDE in Eq. ( <ref type="formula" target="#formula_16">14</ref>), which possesses the following form:</p><formula xml:id="formula_38">dx " f px, tqdt `Gpx, tqdw,</formula><p>where we define f px, tq fi f px, tq ´1 2 ∇ ¨rGpx, tqGpx, tq T s ´1 2 Gpx, tqGpx, tq T ∇ x log p t pxq.</p><p>Inspecting Eq. ( <ref type="formula">32</ref>), we observe that it equates the forward Kolmogorov equation of the following SDE with Gpx, tq fi 0 (the forward Kolmogorov equation holds in this case, as can be justified by the instantaneous change of variables formula in Chen et al. ( <ref type="formula">2018</ref>))</p><p>dx " f px, tqdt `Gpx, tqdw, which is essentially an ODE:</p><p>dx " f px, tqdt, same as the probability flow ODE given by Eq. ( <ref type="formula" target="#formula_17">16</ref>). Therefore, we have shown that the probability flow ODE Eq. ( <ref type="formula" target="#formula_17">16</ref>) induces the same marginal probability density p t pxq as the SDE in Eq. ( <ref type="formula" target="#formula_16">14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 LIKELIHOOD COMPUTATION</head><p>The probability flow ODE in Eq. ( <ref type="formula" target="#formula_17">16</ref>) has the following form when we replace the score ∇ x log p t pxq with the time-dependent score-based model s θ px, tq:</p><formula xml:id="formula_39">dx " " f px, tq ´1 2 ∇ ¨rGpx, tqGpx, tq T s ´1 2 Gpx, tqGpx, tq T s θ px, tq * loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon fi fθ px,tq dt. (<label>33</label></formula><formula xml:id="formula_40">)</formula><p>With the instantaneous change of variables formula <ref type="bibr">(Chen et al., 2018)</ref>, we can compute the loglikelihood of p 0 pxq using</p><formula xml:id="formula_41">log p 0 pxp0qq " log p T pxpT qq ´ż T 0 ∇ ¨f θ pxptq, tqdt,<label>(34)</label></formula><p>where the random variable xptq as a function of t can be obtained by solving the probability flow ODE in Eq. ( <ref type="formula" target="#formula_39">33</ref>). In many cases computing ∇ ¨f θ px, tq is expensive, so we follow <ref type="bibr" target="#b14">Grathwohl et al. (2018)</ref> to estimate it with the Skilling-Hutchinson trace estimator <ref type="bibr" target="#b38">(Skilling, 1989;</ref><ref type="bibr" target="#b18">Hutchinson, 1990)</ref>.</p><p>In particular, we have</p><formula xml:id="formula_42">∇ ¨f θ px, tq " E pp q r T ∇ fθ px, tq s,<label>(35)</label></formula><p>where ∇ fθ denotes the Jacobian of fθ p¨, tq, and the random variable satisfies E pp q r s " 0 and Cov pp q r s " I. The vector-Jacobian product T ∇ fθ px, tq can be efficiently computed using reversemode automatic differentiation, at approximately the same cost as evaluating fθ px, tq. As a result, we can sample " pp q and then compute an efficient unbiased estimate to ∇ ¨f θ px, tq using T ∇ fθ px, tq . Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. ( <ref type="formula" target="#formula_42">35</ref>) to Eq. ( <ref type="formula" target="#formula_41">34</ref>), we can compute the log-likelihood to any accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 PROBABILITY FLOW SAMPLING</head><p>Suppose we have a forward SDE dx " f px, tqdt `Gptqdw, and one of its discretization</p><formula xml:id="formula_43">x i`1 " x i `fi px i q `Gi z i , i " 0, 1, ¨¨¨, N ´1, (<label>36</label></formula><formula xml:id="formula_44">)</formula><p>where z i " N p0, Iq. We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on ∆t into the notations of f i and G i . Using Eq. ( <ref type="formula" target="#formula_17">16</ref>), we can obtain the following probability flow ODE:</p><formula xml:id="formula_45">dx " " f px, tq ´1 2 GptqGptq T ∇ x log p t pxq * dt. (<label>37</label></formula><formula xml:id="formula_46">)</formula><p>We may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to Eq. ( <ref type="formula" target="#formula_43">36</ref>):</p><formula xml:id="formula_47">x i " x i`1 ´fi`1 px i`1 q `1 2 G i`1 G T i`1 s θ ˚px i`1 , i `1q, i " 0, 1, ¨¨¨, N ´1,</formula><p>where the score-based model s θ ˚px i , iq is conditioned on the iteration number i. This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample x N is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:</p><formula xml:id="formula_48">x i " x i`1 `1 2 pσ 2 i`1 ´σ2 i qs θ ˚px i`1 , σ i`1 q, i " 0, 1, ¨¨¨, N ´1. (<label>38</label></formula><formula xml:id="formula_49">)</formula><p>Similarly, for DDPM models, we have</p><formula xml:id="formula_50">x i " p2 ´a1 ´βi`1 qx i`1 `1 2 β i`1 s θ ˚px i`1 , i `1q, i " 0, 1, ¨¨¨, N ´1.<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 PROBABILITY FLOW SAMPLING WITH BLACK-BOX ODE SOLVERS</head><p>For producing figures in Fig. <ref type="figure" target="#fig_4">4</ref>, we use a DDPM model trained on 256 ˆ256 CelebA-HQ with the same settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. We use the RK45 ODE solver <ref type="bibr" target="#b10">(Dormand &amp; Prince, 1980)</ref> provided by scipy.integrate.solve_ivp in all cases. The bits/dim values in Tab. 2 are computed with atol=1e-5 and rtol=1e-5, same as <ref type="bibr" target="#b14">Grathwohl et al. (2018)</ref>. To give the results for DDPM (probability flow) and DDPM cont. (probability flow) in Tab. 2, we average the bits/dim obtained on the test dataset over five different runs. The standard deviation is below 0.005 for both of them. FID scores are computed on samples from the probability flow sampler.</p><p>Aside from the interpolation results in Fig. <ref type="figure" target="#fig_4">4</ref>, we demonstrate more examples of latent space manipulation in Fig. <ref type="figure" target="#fig_7">7</ref>, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 UNIQUELY IDENTIFIABLE ENCODING</head><p>As a sanity check, we train two models (denoted as "Model A" and "Model B") with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in Eq. ( <ref type="formula">7</ref>), and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in Appendix H.</p><p>We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in Fig. <ref type="figure" target="#fig_8">8</ref>. In Fig. <ref type="figure">9</ref>, we show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E REVERSE DIFFUSION SAMPLING</head><p>Given a forward SDE dx " f px, tqdt `Gptqdw, and suppose the following iteration rule is a discretization of it:</p><formula xml:id="formula_51">x i`1 " x i `fi px i q `Gi z i , i " 0, 1, ¨¨¨, N ´1<label>(40)</label></formula><p>where z i " N p0, Iq. Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of f i and G i .</p><p>Based on Eq. ( <ref type="formula" target="#formula_51">40</ref>), we propose to discretize the reverse-time SDE dx " rf px, tq ´GptqGptq T ∇ x log p t pxqsdt `Gptqd w, with a similar functional form, which gives the following iteration rule for i P t0, 1, ¨¨¨, N ´1u:  x 1 (T)</p><formula xml:id="formula_52">x i " x i`1 ´fi`1 px i`1 q `Gi`1 G T i`1 s θ ˚px i`1 , i `1q `Gi`1 z i`1 ,<label>(41)</label></formula><p>Figure <ref type="figure">9</ref>: Left: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models. Right:</p><p>The dimension-wise correlation coefficients of encodings obtained by Model A and Model B.</p><p>where our trained score-based model s θ ˚px i , iq is conditioned on iteration number i.</p><p>When applying Eq. ( <ref type="formula" target="#formula_52">41</ref>) to Eqs. ( <ref type="formula" target="#formula_10">10</ref>) and ( <ref type="formula" target="#formula_21">19</ref>), we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the "predictor" part of Algs. 1 and 2. We name these sampling methods (that are based on the discretization strategy in Eq. ( <ref type="formula" target="#formula_52">41</ref>)) reverse diffusion samplers.</p><p>Note that the ancestral sampling of DDPM <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> (Eq. ( <ref type="formula" target="#formula_4">4</ref>)) matches its reverse diffusion counterpart when β i Ñ 0 for all i (which happens when ∆t Ñ 0 since β i " βi ∆t), because</p><formula xml:id="formula_53">x i " 1 a 1 ´βi`1 px i`1 `βi`1 s θ ˚px i`1 , i `1qq `aβ i`1 z i`1 " ˆ1 `1 2 β i`1 `opβ i`1 q ˙px i`1 `βi`1 s θ ˚px i`1 , i `1qq `aβ i`1 z i`1 « ˆ1 `1 2 β i`1 ˙px i`1 `βi`1 s θ ˚px i`1 , i `1qq `aβ i`1 z i`1 " ˆ1 `1 2 β i`1 ˙xi`1 `βi`1 s θ ˚px i`1 , i `1q `1 2 β 2 i`1 s θ ˚px i`1 , i `1q `aβ i`1 z i`1 « ˆ1 `1 2 β i`1 ˙xi`1 `βi`1 s θ ˚px i`1 , i `1q `aβ i`1 z i`1 " " 2 ´ˆ1 ´1 2 β i`1 ˙x i`1 `βi`1 s θ ˚px i`1 , i `1q `aβ i`1 z i`1 « " 2 ´ˆ1 ´1 2 β i`1 ˙`opβ i`1 q  x i`1 `βi`1 s θ ˚px i`1 , i `1q `aβ i`1 z i`1 "p2 ´a1 ´βi`1 qx i`1 `βi`1 s θ ˚px i`1 , i `1q `aβ i`1 z i`1 .</formula><p>Therefore, the original ancestral sampler of Eq. ( <ref type="formula" target="#formula_4">4</ref>) is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> as a numerical solver to the reverse-time VP SDE in our continuous framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ANCESTRAL SAMPLING FOR SMLD MODELS</head><p>The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales σ 1 ă σ 2 ă ¨¨¨ă σ N as in SMLD. By perturbing a data point x 0 with these noise scales sequentially, we obtain a Markov chain x 0 Ñ x 1 Ñ ¨¨¨Ñ x N , where</p><formula xml:id="formula_54">ppx i | x i´1 q " N px i ; x i´1 , pσ 2 i ´σ2 i´1 qIq, i " 1, 2, ¨¨¨, N.</formula><p>Here we assume σ 0 " 0 to simplify notations. Following <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we can compute</p><formula xml:id="formula_55">qpx i´1 | x i , x 0 q " N ˆxi´1 ; σ 2 i´1 σ 2 i x i `´1 ´σ2 i´1 σ 2 i ¯x0 , σ 2 i´1 pσ 2 i ´σ2 i´1 q σ 2 i I ˙.</formula><p>If we parameterize the reverse transition kernel as p θ px i´1 | x i q " N px i´1 ; µ θ px i , iq, τ 2 i Iq, then</p><formula xml:id="formula_56">L t´1 " E q rD KL pqpx i´1 | x i , x 0 qq } p θ px i´1 | x i qs " E q « 1 2τ 2 i σ 2 i´1 σ 2 i x i `´1 ´σ2 i´1 σ 2 i ¯x0 ´µθ px i , iq 2 2 ff `C " E x0,z « 1 2τ 2 i x i px 0 , zq ´σ2 i ´σ2 i´1 σ i z ´µθ px i px 0 , zq, iq 2 2 ff `C,</formula><p>where L t´1 is one representative term in the ELBO objective (see Eq. ( <ref type="formula" target="#formula_8">8</ref>) in Ho et al. ( <ref type="formula">2020</ref>)), C is a constant that does not depend on θ, z " N p0, Iq, and x i px 0 , zq " x 0 `σi z. We can therefore parameterize µ θ px i , iq via</p><formula xml:id="formula_57">µ θ px i , iq " x i `pσ 2 i ´σ2 i´1 qs θ px i , iq,</formula><p>where s θ px i , iq is to estimate z{σ i . As in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we let τ i "</p><formula xml:id="formula_58">c σ 2 i´1 pσ 2 i ´σ2 i´1 q σ 2 i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Through ancestral sampling on</head><p>ś N i"1 p θ px i´1 | x i q, we obtain the following iteration rule</p><formula xml:id="formula_59">x i´1 " x i `pσ 2 i ´σ2 i´1 qs θ ˚px i , iq `d σ 2 i´1 pσ 2 i ´σ2 i´1 q σ 2 i z i , i " 1, 2, ¨¨¨, N,<label>(42)</label></formula><p>where x N " N p0, σ 2 N Iq, θ ˚denotes the optimal parameter of s θ , and z i " N p0, Iq. We call Eq. ( <ref type="formula" target="#formula_59">42</ref>) the ancestral sampling method for SMLD models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ADDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS</head><p>Training We use the same architecture in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for our score-based models. For the VE SDE, we train a model with the original SMLD objective in Eq. ( <ref type="formula" target="#formula_0">1</ref>); similarly for the VP SDE, we use the original DDPM objective in Eq. (3). We apply a total number of 1000 noise scales for training both models. For results in Fig. <ref type="figure" target="#fig_3">3</ref>, we train an NCSN++ model (definition in Appendix H) on 256 ˆ256 LSUN bedroom and church outdoor <ref type="bibr" target="#b46">(Yu et al., 2015)</ref> datasets with the VE SDE and our continuous objective Eq. ( <ref type="formula">7</ref>).</p><p>The corrector algorithms We take the schedule of annealed Langevin dynamics in <ref type="bibr">Song &amp; Ermon (2019)</ref>, but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in Algs. 3 and 4 respectively, where we call r the "signal-to-noise" ratio. We determine the step size using the norm of the Gaussian noise z 2 , norm of the score-based model s θ ˚ 2 and the signal-to-noise ratio r. When sampling a large batch of samples together, we replace the norm ¨ 2 with the average norm across the mini-batch. When the batch size is small, we suggest replacing z 2 with ? d, where d represents the dimensionality of z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Corrector algorithm (VE SDE).</head><p>Require:</p><formula xml:id="formula_60">tσ i u N i"1 , r, N, M . 1: x 0 N " N p0, σ 2 max Iq 2: for i Ð N to 1 do 3: for j Ð 1 to M do 4: z " N p0, Iq 5: g Ð s θ ˚px j´1 i , σ i q 6:</formula><p>Ð 2pr z 2 { g 2 q 2 7:</p><formula xml:id="formula_61">x j i Ð x j´1 i ` g `?2 z 8: x 0 i´1 Ð x M i return x 0 0</formula><p>Algorithm 4 Corrector algorithm (VP SDE).</p><p>Require:</p><formula xml:id="formula_62">tβ i u N i"1 , tα i u N i"1 , r, N, M . 1: x 0 N " N p0, Iq 2: for i Ð N to 1 do 3: for j Ð 1 to M do 4: z " N p0, Iq 5: g Ð s θ ˚px j´1 i , iq 6:</formula><p>Ð 2α i pr z 2 { g 2 q 2 7:</p><formula xml:id="formula_63">x j i Ð x j´1 i ` g `?2 z 8: x 0 i´1 Ð x M i return x 0 0</formula><p>Denoising For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. ( <ref type="formula">2020</ref>), FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula <ref type="bibr" target="#b11">(Efron, 2011)</ref>.</p><p>Ad-hoc interpolation methods for noise scales Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in <ref type="bibr">Song &amp; Ermon (2019)</ref>). Specifically, for SMLD models, we keep σ min and σ max fixed and double the number of time steps. For DDPM models, we halve βmin and βmax before doubling the number of time steps. Suppose ts θ px, iqu N ´1 i"0 is a score-based model trained on N time steps, and let ts 1 θ px, iqu 2N ´1 i"0</p><p>denote the corresponding interpolated score-based model at 2N time steps. We test two different interpolation strategies for time steps: linear interpolation where s 1 θ px, iq " s θ px, i{2q and rounding interpolation where s 1 θ px, iq " s θ px, ti{2uq. We provide results with linear interpolation in Tab. 1, and give results of rounding interpolation in Tab. 4. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.</p><p>Hyper-parameters of the samplers For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio (r) over a grid that increments at 0.01. We report the best r in Tab. 5. For LSUN bedroom/church outdoor, we fix r to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. We always use a batch size of 128 for training. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church outdoor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ARCHITECTURAL IMPROVEMENTS FOR MODELS WITH VE PERTURBATIONS</head><p>To study whether the VE perturbations are inherently weaker than VP perturbations, we conducted a comprehensive architecture search to compare their best performance. Our results demonstrate that the VE perturbation can be advantageous in certain cases. Further, our endeavor gives rise to new state-of-the-art models on CIFAR-10, and enables the first high-fidelity image samples of resolution 1024 ˆ1024 from score-based generative models. Code and checkpoints will be open sourced.   <ref type="bibr" target="#b27">Krizhevsky et al., 2009)</ref> and 64 ˆ64 CelebA <ref type="bibr" target="#b28">(Liu et al., 2015)</ref>, which is pre-processed following <ref type="bibr">Song &amp; Ermon (2020)</ref>. All models are trained with 1.3M iterations, and we save one checkpoint per 50k iterations. We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. The FIDs are computed on 50k samples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, all models are trained with the original SMLD objective Eq. ( <ref type="formula" target="#formula_0">1</ref>) and use a batch size of 128. Our architecture is mostly based on <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. We additionally search over the following components to explore the potential of score-based models trained with VE perturbations.</p><p>• Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR) <ref type="bibr" target="#b48">(Zhang, 2019)</ref>. We follow the same implementation and hyper-parameters in StyleGAN-2 <ref type="bibr" target="#b23">(Karras et al., 2020b)</ref>. • Rescaling all skip connections by 1 { ? 2. This has been demonstrated effective in several works, including ProgressiveGAN <ref type="bibr" target="#b20">(Karras et al., 2018)</ref>, StyleGAN <ref type="bibr" target="#b21">(Karras et al., 2019)</ref> and <ref type="bibr">StyleGAN-2 (Karras et al., 2020b)</ref>.</p><p>• Replacing the original residual blocks in DDPM with residual blocks from BigGAN <ref type="bibr" target="#b4">(Brock et al., 2018)</ref>. • Increasing the number of residual blocks per resolution from 2 to 4.</p><p>• Incorporating progressive growing architectures. We consider two progressive architectures for input: "input skip" and "residual", and two progressive architectures for output: "output skip" and "residual". These progressive architectures are defined and implemented according to StyleGAN-2.</p><p>We also tested equalized learning rates, a trick used in very successful models like Progressive-GAN <ref type="bibr" target="#b20">(Karras et al., 2018)</ref> and StyleGAN <ref type="bibr" target="#b21">(Karras et al., 2019)</ref>. However, we found it harmful at an early stage of our experiments, and therefore decided not to include it in the architecture search.</p><p>The exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.</p><p>For sampling from models with VE perturbations, we use the PC sampler discretized at 1000 noise scales with one Langevin step per scale and a signal-of-noise ratio of 0.16. For VP perturbations, we use the reverse diffusion sampler discretized at 1000 noise scales without correctors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 RESULTS ON CIFAR-10</head><p>We provide the architecture search results in Fig. <ref type="figure" target="#fig_9">10</ref>. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we searched over can always improve the performance on average. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model uses FIR upsampling/downsampling, rescales skip connections, employs BigGAN-type residual blocks, uses 4 residual blocks per resolution instead of 2, and uses "residual" for input and no progressive growing architecture for output. We name this model "NCSN++", following the naming convention of previous SMLD models <ref type="bibr">(Song &amp; Ermon, 2019;</ref><ref type="bibr">2020)</ref>.</p><p>The basic NCSN++ model with 4 residual blocks achieves an FID of 2.45 on CIFAR-10. Here in order to match the convention used in <ref type="bibr" target="#b20">Karras et al. (2018)</ref>; <ref type="bibr">Song &amp; Ermon (2019)</ref>; <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, the FID value here is the lowest over the course of training, rather than the average over checkpoints after 0.5M iterations (as done in our architecture search). We conducted the same architecture search for a score-based model trained with the DDPM objective Eq. ( <ref type="formula" target="#formula_3">3</ref>). The optimal model obtains an FID of 2.88 on CIFAR-10, significantly worse than the model trained with VE perturbations. Interestingly, the optimal score-based model for VP perturbations has significantly different configurations than those trained with VE perturbation. Some of the components like rescaling skip connections and progressive growing architectures seem to be harmful for them. This further confirms that the relative advantage of VE/VP SDEs is strongly correlated with model architecture, and should be jointly tuned as part of the design choices.</p><p>To further improve the NCSN++ model upon conditioning on continuous time variables, we change positional embeddings, the layers in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> for conditioning on discrete time steps, to random Fourier feature embeddings, as advocated in <ref type="bibr" target="#b44">Tancik et al. (2020)</ref>. The scale parameter of these random Fourier feature embeddings is set to 16. We additionally train the model with our continuous objective Eq. ( <ref type="formula">7</ref>). Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024 ˆ1024 CelebA-HQ <ref type="bibr" target="#b20">(Karras et al., 2018)</ref>, a task that was previously only achievable by some GAN models and VQ-VAE-2 <ref type="bibr" target="#b35">(Razavi et al., 2019)</ref>. We used a batch size of 8, increased the EMA rate to 0.9999, and trained the NCSN++ model with the continuous objective (Eq. ( <ref type="formula">7</ref>)) for around 700k iterations. We use the PC sampler discretized at 2000 steps with one Langevin step per noise scale and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 4. We use the "residual" progressive architecture for the input, and "output skip" progressive architecture for the output. We provide samples in Fig. <ref type="figure" target="#fig_10">11</ref>. Although these samples are not perfect (e.g., there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I CONTROLLABLE GENERATION</head><p>Consider a forward SDE with the following general form dx " f px, tqdt `Gpx, tqdw, and suppose the initial state distribution is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using <ref type="bibr" target="#b1">Anderson (1982)</ref>, the reverse-time SDE is given by dx " tf px, tq ´∇ ¨rGpx, tqGpx, tq T s ´Gpx, tqGpx, tq To test this idea, we trained a Wide ResNet <ref type="bibr" target="#b47">(Zagoruyko &amp; Komodakis, 2016</ref>) (Wide-ResNet-28-10) on CIFAR-10 with VE perturbations. The classifier is conditioned on log σ i using random Fourier features <ref type="bibr" target="#b44">(Tancik et al., 2020)</ref>, and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in Fig. <ref type="figure" target="#fig_11">12</ref>. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in Tab. 3, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in Fig. <ref type="figure" target="#fig_6">5</ref>, and an extended set of conditional samples is given in Fig. <ref type="figure" target="#fig_11">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 IMPUTATION</head><p>Imputation is a special case of conditional sampling. Denote by Ωpxq and Ωpxq the known and unknown dimensions of x respectively, and let fΩp¨, tq and GΩp¨, tq denote f p¨, tq and Gp¨, tq restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient f p¨, tq is element-wise, and the diffusion coefficient Gp¨, tq is diagonal. When f p¨, tq is element-wise, fΩp¨, tq denotes the same element-wise function applied only to the unknown dimensions. When Gp¨, tq is diagonal, GΩp¨, tq denotes the sub-matrix restricted to unknown dimensions.  For imputation, our goal is to sample from pp Ωpxp0qq | Ωpxp0qq " yq. Define a new diffusion process zptq " Ωpxptqq, and note that the SDE for zptq can be written as dz " fΩpz, tqdt `GΩpz, tqdw.</p><p>The reverse-time SDE, conditioned on Ωpxp0qq " y, is given by dz " fΩpz, tq ´∇ ¨rGΩpz, tqGΩpz, tq T s ´GΩpz, tqGΩpz, tq We provided an extended set of inpainting results in Figs. 13 and 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 COLORIZATION</head><p>Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is ˜0.577 ´0.816 0 0.577 0.408 0.707 0.577 0.408 ´0.707</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¸.</head><p>Because the transformations are all orthogonal matrices, the standard Wiener process wptq will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in Appendix I.2.</p><p>We provide an extended set of colorization results in Figs. <ref type="figure" target="#fig_15">15 and 16</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ForwardFigure 1 :</head><label>1</label><figDesc>Figure 1: Solving a reversetime SDE yields a score-based generative model. Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, ∇ x log p t pxq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 2</head><label>2</label><figDesc>DENOISING DIFFUSION PROBABILISTIC MODELS (DDPM) Sohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of score-based generative modeling through SDEs. We can map data to a noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling (Section 3.2). We can also reverse the associated probability flow ODE (Section 4.3), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score ∇ x log p t pxq (Section 3.3).</figDesc><graphic url="image-2.png" coords="4,111.77,105.47,392.57,99.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector. Samples are the best when computation is split between the predictor and corrector.</figDesc><graphic url="image-4.png" coords="6,126.36,213.40,178.20,130.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Probability flow ODE enables fast sampling with adaptive step-sizes as the numerical precision is varied (left), and reduces the number of score function evaluations (NFE) without harming quality (middle). The invertible mapping from latents to images allows for interpolations (right).</figDesc><graphic url="image-6.png" coords="7,203.20,239.16,121.39,80.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: Class-conditional samples on 32 ˆ32 CIFAR-10. Top four rows are automobiles and bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows) results on 256 ˆ256 LSUN. First column is the original image, second column is the masked/grayscale image, remaining columns are sampled image completions or colorizations.</figDesc><graphic url="image-9.png" coords="9,239.93,82.55,261.36,130.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Samples from the probability flow ODE on 256 ˆ256 CelebA-HQ with VP perturbations. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding).</figDesc><graphic url="image-11.png" coords="19,108.00,360.19,396.00,297.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. "Model A" and "Model B" are separately trained with different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The effects of different architecture components for score-based models trained with VE perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Samples on 1024 ˆ1024 CelebA-HQ from continuously trained NCSN++.</figDesc><graphic url="image-12.png" coords="26,108.00,98.11,396.00,594.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.</figDesc><graphic url="image-22.png" coords="27,162.19,524.84,126.72,133.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Extended inpainting results for 256 ˆ256 bedroom images.</figDesc><graphic url="image-23.png" coords="29,108.00,150.23,396.00,490.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Extended inpainting results for 256 ˆ256 church images.</figDesc><graphic url="image-24.png" coords="30,108.00,150.23,396.00,490.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Extended colorization results for 256 ˆ256 bedroom images.</figDesc><graphic url="image-25.png" coords="31,108.00,150.23,396.00,490.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Extended colorization results for 256 ˆ256 church images.</figDesc><graphic url="image-26.png" coords="32,108.00,150.23,396.00,490.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). To estimate ∇ x log p t pxq, we can train a time-dependent score-based model s θ px, tq via a continuous generalization to Eqs. (1) and (3): T s Ñ R `is a positive weighting function, t is sampled from a uniform distribution over r0, T s, xp0q " p 0 pxq and xptq " p 0t pxptq | xp0qq. With sufficient data and model capacity, score matching ensures that the optimal solution to Eq. (7), denoted by s θ ˚px, tq, equals ∇ x log p t pxq for almost all x and t. As in SMLD and DDPM, we can typically choose λ91{E " ∇ xptq log p 0t pxptq | xp0qq</figDesc><table><row><cell>θ ˚" arg min θ</cell><cell>E t</cell><cell>! λptqE xp0q E xptq</cell><cell>"</cell><cell>s θ pxptq, tq ´∇xptq log p 0t pxptq | xp0qq</cell><cell>2 2</cell><cell>‰ ) .</cell><cell>(7)</cell></row><row><cell cols="3">Here λ : r0, 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>˘.03 3.24 ˘.02 3.24 ˘.02 3.21 ˘.02 reverse diffusion 4.79 ˘.07 4.74 ˘.08 3.60 ˘.02 3.21 ˘.02 3.19 ˘.02 3.18 ˘.01 probability flow 15.41 ˘.15 10.54 ˘.08 20.43 ˘.07 3.51 ˘.04 3.59 ˘.04 3.23 ˘.03 19.06 ˘.06 3.06 ˘.03</figDesc><table><row><cell></cell><cell cols="4">Variance Exploding SDE (SMLD)</cell><cell cols="4">Variance Preserving SDE (DDPM)</cell></row><row><cell>FIDÓ</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell></row><row><cell cols="2">ancestral sampling 4.98 ˘.06</cell><cell>4.88 ˘.06</cell><cell></cell><cell>3.62</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>NLLs on CIFAR-10.</figDesc><table /><note>ModelNLL Test Ó FID Ó</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-10 sample quality.</figDesc><table><row><cell>Model</cell><cell>FIDÓ</cell><cell>ISÒ</cell></row><row><cell>Conditional</cell><cell></cell><cell></cell></row><row><cell>BigGAN (Brock et al., 2018)</cell><cell>14.73</cell><cell>9.22</cell></row><row><cell cols="2">StyleGAN2-ADA (Karras et al., 2020a) 2.42</cell><cell>10.14</cell></row><row><cell>Unconditional</cell><cell></cell><cell></cell></row><row><cell cols="2">StyleGAN2-ADA (Karras et al., 2020a) 2.92</cell><cell>9.83</cell></row><row><cell>NCSN</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparing different samplers on CIFAR-10, where "P2000" uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Variance Exploding SDE (SMLD)</cell><cell cols="4">Variance Preserving SDE (DDPM)</cell></row><row><cell>FIDÓ</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell></cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell><cell>P1000</cell><cell>P2000</cell><cell>C2000</cell><cell>PC1000</cell></row><row><cell cols="3">ancestral sampling 4.98 ˘.06</cell><cell>4.92 ˘.02</cell><cell></cell><cell cols="3">3.62 ˘.03 3.24 ˘.02 3.11 ˘.03</cell><cell></cell><cell>3.21 ˘.02</cell></row><row><cell cols="2">reverse diffusion</cell><cell>4.79 ˘.07</cell><cell>4.72 ˘.07</cell><cell>20.43 ˘.07</cell><cell cols="3">3.60 ˘.02 3.21 ˘.02 3.10 ˘.03</cell><cell>19.06 ˘.06</cell><cell>3.18 ˘.01</cell></row><row><cell cols="2">probability flow</cell><cell cols="2">15.41 ˘.15 12.87 ˘.09</cell><cell></cell><cell cols="3">3.51 ˘.04 3.59 ˘.04 3.25 ˘.04</cell><cell></cell><cell>3.06 ˘.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="10">: Optimal signal-to-noise ratios of different samplers. "P1000" or "P2000": predictor-only</cell></row><row><cell cols="10">samplers using 1000 or 2000 steps. "C2000": corrector-only samplers using 2000 steps. "PC1000":</cell></row><row><cell cols="6">PC samplers using 1000 predictor and 1000 corrector steps.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">VE SDE (SMLD)</cell><cell></cell><cell></cell><cell cols="2">VP SDE (DDPM)</cell></row><row><cell>r</cell><cell>Sampler</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predictor</cell><cell></cell><cell cols="8">P1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000</cell></row><row><cell cols="2">ancestral sampling</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.17</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.01</cell></row><row><cell cols="2">reverse diffusion</cell><cell>-</cell><cell>-</cell><cell>0.22</cell><cell>0.16</cell><cell>-</cell><cell>-</cell><cell>0.27</cell><cell>0.01</cell></row><row><cell cols="2">probability flow</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.17</cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.04</cell></row><row><cell cols="2">H.1 SETTINGS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">We consider two datasets: 32 ˆ32 CIFAR-10 (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>These changes improve the FID on CIFAR-10 from 2.45 to 2.38, resulting in a model dubbed NCSN++ cont. (4 blocks/resolution). Finally, we can further improve the FID from 2.38 to 2.2 by doubling the number of residual blocks per resolution, resulting in the model denoted as NCSN++ cont. (8 blocks/resolution). All results are summarized in Tab. 3.</figDesc><table><row><cell>H.3 HIGH RESOLUTION IMAGES</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>T ∇ x log p t px | yqudt `Gpx, tqd w. (43) Since p t pxptq | yq9p t pxptqqppy | xptqq, the score ∇ x log p t pxptq | yq can be computed easily by ∇ x log p t pxptq | yq " ∇ x log p t pxptqq `∇x log ppy | xptqq. (44) This subsumes the conditional reverse-time SDE in Eq. (13) as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation. I.1 CLASS-CONDITIONAL SAMPLING When y represents class labels, we can train a time-dependent classifier p t py | xptqq for classconditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data pxptq, yq by first sampling pxp0q, yq from a dataset and then obtaining xptq " p 0t pxptq | xp0qq. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the time-dependent classifier p t py | xptqq.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>T ∇ z log p t pz | Ωpzp0qq " yq ( dt `GΩpz, tqd w. Although p t pzptq | Ωpxp0qq " yq is in general intractable, it can be approximated. Let A denote the event Ωpxp0qq " y. We have p t pzptq | Ωpxp0qq " yq " p t pzptq | Aq " ż p t pzptq | Ωpxptqq, Aqp t pΩpxptqq | AqdΩpxptqq " E ptpΩpxptqq|Aq rp t pzptq | Ωpxptqq, Aqs « E ptpΩpxptqq|Aq rp t pzptq | Ωpxptqqqs « p t pzptq | Ωpxptqqq, where Ωpxptqq is a random sample from p t pΩpxptqq | Aq, which is typically a tractable distribution. Therefore, ∇ z log p t pzptq | Ωpxp0qq " yq « ∇ z log p t pzptq | Ωpxptqqq " ∇ z log p t przptq; Ωpxptqqsq, where rzptq; Ωpxptqqs denotes a vector uptq such that Ωpuptqq " Ωpxptqq and Ωpuptqq " zptq, and the identity holds because ∇ z log p t przptq; Ωpxptqqsq " ∇ z log p t pzptq | Ωpxptqqq ∇z log pp Ωpxptqqq " ∇ z log p t pzptq | Ωpxptqqq.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, and Tim Salimans for their insightful discussions during the course of this project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where f p¨, tq : R d Ñ R d and Gp¨, tq : R d Ñ R dˆd . The marginal probability density p t pxptqq evolves according to the forward Kolmogorov equation (Fokker-Planck equation) <ref type="bibr">(Øksendal, 2003)</ref> </p><p>We can easily rewrite Eq. ( <ref type="formula">30</ref>) to obtain  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Numerical continuation methods: an introduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Allgower</surname></persName>
		</author>
		<author>
			<persName><surname>Georg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName><forename type="first">D O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Process. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1982-05">May 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to generate samples from noise through infusion training</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06975</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning gradient fields for shape generation</title>
		<author>
			<persName><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9916" to="9926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tweedie&apos;s formula and selection bias</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">496</biblScope>
			<biblScope unit="page" from="1602" to="1614" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4392" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representations of knowledge in complex systems</title>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="581" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</title>
		<author>
			<persName><surname>Michael F Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="1990-04">1990. Apr. 2005</date>
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
	</analytic>
	<monogr>
		<title level="m">Rémi Tachet des Combes, and Ioannis Mitliagkas</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Numerical solution of stochastic differential equations</title>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Peter E Kloeden</surname></persName>
		</author>
		<author>
			<persName><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
				<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Interacting particle solutions of fokker-planck equations through gradient-log-density estimation</title>
		<author>
			<persName><forename type="first">Dimitra</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00702</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName><forename type="first">Neal</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v108/niu20a.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="4474" to="4484" />
			<date type="published" when="2020-08-28">Online, 26-28 Aug 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic differential equations</title>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Øksendal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic differential equations</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="65" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient learning of generative models via finite-difference score matching</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03317</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Correlation functions and computer simulations</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Physics B</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00810</idno>
		<title level="m">On linear identifiability of learned representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Applied stochastic differential equations</title>
		<author>
			<persName><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The eigenvalues of mega-dimensional matrices</title>
		<author>
			<persName><forename type="first">John</forename><surname>Skilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="http://auai.org/uai2019/proceedings/papers/204.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019</title>
				<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">July 22-25, 2019. 2019a</date>
			<biblScope unit="page">204</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mintnet: Building invertible neural networks with masked convolutions</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="11002" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
