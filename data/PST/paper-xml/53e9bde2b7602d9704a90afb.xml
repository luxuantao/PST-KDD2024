<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exposing Digital Image Forgeries by Illumination Color Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tiago</forename><surname>José De Carvalho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="laboratory">RECOD Laboratory</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<postCode>13083-970</postCode>
									<settlement>Campinas</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Barni</forename><forename type="middle">T J</forename><surname>Mauro</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Erlangen-Nuremberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>De Carvalho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Erlangen-Nuremberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pedrini</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Erlangen-Nuremberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Rocha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Erlangen-Nuremberg</orgName>
								<address>
									<postCode>91054</postCode>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exposing Digital Image Forgeries by Illumination Color Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C7F5A66DB179A73C9FA1430C2432200</idno>
					<idno type="DOI">10.1109/TIFS.2013.2265677</idno>
					<note type="submission">received February 05, 2013; revised May 07, 2013; accepted May 08, 2013. Date of publication June 03, 2013; date of current version June 13, 2013. This work was supported in part by Unicamp, in part by Unicamp Faepex,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Color constancy</term>
					<term>illuminant color</term>
					<term>image forensics</term>
					<term>machine learning</term>
					<term>spliced image detection</term>
					<term>texture and edge descriptors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For decades, photographs have been used to document space-time events and they have often served as evidence in courts. Although photographers are able to create composites of analog pictures, this process is very time consuming and requires expert knowledge. Today, however, powerful digital image editing software makes image modifications straightforward. This undermines our trust in photographs and, in particular, questions pictures as evidence for real-world events. In this paper, we analyze one of the most common forms of photographic manipulation, known as image composition or splicing. We propose a forgery detection method that exploits subtle inconsistencies in the color of the illumination of images. Our approach is machine-learningbased and requires minimal user interaction. The technique is applicable to images containing two or more people and requires no expert interaction for the tampering decision. To achieve this, we incorporate information from physics-and statistical-based illuminant estimators on image regions of similar material. From these illuminant estimates, we extract texture-and edge-based features which are then provided to a machine-learning approach for automatic decision-making. The classification performance using an SVM meta-fusion classifier is promising. It yields detection rates of 86% on a new benchmark dataset consisting of 200 images, and 83% on 50 images that were collected from the Internet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E VERY day, millions of digital documents are produced by a variety of devices and distributed by newspapers, magazines, websites and television. In all these information channels, images are a powerful tool for communication. Unfortunately, it is not difficult to use computer graphics and image processing techniques to manipulate images. Quoting Russell Frank, a Professor of Journalism Ethics at Penn State University, in 2003 after a Los Angeles Times incident involving a doctored photograph from the Iraqi front: "Whoever said the camera never lies was a liar". How we deal with photographic manipulation raises a host of legal and ethical questions that must be addressed <ref type="bibr" target="#b0">[1]</ref>. However, before thinking of taking appropriate actions upon a questionable image, one must be able to detect that an image has been altered.</p><p>Image composition (or splicing) is one of the most common image manipulation operations. One such example is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, in which the girl on the right is inserted. Although this image shows a harmless manipulation case, several more controversial cases have been reported, e.g., the 2011 Benetton Un-Hate advertising campaign <ref type="foot" target="#foot_0">1</ref> or the diplomatically delicate case in which an Egyptian state-run newspaper published a manipulated photograph of Egypt's former president, Hosni Mubarak, at the front, rather than the back, of a group of leaders meeting for peace talks <ref type="foot" target="#foot_1">2</ref> .</p><p>When assessing the authenticity of an image, forensic investigators use all available sources of tampering evidence. Among other telltale signs, illumination inconsistencies are potentially effective for splicing detection: from the viewpoint of a manipulator, proper adjustment of the illumination conditions is hard to achieve when creating a composite image <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this spirit, Riess and Angelopoulou <ref type="bibr" target="#b1">[2]</ref> proposed to analyze illuminant color estimates from local image regions. Unfortunately, the interpretation of their resulting so-called illuminant maps is left to human experts. As it turns out, this decision is, in practice, often challenging. Moreover, relying on visual assessment can be misleading, as the human visual system is quite inept at judging illumination environments in pictures <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Thus, it is preferable to transfer the tampering decision to an objective algorithm.</p><p>In this work, we make an important step towards minimizing user interaction for an illuminant-based tampering decision-making. We propose a new semiautomatic method that is also significantly more reliable than earlier approaches. Quantitative evaluation shows that the proposed method achieves a detection rate of 86%, while existing illumination-based work is slightly better than guessing. We exploit the fact that local illuminant estimates are most discriminative when comparing objects of the same (or similar) material. Thus, we focus on the automated comparison of human skin, and more specifically faces, to classify the illumination on a pair of faces as either consistent or inconsistent. User interaction is limited to marking bounding boxes around the faces in an image under investigation. In the simplest case, this reduces to specifying two corners (upper left and lower right) of a bounding box.</p><p>In summary, the main contributions of this work are:</p><p>• Interpretation of the illumination distribution as object texture for feature computation. • A novel edge-based characterization method for illuminant maps which explores edge attributes related to the illumination process. • The creation of a benchmark dataset comprised of 100 skillfully created forgeries and 100 original photographs <ref type="foot" target="#foot_2">3</ref>In Section II, we briefly review related work in color constancy and illumination-based detection of image splicing. In Section III, we present examples of illuminant maps and highlight the challenges in their exploitation. An overview of the proposed methodology, followed by a detailed explanation of all the algorithmic steps is given in Section IV. In Section V, we introduce the proposed benchmark database and present experimental results. Conclusions and potential future work are outlined in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Illumination-based methods for forgery detection are either geometry-based or color-based. Geometry-based methods focus at detecting inconsistencies in light source positions between specific objects in the scene <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Color-based methods search for inconsistencies in the interactions between object color and light color <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Two methods have been proposed that use the direction of the incident light for exposing digital forgeries. Johnson and Farid <ref type="bibr" target="#b6">[7]</ref> proposed a method which computes a low-dimensional descriptor of the lighting environment in the image plane (i.e., in 2-D). It estimates the illumination direction from the intensity distribution along manually annotated object boundaries of homogeneous color. Kee and Farid <ref type="bibr" target="#b8">[9]</ref> extended this approach to exploiting known 3-D surface geometry. In the case of faces, a dense grid of 3-D normals improves the estimate of the illumination direction. To achieve this, a 3-D face model is registered with the 2-D image using manually annotated facial landmarks. Fan et al. <ref type="bibr" target="#b9">[10]</ref> propose a method for estimating 3-D illumination using shape-from-shading. In contrast to <ref type="bibr" target="#b8">[9]</ref>, no 3-D model of the object is required. However, this flexibility comes at the expense of a reduced reliability of the algorithm.</p><p>Johnson and Farid <ref type="bibr" target="#b7">[8]</ref> also proposed spliced image detection by exploiting specular highlights in the eyes. In a subsequent extension, Saboia et al. <ref type="bibr" target="#b13">[14]</ref> automatically classified these images by extracting additional features, such as the viewer position. The applicability of both approaches, however, is somewhat limited by the fact that people's eyes must be visible and available in high resolution.</p><p>Gholap and Bora <ref type="bibr" target="#b11">[12]</ref> introduced physics-based illumination cues to image forensics. The authors examined inconsistencies in specularities based on the dichromatic reflectance model. Specularity segmentation on real-world images is challenging <ref type="bibr" target="#b14">[15]</ref>. Therefore, the authors require manual annotation of specular highlights. Additionally, specularities have to be present on all regions of interest, which limits the method's applicability in real-world scenarios. To avoid this problem, Wu and Fang <ref type="bibr" target="#b12">[13]</ref> assume purely diffuse (i.e., specular-free) reflectance, and train a mixture of Gaussians to select a proper illuminant color estimator. The angular distance between illuminant estimates from selected regions can then be used as an indicator for tampering. Unfortunately, the method requires the manual selection of a "reference block", where the color of the illuminant can be reliably estimated. This is a significant limitation of the method (as our experiments also show).</p><p>Riess and Angelopoulou <ref type="bibr" target="#b1">[2]</ref> followed a different approach by using a physics-based color constancy algorithm that operates on partially specular pixels. In this approach, the automatic detection of highly specular regions is avoided. The authors propose to segment the image to estimate the illuminant color locallyper segment. Recoloring each image region according to its local illuminant estimate yields a so-called illuminant map . Implausible illuminant color estimates point towards a manipulated region. Unfortunately, the authors do not provide a numerical decision criterion for tampering detection. Thus, an expert is left with the difficult task of visually examining an illuminant map for evidence of tampering. The involved challenges are further discussed in Section III.</p><p>In the field of color constancy, descriptors for the illuminant color have been extensively studied. Most research in color constancy focuses on uniformly illuminated scenes containing a single dominant illuminant. For an overview, see e.g., <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. However, in order to use the color of the incident illumination as a sign of image tampering, we require multiple, spatially-bound illuminant estimates. So far, limited research has been done in this direction. The work by Bleier et al. <ref type="bibr" target="#b18">[19]</ref> indicates that many off-the-shelf single-illuminant algorithms do not scale well on smaller image regions. Thus, problem-specific illuminant estimators are required.</p><p>Ebner <ref type="bibr" target="#b19">[20]</ref> presented an early approach to multi-illuminant estimation. Assuming smoothly blending illuminants, the author proposes a diffusion process to recover the illumination distribution. Unfortunately, in practice, this approach oversmooths the illuminant boundaries. Gijsenij et al. <ref type="bibr" target="#b20">[21]</ref> proposed a pixelwise illuminant estimator. It allows to segment an image into regions illuminated by distinct illuminants. Differently illuminated regions can have crisp transitions, for instance between sunlit and shadow areas. While this is an interesting approach, a single illuminant estimator can always fail. Thus, for forensic purposes, we prefer a scheme that combines the results of multiple illuminant estimators. Earlier, Kawakami et al. <ref type="bibr" target="#b21">[22]</ref> proposed a physics-based approach that is custom-tailored for discriminating shadow/sunlit regions. However, for our work, we consider the restriction to outdoor images overly limiting.</p><p>In this paper, we build upon the ideas by <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b12">[13]</ref>. We use the relatively rich illumination information provided by both physics-based and statistics-based color constancy methods as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Decisions with respect to the illuminant color estimators are completely taken away from the user, which differentiates this paper from prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHALLENGES IN EXPLOITING ILLUMINANT MAPS</head><p>To illustrate the challenges of directly exploiting illuminant estimates, we briefly examine the illuminant maps generated by the method of Riess and Angelopoulou <ref type="bibr" target="#b1">[2]</ref>. In this approach, an image is subdivided into regions of similar color (superpixels). An illuminant color is locally estimated using the pixels within each superpixel (for details, see <ref type="bibr" target="#b1">[2]</ref> and Section IV-A). Recoloring each superpixel with its local illuminant color estimate yields a so-called illuminant map. A human expert can then investigate the input image and the illuminant map to detect inconsistencies.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows an example image and its illuminant map, in which an inconsistency can be directly shown: the inserted mandarin orange in the top right exhibits multiple green spots in the illuminant map. All other fruits in the scene show a gradual transition from red to blue. The inserted mandarin orange is the only one that deviates from this pattern.</p><p>In practice, however, such analysis is often challenging, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The top left image is original, while the bottom image is a composite with the right-most girl inserted. Several illuminant estimates are clear outliers, such as the hair of the girl on the left in the bottom image, which is estimated as strongly red illuminated. Thus, from an expert's viewpoint, it is reasonable to discard such regions and to focus on more reliable regions, e.g., the faces. In Fig. <ref type="figure" target="#fig_2">3</ref>, however, it is difficult to justify a tampering decision by comparing the color distributions in the facial regions. It is also challenging to argue, based on these illuminant maps, that the right-most girl in the bottom image has been inserted, while, e.g., the right-most boy in the top image is original.</p><p>Although other methods operate differently, the involved challenges are similar. For instance, the approach by Gholap and Bora <ref type="bibr" target="#b11">[12]</ref> is severely affected by clipping and camera white-balancing, which is almost always applied on images from off-the-shelf cameras. Wu and Fang <ref type="bibr" target="#b12">[13]</ref> implicitly create illuminant maps and require comparison to a reference region. However, different choices of reference regions lead to different results, and this makes this method error-prone.</p><p>Thus, while illuminant maps are an important intermediate representation, we emphasize that further, automated processing is required to avoid biased or debatable human decisions. Hence, we propose a pattern recognition scheme operating on illuminant maps. The features are designed to capture the shape of the superpixels in conjunction with the color distribution. In this spirit, our goal is to replace the expert-in-the-loop, by only requiring annotations of faces in the image.</p><p>Note that, the estimation of the illuminant color is error-prone and affected by the materials in the scene. However, (cf. also Fig. <ref type="figure" target="#fig_1">2</ref>), estimates on objects of similar material exhibit a lower relative error. Thus, we limit our detector to skin, and in particular to faces. Pigmentation is the most obvious difference in skin characteristics between different ethnicities. This pigmentation difference depends on many factors as quantity of melanin, amount of UV exposure, genetics, melanosome content and type of pigments found in the skin <ref type="bibr" target="#b23">[24]</ref>. However, this intramaterial variation is typically smaller than that of other materials possibly occurring in a scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERVIEW AND ALGORITHMIC DETAILS</head><p>We classify the illumination for each pair of faces in the image as either consistent or inconsistent. Throughout the paper, we abbreviate illuminant estimation as IE, and illuminant maps as IM. The proposed method consists of five main components:</p><p>1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Dense Local Illuminant Estimation (IE):</head><p>The input image is segmented into homogeneous regions. Per illuminant estimator, a new image is created where each region is colored with the extracted illuminant color. This resulting intermediate representation is called illuminant map (IM). 2) Face Extraction: This is the only step that may require human interaction. An operator sets a bounding box around each face (e.g., by clicking on two corners of the bounding box) in the image that should be investigated. Alternatively, an automated face detector can be employed. We then crop every bounding box out of each illuminant map, so that only the illuminant estimates of the face regions remain. 3) Computation of Illuminant Features: for all face regions, texture-based and gradient-based features are computed on the IM values. Each one of them encodes complementary information for classification. 4) Paired Face Features: Our goal is to assess whether a pair of faces in an image is consistently illuminated. For an image with faces, we construct joint feature vectors, consisting of all possible pairs of faces. 5) Classification: We use a machine learning approach to automatically classify the feature vectors. We consider an image as a forgery if at least one pair of faces in the image is classified as inconsistently illuminated. Fig. <ref type="figure" target="#fig_3">4</ref> summarizes these steps. In the remainder of this section, we present the details of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dense Local Illuminant Estimation</head><p>To compute a dense set of localized illuminant color estimates, the input image is segmented into superpixels, i.e., regions of approximately constant chromaticity, using the algorithm by Felzenszwalb and Huttenlocher <ref type="bibr" target="#b24">[25]</ref>. Per superpixel, the color of the illuminant is estimated. We use two separate illuminant color estimators: the statistical generalized gray world estimates and the physics-based inverse-intensity chromaticity space, as we explain in the next subsection. We obtain, in total, two illuminant maps by recoloring each superpixel with the estimated illuminant chromaticities of each one of the estimators. Both illuminant maps are independently analyzed in the subsequent steps.</p><p>1) Generalized Gray World Estimates: The classical gray world assumption by Buchsbaum <ref type="bibr" target="#b25">[26]</ref> states that the average color of a scene is gray. Thus, a deviation of the average of the image intensities from the expected gray color is due to the illuminant. Although this assumption is nowadays considered to be overly simplified <ref type="bibr" target="#b16">[17]</ref>, it has inspired the further design of statistical descriptors for color constancy. We follow an extension of this idea, the generalized gray world approach by van de Weijer et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>Let denote the observed RGB color of a pixel at location . Van de Weijer et al. s <ref type="bibr" target="#b22">[23]</ref> assume purely diffuse reflection and linear camera response. Then, is formed by <ref type="bibr" target="#b0">(1)</ref> where denotes the spectrum of visible light, denotes the wavelength of the light, denotes the spectrum of the illuminant, the surface reflectance of an object, and the color sensitivities of the camera (i.e., one function per color channel). Van de Weijer et al. <ref type="bibr" target="#b22">[23]</ref> extended the original gray world hypothesis through the incorporation of three parameters:</p><p>• Derivative order : the assumption that the average of the illuminants is achromatic can be extended to the absolute value of the sum of the derivatives of the image. • Minkowski norm : instead of simply adding intensities or derivatives, respectively, greater robustness can be achieved by computing the -th Minkowski norm of these values. • Gaussian smoothing : to reduce image noise, one can smooth the image prior to processing with a Gaussian kernel of standard deviation . Putting these three aspects together, van de Weijer et al. proposed to estimate the color of the illuminant as <ref type="bibr" target="#b1">(2)</ref> Here, the integral is computed over all pixels in the image, where denotes a particular position (pixel coordinate). Furthermore, denotes a scaling factor, the absolute value, the differential operator, and the observed intensities at position , smoothed with a Gaussian kernel . Note that can be computed separately for each color channel. Compared to the original gray world algorithm, the derivative operator increases the robustness against homogeneously colored regions of varying sizes. Additionally, the Minkowski norm emphasizes strong derivatives over weaker derivatives, so that specular edges are better exploited <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Inverse Intensity-Chromaticity Estimates:</head><p>The second illuminant estimator we consider in this paper is the so-called inverse intensity-chromaticity (IIC) space. It was originally proposed by Tan et al. <ref type="bibr" target="#b27">[28]</ref>. In contrast to the previous approach, the observed image intensities are assumed to exhibit a mixture of diffuse and specular reflectance. Pure specularities are assumed to consist of only the color of the illuminant. Let (as above) be a column vector of the observed RGB colors of a pixel. Then, using the same notation as for the generalized gray world model, is modelled as</p><formula xml:id="formula_0">(3)</formula><p>Let be the intensity and be the chromaticity (i.e., normalized RGB-value) of a color channel at position , respectively. In addition, let be the chromaticity of the illuminant in channel . Then, after a somewhat laborious calculation, Tan et al. <ref type="bibr" target="#b27">[28]</ref> derived a linear relationship between , and by showing that (4)</p><p>Here, mainly captures geometric influences, i.e., light position, surface orientation and camera position. Although can not be analytically computed, an approximate solution is feasible. More importantly, the only aspect of interest in illuminant color estimation is the -intercept . This can be directly estimated by analyzing the distribution of pixels in IIC space. The IIC space is a per-channel 2-D space, where the horizontal axis is the inverse of the sum of the chromaticities per pixel, , and the vertical axis is the pixel chromaticity for that particular channel. Per color channel , the pixels within a superpixel are projected onto inverse intensity-chromaticity (IIC) space.  the illuminant chromaticity is estimated from the joint -axis intercept of all spikes in IIC space <ref type="bibr" target="#b27">[28]</ref>.</p><p>In natural images, noise dominates the IIC diagrams. Riess and Angelopoulou <ref type="bibr" target="#b1">[2]</ref> proposed to compute these estimates over a large number of small image patches. The final illuminant estimate is computed by a majority vote of these estimates. Prior to the voting, two constraints are imposed on a patch to improve noise resilience. If a patch does not satisfy these constraints, it is excluded from voting.</p><p>In practice, these constraints are straightforward to compute. The pixel colors of a patch are projected onto IIC space. Principal component analysis on the distribution of the patch-pixels in IIC space yields two eigenvalues , and their associated eigenvectors and . Let be the larger eigenvalue. Then, is the principal axis of the pixel distribution in IIC space. In the two-dimensional IIC-space, the principal axis can be interpreted as a line whose slope can be directly computed from . Additionally, and can be used to compute the eccentricity as a metric for the shape of the distribution. Both constraints are associated with this eigenanalysis <ref type="foot" target="#foot_3">4</ref> . The first constraint is that the slope must exceed a minimum of 0.003. The second constraint is that the eccentricity has to exceed a minimum of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Extraction</head><p>We require bounding boxes around all faces in an image that should be part of the investigation. For obtaining the bounding boxes, we could in principle use an automated algorithm, e.g., the one by Schwartz et al. <ref type="bibr" target="#b29">[30]</ref>. However, we prefer a human operator for this task for two main reasons: a) this minimizes false detections or missed faces; b) scene context is important when judging the lighting situation. For instance, consider an image where all persons of interest are illuminated by flashlight. The illuminants are expected to agree with one another. Conversely, assume that a person in the foreground is illuminated by flashlight, and a person in the background is illuminated by ambient light. Then, a difference in the color of the illuminants is expected. Such differences are hard to distinguish in a fully-automated manner, but can be easily excluded in manual annotation.</p><p>We illustrate this setup in Fig. <ref type="figure" target="#fig_6">6</ref>. The faces in Fig. <ref type="figure" target="#fig_6">6</ref>(a) can be assumed to be exposed to the same illuminant. As Fig. <ref type="figure" target="#fig_6">6(b)</ref> shows, the corresponding gray world illuminant map for these two faces also has similar values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Texture Description: SASI Algorithm</head><p>We use the Statistical Analysis of Structural Information (SASI) descriptor by Carkacioglu and Yarman-Vural <ref type="bibr" target="#b30">[31]</ref> to extract texture information from illuminant maps. Recently, Penatti et al. <ref type="bibr" target="#b31">[32]</ref> pointed out that SASI performs remarkably well. For our application, the most important advantage of SASI is its capability of capturing small granularities and discontinuities in texture patterns. Distinct illuminant colors interact differently with the underlying surfaces, thus generating distinct illumination "texture". This can be a very fine texture, whose subtleties are best captured by SASI.</p><p>SASI is a generic descriptor that measures the structural properties of textures. It is based on the autocorrelation of horizontal, vertical and diagonal pixel lines over an image at different scales. Instead of computing the autocorrelation for every possible shift, only a small number of shifts is considered. One autocorrelation is computed using a specific fixed orientation, scale, and shift. Computing the mean and standard deviation of all such pixel values yields two feature dimensions. Repeating this computation for varying orientations, scales and shifts yields a 128-dimensional feature vector. As a final step, this vector is normalized by subtracting its mean value, and dividing it by its standard deviation. For details, please refer to <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interpretation of Illuminant Edges: Hogedge Algorithm</head><p>Differing illuminant estimates in neighboring segments can lead to discontinuities in the illuminant map. Dissimilar illuminant estimates can occur for a number of reasons: changing geometry, changing material, noise, retouching or changes in the incident light. Thus, one can interpret an illuminant estimate as a low-level descriptor of the underlying image statistics. We observed that the edges, e.g., computed by a Canny edge detector, detect in several cases a combination of the segment borders and isophotes (i.e., areas of similar incident light in the image). When an image is spliced, the statistics of these edges is likely to differ from original images. To characterize such edge discontinuities, we propose a new feature descriptor called HOGedge. It is based on the well-known HOG-descriptor, and computes visual dictionaries of gradient intensities in edge points. The full algorithm is described in the remainder of this section. Fig. <ref type="figure" target="#fig_7">7</ref> shows an algorithmic overview of the method. We first extract approximately equally distributed candidate points on the edges of illuminant maps. At these points, HOG descriptors are computed. These descriptors are summarized in a visual words dictionary. Each of these steps is presented in greater detail in the next subsections.</p><p>Extraction of Edge Points: Given a face region from an illuminant map, we first extract edge points using the Canny edge detector <ref type="bibr" target="#b32">[33]</ref>. This yields a large number of spatially close edge points. To reduce the number of points, we filter the Canny output using the following rule: starting from a seed point, we eliminate all other edge pixels in a region of interest (ROI) centered around the seed point. The edge points that are closest to the ROI (but outside of it) are chosen as seed points for the next iteration. By iterating this process over the entire image, we reduce the number of points but still ensure that every face has a  comparable density of points. Fig. <ref type="figure" target="#fig_8">8</ref> depicts an example of the resulting points.</p><p>Point Description: We compute Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b33">[34]</ref> to describe the distribution of the selected edge points. HOG is based on normalized local histograms of image gradient orientations in a dense grid. The HOG descriptor is constructed around each of the edge points. The neighborhood of such an edge point is called a cell. Each cell provides a local 1-D histogram of quantized gradient directions using all cell pixels. To construct the feature vector, the histograms of all cells within a spatially larger region are combined and contrast-normalized. We use the HOG output as a feature vector for the subsequent steps.</p><p>Visual Vocabulary: The number of extracted HOG vectors varies depending on the size and structure of the face under examination. We use visual dictionaries <ref type="bibr" target="#b34">[35]</ref> to obtain feature vectors of fixed length. Visual dictionaries constitute a robust representation, where each face is treated as a set of region descriptors. The spatial location of each region is discarded <ref type="bibr" target="#b35">[36]</ref>.</p><p>To construct our visual dictionary, we subdivide the training data into feature vectors from original and doctored images. Each group is clustered in clusters using the -means algorithm <ref type="bibr" target="#b36">[37]</ref>. Then, a visual dictionary with visual words is constructed, where each word is represented by a cluster center. Thus, the visual dictionary summarizes the most representative feature vectors of the training set. Algorithm 1 shows the pseudocode for the dictionary creation. Quantization Using the Precomputed Visual Dictionary: For evaluation, the HOG feature vectors are mapped to the visual dictionary. Each feature vector in an image is represented by the closest word in the dictionary (with respect to the Euclidean distance). A histogram of word counts represents the distribution of HOG feature vectors in a face. Algorithm 2 shows the pseudocode for the application of the visual dictionary on IMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 HOGedge-Face characterization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>(visual dictionary precomputed with visual words) (illuminant map from a face) Ensure:</p><p>(  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Face Pair</head><p>To compare two faces, we combine the same descriptors for each of the two faces. For instance, we can concatenate the SASI-descriptors that were computed on gray world. The idea is that a feature concatenation from two faces is different when one of the faces is an original and one is spliced. For an image containing faces , the number of face pairs is . The SASI and HOGedge descriptors capture two different properties of the face regions. From a signal processing point of view, both descriptors are signatures with different behavior. Fig. <ref type="figure" target="#fig_9">9</ref> shows a very high-level visualization of the distinct information that is captured by these two descriptors. For one of the folds of our experiments (see Section V-C), we computed the mean value and standard deviation per feature dimension. For a less cluttered plot, we only visualize the feature dimensions with the largest difference in the mean values for this fold. This experiment empirically demonstrates two points. Firstly, SASI and HOGedge, in combination with the IIC-based and gray world illuminant maps create features that discriminate well between original and tampered images, in at least some dimensions. Secondly, the dimensions, where these features have distinct value, vary between the four combinations of the feature vectors. We exploit this property during classification by fusing the output of the classification on both feature sets, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Classification</head><p>We classify the illumination for each pair of faces in an image as either consistent or inconsistent. Assuming all selected faces are illuminated by the same light source, we tag an image as manipulated if one pair is classified as inconsistent. Individual feature vectors, i.e., SASI or HOGedge features on either gray world or IIC-based illuminant maps, are classified using a support vector machine (SVM) classifier with a radial basis function (RBF) kernel.</p><p>The information provided by the SASI features is complementary to the information from the HOGedge features. Thus, we use a machine learning-based fusion technique for improving the detection performance. Inspired by the work of Ludwig et al. <ref type="bibr" target="#b37">[38]</ref>, we use a late fusion technique named SVM-Meta Fusion. We classify each combination of illuminant map and feature type independently (i.e., SASI-Gray-World, SASI-IIC, HOGedge-Gray-World and HOGedge-IIC) using a two-class SVM classifier to obtain the distance between the image's feature vectors and the classifier decision boundary. SVM-Meta Fusion then merges the marginal distances provided by all individual classifiers to build a new feature vector. Another SVM classifier (i.e., on meta level) classifies the combined feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>To validate our approach, we performed six rounds of experiments using two different databases of images involving people. We show results using classical ROC curves where sensitivity represents the number of composite images correctly classified and specificity represents the number of original images (non-manipulated) correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Data</head><p>To quantitatively evaluate the proposed algorithm, and to compare it to related work, we considered two datasets. One consists of images that we captured ourselves, while the second one contains images collected from the internet. Additionally, we validated the quality of the forgeries using a human study on the first dataset. Human performance can be seen as a baseline for our experiments.</p><p>1) DSO-1: This is our first dataset and it was created by ourselves. It is composed of 200 indoor and outdoor images with an image resolution of . Out of this set of images, 100 are original, i.e., have no adjustments whatsoever, and 100 are forged. The forgeries were created by adding one or more individuals in a source image that already contained one or more persons. When necessary, we complemented an image splicing operation with postprocessing operations (such as color and brightness adjustments) in order to increase photorealism.</p><p>2) DSI-1: This is our second dataset and it is composed of 50 images (25 original and 25 doctored) downloaded from different websites in the Internet with different resolutions <ref type="foot" target="#foot_4">5</ref> . Fig. <ref type="figure" target="#fig_10">10</ref> depicts some example images from our databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human Performance in Spliced Image Detection</head><p>To demonstrate the quality of DSO-1 and the difficulty in discriminating original and tampered images, we performed an experiment where we asked humans to mark images as tampered or original. To accomplish this task, we have used Amazon Mechanical Turk <ref type="foot" target="#foot_5">6</ref> . Note that on Mechanical Turk categorization ex- periments, each batch is evaluated only by experienced users which generally leads to a higher confidence in the outcome of the task. In our experiment, we setup five identical categorization experiments, where each one of them is called a batch. Within a batch, all DSO-1 images have been evaluated. For each image, two users were asked to tag the image as original or manipulated. Each image was assessed by ten different users, each user expended on average 47 seconds to tag an image. The final accuracy, averaged over all experiments, was 64.6%. However, for spliced images, the users achieved only an average accuracy of 38.3%, while human accuracy on the original images was 90.9%. The kappa-value, which measures the degree of agreement between an arbitrary number of raters in deciding the class of a sample, based on the Fleiss <ref type="bibr" target="#b38">[39]</ref> model, is 0.11. Despite being subjective, this kappa-value, according to the Landis and Koch <ref type="bibr" target="#b39">[40]</ref> scale, suggests a slight degree of agreement between users, which further supports our conjecture about the difficulty of forgery detection in DSO-1 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of Forgery Detection Using Semiautomatic Face Annotation in DSO-1</head><p>We compare five variants of the method proposed in this paper. Throughout this section, we manually annotated the faces using corner clicking (see Section V-D). In the classification stage, we use a five-fold cross validation protocol, an SVM classifier with an RBF kernel, and classical grid search for adjusting parameters in training samples <ref type="bibr" target="#b36">[37]</ref>. Due to the different number of faces per image, the number of feature vectors for the original and the spliced images is not exactly equal. To address this issue during training, we weighted feature vectors from original and composite images. Let and denote the number of feature vectors from original and composite images, respectively. To obtain a proportional class weighting, we set the weight of features from original images to and the weight of features from composite images to . We compared the five variants SASI-IIC, SASI-Gray-World, HOGedge-IIC, HOGedge-Gray-World and Metafusion. Compound names, such as SASI-IIC, indicate the data source (in this case: IIC-based illuminant maps) and the subsequent feature extraction method (in this case: SASI). The single components are configured as follows:</p><p>• IIC: IIC-based illuminant maps are computed as described in <ref type="bibr" target="#b1">[2]</ref>. • Gray-World: Gray world illuminant maps are computed by setting , , and in (2). • SASI: The SASI descriptor is calculated over the channel from the color space. All remaining parameters are chosen as presented in <ref type="bibr" target="#b31">[32]</ref> <ref type="foot" target="#foot_6">7</ref> .</p><p>• HOGedge: Edge detection is performed on the channel of the color space, with a Canny low threshold of 0 and a high threshold of 10. The square region for edge point filtering was set to 32 32 pixels. Furthermore, we used 8-pixel cells without normalization in HOG. If applied on IIC-based illuminant maps, we computed 100 visual words for both the original and the tampered images (i.e., the dictionary consisted of 200 visual words). On gray world illuminant maps, the size of the visual word dictionary was set to 75 for each class, leading to a dictionary of 150 visual words.</p><p>• Metafusion: We implemented a late fusion as explained in Section IV-F. As input, it uses SASI-IIC, SASI-Gray-World, and HOGedge-IIC. We excluded HOGedge-Gray-World from the input methods, as its weaker performance leads to a slightly worse combined classification rate (see below). Fig. <ref type="figure" target="#fig_11">11</ref> depicts a ROC curve of the performance of each method using the corner clicking face localization. The area under the curve (AUC) is computed to obtain a single numerical measure for each result.</p><p>From the evaluated variants, Metafusion performs best, resulting in an AUC of 86.3%. In particular for high specificity (i.e., few false alarms), the method has a much higher sensitivity compared to the other variants. Thus, when the detection threshold is set to a high specificity, and a photograph is classified as composite, Metafusion provides to an expert high confidence that the image is indeed manipulated. Note also that Metafusion clearly outperforms human assessment in the baseline Mechanical Turk experiment (see Section V-B). Part of this improvement comes from the fact that Metafusion achieves, on spliced images alone, an average accuracy of 67%, while human performance was only 38.3%.</p><p>The second best variant is SASI-Gray-World, with an AUC of 84.0%. In particular for a specificity below 80.0%, the sensitivity is comparable to Metafusion. SASI-IIC achieved an AUC of 79.4%, followed by HOGedge-IIC with an AUC of 69.9% and HOGedge-Gray-World with an AUC of 64.7%. The weak performance of HOGedge-Gray-World comes from the fact that illuminant color estimates from the gray world algorithm vary more smoothly than IIC-based estimates. Thus, the differences in the illuminant map gradients (as extracted by the HOGedge descriptor) are generally smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fully Automated Versus Semiautomatic Face Detection</head><p>In order to test the impact of automated face detection, we reevaluated the best performing variant, Metafusion, on three versions of automation in face detection and annotation.</p><p>• Automatic Detection:we used the PLS-based face detector <ref type="bibr" target="#b29">[30]</ref> to detect faces in the images. In our experiments, the PLS detector successfully located all present faces in only 65% of our images. We then performed a 3-fold cross validation on this 65% of the images. For training the classifier, we used the manually annotated bounding boxes. In the test images, we used the bounding boxes found by the automated detector. • Semiautomatic Detection 1 (Eye Clicking): an expert does not necessarily have to mark a bounding box. In this variant, the expert clicks on the eye positions. The Euclidean distance between the eyes is the used to construct a bounding box for the face area. For classifier training and testing we use the same setup and images as in the automatic detection. • Semiautomatic Detection 2 (Corner Clicking): in this variant, we applied the same marking procedure as in the previous experiment and the same classifier training/testing procedure as in automatic detection. Fig. <ref type="figure" target="#fig_12">12</ref> shows the results of this experiment. The semiautomatic detection using corner clicking resulted in an AUC of 78.0%, while the semiautomatic using eye clicking and the fully-automatic approaches yielded an AUC of 63.5% and AUC of 63.0%, respectively. Thus, as it can also be seen in Figs. <ref type="figure" target="#fig_13">13(a</ref>)-13(c), proper face location is important for improved performance.</p><p>Although automatic face detection algorithms have improved over the years, we find user-selected faces more reliable for a forensic setup mainly because automatic face detection algorithms are not accurate in bounding box detection (location and size). In our experiments, automatic and eye clicking detection have generated an average bounding box size which was 38.4% and 24.7% larger than corner clicking detection, respectively. Thus, such bounding boxes include part of the background in a region that should contain just face information. The precision of bounding box location in automatic detection and eye clicking has also been worse than semiautomatic using corner clicking. Note, however, that the selection of faces under similar illumination conditions is a minor interaction that requires no particular knowledge in image processing or image forensics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison With State-of-the-art Methods</head><p>For experimental comparison, we implemented the methods by Gholap and Bora <ref type="bibr" target="#b11">[12]</ref> and Wu and Fang <ref type="bibr" target="#b12">[13]</ref>. Note that neither of these works includes a quantitative performance analysis. Thus, to our knowledge, this is the first direct comparison of illuminant color-based forensic algorithms.</p><p>For the algorithm by Gholap and Bora <ref type="bibr" target="#b11">[12]</ref>, three partially specular regions per image were manually annotated. For manipulated images, it is guaranteed that at least one of the regions belongs to the tampered part of the image, and one region to the original part. Fully saturated pixels were excluded from the computation, as they have presumably been clipped by the camera. Camera gamma was approximately inverted by assuming a value of 2.2. The maximum distance of the dichromatic lines per image were computed. The threshold for discriminating original and tampered images was set via five-fold cross-validation, yielding a detection rate of 55.5% on DSO-1.</p><p>In the implementation of the method by Wu and Fang, the Weibull distribution is computed in order to perform image classification prior to illuminant estimation. The training of the image classifier was performed on the ground truth dataset by Ciurea and Funt <ref type="bibr" target="#b40">[41]</ref> as proposed in the work <ref type="bibr" target="#b12">[13]</ref>. As the resolution of this dataset is relatively low, we performed the training on a central part of the images containing 180 240 pixels (excluding the ground-truth area). To provide images of the same resolution for illuminant classification, we manually annotated the face regions in DSO-1 with bounding boxes of fixed size ratio. Setting this ratio to 3:4, each face was then rescaled to a size of . As the selection of suitable reference regions is not well-defined (and also highly image-dependent), we directly compare the illuminant estimates of the faces in the scene. Here, the best result was obtained with three-fold cross-validation, yielding a detection rate of 57%. We performed five-fold cross-validation, as in the previous experiments. The results drop to 53% detection rate, which suggests that this algorithm is not very stable with respect to the selection of the data.</p><p>To reduce any bias that could be introduced from training on the dataset by Ciurea and Funt, we repeated the image classifier training on the reprocessed ground truth dataset by Gehler <ref type="bibr" target="#b41">[42]</ref>. During training, care was taken to exclude the ground truth information from the data. Repeating the remaining classification yielded a best result of 54.5% on two-fold cross-validation, or 53.5% for five-fold cross-validation.</p><p>Fig. <ref type="figure" target="#fig_14">14</ref> shows the ROC curves for both methods. The results of our method clearly outperform the state-of-the-art. However, these results also underline the challenge in exploiting illuminant color as a forensic cue on real-world images. Thus, we hope our database will have a significant impact in the development of new illuminant-based forgery detection algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Detection After Additional Image Processing</head><p>We also evaluated the robustness of our method against different processing operations. The results are computed on DSO-1. Apart from the additional preprocessing steps, the evaluation protocol was identical to the one described in Section V-C. In a first experiment, we examined the impact of JPEG compression. Using , the images were recompressed at the JPEG quality levels 70, 80 and 90. The detection rates were 63.5%, 64% and 69%, respectively. Using , we conducted a second experiment adding per image a random amount of Gaussian noise, with an attenuated value varying between 1% and 5%. On average, we obtained an accuracy of 59%. Finally, again using , we randomly varied the brightness and/or contrast of the image by either or . These brightness/contrast manipulations resulted in an accuracy of 61.5%.</p><p>These results are expected. For instance, the performance deterioration after strong JPEG compression introduces blocking artifacts in the segmentation of the illuminant maps. One could consider compensating for the JPEG artifacts with a deblocking algorithm. Still, JPEG compression is known to be a challenging scenario in several classes of forensic algorithms <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref> One could also consider optimizing the machine-learning part of the algorithm. However, here, we did not fine-tune the algorithm for such operations, as postprocessing can be addressed by specialized detectors, such as the work by Bayram et al. for brightness and contrast changes <ref type="bibr" target="#b45">[46]</ref>, combined with one of the recent JPEG-specific algorithms (e.g., <ref type="bibr" target="#b46">[47]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Performance of Forgery Detection Using a Cross-Database Approach</head><p>To evaluate the generalization of the algorithm with respect to the training data, we followed an experimental design similar to the one proposed by Rocha et al. <ref type="bibr" target="#b47">[48]</ref>. We performed a cross-database experiment, using DSO-1 as training set and the 50 images of DSI-1 (internet images) as test set. We used the pretrained Metafusion classifier from the best performing fold in Section V-C without further modification. Fig. <ref type="figure" target="#fig_15">15</ref> shows the ROC curve for this experiment. The results of this experiment are similar to the best ROC curve in Section V-C, with an AUC of 82.6%. This indicates that the proposed method offers a degree of generalization to images from different sources and to faces of varying sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this work, we presented a new method for detecting forged images of people using the illuminant color. We estimate the illuminant color using a statistical gray edge method and a physics-based method which exploits the inverse intensity-chromaticity color space. We treat these illuminant maps as texture maps. We also extract information on the distribution of edges on these maps. In order to describe the edge information, we propose a new algorithm based on edge-points and the HOG descriptor, called HOGedge. We combine these complementary cues (texture-and edge-baed) using machine learning late fusion. Our results are encouraging, yielding an AUC of over 86% correct classification. Good results are also achieved over internet images and under cross-database training/testing.</p><p>Although the proposed method is custom-tailored to detect splicing on images containing faces, there is no principal hindrance in applying it to other, problem-specific materials in the scene.</p><p>The proposed method requires only a minimum amount of human interaction and provides a crisp statement on the authenticity of the image. Additionally, it is a significant advancement in the exploitation of illuminant color as a forensic cue. Prior color-based work either assumes complex user interaction or imposes very limiting assumptions.</p><p>Although promising as forensic evidence, methods that operate on illuminant color are inherently prone to estimation errors. Thus, we expect that further improvements can be achieved when more advanced illuminant color estimators become available. For instance, while we were developing this work, Bianco and Schettini <ref type="bibr" target="#b48">[49]</ref> proposed a machine-learning based illuminant estimator particularly for faces. An incorporation of this method is subject of future work.</p><p>Reasonably effective skin detection methods have been presented in the computer vision literature in the past years. Incorporating such techniques can further expand the applicability of our method. Such an improvement could be employed, for instance, in detecting pornography compositions which, according to forensic practitioners, have become increasingly common nowadays.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. How can one assure the authenticity of a photograph? Example of a spliced image involving people.</figDesc><graphic coords="1,344.04,170.16,169.92,126.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example illuminant map that directly shows an inconsistency.</figDesc><graphic coords="3,75.00,64.14,181.02,87.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example illuminant maps for an original image (top) and a spliced image (bottom). The illuminant maps are created with the IIC-based illuminant estimator (see Section IV-A).</figDesc><graphic coords="3,306.00,64.14,246.00,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overview of the proposed method.</figDesc><graphic coords="4,49.02,64.14,492.00,199.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of the inverse intensity-chromaticity space (blue color channel). Left: synthetic image (violet and green balls). Right: specular pixels converge towards the blue portion of the illuminant color (recovered at the -axis intercept). Highly specular pixels are shown in red.</figDesc><graphic coords="5,57.00,64.14,216.00,96.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>depicts an exemplary IIC diagram for the blue channel. A synthetic image is rendered (left) and projected onto IIC space (right). Pixels from the green and purple balls form two clusters. The clusters have spikes that point towards the same location at the -axis. Considering only such spikes from each cluster,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Original image and its gray world map. Highlighted regions in the gray world map show a similar appearance. (a) Original. (b) Gray world with highlighted similar parts.</figDesc><graphic coords="5,325.98,64.14,205.02,102.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Overview of the proposed H OGedge algorithm.</figDesc><graphic coords="6,314.04,64.14,226.92,153.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Gray world IM for the left face in Fig. 6(a). (b) Result of the Canny edge detector when applied on this IM. (c) Final edge points after filtering using a square region. (a) IM derived from gray world. (b) Canny Edges. (c) Filtered Points.</figDesc><graphic coords="6,325.02,260.10,205.98,105.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Average signatures from original and spliced images. The horizontal axis corresponds to different feature dimensions, while the vertical axis represents the average feature value for different combinations of descriptors and illuminant maps. From top to bottom, left to right: SASI-IIC, HOGedge-IIC, SASI-Gray-World, HOGedge-Gray-World.</figDesc><graphic coords="7,321.00,64.14,216.00,174.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Original (left) and spliced images (right) from both databases. (a) DSO-1 Original image. (b) DSO-1 Spliced image. (c) DSI-1 Original image. (d) DSI-1 Spliced image.</figDesc><graphic coords="8,325.02,64.14,205.02,172.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparison of different variants of the algorithm using semiautomatic (corner clicking) annotated faces.</figDesc><graphic coords="9,43.02,64.14,243.96,183.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Experiments showing the differences for automatic and semiautomatic face detection.</figDesc><graphic coords="9,305.04,64.14,246.96,184.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Different types of face location. Automatic and semiautomatic locations select a considerable part of the background, whereas manual location is restricted to face regions. (a) Automatic. (b) Semiautomatic (eye clicking). (c) Semiautomatic (corner clicking).</figDesc><graphic coords="10,49.02,64.14,492.00,132.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Comparative results between our method and state-of-the-art approaches performed using DSO-1.</figDesc><graphic coords="10,303.00,248.16,249.00,185.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. ROC curve provided by cross-database experiment.</figDesc><graphic coords="11,304.02,64.14,249.00,186.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>HOGedge-Visual dictionary creation</figDesc><table><row><cell>Require:</cell><cell cols="3">(training database examples) (the number of</cell></row><row><cell cols="3">visual words per class)</cell></row><row><cell>Ensure:</cell><cell cols="2">(visual dictionary containing</cell><cell>visual words)</cell></row><row><cell></cell><cell>;</cell><cell></cell></row><row><cell></cell><cell>;</cell><cell></cell></row><row><cell></cell><cell>;</cell><cell></cell></row><row><cell cols="2">for each face IM</cell><cell>do</cell></row><row><cell></cell><cell cols="2">edge points extracted from ;</cell></row><row><cell cols="2">for each point</cell><cell>do</cell></row><row><cell></cell><cell cols="3">apply HOG in image at position ;</cell></row><row><cell cols="3">if is a doctored face then</cell></row><row><cell></cell><cell></cell><cell>;</cell></row><row><cell cols="2">else</cell><cell></cell></row><row><cell></cell><cell></cell><cell>;</cell></row><row><cell cols="2">end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell>Cluster</cell><cell>using centers;</cell><cell></cell></row><row><cell>Cluster</cell><cell>using centers;</cell><cell></cell></row><row><cell></cell><cell>;</cell><cell></cell></row><row><cell>return</cell><cell>;</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://press.benettongroup.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://thelede.blogs.nytimes.com/2010/09/16/doctored-photo-flatters-egyptian-president/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The dataset will be available in full two-megapixel resolution upon the acceptance of the paper. For reference, all images in lower resolution can be viewed at: http://www.ic.unicamp.br/ tjose/files/database-tifs-small-resolution.zip.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The parameter values were previously investigated by Riess and Angelopoulou<ref type="bibr" target="#b1">[2]</ref>,<ref type="bibr" target="#b28">[29]</ref>. In this paper, we rely on their findings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Original images were downloaded from Flickr (http://www.flickr.com) and doctored images were collected from different websites such as Worth 1000 (http://www.worth1000.com/), Benetton Group 2011 (http://press.benettongroup.com/), Planet Hiltron (http://www.facebook.com/pages/Planet-Hiltron/ 150175044998030), etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p><ref type="bibr" target="#b5">6</ref> https://www.mturk.com/mturk/welcome</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We gratefully thank the authors for the source code.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Christian Riess (S'10-A'12) received the Diploma degree in computer science in 2007 and the doctoral degree in 2013, both from the University of Erlangen-Nuremberg, Germany.</p><p>From 2007 to 2010, he was working on an industry project with Giesecke+Devrient on optical inspection. He is currently doing his postdoc at the Radiological Sciences Laboratory at Stanford University, Stanford, CA, USA. His research interests include all aspects of image processing, in particular with applications in image forensics, medical imaging, optical inspection, and computer vision.</p><p>Elli Angelopoulou (S'89-M'90) received the Ph.D. degree in computer science from the Johns Hopkins University in 1997.</p><p>She did her postdoc at the General Robotics, Automation, Sensing and Perception (GRASP) Laboratory at the University of Pennsylvania. She then became an assistant professor at Stevens Institute of Technology. She is currently an associate research professor at the University of Erlangen-Nuremberg. Her research focuses on multispectral imaging, skin reflectance, reflectance analysis in support of shape recovery, image forensics, image retrieval, and reflectance analysis in medical imaging (e.g., capsule endoscopy).</p><p>Dr. Angelopoulou has over 50 publications, multiple patents, and has received numerous grants, including an NSF CAREER award. She has served on the program committees of ICCV, CVPR, and ECCV and is an associate editor of Machine Vision and Applications (MVA) and the Journal of Intelligent Service Robotics (JISR).</p><p>Hélio Pedrini (S'99-M'00) received the Ph.D. degree in electrical and computer engineering from Rensselaer Polytechnic Institute, Troy, NY, USA. He received the M.Sc. degree in electrical engineering and the B.Sc. degree in computer science, both from the University of Campinas, Brazil.</p><p>He is currently a professor with the Institute of Computing at the University of Campinas, Brazil. His research interests include image processing, computer pattern recognition, computer graphics, and computational geometry. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision of the unseen: Current trends and challenges in digital image and video forensics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene illumination as an indicator of image manipulation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Hiding</title>
		<imprint>
			<biblScope unit="volume">6387</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image forensic analyses that elude the human visual system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Electron. Imaging (SPIE)</title>
		<meeting>Symp. Electron. Imaging (SPIE)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceiving illumination inconsistencies in scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1301" to="1314" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A 3-D lighting and shadow analysis of the JFK Zapruder film (Frame 317)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<idno>TR2010-677</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Dartmouth College</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries by detecting inconsistencies in lighting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Workshop on Multimedia and Security</title>
		<meeting>ACM Workshop on Multimedia and Security<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries in complex lighting environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="461" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries through specular highlights on the eye</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop on Inform. Hiding</title>
		<meeting>Int. Workshop on Inform. Hiding</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="311" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries from 3-D lighting environments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop on Inform. Forensics and Security (WIFS)</title>
		<meeting>IEEE Int. Workshop on Inform. Forensics and Security (WIFS)</meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D lighting-based image forgery detection using shape-from-shading</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cayre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Signal Processing Conf. (EUSIPCO)</title>
		<meeting>Eur. Signal essing Conf. (EUSIPCO)</meeting>
		<imprint>
			<date type="published" when="2012-08">Aug. 2012</date>
			<biblScope unit="page" from="1777" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exposing photo manipulation with inconsistent reflections</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Illuminant colour based image forensics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gholap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Bora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Region 10 Conf</title>
		<meeting>IEEE Region 10 Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image splicing detection using illuminant color inconsistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Inform. Networking and Security</title>
		<meeting>IEEE Int. Conf. Multimedia Inform. Networking and Security</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="600" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eye specular highlights telltales for digital forensics: A machine learning approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saboia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1937" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Physics-based illuminant color estimation as an image semantics clue</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing</title>
		<meeting>IEEE Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparison of computational color constancy algorithms-Part I: Methodology and Experiments With Synthesized Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cardei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="972" to="983" />
			<date type="published" when="2002-09">Sep. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comparison of computational color constancy algorithms -Part II: Experiments With Image Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2002-09">Sep. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational color constancy: Survey and experiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2475" to="2489" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Color constancy and non-uniform illumination: Can existing algorithms work?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bleier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beigpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eibenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Color and Photometry in Comput. Vision Workshop</title>
		<meeting>IEEE Color and Photometry in Comput. Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="774" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Color constancy using local color shifts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vision</title>
		<meeting>Eur. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="276" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color constancy for multiple light sources</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="697" to="707" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Consistent surface color for texturing large objects in outdoor scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1200" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Edge-based color constancy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2207" to="2214" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The appearance of human skin: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Comput. Graph. Vis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A spatial processor model for color perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Inst</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1980-07">Jul. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving color constancy by photometric edge weighting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="929" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color constancy through inverse-intensity chromaticity space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="321" to="334" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Illuminant color estimation for real-world mixed-illuminant scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eibenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Color and Photometry in Comput. Vision Workshop</title>
		<meeting>IEEE Color and Photometry in Comput. Vision Workshop<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sasi: A generic texture descriptor for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carkacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Yarman-Vural</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2615" to="2633" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparative study of global color and texture descriptors for web image retrieval</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Representat</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="380" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-06">Jun. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Comput. Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Statistical Learning in Comput. Vision</title>
		<meeting>Workshop on Statistical Learning in Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trainable classifier-fusion schemes: An application to pedestrian detection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Intell. Transportation Syst</title>
		<meeting>IEEE Int. Conf. Intell. Transportation Syst</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A large image database for color constancy research</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ciurea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SID Eleventh Color Imaging Conf.: Color Sci. and Eng. Syst., Technologies, Applicat. (CIC 2003)</title>
		<meeting>IS&amp;T/SID Eleventh Color Imaging Conf.: Color Sci. and Eng. Syst., Technologies, Applicat. (CIC 2003)<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="page" from="160" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Re-processed Version of the Gehler Color Constancy Dataset of 568 Images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Funt</surname></persName>
		</author>
		<ptr target="http://www.cs.sfu.ca/colour/data/shi_gehler/" />
		<imprint>
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical tools for digital forensics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inf. Hiding Conf</title>
		<meeting>Inf. Hiding Conf</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="395" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Linear row and column predictors for the analysis of resized images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMM Multimedia Security Workshop</title>
		<meeting>ACM SIGMM Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Digital camera identification from sensor pattern noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goljan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image manipulation detection with binary similarity measures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Avcibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Signal Processing Conf. (EUSIPCO)</title>
		<meeting>Eur. Signal essing Conf. (EUSIPCO)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="752" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detection of non-aligned double JPEG compression based on integer periodicity maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="842" to="848" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Points of interest and visual dictionaries for automatic retinal lesion detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Goldenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2244" to="2253" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tiago José de Carvalho (S&apos;12) received the B.Sc. degree (computer science) from Federal University of Juiz de Fora (UFJF), Brazil</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He received the M.Sc. degree (computer science) from University of Campinas (Unicamp), Brazil, in 2010. Currently, he is working toward the Ph.D. degree at the Institute of Computing, Unicamp, Brazil. His main interests include digital forensics</title>
		<meeting><address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">Jun. 2012. 2008</date>
		</imprint>
	</monogr>
	<note>Proc. IEEE Comput. Vision and Pattern Recognition. pattern analysis, data mining, machine learning, computer vision, and image processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
