<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepNet: Scaling Transformers to 1,000 Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>&lt;fuwei@microsoft.com&gt;.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Roberta Megatron-LM T5 Turing-NLG GPT-3 GShard MT-NLG XLM-R XGLM GLaM Gopher</orgName>
								<address>
									<postBox>DeepNet 0 200 400 600 800 1000</postBox>
									<postCode>2017 2018, 2019 2020, 2021 2022</postCode>
									<settlement>Depth</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepNet: Scaling Transformers to 1,000 Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.00555v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DEEPNORM) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DEEPNORM a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-big BERT-large</head><p>GPipe GPT-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(12M ) -1 4 (e.g., NMT, T5)</p><p>Figure <ref type="figure">2</ref>: (a) Pseudocode for DEEPNORM. We take Xavier initialization <ref type="bibr" target="#b16">(Glorot and Bengio, 2010)</ref> as an example, and it can be replaced with other standard initialization. Notice that ? is a constant. (b) Parameters of DEEPNORM for different architectures (N -layer encoder, M -layer decoder).</p><p>As shown in Figure <ref type="figure">2</ref>, it is simple to implement our method based on Transformers with Post-LN. Compared to Post-LN, DEEPNORM up-scales the residual connection before performing layer normalization. Besides, we down-scale the parameters during initialization. Notably, we only scale the weights of feed-forward networks, as well as the value projection and the output projection of attention layers. Moreover, the scales of residual connection and initialization are dependent on the architecture (Figure <ref type="figure">2</ref>). We provide more details in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instability of Deep Transformer</head><p>We study the causes of the instability for deep Transformers. Our analysis begins with the observation: better initialization methods stabilize the training of Transformer. This has also been verified by previous work <ref type="bibr">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b19">Huang et al., 2020;</ref><ref type="bibr" target="#b40">Xu et al., 2021)</ref>. Therefore, we study the training process of Post-LN with or without proper initialization. With better initialization, we downscale the weights of l-th layer by k l = N -l + 1, l ? [1, N ] after performing Xavier initialization. For example, the output projection W l o of FFN in l-th layer is initialized as:</p><formula xml:id="formula_0">W l o N 0, 1 k 2 l d</formula><p>, where d is an average of input and output dimensions. We name this model Post-LN-init. Notice that different from the prior work <ref type="bibr">(Zhang et al., 2019a)</ref>, we narrow the scale of lower layers instead of the higher layers. We believe that it helps to separate the effect of the gradient scale from the model update. Besides, Post-LN-init has the same architecture as Post-LN, which eliminates the impact from the architecture.</p><p>We train 18L-18L Post-LN and 18L-18L Post-LN-init on the IWSLT-14 De-En machine translation dataset. Figure <ref type="figure">3</ref> visualizes their gradients and validation loss curves. As shown in Figure <ref type="figure">3</ref>(c), Post-LN-init converged while Post-LN did not. Post-LN-init has an even larger gradient norm in the last several layers, although its weights have been scaled down. Furthermore, we visualize the gradient norm of the last decoder layer with varying model depth from 6L-6L to 24L-24L. Figure <ref type="figure">3</ref> shows that the gradient norm of Post-LN-init in the last layer is still much larger than that of Post-LN, regardless of model depth. It concludes that the exploding gradients in deep layers should not be the root cause of instability of Post-LN, while the scale of model update tends to account for it.</p><p>Then we demonstrate that the instability of Post-LN comes from a chain of several issues, including gradient vanishing as well as too large model updates. As shown in Figure <ref type="figure" target="#fig_1">4</ref>(a), we first visualize the norm of model update ||?F || at the early stage of training:</p><formula xml:id="formula_1">||?F || = ||F (x, ? i ) -F (x, ? 0 )||,</formula><p>where x and ? i denotes input, and model parameters after i-th updates. Post-LN has an exploding update at the very beginning of training, and then nearly no update shortly. It indicates that the model has been stuck in a spurious local optima. Both warm-up and better initialization help alleviate this issue, enabling the model to update smoothly. When the update explodes, the inputs to LN become large (see Figure <ref type="figure" target="#fig_1">4</ref>(b) and Figure <ref type="figure" target="#fig_1">4</ref>(c)). According to the theoretical analysis from <ref type="bibr" target="#b39">Xiong et al. (2020)</ref>, the magnitude of gradient through LN is inversely proportional to the magnitude of its input:</p><formula xml:id="formula_2">|| ?LN (x) ?x || = O( ? d ||x||</formula><p>). Above all, the instability starts from the large model update at the beginning of training. It renders the model trapped in a bad local optima, which in turn increases the magnitude of inputs to each LN. As training continues, the gradient through LN becomes increasingly small, thus resulting in severe gradient vanishing. The vanishing gradients make it difficult to escape from the local optima, and further destabilize the optimization. On the contrary, Post-LN-init has relatively small updates, and the inputs to LN are stable. This relieves suffering from gradient vanishing, making optimization more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEPNET: Extremely Deep Transformers</head><p>In this section, we introduce our extremely deep Transformers named DEEPNET. It can stabilize the optimization by mitigating the exploding model update problem. We first provide the estimation of the expected magnitude of DEEPNET's model update. Then we provide the theoretical analysis to show that its updates can be bounded by a constant with our proposed DEEPNORM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>DEEPNET is based on the Transformer architecture. Compared to the vanilla Transformer, it uses our new DEEPNORM, instead of Post-LN, for each sub-layer. The formulation of DEEPNORM can be written as:</p><formula xml:id="formula_3">x l+1 = LN (?x l + G l (x l , ? l )),</formula><p>where ? is a constant, and G l (x l , ? l ) is the function of the l-th Transformer sub-layer (i.e., attention or feed-forward network) with parameters ? l . Besides, DEEPNET scales the weights ? l inside residual branches by ?. Notably, both ? and ? are constants that only depend on the architecture, and we provide the derivation in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Expected Magnitude of Model Update</head><p>Attention is an important part of Transformer. Without loss of generality, we study the 1-head case. Let Q, K, V ? R n?d denote the query, key, value, respectively. W Q , W K , W V ? R d?d k are the input projection matrices, and W O ? R d k ?d is the output projection matrix. Then, the attention module can be formulated as:</p><formula xml:id="formula_4">Attn(Q, K, V ) = sof tmax( QW Q (KW K ) T ? d k )V W V W O</formula><p>We study the magnitude of the attention module. Lemma 4.1 proves that W Q and W K do not change the bound of attention output's magnitude. Lemma 4.1.</p><formula xml:id="formula_5">Given X = (x 1 , x 2 , ...x n ) T ? R n?d , where var(x i ) = 1, mean(x i ) = 0 and q i ? R for all i ? [1, n], it satisfies that sof tmax(q 1 , q 2 , ..., q n )X ? = x i ,</formula><p>where ? = stands for equal bound of magnitude.</p><p>In other words, the magnitude of attention output only depends on the value and output projection:</p><formula xml:id="formula_6">Attn(Q, K, V ) ? = V W V W O .</formula><p>In this work, we only consider the magnitude of model update, so it is sufficiently instructive to study the case where the hidden dimension equals to 1. For simplicity, we reduce the matrices W V , W O to the scalars v, w, which means Attn(Q, K, V ) ? = vwV . Similarly, we have F F N (X) ? = vwX, where v, w denotes the parameters of the feed-forward network.</p><p>We define the model update as</p><formula xml:id="formula_7">||?F || = ||F (x, ? * ) -F (x, ?)||.</formula><p>Based on the analysis above, we have the following theorem to characterize ||?F ||'s magnitude of an N -layer DEEPNET with N attentions and FFNs. Theorem 4.2. Given an N -layer DEEPNET F (x, ?) (? = {? 1 , ? 2 , ..., ? 2N }), where ? 2l-1 and ? 2l denote the parameters of self-attention and FFN in l-th layer, and each sub-layer is normalized with DEEPNORM:</p><formula xml:id="formula_8">x l+1 = LN (?x l + G l (x l , ? l )), ||?F || satisfies: ||?F || ? 2N i=1 v 2 i + w 2 i ? ||? * i -? i ||</formula><p>Vanilla Post-LN can be regarded as a special case of DEEPNET, where ? = 1 and v l = w l = 1 at Xavier initialization <ref type="bibr" target="#b16">(Glorot and Bengio, 2010)</ref>. Based on Theorem 4.2, we have</p><formula xml:id="formula_9">||?F || = O( 2N i=1 ||? * i -? i ||) for vanilla Post-LN.</formula><p>It shows that the model tends to accumulate the update of each sub-layer, which leads to exploding magnitude of model's update and destabilizes the optimization at the early stage. This explains our findings in Section 3.</p><p>Besides, Theorem 4.2 also explains why warm-ups and smaller initialization can stabilize the training of Post-LN. Warm-ups can reduce the magnitude of the model update by decreasing ||? * i -? i ||, while smaller initialization lowers v 2 i + w 2 i . Furthermore, we study the magnitude of DEEPNET with an N -layer encoder and an M -layer decoder. Let F ed (x, y, ? e , ? d ) denotes the model, where x, y is the input of encoder and decoder. ? e follows the same definition as ? in Theorem 4.2. ? d = {? d1 , ? d2 , ..., ? d,3M } stands for the parameters of selfattentions, cross-attentions, and FFNs. We use {? e , G el } and {? d , G dl } to distinguish the notations between the encoder and the decoder. The following theorem shows the expected magnitude of the encoder-decoder's model update ||?F ed || = ||F ed (x, y, ? * e , ? * d ) -F ed (x, y, ? e , ? d )||. Theorem 4.3. Given an encoder-decoder DEEPNET F ed (x, y, ? e , ? d ) with N encoder layers and M decoder layers, where each encoder sub-layer is normalized as x l+1 = LN (? e x l + G el (x l , ? el )), and the decoder sub-layer is normalized as</p><formula xml:id="formula_10">x l+1 = LN (? d x l + G dl (x l , ? dl )), ||?F ed || satisfies: ||?F ed || ? M j=1 v d,3j-1 w d,3j-1 ? d 2N i=1 v 2 ei + w 2 ei ? e ||? * ei -? ei || + 3M j=1 v 2 dj + w 2 dj ? d ||? * dj -? dj ||<label>(1)</label></formula><p>The vanilla encoder-decoder model satisfies that all of {? e , ? d , v ei , w ei , v di , w di } equal to 1, so we have</p><formula xml:id="formula_11">||?F ed || = O(M 2N i=1 ||? * ei -? ei || + 3M j=1 ||? * dj -? dj ||).</formula><p>It indicates the similar accumulative effect which leads to fast growth of the magnitude regarding the model depth (see Figure <ref type="figure" target="#fig_2">5</ref>). Furthermore, the cross-attention propagates the magnitude from the encoder to the decoder, which explains why the decoder is more unstable than the encoder <ref type="bibr" target="#b25">(Liu et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Derivation for DEEPNORM and the Initialization</head><p>We show that the expected model updates for DEEPNET can be bounded by a constant with proper parameters ? and ?. Our analysis is based on SGD update, and we empirically verify it works well for Adam optimizer <ref type="bibr" target="#b22">(Kingma and Ba, 2015)</ref>. We provide the analysis on the encoder-decoder architecture, which can be naturally extended to encoder-only and decoder-only models in the same way. Analogous to <ref type="bibr">Zhang et al. (2019b)</ref>, we set our goal for the model update as follows:</p><p>GOAL: F ed (x, y, ? e , ? d ) is updated by ?(?) per SGD step after initialization as ? ? 0. That is</p><formula xml:id="formula_12">||?F ed || = ?(?) where ?F ed ? = F ed (x, y, ? e -? ?L ??e , ? d -? ?L ?? d ) -F ed (x, y, ? e , ? d ).</formula><p>For SGD optimizer, the update of each decoder layer ||?  <ref type="formula" target="#formula_10">1</ref>) can be bounded as:</p><formula xml:id="formula_13">3M j=1 v 2 dj + w 2 dj ? d ||? * dj -? dj || ? ?|| ?L ?F || ? || ?F ?? d,3M || 3M j=1 v 2 dj + w 2 dj ? d ? = 3?M v 2 d + w 2 d ? 2 d (2)</formula><p>There are multiple schemes to bound Equation (2) by ?(?). In order to balance the effect of residual connections and the initialization, we set In comparison with Post-LN, we visualize the model updates for DEEPNET on IWSLT-14 De-En translation dataset at the early training stage. Figure <ref type="figure" target="#fig_2">5</ref> shows that the model update of DEEPNET is nearly constant, while the model update of Post-LN is exploding.</p><formula xml:id="formula_14">? 2 d = (3M ) 1 2 , v 2 d + w d 2 = (3M ) 1 2 and v d = w d = ? d due to symmetry, that is ? d = (3M ) 1 4 , ? d = (12M ) -</formula><p>In summary, we apply our approach as follows:</p><formula xml:id="formula_15">Models LN 6L-6L 18L-18L 50L-50L 100L-100L</formula><p>Vanilla Post-LN <ref type="bibr" target="#b36">(Vaswani et al., 2017)</ref> Post 28.1 diverged DS-Init <ref type="bibr">(Zhang et al., 2019a)</ref> Post 27.9 diverged Admin <ref type="bibr" target="#b25">(Liu et al., 2020)</ref> Post 27.9 28.8 diverged ReZero <ref type="bibr" target="#b0">(Bachlechner et al., 2020)</ref> No 26.9 diverged R-Fixup <ref type="bibr">(Zhang et al., 2019b)</ref> No 27.5 28.4 27.7 diverged T-Fixup <ref type="bibr" target="#b19">(Huang et al., 2020)</ref> No 27.5 28.4 27.9 diverged Vanilla Pre-LN <ref type="bibr">(Vaswani et al.,</ref>  Encoder-decoder architecture 1. Apply standard initialization (e.g., Xavier initialization) for each encoder and decoder layer. 2. For encoder layers, scale the weights of feed-forward networks as well as the value projection and the output projection of attention layers by 0.87(N 4 M ) -1 16 , and set the weight of residual connections as 0.81(N 4 M ) 1 16 . 3. For decoder layers, scale the weights of feed-forward networks as well as the value projection and the output projection of attention layers by (12M ) -1 4 , and set the weight of residual connections as (3M )</p><formula xml:id="formula_16">1 4 .</formula><p>The derivation of encoder-only (such as BERT) and decoder-only (such as GPT) architectures can be conducted in the same way (see Appendix C). We summarize the steps as follows:</p><p>Encoder-only (or decoder-only) architecture 1. Apply standard initialization (e.g., Xavier initialization) for each layer. 2. For each layer, scale the weights of feed-forward networks as well as the value projection and the output projection of attention layers by (8N ) -1 4 (or (8M ) -1 4 ), and set the weight of residual connections as (2N ) 1 4 (or (2M ) 1 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Machine Translation</head><p>We verify the effectiveness of DEEPNET on the popular machine translation benchmarks, including IWSLT-14 German-English (De-En) dataset and WMT-17 English-German (En-De) dataset. We compare our method with multiple state-of-the-art deep Transformer models, including DLCL <ref type="bibr" target="#b37">(Wang et al., 2019)</ref>, NormFormer <ref type="bibr" target="#b34">(Shleifer et al., 2021)</ref>, ReZero <ref type="bibr" target="#b0">(Bachlechner et al., 2020)</ref>, R-Fixup <ref type="bibr">(Zhang et al., 2019b)</ref>, T-Fixup <ref type="bibr" target="#b19">(Huang et al., 2020)</ref>, DS-init <ref type="bibr">(Zhang et al., 2019a)</ref>, and Admin <ref type="bibr" target="#b25">(Liu et al., 2020)</ref>. We reproduce the baselines with their open-source code, and set the hyper-parameters the same for a fair comparison.</p><p>We use BLEU as the evaluation metric for all experiments. Table <ref type="table" target="#tab_2">1</ref> reports the results of the baselines and DEEPNET on WMT-17 En-De translation dataset. According to their LNs, the baselines are grouped into three categories: Pre-LN, Post-LN, and No-LN. All the compared models are base-size with different depths.</p><p>Compared with the models with Post-LN, DEEPNET is more stable, and can successfully scale to 100L-100L, reaching the 28.9 BLEU on the test set. In contrast, the baselines with Post-LN lead to   unstable optimization when the depth goes to 50L-50L. Besides, DEEPNET achieves comparable performance with these baselines when the models are shallow.</p><p>In addition, we compare DEEPNET with the methods without LN. Both R-Fixup and T-Fixup introduce better initialization methods, which stabilize the training of No-LN Transformer with up to 50-50 layers. Yet, their performance is not as good as those with Post-LN. Besides, half-precision could destabilize the training of ReZero, leading to its divergence with 18-18 layers. This observation is also reported by <ref type="bibr" target="#b25">Liu et al. (2020)</ref>. Moreover, deeper models (50L-50L) do not outperform the shallow models (18L-18L). In comparison, DEEPNET achieves better translation accuracy than these methods, and scaling to deeper models brings no harm to the performance.</p><p>Compared with the Post-LN baselines, the models with Pre-LN are more stable. Both vanilla Pre-LN and DLCL can be scaled to 100L-100L, and 50L-50L NormFormer is also trained successfully. Nevertheless, Pre-LN leads to a 0.5-1.0 BLEU drop compared with the converged Post-LN models. We presume this should be caused by the problem that gradients of Pre-LN at earlier layers tend to be larger than gradients at later layers <ref type="bibr" target="#b34">(Shleifer et al., 2021)</ref>. We leave it as the future work. In contrast, DEEPNET alleviates the problem by using Post-LN, and outperforms all the Pre-LN baselines.</p><p>Convergence with varying depth. We vary the depths of the models from 10L-10L to 100L-100L with an interval of 10 layers.All experiments are conducted with mixed precision training, except ReZero<ref type="foot" target="#foot_0">3</ref> . Figure <ref type="figure" target="#fig_5">6</ref> shows the results on the IWSLT-14 dataset. We train the models for 8,000 steps because we find most divergence occurs at the beginning of optimization. Overall, DEEPNET is stable from shallow to deep. It converges fast, achieving over 30 BLEU in only 8,000 steps while most of the baselines do not. Moreover, the performance keeps improving as the model goes deeper.</p><p>Large learning rate, batch size, and hidden dimension. We further scale DEEPNET to larger learning rate, batch size, and hidden dimension, respectively. For each experiment, we only change one hyperparameter with the others fixed. Figure <ref type="figure" target="#fig_6">7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Massively Multilingual Neural Machine Translation</head><p>We conduct experiments on the large-scale multilingual machine translation, which is a good testbed for large models. We first use OPUS-100 corpus <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref> to evaluate our model. OPUS-100 is an English-centric multilingual corpus covering 100 languages, which is randomly sampled from the OPUS collection. We scale DEEPNET up to 1,000 layers. The model has a 500-layer encoder, a 500-layer decoder, 512 hidden size, 8 attention head, and 2,048 dimensions of feed-forward layers. More details can be found in the Appendix.</p><p>Table <ref type="table" target="#tab_3">2</ref> summarizes the results of DEEPNET and the baselines. It shows that increasing the depth can significantly improve the translation quality of NMT: the baseline of 48 layers achieves a gain of 3.2 points on average over the 12-layer model. DEEPNET can successfully scale up the depth to 1,000 layers, outperforming the baseline by an improvement of 4.4 BLEU. It is noted that DEEPNET is only trained for 4 epochs, and the performance can be further improved given more computation budgets.</p><p>Scaling law in terms of depth We train DEEPNET of {12, 20, 100, 200, 1000} layers on the OPUS-100 dataset. Figure <ref type="figure">8</ref> illustrates the scaling curve. Compared with bilingual NMT, multilingual NMT benefits more from scaling the depth of the model because of its hunger in model capacity. We observe logarithmic growth of the BLEU score for multilingual NMT, and the scaling law can be written as:</p><formula xml:id="formula_17">L(d) = A log(d) + B</formula><p>where d is the depth, and A, B are the constants regarding the other hyper-parameters.</p><p>More data and language directions. To explore the limits of DEEPNET on multilingual NMT, we then scale up the training data by using CCMatrix <ref type="bibr" target="#b33">(Schwenk et al., 2021)</ref>. We also expand the data from CCAligned (El-Kishky et al., 2020), OPUS <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref> We compare DEEPNET with the state-of-the-art multilingual NMT model M2M-100 <ref type="bibr" target="#b15">(Fan et al., 2021)</ref>. M2M-100 has a 24-layer encoder, a 24-layer decoder, and 4,096 hidden size, resulting in up to 12B parameters. Compared with M2M-100, DEEPNET is deep and narrow with only 3.2B parameters. For a fair comparison, we generate the model with beam size 5 and length penalty 1.</p><p>Following M2M-100 <ref type="bibr" target="#b15">(Fan et al., 2021)</ref>, we evaluate the models on several multilingual translation evaluation datasets, including WMT <ref type="bibr" target="#b4">(Bojar et al., 2014;</ref><ref type="bibr">2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b3">Barrault et al., 2019)</ref>, OPUS <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref>, TED <ref type="bibr" target="#b29">(Qi et al., 2018)</ref>, and Flores <ref type="bibr" target="#b17">(Goyal et al., 2021)</ref>. The language pairs from the WMT dataset are English-centric. There are 10 languages including English, and most of them are high-resource. For the OPUS dataset, we select the non-English directions from the test set, which has 30 evaluation pairs. The TED evaluation set has 28 languages and 756 directions, and the data is from the spoken language domain. The Flores dataset has all translation pairs between 102 languages. We use a subset covering the languages supported by both M2M-100 and DEEPNET, resulting in 87 languages and 7,482 translation directions.</p><p>We report the results in Table <ref type="table" target="#tab_4">3</ref>. For a fair comparison, we use the same evaluation methods as the baseline. The details can be found in the Appendix. It shows that DEEPNET has significantly better performance than M2M-100 on all evaluation datasets, indicating that deepening the model is a very promising direction to improve the quality of NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We improve the stability of Transformer and successfully scale it to 1,000 layers. This is achieved by our DEEPNET with a novel normalization function called DEEPNORM. It has theoretical justification to stabilize the optimization with a constant upper bound for model updates. Experimental results verify the effectiveness of our methods across various benchmarks. We focus on machine translation as a test bed in the current experiments. In the future, we will extend DEEPNET to support more diverse tasks, e.g., language model pre-training <ref type="bibr" target="#b12">(Dong et al., 2019;</ref><ref type="bibr" target="#b1">Bao et al., 2020;</ref><ref type="bibr">Chi et al., 2021a;</ref><ref type="bibr" target="#b26">Ma et al., 2021;</ref><ref type="bibr">Chi et al., 2021b)</ref>, protein structure prediction <ref type="bibr" target="#b21">(Jumper et al., 2021)</ref>, and BEiT vision pre-training <ref type="bibr" target="#b2">(Bao et al., 2022;</ref><ref type="bibr" target="#b38">Wang et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Main Theorem Proof</head><formula xml:id="formula_18">A.1 Proof of Theorem 4.1 Lemma A.1. Given X = (x 1 , x 2 , ...x n ) T ? R n?d , where var(x i ) = 1, mean(x i ) = 0 and q i ? R for all i ? [1, n], it satisfies that sof tmax(q 1 , q 2 , ..., q n )X ? = x i ,</formula><p>where ? = stands for equal bound of magnitude.</p><p>Proof. The weight</p><formula xml:id="formula_19">s i of x i to output is s i = e qi e n j=1 qj , n i=1 s i = 1. ||softmax(q 1 , q 2 , ..., q n )X|| = || n i=1 s i x i || ? n i=1 s i ||x i || (3) With var(x i ) = 1, mean(x i ) = 0, for all i ? [1, n], we have ||x i || = d. Therefore, ||softmax(q 1 , q 2 , ..., q n )X|| ? ||x i || = d, which is equivalent to softmax(q 1 , q 2 , ..., q n )X ? = x i . A.2 Proof of Theorem 4.2 Theorem A.2. Given an N -layer DEEPNET F (x, ?) (? = {? 1 , ? 2 , ..., ? 2N })</formula><p>, where ? 2l-1 and ? 2l denote the parameters of self-attention and FFN in l-th layer, and each sub-layer is normalized with DEEPNORM:</p><formula xml:id="formula_20">x l+1 = LN (?x l + G l (x l , ? l )), ||?F || satisfies: ||?F || ? 2N i=1 v 2 i + w 2 i ? ||? * i -? i || Proof.</formula><p>Our aim is to study the magnitude of model updates. Following <ref type="bibr">Zhang et al. (2019b)</ref>, we make the following assumptions to simplify the derivations:</p><p>1. Hidden dimension d equals to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">var(x</head><formula xml:id="formula_21">+ G l (x)) ? = var(x) + var(G l (x))</formula><p>3. All relevant weights v, w are positive with magnitude less than 1 and ?, ? for DEEPNORM are positive with magnitude greater than 1.</p><formula xml:id="formula_22">Given Assumption 1, if G(x) is feed-forward network with ? = {v, w}, then G(x) ? = vwx.</formula><p>According to Theorem 4.1, the query and key projections do not change the bound of the attention output's magnitude. Therefore, if G(x) is self-attention with ? = {q, k, v, w}, then G(x) ? = vwx. Especially, if Xavier initialization is used for the projection, then the output can preserve the input variance, which is equivalent to v = w = 1. With Assumption 2, we have:</p><formula xml:id="formula_23">x l+1 = f l (x l , ? l ) = ?x + G l (x) V ar(?x + G l (x)) ? = ? + v l w l ? 2 + v 2 l w 2 l x<label>(4)</label></formula><p>With Equation ( <ref type="formula" target="#formula_23">4</ref>), the magnitude of ?f l ?x and ?f l ?? l is bounded by:</p><formula xml:id="formula_24">?f l ?x ? = ? + v l w l ? 2 + v 2 l w 2 l ?f l ?? l ? = ( ?f ?v l , ?f ?w l ) ? = ?x l (? -v l w l ) (? 2 + v 2 l w 2 l ) 3 2 (w l , v l )<label>(5)</label></formula><p>Besides, the model update ||?F || satisfies:</p><formula xml:id="formula_25">||?F || = ||F (x, ? * ) -F (x, ?)|| = ||x * 2N +1 -x 2N +1 || = ||f (x * 2N , ? * 2N ) -f (x 2N , ? 2N )|| (6)</formula><p>Using Taylor expansion for Equation ( <ref type="formula">6</ref>), we get:</p><formula xml:id="formula_26">||?F || = ||x * 2N +1 -x 2N +1 || (7) ? || ?f ?x (x 2N , ? 2N )(x * 2N -x 2N ) + ?f ?? (x 2N , ? 2N )(? * 2N -? 2N ) T || ? || ?f ?x (x 2N , ? 2N )|| ? ||x * 2N -x 2N || + || ?f ?? (x 2N , ? 2N )|| ? ||? * 2N -? 2N || = ? + v 2N w 2N ? 2 + v 2 2N w 2 2N ||x * 2N -x 2N || + ?(? -v 2N w 2N ) (? 2 + v 2 2N w 2 2N ) 3 2 v 2 2N + w 2 2N ||? * 2N -? 2N || ? ||x * 2N -x 2N || + v 2 2N + w 2 2N ? ||? * 2N -? 2N ||<label>(8)</label></formula><p>Then, we have:</p><formula xml:id="formula_27">||x * 2N +1 -x 2N +1 || ? 2N i=1 v 2 i + w 2 i ? ||? * i -? i || (9) For vanilla Post-LN with standard initialization, ? = v i = w i = 1, so ||?F || = O( 2N i=1 ||? * i -? i ||). Proof of Theorem 4.3</formula><p>Theorem A.3. Given an encoder-decoder DEEPNET F ed (x, y, ? e , ? d ) with N encoder layers and M decoder layers, where each encoder sub-layer is normalized as x l+1 = LN (? e x l + G el (x l , ? el )), and the decoder sub-layer is normalized as</p><formula xml:id="formula_28">x l+1 = LN (? d x l + G dl (x l , ? dl )), ||?F ed || satisfies: ||?F ed || ? M j=1 v d,3j-1 w d,3j-1 ? d 2N i=1 v 2 ei + w 2 ei ? e ||? * ei -? ei || + 3M j=1 v 2 dj + w 2 dj ? d ||? * dj -? dj ||<label>(10)</label></formula><p>Proof. The derivation of self-attention and FFN layers is given in Appendix A.2. For the crossattention layers, we have:</p><formula xml:id="formula_29">y l+1 = f dl (y l , x e , ? dl ) = ? d y l + G l (x e , y l ) V ar(? d y l + G dl (x e , y l )) ? = ? d y l + v l w l x e ? 2 d + v 2 l w 2 l (<label>11</label></formula><formula xml:id="formula_30">)</formula><p>With Equation ( <ref type="formula" target="#formula_29">11</ref>), we have the bound of the derivative of f dl :</p><formula xml:id="formula_31">?f dl ?y ? = ? d ? 2 d + v 2 l w 2 l , ?f dl ?x e ? = v l w l ? 2 d + v 2 l w 2 l ?f dl ?? dl ? = ( ?f dl ?v dl , ?f dl ?w dl ) ? = ? d x e (? d -v dl w dl ) (? 2 d + v 2 dl w 2 dl ) ||y * l+1 -y l+1 || = ||f * dl (y * l , x * 2N +1 , ? * dl ) -f dl (y l , x 2N +1 , ? dl )|| ? ? d ? 2 d + v 2 dl w 2 dl ||y * l -y l || + v dl w dl ? 2 d + v 2 dl w 2 dl ||x * 2N +1 -x 2N +1 || + ? d (? d -v dl w dl ) (? 2 d + v 2 dl w 2 dl ) 3 2 v 2 dl + w 2 dl ||? * dl -? dl || ? ||y * l -y l || + v dl w dl ? d ||x * 2N +1 -x 2N +1 || + v 2 dl + w 2 dl ? d ||? * dl -? dl ||<label>(12)</label></formula><p>According to Theorem 4.2, we have</p><formula xml:id="formula_32">||x * 2N +1 -x 2N +1 || = O( 2N i=1 ? v 2 ei +w 2 ei ?e ||? * ei -? ei ||).</formula><p>Therefore, the magnitude of ||?F ed || satisfies:</p><formula xml:id="formula_33">||?F ed || ? M j=1 v d,3j-1 w d,3j-1 ? d 2N i=1 v 2 ei + w 2 ei ? e ||? * ei -? ei || + 3M j=1 v 2 dj + w 2 dj ? d ||? * dj -? dj || (13)</formula><p>As a special case, the corresponding parameters in Equation ( <ref type="formula">13</ref>) for vanilla Post-LN with standard initialization are 1, so its model update</p><formula xml:id="formula_34">||?F ed || = O(M 2N i=1 ||? * ei -? ei || + 3M j=1 ||? * dj -? dj ||).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Derivation for Encoder-Decoder Architecture</head><p>Here, we give the derivation of DEEPNET for the encoder-decoder architecture with an N -layer encoder and an M -layer decoder. As in Section 4.3, we have</p><formula xml:id="formula_35">v d = w d = (12M ) -1 4 , ? d = (3M ) 1 4</formula><p>to bound the second term of Equation ( <ref type="formula">13</ref>) to ?(?). For the first term, we set v ei = v e , w ei = w e , so that it goes to:</p><formula xml:id="formula_36">M j=1 v d,3j-1 w d,3j-1 ? d 2N i=1 v 2 ei + w 2 ei ? e ||? * ei -? ei || = M (12M ) -1 2 (3M ) 1 4 2N i=1 v 2 ei + w 2 ei ? e ||? * ei -? ei || (14) ? = ? N 4 M 27 1 4 v 2 e + w 2 e ? 2 e<label>(15)</label></formula><p>In this work, we use ? 2 e = (N 4 M/27)</p><formula xml:id="formula_37">1 8 , v 2 e + w 2 e = (N 4 M/27) -1 8 and v e = w e = ? e that is ? e = 0.81(N 4 M ) 1 16 , ? e = 0.87(N 4 M ) -1</formula><p>16 to satisfy the condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Derivation for Encoder-only (Decoder-only) Architecture</head><p>For an N -layer DEEPNET, starting from Theorem 4.2 we have,</p><formula xml:id="formula_38">||x * 2N +1 -x 2N +1 || ? 2N i=1 v 2 i + w 2 i ? ||? * i -? i || ? ? 2N i=1 v 2 i + w 2 i ? || ?L ?F || ? || ?F ?? i ||<label>(16)</label></formula><p>By assumption || ?L ?F || = O(1), and</p><formula xml:id="formula_39">|| ?F ??i || ? || ?F ?? 2N || ? = ||? 2N || ? , we achieve: 2N i=1 v 2 i + w 2 i ? || ?L ?F || ? || ?F ?? i || ? O( v 2 2N + w 2 2N ? 2N i=1 v 2 i + w 2 i ? ) = O(1)<label>(17)</label></formula><p>Due to symmetry, we set v i = v, w j = w, so it goes to 2N v 2 +w 2 ? 2 = 1. In this work, we use v = w = (8N ) -1 4 and ? = (2N ) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Evaluation Details</head><p>For IWSLT-14 and WMT-17, we use the in-built BLEU scripts of Fairseq to report the scores. Besides, we report the case-sensitive detokenized BLEU using sacreBLEU <ref type="bibr">(Post, 2018)</ref> for the results of OPUS-100.<ref type="foot" target="#foot_3">5</ref> </p><p>For WMT, OPUS, and TED, we use the same test sets and evaluation scripts as in M2M <ref type="bibr" target="#b15">(Fan et al., 2021)</ref>, and the results of M2M are directly from the paper <ref type="bibr" target="#b15">(Fan et al., 2021)</ref>. For the Flores-101 evaluation set, we report the spBLEU<ref type="foot" target="#foot_4">6</ref> of M2M-12B with the public checkpoint and script.  The i-th row is the source language, while j-th column is the target language. There are 87 languages and 7,482 directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: (a) Gradient norm in the top layers of 18L-18L models. (b) Gradient norm in the last layer of the models with depths varying from 6L-6L to 24L-24L. (c) Validation loss curves of 18L-18L models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4(b) and Figure 4(c) show that ||x|| is significantly larger than ? d (d = 512) without warm-up or proper initialization. This explains the gradient vanishing problem occurred in the training of Post-LN (see Figure 4(d)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model updates of vanilla Post-LN and DEEPNET at the early stage of training. The visualization is conducted on 64-128-2 tiny Transformers with depth varying from 6L-6L to 100L-100L. It shows that DEEPNET has much smaller and more stable updates than Post-LN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 4 . Similarly, we use v e = w e = ? e = 0.87(N 4 M ) -1 16 , ? e = 0.81(N 4 M ) 1 16 to bound the first term in Equation (1). Detailed derivation is shown in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: BLEU scores on the IWSLT-14 De-En test set for different deep models with varing depth from 10L-10L to 100L-100L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: WMT-17 En-De validation loss curves for 18L-18L DEEPNET with varing learning rate, batch size and hidden dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Evaluation results of 3.2B DEEPNET on a subset of FLORES-101 devtest set. The i-th row is the source language, while j-th column is the target language. There are 87 languages and 7,482 directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>BLEU scores on the WMT-17 En-De test set for different models with varying depth. AL-BL refers to A-layer encoder and B-layer decoder.</figDesc><table><row><cell>2017)</cell><cell>Pre</cell><cell>27.0</cell><cell>28.1</cell><cell>28.0</cell><cell>27.4</cell></row><row><cell>DLCL (Wang et al., 2019)</cell><cell>Pre</cell><cell>27.4</cell><cell>28.2</cell><cell>diverged</cell><cell>27.5</cell></row><row><cell>NormFormer (Shleifer et al., 2021)</cell><cell>Pre</cell><cell>27.0</cell><cell>28.3</cell><cell>27.8</cell><cell>diverged</cell></row><row><cell>DEEPNET (ours)</cell><cell>Deep</cell><cell>27.8</cell><cell>28.8</cell><cell>29.0</cell><cell>28.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>reports the loss curves on the WMT-17 validation set. It shows that DEEPNET can be trained without difficulty in all the largest settings. The loss of DEEPNET with 1024 hidden size increases after 10K steps because of overfitting. Besides, it indicates that DEEPNET can benefit from the larger settings, resulting in faster convergence and lower validation loss. Average BLEU for DEEPNET and the baseline on the OPUS-100 test sets.</figDesc><table><row><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>31</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEU</cell><cell>29</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>27</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 25</cell><cell></cell><cell>100 Depth</cell><cell>1000</cell><cell></cell></row><row><cell cols="7">Figure 8: Average BLEU scores for DEEPNET with varying depth on the OPUS-100 En-X and X-En</cell></row><row><cell>test sets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell cols="5"># Layers # Params X?En En?X Avg</cell></row><row><cell></cell><cell></cell><cell>12</cell><cell>133M</cell><cell>27.5</cell><cell>21.4</cell><cell>24.5</cell></row><row><cell cols="2">Baseline (Zhang et al., 2020)</cell><cell>24</cell><cell>173M</cell><cell>29.5</cell><cell>22.9</cell><cell>26.2</cell></row><row><cell></cell><cell></cell><cell>48</cell><cell>254M</cell><cell>31.4</cell><cell>24.0</cell><cell>27.7</cell></row><row><cell>DEEPNET (ours)</cell><cell></cell><cell>200 1000</cell><cell>863M 3.8B</cell><cell>33.2 33.9</cell><cell>29.0 30.2</cell><cell>31.1 32.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores for DEEPNET and M2M-100 on various evaluation sets.</figDesc><table><row><cell>Models</cell><cell cols="6"># Layers # Params WMT OPUS TED Flores</cell></row><row><cell>M2M-100 (Fan et al., 2021)</cell><cell>48</cell><cell>12B</cell><cell>31.9</cell><cell>18.4</cell><cell>18.7</cell><cell>13.6</cell></row><row><cell>DEEPNET (ours)</cell><cell>200</cell><cell>3.2B</cell><cell>33.9</cell><cell>23.0</cell><cell>20.1</cell><cell>18.6</cell></row></table><note><p>, and Tatoeba 4 to cover all languages of Flores101 evaluation sets. The final data consists of 102 languages, 1932 directions, and 12B sentence pairs. With the data, we train DEEPNET with a 100-layer encoder, 100-layer decoder, 1,024 hidden dimension, 16 heads, and 4,096 intermediate dimension of feed-forward layers. More details can be found in the Appendix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for the machine translation experiments on the IWSLT-14 De-En dataset.</figDesc><table><row><cell cols="4">1 4 to satisfy the condition.</cell><cell></cell></row><row><cell>D Experimental Details</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">D.1 Hyperparameters for IWSLT-14 De-En</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hyperparameters</cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell cols="2">Learning rate</cell><cell></cell><cell>5e-4</cell><cell></cell></row><row><cell cols="3">Learning rate scheduler</cell><cell>inverse sqrt</cell><cell></cell></row><row><cell cols="2">Warm-up updates</cell><cell></cell><cell>4000</cell><cell></cell></row><row><cell cols="3">Warm-up init learning rate</cell><cell>1e-7</cell><cell></cell></row><row><cell>Max tokens</cell><cell></cell><cell></cell><cell>4000</cell><cell></cell></row><row><cell>Adam</cell><cell></cell><cell></cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell></cell><cell></cell><cell>(0.9, 0.98)</cell><cell></cell></row><row><cell cols="2">Label smoothing</cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell cols="2">Training updates</cell><cell></cell><cell>8K</cell><cell></cell></row><row><cell cols="2">Gradient clipping</cell><cell></cell><cell>0.0</cell><cell></cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell cols="2">Weight decay</cell><cell></cell><cell>0.0001</cell><cell></cell></row><row><cell>Hidden size</cell><cell></cell><cell></cell><cell>512</cell><cell></cell></row><row><cell cols="2">FFN inner hidden size</cell><cell></cell><cell>2048</cell><cell></cell></row><row><cell cols="2">Attention heads</cell><cell></cell><cell>8</cell><cell></cell></row><row><cell cols="2">D.2 Hyperparameters for WMT-17 En-De</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hyperparameters</cell><cell cols="4">No-LN Pre-LN Post-LN DEEPNORM</cell></row><row><cell>Learning rate</cell><cell>5e-4</cell><cell>1.5e-3</cell><cell>1.5e-3</cell><cell>1.5e-3</cell></row><row><cell>Learning rate scheduler</cell><cell></cell><cell></cell><cell>inverse sqrt</cell><cell></cell></row><row><cell>Warm-up updates</cell><cell></cell><cell></cell><cell>4000</cell><cell></cell></row><row><cell>Warm-up init learning rate</cell><cell></cell><cell></cell><cell>1e-7</cell><cell></cell></row><row><cell>Max tokens</cell><cell></cell><cell cols="2">128 ? 4096</cell><cell></cell></row><row><cell>Adam</cell><cell></cell><cell></cell><cell>1e-8</cell><cell></cell></row><row><cell>Adam ?</cell><cell></cell><cell></cell><cell>(0.9, 0.98)</cell><cell></cell></row><row><cell>Label smoothing</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell>Training updates</cell><cell></cell><cell></cell><cell>100K</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell>Weight decay</cell><cell></cell><cell></cell><cell>0.0001</cell><cell></cell></row><row><cell>Hidden size</cell><cell></cell><cell></cell><cell>512</cell><cell></cell></row><row><cell>FFN inner hidden size</cell><cell></cell><cell></cell><cell>2048</cell><cell></cell></row><row><cell>Attention heads</cell><cell></cell><cell></cell><cell>8</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters for the base-setting experiments on the WMT-17 En-De dataset.</figDesc><table><row><cell>Hyperparameters</cell><cell cols="3">Base size Medium size Large size</cell></row><row><cell>Hidden size</cell><cell>512</cell><cell>768</cell><cell>1,024</cell></row><row><cell>FFN inner hidden size</cell><cell>2048</cell><cell>3072</cell><cell>4096</cell></row><row><cell>Attention heads</cell><cell>8</cell><cell>12</cell><cell>16</cell></row><row><cell>Layers</cell><cell></cell><cell>18-18</cell><cell></cell></row><row><cell>Learning rate</cell><cell></cell><cell>5e-4</cell><cell></cell></row><row><cell>Learning rate scheduler</cell><cell></cell><cell>inverse sqrt</cell><cell></cell></row><row><cell>Warm-up updates</cell><cell></cell><cell>4000</cell><cell></cell></row><row><cell>Warm-up init learning rate</cell><cell></cell><cell>1e-7</cell><cell></cell></row><row><cell>Max tokens</cell><cell></cell><cell>128 ? 4096</cell><cell></cell></row><row><cell>Adam</cell><cell></cell><cell>1e-6</cell><cell></cell></row><row><cell>Adam ?</cell><cell></cell><cell>(0.9, 0.98)</cell><cell></cell></row><row><cell>Label smoothing</cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell>Training updates</cell><cell></cell><cell>30K</cell><cell></cell></row><row><cell>Gradient clipping</cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell>Dropout</cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell>Weight decay</cell><cell></cell><cell>0.0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for the large-setting experiments on the WMT-17 En-De dataset.</figDesc><table><row><cell>D.3 Hyperparameters for OPUS-100</cell><cell></cell></row><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>Learning rate</cell><cell>5e-4</cell></row><row><cell>Learning rate scheduler</cell><cell>inverse sqrt</cell></row><row><cell>Warm-up updates</cell><cell>4000</cell></row><row><cell>Warm-up init learning rate</cell><cell>1e-7</cell></row><row><cell>Max tokens</cell><cell>128 ? 4096</cell></row><row><cell>Adam</cell><cell>1e-8</cell></row><row><cell>Adam ?</cell><cell>(0.9, 0.98)</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell></row><row><cell>Training epochs</cell><cell>4</cell></row><row><cell>Gradient clipping</cell><cell>0.0</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.0</cell></row><row><cell>Hidden size</cell><cell>512</cell></row><row><cell>FFN inner hidden size</cell><cell>2048</cell></row><row><cell>Attention heads</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for the machine translation experiments on the OPUS-100 dataset.</figDesc><table><row><cell cols="2">D.4 Hyperparameters for 102-Language Machine Translation</cell></row><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>Learning rate</cell><cell>5e-4</cell></row><row><cell>Learning rate scheduler</cell><cell>inverse sqrt</cell></row><row><cell>Warm-up updates</cell><cell>6000</cell></row><row><cell>Warm-up init learning rate</cell><cell>1e-7</cell></row><row><cell>Max tokens</cell><cell>256 ? 4096</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row><row><cell>Adam ?</cell><cell>(0.9, 0.98)</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell></row><row><cell>Training updates</cell><cell>260K</cell></row><row><cell>Gradient clipping</cell><cell>1.0</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.0</cell></row><row><cell>Hidden size</cell><cell>1024</cell></row><row><cell>FFN inner hidden size</cell><cell>4096</cell></row><row><cell>Attention heads</cell><cell>16</cell></row><row><cell>Layers</cell><cell>100-100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters for the machine translation experiments on the 102-language dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure9: Evaluation results of 12B M2M-100 on a subset of FLORES-101 devtest set. The i-th row is the source language, while j-th column is the target language. There are 87 languages and 7,482 directions.</figDesc><table><row><cell></cell><cell>af</cell><cell></cell><cell cols="2">am</cell><cell>ar</cell><cell></cell><cell cols="2">ast</cell><cell></cell><cell cols="2">az</cell><cell></cell><cell cols="2">be</cell><cell>bg</cell><cell>bn</cell><cell>bs</cell><cell>ca</cell><cell cols="3">ceb cs</cell><cell>cy</cell><cell cols="2">da</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>et</cell><cell>fa</cell><cell>ff</cell><cell>fi</cell><cell>fr</cell><cell></cell><cell>ga</cell><cell>gl</cell><cell>gu</cell><cell>ha</cell><cell>he</cell><cell>hi</cell><cell>hr</cell><cell cols="2">hu</cell><cell>hy</cell><cell>id</cell><cell>ig</cell><cell>is</cell><cell>it</cell><cell>ja</cell><cell>jv</cell><cell></cell><cell>ka</cell><cell cols="2">kk</cell><cell>km</cell><cell cols="2">kn</cell><cell>ko</cell><cell>lb</cell><cell></cell><cell>lg</cell><cell></cell><cell>ln</cell><cell></cell><cell>lo</cell><cell>lt</cell><cell>lv</cell><cell></cell><cell>mk</cell><cell>ml</cell><cell>mn</cell><cell>mr</cell><cell>ms</cell><cell>my</cell><cell>ne</cell><cell>nl</cell><cell>no</cell><cell>ns</cell><cell>oc</cell><cell>or</cell><cell></cell><cell>pa</cell><cell>pl</cell><cell>ps</cell><cell>pt</cell><cell>ro</cell><cell>ru</cell><cell>sd</cell><cell>sk</cell><cell>sl</cell><cell>so</cell><cell>sr</cell><cell>sv</cell><cell>sw</cell><cell>ta</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>uk</cell><cell>ur</cell><cell>uz</cell><cell>vi</cell><cell>wo</cell><cell>xh</cell><cell>yo</cell><cell>zh</cell><cell>zu</cell></row><row><cell>af</cell><cell>-</cell><cell></cell><cell></cell><cell cols="6">10.7 23.8 21.7</cell><cell></cell><cell cols="20">8.6 12.8 37.4 24.2 27.6 29.4 16.1 31.5 21.5 39.5 34.6 28.6 55.5 25.9 29.5 25.7</cell><cell></cell><cell>0.1</cell><cell cols="2">26 41.5</cell><cell cols="2">0.3 32.7</cell><cell cols="7">7.2 10.9 28.3 29.4 28.1 28.2</cell><cell cols="8">9.6 35.5 13.8 23.4 28.2 24.4 19.5</cell><cell cols="3">5.1 10.1</cell><cell>14</cell><cell></cell><cell>4</cell><cell cols="3">19 23.9</cell><cell></cell><cell>1.3</cell><cell></cell><cell cols="10">0.8 13.7 29.3 22.7 33.6 18.8 11.3 15.8 32.9</cell><cell>5.6</cell><cell cols="3">2.4 28.4 30.9</cell><cell cols="2">4 28.9</cell><cell></cell><cell cols="5">0.4 13.6 23.3 12.8 41.3</cell><cell cols="2">36 29.2 12.1 32.2 28.9</cell><cell>4.8 30.8</cell><cell>38 29.2</cell><cell>5.7 20.1</cell><cell>23 27.5</cell><cell>28 16.2</cell><cell>1.6 35.3</cell><cell>0.7 12.9</cell><cell>2.9 18.9 13.5</cell></row><row><cell>am</cell><cell cols="3">12.3 -</cell><cell></cell><cell></cell><cell>3.1</cell><cell></cell><cell cols="2">4.7</cell><cell></cell><cell cols="2">3.9</cell><cell></cell><cell cols="3">4.7 13.9 13.6</cell><cell cols="2">9.6 14.2</cell><cell></cell><cell>4.9</cell><cell cols="10">12 11.6 12.9 11.9 13.8 16.9 12.3 13.7 11.3</cell><cell></cell><cell cols="3">0.1 11.7 18.4</cell><cell cols="2">0.3 12.3</cell><cell>5.2</cell><cell cols="6">5.6 10.2 16.1 10.8 13.6</cell><cell cols="2">3.2 10.3</cell><cell>9.5</cell><cell cols="2">9.4 13.3</cell><cell></cell><cell>7.9</cell><cell>8</cell><cell>3.6</cell><cell></cell><cell>3.9</cell><cell>8.6</cell><cell></cell><cell>2.3</cell><cell>7.4</cell><cell></cell><cell>9.3</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="4">7 13.9 12.9 14.8</cell><cell>9.3</cell><cell>8.1</cell><cell>8</cell><cell>9.5</cell><cell>1.8</cell><cell>5.8</cell><cell cols="2">9.3 10.2</cell><cell cols="2">1.6 11.2</cell><cell></cell><cell>0.4</cell><cell cols="2">9.7 10.3</cell><cell cols="3">8.8 17.1 15.3</cell><cell>9.2</cell><cell>7.5 13.4 13.5</cell><cell>2.5 12.5 13.4 13.6</cell><cell>4 11.1 12.5</cell><cell>9.3 11.8</cell><cell>9.1</cell><cell>0.8 17.1</cell><cell>0.6</cell><cell>8.2</cell><cell>1.2</cell><cell>8</cell><cell>8.9</cell></row><row><cell>ar</cell><cell cols="2">25.2</cell><cell></cell><cell cols="2">8.4 -</cell><cell></cell><cell></cell><cell cols="2">15.8</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell cols="5">1.2 31.8 21.9 22.3 24.2</cell><cell></cell><cell cols="11">3.3 26.4 16.4 29.5 27.1 26.1 36.6 22.8 24.4 23.9</cell><cell></cell><cell cols="3">0 21.9 34.5</cell><cell cols="2">0.3 26.1</cell><cell>0.8</cell><cell cols="6">8 25.3 26.2 23.4 23.9</cell><cell cols="3">7.1 29.7 12.2</cell><cell cols="4">20 24.2 22.2</cell><cell>15</cell><cell>1.9</cell><cell></cell><cell>2.5</cell><cell>12</cell><cell></cell><cell cols="4">0.7 17.8 15.9</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="2">8.7 25.1</cell><cell cols="2">27 28.1</cell><cell cols="2">8.4 10.2</cell><cell cols="2">5.8 25.8</cell><cell>2.6</cell><cell>0.2</cell><cell cols="2">23 22.4</cell><cell cols="2">1.6 21.5</cell><cell></cell><cell>0.4</cell><cell cols="5">9.1 19.6 11.4 33.3 29.5</cell><cell>25 10.5 27.3 24.7</cell><cell>2.5 21.5 29.1 21.2</cell><cell>1.1 18.8</cell><cell>7.4</cell><cell>23</cell><cell>23 12.6</cell><cell>1.1 30.7</cell><cell>0.6 10.7</cell><cell>1.3 18.5 11.3</cell></row><row><cell>ast</cell><cell cols="2">22.8</cell><cell></cell><cell cols="4">5 19.7 -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell cols="5">8.6 26.8 18.8 16.5 26.4</cell><cell></cell><cell cols="11">7.4 21.1 14.2 27.6 25.8 25.2 38.4 24.3 23.8 18.6</cell><cell></cell><cell cols="3">0 21.2 32.5</cell><cell cols="2">0.3 29.9</cell><cell>1.8</cell><cell cols="6">5.3 23.1 22.4 18.6 22.7</cell><cell cols="5">2.9 27.3 10.8 16.3 22.9</cell><cell></cell><cell cols="2">20 12.7</cell><cell>1.3</cell><cell></cell><cell>3.3</cell><cell>8.6</cell><cell></cell><cell cols="2">1 14.2</cell><cell></cell><cell>17</cell><cell></cell><cell>1</cell><cell></cell><cell>0.4</cell><cell></cell><cell cols="5">7.6 22.7 17.1 25.2 10.7</cell><cell>8.4</cell><cell cols="2">9.5 23.8</cell><cell>2.2</cell><cell cols="3">1.5 19.4 21.3</cell><cell>1.9</cell><cell>19</cell><cell></cell><cell>0.2</cell><cell cols="5">8 18.5 10.1 33.2 26.3</cell><cell>21</cell><cell>5.2 21.2</cell><cell>19</cell><cell>2 23.1 25.8 18.9</cell><cell>1.1 16.9 17.7</cell><cell>21</cell><cell>21</cell><cell>9</cell><cell>0.8 27.8</cell><cell>0.7 10.2</cell><cell>1.1 14.7 10.2</cell></row><row><cell>az</cell><cell></cell><cell>7.4</cell><cell></cell><cell>3.7</cell><cell></cell><cell>1.7</cell><cell></cell><cell cols="3">2.6 -</cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.2 11.7</cell><cell>7.8</cell><cell>6.1</cell><cell>10</cell><cell></cell><cell>1.4</cell><cell>8.1</cell><cell>6.6</cell><cell></cell><cell>7.7</cell><cell>9.9</cell><cell>8.3</cell><cell>9.3</cell><cell cols="2">9.8 11.8</cell><cell>7.2</cell><cell></cell><cell cols="3">0.1 10.2 13.1</cell><cell>0.3</cell><cell>8.4</cell><cell>2.6</cell><cell>3.9</cell><cell cols="2">4.6</cell><cell>10</cell><cell cols="2">7.6 10.3</cell><cell>2.1</cell><cell>5.9</cell><cell>7.6</cell><cell cols="2">6.8 10.6</cell><cell></cell><cell>5.2</cell><cell>3.1</cell><cell>1.8</cell><cell></cell><cell>1.9</cell><cell>4.4</cell><cell></cell><cell>1</cell><cell>5.7</cell><cell></cell><cell>3.5</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="4">2.8 12.4 13.6 10.6</cell><cell>4.4</cell><cell>6.5</cell><cell>4.2</cell><cell>5.1</cell><cell>0.5</cell><cell>1.1</cell><cell>8</cell><cell>6.5</cell><cell>1.5</cell><cell>5.3</cell><cell></cell><cell>0.3</cell><cell>6.6</cell><cell>8.4</cell><cell cols="3">6.9 12.4 10.9</cell><cell>9.4</cell><cell>3.5 11.3 10.1</cell><cell>1.1</cell><cell>7.6</cell><cell>9.6</cell><cell>8</cell><cell>2.1 10.9</cell><cell>3.2 11.5 11.5</cell><cell>4.9</cell><cell>0.9 16.4</cell><cell>0.6</cell><cell>5.8</cell><cell>0.6</cell><cell>5.5</cell><cell>6.2</cell></row><row><cell>be</cell><cell cols="2">13.9</cell><cell></cell><cell cols="3">5.4 10.4</cell><cell></cell><cell cols="2">6.9</cell><cell></cell><cell cols="3">4.2 -</cell><cell></cell><cell cols="4">19.4 13.1 12.3 13.9</cell><cell></cell><cell>7.4</cell><cell cols="4">14 10.1 13.7</cell><cell cols="6">15 15.4 16.5 14.2 14.7 11.1</cell><cell></cell><cell cols="3">0.1 12.5 17.2</cell><cell>0.3</cell><cell>16</cell><cell>4.3</cell><cell cols="4">5.3 12.7 15.8</cell><cell cols="2">13 12.2</cell><cell cols="2">5.4 12.6</cell><cell cols="5">8.6 11.7 15.5 11.9</cell><cell>8.9</cell><cell>4.8</cell><cell></cell><cell>5.3</cell><cell>9.1</cell><cell></cell><cell>1.9</cell><cell cols="3">9.7 12.4</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="2">7.5 15.9</cell><cell cols="2">9.4 18.3</cell><cell>9.3</cell><cell>7.3</cell><cell cols="2">8.7 11.5</cell><cell>2.2</cell><cell cols="3">3.5 14.7 11.4</cell><cell cols="2">1.8 13.4</cell><cell></cell><cell>0.3</cell><cell cols="2">7.5 13.9</cell><cell>7</cell><cell cols="3">16 16.5 18.4</cell><cell>6.3 14.7</cell><cell>14</cell><cell>2.5 15.5 15.5 13.3</cell><cell>2.5 12.6 12.2</cell><cell>8.1 18.7</cell><cell>8</cell><cell>0.9 17.4</cell><cell>0.7</cell><cell>6.8</cell><cell>1.5 10.6</cell><cell>7.2</cell></row><row><cell>bg</cell><cell cols="2">29.2</cell><cell></cell><cell cols="6">9.3 23.9 20.6</cell><cell></cell><cell cols="2">9.2</cell><cell></cell><cell cols="2">4.3 -</cell><cell cols="15">23.9 28.2 28.5 10.6 30.4 18.7 34.2 32.2 29.6 41.8 26.3 29.6 25.2</cell><cell></cell><cell cols="3">0.1 25.5 38.9</cell><cell cols="2">0.3 31.3</cell><cell>5.9</cell><cell cols="6">9.2 27.8 27.9 28.3 27.9</cell><cell cols="8">9.3 31.7 12.9 22.3 28.7 24.2 16.6</cell><cell>3.9</cell><cell></cell><cell cols="2">7.5 13.5</cell><cell></cell><cell cols="2">3.3 19.9</cell><cell></cell><cell>22</cell><cell></cell><cell>0.7</cell><cell></cell><cell cols="8">0.5 11.8 29.4 17.1 35.9 17.2 11.1</cell><cell cols="2">15 27.6</cell><cell>4.7</cell><cell cols="3">1 27.2 26.3</cell><cell cols="2">2.6 26.3</cell><cell></cell><cell cols="7">0.4 13.7 24.7 12.1 37.5 32.7 30.5 11.5 31.2 28.8</cell><cell>4.4 30.2 33.9</cell><cell>26</cell><cell>2.9 19.7</cell><cell>19 25.2 29.5 15.7</cell><cell>1.7 33.7</cell><cell>0.7 11.2</cell><cell>2.2</cell><cell>19 11.9</cell></row><row><cell>bn</cell><cell cols="2">21.9</cell><cell></cell><cell cols="6">9.6 18.2 13.4</cell><cell></cell><cell cols="2">7.5</cell><cell></cell><cell cols="3">6.3 25.6 -</cell><cell cols="2">18.7 21.1</cell><cell></cell><cell cols="9">8.3 21.3 15.2 24.8 22.4 21.5 30.8 19.2</cell><cell cols="2">21 20.2</cell><cell></cell><cell cols="3">0.2 18.9 28.9</cell><cell cols="2">0.3 22.3</cell><cell>8.4</cell><cell cols="6">8.1 20.2 26.7 19.7 20.9</cell><cell cols="8">8.4 24.8 11.8 17.5 20.9 21.3 13.7</cell><cell>5.2</cell><cell></cell><cell cols="2">5 12.5</cell><cell></cell><cell cols="4">5 16.7 15.8</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="6">0.5 11.3 21.2 22.2 24.3</cell><cell>18</cell><cell cols="3">10 15.6 22.9</cell><cell>5.2</cell><cell>1.5</cell><cell cols="2">20 18.8</cell><cell cols="2">2.5 20.2</cell><cell></cell><cell cols="7">0.4 15.1 16.9 11.4 27.2 25.2 20.6 11.8 21.6 20.7</cell><cell>3.3 20.5 24.3 20.7</cell><cell>6.5 16.5 13.3 19.9 19.6 15.1</cell><cell>1.3 27.3</cell><cell>0.6</cell><cell>10</cell><cell>1.9 14.9 10.5</cell></row><row><cell>bs</cell><cell cols="17">29.8 11.4 23.9 19.4 10.4 11.3 38.2 24.4 -</cell><cell cols="7">28.6 16.6 33.7 19.5 35.2</cell><cell cols="6">33 28.8 41.7 26.4 30.3 25.2</cell><cell></cell><cell cols="2">0.3 26.7</cell><cell>40</cell><cell cols="3">0.3 32.5 12.4</cell><cell>9.7</cell><cell></cell><cell cols="8">28 28.4 34.1 29.1 13.1 33.6 13.8 23.1</cell><cell cols="8">29 23.7 18.7 11.1 12.7 14.5</cell><cell></cell><cell cols="4">5.9 19.1 23.4</cell><cell></cell><cell>1.3</cell><cell></cell><cell cols="10">0.5 12.6 30.6 23.2 37.2 18.8 11.6 16.3 29.5</cell><cell>7</cell><cell cols="3">7 27.5 27.5</cell><cell cols="2">3.3 28.1</cell><cell></cell><cell cols="2">0.5 13.4</cell><cell cols="5">25 12.4 38.1 34.7 30.3 12.8</cell><cell>34 32.9</cell><cell>5.3 36.4 34.7 27.6</cell><cell>9.1 20.2 22.5</cell><cell>27 29.5 15.9</cell><cell>1.9 34.1</cell><cell>0.7 12.8</cell><cell>2.8 18.6 13.2</cell></row><row><cell>ca</cell><cell></cell><cell cols="8">28 10.2 21.7 27.6</cell><cell></cell><cell cols="2">8.5</cell><cell></cell><cell cols="5">9.1 33.8 21.8 24.3 -</cell><cell></cell><cell cols="11">13.7 27.8 17.7 33.4 30.9 27.7 42.5 26.6 26.1 22.2</cell><cell></cell><cell cols="3">0.1 23.8 39.7</cell><cell cols="2">0.3 32.8</cell><cell>6.4</cell><cell>9.5</cell><cell></cell><cell cols="4">25 25.8 25.9 24.7</cell><cell cols="8">8.1 31.6 12.3 20.5 29.4 22.6 15.9</cell><cell>3.2</cell><cell></cell><cell cols="2">8.9 12.3</cell><cell></cell><cell cols="4">3.8 18.1 21.3</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="7">0.5 11.6 25.9 25.6 30.3 15.7</cell><cell cols="3">9.2 13.5 27.3</cell><cell>5.4</cell><cell cols="3">1.6 25.2 25.9</cell><cell cols="2">3.1 28.2</cell><cell></cell><cell cols="7">0.5 11.7 21.8 10.6 38.1 33.4 27.1 10.1</cell><cell>28 26.4</cell><cell>4.5</cell><cell>26 32.2</cell><cell>25</cell><cell>2.3</cell><cell>18 18.7 24.3 25.4 14.2</cell><cell>1.3 31.6</cell><cell>0.9 11.2</cell><cell>2.3 17.7 11.6</cell></row><row><cell>ceb</cell><cell cols="2">18.5</cell><cell></cell><cell cols="6">6.9 10.9 11.2</cell><cell></cell><cell cols="2">6.3</cell><cell></cell><cell cols="6">9.5 19.4 14.5 14.9 17.7 -</cell><cell></cell><cell cols="4">15.6 14.3 17.6</cell><cell cols="3">17 16.6 27.8</cell><cell cols="3">17 15.8 13.5</cell><cell></cell><cell cols="3">0.4 13.5 23.2</cell><cell>0.3</cell><cell>20</cell><cell>6.7</cell><cell cols="6">8.8 13.8 16.7 14.7 15.6</cell><cell>5.4</cell><cell cols="7">21 10.8 13.6 17.9 14.9 14.9</cell><cell>4.9</cell><cell></cell><cell>8.3</cell><cell>9.9</cell><cell></cell><cell cols="4">3.7 10.6 14.8</cell><cell></cell><cell>1.8</cell><cell></cell><cell>0.9</cell><cell></cell><cell>9</cell><cell cols="4">15 15.3 18.8 11.1</cell><cell cols="3">7.3 10.3 17.6</cell><cell>5.1</cell><cell cols="3">8.7 15.3 14.6</cell><cell cols="2">4.3 17.3</cell><cell></cell><cell>0.4</cell><cell cols="2">8.7 13.2</cell><cell cols="4">8.7 22.8 19.3 15.7</cell><cell>8.8 16.3 15.5</cell><cell>5.2 16.4 17.8 17.8</cell><cell>5.6 13.2 22.6 13.4 15.2</cell><cell>9.6</cell><cell>1.9 20.8</cell><cell>0.9 10.6</cell><cell>2.9 11.5 11.1</cell></row><row><cell>cs</cell><cell cols="2">28.3</cell><cell></cell><cell cols="6">9.6 22.6 19.2</cell><cell></cell><cell cols="2">8.6</cell><cell></cell><cell cols="3">2.8 35.6 23.2</cell><cell cols="2">28 27.1</cell><cell></cell><cell>9.8 -</cell><cell></cell><cell cols="9">17.5 34.4 32.9 28.1 40.6 25.8 29.8 23.2</cell><cell></cell><cell cols="3">0.1 26.6 38.4</cell><cell cols="2">0.3 30.4</cell><cell>5.5</cell><cell>9.5</cell><cell></cell><cell cols="4">27 26.6 29.5 28.6</cell><cell cols="8">8.2 31.7 12.9 22.4 28.4 24.2 16.9</cell><cell>3.5</cell><cell></cell><cell cols="2">5.9 13.2</cell><cell></cell><cell cols="4">3.2 19.3 21.9</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="5">0.6 11.8 29.8 23.2</cell><cell cols="5">32 16.7 10.8 14.1 28.1</cell><cell>4.1</cell><cell cols="2">0.5 27.2</cell><cell>26</cell><cell cols="2">2.9 24.4</cell><cell></cell><cell cols="7">0.5 12.4 25.6 12.2 36.6 33.8 29.6 10.6 36.6 31.1</cell><cell>3.6 27.6 33.5</cell><cell>26</cell><cell>2.5 19.8 17.9 25.2 28.2 14.8</cell><cell>1.6 33.3</cell><cell>0.7 11.2</cell><cell>2.2 19.5</cell><cell>12</cell></row><row><cell>cy</cell><cell cols="2">21.9</cell><cell></cell><cell>7.8</cell><cell></cell><cell>8</cell><cell></cell><cell cols="2">8.3</cell><cell></cell><cell cols="2">4.1</cell><cell></cell><cell cols="7">8.9 21.8 16.5 15.2 19.8 11.5</cell><cell cols="2">17 -</cell><cell></cell><cell cols="7">22.1 18.5 18.2 29.5 17.2 18.8 15.8</cell><cell></cell><cell cols="3">0.4 15.2 25.9</cell><cell cols="2">0.3 21.3</cell><cell>7.5</cell><cell cols="6">6.7 17.2 19.6 15.2 16.7</cell><cell cols="8">6.5 20.7 12.1 15.8 18.1 10.6 15.5</cell><cell>6.2</cell><cell></cell><cell cols="2">6.6 11.5</cell><cell></cell><cell>3.9</cell><cell cols="3">8.4 17.1</cell><cell></cell><cell>1.2</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="3">6.9 16.7 14.9</cell><cell cols="2">21 12.3</cell><cell cols="3">8.3 10.5 19.3</cell><cell>4</cell><cell cols="3">8.5 14.9 17.9</cell><cell>3.7</cell><cell>19</cell><cell></cell><cell>0.4</cell><cell cols="2">7.5 13.4</cell><cell cols="4">9.5 25.6 22.7 15.1</cell><cell>9.7 18.4 17.6</cell><cell>4.3 18.6 21.4 19.5</cell><cell>6</cell><cell>13 18.3 13.1</cell><cell>17 11.1</cell><cell>1.2 22.9</cell><cell>0.7 10.4</cell><cell>2.6 10.2 11.5</cell></row><row><cell>da</cell><cell cols="2">33.7</cell><cell></cell><cell cols="6">8.1 24.8 20.8</cell><cell></cell><cell cols="2">8.1</cell><cell></cell><cell cols="2">5.3 37.8</cell><cell cols="3">24 28.4 30.7</cell><cell></cell><cell cols="2">14 32.3</cell><cell cols="2">20 -</cell><cell></cell><cell cols="6">36.5 29.5 49.3 27.4 31.1 25.2</cell><cell></cell><cell cols="3">0.1 28.9 42.1</cell><cell cols="2">0.3 32.5</cell><cell cols="7">5.7 10.5 28.9 29.1 29.4 29.5</cell><cell cols="2">8.3 35.7</cell><cell>14</cell><cell cols="5">26 30.4 25.3 18.4</cell><cell>3.3</cell><cell></cell><cell cols="2">6.4 13.7</cell><cell></cell><cell cols="2">3.5 20.9</cell><cell></cell><cell>25</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="10">0.5 12.1 29.6 29.1 34.1 17.4 11.2 14.3 31.4</cell><cell>4.8</cell><cell cols="3">1 29.8 32.6</cell><cell cols="2">3.6 27.2</cell><cell></cell><cell>0.3</cell><cell cols="6">13 24.1 12.7 40.4 36.4 29.5 11.5 33.1 30.2</cell><cell>4.5 30.2 42.3 27.7</cell><cell>3.1 21.1 21.9 27.6 27.7 16.3</cell><cell>1.6 35.7</cell><cell>0.8 12.8</cell><cell>2.5 19.7 13.2</cell></row><row><cell>de</cell><cell cols="9">31.6 10.4 24.3 21.3</cell><cell></cell><cell cols="2">8.8</cell><cell></cell><cell cols="5">7.2 37.3 23.6 28.4 29.7</cell><cell></cell><cell cols="6">11 32.8 18.7 38.8 -</cell><cell cols="5">29.4 44.7 27.2 30.5 24.9</cell><cell></cell><cell cols="2">0.1 28.6</cell><cell>42</cell><cell cols="2">0.3 32.9</cell><cell>6.8</cell><cell cols="6">9.8 28.1 28.4 29.5 30.4</cell><cell cols="8">8.6 33.4 13.4 24.4 30.2 25.4 16.9</cell><cell>3.9</cell><cell></cell><cell cols="2">8.5 13.2</cell><cell></cell><cell cols="4">3.6 20.6 26.3</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="10">1.1 12.3 30.1 17.1 33.2 18.5 10.7 15.3 29.3</cell><cell>4.2</cell><cell cols="3">1.2 30.1 29.7</cell><cell cols="2">3.1 26.1</cell><cell></cell><cell cols="7">0.5 13.7 25.5 12.4 39.3 35.7 30.3 11.3 33.9 31.6</cell><cell>3.5 29.4 37.9 26.8</cell><cell>3.4 20.4 21.9 26.9 28.2 15.8</cell><cell>1.5 34.2</cell><cell>0.8 12.2</cell><cell>2.1 20.3 12.5</cell></row><row><cell>el</cell><cell cols="2">25.8</cell><cell></cell><cell cols="6">8.6 21.7 19.9</cell><cell></cell><cell cols="2">8.2</cell><cell></cell><cell cols="11">3 32.5 21.5 23.9 25.1 11.2 27.1 16.7 30.8</cell><cell cols="2">29 -</cell><cell cols="2">36.5 24.4</cell><cell cols="2">25 23.1</cell><cell></cell><cell cols="3">0.1 22.8 35.9</cell><cell cols="2">0.3 27.8</cell><cell>4.3</cell><cell cols="6">8.3 25.3 24.7 25.1 24.4</cell><cell cols="8">8.2 28.8 12.3 20.2 26.3 23.4 15.7</cell><cell>3.2</cell><cell></cell><cell cols="2">4.7 12.4</cell><cell></cell><cell cols="4">3 18.5 19.9</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="8">0.4 10.1 25.7 27.6 29.8 15.2 10.1</cell><cell cols="2">13 25.8</cell><cell>4.1</cell><cell cols="3">0.2 24.7 23.3</cell><cell cols="2">2.5 23.8</cell><cell></cell><cell cols="7">0.4 11.9 21.5 11.1 33.6 29.3 25.7 10.4 28.3 26.5</cell><cell>3.3 25.3 29.4 21.6</cell><cell>2.4 18.5 16.7 23.4 24.5 14.4</cell><cell>1.3 30.1</cell><cell>0.6 10.8</cell><cell>1.9 18.2 11.6</cell></row><row><cell>en</cell><cell cols="12">43.4 13.1 29.8 31.5 10.4</cell><cell></cell><cell cols="14">13 46.2 28.7 33.3 36.6 20.5 38.1 25.5 47.6 42.1 33.7 -</cell><cell cols="2">30.3 34.8</cell><cell>30</cell><cell></cell><cell cols="3">0.1 31.6 51.4</cell><cell cols="2">0.3 39.4</cell><cell cols="9">9.9 12.2 35.3 35.9 33.4 34.7 10.4 45.4</cell><cell cols="6">15 26.8 33.6 29.1 23.1</cell><cell cols="4">3.4 10.6 15.6</cell><cell></cell><cell>4.7</cell><cell cols="3">23 28.1</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="3">0.9 14.4</cell><cell cols="4">34 36.9 40.4 20.7</cell><cell cols="3">12 18.5 39.7</cell><cell>6.9</cell><cell cols="3">2.9 32.3 37.2</cell><cell cols="2">4.2 37.9</cell><cell></cell><cell cols="7">0.5 15.2 26.5 14.3 51.8 43.5 35.3 13.4 39.4 34.4</cell><cell>5.2</cell><cell>35 47.1 35.1</cell><cell>3 23.4 26.6 32.8 33.7 19.1</cell><cell>2.1 40.9</cell><cell>0.8 15.6</cell><cell>3.3 23.3</cell><cell>15</cell></row><row><cell>es</cell><cell cols="2">23.1</cell><cell></cell><cell cols="6">8.9 20.4 18.6</cell><cell></cell><cell cols="2">7.8</cell><cell></cell><cell cols="7">3.1 30.6 20.9 21.8 23.4 10.3</cell><cell cols="8">25 15.9 29.2 27.1 26.3 31.1 -</cell><cell cols="2">24.1 20.9</cell><cell></cell><cell cols="3">0.1 22.6 32.8</cell><cell cols="2">0.3 27.8</cell><cell>4.2</cell><cell cols="6">8.5 23.3 23.9 22.5 24.6</cell><cell cols="8">7 26.9 12.4 19.4 27.1 22.1 14.7</cell><cell>2.2</cell><cell></cell><cell cols="2">5.3 11.9</cell><cell></cell><cell cols="4">2.6 17.4 18.9</cell><cell></cell><cell>1.1</cell><cell></cell><cell>1</cell><cell></cell><cell cols="5">9.7 24.5 25.7 28.2 14.4</cell><cell cols="3">9.2 11.6 23.3</cell><cell>3.8</cell><cell cols="3">0.6 23.5 23.1</cell><cell cols="2">2.8 23.8</cell><cell></cell><cell>0.4</cell><cell cols="6">9.6 21.6 10.5 30.1 29.2 24.2</cell><cell>9.8 25.9 24.6</cell><cell>3.2 23.3 27.5 14.3</cell><cell>1.6 17.8</cell><cell>14 22.3 22.4 12.9</cell><cell>1.4 28.3</cell><cell>0.7 10.4</cell><cell>1.8 16.8 11.3</cell></row><row><cell>et</cell><cell cols="2">27.6</cell><cell></cell><cell cols="6">8.8 21.8 20.5</cell><cell></cell><cell cols="2">8.4</cell><cell></cell><cell cols="16">5.2 34.3 23.2 26.6 25.9 10.1 29.6 16.8 32.8 31.5 26.6 38.3 25.5 -</cell><cell>23.8</cell><cell></cell><cell cols="3">0.1 26.1 36.9</cell><cell cols="2">0.3 28.6</cell><cell>6.9</cell><cell cols="6">9.2 25.9 26.8 27.5 25.9</cell><cell cols="8">8.4 30.7 12.8 22.2 27.1 24.2 16.8</cell><cell>4.3</cell><cell></cell><cell cols="2">4.4 12.9</cell><cell></cell><cell cols="4">3.8 19.2 20.9</cell><cell></cell><cell>1</cell><cell></cell><cell cols="6">0.4 12.3 29.5 30.9 30.7</cell><cell cols="4">17 10.5 14.4 27.3</cell><cell>4.9</cell><cell cols="3">0.9 26.6 25.4</cell><cell cols="2">2.7 23.3</cell><cell></cell><cell cols="7">0.4 13.2 23.7 11.9 34.9 32.3 28.6</cell><cell>11 30.9 28.9</cell><cell>4.1 27.2 32.9 23.6</cell><cell>2.9 19.7 19.1 24.7 26.6 15.2</cell><cell>1.6 32.6</cell><cell>0.6</cell><cell>11</cell><cell>2.4 19.4</cell><cell>12</cell></row><row><cell>fa</cell><cell cols="2">24.4</cell><cell></cell><cell cols="6">8.9 22.3 14.5</cell><cell></cell><cell cols="2">6.9</cell><cell></cell><cell cols="5">2.6 29.8 21.7 22.3 23.7</cell><cell></cell><cell cols="6">6.4 25.3 16.2 27.4 26.2</cell><cell cols="5">25 33.7 22.3 24.1 -</cell><cell></cell><cell cols="2">0 21.2</cell><cell>33</cell><cell cols="2">0.3 25.6</cell><cell>1.9</cell><cell cols="6">8.7 23.6 25.8 23.3 23.6</cell><cell>8.3</cell><cell cols="7">28 12.4 19.2 24.4 21.8 14.7</cell><cell>3.7</cell><cell></cell><cell cols="2">2.9 12.4</cell><cell></cell><cell cols="4">1.6 17.9 16.4</cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="6">10 24.1 26.3 27.3 11.8 10.6</cell><cell cols="2">8.9 24.8</cell><cell>3.1</cell><cell cols="3">0.3 22.3 21.8</cell><cell cols="2">2.1 20.7</cell><cell></cell><cell cols="7">0.5 10.6 19.7 12.2 31.7 28.2 23.4</cell><cell>11 25.9 24.3</cell><cell>2.9 22.3</cell><cell>27 20.7</cell><cell>2.8 18.5</cell><cell>8.6 22.4</cell><cell>22 14.7</cell><cell>1.2 30.4</cell><cell>0.7 10.5</cell><cell>1.6 17.5 11.3</cell></row><row><cell>ff</cell><cell></cell><cell>3.3</cell><cell></cell><cell>1.5</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="2">1.4</cell><cell></cell><cell cols="2">1.3</cell><cell></cell><cell>2</cell><cell>3.7</cell><cell>3</cell><cell>2.4</cell><cell>3.2</cell><cell></cell><cell>2.4</cell><cell>2.7</cell><cell>2.6</cell><cell></cell><cell>2.9</cell><cell>2.8</cell><cell>3.2</cell><cell>3.9</cell><cell>3</cell><cell>2.9</cell><cell cols="2">2.1 -</cell><cell></cell><cell>2.1</cell><cell>3.6</cell><cell>0.3</cell><cell>3</cell><cell>1.7</cell><cell>2.5</cell><cell cols="2">2.3</cell><cell>3.3</cell><cell>2.4</cell><cell>3.2</cell><cell>1.2</cell><cell>2.7</cell><cell>2.1</cell><cell>2.6</cell><cell>3</cell><cell></cell><cell>3.3</cell><cell>2.3</cell><cell>1.1</cell><cell></cell><cell>1.9</cell><cell>2</cell><cell></cell><cell>0.9</cell><cell>2.5</cell><cell></cell><cell>2.2</cell><cell></cell><cell>0.8</cell><cell></cell><cell>0.4</cell><cell></cell><cell>1.3</cell><cell>2.5</cell><cell>2.2</cell><cell>3.5</cell><cell>2.5</cell><cell>1.4</cell><cell>1.9</cell><cell>2.6</cell><cell>1</cell><cell>1.7</cell><cell>2.9</cell><cell>2.6</cell><cell>1.4</cell><cell>3.4</cell><cell></cell><cell>0.2</cell><cell>1.3</cell><cell>2.8</cell><cell>1.5</cell><cell>3.3</cell><cell>2.9</cell><cell>2.6</cell><cell>1</cell><cell>2.7</cell><cell>2.8</cell><cell>0.8</cell><cell>2.3</cell><cell>2.7</cell><cell>2.5</cell><cell>1.5</cell><cell>2.3</cell><cell>2.6</cell><cell>2.2</cell><cell>2.8</cell><cell>1.3</cell><cell>0.9</cell><cell>2.9</cell><cell>0.6</cell><cell>2.1</cell><cell>1</cell><cell>2.5</cell><cell>2.2</cell></row><row><cell>fi</cell><cell cols="2">25.4</cell><cell></cell><cell cols="6">8.3 20.4 17.7</cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="5">3 31.2 21.4 23.3 24.5</cell><cell></cell><cell cols="3">7.9 27.4 15.5</cell><cell></cell><cell>31</cell><cell cols="6">29 25.3 35.3 24.3 26.3 21.4</cell><cell></cell><cell>0.1 -</cell><cell cols="2">35.3</cell><cell cols="2">0.3 26.4</cell><cell>5.2</cell><cell cols="6">8.4 24.1 24.8 24.9 25.5</cell><cell cols="7">7.6 28.4 12.2 21.2 25.8 23.8</cell><cell>15</cell><cell>3.5</cell><cell></cell><cell cols="2">4.3 11.8</cell><cell></cell><cell cols="4">3.1 18.7 19.7</cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.5</cell><cell></cell><cell>11</cell><cell cols="4">27 28.5 28.7 15.1</cell><cell cols="3">9.8 11.7 24.5</cell><cell>3.7</cell><cell cols="3">0.5 25.6 23.5</cell><cell cols="2">2.8 20.5</cell><cell></cell><cell cols="7">0.4 11.4 22.2 10.7 32.4 29.7 26.8</cell><cell>9.8</cell><cell>28 26.9</cell><cell>2.8 23.9 31.6 20.1</cell><cell>2.4</cell><cell>19 13.1 23.1 23.8 14.1</cell><cell>1.4 30.7</cell><cell>0.6 10.1</cell><cell>1.8 18.4 11.3</cell></row><row><cell>fr</cell><cell cols="2">30.9</cell><cell></cell><cell cols="6">9.7 24.6 23.8</cell><cell></cell><cell cols="2">8.8</cell><cell></cell><cell cols="17">4.4 36.7 23.4 27.9 31.3 13.5 31.6 19.3 36.9 34.5 30.4 45.5 28.2 29.2 24.9</cell><cell></cell><cell cols="2">0.1 27.3 -</cell><cell></cell><cell cols="2">0.3 30.3</cell><cell cols="7">5.8 10.1 28.4 28.4 28.6 29.1</cell><cell cols="8">8.5 34.5 13.3 23.2 32.2 25.2 17.9</cell><cell>2.9</cell><cell></cell><cell cols="2">6.7 13.6</cell><cell></cell><cell>3.1</cell><cell cols="3">20 23.9</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="3">1.1 12.1</cell><cell cols="7">30 30.9 32.9 17.7 10.9 14.2 29.8</cell><cell>4.8</cell><cell cols="3">0.8 28.2 28.1</cell><cell cols="2">3.3 30.8</cell><cell></cell><cell cols="7">0.4 12.7 24.9 12.3 41.6 36.6 30.1 11.3 32.8 29.7</cell><cell>4.4 28.7 36.2 22.5</cell><cell>2.4 20.2 19.7 26.6 27.8 15.9</cell><cell>1.5 34.1</cell><cell>0.9 12.4</cell><cell>2.4 19.7 12.9</cell></row><row><cell>ga</cell><cell></cell><cell>1.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">0.2</cell><cell></cell><cell cols="2">0.3</cell><cell></cell><cell>0.5</cell><cell>1</cell><cell>0.4</cell><cell>0.3</cell><cell>0.9</cell><cell></cell><cell>0.6</cell><cell>0.8</cell><cell>1.4</cell><cell></cell><cell>1.1</cell><cell>1.1</cell><cell>0.5</cell><cell>2.6</cell><cell>1</cell><cell>1</cell><cell>0.2</cell><cell></cell><cell>0.1</cell><cell>1</cell><cell cols="2">1.5 -</cell><cell>1.2</cell><cell>0.4</cell><cell>1</cell><cell cols="2">0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>1.1</cell><cell>0.3</cell><cell>0.8</cell><cell>1.8</cell><cell>0.8</cell><cell>0.9</cell><cell></cell><cell>0.8</cell><cell>1.2</cell><cell>0</cell><cell></cell><cell>0.3</cell><cell>0.2</cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell></cell><cell>1.4</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell><cell>0.8</cell><cell>0.1</cell><cell>0.9</cell><cell>0.3</cell><cell>0.5</cell><cell>0.6</cell><cell>0.5</cell><cell>0</cell><cell>0.3</cell><cell>1</cell><cell>1</cell><cell>0.7</cell><cell>1</cell><cell></cell><cell>0.1</cell><cell>0.4</cell><cell>0.8</cell><cell>1</cell><cell>1.2</cell><cell>0.8</cell><cell>0.5</cell><cell>0.5</cell><cell>0.7</cell><cell>1</cell><cell>0.5</cell><cell>0.6</cell><cell>0.9</cell><cell>0.8</cell><cell>0.3</cell><cell>1</cell><cell>1.4</cell><cell>0.4</cell><cell>0.7</cell><cell>0.2</cell><cell>0.3</cell><cell>0.9</cell><cell>0.7</cell><cell>1.2</cell><cell>0.4</cell><cell>0.5</cell><cell>1.3</cell></row><row><cell>gl</cell><cell cols="2">28.9</cell><cell></cell><cell cols="6">9.3 22.9 26.9</cell><cell></cell><cell cols="2">8.5</cell><cell></cell><cell>9.3</cell><cell cols="10">35 22.5 25.5 29.2 14.7 29.2 17.7 33.9</cell><cell cols="6">32 28.6 41.5 28.2 26.5 23.8</cell><cell></cell><cell cols="3">0.2 24.6 34.4</cell><cell cols="2">0.4 -</cell><cell>5.4</cell><cell cols="4">9.2 27.1 26.7</cell><cell cols="2">27 26.2</cell><cell cols="2">7.9 32.7</cell><cell cols="6">13 21.9 29.8 23.5 17.2</cell><cell>2.6</cell><cell></cell><cell cols="2">8.8 12.8</cell><cell></cell><cell cols="4">2.8 19.2 22.5</cell><cell></cell><cell>1.2</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="2">12 27.4</cell><cell cols="3">29 32.2 16.3</cell><cell cols="3">9.9 14.3 28.5</cell><cell>5.4</cell><cell cols="3">1.3 26.4 26.7</cell><cell cols="2">3.4 25.7</cell><cell></cell><cell cols="7">0.5 12.3 23.5 11.1 36.8 34.2 28.3 10.9 30.2 28.7</cell><cell>4.1 27.6 33.7</cell><cell>25</cell><cell>2.3 19.5 20.9 25.2 25.9 14.8</cell><cell>1.6 33.3</cell><cell>0.8 11.3</cell><cell>2.6 18.5 11.9</cell></row><row><cell>gu</cell><cell></cell><cell>1.9</cell><cell></cell><cell>1</cell><cell></cell><cell>0</cell><cell></cell><cell cols="2">0.4</cell><cell></cell><cell cols="2">0.7</cell><cell></cell><cell>0.5</cell><cell>1.4</cell><cell>1.5</cell><cell>0.8</cell><cell>1.2</cell><cell></cell><cell>0.6</cell><cell>1</cell><cell>2.3</cell><cell></cell><cell>1.8</cell><cell>1.2</cell><cell>0.7</cell><cell>1</cell><cell>1.2</cell><cell>1.2</cell><cell>0.7</cell><cell></cell><cell>0.1</cell><cell>1</cell><cell>1.9</cell><cell>0.3</cell><cell cols="2">0.5 -</cell><cell>0.7</cell><cell cols="2">0.5</cell><cell>1.5</cell><cell>1.2</cell><cell>1.2</cell><cell>0.4</cell><cell>0.9</cell><cell>1.9</cell><cell>1.3</cell><cell>1.6</cell><cell></cell><cell>0.6</cell><cell>1.3</cell><cell>0.5</cell><cell></cell><cell>0.6</cell><cell>0.7</cell><cell></cell><cell>0.6</cell><cell>0.5</cell><cell></cell><cell>1.9</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0</cell><cell></cell><cell>0.7</cell><cell>1.9</cell><cell>1.2</cell><cell>1.8</cell><cell>1.2</cell><cell>1.1</cell><cell>1.5</cell><cell>1.1</cell><cell>0.4</cell><cell>0.5</cell><cell>0.9</cell><cell>1.2</cell><cell>0.7</cell><cell>1.4</cell><cell></cell><cell>0.3</cell><cell>2.8</cell><cell>1</cell><cell>1.3</cell><cell>1.3</cell><cell>1.5</cell><cell>0.6</cell><cell>1.2</cell><cell>1.1</cell><cell>1.4</cell><cell>0.6</cell><cell>1.4</cell><cell>1.3</cell><cell>1.4</cell><cell>0.6</cell><cell>1.8</cell><cell>1.6</cell><cell>0.7</cell><cell>1</cell><cell>1.2</cell><cell>0.1</cell><cell>2.2</cell><cell>0.6</cell><cell>1.1</cell><cell>0.3</cell><cell>0.6</cell><cell>1.1</cell></row><row><cell>ha</cell><cell cols="2">13.2</cell><cell></cell><cell>4.9</cell><cell></cell><cell>9.2</cell><cell></cell><cell cols="2">6.1</cell><cell></cell><cell cols="2">4.6</cell><cell></cell><cell cols="5">7.2 14.3 11.2 10.1 12.5</cell><cell></cell><cell cols="2">8.8 10.8</cell><cell cols="9">11 13.6 11.2 11.6 17.7 11.9 11.7 10.8</cell><cell></cell><cell>0.2</cell><cell cols="2">9.8 15.9</cell><cell cols="2">0.5 13.7</cell><cell cols="2">7 -</cell><cell cols="5">11.3 13.5 10.8 11.3</cell><cell cols="2">7.2 15.1</cell><cell cols="6">9.5 10.7 11.6 10.6 11.5</cell><cell>5.6</cell><cell></cell><cell>6.4</cell><cell>8.4</cell><cell></cell><cell>4.6</cell><cell cols="3">7 11.1</cell><cell></cell><cell>1.3</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="4">4.7 10.4 11.2 13.1</cell><cell>9.3</cell><cell>4.8</cell><cell cols="2">7.6 14.4</cell><cell>4.1</cell><cell cols="3">7.8 10.8 11.5</cell><cell cols="2">2.9 11.9</cell><cell></cell><cell>0.5</cell><cell cols="2">5.4 10.2</cell><cell cols="4">6.2 15.3 13.8 11.5</cell><cell>6.6 11.6 11.2</cell><cell>3.9 11.1 13.2 14.1</cell><cell>7.4</cell><cell>9.1 13.8 10.6</cell><cell>11</cell><cell>7.5</cell><cell>1.3 15.3</cell><cell>0.7</cell><cell>7.9</cell><cell>2.6</cell><cell>8.4</cell><cell>8.8</cell></row><row><cell>he</cell><cell cols="2">28.5</cell><cell></cell><cell cols="6">9.2 23.5 18.9</cell><cell></cell><cell cols="2">6.7</cell><cell></cell><cell cols="5">2.5 34.5 22.4 24.9 26.9</cell><cell></cell><cell cols="5">6.8 29.3 17.9 32.7</cell><cell cols="6">30 27.5 40.1 24.1 27.3 24.6</cell><cell></cell><cell cols="3">0.2 24.7 38.4</cell><cell cols="2">0.3 28.9</cell><cell>3.6</cell><cell cols="2">9.1 -</cell><cell cols="3">27.4 26.1</cell><cell>26</cell><cell cols="3">9.3 31.2 12.7</cell><cell cols="5">21 26.2 22.8 16.1</cell><cell>3.5</cell><cell></cell><cell cols="2">1.9 13.1</cell><cell></cell><cell cols="4">2.8 18.9 19.5</cell><cell></cell><cell>0.7</cell><cell></cell><cell cols="3">0.4 11.4</cell><cell>27</cell><cell cols="4">29 31.5 11.9 10.6</cell><cell cols="2">9.1 27.7</cell><cell>3.8</cell><cell cols="3">0.5 24.9 25.5</cell><cell cols="2">2.1 23.6</cell><cell></cell><cell cols="7">0.4 11.1 22.1 11.9 36.1 32.5 27.4 10.8 29.4</cell><cell>27</cell><cell>3.7 26.9 31.8</cell><cell>23</cell><cell>2.5 19.1</cell><cell>9.2 24.4 25.3 14.2</cell><cell>1.3 32.2</cell><cell>0.7 11.2</cell><cell>1.8 17.8 11.9</cell></row><row><cell>hi</cell><cell cols="9">24.9 11.2 20.3 15.3</cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="5">6.7 29.8 25.5 21.3 22.9</cell><cell></cell><cell cols="11">8 24.2 17.2 27.9 25.4 23.2 37.1 20.9 23.4 22.7</cell><cell></cell><cell cols="3">0.1 20.4 32.4</cell><cell>0.3</cell><cell>25</cell><cell>7.2</cell><cell cols="3">9.4 22.3 -</cell><cell cols="3">21.7 22.9</cell><cell cols="8">9.5 27.3 12.3 18.7 22.5 22.4 14.5</cell><cell>5.2</cell><cell></cell><cell>5</cell><cell>13</cell><cell></cell><cell cols="4">4.4 17.5 16.9</cell><cell></cell><cell>0.7</cell><cell></cell><cell cols="8">0.5 11.4 23.4 25.1 26.8 18.4 10.6</cell><cell cols="2">18 25.7</cell><cell>5.2</cell><cell cols="3">2.4 21.5 21.9</cell><cell cols="2">2.5 22.1</cell><cell></cell><cell cols="4">0.4 17.3 19.1 12.6</cell><cell cols="3">31 27.9 22.9 13.4 24.8 23.5</cell><cell>3.6 22.8 26.9 21.8</cell><cell>5.8 18.2 12.2 22.7</cell><cell>22 18.8</cell><cell>1.4 29.8</cell><cell>0.7</cell><cell>11</cell><cell>1.8 16.8 11.5</cell></row><row><cell>hr</cell><cell></cell><cell cols="8">28 10.4 22.4 19.3</cell><cell></cell><cell cols="2">9.5</cell><cell></cell><cell cols="2">4.6 35.7</cell><cell cols="15">23 31.6 27.2 10.9 31.6 17.7 33.4 31.6 27.7 38.3 25.8 28.8 23.3</cell><cell></cell><cell cols="3">0.2 25.6 37.8</cell><cell cols="2">0.3 30.1</cell><cell>6.8</cell><cell cols="4">9.6 26.6 26.7 -</cell><cell></cell><cell>27.8</cell><cell cols="4">8.7 31.4 12.8 22.1</cell><cell cols="4">28 23.8 16.2</cell><cell>4.2</cell><cell></cell><cell cols="2">8.7 13.5</cell><cell></cell><cell cols="4">3.8 18.8 21.4</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="10">0.4 12.3 29.3 14.2 34.6 16.9 10.8 14.3 27.5</cell><cell>5.1</cell><cell cols="3">1 27.6 26.2</cell><cell cols="2">3.5 25.1</cell><cell></cell><cell cols="7">0.4 13.4 24.2 11.5 35.4 32.6 28.9</cell><cell>11 32.3 31.9</cell><cell>4.2 26.2 32.8 25.6</cell><cell>2.7 19.5 20.2 24.8 27.8</cell><cell>15</cell><cell>1.8 32.5</cell><cell>0.8 11.8</cell><cell>2.3 19.4 12.3</cell></row><row><cell>hu</cell><cell cols="2">26.5</cell><cell></cell><cell cols="6">9.1 21.5 18.9</cell><cell></cell><cell cols="2">8.8</cell><cell></cell><cell cols="17">4.4 32.5 22.4 24.8 25.7 10.1 29.5 16.5 31.3 30.9 26.1 36.6 24.9 26.1 22.5</cell><cell></cell><cell cols="3">0.1 24.7 36.3</cell><cell cols="2">0.3 28.1</cell><cell>6.2</cell><cell cols="5">9 24.7 25.1 26.2 -</cell><cell></cell><cell cols="8">8 29.4 12.4 21.1 27.3 23.7 15.6</cell><cell>3.9</cell><cell></cell><cell cols="2">5.2 12.8</cell><cell></cell><cell cols="4">4.1 19.2 20.7</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="3">0.5 10.8</cell><cell cols="7">27 26.8 29.8 16.5 10.5 13.6 26.4</cell><cell>5</cell><cell cols="3">0.9 26.1 24.6</cell><cell>2.7</cell><cell>23</cell><cell></cell><cell cols="7">0.4 12.1 22.5 11.7 34.3 30.8 26.9</cell><cell>10 29.4 27.7</cell><cell>3.9 25.1</cell><cell>31 21.5</cell><cell>2.8 19.1 16.3 24.3 24.3</cell><cell>14</cell><cell>1.8 31.2</cell><cell>0.7 10.9</cell><cell>2.2 19.4 11.6</cell></row><row><cell>hy</cell><cell cols="2">16.5</cell><cell></cell><cell>5.9</cell><cell></cell><cell>8.2</cell><cell></cell><cell cols="2">5.4</cell><cell></cell><cell cols="2">6.1</cell><cell></cell><cell cols="5">7.5 21.6 14.8 13.5 16.8</cell><cell></cell><cell cols="11">2.2 16.5 12.2 17.9 17.4 16.9 22.2 15.2 17.6 14.3</cell><cell></cell><cell cols="3">0 14.3 23.4</cell><cell cols="2">0.3 18.3</cell><cell>5.6</cell><cell cols="7">5.3 14.1 18.6 14.8 15.5 -</cell><cell>16.7</cell><cell cols="5">10 12.9 16.2 12.4</cell><cell>8.4</cell><cell>6.2</cell><cell></cell><cell>5.6</cell><cell>7.3</cell><cell></cell><cell cols="4">2.6 10.1 11.7</cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.4</cell><cell></cell><cell cols="4">6.7 17.6 19.2 19.7</cell><cell>9.1</cell><cell>8.5</cell><cell cols="2">9.4 14.6</cell><cell>0.5</cell><cell>4.4</cell><cell cols="2">14 13.4</cell><cell cols="2">1.4 15.8</cell><cell></cell><cell>0.3</cell><cell cols="2">9.2 13.3</cell><cell cols="4">9.5 21.7 19.8 16.7</cell><cell>8.6 17.5 16.9</cell><cell>1.6 16.3 18.1 16.1</cell><cell>4.2 12.1 10.9 14.1</cell><cell>17 10.8</cell><cell>0.6 21.4</cell><cell>0.5</cell><cell>8.6</cell><cell>1</cell><cell>9.8</cell><cell>9.2</cell></row><row><cell>id</cell><cell cols="9">29.1 10.5 22.8 19.7</cell><cell></cell><cell cols="2">5.7</cell><cell></cell><cell cols="8">4.2 33.7 23.9 25.3 27.3 12.2 28.4</cell><cell cols="3">20 33.3</cell><cell cols="5">30 26.6 42.7 24.5 27.4</cell><cell>24</cell><cell></cell><cell cols="3">0.3 24.3 37.3</cell><cell cols="2">0.3 29.1</cell><cell>3.8</cell><cell cols="6">11 26.2 28.1 26.8 26.7</cell><cell cols="2">8.6 -</cell><cell cols="6">14 22.2 25.9 24.7 21.8</cell><cell>4.3</cell><cell></cell><cell cols="2">3.3 14.4</cell><cell></cell><cell cols="4">2.9 19.8 19.9</cell><cell></cell><cell>1.3</cell><cell></cell><cell cols="3">0.8 13.1</cell><cell cols="5">27 29.2 31.4 12.8 11.1</cell><cell cols="2">7.8 33.2</cell><cell>5.2</cell><cell cols="3">0.6 26.4 26.3</cell><cell>3.5</cell><cell>25</cell><cell></cell><cell cols="7">0.4 12.9 21.8 12.6 36.4 32.5 26.4</cell><cell>11 29.2 27.3</cell><cell>5.1 25.7 32.5 25.3</cell><cell>3.1 20.9</cell><cell>13 25.4 25.5 15.3</cell><cell>1.9 35.8</cell><cell>1.1 12.8</cell><cell>2.7</cell><cell>20 13.3</cell></row><row><cell>ig</cell><cell cols="2">11.8</cell><cell></cell><cell>4.7</cell><cell></cell><cell>8.6</cell><cell></cell><cell cols="2">6.3</cell><cell></cell><cell cols="2">4.9</cell><cell></cell><cell cols="3">6.6 13.3 10.4</cell><cell cols="2">9.2 11.2</cell><cell></cell><cell cols="2">8.8 10.5</cell><cell cols="6">9.8 12.6 10.7 10.9 16.3</cell><cell cols="2">11 10.1</cell><cell>9.8</cell><cell></cell><cell>0.2</cell><cell>8.6</cell><cell>15</cell><cell cols="2">0.7 12.2</cell><cell>7</cell><cell>5.3</cell><cell></cell><cell cols="2">10 12.3</cell><cell cols="2">9.5 10.3</cell><cell cols="3">6.6 13.4 -</cell><cell cols="2">10 11.1</cell><cell></cell><cell cols="2">11 10.3</cell><cell>5</cell><cell></cell><cell>5.7</cell><cell>8.3</cell><cell></cell><cell>3.9</cell><cell>7.2</cell><cell></cell><cell>9.5</cell><cell></cell><cell>1.2</cell><cell></cell><cell>0.6</cell><cell></cell><cell>5.3</cell><cell>9.4</cell><cell cols="2">9.8 11.7</cell><cell>8.6</cell><cell>4.3</cell><cell cols="2">7.2 12.6</cell><cell>3.7</cell><cell cols="3">6.8 10.2 10.1</cell><cell cols="2">3.2 11.7</cell><cell></cell><cell>0.6</cell><cell>5</cell><cell>8.8</cell><cell cols="4">5.4 13.8 12.4 10.3</cell><cell>6.2 10.4 10.1</cell><cell>3.5 10.4</cell><cell>12 12.6</cell><cell>6.9</cell><cell>8.8 12.3 10.1</cell><cell>9.8</cell><cell>6.9</cell><cell>1.3</cell><cell>14</cell><cell>0.7</cell><cell>7.2</cell><cell>2.5</cell><cell>8.4</cell><cell>7.4</cell></row><row><cell>is</cell><cell cols="2">24.1</cell><cell></cell><cell>7.4</cell><cell></cell><cell cols="4">18 15.3</cell><cell></cell><cell cols="2">6.8</cell><cell></cell><cell cols="2">6.5 27.3</cell><cell cols="10">19 20.5 22.7 10.1 23.4 16.2 29.2 25.2</cell><cell cols="5">22 31.7 20.9 23.4 18.9</cell><cell></cell><cell cols="2">0.1 21.4</cell><cell>31</cell><cell cols="2">0.3 24.3</cell><cell>5.1</cell><cell cols="6">9.1 20.4 22.2 22.1 21.7</cell><cell cols="4">7.2 26.2 12.7 -</cell><cell cols="4">22.5 20.7 13.9</cell><cell>3.3</cell><cell></cell><cell cols="2">6.5 11.9</cell><cell></cell><cell cols="4">3 16.1 17.6</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="3">0.7 11.3</cell><cell cols="4">23 20.6 25.1 14.2</cell><cell cols="3">9 12.1 23.3</cell><cell>4.1</cell><cell>1.2</cell><cell cols="2">22 22.5</cell><cell>3.2</cell><cell>20</cell><cell></cell><cell cols="4">0.4 10.1 18.5 10.2</cell><cell cols="3">29 26.7 21.6</cell><cell>9.6 24.4 23.3</cell><cell>3.8 21.2 27.4 22.2</cell><cell>3.2 16.4 17.8 20.8</cell><cell>21 12.5</cell><cell>1.3 28.1</cell><cell>0.8 10.9</cell><cell>2.4 15.4 11.3</cell></row><row><cell>it</cell><cell cols="6">25.3 10.2 21.7</cell><cell></cell><cell cols="2">19</cell><cell></cell><cell cols="2">8.6</cell><cell></cell><cell cols="13">5.8 33.4 22.2 23.9 26.9 12.7 27.3 16.5 31.2 30.1 27.2</cell><cell>35</cell><cell cols="3">27 26.1 22.5</cell><cell></cell><cell cols="3">0.1 24.2 37.8</cell><cell cols="2">0.3 30.3</cell><cell>6.1</cell><cell cols="6">8.9 24.7 25.8 25.1 26.9</cell><cell cols="5">7.8 29.2 12.5 20.9 -</cell><cell cols="3">23.7 15.4</cell><cell>2.6</cell><cell></cell><cell cols="2">8.2 12.7</cell><cell></cell><cell cols="4">2.9 18.4 20.9</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="4">1.3 11.8 26.4</cell><cell cols="4">14 30.2 17.2 10.1</cell><cell cols="2">14 25.5</cell><cell>5</cell><cell cols="3">0.6 25.7 24.9</cell><cell cols="2">3.3 23.9</cell><cell></cell><cell>0.4</cell><cell cols="4">12 22.9 10.7 34.4</cell><cell cols="2">32 26.3 10.4 28.2 27.2</cell><cell>3.7 24.9 30.4 24.1</cell><cell>2.5 18.5 19.6 23.8 24.5 14.2</cell><cell>1.5 31.2</cell><cell>0.7 11.1</cell><cell>2.4 18.2 11.2</cell></row><row><cell>ja</cell><cell cols="2">18.9</cell><cell></cell><cell cols="6">8.3 17.3 12.8</cell><cell></cell><cell cols="2">7.3</cell><cell></cell><cell cols="2">4.5 24.2</cell><cell cols="3">20 17.1 19.1</cell><cell></cell><cell>7.8</cell><cell cols="10">21 12.6 22.6 21.4 21.1 26.1 19.4 20.9 18.2</cell><cell></cell><cell cols="3">0.1 18.7 26.6</cell><cell cols="2">0.5 20.6</cell><cell>5.6</cell><cell cols="6">6.6 19.1 22.5 18.6 20.2</cell><cell>6.8</cell><cell cols="5">23 10.1 16.6 20.3 -</cell><cell></cell><cell>12</cell><cell>3.5</cell><cell></cell><cell cols="2">6.1 11.1</cell><cell></cell><cell cols="4">3 20.2 14.6</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="7">0.5 10.5 21.3 13.7 22.5 14.7</cell><cell cols="3">9.3 12.8 20.6</cell><cell>3.6</cell><cell>1.9</cell><cell cols="2">20 17.8</cell><cell cols="2">2 17.5</cell><cell></cell><cell cols="3">0.5 11.7 17.6</cell><cell cols="4">9.3 24.7 23.1 20.1</cell><cell>9.4 20.7 19.6</cell><cell>2.3</cell><cell>18 21.7 19.3</cell><cell>3 17.2 15.2 18.6 17.5 12.9</cell><cell>1.1 25.7</cell><cell>0.6</cell><cell>8.4</cell><cell>1.5 17.5</cell><cell>8.8</cell></row><row><cell>jv</cell><cell cols="2">20.2</cell><cell></cell><cell cols="6">7.8 12.4 11.5</cell><cell></cell><cell cols="2">7.2</cell><cell></cell><cell cols="17">9.6 21.9 17.3 16.3 17.9 14.8 18.8 15.3 19.9 18.5 18.2 28.7 17.3 17.3 15.6</cell><cell></cell><cell cols="3">0.3 15.6 24.3</cell><cell cols="2">0.3 20.5</cell><cell>8.3</cell><cell cols="6">8.5 15.8 19.3 16.2 17.6</cell><cell cols="5">7.2 27.5 11.4 15.4 17.1</cell><cell></cell><cell>16 -</cell><cell></cell><cell>6.1</cell><cell></cell><cell cols="2">8.7 11.7</cell><cell></cell><cell>4.5</cell><cell cols="3">12 15.4</cell><cell></cell><cell>1.4</cell><cell></cell><cell>0.7</cell><cell></cell><cell>9.9</cell><cell cols="4">17 18.1 20.8 12.9</cell><cell cols="3">8.2 11.2 23.6</cell><cell>5</cell><cell cols="3">4.5 16.5 16.7</cell><cell cols="2">3.8 19.7</cell><cell></cell><cell>0.5</cell><cell cols="2">8.4 14.7</cell><cell cols="2">9.5 24.4</cell><cell>22</cell><cell>17</cell><cell>9.1 18.9 17.3</cell><cell>5.3 17.8 20.5 20.8</cell><cell>7.1 14.9 18.3 16.5 17.1 10.1</cell><cell>1.8 24.4</cell><cell>1 10.1</cell><cell>2.7 12.6 10.8</cell></row><row><cell>ka</cell><cell cols="2">12.3</cell><cell></cell><cell>5.2</cell><cell></cell><cell>5.9</cell><cell></cell><cell cols="2">4.3</cell><cell></cell><cell cols="2">3.3</cell><cell></cell><cell cols="3">6 16.2 13.4</cell><cell cols="2">9.5 13.1</cell><cell></cell><cell cols="7">2.2 12.5 10.3 13.4 12.8 13.9</cell><cell cols="4">15 13.2 14.6 11.8</cell><cell></cell><cell cols="2">0 11.4</cell><cell>18</cell><cell cols="2">0.3 14.6</cell><cell>5.1</cell><cell cols="6">3.8 12.1 15.1 11.6 13.3</cell><cell cols="2">5.7 12.1</cell><cell>8.8</cell><cell cols="2">9.8 13.7</cell><cell></cell><cell>9.5</cell><cell cols="2">7.1 -</cell><cell></cell><cell>1.6</cell><cell>6.7</cell><cell></cell><cell>2.4</cell><cell>7.5</cell><cell></cell><cell>11</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="4">6.3 14.4 15.6 16.2</cell><cell>7.9</cell><cell>7</cell><cell cols="2">7.8 10.7</cell><cell>0.4</cell><cell cols="3">3.1 11.1 10.9</cell><cell cols="2">1.7 13.6</cell><cell></cell><cell>0.2</cell><cell cols="2">7.7 10.7</cell><cell cols="4">7.9 16.6 15.3 10.3</cell><cell>7.3 13.3 12.9</cell><cell>1.5 12.7 13.3 11.4</cell><cell>3.4 10.7</cell><cell>8</cell><cell>9.3 13.1</cell><cell>8.9</cell><cell>0.6 16.2</cell><cell>0.5</cell><cell>6.9</cell><cell>1</cell><cell>7.7</cell><cell>7.3</cell></row><row><cell>kk</cell><cell cols="2">11.9</cell><cell></cell><cell>6.1</cell><cell></cell><cell>3.5</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell cols="2">4.6</cell><cell></cell><cell cols="3">6.1 14.5 11.3</cell><cell cols="2">9.9 12.1</cell><cell></cell><cell>5.1</cell><cell cols="6">9.9 10.5 11.4 11.3 11.3</cell><cell cols="3">2.6 11.8 13.3</cell><cell>9.8</cell><cell></cell><cell cols="2">0.2 10.6</cell><cell>16</cell><cell cols="2">0.3 13.5</cell><cell>5.3</cell><cell>4.6</cell><cell cols="5">7.8 13.7 10.5 11.9</cell><cell>6.3</cell><cell>9</cell><cell>8</cell><cell cols="2">10 12.5</cell><cell></cell><cell>7.6</cell><cell>8.2</cell><cell cols="2">3.9 -</cell><cell></cell><cell>7.9</cell><cell></cell><cell>2.8</cell><cell cols="3">5.8 10.3</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="4">5.4 14.2 13.5 15.7</cell><cell>7.3</cell><cell>8.7</cell><cell>7.7</cell><cell>9</cell><cell>1.7</cell><cell>5.4</cell><cell>7.9</cell><cell>9.7</cell><cell>1.9</cell><cell>11</cell><cell></cell><cell>0.5</cell><cell>7.5</cell><cell>9.7</cell><cell cols="3">7.8 15.6 13.7</cell><cell>9.9</cell><cell>5.5</cell><cell>12 12.1</cell><cell>2.4 11.8</cell><cell>11 10.5</cell><cell>3.5 10.9</cell><cell>8.4</cell><cell>6.6 13.8</cell><cell>7.8</cell><cell>1.1 15.7</cell><cell>0.7</cell><cell>7.1</cell><cell>1.4</cell><cell>8.3</cell><cell>7.9</cell></row><row><cell>km</cell><cell cols="2">15.8 af</cell><cell cols="2">6.4 am</cell><cell>ar</cell><cell>5.3</cell><cell cols="2">ast</cell><cell>7</cell><cell cols="2">az</cell><cell>4</cell><cell cols="6">7.4 18.5 14.4 12.9 16.2 be bg bn bs ca</cell><cell cols="12">6.8 14.1 12.7 18.2 14.6 15.7 17.5 14.6 15.8 13.3 ceb cs cy da de el en es et fa</cell><cell>ff</cell><cell cols="3">0.3 13.4 21.6 fi fr</cell><cell cols="2">0.3 16.7 ga gl</cell><cell>6.3 gu</cell><cell>7.2 ha</cell><cell>he</cell><cell cols="4">12 16.2 14.2 15.5 hi hr hu</cell><cell cols="5">6.5 15.7 11.1 13.2 15.5 hy id ig is it</cell><cell>ja</cell><cell cols="2">9.6 10.7 jv</cell><cell>4.8 ka</cell><cell cols="3">4.7 -kk km</cell><cell cols="2">3.1 kn</cell><cell cols="3">7.1 12.3 ko lb</cell><cell>lg</cell><cell>1</cell><cell>ln</cell><cell>0.5</cell><cell>lo</cell><cell cols="3">9.6 15.6 15.8 lt lv</cell><cell>18 mk</cell><cell>8.6 ml</cell><cell>7.4 mn</cell><cell cols="2">7.8 15.2 mr ms</cell><cell>2.1 my</cell><cell cols="3">6.3 12.4 14.2 ne nl no</cell><cell cols="2">2.7 14.2 ns oc</cell><cell>or</cell><cell>0.4</cell><cell cols="2">9.4 12.8 pa pl</cell><cell cols="4">8.5 19.9 18.7 13.1 ps pt ro ru</cell><cell>7.3 15.5 15.7 sd sk sl</cell><cell>3.3 14.5 16.1 16.7 so sr sv sw</cell><cell>ta</cell><cell>4.1 14.6 15.5 th tl</cell><cell>tr</cell><cell>9.5 15.2 uk</cell><cell>ur</cell><cell>9.4</cell><cell>0.8 22.1 uz vi</cell><cell>0.7 wo</cell><cell>9.6 xh</cell><cell>1.8 10.4 10.2 yo zh zu</cell></row><row><cell>kn af</cell><cell>-</cell><cell>1.8</cell><cell></cell><cell cols="6">0.9 22.5 28.1 20.7 0 0.4</cell><cell></cell><cell cols="10">0.5 16 17.5 38.9 29.3 31.5 30.8 25.5 0.4 1.7 1.4 1 1.4 0.6</cell><cell cols="10">0.8 33 44.1 41.5 36.7 29.7 60.5 27.2 30.8 27.6 1.9 1.4 1.2 0.9 0.8 1.2 1.2 0.7</cell><cell></cell><cell cols="2">0 0.2 27.5 1.1</cell><cell cols="5">1.9 44 33.7 34.2 20.3 15.3 0.3 0.5 0.5 0.6</cell><cell cols="7">0.5 34 33.4 31.8 29.6 29.4 38.3 1.6 1.2 1.3 0.2 0.7</cell><cell cols="9">1.9 16 25.2 29.7 27.5 16.6 25.5 15.7 1.5 1.5 0.8 1 0.3 0.6</cell><cell cols="2">0.7 -9.7</cell><cell cols="2">0.6 14 21.6</cell><cell></cell><cell cols="3">1.6 11 10.7 0.2</cell><cell></cell><cell>0.1 18</cell><cell></cell><cell cols="5">0.7 8.1 30.1 12.2 36.3 28.1 1.5 1 1.6 1.1</cell><cell>1.3 17</cell><cell cols="2">1.1 23 36.7 0.9</cell><cell>0.4 9.6</cell><cell cols="3">0.2 3.5 29.7 32.2 0.7 1.1</cell><cell cols="4">0.6 3.9 31.1 10.5 1.3 0.2</cell><cell cols="6">2.1 22 25.1 11.8 43.4 37.4 30.5 10.7 33.6 31.2 1.1 1.4 1.5 1.3 0.3 0.9 0.9 1.2</cell><cell>0.5 6.5 34.9 39.2 31.4 16.4 22.7 28.1 31.8 31.5 19.5 25.6 36.5 1.2 1.2 1 0.5 1.7 1.1 0.6 0.8 1 0.1 1.5</cell><cell>0.6 2.8 21.5 1.1</cell><cell>0.3 2.6 24.3 17.4 0.7 1.2</cell></row><row><cell>ko am</cell><cell cols="3">19.8 23.6 -</cell><cell cols="9">8.6 17.8 12.8 20.2 12.4 12.4 8.2</cell><cell></cell><cell cols="17">5.6 25.6 20.5 18.2 20.2 9.7 27 23.9 20.2 21.8 19.2 6.9 21.4 13.2 23.8 22.5 21.5 27.9 19.1 21.7 19.4 23 27.8 26.7 23.7 21.7 32.5 19.9 21.9 22.3</cell><cell></cell><cell cols="27">0.1 19.5 27.1 0.5 18.7 29 22.5 23.2 16.5 12.5 22.5 25.5 21.4 21.7 22.5 25.9 14.5 18.3 20.8 21.7 12.8 19.7 11.9 0.3 21.5 5.9 7.7 19.3 23.2 19.6 21.2 7.2 24.9 10.9 17.4 20.4 25.3 12.5 4.2 6 11.2 8.6 13.3 17.3 3.8 -</cell><cell></cell><cell>15 6.2</cell><cell></cell><cell cols="3">0.7 9 15.4 0.7</cell><cell></cell><cell cols="8">8.9 21.9 11.5 23.5 15.5 7.8 21.7 16.2 25.7 22.9 13.1 19.1 9.8 13.2 21.8 24</cell><cell>3 9.3</cell><cell cols="3">2.2 9.8 20.8 21.5 20 18.2</cell><cell cols="2">2.1 17.9 2.5 19.9</cell><cell></cell><cell cols="3">0.5 9 18.1 17.6 11 17.3</cell><cell cols="4">10 25.8 23.8 20.2 9.5 28.8 25.7 20.8</cell><cell>9.7 22.1 21.2 7.6 23.3 22.2</cell><cell>2.2 18.9 23.2 19.9 5 23.8 25.4 24.6 21.3 16.9 21.8 22.8 21.5 17.6 20.5 3.6 17.3 14.2 20.5 19 12.9 1.1 27.1 27</cell><cell>0.6 2.4 16.3 8.9</cell><cell>1.7 17.2 1.9 18.7 13.9 9.5</cell></row><row><cell>lb ar</cell><cell cols="30">27.4 29.5 22.6 -8.7 14.9 15.4 14.3 15.3 11.1 34.9 8 12.8 29.3 19.3 21.8 24.2 15.7 24.5 16.5 27 27.5 27.6 22.3 29.9 35.9 34.4 31.2 28.3 44.2 25.5 31 33.9 23 37.3 21.9 24.7 18.1 28 28.5</cell><cell></cell><cell cols="28">0.2 21.2 33.3 0.8 25.1 38.3 29.7 0.3 27.8 30 17.1 13.2 30.8 29.9 28.1 27.3 26.2 33.3 14.1 24.6 27.9 26.8 14.9 24.4 10.8 6.7 8.5 19.8 21.4 21.9 23.4 7.5 24.9 11.7 19.7 24.4 17.4 16 5 9.8 12.1 8.7 13.5 21.5 3.9 14 -</cell><cell>6.9</cell><cell></cell><cell cols="10">1.2 9.5 16.8 0.7 11.1 22.7 13.9 8.4 28.6 13.6 14.4 28 14.2 9.6 14.9 8.7</cell><cell cols="3">12 21.8 6.3 30.6 10.5 5</cell><cell cols="3">4.6 24.3 24.6 0.1 26.1 26.2</cell><cell cols="2">3.2 24.6 2.6 26.6</cell><cell></cell><cell cols="7">0.4 10.5 18.9 10.3 31.7 28.9 9.8 20.7 22.6 9.3 36.6 33.1 27.9 24</cell><cell>9.9 27.4 8.5 30.6 28.8 24</cell><cell>4.4 5.5 29.5 25 29.7 22.6 33 24.8</cell><cell>5.7 17.2 19.9 18.1 22.9 11.8 4.3 21.5 10.2 27.5 27.9 14.9</cell><cell>1.7 26.7 23 33.5</cell><cell>0.8 10.6 2.6 18.3</cell><cell>2.7 15.3 11.2 2 23.3 16.6</cell></row><row><cell>lg ast</cell><cell cols="2">3.7 28.9</cell><cell></cell><cell cols="4">1.9 6.7 24.3 -2.1</cell><cell cols="2">2.1</cell><cell></cell><cell cols="10">2 13.6 14.9 32.9 25.4 24.6 27.7 18.6 2.7 5.4 4.4 3.3 4.6 3.5</cell><cell cols="9">4.2 28 31.1 32.8 30.3 27.2 39.8 24.4 27.3 3.7 4.4 4.4 4.5 5.3 4.7 3.9</cell><cell>3.1 25</cell><cell></cell><cell cols="3">0.4 0.4 23.7 37.8 3.1 5.9</cell><cell cols="11">0.4 23 31.9 16.3 12.1 27.9 27.1 26.3 26.2 24.4 30.8 4.2 2.5 2.2 3.7 4.3 3.4 4.2 1.6 4.5</cell><cell cols="2">2.9 12 22.5 3.7</cell><cell cols="7">4.8 28 25.5 13.2 22.1 12.6 5.6 3.1 1.5 2.9</cell><cell>2.3 7.5</cell><cell></cell><cell cols="2">1.4 12 19.8 3.2</cell><cell></cell><cell cols="2">3.3 -8.5</cell><cell cols="3">0.7 8.7 16.6</cell><cell></cell><cell cols="4">1.7 8.4 27.2 11.8 31.7 3.5 3.1 4.9</cell><cell cols="4">2.8 4.4 13.1 11.8 28.7 1.9 2.2 4.4</cell><cell>1.3 8.3</cell><cell cols="3">2.1 3.7 25.6 25.6 4.6 4.4</cell><cell cols="2">2.3 3.4 27.8 4.2</cell><cell></cell><cell cols="3">0.3 8.5 17.2 22.9 2 3.8</cell><cell cols="4">1.8 8.6 36.6 31.9 26.4 5.2 4.5 3.8</cell><cell>1.5 8.4 28.6 27.5 4.1 4</cell><cell>1.6 5.2 29.4 31.8 18.3 3.5 4.5 3.9</cell><cell>1.5 5.7 18.5 23.1 3.3 4.3</cell><cell>3.2 26 26.8 13.9 18.8 31.4 3.8 1.8 0.8 4.4</cell><cell>0.9 1.9 15.7 3.2</cell><cell>1.3 1.8 21.8 4</cell><cell>3.6 8.5</cell></row><row><cell>ln az</cell><cell cols="10">4.6 18.5 16.7 11.6 11.5 -2.2 3 2.5</cell><cell cols="2">2.3</cell><cell></cell><cell cols="9">3.6 4.8 24.1 19.9 17.5 19.1 15.8 18.9 23.5 6.3 5.1 4 5.8 3.8 5 4.7</cell><cell></cell><cell cols="5">5.4 22 20.3 20.6 20.7 18.6 5.3 6.5 6.7 6.2</cell><cell cols="2">5 20 19.6 3.6</cell><cell></cell><cell>0.4 0.2</cell><cell cols="5">4.7 18 26.1 19.6 19.4 13.3 7.6 0.3 5.6 2.8</cell><cell cols="5">2.9 9.3 18.5 21.1 18.3 5.2 5.6 4.5</cell><cell cols="2">5.9 20 12.2 2.5</cell><cell cols="6">6.2 20 10.9 14.8 20.1 14.2 3.4 5.2 6.1 5.9</cell><cell cols="2">3.9 7.1 21.5 2</cell><cell></cell><cell>3 6.5</cell><cell>3.3 5</cell><cell></cell><cell cols="2">1.7 9.5 15.1 3.9</cell><cell></cell><cell>4.2 5.8</cell><cell></cell><cell cols="6">1.3 -6.5 13.4 10.1 20.2 2.2 4.9</cell><cell cols="6">4.5 9.1 22.8 17.5 10.8 10.9 16.9 6.2 3.7 2.5 3.3 5.5</cell><cell>1.7 7.8</cell><cell cols="3">3.1 2.2 18.5 18.2 6 5.2</cell><cell cols="2">2.6 2.3 10.5 5.7</cell><cell></cell><cell cols="3">0.3 7 14.8 16.7 2.7 4.9</cell><cell>2.2 6.9</cell><cell cols="3">7.4 24 22.5 19.5 5.4 5.2</cell><cell>1.9 5.6 20.3 20.6 5 5.2</cell><cell>1.9 3.8 19.6 20.8 15.7 10.9 15.9 4.6 5.7 5.2 1.8 4.5</cell><cell>5 9.9 20.2 18.8 3.5 4.8</cell><cell>2.3 6.2 16.9 20.2 1.1 5.8</cell><cell>0.9 1.8 13.1 3.4</cell><cell>1.4 1.3 16.2 5.1</cell><cell>3.5 6.4</cell></row><row><cell>lo be</cell><cell cols="6">11.3 17 15.4 3.8 12.1 9.7</cell><cell></cell><cell cols="2">3.3 12</cell><cell></cell><cell cols="3">3.7 8.8 -</cell><cell cols="17">6.5 13.8 13.5 16.6 15.8 15.4 18.1 14.9 10.7 22.5 20.2 18.8 19.1 22.1 18.2 19.1 12.2 6.8 12.6 5.2 10.7 8.8 10.4 9.4 11.9 17.9 10.8 12.3 11.9</cell><cell></cell><cell cols="6">0 10.8 16.4 0.1 15.6 23 19.3 20.5 13.5 0.4 13.9 8.8</cell><cell cols="4">2.5 12.5 14.8 8.4 13.4 17.9</cell><cell cols="4">10 11.4 17 17.1 16.2 15.5 7.2 14.9</cell><cell cols="5">6.2 10 14.6 19.6 14.2 7.9 12.5 10.5</cell><cell cols="2">9.5 7.9 21.7 4.7</cell><cell></cell><cell cols="2">5.7 10.5 8.4 6.3</cell><cell></cell><cell cols="4">5.4 9.6 11.4 7.4 10.5 5.7</cell><cell></cell><cell cols="4">0.7 6.5 12.1 0.5 -</cell><cell cols="2">10.6 4.6 20.9</cell><cell cols="6">9 12.6 0.1 21.8 12.8 10.2 11.6 9.9 5.1 6.9 13.4 16</cell><cell>3.4 5.8</cell><cell cols="3">8.1 3.2 17.6 15.8 9 9.1</cell><cell cols="2">1.9 2.4 14.8 8.9</cell><cell></cell><cell cols="3">0.4 5.7 14.2 10.9 4.8 9.8</cell><cell cols="4">5.7 16.1 15.2 11.4 7.3 21 19.2 18</cell><cell>5.5 6 11.2 12.5 9.3 8.6</cell><cell>2.3 3.8 14.6 18.5 15.6 9.5 9.2 13</cell><cell>7.1 14.3 12.4 12.2 11.3 9.1 14.3 11.7 3.3 20 11.9 16.1 19.7 8.2 1 16.3</cell><cell>0.6 1.4 11.8 5.4</cell><cell>1.6 1.3 13.6 8.8</cell><cell>7.4 7.2</cell></row><row><cell>lt bg</cell><cell cols="12">25.4 31.1 24.3 27.6 17.1 16.8 8.7 21.2 17.3 8.8</cell><cell></cell><cell cols="17">7.5 32.2 20.5 24.5 24.4 4.3 -28.2 30.9 28.8 22.9 32.1 37.5 36.4 34.3 7.9 27.2 16 29.9 28.1 25.2 34.6 23.8 27.4 21.8 30 45 27 31 28.4</cell><cell></cell><cell cols="5">0.1 23.6 35.1 0.9 26.9 40 31.3 32.3 0.3 27.3</cell><cell cols="24">5.8 19 13.5 31.9 9 24.3 24.8 31 30.5 29.9 29.9 33.8 14.8 25.5 29.9 27.8 15.6 28.4 17.3 26 24.6 8.2 28.5 12.2 20.6 26.3 23 15.3 3.8 4.9 12.2 9.4 14.1 22.2 3.8 18.2 19.7 8.3</cell><cell></cell><cell cols="3">1 9.6 17.4 0.4</cell><cell></cell><cell cols="9">12 -7.4 30.6 17.1 37.5 27.9 16.9 21.3 30.7 11.8 27.9 29.4 15.2 10.6 13.1 24.9 4.6</cell><cell cols="3">0.6 24.9 23.2 3.6 28 28.2</cell><cell cols="10">2.7 21.6 3.1 27.9 11.5 22.1 25.8 0.3 12.5 23.3 11.3 32.5 29.7 27.5 10.2 28.2 27.2 11 38.1 34.9 31.4 9.7 32.4 30.9</cell><cell>3.8 25.2 28.9 5.7 34.3 35.7 27.5 14.7 21.5 25.2 29.5 31.5 19.8 26.1 34.1 21 3.3 18.6 15.1 23.1 25.6 13.7 1.8 30.6</cell><cell>0.7 10.6 2.9 19.3</cell><cell>2 17.9 11.4 2.4 24.8 16.9</cell></row><row><cell>lv bn</cell><cell cols="12">25 25.4 21.3 22.5 13.8 14.2 9.4 21.5 7.8 9.2</cell><cell></cell><cell cols="3">3.7 16.8 22.1 7.3 30.1 -</cell><cell cols="14">5.9 25.2 23 23.9 20.3 24.9 30.9 29.2 26.5 23.3 38.8 21.8 24.7 23.9 8.7 19.1 15.4 29.9 18.7 26.6 36.7 24.6 29 22.8</cell><cell></cell><cell cols="5">0 26.3 36.5 0.5 21.3 32.6 26.1 25.3 0.3 28.4</cell><cell cols="22">4.5 20 12.1 24.8 30.2 7.4 25 25.9 11.5 24 23.5 22.4 29.3 13.6 20.7 23.7 24.7 12.8 22.7 11.2 25 7.9 29.6 12.5 21.3 18.6 19.5 15.4 3.6 5.9 12.3 8.3 15.1 19.9 1.9 7.4</cell><cell></cell><cell>18 7.2</cell><cell></cell><cell cols="6">0.7 8.3 15.3 0.4 10.3 28.6 -7.1 23.9</cell><cell cols="6">29 15.8 10.9 13.6 25.8 9.2 28.2 27.5 13.9 22.8 27.3</cell><cell cols="4">2.9 9.4 13.8 23.4 22.8 0.4 12.1 23.7</cell><cell cols="10">2 21.6 2.5 20.8 11.4 21.7 19.6 0.4 12.7 21.2 11.6 33.8 14.8 15.9 10.5 13.6 11.4 9.7 30.9 28.5 23.8 9.6 25.9 24.2</cell><cell>3 11.4 17.9 22.9 4.9 25.9 28 23.3 22.8 18.7 2.5 18.9 17.8 24.3 18.3 14.8 21 24.4 23.5 18.2 21.2 29.4 1.5 18.3</cell><cell>0.6 10.4 2 17</cell><cell>1.8 2 20.6 13.7 11 11</cell></row><row><cell>mk bs</cell><cell cols="9">29.5 10.6 32.8 23.9 28.3 17.6 24 20.9</cell><cell></cell><cell cols="20">9.8 17 12.2 40.3 28.4 -8.5 39 23.7 30.4 28.5 11.1 30.3 18.9 34.8 31.8 29.6 42.1 25.9 29.1 29.8 24.2 35.1 38.3 38.8 35.4 31 47.9 28.1 32.4 29.4 25</cell><cell></cell><cell cols="12">0.1 25.8 39.1 0.4 29.1 42.3 32.3 33.7 18.6 14.4 33.3 31.8 35.5 30.9 0.3 32 6.7 9.7 28.5 28 30.7 27.6</cell><cell cols="17">9.8 33.3 13.1 30 36 15.3 26.3 23 28.1 24.8 17.1 31 28.9 17 28.1 4.4 10.2 13.4 16 9.8 13.4 23.3 4.2 20.2 22.5 9.1</cell><cell></cell><cell cols="8">0.6 10 18.1 0.5 11.9 29.5 23.1 -9.3 32.3 16.3 39.2</cell><cell cols="4">17.6 11.3 15.3 28.7 26 16.3 14.1 32.7</cell><cell>4.8 12</cell><cell cols="3">1.4 27.1 26.9 1.1 29.5 29.4</cell><cell cols="10">2.4 26.4 3.5 27.5 10.8 0.4 13.1 23.8 21 27.4 11.1 40.3 36.5 31.9 12 37.8 34.2 29.5</cell><cell>12 32.2 30.7 8.6 36.1 35</cell><cell>4 31.4 33.2 26.6 6.3 36.7 37 29.1 13.3 23.1 25.7 31.4 32.4 19.9 26.1 35.7 3.7 20.5 19.5 25.7 28.5 15.8 1.6 33.9</cell><cell>0.6 12.3 3 20</cell><cell>2.3 19.6 12.5 2.5 25.8 16.8</cell></row><row><cell>ml ca</cell><cell cols="18">19.5 30 19.1 25.7 20.8 14.7 16.7 35.7 26.1 28.1 -9.1 15.6 11.4 6.6 7.4 22.8 20.3 15.4 17.9</cell><cell></cell><cell cols="11">8.9 18.5 13.9 21.7 19.2 18.3 26.8 16.9 18.2 17.2 22.8 29.2 36.4 35 32.5 28.1 46.2 26.9 27.7 25.3</cell><cell></cell><cell cols="11">0.2 15.7 0.5 24.7 41.6 28.6 25 0.3 19.3 35 18.7 13.2 29.6 28.7 28.6 8.7 7.3 16.8 22.8 16.3</cell><cell cols="9">17 27 25.7 32.9 14.9 23.1 30.6 26.5 14.8 7.8 20.8 10.5 15.2 17.9 17.5 11.9</cell><cell cols="7">5.3 23 13.9 6 11.3 8.5 13.5 20.6 6.2 13.8</cell><cell></cell><cell>15 9</cell><cell></cell><cell cols="12">0.4 9.5 17.2 0.4 10.7 8.1 27.2 13.1 33.1 24.5 14.7 19.7 18 19.8 20.4 -9.3 14.3 18.6 30</cell><cell>5.1 8.8</cell><cell cols="3">1.9 17.3 16.8 1.8 26.2 26.9</cell><cell cols="10">2.2 17.9 4 31.9 10.3 20.4 23.4 10.2 39.7 34.9 28.2 0.4 12.9 14.2 10.1 23.2 20.9 17 10.7 18.8 18.3 9.2 29.7 28.7</cell><cell>3 17.8 20.5 5.9 31.2 34.3 26.1 11.1 19.8 25.3 28.2 28.3 17.2 22.4 32.4 19 7.3 14.3 13.7 17.5 16.3 13.7 1 23.2</cell><cell>0.6 2.7 18.2 8.9</cell><cell>1.7 12.8 2.3 23.5 14.2 9.8</cell></row><row><cell>mn ceb</cell><cell cols="19">16.5 28.7 20.2 21.7 16.2 12.9 8 14.3 7.8 7.9 10.5 19.8 17.1 13.9 15.3 13 31.7 23 23.4 25.8 -</cell><cell>8.7</cell><cell cols="10">16 12.2 17.9 15.9 16.3 20.9 14.9 16.2 16.4 26.6 32.4 31.6 28.4 25.4 41.6 23.2 25.6 23</cell><cell></cell><cell cols="29">0.1 0.4 22.8 35.6 25.2 27.8 17.9 13.7 25.6 25.6 25.3 24.7 23.9 32.5 14.4 21.3 25.9 14 20.8 0.4 17.5 11.7 4.5 15.8 18.6 15.1 15.8 10.9 19.2 8.7 13.8 15.9 17.3 11.4 22 12 22.3 11.9 9.3 10.5 11.3 8.2 12.2 19.6 7.6 12.4 13.9 7.9</cell><cell></cell><cell cols="3">0.5 9.4 16.8 0.4</cell><cell></cell><cell cols="8">6.3 15.7 17.8 18.6 14.8 -4.5 24.8 2.5 30 16.1 13.5 16.3 11.9 17.7 30</cell><cell cols="4">5.7 11.5 14.6 14.7 8 9.5 23.7 24.4</cell><cell cols="2">2 15.8 4.8 20.7</cell><cell></cell><cell cols="3">0.5 9.1 18.8 20.6 7.4 13.3</cell><cell cols="4">8 20.2 18.5 15.5 9.1 35 29.6 24.7</cell><cell>9.9 15.9 15.9 7.3 27.2 26.5</cell><cell>3.4 5.9 27.8 30.1 25.9 17 17.3 16.9 10.7 13.2 15.1 16.1 15.5 11.1 8.6 20 28.5 25.3 25 15 20.9 31.6 1.4 21.1</cell><cell>0.6 3 17.8 7.3</cell><cell>1.7 2.5 20.2 12.7 12 8.5</cell></row><row><cell>mr cs</cell><cell cols="12">18.4 30.6 22.7 26.3 16.9 15.8 8.4 14.2 9 6.9</cell><cell></cell><cell cols="17">5.9 21.3 20.8 15.1 17.1 3.1 36.9 27.8 30.8 28.2 23.1 -9.3 16.9 13.1 19.4 18.4 17.5 35.9 35.8 34.4 28.9 44.2 26.8 31.1 27.3 26 16 17 16.2</cell><cell></cell><cell cols="21">0.2 14.6 22.7 0.8 27.6 40.3 30.3 31.8 17.8 13.7 30.9 30.4 31.8 30.2 27.9 33.6 14.4 25.3 29.6 28.1 15.7 26.5 0.3 18.1 9 7.3 15.6 24.5 15.6 16.1 7.7 19.8 10.7 14.5 17.5 17.7 12 4.7</cell><cell></cell><cell cols="7">5.5 10.6 16 9.4 13.8 22.1 5.8 13.7 14.4 8.7</cell><cell></cell><cell cols="13">0.6 9.6 17.7 0.2 10.3 17.2 8.8 31.3 16.5 35.1 26.6 15.8 20.6 31.1 11.2 18 19.9 16.6 9 -18.6 4</cell><cell cols="3">2.7 1.2 27.5 27.6 16 15.4</cell><cell cols="10">2.2 16.5 3.7 26.4 10.8 20.4 26.7 10.9 38.1 34.9 30.4 0.5 14.6 14 10.3 22.6 20.5 16.5 11.5 17.6 16.8 9.4 36.2 32.5</cell><cell>3.2 16.5 19.9 17.5 6.1 32.6 35.1 27.4 11.8 21.4 25.9 29.5 30.7 19.4 25.3 6.8 13.8 12.9 15.6 15.9 14.3 1.2 22.8 34</cell><cell>0.6 3</cell><cell>9.2 19</cell><cell>1.9 12.6 2.4 24.8 16.7 9.5</cell></row><row><cell>ms cy</cell><cell cols="2">28.9 37.1</cell><cell></cell><cell cols="9">10 22.4 19.3 6.4 29.3 19.9 16.6 7.6</cell><cell></cell><cell cols="9">8.7 32.8 23.1 24.3 26.1 10.6 27.8 20.1 8.8 39.4 22.4 30.7 31.6 24.8 33.3 -</cell><cell></cell><cell cols="7">32 41.3 36.2 29.9 59.7 29 25.9 42.8 23.5 25.3 23.2 28 32.6 28.8</cell><cell></cell><cell cols="20">0.2 22.8 35.9 0.3 29.3 45.1 31.9 34.4 19.7 15.4 0.3 28.2 4.9 11 24.7 27.6 34 33.1 31.4 30.8 28.5 40.6 15.8 27.3 30.9 28.3 17.7 25 25.4 8.4 36.2 13.4 21.2 25.6 24.1 20</cell><cell cols="23">5.7 22 14.9 4.7 14.2 8.7 13.8 21.8 3.5 18.8 19.4 9.4 10.2 17.8 1.3 0.6 13.3 7.9 30.8 21.6 36.7 25 26.3 29.4 14.9 10.7 13.2 -2.8 16.3 7.7 37.6 10.2 3.9</cell><cell cols="3">1.2 24.9 25.5 0.8 28.7 31.9</cell><cell cols="2">3.8 23.6 4.4 31.9</cell><cell></cell><cell cols="7">0.4 11.7 20.8 12.2 35.5 31.4 25.8 9.6 21.9 25.1 10.3 43.5 38.6 30.3</cell><cell>11 27.7 25.7 9.5 34.3 32.3</cell><cell>4.2 6 35.5 40.1 33.9 25 31.5 27.2</cell><cell>3.9 19.9 14.8 24.4 24.3 14.9 9.9 21.1 31.5 32.8 31.6 17.2 25.9 37.4 1.8 34</cell><cell>0.9 3.6 20.1 13</cell><cell>2.5 18.1 13.6 2.8 26.1 17.9</cell></row><row><cell>my da</cell><cell cols="14">8.8 35.4 24.8 28.8 19.2 16.8 16.5 4.2 2.4 2.9 1.8 2.9</cell><cell cols="9">9.9 10.8 40 29.6 31.7 31.1 25.3 33.9 41.7 -6.2 11.1 2.8 6.9 8.9</cell><cell>9.7</cell><cell cols="6">7.6 10.5 38.1 30.3 52.7 28.5 33.1 28.8 8 9.3 9.7 8.1</cell><cell></cell><cell cols="5">0.1 0.8 30.9 44.4 34.2 34.4 8 13.6 0.3 11.2</cell><cell cols="18">4.3 20 14.8 33.9 33.1 32.2 31.3 29.7 37.8 15.3 28.6 31.5 29.5 17.2 26.4 16.5 4.1 8.6 11.7 7.3 9.3 2.7 7.2 8.3 8.1 10.3 6.4 5.9 2.4 1.8</cell><cell>7.8 9.7</cell><cell></cell><cell cols="8">2.2 14 23.2 10.1 10.3 18.8 4 7.8 0.4 0.4</cell><cell></cell><cell>5.6 9.3</cell><cell cols="8">9.8 31 17.5 36.2 29.5 16.9 23.4 34.5 11.8 8.2 12.1 5.6 6.1 6.4 8.7 -</cell><cell cols="3">4.8 4.1 30.8 32.9 7.2 7.3</cell><cell cols="2">1.4 4.3 29.7 9.4</cell><cell></cell><cell cols="7">0.3 12 22.4 26.1 12.1 42.4 37.8 30.7 10.3 34.8 32.5 8.2 7 7.1 12.7 11.1 5.8 4.5 8.6 9</cell><cell>1.4 6.7</cell><cell>7.8 35</cell><cell>8.3 43 29.7 12.7 23.1 28.3 33.4 32.4 20.6 26.8 36.1 9.7 2.5 9.5 10.6 4.6 8.9 6.2 0.4 14.4</cell><cell>0.6 3.5 20.8 6</cell><cell>1 2.7 25.6 19.1 5.9 7.2</cell></row><row><cell cols="88">ne nl no ns oc or pa de 2.6 25.9 ff 14.8 5.5 7.2 5.6 3.6 6 15.8 15.3 11.3 12.5 7.3 12.5 10.4 13.6 14.1 13.7 19 11.7 12.7 9.1 0.1 11.8 16.5 0.3 14 7.2 5.3 10.6 20.1 11.4 12.3 4.8 13.7 7.7 10.7 13.1 11.6 9.2 3.8 3.8 8.3 3.4 8.6 11.4 0.8 0.3 8.2 12.8 12.9 15.5 11.2 7.5 12.1 12.5 2.4 -11.7 10.9 1.9 13.1 0.5 11.8 10.7 8.2 16.6 15.3 11.8 7.9 13.6 12.6 2.3 12.7 14.6 13.1 5 11.7 11.8 9 12.2 10.1 1 16.2 0.5 7 1.5 8.3 24.9 9.4 19.5 17.5 8.3 9.7 30.5 20.2 22 24.3 10.6 25 15.9 30 27.9 24.2 33.7 23.3 24.4 19.9 0.1 23.1 33.2 0.3 26.6 6.1 8.9 22.5 23.7 23.5 24.6 7.7 26.9 11.9 20 25 21.9 14.5 3.7 7.2 12 3.7 17.3 19.2 1 1.2 11.2 24.7 14 27.7 16.2 9.5 13.3 23.7 4.7 1.1 -24.2 3.1 21.6 0.5 12 20.6 11 30.8 28 24 9.7 25.9 25 3.2 22.6 28.8 22.2 2.7 17.5 17.6 22.4 22.6 13.6 1.5 28.9 0.8 10.7 2.2 17.4 11.2 7.9 30.2 7.5 22.7 18.4 6.6 4 34.3 22.2 25.9 27.5 10.9 28.6 18.8 35.9 32 27.1 45.1 25.6 27.3 23.4 0.1 25.5 37.6 0.3 29.9 4.9 9.8 26.3 27.1 26 26.6 7.9 32.7 12.8 23.1 27.7 23.2 15.9 3.1 3.5 12.9 2.5 19.4 21.7 1.1 0.8 11.6 26.7 26.7 31.5 14.5 10.5 13.2 28.5 3.8 0.6 27.8 -3.2 24.9 0.4 11.3 22.5 11.8 37 32.8 27.2 10.4 30.2 27.4 4 25.6 36.5 25.9 2.6 19.1 19.1 25 25.2 14.6 1.4 33.1 0.7 11.8 2.4 18.9 12.2 5 2.5 2.6 2.6 2.5 3.8 7.5 5.9 4.7 6.4 5.5 5.5 5.7 6.7 5.8 7.5 8.8 6.6 5.5 4.2 0.5 5.4 8.2 0.3 6.1 3.6 3.6 5.6 6.4 5.2 5.9 2.8 7 4.2 5.2 6.9 6.6 4.9 2.1 3.5 3.5 2.2 3.7 5 1.8 0.9 2.7 6 5.2 6.8 3.8 2.8 3.6 6.9 2.1 2.8 6.5 6.3 -6 0.4 3 5.6 2.8 7.7 6.6 6 2.7 5.6 5.7 1.6 5.6 6 6 2.4 5.5 6.6 4.1 5.6 3 1.2 7 1.1 4.6 1.8 5 4.9 29 8.8 22.5 22.1 8.6 6.7 34.8 22 23.6 29 15 28.7 18.1 32.7 30.8 27.9 47.5 26.1 26.2 22.2 0.4 24.1 41.3 0.3 28.6 5.2 9.8 24.1 26.7 24.7 25.7 6.6 33.3 12.9 19.9 27.9 23.2 18.4 2.8 8.1 13 2.9 18.7 22.5 1.5 0.7 12.7 25.6 26.7 30.7 15.1 10.1 12.7 26.2 5.2 0.9 25.2 25.5 3.3 -0.4 12.7 21.8 11 40.1 33.2 26.4 10.2 29.3 27.4 4.4 26 32.9 24.8 2.2 19.2 21 24.2 25 13.4 1.7 32.2 0.8 11.7 2.6 17.7 12.2 0.6 0.2 0.2 0.6 0.5 0.4 0.9 0.6 0.5 0.7 0.3 0.3 1.1 0.7 0.6 0.3 0.5 0.7 0.5 0.2 0.1 0.5 0.8 0.3 0.3 0.5 0.5 0.1 0.6 0.4 0.5 0.2 0.4 1.5 0.4 0.6 0.3 0.4 0.3 0.2 0.4 0.3 0.2 0.9 0.3 0.5 0.4 0.7 0.6 0.6 0.3 0.3 0.4 0.5 0.2 0.3 0.4 0.5 0.8 1 -0.7 0.6 0.7 0.6 0.6 0.5 0.9 0.5 0.5 0.4 0.4 0.4 0.4 0.3 0.4 0.9 0.2 0.5 0.5 0.4 0.4 0.6 0.6 0.3 0.2 0.6 12.9 4.7 13.8 2.8 4.4 7.2 16.9 18.9 8.2 14.2 3.5 11.7 9.2 12.7 11.7 14.2 21.6 12.2 14.1 14.2 0 12.6 19.1 0.3 15.5 15.1 1.3 14 24.4 11.9 12.7 9.1 15.4 5.4 8.1 14 12 7.8 5.9 6 8.8 8.5 8.6 12.5 0.2 0.3 3.3 12.7 13.2 15.3 13.7 5.9 12 13.2 3.9 13.4 9.6 10.9 1.4 9.9 0.4 -10.3 7.8 19.1 17.1 13.1 9 11.2 10.7 1.7 12.5 11 13.6 9.3 12 14.6 13.6 13.8 13.1 0.8 15.1 0.5 5.2 1.1 9 6.5 33.4 23.8 28.1 19.3 16.8 15.2 39 28.8 30.7 30.7 24.3 34 39.1 40.3 -30.4 48 28.3 32.6 29 0.9 29.1 43.4 32.5 34.2 20 14.3 32.9 31.9 32.3 31.2 28.8 35.9 15.2 27.2 32.2 29.7 16.8 27.8 17.4 9.5 15.8 22.9 11.2 9.9 17.5 7.8 31.5 16.9 35.8 28.5 16.8 23.3 32.6 11.2 1.9 31 30.2 4 28.8 11.9 22.1 27.2 11.4 40.9 37 31.3 9.6 35.1 33.6 6.1 34.3 39.2 28.5 15.5 23.2 27.1 32 32 20.9 26.8 35.1 3.2 19.5 3.8 1.5 0.8 2 1.4 0.6 3.2 3.8 2 2.5 2.4 1.7 2.1 2.2 2.2 2.1 4.6 2.3 1.8 2.4 -1.6 2.6 2.3 2.4 3.2 1.9 1.7 2.9 2 1.8 3 2.4 1.8 2.3 2.2 2.4 1.5 3.1 1.3 1.5 2.7 1.4 1.4 1.4 2.2 1.3 1.7 1.1 2.5 2.8 1.6 3.3 2.2 0.8 0.2 2 2.1 0.6 2 1.1 2.5 1.9 1.5 2.6 2.2 1.7 1.5 1.9 2.2 1.1 1.6 2 2.1 2.4 2.4 2.4 1.9 2.2 1.9 2 1.9 0.4 1.7 0.5 1.9 0.8</cell></row><row><cell>pl</cell><cell></cell><cell>23</cell><cell></cell><cell>9.2</cell><cell></cell><cell>20</cell><cell></cell><cell cols="2">18</cell><cell></cell><cell cols="2">8.5</cell><cell></cell><cell cols="5">2.6 31.2 20.9 22.3 24.1</cell><cell></cell><cell cols="11">9.3 27.1 15.3 28.1 27.3 24.9 31.5 23.8 25.4 20.7</cell><cell></cell><cell cols="3">0.1 23.1 32.8</cell><cell cols="2">0.3 26.7</cell><cell>6</cell><cell cols="6">8.6 22.8 23.5 23.4 24.8</cell><cell cols="5">7.4 26.7 11.9 19.7 25.5</cell><cell></cell><cell cols="2">22 14.7</cell><cell>3</cell><cell></cell><cell>5.1</cell><cell>12</cell><cell></cell><cell cols="4">3.2 17.4 18.4</cell><cell></cell><cell>0.9</cell><cell></cell><cell cols="3">0.7 10.3</cell><cell cols="4">27 21.2 27.5 15.4</cell><cell cols="3">9.9 12.9 23.3</cell><cell>4.3</cell><cell cols="3">0.6 24.4 21.7</cell><cell>2.8</cell><cell>22</cell><cell></cell><cell cols="3">0.4 11.6 -</cell><cell cols="4">10.5 30.4 28.8 25.3</cell><cell>9.7 27.2 25.9</cell><cell>2.9 21.9 27.1 21.7</cell><cell>2.4 17.6 14.6 21.2 23.6 13.7</cell><cell>1.4 29.1</cell><cell>0.7 10.4</cell><cell>2.2 17.7 11.2</cell></row><row><cell>ps</cell><cell cols="2">17.6</cell><cell></cell><cell cols="6">8.8 13.7 11.5</cell><cell></cell><cell cols="2">7.1</cell><cell></cell><cell cols="12">10 20.2 17.7 14.4 16.3 12.1 17.1 12.8 19.3 17.7</cell><cell cols="5">16 22.6 15.2 16.4 16.5</cell><cell></cell><cell cols="3">0.2 13.8 22.4</cell><cell cols="3">0.5 17.8 13.3</cell><cell cols="4">6.1 14.7 19.8</cell><cell cols="2">15 16.1</cell><cell cols="7">11 18.7 10.3 13.6 15.8 15.2</cell><cell>12</cell><cell>9</cell><cell></cell><cell>9.6</cell><cell>11</cell><cell></cell><cell cols="4">8.3 11.4 14.4</cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="5">8.4 15.8 17.1 18.4 14.9</cell><cell cols="3">7.4 12.1 17.4</cell><cell cols="4">6.3 12.2 15.1 14.9</cell><cell cols="2">2.3 17.6</cell><cell></cell><cell cols="2">0.6 10.7</cell><cell cols="2">13 -</cell><cell cols="3">21.1 19.1 15.3 11.4 16.5 16.3</cell><cell>4</cell><cell>16 17.7</cell><cell>17 10.6 12.8 16.2 16.3 15.5 13.5</cell><cell>1.5 21.3</cell><cell>0.6</cell><cell>8.6</cell><cell>1.9 11.2</cell><cell>9.3</cell></row><row><cell>pt</cell><cell cols="9">32.7 10.8 25.2 27.6</cell><cell></cell><cell cols="2">9.6</cell><cell></cell><cell cols="7">5.7 38.5 24.2 29.2 32.1 13.8</cell><cell cols="9">33 20.9 38.3 35.3 30.6 49.8 29.1 30.4</cell><cell>26</cell><cell></cell><cell cols="3">0.1 27.5 44.9</cell><cell cols="2">0.3 34.9</cell><cell cols="2">6.3 10.3</cell><cell></cell><cell cols="4">30 29.3 29.5 29.5</cell><cell cols="8">9.4 36.5 13.9 23.8 31.7 25.6 19.2</cell><cell>3.7</cell><cell></cell><cell cols="2">7.4 14.1</cell><cell></cell><cell cols="4">3.7 21.4 23.9</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="10">1 13.5 29.7 31.1 34.6 16.6 10.9 14.4 31.4</cell><cell>5.4</cell><cell cols="3">0.7 28.6 29.9</cell><cell cols="2">3.6 31.9</cell><cell></cell><cell cols="5">0.4 13.8 24.5 12.7 -</cell><cell cols="2">38.3 30.5 11.8 33.3 30.4</cell><cell>4.9 30.9 37.3</cell><cell>24</cell><cell>2.7 20.7 20.6 28.5 29.4 16.2</cell><cell>1.8 35.7</cell><cell>0.8 12.9</cell><cell>2.4 20.1 13.2</cell></row><row><cell>ro</cell><cell cols="12">30.9 11.4 24.6 22.9 10.1</cell><cell></cell><cell cols="9">8.6 38.5 24.4 29.2 30.5 14.1 33.3 18.8</cell><cell></cell><cell cols="7">37 34.5 30.3 44.6 28.3 29.9 25.7</cell><cell></cell><cell cols="3">0.1 27.5 43.8</cell><cell cols="2">0.3 34.4</cell><cell>7.3</cell><cell cols="3">9.8 29.2</cell><cell cols="3">29 29.9 29.7</cell><cell cols="8">8.9 35.2 13.4 23.5 31.9 25.6 18.7</cell><cell>3.5</cell><cell></cell><cell cols="2">9.5 14.1</cell><cell></cell><cell cols="2">4.1 20.6</cell><cell></cell><cell>24</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="10">0.8 13.3 30.9 17.4 34.8 18.2 11.3 15.9 30.4</cell><cell>5.7</cell><cell>1.3</cell><cell cols="2">28 29.1</cell><cell cols="2">3.3 28.7</cell><cell></cell><cell cols="6">0.4 13.6 25.6 12.4 41.3 -</cell><cell>31.1 11.8 33.5 30.9</cell><cell>4.4 29.4 36.3</cell><cell>28</cell><cell>3.3 20.5 20.6 27.7 29.3 16.2</cell><cell>1.7 35.4</cell><cell>0.7 12.7</cell><cell>2.6 20.6 13.2</cell></row><row><cell>ru</cell><cell cols="2">25.4</cell><cell></cell><cell cols="6">9.9 21.7 18.4</cell><cell></cell><cell cols="2">7.9</cell><cell></cell><cell>7</cell><cell cols="4">35 22.3 25.1 25.4</cell><cell></cell><cell cols="3">9.3 29.3 16.2</cell><cell></cell><cell cols="7">31 29.9 27.2 35.7 24.9 28.3 23.1</cell><cell></cell><cell cols="3">0 24.8 36.1</cell><cell cols="2">0.3 28.9</cell><cell>6.4</cell><cell cols="6">8.2 25.4 25.6 26.6 26.2</cell><cell>8.9</cell><cell cols="7">29 12.3 20.9 26.8 23.8 15.1</cell><cell>3.9</cell><cell></cell><cell cols="2">6 12.9</cell><cell></cell><cell cols="2">2.9 18.9</cell><cell></cell><cell>20</cell><cell></cell><cell>0.6</cell><cell></cell><cell cols="4">0.8 11.2 29.5</cell><cell cols="6">19 30.7 16.8 10.7 14.2 25.5</cell><cell>4.2</cell><cell cols="3">1.3 24.8 23.9</cell><cell cols="2">2.2 23.3</cell><cell></cell><cell cols="5">0.5 12.7 23.5 10.9 34.6</cell><cell cols="2">31 -</cell><cell>10.9 29.4 28.6</cell><cell>3.2 26.3 30.3 23.8</cell><cell>2.8 18.9 17.6</cell><cell>22</cell><cell>33 14.9</cell><cell>1.6 31.5</cell><cell>0.6 10.7</cell><cell>1.9 18.7 11.6</cell></row><row><cell>sd</cell><cell></cell><cell>4.8</cell><cell></cell><cell>2.2</cell><cell></cell><cell>0.8</cell><cell></cell><cell cols="2">1.9</cell><cell></cell><cell cols="2">1.6</cell><cell></cell><cell>1.5</cell><cell>4.7</cell><cell>4.4</cell><cell>2.8</cell><cell>4.9</cell><cell></cell><cell>1.5</cell><cell>3.8</cell><cell>4.8</cell><cell></cell><cell>4.8</cell><cell>4.1</cell><cell>3</cell><cell>6</cell><cell>4.2</cell><cell>5.4</cell><cell>3.4</cell><cell></cell><cell>0.1</cell><cell>4.6</cell><cell>6.1</cell><cell>0.3</cell><cell>4.3</cell><cell>2.1</cell><cell>2.9</cell><cell cols="2">1.6</cell><cell>6.4</cell><cell>3.1</cell><cell>4.5</cell><cell>1.2</cell><cell>2.9</cell><cell>5.5</cell><cell>3.8</cell><cell>4.1</cell><cell></cell><cell>2</cell><cell>2.1</cell><cell>1</cell><cell></cell><cell>1.1</cell><cell>3.5</cell><cell></cell><cell>0.9</cell><cell>2</cell><cell></cell><cell>2.7</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.6</cell><cell></cell><cell>2.6</cell><cell>5.4</cell><cell>6</cell><cell>4.5</cell><cell>2.7</cell><cell>3.3</cell><cell>3.1</cell><cell>2.5</cell><cell>0.8</cell><cell>1.1</cell><cell>3.7</cell><cell>3.5</cell><cell>1.2</cell><cell>3.7</cell><cell></cell><cell>0.3</cell><cell>7.1</cell><cell>3.3</cell><cell>6.2</cell><cell>5.9</cell><cell>4.8</cell><cell>3.5 -</cell><cell>4.7</cell><cell>4.6</cell><cell>1.3</cell><cell>3.8</cell><cell>4.7</cell><cell>4.7</cell><cell>1.5</cell><cell>6.4</cell><cell>3.8</cell><cell>2.8</cell><cell>3.9</cell><cell>5</cell><cell>0.4</cell><cell>6.4</cell><cell>0.6</cell><cell>3.9</cell><cell>0.6</cell><cell>1.6</cell><cell>4.2</cell></row><row><cell>sk</cell><cell cols="2">28.4</cell><cell></cell><cell cols="3">9.8 22.8</cell><cell></cell><cell cols="2">19</cell><cell></cell><cell cols="2">9.5</cell><cell></cell><cell cols="17">3.2 35.8 23.2 28.6 27.8 11.8 36.8 17.7 34.7 32.9 27.7 40.4 26.2 30.3 24.2</cell><cell></cell><cell cols="3">0.1 27.3 38.2</cell><cell>0.3</cell><cell>31</cell><cell>7.1</cell><cell>9.4</cell><cell></cell><cell cols="4">27 26.9 29.8 28.3</cell><cell cols="8">8.8 32.3 12.9 23.1 28.8 24.1 16.7</cell><cell>4.1</cell><cell></cell><cell cols="2">6.7 13.4</cell><cell></cell><cell cols="4">3.7 19.5 22.7</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="9">0.6 12.4 29.8 16.1 32.9 17.4 10.4 14.8</cell><cell>28</cell><cell>5</cell><cell>0.9</cell><cell cols="2">28 26.9</cell><cell cols="2">3.1 25.4</cell><cell></cell><cell cols="7">0.5 12.9 25.4 12.1 36.7 33.5 29.8 11.3 -</cell><cell>30.5</cell><cell>4.5 28.5 32.9 25.9</cell><cell>2.9</cell><cell>20 19.1 25.7 28.1 15.2</cell><cell>1.7 33.3</cell><cell>0.7 11.6</cell><cell>2.5 18.8 11.8</cell></row><row><cell>sl</cell><cell cols="2">27.1</cell><cell></cell><cell cols="6">9.4 21.7 18.3</cell><cell></cell><cell cols="2">8.8</cell><cell></cell><cell cols="8">3.5 33.6 22.4 27.4 26.1 11.1 29.4</cell><cell>17</cell><cell></cell><cell cols="7">32 30.5 26.4 36.7 24.7 28.1 23.1</cell><cell></cell><cell cols="2">0.1 25.5</cell><cell>36</cell><cell cols="2">0.3 28.9</cell><cell>6.6</cell><cell>8.7</cell><cell></cell><cell cols="4">25 25.7 29.2 26.6</cell><cell>8.4</cell><cell cols="7">30 12.9 21.4 27.8 23.5 16.1</cell><cell>4.1</cell><cell></cell><cell>7.3</cell><cell>13</cell><cell></cell><cell cols="4">4.3 18.8 21.6</cell><cell></cell><cell>1.1</cell><cell></cell><cell cols="9">0.6 11.9 28.4 12.3 32.1 16.3 10.2 14.3</cell><cell>26</cell><cell>4.9</cell><cell cols="3">1 26.9 24.3</cell><cell cols="2">2.9 23.8</cell><cell></cell><cell>0.4</cell><cell cols="4">13 23.4 11.4 34.3</cell><cell cols="2">31 28.1 10.8</cell><cell>30 -</cell><cell>3.9 26.7 30.2 24.6</cell><cell>3.2 18.8 18.4 24.5 26.9 14.6</cell><cell>1.7 31.9</cell><cell>0.6 11.1</cell><cell>2.2 18.2 11.6</cell></row><row><cell>so</cell><cell></cell><cell>4.3</cell><cell></cell><cell>2.1</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="2">1.9</cell><cell></cell><cell cols="2">0.7</cell><cell></cell><cell>1.8</cell><cell>3.8</cell><cell>3.8</cell><cell>2.2</cell><cell>4.7</cell><cell></cell><cell>3</cell><cell>2.7</cell><cell>6.6</cell><cell></cell><cell>4.2</cell><cell>2.7</cell><cell>2.6</cell><cell>4.2</cell><cell>3.1</cell><cell>4.8</cell><cell>1.3</cell><cell></cell><cell>0.3</cell><cell>2.4</cell><cell>5.8</cell><cell>0.3</cell><cell>3.7</cell><cell>2.3</cell><cell>3.4</cell><cell cols="2">1.2</cell><cell>3.4</cell><cell>2</cell><cell>3.1</cell><cell>0.7</cell><cell>2.1</cell><cell>5.5</cell><cell>3</cell><cell>3</cell><cell></cell><cell>2.6</cell><cell>4.7</cell><cell>1</cell><cell></cell><cell>1.9</cell><cell>3.9</cell><cell></cell><cell>1.2</cell><cell>1.8</cell><cell></cell><cell>3.9</cell><cell></cell><cell>0.9</cell><cell></cell><cell>0.4</cell><cell></cell><cell>1.6</cell><cell>5.2</cell><cell>3.7</cell><cell>4.6</cell><cell>1.4</cell><cell>3.4</cell><cell>1.7</cell><cell>1.6</cell><cell>0.7</cell><cell>2.7</cell><cell>2.2</cell><cell>2.7</cell><cell>2.4</cell><cell>4.1</cell><cell></cell><cell>0.3</cell><cell>3.6</cell><cell>3.1</cell><cell>4.3</cell><cell>3.3</cell><cell>4.5</cell><cell>1.4</cell><cell>2.5</cell><cell>3.5</cell><cell>3.7 -</cell><cell>2.7</cell><cell>3.8</cell><cell>3.8</cell><cell>1.9</cell><cell>5.3</cell><cell>5</cell><cell>1.6</cell><cell>2.7</cell><cell>2.4</cell><cell>0.6</cell><cell>2</cell><cell>0.7</cell><cell>5.4</cell><cell>0.9</cell><cell>2.1</cell><cell>6.2</cell></row><row><cell>sr</cell><cell cols="2">30.1</cell><cell></cell><cell cols="6">11 24.1 21.7</cell><cell></cell><cell cols="2">9.7</cell><cell></cell><cell cols="16">4.9 39.3 24.6 32.9 28.1 13.7 32.3 18.7 35.7 33.2 29.9 41.9 26.2 29.9</cell><cell>26</cell><cell></cell><cell cols="3">0.2 26.2 40.4</cell><cell cols="2">0.3 32.3</cell><cell>7.9</cell><cell cols="3">9.4 28.5</cell><cell cols="11">28 32.1 28.3 10.2 33.2 13.4 22.8 28.1 25.4 17.6</cell><cell>5</cell><cell></cell><cell cols="2">7.3 14.3</cell><cell></cell><cell cols="4">4.2 19.7 22.9</cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell cols="8">13 30.3 14.1 37.7 19.3 11.6 15.9 28.7</cell><cell>5.1</cell><cell cols="3">1.5 26.6 26.9</cell><cell cols="2">2.7 27.8</cell><cell></cell><cell cols="7">0.5 13.5 24.9 12.4 38.3 34.6 30.7 12.4 33.4</cell><cell>32</cell><cell>4.5 -</cell><cell>34.4 27.5</cell><cell>4.6 20.3 21.1 26.5 30.3 16.1</cell><cell>1.5 34.2</cell><cell>0.6 11.7</cell><cell>2.3 19.6 12.8</cell></row><row><cell>sv</cell><cell cols="2">33.6</cell><cell></cell><cell cols="6">11 23.8 21.6</cell><cell></cell><cell cols="6">9.7 11.6 38.4 24.7</cell><cell cols="14">29 30.1 14.5 32.7 19.7 42.9 36.9 29.2 48.7 27.5 31.1 25.4</cell><cell></cell><cell>0.2</cell><cell cols="2">30 42.5</cell><cell>0.3</cell><cell>33</cell><cell cols="4">9 10.5 28.5</cell><cell cols="2">29 30.1</cell><cell>30</cell><cell cols="4">9.4 35.6 13.6 25.4</cell><cell cols="4">30 25.1 18.9</cell><cell>4.7</cell><cell></cell><cell cols="2">9.2 13.9</cell><cell></cell><cell cols="4">4.9 20.5 24.9</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="10">1.5 13.6 30.1 18.4 34.6 18.4 10.9 15.9 31.1</cell><cell>5.9</cell><cell cols="3">1.6 30.1 33.4</cell><cell cols="2">3.2 27.5</cell><cell></cell><cell cols="7">0.5 13.8 24.4 12.3 40.6 35.8 30.3 11.7</cell><cell>33 30.4</cell><cell>4.5 30.4 -</cell><cell>27.9</cell><cell>3.6 20.3 21.2 27.2 29.2 15.9</cell><cell>1.8 35.2</cell><cell>0.9 12.6</cell><cell>2.6 20.4 13.1</cell></row><row><cell>sw</cell><cell cols="2">25.4</cell><cell></cell><cell cols="6">8.8 18.4 13.3</cell><cell></cell><cell cols="20">7.1 11.6 29.3 20.9 21.4 23.1 10.1 24.8 17.2 27.8 24.6 22.5 37.5 20.2 22.7 20.8</cell><cell></cell><cell>0.1</cell><cell>20</cell><cell>32</cell><cell cols="2">0.3 24.4</cell><cell cols="7">6.5 10.5 21.7 24.6 21.9 21.1</cell><cell cols="8">9.1 29.2 14.4 19.2 22.2 20.8 16.7</cell><cell>4.9</cell><cell></cell><cell cols="2">6.9 12.9</cell><cell></cell><cell cols="4">4.5 15.5 17.7</cell><cell></cell><cell>1.2</cell><cell></cell><cell cols="5">0.9 11.9 22.8 24.2</cell><cell cols="5">26 15.3 10.1 13.1 26.3</cell><cell>5.4</cell><cell cols="3">2.6 21.3 21.5</cell><cell>3.4</cell><cell>22</cell><cell></cell><cell>0.4</cell><cell cols="6">12 18.2 11.1 30.7 27.7 22.8 11.1</cell><cell>25 23.7</cell><cell>2.3 23.4 27.5 -</cell><cell>4.9 17.3</cell><cell>19 20.9 21.7 13.8</cell><cell>1.4 29.3</cell><cell>0.8 12.8</cell><cell>2.1 15.8 13.7</cell></row><row><cell>ta</cell><cell></cell><cell>9.8</cell><cell></cell><cell>4.9</cell><cell></cell><cell>3</cell><cell></cell><cell cols="2">3.3</cell><cell></cell><cell cols="2">3.2</cell><cell></cell><cell>3.4</cell><cell cols="2">12 10.4</cell><cell>6.6</cell><cell>9.9</cell><cell></cell><cell>2.8</cell><cell>8.4</cell><cell>8.3</cell><cell></cell><cell>9.3</cell><cell cols="3">9.7 10.4 13.1</cell><cell cols="2">9 10.1</cell><cell>8.2</cell><cell></cell><cell>0.1</cell><cell cols="2">8.7 13.4</cell><cell>0.3</cell><cell>9.7</cell><cell>4.3</cell><cell>4.6</cell><cell cols="3">6.7 12.2</cell><cell>7.9</cell><cell>9.6</cell><cell>3.6</cell><cell>8.9</cell><cell>8.3</cell><cell>7.8</cell><cell>9.4</cell><cell></cell><cell>6.1</cell><cell>5.2</cell><cell>2.7</cell><cell></cell><cell>1.9</cell><cell>6.5</cell><cell></cell><cell>2.8</cell><cell>5.8</cell><cell></cell><cell>6.1</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.3</cell><cell></cell><cell cols="4">5.5 10.3 11.1 10.5</cell><cell>8.1</cell><cell>6.2</cell><cell>7</cell><cell>8.6</cell><cell>1.6</cell><cell>1.3</cell><cell>8.2</cell><cell>7.3</cell><cell>1.6</cell><cell>7.6</cell><cell></cell><cell>0.3</cell><cell>8</cell><cell>7.5</cell><cell cols="3">7 12.6 11.4</cell><cell>8.4</cell><cell>5.7 10.1</cell><cell>9.3</cell><cell>1.6</cell><cell>9.1 10.3</cell><cell>10 -</cell><cell>8.9</cell><cell>5.3</cell><cell>7</cell><cell>8.9</cell><cell>7.3</cell><cell>0.5 11.7</cell><cell>0.6</cell><cell>6.4</cell><cell>0.8</cell><cell>5.3</cell><cell>7.1</cell></row><row><cell>th</cell><cell cols="2">20.3</cell><cell></cell><cell cols="6">9.5 18.4 14.9</cell><cell></cell><cell cols="8">8.9 12.9 25.9 21.1 18.5 20.4</cell><cell></cell><cell cols="11">13 22.3 14.1 24.2 22.4 21.3 27.4 19.7 21.1 19.6</cell><cell></cell><cell cols="3">0.1 19.9 28.5</cell><cell cols="3">0.7 23.1 12.3</cell><cell cols="18">7.5 19.9 23.1 19.8 20.7 11.6 26.1 11.6 18.1 21.2 21.6 14.9 10.5 10.9 13.7</cell><cell></cell><cell cols="4">7.7 16.7 16.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell cols="4">0.4 12.9 20.9</cell><cell cols="2">23 23.5</cell><cell>17</cell><cell>9</cell><cell cols="2">14 22.6</cell><cell cols="4">6.7 13.4 20.2 19.3</cell><cell cols="2">2.7 20.4</cell><cell></cell><cell cols="7">0.6 10.9 17.7 10.1 26.2 24.8 21.1 10.9 22.3 21.1</cell><cell>4.2 21.3 23.5 20.6 11.5 -</cell><cell>18.5 20.5 19.9 12.9</cell><cell>1.5 28.1</cell><cell>0.6</cell><cell>9.5</cell><cell>2.2 16.7 10.3</cell></row><row><cell>tl</cell><cell cols="2">24.2</cell><cell></cell><cell cols="6">8.7 16.1 13.7</cell><cell></cell><cell cols="10">6.8 10.2 26.4 19.3 19.4 22.3 19.2</cell><cell cols="10">22 17.8 25.2 23.7 20.7 34.1 20.5 21.3 18.8</cell><cell></cell><cell cols="3">0.1 17.7 30.2</cell><cell cols="2">0.3 24.6</cell><cell>5.6</cell><cell cols="3">9.7 20.2</cell><cell cols="3">23 20.1 20.1</cell><cell cols="8">8.4 28.4 13.3 17.6 21.4 18.9 17.7</cell><cell>5.5</cell><cell></cell><cell cols="2">7.2 12.6</cell><cell></cell><cell cols="4">4.1 14.3 18.6</cell><cell></cell><cell>1.4</cell><cell></cell><cell cols="4">0.8 10.7 20.1</cell><cell cols="3">22 24.5 14.9</cell><cell cols="2">9.3 12.5</cell><cell>25</cell><cell>5</cell><cell cols="3">2.3 19.7 20.6</cell><cell>3.8</cell><cell>23</cell><cell></cell><cell>0.4</cell><cell cols="6">9.5 16.9 10.9 29.7 25.6 20.6 10.6 22.1 20.7</cell><cell>5 21.7 24.5 23.1</cell><cell>5.9 15.9 -</cell><cell>20 20.8 13.3</cell><cell>1.6 27.2</cell><cell>0.8 12.2</cell><cell>3 14.4</cell><cell>13</cell></row><row><cell>tr</cell><cell cols="9">26.5 10.5 21.8 15.3</cell><cell></cell><cell cols="2">9.6</cell><cell></cell><cell cols="3">1.8 32.2 22.8</cell><cell cols="2">23 25.2</cell><cell></cell><cell cols="3">6.6 26.7 16.6</cell><cell></cell><cell cols="4">31 28.4 25.9 36.9</cell><cell cols="3">24 26.8 23.6</cell><cell></cell><cell cols="3">0.1 23.5 35.1</cell><cell>0.3</cell><cell>28</cell><cell>2.6</cell><cell cols="6">9 24.7 26.3 24.8 25.8</cell><cell cols="5">9.6 30.2 12.7 20.8 26.2</cell><cell></cell><cell cols="2">24 15.7</cell><cell>5.6</cell><cell></cell><cell cols="2">1 12.5</cell><cell></cell><cell cols="4">2.6 18.4 17.6</cell><cell></cell><cell>1.2</cell><cell></cell><cell>0.8</cell><cell></cell><cell cols="5">10 26.1 26.9 28.7 13.4</cell><cell>11</cell><cell cols="2">9 26.9</cell><cell>2.1</cell><cell cols="3">0.3 25.1 23.8</cell><cell cols="2">2.9 22.9</cell><cell></cell><cell>0.4</cell><cell cols="6">9.9 21.1 11.7 34.2 30.8 23.5 10.2 28.1 26.1</cell><cell>3.9</cell><cell>24</cell><cell>30 21.9</cell><cell>3 19.4</cell><cell>9.6 -</cell><cell>23.9 14.5</cell><cell>1.8 31.7</cell><cell>0.8 11.1</cell><cell>2.2 18.4 11.7</cell></row><row><cell>uk</cell><cell cols="24">27.6 10.1 22.9 19.7 10.1 10.6 36.4 22.9 26.9 26.5 11.9 30.8 17.5 32.4</cell><cell cols="5">31 27.5 38.5 25.2 28.2</cell><cell>24</cell><cell></cell><cell cols="3">0.1 25.4 37.3</cell><cell cols="2">0.3 29.7</cell><cell>7.5</cell><cell cols="5">9.3 26.6 26.7 28.2</cell><cell>27</cell><cell cols="8">9.9 31.3 12.5 21.6 27.4 23.9 16.4</cell><cell cols="4">4.6 10.3 13.5</cell><cell></cell><cell cols="4">4.2 18.9 21.6</cell><cell></cell><cell>0.7</cell><cell></cell><cell cols="10">0.5 12.7 29.9 16.9 33.1 17.3 11.5 15.3 26.6</cell><cell>4.7</cell><cell cols="3">1.4 25.7 24.9</cell><cell cols="2">2.5 26.3</cell><cell></cell><cell cols="7">0.5 13.3 23.9 11.6 35.7 33.1 33.9 11.6 31.2 29.1</cell><cell>4.5 29.3 31.6 25.5</cell><cell>3.2 18.8 19.5 24.8 -</cell><cell>15.2</cell><cell>1.6 32.4</cell><cell>0.7 11.3</cell><cell>2.2 18.6 11.8</cell></row><row><cell>ur</cell><cell cols="2">16.5</cell><cell></cell><cell cols="3">6.9 11.6</cell><cell></cell><cell cols="2">7.3</cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>5.8</cell><cell>20</cell><cell cols="3">18 13.6 16.6</cell><cell></cell><cell>5</cell><cell cols="7">17 12.2 18.2 17.3 16.3 23.8</cell><cell cols="3">15 15.9 15.6</cell><cell></cell><cell cols="2">0 14.3</cell><cell>22</cell><cell cols="2">0.3 16.5</cell><cell>7.3</cell><cell cols="6">6.5 14.8 22.9 14.7 15.5</cell><cell cols="2">7.7 18.1</cell><cell cols="2">9.9 13.4</cell><cell cols="3">16 14.8</cell><cell>9.5</cell><cell>4.8</cell><cell></cell><cell>3.7</cell><cell>9.9</cell><cell></cell><cell cols="2">4 12.3</cell><cell></cell><cell>11</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.4</cell><cell></cell><cell cols="2">8.7 16.1</cell><cell cols="3">17 18.1 13.3</cell><cell cols="3">8 12.2 15.9</cell><cell>3.1</cell><cell>1.7</cell><cell cols="2">15 14.3</cell><cell cols="2">1.8 15.3</cell><cell></cell><cell cols="4">0.3 14.7 12.8 10.9</cell><cell cols="3">21 19.2 15.3 12.7</cell><cell>17 16.6</cell><cell>2 14.8 18.6 15.4</cell><cell>5.7 13.1 10.2 14.5 14.4 -</cell><cell>0.9 21.6</cell><cell>0.5</cell><cell>8</cell><cell>1</cell><cell>11</cell><cell>8.5</cell></row><row><cell>uz kk</cell><cell cols="4">3.1 16.2 12.9 1.5</cell><cell></cell><cell>1.2 5.7</cell><cell></cell><cell cols="2">1.6 9.5</cell><cell></cell><cell cols="2">1.8 5.2</cell><cell></cell><cell cols="17">1.3 7.1 21.3 13.4 14.2 16.2 14.2 14.8 21.5 20.6 16.9 15.1 21.3 14.6 16.8 10.5 4.2 3.2 2.2 3.4 2 2.3 3.1 3.1 3.2 3.2 2.3 3.7 3.7 1.9</cell><cell></cell><cell cols="6">0.1 0.4 14.5 19.8 16.3 16.3 12.8 3.1 4.7 0.3 4.4 1.7</cell><cell>1.7 7.6</cell><cell cols="5">1.6 11 17.3 15.5 15.4 3.3 2.4 3.5</cell><cell>0.8 6.5</cell><cell>2.3 13</cell><cell cols="5">2.5 8.6 11.9 16.5 10.2 3.3 3.3 3</cell><cell cols="3">2.2 5.3 18.5 -1</cell><cell>2.4</cell><cell>2 5</cell><cell></cell><cell>0.9 7.6</cell><cell>1.9 8.8</cell><cell></cell><cell>3.3 5</cell><cell></cell><cell>0.6 5.8</cell><cell></cell><cell>0.6 12</cell><cell></cell><cell cols="2">1.6 4.8 16.3 3.4</cell><cell cols="3">4 0.9 20.7 12.5 4.5 2.2</cell><cell>2.1 8.1</cell><cell cols="2">2.2 8.1 12.6 2</cell><cell>0.8 5</cell><cell cols="3">0.8 2.6 15.2 15.2 2.5 2.7</cell><cell cols="2">1.5 1.9 11.5 4.1</cell><cell></cell><cell cols="2">0.4 5.6 12.4 2.8</cell><cell>2.5 13</cell><cell cols="4">1.9 6.3 20.2 18.3 17.5 4.1 3.1 3.6</cell><cell>1.3 5.7 16.3 16.8 3 3</cell><cell>1.2 3.3 17.8 18.5 2.9 2.7</cell><cell>2.5 9.5</cell><cell>0.9 8.7 12.4 3.6</cell><cell>3.1 7.9</cell><cell>1.8 8.4 16.3 2.8</cell><cell>1.9 -9.2 11.2</cell><cell>3 7.1</cell><cell>0.7 1.4</cell><cell>2.1 11</cell><cell>0.6 1.1</cell><cell>2.6 9</cell><cell>2.3 3.6</cell></row><row><cell>vi km</cell><cell cols="9">25.4 20.4 15.2 17.4 9.2 21.5 17.7 11</cell><cell></cell><cell cols="20">9.1 10.4 31.5 21.9 23.2 24.4 8.9 4.3 25.2 19.6 19.3 19.6 17.2 21.6 24.4 24.9 22.1 20.8 27.5 18.6 20.6 16.2 9.5 26 17.1 30.1 27.2 25.1 36.1 22.8 25.1 22.2</cell><cell></cell><cell cols="19">0.1 22.8 33.6 0.6 17.8 27.1 20.8 0.3 27.2 21 14.6 11.1 5.3 9.5 23.7 25.2 24.1 24.2 15 22.5 19.8 18.4 16.1 22.8 12.7 17.2 19.9 19.6 8.5 32.2 13 21.3 25.2 23.9</cell><cell cols="2">18 6.2 18.4 4.2</cell><cell></cell><cell cols="2">7.3 13.5 6.1 -</cell><cell></cell><cell cols="4">4.2 19.5 18.5 9.4 17.1 6.2</cell><cell></cell><cell>1.2 8</cell><cell></cell><cell>0.8 15</cell><cell></cell><cell cols="8">13 25.4 13.5 28.5 14.7 10.3 12.5 27.2 7.9 19.9 3.9 23.8 17.6 11 12.3 23.9</cell><cell>4.5 7.5</cell><cell cols="3">1.5 23.9 3.3 19.2 19.1 24</cell><cell cols="2">3.3 22.8 3 16</cell><cell></cell><cell cols="7">0.4 12.6 21.2 11.3 32.9 29.5 25.9 10.4 26.8 25.5 6.3 15.2 17.2 7.1 23.9 24 20 5.3 21.8 21.1</cell><cell>3.8 22.8 4.8 22.1 23.7 18.4 29 22.3</cell><cell>2.6 19.7 11.9 10 18.2 12.6 16.7 19.7 24 24.3 14.4 9.2 13.4 26.6 1.9 -</cell><cell>0.8 11.7 2.5 14.3</cell><cell>2.4 19.3 12.2 1.6 18.4 8.5</cell></row><row><cell>wo kn</cell><cell cols="6">5.2 20.7 16.9 15.8 2.3 3.2</cell><cell></cell><cell cols="5">2.2 11 10.7 2</cell><cell></cell><cell cols="16">3.5 4.7 24.8 22.8 18.6 18.5 17.4 20.4 25.1 24.1 21.5 21.4 13.9 19.3 20.6 6.4 4.8 4.2 5.6 4.1 4.7 4.1 5 5 6.2 6.6 6 4.7</cell><cell>3.7 19</cell><cell></cell><cell>0.4 0.3</cell><cell cols="2">4.3 18 26.4 6.9</cell><cell cols="2">0.3 21 20.6 5.7</cell><cell>3.1 15</cell><cell cols="3">2.7 9.7 17.3 5.1</cell><cell cols="10">5.9 25 19.5 19.8 17.1 21.8 10.3 16.8 19.8 18.4 4.6 5.7 2.5 6 3 4.7 5.6 5</cell><cell cols="2">4 8.4 18.3 2.3</cell><cell></cell><cell>2.6 8.3</cell><cell cols="2">2.9 1.8 -</cell><cell>1.8</cell><cell>4.3 17.7</cell><cell></cell><cell>4.1 5.1</cell><cell></cell><cell>1.3 6.9</cell><cell></cell><cell>0.5 14</cell><cell></cell><cell>1.6 8</cell><cell>4.9 20</cell><cell cols="3">4.4 5.8 23.4 16.7 6.1 3.2</cell><cell cols="3">2.4 8.7 18.8 21.5 3.1 5.4</cell><cell>1.3 5.5</cell><cell cols="3">3 6.8 19.4 18.5 5.7 4.9</cell><cell cols="2">2.4 2.2 15.6 5.6</cell><cell></cell><cell cols="3">0.3 7.9 15.5 16.6 2.8 5</cell><cell cols="4">1.9 7.7 25.4 22.5 18.3 6.8 5.5 4.9</cell><cell>1.7 6.5</cell><cell>5 21 20.7 4.7</cell><cell>1.4 3.9 21.5 22.7 17.3 21.1 15.9 14.6 18.9 18.2 15.3 13.7 21.1 4.3 5.2 4.8 1.8 4.5 4.5 3.6 4.5 2.6 1.1 6 -</cell><cell>3.4 1.7 14.3</cell><cell>1.2 1 17.6 5.1</cell><cell>3.6 6.8</cell></row><row><cell>xh ko</cell><cell cols="12">17.6 22.3 19.6 22.8 13.1 13.1 8.4 13.3 11.5 6.5</cell><cell></cell><cell cols="17">9.2 19.6 15.9 14.1 16.5 12.4 15.5 14.6 18.6 15.9 15.4 25.3 7.2 28.2 24.4 21.9 22.1 19.1 24.1 28.7 28.5 25.8 23.9 34.4 21.4 15 15.1 14.3 24 23.1</cell><cell></cell><cell cols="29">0.3 12.7 22.3 0.9 22.3 30.7 24.8 24.1 15.4 11.5 23.3 26.2 1.4 17.9 11 7.5 14.6 17.7 14.1 14.3 23 23.8 20.5 28.9 12.3 20.9 23.8 28.1 12.6 22.6 10.1 9.3 20 12 13.2 15.3 14.9 13.9 8 9 10.7 8.8 13.6 -6.6 10.5 13.9 6.4</cell><cell></cell><cell cols="3">1.3 8.1 15.4 0.7</cell><cell></cell><cell cols="12">8.4 15.1 15.4 17.8 13.5 8.2 24.3 10.9 26.4 22.4 13.6 13.8 25.4 10.1 7.6 10.8 19.5 5.9 10.4 14.7 14.8 1.1 22.4 22</cell><cell cols="2">3.5 17.3 2.6 18.9</cell><cell></cell><cell cols="3">0.7 8.8 17.6 19.9 7.9 12.5</cell><cell cols="4">8.7 21.4 18.3 15.1 8.4 29.6 27.6 22.1</cell><cell>9.1 15.7 15.3 7.5 25.4 24.7</cell><cell>4.9 4.8 23.3 26.9 22.1 13.2 19.8 18.8 24.6 22.8 17.1 19.8 29.9 15 17.1 18.5 9.4 11.5 17.6 15.4 14.8 10.5 1.9 20.8</cell><cell>0.8 -2.3 15.6</cell><cell>2.9 10.9 2 23 12.8 13</cell></row><row><cell>yo lb</cell><cell cols="2">5.1 21.3</cell><cell></cell><cell cols="3">1.3 1 13.3 1</cell><cell></cell><cell cols="2">2 9.2</cell><cell></cell><cell cols="2">1.6 8.2</cell><cell></cell><cell cols="5">2.5 7 24.9 18.6 14.4 19.4 5.7 3.9 3.5 4.4</cell><cell></cell><cell cols="11">4.1 9 19.4 12.8 24.8 25.3 19.2 26.2 17.8 18.8 17.7 3.3 4.6 4.2 3.6 4.3 4 4.5 4.3 2.8</cell><cell></cell><cell cols="3">0.3 0.7 16.5 26.1 3.2 5.3</cell><cell cols="2">0.3 8.8 20.6 4.6</cell><cell>2.2 9.5</cell><cell cols="8">3.2 5.2 19.1 19.5 18.1 18.1 15.9 21.1 1.6 3.8 3.8 3.8 1 4.6</cell><cell cols="5">4.1 5 15.4 20.1 15.8 4.2 3.7 3.5</cell><cell cols="2">4 6.5 13.2 1</cell><cell></cell><cell>1.8 7.6</cell><cell>2.7 4.4</cell><cell></cell><cell cols="3">0.9 8.1 12.4 -2.2</cell><cell>4.4</cell><cell></cell><cell>1.2 3.9</cell><cell></cell><cell>0.6 7.8</cell><cell></cell><cell cols="2">1.7 5.4 18.7 3.6</cell><cell cols="2">3.1 8.1 22.8 5.7</cell><cell>2.1 7.5</cell><cell>2 7.3</cell><cell cols="2">1.9 6.7 19.1 3.3</cell><cell>1 3.1</cell><cell cols="3">2.8 3.7 17.4 19.8 3.7 3.8</cell><cell cols="2">2.3 1.8 14.5 5.4</cell><cell></cell><cell>0.3 3.9</cell><cell cols="2">2.3 8.2 15.2 3.7</cell><cell cols="2">2.7 4.4 24.9 4.5</cell><cell cols="2">4.7 22 18.6 4</cell><cell>1.8 5.2 21.5 19.8 4.2 4.3</cell><cell>1.7 2.7 20.6 23.3 14.4 3.6 4 3.9</cell><cell>1.3 9.5 12.9 16.5 17.4 18.7 3.9 5.5 2.1 3.9</cell><cell>2.3 4 11.5 21.2 0.7 4.5</cell><cell>0.7 1.3</cell><cell>3.7 -6.4</cell><cell>3.4 1.2 12.2</cell><cell>4.4 2.7</cell></row><row><cell>zh lg</cell><cell cols="2">20.3 13.5</cell><cell></cell><cell cols="6">8.8 18.9 12.5 6.9 8.2 7.4</cell><cell></cell><cell cols="2">8.2 6.3</cell><cell></cell><cell cols="17">8.2 5.6 15.5 12.9 10.7 11.8 11.6 12.2 12.6 15.1 13.1 13.2 11.2 11.9 11.3 26 20.2 19 20.8 6.4 22.2 13.2 24.2 22.1 22 26.8 20.2 21.6 19.6 12</cell><cell></cell><cell cols="3">0 20.4 0.8 10.2 16.4 29</cell><cell cols="2">0.3 23.3 8.8 13.4</cell><cell>5.6 7.6</cell><cell cols="15">7.1 19.9 22.4 20.4 22.3 8.4 11.6 14.5 11.5 11.7 11.4 15.1 8 24.8 11.3 17.6 21.6 22.9 13.3 9 10.7 11.9 10.1 6.1 11.4 4.8</cell><cell></cell><cell cols="2">6.5 12.1 6 2.3</cell><cell></cell><cell cols="5">4 18.4 14.8 6 10.3 4 -</cell><cell>0.7</cell><cell cols="2">0.6 11.8</cell><cell></cell><cell cols="5">9.7 22.8 11.3 23.8 14.8 6 11.7 11.3 14.8 9</cell><cell cols="3">9.5 12.8 22.1 5.9 7.9 13.8</cell><cell>3.9 3.8</cell><cell cols="3">1.6 20.4 19.2 4.7 11.5 12.1</cell><cell cols="2">2.2 3 11.4 19</cell><cell></cell><cell cols="3">0.5 10.1 18.4 4.2 8.5 10.1</cell><cell cols="4">10 26.8 24.7 21.5 4 16 13.9 11.8</cell><cell>9.9 22.4 21.5 3.9 12.7 12.4</cell><cell>2.3 18.3 23.6 20.4 3.7 12.9 14.2 12.9</cell><cell>2.6 18.5 14.3 8 9.5 13.7 10.6 11.6 20 19.6</cell><cell>13 7.1</cell><cell>1.2 29.1 6.7 14.9</cell><cell>0.7 2</cell><cell>8.8 10</cell><cell>1.6 -1.4 10.7</cell><cell>9.9 4</cell></row><row><cell>zu ln</cell><cell cols="9">17.9 16.5 13.8 15.4 8.3 13 10.6 9.7</cell><cell></cell><cell cols="2">6.7 9.4</cell><cell></cell><cell cols="17">9.2 19.6 15.3 14.3 16.3 12.4 15.9 14.9 19.1 16.4 15.5 24.8 14.6 15.9 14.6 7.2 20.3 16.8 15.7 15.6 15.3 16.8 18.1 20.2 17.2 17.6 17.5 16.2 16.5 16.7</cell><cell></cell><cell cols="6">0.1 12.7 0.6 14.5 22.7 14.5 18.1 22 1.2 17.8 11.1 11</cell><cell cols="14">7.4 15.1 17.8 14.1 14.5 11 16.4 18.2 16.7 16.3 15.7 20.7 11.9 14.4 16.7 9.4 20.6 12.1 14.1 15.1 15.3 13.9 14 9.5</cell><cell>7.7 15</cell><cell></cell><cell cols="2">9.6 10.9 7.5 3.2</cell><cell></cell><cell cols="4">6.9 10.5 14.5 8 14.3 4.9</cell><cell></cell><cell cols="2">1.2 7.8 -</cell><cell>0.5</cell><cell></cell><cell cols="5">8.9 8.1 16.6 15.9 15 15.7 17.5 13.4 19 14.4</cell><cell cols="3">7.4 10.8 8.6 12.4 19.4 20</cell><cell cols="4">5.7 10.6 14.5 5.7 6.3 15.9 15.7 15</cell><cell cols="2">3.2 17.6 3.5 15.7</cell><cell></cell><cell cols="3">0.6 5.7 12.2 14.3 8.1 12.8</cell><cell cols="4">8.3 21.6 19.2 15.4 5.6 21.6 18.3 16.7</cell><cell>9.5 15.6 15.5 5.1 17.3 17.5</cell><cell>5.1 15.6 17.6 19.9 4.4 17.5 18.3 16.9</cell><cell>9.9 11.5 8.5 13.4 17.4 15.8 16.2 11.6 10.5 21.3 18 14.9 15 10.8 1.6 21.1</cell><cell>0.8 12.7 2.5 13</cell><cell>2.9 10.9 -1.7 15.2</cell><cell>7.7</cell></row><row><cell>lo</cell><cell cols="12">21.9 15.4 19.4 11.6 10.7</cell><cell></cell><cell cols="5">9.1 25.5 20.5 19.4 18.4</cell><cell></cell><cell cols="11">18 21.6 24.3 25.2 22.4 19.8 25.6 18.5 20.1 20.6</cell><cell></cell><cell cols="7">1.1 18.2 26.3 20.2 20.7 13.9 11.8</cell><cell></cell><cell cols="2">20 22.6</cell><cell>20</cell><cell cols="10">20 18.3 26.7 12.9 18.3 19.1 18.9 10.7 18.4</cell><cell></cell><cell>8.5</cell><cell cols="4">9.2 10.1 17.6</cell><cell></cell><cell>6.4</cell><cell></cell><cell cols="4">8.6 15.4 -</cell><cell cols="2">20.9</cell><cell cols="6">7.6 23.5 18.6 10.6 15.2 24.5</cell><cell>6.9</cell><cell cols="3">7.4 19.3 19.5</cell><cell cols="2">3.6 17.5</cell><cell></cell><cell cols="3">6.8 14.8 17.1</cell><cell>7.1</cell><cell>26</cell><cell cols="2">24 20.1</cell><cell>5.2 21.7 21.3</cell><cell>5.3 21.9 23.7 20.9 16.3 19.8 19.6 19.9 19.2 12.9 14.1 28.2</cell><cell>3.4 14.5</cell><cell>2.4 17.4 10.6</cell></row><row><cell>lt</cell><cell cols="25">26.6 21.4 24.2 15.2 15.5 16.3 33.5 24.6 26.5 25.9 21.1 29.5 31.6 31.9 29.8</cell><cell cols="2">27 38.6</cell><cell>25</cell><cell>29</cell><cell>25</cell><cell></cell><cell cols="5">0.8 25.6 36.6 28.2 28.8</cell><cell cols="16">17 12.4 26.8 27.5 28.1 27.3 25.8 30.3 13.5 22.8 27.9 26.3 13.8 25.6</cell><cell></cell><cell>15</cell><cell cols="3">8.3 12.3</cell><cell>21</cell><cell></cell><cell>7.3</cell><cell></cell><cell cols="3">8.8 16.8</cell><cell></cell><cell>8.1 -</cell><cell cols="8">16.5 31.1 21.7 14.1 12.9 27.1 10.1</cell><cell cols="3">1.1 25.8 24.3</cell><cell cols="2">3.2 22.7</cell><cell></cell><cell cols="3">9.6 19.4 24.5</cell><cell cols="2">8.8 34.5</cell><cell cols="2">31 28.3</cell><cell>8 29.8 29.2</cell><cell>5.4 28.8 30.5 20.3</cell><cell>4.8 19.9 22.5 26.7 28.1 13.6 22.5 30.8</cell><cell>2.9 17.7</cell><cell>2.2 23.2 14.6</cell></row><row><cell>lv</cell><cell></cell><cell cols="3">4.8 11.3</cell><cell></cell><cell>7.2</cell><cell></cell><cell cols="2">3.3</cell><cell></cell><cell cols="2">4.4</cell><cell></cell><cell cols="2">0.1 16.7</cell><cell>2.1</cell><cell cols="2">7.5 10.4</cell><cell></cell><cell>10</cell><cell cols="5">8.5 21.8 15.7 15.6</cell><cell cols="2">9.2 40.3</cell><cell cols="3">13 16.9 10.1</cell><cell></cell><cell>0</cell><cell cols="3">11 19.9 11.8</cell><cell>7.1</cell><cell>1.5</cell><cell>3.6</cell><cell></cell><cell>6</cell><cell>9</cell><cell>8.5</cell><cell>5.3</cell><cell cols="2">1.1 11.1</cell><cell>2.6</cell><cell cols="4">4 15.6 10.5</cell><cell>0.4</cell><cell>7.8</cell><cell></cell><cell>0.2</cell><cell>1.3</cell><cell></cell><cell>1.7</cell><cell>3.5</cell><cell></cell><cell>0.1</cell><cell></cell><cell>2</cell><cell></cell><cell>7.7</cell><cell></cell><cell cols="2">2 18.5 -</cell><cell></cell><cell>12.8</cell><cell>2.1</cell><cell>3.9</cell><cell>0.5</cell><cell>5.1</cell><cell>0.4</cell><cell cols="2">0.1 12.4</cell><cell>9.9</cell><cell>1.1</cell><cell>0.2</cell><cell></cell><cell>0.7</cell><cell>2.8</cell><cell>6.9</cell><cell cols="2">1.3 18.5</cell><cell cols="2">5.5 19.3</cell><cell>2.9 10.3</cell><cell>9.6</cell><cell>3.8</cell><cell>7.6 12.5 23.1</cell><cell>0.3</cell><cell>7</cell><cell>1 12.7 15.4</cell><cell>1.2</cell><cell>4.9 11.8</cell><cell>0.3</cell><cell>6.8</cell><cell>0 12.9</cell><cell>0.8</cell></row><row><cell>my</cell><cell></cell><cell cols="5">11 14.4 11.5</cell><cell></cell><cell cols="2">7.9</cell><cell></cell><cell cols="2">5.7</cell><cell></cell><cell>0.7</cell><cell cols="2">20 15.1</cell><cell cols="4">15 14.4 13.7</cell><cell cols="10">15 20.7 18.3 17.3 15.6 20.1 15.3 14.4 14.1</cell><cell></cell><cell cols="6">0.3 13.4 20.7 16.9 14.5 12.2</cell><cell cols="6">6.9 12.1 17.6 13.6 14.4</cell><cell cols="2">9.9 17.6</cell><cell cols="3">8.9 11.6 16.3</cell><cell></cell><cell>16</cell><cell cols="2">2.3 15.9</cell><cell></cell><cell>4</cell><cell>5.6</cell><cell></cell><cell cols="2">6.5 14.4</cell><cell></cell><cell>3.9</cell><cell></cell><cell cols="3">5.3 12.5</cell><cell></cell><cell cols="2">7 13.7</cell><cell cols="3">0.9 17.9 14.4</cell><cell>8.4</cell><cell cols="3">8.5 14.3 -</cell><cell cols="3">1.4 15.9 14.1</cell><cell cols="2">1.2 10.2</cell><cell></cell><cell cols="2">3.3 11.8</cell><cell>13</cell><cell cols="3">4.7 20.1 17.7</cell><cell>15</cell><cell>4.6 16.4 15.1</cell><cell>2.8 16.3 18.7</cell><cell>8.8 10.1 13.6</cell><cell>4.9 13.9</cell><cell>8.6</cell><cell>9.4</cell><cell>9.6 18.7</cell><cell>0.6 11.5</cell><cell>0.6 13.9</cell><cell>2.6</cell></row><row><cell>ne</cell><cell cols="4">15.6 17.6</cell><cell></cell><cell cols="4">2.4 11.1</cell><cell></cell><cell cols="2">7.8</cell><cell></cell><cell cols="7">3 17.5 20.4 12.8 14.9 18.2</cell><cell cols="5">9.3 28.4 18.7 15.9</cell><cell>8.1</cell><cell>32</cell><cell cols="2">5.5 13.5</cell><cell>3.7</cell><cell></cell><cell>0.2</cell><cell>6.8</cell><cell>5.2</cell><cell cols="4">16 13.3 17.8 10.4</cell><cell cols="3">3.7 26.8</cell><cell>9.8</cell><cell cols="2">4.1 12.5</cell><cell cols="4">4.2 12.5 10.1 15.1</cell><cell></cell><cell>8.6</cell><cell cols="2">3.1 19.7</cell><cell></cell><cell>4.9</cell><cell cols="3">6 12.6</cell><cell>9.1</cell><cell></cell><cell>5.7</cell><cell></cell><cell>7.9</cell><cell></cell><cell cols="3">14 11.1</cell><cell>7.4</cell><cell cols="3">0.7 16.9 15.5</cell><cell cols="3">11 15.8 10.3</cell><cell cols="2">7 -</cell><cell cols="2">13 13.9</cell><cell>2.4</cell><cell>8.9</cell><cell></cell><cell cols="2">9.4 19.9</cell><cell>9.2</cell><cell>7.9</cell><cell cols="2">8.1 14.8</cell><cell>9.8</cell><cell>6.9 12.9 13.9</cell><cell>4.2 14.4 15.7 10.1</cell><cell>9.2 11.7</cell><cell>7.1</cell><cell>2.3</cell><cell>10</cell><cell>13 11.5</cell><cell>5</cell><cell>1.9 14.7</cell><cell>1.2</cell><cell>9</cell><cell>6.4</cell></row><row><cell>nl</cell><cell cols="14">25.7 20.8 22.6 16.1 14.3 13.9</cell><cell cols="16">31 24.5 24.5 24.7 20.7 26.4 31.9 30.9 29.1 24.9 36.7 24.1 25.9 23.1</cell><cell></cell><cell cols="23">0.8 24.3 35.3 27.5 27.2 17.4 12.2 26.1 26.2 25.7 25.8 23.7 28.6 13.5 22.2 26.3 25.9 13.6 23.2 14.5</cell><cell cols="4">8.7 13.8 19.2</cell><cell></cell><cell>8.5</cell><cell></cell><cell cols="3">9.1 16.3</cell><cell></cell><cell cols="9">8.4 25.7 12.6 28.8 24.5 14.4 17.4 26.3 11.2</cell><cell cols="2">1 -</cell><cell>24.8</cell><cell cols="5">4 23.8 11.3 18.7</cell><cell>22</cell><cell>9.9</cell><cell cols="3">32 29.1 24.9</cell><cell>8.8 27.3 26.2</cell><cell>5.6 25.8 29.7</cell><cell>23 10.1 19.5 22.2 25.8 25.3 16.9 21.8 29.1</cell><cell>3 17.1</cell><cell>2.5</cell><cell>22 15.3</cell></row><row><cell>no</cell><cell></cell><cell cols="29">32 23.2 26.4 17.3 15.6 16.8 35.9 26.9 28.4 27.9 23.2 30.2 38.6 36.9 33.5 27.9 47.6 26.6 29.3 26.6</cell><cell></cell><cell cols="17">0.7 27.4 40.4 30.5 30.8 18.5 13.6 31.1 30.6 28.7 28.4 26.8 33.9 14.6 25.8 28.9</cell><cell></cell><cell cols="5">27 16.2 25.1 15.1</cell><cell cols="3">9.7 13.6</cell><cell>21</cell><cell></cell><cell>9.1</cell><cell></cell><cell cols="3">9.9 17.5</cell><cell></cell><cell cols="9">9.5 28.3 15.5 33.8 26.6 15.8 20.4 31.4 11.3</cell><cell cols="3">2.3 28.6 -</cell><cell cols="2">4 26.4</cell><cell></cell><cell>11</cell><cell cols="6">21 23.7 11.3 38.1 33.8 28.2</cell><cell>9.7 31.5 29.4</cell><cell>6.1 30.8 36.9 27.6 12.9 21.2 25.5 29.5</cell><cell>29 18.9 24.6 33.6</cell><cell>3.1 19.4</cell><cell>2.5 24.6 17.5</cell></row><row><cell>ns</cell><cell></cell><cell>6.4</cell><cell></cell><cell>2.4</cell><cell></cell><cell>3.7</cell><cell></cell><cell cols="2">3.9</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>3</cell><cell>7.4</cell><cell>7.4</cell><cell>4.4</cell><cell>7.2</cell><cell></cell><cell>6.3</cell><cell>5.5</cell><cell>5.7</cell><cell></cell><cell>6.9</cell><cell>6.7</cell><cell>7.2</cell><cell>7.8</cell><cell>6.7</cell><cell>4.9</cell><cell>5</cell><cell></cell><cell>1</cell><cell>5</cell><cell>8.6</cell><cell>3.8</cell><cell>5.8</cell><cell>5.2</cell><cell>3.5</cell><cell cols="2">6.3</cell><cell>7.5</cell><cell>4.5</cell><cell>4.9</cell><cell>6.4</cell><cell>5.6</cell><cell>3.6</cell><cell>4.8</cell><cell>6.6</cell><cell></cell><cell>7.4</cell><cell>2.4</cell><cell>7.1</cell><cell></cell><cell>2.8</cell><cell>2.2</cell><cell></cell><cell>3.9</cell><cell>6.1</cell><cell></cell><cell>2.4</cell><cell></cell><cell>3.1</cell><cell></cell><cell>5.7</cell><cell></cell><cell>2.3</cell><cell>5.2</cell><cell>4.1</cell><cell>6.6</cell><cell>1.7</cell><cell>3.2</cell><cell>4.6</cell><cell>4.4</cell><cell>0.8</cell><cell>3.2</cell><cell>6.1</cell><cell cols="2">6.2 -</cell><cell>5.9</cell><cell></cell><cell>1.9</cell><cell>4.2</cell><cell>5.7</cell><cell>2</cell><cell>8.1</cell><cell>6.4</cell><cell>5.9</cell><cell>2.5</cell><cell>5.6</cell><cell>5.5</cell><cell>2.2</cell><cell>5.5</cell><cell>6.2</cell><cell>4.8</cell><cell>5.6</cell><cell>5.1</cell><cell>5.2</cell><cell>6.3</cell><cell>6.1</cell><cell>3.6</cell><cell>3.6</cell><cell>6.1</cell><cell>1.2</cell><cell>4.6</cell><cell>1.2</cell><cell>4.3</cell><cell>2.1</cell></row><row><cell>oc</cell><cell cols="24">35.2 19.1 29.2 20.7 14.5 18.1 38.8 27.4 30.4 32.4 23.5 33.2 40.4 39.2</cell><cell>36</cell><cell cols="2">31 54.4</cell><cell cols="3">28 30.1 28.6</cell><cell></cell><cell cols="6">0.4 28.2 45.4 31.9 35.5 19.6</cell><cell cols="4">14 33.5 31.4</cell><cell cols="11">31 29.9 27.3 38.4 14.3 25.5 31.4 28.2 15.1 25.9</cell><cell></cell><cell>13</cell><cell cols="4">8.6 13.2 23.4</cell><cell></cell><cell>9.4</cell><cell></cell><cell cols="3">9.2 17.6</cell><cell></cell><cell cols="2">8.1 30.6</cell><cell cols="6">2.1 37.1 23.7 14.1 20.6 33.7</cell><cell>9.4</cell><cell cols="3">6.1 28.9 29.9</cell><cell cols="2">4.3 -</cell><cell></cell><cell>9</cell><cell cols="2">20 25.2</cell><cell cols="4">9.8 44.3 36.4 30.3</cell><cell>8.6</cell><cell>33 31.8</cell><cell>5.9</cell><cell>35</cell><cell>39 28.3 12.8 21.8 27.5 31.4 31.1 18.7 21.1 36.1</cell><cell>2.7 19.7</cell><cell>1.9 25.1</cell><cell>11</cell></row><row><cell>or</cell><cell cols="4">13.3 11.6</cell><cell></cell><cell>3.8</cell><cell></cell><cell cols="2">6.5</cell><cell></cell><cell cols="2">4.9</cell><cell></cell><cell cols="7">2.2 16.6 20.6 12.8 13.5 12.3</cell><cell cols="9">9.9 19.9 15.3 13.8 11.4 20.8 11.7 12.9</cell><cell>7.7</cell><cell></cell><cell>0.5</cell><cell cols="4">9.7 14.7 13.6 12.7</cell><cell>15</cell><cell>6.2</cell><cell cols="5">4.9 21.5 10.5 10.2</cell><cell>7.8</cell><cell>9.3</cell><cell>8.1</cell><cell cols="2">8.1 13.9</cell><cell></cell><cell>7.1</cell><cell cols="2">0.8 14.7</cell><cell></cell><cell>3.8</cell><cell>4.2</cell><cell></cell><cell>9</cell><cell>7.1</cell><cell></cell><cell>4.4</cell><cell></cell><cell cols="3">4.7 10.3</cell><cell></cell><cell>4.2</cell><cell>10</cell><cell cols="3">0.6 15.6 11.4</cell><cell cols="3">6.9 12.2 10.6</cell><cell>1.6</cell><cell cols="3">4.2 11.9 10.9</cell><cell>1.2</cell><cell cols="2">9.8 -</cell><cell></cell><cell>14.3</cell><cell>9.1</cell><cell>5.5</cell><cell cols="3">14 14.2 10.7</cell><cell>6.3 11.8 11.5</cell><cell>2.2 10.7 13.9</cell><cell>7.3 11.5 11.1</cell><cell>6.5</cell><cell>5</cell><cell>6.1 12.2</cell><cell>10</cell><cell>6.8</cell><cell>0.8</cell><cell>9.3</cell><cell>0.8</cell><cell>8.2</cell><cell>2.2</cell></row><row><cell>wo</cell><cell></cell><cell>9.3</cell><cell></cell><cell>2.3</cell><cell></cell><cell>4.1</cell><cell></cell><cell cols="2">4.4</cell><cell></cell><cell cols="2">4.1</cell><cell></cell><cell cols="2">4 10.7</cell><cell>8.8</cell><cell>6.5</cell><cell>9.3</cell><cell></cell><cell>7.4</cell><cell>7.1</cell><cell>4.6</cell><cell></cell><cell>8.9</cell><cell>7.9</cell><cell>9.7</cell><cell>9.2</cell><cell>9</cell><cell>6.8</cell><cell>7.7</cell><cell></cell><cell>0.6</cell><cell cols="2">6.6 11.8</cell><cell>3.7</cell><cell>8.7</cell><cell>6</cell><cell>4</cell><cell cols="2">7.7</cell><cell>9.6</cell><cell>7</cell><cell>6.6</cell><cell>8</cell><cell>8.7</cell><cell>4.4</cell><cell>6.5</cell><cell>8.6</cell><cell></cell><cell>8.2</cell><cell>2.8</cell><cell>7.6</cell><cell></cell><cell>3.7</cell><cell>2.7</cell><cell></cell><cell>4.7</cell><cell>6.9</cell><cell></cell><cell>2.6</cell><cell></cell><cell>3.3</cell><cell></cell><cell>6.8</cell><cell></cell><cell>2.4</cell><cell>7.7</cell><cell>5.6</cell><cell>9.8</cell><cell>3</cell><cell>3.9</cell><cell>4.8</cell><cell>7.2</cell><cell>1.3</cell><cell>1.4</cell><cell>7.8</cell><cell>8</cell><cell>1.5</cell><cell>6</cell><cell></cell><cell>2.1</cell><cell>5.3</cell><cell>7.1</cell><cell cols="2">2.6 10.6</cell><cell>8.6</cell><cell>7.8</cell><cell>2.5</cell><cell>7.9</cell><cell>8</cell><cell>2.4</cell><cell>8.3</cell><cell>7.7</cell><cell>5.8</cell><cell>6.3</cell><cell>6</cell><cell>8.2</cell><cell>8.1</cell><cell>7.9</cell><cell>4.1</cell><cell>4.2</cell><cell>9.7 -</cell><cell>4.5</cell><cell>1.1</cell><cell>4.9</cell><cell>1.6</cell></row><row><cell>xh</cell><cell cols="9">24.6 16.4 19.6 12.7</cell><cell></cell><cell></cell><cell cols="19">12 13.5 27.1 22.6 20.4 21.9 20.4 22.6 27.3 27.8 23.8 21.8 34.5 19.7 22.2 20.9</cell><cell></cell><cell cols="23">0.9 19.3 30.3 21.3 23.3 15.2 13.8 21.7 24.8 21.6 21.2 20.9 27.8 14.1 18.7 21.3 21.5 12.2 19.3 10.4</cell><cell cols="4">6.7 11.6 17.5</cell><cell></cell><cell cols="5">6.4 10.1 17.2</cell><cell></cell><cell cols="3">9 21.9 22.1</cell><cell cols="3">26 11.8 11.2</cell><cell cols="2">18 26.6</cell><cell>5.4</cell><cell cols="3">7.8 20.2 20.4</cell><cell cols="2">3.6 21.4</cell><cell></cell><cell cols="3">7 16.8 18.3</cell><cell cols="4">7.4 28.9 25.9 21.3</cell><cell>7.6 22.8 22.5</cell><cell>5.3 24.2 25.1 25.2 20.3 15.6 23.7 22.2 21.7</cell><cell>16 16.8 27.4</cell><cell>2.5 -</cell><cell>1.8 18.5</cell><cell>7.4</cell></row><row><cell>yo</cell><cell></cell><cell>5.2</cell><cell></cell><cell>1.4</cell><cell></cell><cell>1.8</cell><cell></cell><cell cols="2">3.1</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>1.2</cell><cell>7.3</cell><cell>6.1</cell><cell>4.9</cell><cell>7</cell><cell></cell><cell>4.9</cell><cell>4.8</cell><cell>4.5</cell><cell></cell><cell>6.7</cell><cell>6</cell><cell>6.3</cell><cell>7.3</cell><cell>6.3</cell><cell>4.8</cell><cell>4.3</cell><cell></cell><cell>0.3</cell><cell>4.4</cell><cell>7.9</cell><cell>3.1</cell><cell>6.1</cell><cell>5</cell><cell>2.6</cell><cell cols="2">3.8</cell><cell>6.8</cell><cell>4.4</cell><cell>4.1</cell><cell>5.9</cell><cell>6</cell><cell>2.8</cell><cell>4.1</cell><cell>6.5</cell><cell></cell><cell>3.7</cell><cell>1</cell><cell>6.4</cell><cell></cell><cell>2.4</cell><cell>1.6</cell><cell></cell><cell>3.9</cell><cell>4</cell><cell></cell><cell>1.9</cell><cell></cell><cell>2.2</cell><cell></cell><cell>4.4</cell><cell></cell><cell>2.1</cell><cell>4.4</cell><cell>2.5</cell><cell>6.4</cell><cell>2.6</cell><cell>2.7</cell><cell>4</cell><cell>4.1</cell><cell>1.5</cell><cell>0.3</cell><cell>5.3</cell><cell>6.1</cell><cell>1.5</cell><cell>4.1</cell><cell></cell><cell>1.6</cell><cell>3.8</cell><cell>4.4</cell><cell>1.8</cell><cell>7.6</cell><cell>5.9</cell><cell>4.5</cell><cell>2.3</cell><cell>5.3</cell><cell>5.1</cell><cell>1.6</cell><cell>5.6</cell><cell>5.7</cell><cell>3.6</cell><cell>3.8</cell><cell>5</cell><cell>4.6</cell><cell>3.3</cell><cell>3.9</cell><cell>3.1</cell><cell>3.1</cell><cell>5.4</cell><cell>0.5</cell><cell>3.4 -</cell><cell>4.3</cell><cell>1.2</cell></row><row><cell>zh</cell><cell></cell><cell cols="8">23 18.2 23.1 13.4</cell><cell></cell><cell></cell><cell>14</cell><cell></cell><cell>8.7</cell><cell cols="8">29 24.7 23.2 23.1 19.6 25.4 28.7</cell><cell></cell><cell cols="7">28 26.2 24.1 34.6 22.7 24.5 23.7</cell><cell></cell><cell cols="10">0.9 23.1 32.6 24.9 25.4 15.9 11.2 24.2 25.8</cell><cell cols="7">24 24.6 22.1 28.9 12.8 20.7 25.1</cell><cell></cell><cell cols="5">27 13.3 22.6 11.8</cell><cell cols="4">9 13.1 21.5</cell><cell></cell><cell>6.5</cell><cell></cell><cell cols="3">8 15.6</cell><cell></cell><cell cols="6">8.6 25.9 11.6 27.5 22.2 14.1</cell><cell cols="3">17 26.3 11.5</cell><cell cols="3">1.9 24.1 22.3</cell><cell cols="2">2.8 19.9</cell><cell></cell><cell cols="3">9.5 18.4 21.1</cell><cell cols="4">8.9 30.4 27.5 23.9</cell><cell>7.1 25.4 24.4</cell><cell>5 23.9 27.9 22.6</cell><cell>8.7 20.3 20.1 24.2 23.7 17.2 20.2 31.5</cell><cell>2.5 15.3</cell><cell>2.1 -</cell><cell>13.5</cell></row><row><cell>zu</cell><cell cols="30">23.1 14.8 18.3 11.7 11.3 12.7 25.6 21.6 19.4 21.3 18.8 21.7 25.7 26.1 23.1 20.2 30.8 18.5 20.3 19.7</cell><cell></cell><cell cols="6">0.9 17.3 28.8 19.4 21.7 14.5</cell><cell cols="9">13 21.1 23.7 20.1 19.8 20.6 26.7 13.8</cell><cell cols="6">18 20.3 20.3 10.1 17.7</cell><cell></cell><cell>9.5</cell><cell>5.8</cell><cell></cell><cell cols="2">9.7 15.5</cell><cell></cell><cell>5.9</cell><cell></cell><cell cols="3">9.1 15.8</cell><cell></cell><cell cols="8">6.1 20.5 21.2 24.4 13.3 11.3 16.9 25.6</cell><cell>2</cell><cell cols="3">5.3 18.8 20.1</cell><cell>2.8</cell><cell>19</cell><cell></cell><cell cols="3">5.1 15.6 16.6</cell><cell cols="4">6.8 28.3 25.2 20.6</cell><cell>6.7 21.7 21.7</cell><cell>4.9 22.9 24.4 25.2 19.5 14.5 22.7 21.3 20.9 15.5 17.1 27.2</cell><cell>1.7 19.5</cell><cell>1.3 17.5 -</cell></row></table><note><p><p>7</p>E Experimental Results in Section 6</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>According to our experiments, ReZero is unstable with half precision, even when the model is shallow.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://tatoeba.org/en/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>2(w dl , v dl ) By means of Taylor expansion, we estimate the update of l-th cross-attention layer ||y * l+1 -y l+1 || as:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/facebookresearch/flores</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/pytorch/fairseq/tree/main/examples/m2m_100</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement We would like to acknowledge <rs type="person">Saksham Singhal</rs> for the CCMatrix corpus.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rezero is all you need: Fast convergence at large depth</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Huanru</surname></persName>
		</author>
		<author>
			<persName><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauley</surname></persName>
		</author>
		<idno>CoRR, abs/2003.04887</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">UniLMv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BEiT: BERT pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2019 conference on machine translation (WMT19)</title>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santanu</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri ; Rajen Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</editor>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-01">2019. August 1-2, 2019. 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014">June 26-27, 2014. 2014</date>
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (WMT17)</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi ; Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><surname>Jimeno-Yepes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<editor>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</editor>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-07">2017. September 7-8, 2017. 2017</date>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Findings of the 2018 conference on machine translation (WMT18)</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz ; Rajen Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</editor>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium; Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 1, 2018. 2018</date>
			<biblScope unit="page" from="272" to="303" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">InfoXLM: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3576" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">XLM-E: cross-lingual language model pre-training via ELECTRA</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2106.16138</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Emma</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</author>
		<idno>CoRR, abs/2112.06905</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CCAligned: A massive collection of cross-lingual web-document pairs</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="5960" to="5969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond english-centric multilingual machine translation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2010</title>
		<editor>
			<persName><forename type="first">Yee</forename><surname>Whye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Teh</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">Mike</forename><surname>Titterington</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The FLORES-101 evaluation benchmark for low-resource and multilingual machine translation</title>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno>CoRR, abs/2106.03193</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shi Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4475" to="4483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishub</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Berghammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03819-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>ICLR 2021</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Few-shot learning with multilingual language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2112.10668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DeltaLM: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders</title>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Hassan Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2106.13736</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Salazar</surname></persName>
		</author>
		<idno>CoRR, abs/1910.05895</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post ; Rajen Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</editor>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 1, 2018. 2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Ondrej Bojar</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<editor>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">June 1-6, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CCMatrix: Mining billions of high-quality parallel sentences on the web</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6490" to="6500" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Normformer: Improved transformer pretraining with extra normalization</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<idno>CoRR, abs/2110.09456</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elton</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/2201.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Vlmo: Unified vision-language pre-training with mixture-of-modality-experts</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, abs/2111.02358</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimizing deeper transformers on small datasets</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2089" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving deep transformer with depth-scaled initialization and merged attention</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="898" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
