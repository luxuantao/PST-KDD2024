<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images</title>
				<funder ref="#_SqSBcsX">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_mzrNbmq">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_XUyp5cp">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yudong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2018.2837742</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2837742, IEEE Transactions on Pattern Analysis and Machine Intelligence SUBMITTED TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D face reconstruction</term>
					<term>face tracking</term>
					<term>face performance capturing</term>
					<term>3D face dataset</term>
					<term>image synthesis</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the powerfulness of convolution neural networks (CNN), CNN based face reconstruction has recently shown promising performance in reconstructing detailed face shape from 2D face images. The success of CNN-based methods relies on a large number of labeled data. The state-of-the-art synthesizes such data using a coarse morphable face model, which however has difficulty to generate detailed photo-realistic images of faces (with wrinkles). This paper presents a novel face data generation method. Specifically, we render a large number of photo-realistic face images with different attributes based on inverse rendering. Furthermore, we construct a fine-detailed face image dataset by transferring different scales of details from one image to another. We also construct a large number of video-type adjacent frame pairs by simulating the distribution of real video data 1 . With these nicely constructed datasets, we propose a coarseto-fine learning framework consisting of three convolutional networks. The networks are trained for real-time detailed 3D face reconstruction from monocular video as well as from a single image. Extensive experimental results demonstrate that our framework can produce high-quality reconstruction but with much less computation time compared to the state-of-the-art. Moreover, our method is robust to pose, expression and lighting due to the diversity of data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>This paper considers the problem of dense 3D face reconstruction from monocular video as well as from a single face image. Single-image based 3D face reconstruction can be considered as a special case of video based reconstruction. It also plays an essential role. Actually image-based 3D face reconstruction itself is a fundamental problem in computer vision and graphics, and has many applications such as face recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b53">[54]</ref> and face animation <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Video-based dense face reconstruction and tracking or facial performance capturing has a long history <ref type="bibr" target="#b56">[57]</ref> also with many applications such as facial expression transfer <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> and face replacement <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Traditional facial performance capture methods usually require complex hardware and significant user intervention <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b20">[21]</ref> to achieve a sufficient reality and therefore are not suitable for consumer-level applications. Commodity RGB-D camera based methods <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b51">[52]</ref> have demonstrated real-time reconstruction and animation Yudong Guo, Juyong Zhang and Boyi Jiang are with School of Mathematical Sciences, University of Science and Technology of China.</p><p>Jianfei Cai and Jianmin Zheng are with School of Computer Science and Engineering, Nanyang Technological University.</p><p>? Corresponding author. Email: juyong@ustc.edu.cn. 1 All these coarse-scale and fine-scale photo-realistic face image datasets can be downloaded from https://github.com/Juyong/3DFace. results. However, RGB-D devices, such as Microsoft's Kinect, are still not that common and not of high resolution, compared to RGB devices.</p><p>Recently, several approaches have been proposed for RGB video based facial performance captureing <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Compared to image-based 3D face reconstruction that is considered as an ill-pose and challenging task due to the ambiguities caused by insufficient information conveyed in 2D images, video-based 3D reconstruction and tracking is even more challenging especially when the reconstruction is required to be real-time, fine-detailed and robust to pose, facial expression, lighting, etc. These proposed approaches only partially comply with the requirements. For example, <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b6">[7]</ref> learn facial geometry while not recovering facial appearance property, such as albedo. <ref type="bibr" target="#b17">[18]</ref> can reconstruct personalized face rig of high-quality, but their optimization-based method is time-consuming and needs about 3 minutes per frame. <ref type="bibr" target="#b52">[53]</ref> achieves real-time face reconstruction and facial reenactment through data-parallel optimization strategy, but their method cannot recover fine-scale details such as wrinkles and also requires facial landmark inputs.</p><p>In this paper, we present a solution to tackle all these problems by utilizing the powerfulness of convolutional neural networks (CNN). CNN based approaches have been proposed for face reconstruction from a single image <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b23">[24]</ref>, but CNN is rarely explored for video-based dense face reconstruction and tracking, especially for real-time reconstruction. Inspired by the state-of-the-art single-image based face reconstruction method <ref type="bibr" target="#b41">[42]</ref>, which employs two cascaded CNNs (coarse-layer CNN and fine-layer CNN) to reconstruct a detailed 3D facial surface from a single image, we develop a dense face reconstruction and tracking framework. The framework includes a new network architecture called 3DFaceNet for online real-time dense face reconstruction from monocular video (supporting a single-image input as well), and optimization-based inverse rendering for offline generating large-scale training datasets.</p><p>In particular, our proposed 3DFaceNet consists of three convolutional networks: a coarse-scale single-image network (named Single-image CoarseNet for the first frame or the single image case), a coarse-scale tracking network (Tracking CoarseNet) and a fine-scale network (FineNet). For singleimage based reconstruction, compared with <ref type="bibr" target="#b41">[42]</ref>, the key uniqueness of our framework lies in the photo-realistic datasets we generate for training CoarseNet and FineNet.</p><p>It is known that one major challenge for CNN-based methods lies in the difficulty to obtain a large number of labelled training data. For our case, there is no publicly available dataset that can provide large-scale face images with their corresponding high-quality 3D face models. For training CoarseNet, <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref> resolve the training data problem by directly synthesizing face images with randomized parametric face model parameters. Nevertheless, due to the low dimensionality of the parametric face model, albedo and random background synthesized, the rendered images in <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> are not photo-realistic. In contrast, we propose to create realistic face images by starting from real photographs and manipulating them after an inverse rendering procedure. For training FineNet, because of no dataset with detailed face geometry, <ref type="bibr" target="#b41">[42]</ref> uses an unsupervised training by adopting the shading energy as the loss function. However, to make back-propagation trackable, <ref type="bibr" target="#b41">[42]</ref> employs the first-order spherical harmonics to model the lighting, which makes the final detailed reconstruction not so accurate. On the contrary, we propose a novel approach to transfer different scales of details from one image to another. With the constructed fine-detailed face image dataset, we can train FineNet in a fully supervised manner, instead of the unsupervised way in <ref type="bibr" target="#b41">[42]</ref>, and thus can produce more accurate reconstruction results. Moreover, for training our coarse-scale tracking network for the video input case, we consider the coherence between adjacent frames and simulate adjacent frames according to the statistics learned from real facial videos for training data generation.</p><p>Contributions. In summary, the main contributions of this paper lie in the following five aspects: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>3D face reconstruction and facial performance capturing have been studied extensively in computer vision and computer graphics communities. For conciseness, we only review the most relevant works here.</p><p>Low-dimensional Face Models. Model-based approaches for face shape reconstruction have grown in popularity over the last decade. Blanz and Vetter <ref type="bibr" target="#b3">[4]</ref> proposed to represent a textured 3D face with principal components analysis (PCA), which provides an effective low-dimensional representation in terms of latent variables and corresponding basis vectors <ref type="bibr" target="#b49">[50]</ref>. The model has been widely used in various computer vision tasks, such as face recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b53">[54]</ref>, face alignment <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and face reenactment <ref type="bibr" target="#b52">[53]</ref>. Although such a model is able to capture the global structure of a 3D face from a single image <ref type="bibr" target="#b3">[4]</ref> or multiple images <ref type="bibr" target="#b1">[2]</ref>, the facial details like wrinkles and folds are not possible to be captured. In addition, the reconstructed face models rely heavily on training samples. For example, a face shape is difficult to be reconstructed if it is far away from the span of the training samples. Thus, similar to <ref type="bibr" target="#b41">[42]</ref>, we only use the low-dimensional model in our coarse layer to reconstruct a rough geometry and we refine the geometry in our fine layer.</p><p>Shape-from-shading (SFS). SFS <ref type="bibr" target="#b38">[39]</ref> makes use of the rendering principle to recover the underlying shape from shading observations. The performance of SFS largely depends on constraints or priors. For 3D face reconstruction, in order to achieve plausible results, the prior knowledge about the geometry must be applied. For instance, in order to reduce the ambiguity and the complexity of SFS, the symmetry of the human face has often been employed <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Kemelmacher et al. <ref type="bibr" target="#b28">[29]</ref> used a reference model prior to align with the face image and then applied SFS to refine the reference model to better match the image. Despite the improved performance of this technique, its capability to capture global face structure is limited.</p><p>Inverse Rendering. The generation of a face image depends on several factors: face geometry, albedo, lighting, pose and camera parameters. Face inverse rendering refers to the process of estimating all these factors from a real face image, which can then be manipulated to render new images. Inverse rendering is similar to SFS with the difference that inverse rendering aims to estimate all the rendering parameters while SFS mainly cares about reconstructing the geometry. Aldrian et al. <ref type="bibr" target="#b0">[1]</ref> did face inverse rendering with a parametric face model using a multilinear approach, where the face geometry and the albedo are encoded on parametric face model. In <ref type="bibr" target="#b0">[1]</ref>, the geometry is first estimated based on the detected landmarks, and then the albedo and the lighting are iteratively estimated by solving the rendering equation. However, since the landmark constraint is a sparse constraint, the reconstructed geometry may not fit the face image well. <ref type="bibr" target="#b17">[18]</ref> fits a 3D face in a multi-layer approach and extracts a high-fidelity parameterized 3D rig that contains a generative wrinkle formation model capturing the person-specific idiosyncrasies. <ref type="bibr" target="#b2">[3]</ref> presents an algorithm for fully automatically fitting a 3D Morphable Model to a single image using landmarks and edge features. <ref type="bibr" target="#b45">[46]</ref> introduces a framework to fit a parametric face model with Bayesian inference. <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref> estimate an occlusion map and fit a statistical model to a face image with an EM-like probabilistic estimation process. <ref type="bibr" target="#b25">[26]</ref> adopts the similar approach to recover the 3D face model with geometry details. While these methods provide impressive results, they are usually time-consuming due to complex optimization.</p><p>Face Capture from RGB Videos. Recently, a variety of methods have been proposed to do 3D face reconstruction with monocular RGB video. Most of them use a 3D Morphable Model <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref> or a multi-linear face model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b44">[45]</ref> as a prior. <ref type="bibr" target="#b14">[15]</ref> reconstructs the dense 3D face from a monocular video sequence by a variational approach, which is formulated as estimating dense low-rank smooth 3D shapes for each frame of the video sequence. <ref type="bibr" target="#b16">[17]</ref> adapts a generic template to a static 3D scan of an actor's face, then fits the blendshape model to monocular video off-line, and finally extracts surface detail by shading-based shape refinement under general lighting. <ref type="bibr" target="#b47">[48]</ref> uses a similar tracking approach and achieves impressive results based on global energy optimization of a set of selected keyframes. <ref type="bibr" target="#b17">[18]</ref> fits a 3D face in a multilayer approach and extracts a high-fidelity parameterized 3D rig that contains a generative wrinkle formation model capturing the person-specific idiosyncrasies. Although all these methods provide impressive results, they are time-consuming and are not suitable for real-time face video reconstruction and editing. <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref> adopt a learning-based regression model to fit a generic identity and expression model to a RGB face video in realtime and <ref type="bibr" target="#b6">[7]</ref> extends this approach by also regressing fine-scale face wrinkles. <ref type="bibr" target="#b44">[45]</ref> presents a method for unconstrained realtime 3D facial performance capture through explicit semantic segmentation in the RGB input. <ref type="bibr" target="#b21">[22]</ref> tracks face by fitting 3D Morphable Model to the detected landmarks. Although they are able to reconstruct and track 3D face in real-time, they do not estimate facial appearance. Recently, <ref type="bibr" target="#b52">[53]</ref> presented an approach for real-time face tracking and facial reenactment, but the method is not able to recover fine-scale details and requires external landmark inputs. In contrast, our method is the first work that can do real-time reconstruction of face geometry at fine details as well as real-time recovery of albedo, lighting and pose parameters.</p><p>Learning-based Single-image 3D Face Reconstruction. With the powerfulness of convolution neural networks, deep learning based methods have been proposed to do 3D face reconstruction from one single image. <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b30">[31]</ref> use 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref> to represent 3D faces and use CNN to learn the 3DMM and pose parameters. <ref type="bibr" target="#b40">[41]</ref> follows the method and uses synthetic face images generated by rendering textured 3D faces encoded on 3DMM with random lighting and pose for training data. However, the reconstruction results of these methods do not contain geometry details. Besides learning the 3DMM and pose parameters, <ref type="bibr" target="#b41">[42]</ref> extends these methods by also learning detailed geometry in an unsupervised manner. <ref type="bibr" target="#b53">[54]</ref> proposes to regress robust and discriminative 3DMM with a very deep neural network and uses it for face recognition. <ref type="bibr" target="#b50">[51]</ref> proposes to use an analysis-by-synthesis energy function as the loss function during network training <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref>. <ref type="bibr" target="#b23">[24]</ref> proposes to directly regress volumes with CNN for a single face image. Although these methods utilize the powerfulness of CNNs, they all concentrate on images and do not account for videos. In comparison, we focus on monocular face video input and reconstruct face video in real-time by using CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FACE RENDERING PROCESS</head><p>This section describes some background information, particularly on the face representations and the face rendering process considered in our work. The rendering process of a face image depends on several factors: face geometry, albedo, lighting, pose and camera parameters. We encode 3D face geometry into two layers: a coarse-scale shape and fine-scale details. While the coarse-scale shape and albedo are represented by a parametric textured 3D face model, the fine-scale details are represented by a pixel depth displacement map. The face shape is represented via a mesh of n vertices with fixed connectivity as a vector</p><formula xml:id="formula_0">p = [p T 1 , p T 2 , . . . , p T n ] T ? R 3n , where p i denotes the position of vertex v i (i = 1, 2, . . . , n).</formula><p>Parametric face model. We use 3D Morphable Model (3DMM) <ref type="bibr" target="#b3">[4]</ref> as the parametric face model to encode 3D face geometry and albedo on a lower-dimensional subspace, and extend the shape model to also cover facial expressions by adding delta blendshapes. Specifically, the parametric face model describes 3D face geometry p and albedo b with PCA (principle component analysis):</p><formula xml:id="formula_1">p = p + A id ? id + A exp ? exp ,<label>(1) b</label></formula><formula xml:id="formula_2">= b + A alb ? alb ,<label>(2)</label></formula><p>where p and b denote respectively the shape and the albedo of the average 3D face, A id and A alb are the principle axes extracted from a set of textured 3D meshes with a neutral expression, A exp represents the principle axes trained on the offsets between the expression meshes and the neutral meshes of individual persons, and ? id , ? exp and ? alb are the corresponding coefficient vectors that characterize a specific 3D face model. For diversity and mutual complement, we use the Basel Face Model (BFM) <ref type="bibr" target="#b36">[37]</ref> for A id and A alb and FaceWarehouse <ref type="bibr" target="#b9">[10]</ref> for A exp .</p><p>Fine-scale details. As 3DMM is a low-dimensional model, some face details such as wrinkles and dimples cannot be expressed by 3DMM. Thus, we encode the geometry details in a displacement along the depth direction for each pixel.</p><p>Rendering process. For camera parametrization, following <ref type="bibr" target="#b41">[42]</ref>, we use the weak perspective model to project the 3D face onto the image plane:</p><formula xml:id="formula_3">q i = s 1 0 0 0 1 0 Rp i + t,<label>(3)</label></formula><p>where p i and q i are the locations of vertex v i in the world coordinate system and in the image plane, respectively, s is the scale factor, R is the rotation matrix constructed from Euler angles pitch, yaw, roll and t = (t x , t y ) T is the translation vector.</p><p>To model the scene lighting, we assume the face to be a Lambertian surface. The global illumination is approximated using the spherical harmonics (SH) basis functions <ref type="bibr" target="#b34">[35]</ref>. Then, the irradiance of a vertex v i with surface normal n i and scalar albedo b i is expressed as <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_4">L(n i , b i | ?) = b i ? B 2 k=1 ? k ? k (n i ),<label>(4)</label></formula><p>where Given the parametric face model and the parameter set ?, a face image can be rendered as follows. First, a textured 3D mesh is constructed using Eq. ( <ref type="formula" target="#formula_1">1</ref>) and Eq. ( <ref type="formula" target="#formula_2">2</ref>). Then we do a rasterization via Eq. ( <ref type="formula" target="#formula_3">3</ref>). Particularly, in the rasterization, for every pixel in the face region of the 2D image, we obtain the underlying triangle index on the 3D mesh and its barycentric coordinates. In this way, for every pixel in the face region, we obtain its normal by using the underlying triangle's normal, and its albedo value by barycentrically interpolating the albedos of the vertices of the underlying triangle. Finally, with the normal, the albedo and the lighting, the color of a pixel can be rendered using Eq. (4).</p><formula xml:id="formula_5">?(n i ) = [? 1 (n i ), . . . , ? B 2 (n i )] T is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERVIEW OF PROPOSED LEARNING-BASED DENSE FACE RECONSTRUCTION</head><p>Figure <ref type="figure" target="#fig_1">1</ref>: The pipeline of our proposed learning based dense 3D face reconstruction and tracking framework. The first frame of the input video is initially reconstructed by a single-image CoarseNet for coarse face geometry reconstruction, followed by using FineNet for detailed face geometry recovery. Each of the subsequent frames is processed by a tracking CoarseNet followed by FineNet.</p><p>To achieve real-time face video reconstruction and tracking, we need real-time face inverse rendering. However, recon-structing detailed 3D face using traditional optimization-based methods <ref type="bibr" target="#b17">[18]</ref> is far from real-time. To address this problem, we develop a novel CNN based framework to achieve realtime detailed face inverse rendering. Specifically, we use two CNNs for each frame, namely CoarseNet and FineNet. The first one estimates coarse-scale geometry, albedo, lighting and pose parameters altogether, and the second one reconstructs the fine-scale geometry encoded on pixel level. We would like to point out that although we advocate the CNN based solution, it still needs to work together with optimization based inverse rendering methods. This is because CNN requires large amount of data with labels, which is usually not available, and optimization based inverse rendering methods are a natural solution for generating labels (optimal parameters) and synthesizing new images offline. Thus, our proposed dense face reconstruction and tracking framework includes both optimization based inverse rendering and the two-stage CNN based solution, where the former is for offline training data generation and the latter is for real-time online operations. In the subsequent sections, we first introduce our optimization based inverse face rendering, which will be used to construct training data for CoarseNet and FineNet; and then we present our three convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OPTIMIZATION BASED FACE INVERSE RENDERING</head><p>Inverse rendering is an inverse process of image generation. That is, given a face image, we want to estimate a 3D face with albedo, lighting condition, pose and projection parameters simultaneously. Since directly estimating these unknowns with only one input image is an ill-posed problem, we use the parametric face model as a prior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stage 1 -Model Fitting</head><p>The purpose of model fitting is to estimate the coarse face geometry, albedo, lighting, pose and projection parameters from a face image I in . That is to estimate ? = {? id , ? exp , ? alb , s, pitch, yaw, roll, t, r}. For convenience, we group these parameters into the following sets ? g = {? id , ? exp }, ? p = {pitch, yaw, roll}, ? t = {s, t} and ? l = {? alb , r}. The fitting process is based on the analysisby-synthesis strategy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref>, and we seek a solution that by minimizes the difference between the input face image and the rendered image with ?. Specifically, we minimize the following objective function:</p><formula xml:id="formula_6">E(?) = E con + w l E lan + w r E reg ,<label>(5)</label></formula><p>where E con is a photo-consistency term, E lan is a landmark term and E reg is a regularization term, and w l and w r are tradeoff parameters. The photo-consistency term, aiming to minimize the difference between the input face image and the rendered image, is defined as</p><formula xml:id="formula_7">E con (?) = 1 |F| I ren -I in 2 ,<label>(6)</label></formula><p>where I ren is the rendered image, I in is the input image, and F is the set of all pixels in the face region. The landmark term aims to make the projected vertices close to the corresponding landmarks in the image plane:</p><formula xml:id="formula_8">E lan (?) = 1 |L| i?L q i -(?Rp i + t) 2 , (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where L is the set of landmarks, q i is a landmark position in the image plane, p i is the corresponding vertex location in the fitted 3D face and ? = s 1 0 0 0 1 0 . The regularization term aims to ensure that the fitted parametric face model parameters are plausible:</p><formula xml:id="formula_10">E reg (?) = 100 i=1 ? id,i ? id,i 2 + ? alb,i ? alb,i 2 + 79 i=1 ? exp,i ? exp,i 2 , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where ? is the standard deviation of the corresponding principal direction. Here we use 100 principle components for identity &amp; albedo, and 79 for expression. In our experiments, we set w l to be 10 and w r to be 5 ? 10 -5 . Eq. ( <ref type="formula" target="#formula_6">5</ref>) is minimized via Gauss-Newton iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stage 2 -Geometry Refinement</head><p>As the parametric face model is a low-dimensional model, some face details such as wrinkles and dimples are not encoded in parametric face model. Thus, the purpose of the second stage is to refine the geometry by adding the geometry details in a displacement along the depth direction for every pixel. In particular, by projecting the fitted 3D face with parameter ?, we can obtain a depth value for every pixel in the face region. Let z be all stacked depth values of pixels, d be all stacked displacements and z = z + d be all new depth values. Given new depth values z, the normal at pixel (i, j) can be computed using the normal of triangle (p (i,j) , p (i+1,j) , p (i,j+1) ), where p (i,j) = [i, j, z(i, j)] T is the coordinates of pixel (i, j) at the camera system. Inspired by <ref type="bibr" target="#b22">[23]</ref>, we estimate d using the following objective function:</p><formula xml:id="formula_12">E(d) = E con + ? 1 d 2 2 + ? 2 d 1 ,<label>(9)</label></formula><p>where E con is the same as that in Eq. ( <ref type="formula" target="#formula_6">5</ref>), d 2 2 is to encourage small displacements, the Laplacian of displacements d is to make the displacement smooth, and ? 1 and ? 2 are tradeoff parameters. We use 1 norm for the smooth term as it allows preserving sharp discontinuities while removing noise. We set ? 1 to be 1 ? 10 -3 and ? 2 to be 0.3 in our experiments. Eq. ( <ref type="formula" target="#formula_12">9</ref>) is minimized by using an iterative reweighing approach <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stage 3 -Albedo Blending</head><p>Similar to the geometry, the albedo encoded in the parametric face model (denoted as b c ) in stage 1 is also smooth because of the low dimension. For photo-realistic rendering, we extract a fine-scale albedo as  <ref type="formula" target="#formula_4">4</ref>) for our subsequent data generation process.</p><formula xml:id="formula_13">b f = I in ./(r T ?(n)),<label>(10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SINGLE-IMAGE COARSENET FOR COARSE RECONSTRUCTION FROM A SINGLE IMAGE</head><p>In this section, we describe how to train a coarse-layer CNN (called Single-image CoarseNet) that can output the parametric face model parameters (corresponding to a coarse shape) and the pose parameters from the input of a single face image or an independent video frame. Although the network structure of Single-image CoarseNet is similar to that of <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b41">[42]</ref>, we use our uniquely constructed training data and loss function, which are elaborated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constructing Single-image CoarseNet Training Data</head><p>To train Single-image CoarseNet, we need a large-scale dataset of face images with ground-truth 3DMM parameters and pose parameters. Recently, <ref type="bibr" target="#b59">[60]</ref> proposed to synthesize a large number of face images by varying the 3DMM parameters fitted from a small number of real face images. <ref type="bibr" target="#b59">[60]</ref> focuses on the face alignment problem. The color of the synthesized face images are directly copied from the source images without considering the underlying rendering process, which makes the synthesized images not photo-realistic and thus unsuitable for high-quality 3D face reconstruction. Later, <ref type="bibr" target="#b41">[42]</ref> follows the idea of using synthetic data for learning detailed 3D face reconstruction and directly renders a large number of face images by varying the existing 3DMM parameters with random texture, lighting, and reflectance. However, since 3DMM is a low-dimensional model and the albedo is also of low frequency, the synthetic images in <ref type="bibr" target="#b41">[42]</ref> are not photo-realistic as well, not to mention the random background used in the rendered images. In addition, the synthetic images in <ref type="bibr" target="#b41">[42]</ref> are not available to the public.</p><p>Therefore, in this paper, we propose to use our developed inverse rendering described in Sec. V to synthesize photorealistic images at large scale, which well addresses the shortcoming of the synthetic face images generated in <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In particular, we choose 4000 face images (dataset A), in which faces are not occluded, from 300W <ref type="bibr" target="#b43">[44]</ref> and Multipie <ref type="bibr" target="#b18">[19]</ref>. For each of the 4000 images, we use our optimization based inverse rendering method to obtain the parameter set ?. Then, to make our coarse-layer network robust to expression and pose, we render new face images by randomly changing the pose parameters ? p and the expression parameter ? exp , each of which leads to a new parameter set ?. By doing the rasterization with ?, we can obtain the normals of all pixels in the new face region as described in Sec. III. With these normals and the albedos obtained according to Sec. V-C, a new face is then rendered using Eq. ( <ref type="formula" target="#formula_4">4</ref>). We also warp the background region of the source image to fit the new face region by using the image meshing <ref type="bibr" target="#b59">[60]</ref>. Fig. <ref type="figure" target="#fig_4">3</ref> shows an example of generating three synthetic images from an input real images by simultaneously changing the expression and pose parameters. In this way, we generate a synthetic dataset of totally 80,000 face images for the Single-image CoarseNet training by randomly varying the expression and the pose parameters 20 times for each of the 4000 real face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single-image CoarseNet</head><p>The input to our Single-image CoarseNet is a face image, and the output is the parameters related to the shape of 3D face and the projection, i.e. T = {? id , ? exp , s, pitch, yaw, roll, t x , t y }. The network is based on the Resnet-18 <ref type="bibr" target="#b19">[20]</ref> with the modification of changing the output number of the fully-connected layer to 185 (100 for identity, 79 for expression, 3 for rotation, 2 for translation and 1 for scale). The input image size is 224 ? 224.</p><p>As pointed out in <ref type="bibr" target="#b59">[60]</ref>, different parameters in T have different influence to the estimated geometry. Direct MSE (mean square error) loss on T might not lead to good geometry reconstruction. <ref type="bibr" target="#b59">[60]</ref> uses a weighted MSE loss, where the weights are based on the projected vertex distances. <ref type="bibr" target="#b41">[42]</ref> uses 3D vertex distances to measure the loss from the geometry parameters and MSE for the pose parameters. Considering these vertex based distance measures are calculated on the vertex grid, which might not well measure how the parameters fit the input face image, in this work we use a loss function that computes the distance between the ground-truth parameters T g and the network output parameters T n at the per-pixel level.</p><p>In particular, we first do the rasterization with the groundtruth parameters T g to get the underlying triangle index and the barycentric coordinates for each pixel in the face region. With this information, we then construct the pixels' 3D average pq , base A q,id and base A q,exp by barycentrically interpolating the corresponding rows in p, A id , A exp , respectively. In this way, given parameters T , we can project all the corresponding 3D locations of the pixels onto the image plane using P roj(T ) = ?R(p q + A q,id ? id + A q,exp ? exp ) + t. <ref type="bibr" target="#b10">(11)</ref> Then the loss between the ground-truth parameters T g and the network output parameters T n is defined as:</p><formula xml:id="formula_14">D(T g , T n ) = P roj(T g ) -P roj(T n ) 2 2 . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>Note that there is no need to compute P roj(T g ) since it corresponds to the original pixel locations in the image plane.</p><p>For better convergence, we further separate the loss in Eq. ( <ref type="formula" target="#formula_14">12</ref>) into the pose-dependent loss as</p><formula xml:id="formula_16">L pose = P roj(T g ) -P roj(T n,pose , T g,geo ) 2 2 ,<label>(13)</label></formula><p>where T pose = ? p ? ? t represents the pose parameters, and the geometry-dependent loss as</p><formula xml:id="formula_17">L geo = P roj(T g ) -P roj(T n,geo , T g,pose ) 2 2 ,<label>(14)</label></formula><p>where T geo = ? g represents the geometry parameters. In Eq. ( <ref type="formula" target="#formula_16">13</ref>) and Eq. ( <ref type="formula" target="#formula_17">14</ref>), P roj(T n,pose , T g,geo ) (resp., P roj(T n,geo , T g,pose )) refers to the projection with the groundtruth geometry (resp., pose) parameters and the network estimated pose (resp., geometry) parameters. The final loss is a weighted sum of the two losses:</p><formula xml:id="formula_18">L = w ? L pose + (1 -w) ? L geo ,<label>(15)</label></formula><p>where w the tradeoff parameter. We set w =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lgeo</head><p>Lpose+Lgeo for balancing the two losses and we assume is a constant when computing the derivatives for back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. TRACKING COARSENET FOR COARSE RECONSTRUCTION FROM MONOCULAR VIDEO</head><p>The purpose of Tracking CoarseNet is to predict the current frame's parameters, given not only the current video frame but also the previous frame's parameters. As there does not exist large-scale dataset that captures the correlations among adjacent video frames, our Tracking CoarseNet also faces the problem of no sufficient well-labelled training data. Similarly, we synthesize training data for Tracking CoarseNet. However, it is non-trivial to reuse the (k -1)-th frame's parameters to predict k-th frame's parameters. Directly using all the previous frame's parameters as the input to Tracking CoarseNet will introduce too many uncertainties during training, which results in huge complexity in synthesizing adjacent video frames for training, and make the training hard to converge and the testing unstable. Through vast experiments, we find that only utilizing the previous frame's pose parameters is a good way to inherit the coherence while keeping the network trainable and stable.</p><p>Specifically, the input to the tracking network is the k-th face frame cropped by the k -1 frame's landmarks and a Projected Normalized Coordinate Code (PNCC) <ref type="bibr" target="#b59">[60]</ref> rendered using the k -1 frame's pose parameters ? k-1 p , ? k-1 t and the mean 3D face p in Eq. ( <ref type="formula" target="#formula_1">1</ref>). The output of the tracking network is parameters</p><formula xml:id="formula_19">T k = {? k id , ? k exp , ? k alb , ? k (s), ? k (pitch), ? k (yaw),</formula><p>? k (roll), ? k (t), r k }, where ?(?) denotes the difference between the current frame and the previous frame. Note that here the output also includes albedo and lighting parameters, which could be used for different video editing applications.</p><p>The network structure is the same as Single-image CoarseNet except that the output number of the fully-connected layer is 312 (100 for identity, 79 for expression, 3 for rotation, 2 for translation, 1 for scale, 100 for albedo and 27 for lighting coefficients). In addition to the loss terms L pose and L geo defined in Eq. ( <ref type="formula" target="#formula_16">13</ref>) and Eq. ( <ref type="formula" target="#formula_17">14</ref>) respectively, Tracking CoarseNet also uses another term for ? k alb and r k that measures the distance between the rendered image and the input frame:</p><formula xml:id="formula_20">L col = I k ren (? k alb , r k ) -I k in 2 2 , (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where I k ren (? k alb , r k ) is the rendered face image with the groundtruth geometry and pose, and the estimated albedo and lighting, and I k in is the input face frame. In this way, the final total loss becomes a weighted sum of the three losses:</p><formula xml:id="formula_22">L = w 1 ? L pose + w 2 ? L geo + (1 -w 1 -w 2 ) ? L col , (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>where w 1 = Lgeo+Lcol 2(Lpose+Lgeo+Lcol) and w 2 = Lpos+Lcol 2(Lpose+Lgeo+Lcol) are the tradeoff parameters to balance the three losses, and we assume w 1 and w 2 are constant when computing the derivatives for back propagation.</p><p>Training data generation for Tracking CoarseNet. To train Tracking CoarseNet, large-scale adjacent video frame pairs with ground-truth parameters ? are needed as training data. Again, there is no such public dataset. To address this problem, we propose to simulate adjacent video frames, i.e., to generate the previous frame for each of the 80,000 synthesized images used in the Single-image CoarseNet training. Randomly varying the parameter set ? for a training image does not capture the tight correlations among adjacent frames. Thus, we propose to do simulation by analysing the distribution of the previous frame's parameters ? k-1 given the current k-th frame from real videos. Considering our tracking network only makes use of the previous frame's pose parameters, we just need to obtain the distribution of ? k-1 p and ? k-1 t given ? k p and ? k t . Particularly, we assume each parameter in</p><formula xml:id="formula_24">? k (? p ) = ? k-1 p -? k p and ? k (? t ) = ? k-1 t -? k</formula><p>t follows normal distribution. We extract about 160,000 adjacent frame pairs from the 300-VW video dataset <ref type="bibr" target="#b46">[47]</ref> and use our Single-image CoarseNet to get the parameters for fitting the normal distribution. Finally, for each of the 80,000 synthesized images, we can simulate its previous frame by generating ? k-1 p and ? k-1 t according to the obtained normal distribution. Examples of several simulated pairs with the previous frame's PNCC and the current image are shown in Fig. <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. FINENET FOR FINE-SCALE GEOMETRY RECONSTRUCTION</head><p>In this section, we present our solution on how to train a finelayer CNN (called FineNet). The input to FineNet is a coarse depth map stacked with the face image. The coarse depth map is generated by using the method described in Sec. V-B with the parameters T estimated by either Single-image CoarseNet or Tracking CoarseNet. The output of our FineNet is a per-pixel displacement map. Again, the key challenge here is that there is no fine-scale face dataset available that can provide a large number of detailed face geometries with their corresponding 2D images, as pointed out in <ref type="bibr" target="#b41">[42]</ref>. In addition, the existing morphable face models such as 3DMM cannot capture the finescale face details. <ref type="bibr" target="#b41">[42]</ref> bypasses this challenge by converting the problem into an unsupervised setting, i.e. relating the output depth map to the 2D image by using the shading energy as the loss function. However, to make the back-propagation trackable under the shading energy, they have to use first-order spherical harmonics to model the lighting, which is not accurate.</p><p>In our work, instead of doing unsupervised training <ref type="bibr" target="#b41">[42]</ref>, we go for fully supervised training of FineNet, i.e. directly constructing a large-scale detailed face dataset based on our developed inverse rendering and a novel face detail transfer approach, will be elaborated below. Note that our FineNet architecture is based on the U-Net <ref type="bibr" target="#b42">[43]</ref> and we use Euclidean distance as the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constructing FineNet Training Data</head><p>Our synthesized training data for FineNet is generated by transferring the displacement map from a source face image with fine-scale details such as wrinkles and folds to other target face images without the details. Fig. <ref type="figure" target="#fig_6">5</ref> gives such an example. In particular, we first apply our developed inverse rendering in Sec. V on both images. Then we find correspondences between the source image pixels and the target image pixels using the rasterization information described in Sec. III. That is, for a pixel (i, j) in the target face region, if its underlying triangle is visible in the source image, we find its corresponding 3D location on the target 3D mesh by barycentric interpolation, and then we project the 3D location onto the source image plane using Eq. ( <ref type="formula" target="#formula_3">3</ref>) to get the corresponding pixel (i , j ). With these correspondences, the original source displacement d s and the original target displacement d t , a new displacement d t for the target image is generated by matching its gradients with the scaled source displacement gradient in the intersected region ? by solving the following poisson problem:</p><formula xml:id="formula_25">min dt (i,j)?? ? d t (i, j) -w(i, j) 2 , s.t. d t (i, j) = d t (i, j) (i, j) ? ??<label>(18)</label></formula><p>where w(i, j) = s d [d s (i + 1, j ) -d s (i , j ), d s (i , j + 1)d s (i , j )] T and s d is a scale factor within the range [0.7, 1.3] so as to create different displacement fields. After that, we add d t into the coarse target depth z to get the final depth map. Then the normals of the target face pixels are updated as in Sec. V-B. With the updated normals, a new face image is rendered using Eq. ( <ref type="formula" target="#formula_4">4</ref>). We would like to point out that besides generating a large number of detailed face images to train the network, there are also other benefits to do such detail transfer. First, by rendering the same type of detail information under different lighting conditions, we can train our FineNet to be robust to lighting. Second, by changing the scale of the displacement randomly, our method can be trained to be robust to different scales of details.</p><p>For the details of the dataset construction, we first download 1000 real face images (dataset B) that contain rich geometry details from internet. Then, we transfer the details from dataset B to the 4000 real face images in dataset A, the one used in constructing synthetic data for Single-image CoarseNet. For every image in A, we randomly choose 30 images in B for transferring. In this way, we construct a synthesized fine-detailed face image dataset of totally 120,000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. EXPERIMENTS</head><p>In this section, we conduct qualitative and quantitative evaluation on the proposed detailed 3D face reconstruction and tracking framework and compare it with the state-of-theart methods. (a gray image and its coarse depth). We train all the networks using Adam solver with the mini-batch size of 100 and 30k iterations. The base learning rate is set to be 0.00005.</p><p>The CNN based 3D face reconstruction and tracking are implemented in C++ and tested on various face images and videos. All experiments were conducted on a desktop PC with a quad-core Intel CPU i7, 4GB RAM and NVIDIA GTX 1070 GPU. As for the running time for each frame, it takes 5 ms for CoarseNet and 15 ms for FineNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results of Dense 3D Face Reconstruction from Monocular Video</head><p>CoarseNet vs. FineNet. Our approach is to progressively and continuously estimate the detailed facial geometry, albedo and lighting parameters from a monocular face video. Fig. <ref type="figure" target="#fig_7">6</ref> shows the tracking output results of the two stages. The results of CoarseNet include the smooth geometry and the corresponding rendered face image shown in the middle column. The FineNet further predicts the pixel level displacement given in the last column. We can see that CoarseNet produces smooth geometry and well matched rendered face images, which show the good recovery of pose, albedo, lighting and projection parameters, and FineNet nicely recovers the geometry details such as wrinkles. A complete reconstruction results of all the video frames are given in the accompanying video or via the link: https://youtu.be/dghlMXxD-rk.</p><p>Single-image CoarseNet vs. Tracking CoarseNet. Given a RGB video, a straightforward way for dense face tracking is to treat all frames as independent face images, and apply our Single-image on each frame, followed by applying FineNet. Thus, we give a comparison of our proposed Tracking CoarseNet, which estimates the differences of the pose parameters w.r.t. the previous frame, with the baseline that simply uses our Single-image CoarseNet on each frame. As demonstrated in Fig. <ref type="figure" target="#fig_8">7</ref>, Tracking CoarseNet achieves more robust tracking than the baseline, since it well utilizes the guidance from the previous frame's pose.</p><p>Comparisons with dense face tracking methods. We compare our method with the state-of-the-art monocular video based dense face tracking methods <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr" target="#b47">[48]</ref> performs 3D face reconstruction in an iterative manner. In each iteration, they first reconstruct coarse-scale facial geometry from sparse facial features and then refine the geometry via shape from shading. <ref type="bibr" target="#b17">[18]</ref> employs a multi-layer approach to reconstruct fine-scale details. They encode different scales of 3D face geometry on three different layers and do optimization for each layer. <ref type="bibr" target="#b21">[22]</ref> reconstructs the 3D face shape by only fitting the 2D landmarks via 3DMM, and we can observe that <ref type="bibr" target="#b21">[22]</ref> can only produce smooth face reconstruction. As shown in Fig. <ref type="figure">8</ref>, our method produces visually better results compared to <ref type="bibr" target="#b21">[22]</ref>, and comparable results compared to <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b47">[48]</ref>.</p><p>Different from optimization based methods, our learning based approach is much faster while obtaining comparable or better results. Our method is several orders of magnitude faster than the state-of-the-art optimization-based approach <ref type="bibr" target="#b17">[18]</ref>, i.e., 5 ms for CoarseNet and 15 ms for FineNet with our hardware setting, while 175.5s reported in their paper <ref type="bibr" target="#b17">[18]</ref>. It needs to be pointed out that the existing optimization based dense tracking methods need facial landmark constraints. Therefore, they might not reconstruct well for faces with large poses and extreme expressions. On the other hand, we do large-scale photo-realistic image synthesis that includes many challenging Input <ref type="bibr" target="#b47">[48]</ref>(-) <ref type="bibr" target="#b17">[18]</ref>(175.5s) <ref type="bibr" target="#b21">[22]</ref>(100ms) Ours(25ms)</p><p>Figure <ref type="figure">8</ref>: Comparisons with the state-of-art dense face tracking methods <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The average computation time for each frame is given in the bracket. <ref type="bibr" target="#b47">[48]</ref> does not report the running time of their method, while it should take much longer time than ours since it iteratively solves several complex optimization problems. data with well labelled parameters, and thus we can handle those challenging cases as demonstrated in Fig. <ref type="figure" target="#fig_9">9</ref>.</p><p>Quantitative results of face reconstruction from monocular video. For quantitative evaluation, we test on the FaceCap dataset <ref type="bibr" target="#b54">[55]</ref>. The dataset consists of 200 frames along with 3D meshes constructed using the binocular approach. We compare our proposed inverse rendering approach and our learning based solutions including Tracking CoarseNet and Tracking CoarseNet+FineNet. For each method, we register the depth cloud to the groundtruth 3D mesh and compare point to point distance. Table <ref type="table" target="#tab_1">I</ref> shows the average pointto-point distance results. It can be seen that our proposed inverse rendering achieves an average distance of 1.81mm, which is quite accurate. It demonstrates the suitability of  From top to bottom: input face video frame and groundtruth mesh in dataset <ref type="bibr" target="#b54">[55]</ref>, results of the inverse rendering approach, results of our learning based dense face tracking solution.</p><p>using the inverse rendering results for constructing the training data. On the other hand, our CoarseNet+FineNet achieves an average distance of 2.08mm, which is comparable to that of the inverse rendering but with much faster processing speed (25ms vs 8s per frame). Some samples are shown in Fig. <ref type="figure" target="#fig_10">10</ref>.</p><p>In addition, the reconstruction accuracy by CoarseNet+FineNet outperforms the one by CoarseNet alone. Since the face region containing wrinkles is only a small part of the whole face region, the difference is not significant since the accuracy statistics is computed over a large face region. By comparing the reconstruction accuracy on a small region that contains wrinkles, the improvement is more obvious, as shown in Fig. <ref type="figure" target="#fig_1">11</ref>.</p><p>For the quantitative comparison with the state-of-the-art monocular video based face tracking method <ref type="bibr" target="#b17">[18]</ref>, we evaluate Figure <ref type="figure" target="#fig_1">11</ref>: Comparison of our CoarseNet and FineNet on a small region that is rich of wrinkles. On the left is the input frame, on the top are results of CoarseNet, on the bottom are results of FineNet. On the subregion, the mean error is 2.20mm for CoarseNet and 2.03mm for FineNet.</p><p>Input Stereo <ref type="bibr" target="#b54">[55]</ref> [18] Ours Figure <ref type="figure" target="#fig_2">12:</ref> The reconstruction accuracy comparison. The reconstruction quality of our dense face tracking method is comparable to the optimization based method <ref type="bibr" target="#b17">[18]</ref> but with much faster processing speed. The groundtruth mesh is constructed using the binocular approach <ref type="bibr" target="#b54">[55]</ref>.</p><p>Figure <ref type="figure" target="#fig_4">13</ref>: For each pair, on the left is the input face image; on the right is the projected 3D mesh reconstructed by our single-image based solution. The first, second and third rows respectively demonstrate that our method is robust to large poses, extreme expressions and different types and scales of wrinkles.</p><p>the geometric accuracy of the reconstruction of a video frame with rich face details (note that <ref type="bibr" target="#b17">[18]</ref> did not provide the results for the entire video). Fig. <ref type="figure" target="#fig_2">12</ref> shows the results, where our method achieves a mean error of 1.96mm compared to the groundtruth 3D face shape generated by the binocular facial performance capture proposed in <ref type="bibr" target="#b54">[55]</ref>. We can see that the result of our learning based face tracking method is quite close to the groundtruth, and is comparable (1.96mm vs. 1.8mm) to that of the complex optimization based approach <ref type="bibr" target="#b17">[18]</ref> but with much faster processing speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of Dense 3D Face Reconstruction from A Single Image</head><p>Visual results of our single-image based reconstruction. To evaluate the single-image based reconstruction performance, we show the reconstruction results of our method (Single-image CoarseNet+FineNet) on some images from AFLW <ref type="bibr" target="#b31">[32]</ref> dataset, VGG-Face dataset <ref type="bibr" target="#b35">[36]</ref> and some face images downloaded from internet. The three rows in Fig. <ref type="figure" target="#fig_4">13</ref> from top to bottom respectively show the projected 3D meshes reconstructed by our method under large poses, extreme expressions and face images with detailed wrinkles, which demonstrate that our method is robust to all of them. Figure <ref type="figure" target="#fig_5">14</ref>: From left to right: input face image with detected landmarks, geometry reconstructed by inverse rendering, geometry reconstructed by our learning method. It can be seen that our inverse rendering approach fails to recover the face shape as the landmarks are not accurate. On the other hand, our proposed learning-based approach recovers the face shape well. Comparisons with inverse rendering. Similar to the video input scenario, directly using our developed inverse rendering approach can also reconstruct detailed geometries from a single image, but our learning-based method does provide some advantages. First, unlike the inverse rendering approach, our learning-based method does not need face alignment information. Therefore, the learning-based method is more robust to input face image with large pose, as shown in Fig. <ref type="figure" target="#fig_5">14</ref>. Second, once the two CNNs are trained, our learning method is much faster to reconstruct a face geometry from a single input image. Third, as we render the same type of wrinkles under different lightings and directly learn the geometry in a supervised manner, our method is more robust to lighting, as illustrated in Fig. <ref type="figure" target="#fig_11">15</ref>. The reason why the learning based method can do better in these scenarios lies in the large numbers of diverse training data we construct, which facilitate the learning of the two networks, while the inverse rendering approach only explores the information from each single image.</p><p>Comparisons with state-of-the-art single-image based face reconstruction. We compare our method with <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> on single-image based face reconstruction. We thank the authors of <ref type="bibr" target="#b41">[42]</ref> for providing us the same 11 images listed in <ref type="bibr" target="#b41">[42]</ref>, as well as their results of another 8 images supplied by us. We show the reconstruction results of 4 images in Fig. <ref type="figure" target="#fig_7">16</ref> and the full comparisons on all the 19 images are given in the accompanying material. It can be observed that our method produces more convincing reconstruction results in both the global geometry (see the mouth regions) and the fine-scale details (see the forehead regions). The reconstruction results of the methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> are generated using the source codes provided by the authors 2345 . The reasons why our method produces better results than <ref type="bibr" target="#b41">[42]</ref> are threefold: 1) For CoarseNet training, <ref type="bibr" target="#b41">[42]</ref> only renders face region and uses random background, while our rendering is based on real images and the synthesized images are more photo-realistic. For FineNet training, we render images with fine-scale details, and train FineNet in a supervised manner, while <ref type="bibr" target="#b41">[42]</ref> trains FineNet in an unsupervised manner. 2) For easy back propagation, <ref type="bibr" target="#b41">[42]</ref> adopts the first-order spherical harmonics (SH) to model lighting, while we use the secondorder SH, which can reconstruct more accurate geometry details. 3) Our proposed loss function in CoarseNet better fits the goal and calculating the parameters in pixel level can achieve more stable and faster convergence. We did an experiment to compare our loss function L in Eq. ( <ref type="formula" target="#formula_22">17</ref>) with the one used in <ref type="bibr" target="#b41">[42]</ref>. Specifically, we used the two loss functions separately to train CoarseNet with 15000 iterations and batch size 100. Table <ref type="table" target="#tab_2">II</ref> shows the results of the test errors under different metrics on the test set (about 700 AFLW images). We can see that no matter which metric is used, either our defined metrics (L pose and L geo ), or the metrics employed in <ref type="bibr" target="#b41">[42]</ref> (MSE for pose parameters and vertex distance for geometry parameters), our method always achieves lower testing errors than <ref type="bibr" target="#b41">[42]</ref>, which demonstrates the effectiveness of the defined loss function for training.</p><p>Quantitative results of single-image based dense face reconstruction. For quantitative evaluation, we compare our method with the landmark-based method <ref type="bibr" target="#b60">[61]</ref> and the learning-based method <ref type="bibr" target="#b59">[60]</ref> on the Spring2004range subset of Face Recognition Grand Challenge dataset V2 <ref type="bibr" target="#b37">[38]</ref>. The Spring2004range has 2114 face images and their corresponding depth images. We use the face alignment method <ref type="bibr" target="#b27">[28]</ref> to detect facial landmarks as the input of <ref type="bibr" target="#b60">[61]</ref>. For comparison, we project the reconstructed 3D face on the depth image, and use both Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics to measure the difference between the reconstructed depth and the ground truth depth on the valid pixels. We discard some images in which the projected face regions are very far away from the the real face regions for any of the three methods, which leads to a final 2100 images being chosen for the comparisons. The results are shown in Table <ref type="table" target="#tab_3">III</ref>. It can be seen that our method outperforms the other two recent methods in both RMSE and MAE. The results of <ref type="bibr" target="#b60">[61]</ref> and <ref type="bibr" target="#b59">[60]</ref> are generated by directly running their released codes in public.</p><p>Figure <ref type="figure" target="#fig_7">16</ref>: Comparisons with the state-of-art methods. From the first row to the last row, it respectively shows the input images, and the results of <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b13">[14]</ref> and ours. It can be seen that our results are more convincing in both the global geometry and the fine-scale details. Note that the method of <ref type="bibr" target="#b2">[3]</ref> uses a 3DMM with identity variation only, and thus is not able to handle facial expressions well. Note that we are not able to perform a quantitative comparison with the state-of-the-art method <ref type="bibr" target="#b41">[42]</ref>, since their code is not released. Their reported MAE value for the Spring2004range dataset is lower than what we obtain in Table <ref type="table" target="#tab_3">III</ref>. We believe it is due to the masks they used in their MAE computation, which are unfortunately not available to us. Although we cannot give a quantitative comparison, the visual comparison shown in Fig. <ref type="figure" target="#fig_7">16</ref> clearly demonstrates the superior face reconstruction performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSIONS</head><p>We have presented a coarse-to-fine CNN framework for realtime textured dense 3D face reconstruction and tracking from monocular RGB video as well as from a single RGB image. The training data to our convolutional networks are constructed by the optimization based inverse rendering approach. Particularly, we construct the training data by varying the pose and expression parameters, detail transfer as well as simulating the video-type adjacent frame pairs. With the well constructed large-scale training data, our framework recovers the detailed geometry, albedo, lighting, pose and projection parameters in real-time. We believe that our well constructed datasets including 2D face images, 3D coarse face models, 3D fine-scale face models, and multi-view face images of the same person could be applied to many other face analysis problems like face pose estimation, face recognition and face normalization.</p><p>Our work has limitations. Particularly, like many recent 3D face reconstruction works <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we assume Lambertian surface reflectance and smoothly varying illumination in our inverse rendering procedure, which may lead to inaccurate fitting for face images with specular reflections or self-shadowing. It is worth to investigate more powerful formulation to handle general reflectance and illumination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the SH basis functions computed with normal n i , and ? = [? 1 , . . . , ? B 2 ] T is the SH coefficients. We use the first B = 3 bands of SHs for the illumination model. Thus, the rendering process depends on the parameter set ? = {? id , ? exp , ? alb , s, pitch, yaw, roll, t, r}, where r = (? T r , ? T g , ? T b ) T denotes RGB channels' SH illumination coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>shows the entire system pipeline. It can be seen that there are two types of CoarseNet: Single-image CoarseNet and Tracking CoarseNet. Tracking CoarseNet makes use of the predicted parameters of the previous frame, while Singleimage CoarseNet is for the first frame case where there is no previous frame available. Such Single-image CoarseNet could be applied to other key frames as well to avoid any potential drifting problem if needed. The combination of all the networks including Single-image CoarseNet, Tracking CoarseNet and FineNet, makes up a complete framework for real-time dense 3D face reconstruction from monocular video. Note that the entire framework can be easily degenerated to the solution for dense 3D face reconstruction from a single image by combining only Single-image CoarseNet with FineNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The pipeline of our proposed inverse rendering method. Given an input face image (left), our inverse rendering consists of three stages: Model fitting (second column), geometry refinement (third column) and albedo blending (last column). At each stage, the top to bottom rows are the corresponding recovered lighting, geometry and albedo, and the rendered face image is shown on the right. The arrows indicate which component is updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>)where ./ represents the elementwise division operation, I in is the color of the input image and n is the normal computed from the refined geometry. However, the fine-scale albedo b f might contain some geometry details due to imperfect geometry refinement. To avoid this, we linearly blend b c and b f , i.e. ?b c + (1 -?)b f , with different weights ? at different regions. Particularly, in the regions where geometry details are likely to appear such as forehead and eye corners, we make the blended albedo close to b c by setting ? to be 0.65, while in the other regions we encourage the blended albedo close to b f by setting ? to be 0.35. Around the border of the regions ? is set continuously from 0.35 to 0.65. Finally, we use this blended albedo as b in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training data synthesis for Single-image CoarseNet. Given a real face image, we first do the inverse rendering to estimate lighting, albedo and geometry. Then, by changing the expression parameter ? exp and the pose parameters pitch, yaw, roll, the face geometry is augmented. In the final, a set of new face images is obtained by rendering the newly changed face geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of adjacent frame simulations. For each pair, the left is the PNCC image generated by simulating the previous frame, and the right is the current face frame.</figDesc><graphic url="image-43.png" coords="8,48.96,121.60,60.89,60.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Synthetic data generation for training FineNet. Given a target face image without many geometry details (top left) and a source face image (bottom left) that rich of wrinkles, we first apply our developed inverse rendering on both images to obtain the projected geometry for target face (top second) and a displacement map for the source face (bottom right). Then we transfer the displacement map of the source face to the geometry of the target face. Finally we render the updated geometry to get a new face image (top right) which contains the same type of wrinkles as the source face.</figDesc><graphic url="image-48.png" coords="8,311.98,125.86,60.21,59.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of our two-stage CNN based face tracking. Left: four frames of a video. Middle: results of Tracking CoarseNet (projected mesh and rendered face). Right: results of FineNet.</figDesc><graphic url="image-63.png" coords="9,238.06,183.61,61.97,61.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparisons with image-based dense face tracking. Top row: four continuous frames from a video. Middle row: results of using our Single-image CoarseNet on each frame. Bottom row: results of our Tracking CoarseNet. It can be observed that CoarseNet achieves more robust tracking.</figDesc><graphic url="image-73.png" coords="9,311.98,184.60,62.09,62.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Reconstruction results for faces with large poses and extreme expressions. Top row: several frames from one input video. Bottom row: the reconstructed face shapes with geometry details. See the complete sequence in the accompanying video or via the link: https://youtu.be/dghlMXxD-rk.</figDesc><graphic url="image-101.png" coords="10,48.96,434.20,62.31,62.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparisons of our inverse rendering and our learning based dense face tracking solution. From top to bottom: input face video frame and groundtruth mesh in dataset [55], results of the inverse rendering approach, results of our learning based dense face tracking solution.</figDesc><graphic url="image-114.png" coords="10,311.98,247.60,55.19,57.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: From left to right: input face image, geometry reconstructed by inverse rendering, geometry reconstructed by our learning based method. It can be seen that our method can better reconstruct unclear wrinkles under strong lighting.</figDesc><graphic url="image-150.png" coords="11,318.36,250.83,77.11,77.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Quantitative results of dense face reconstruction from monocular video.</figDesc><table><row><cell cols="3">Average point-to-point distance (mm)</cell></row><row><cell>Inverse rendering</cell><cell>CoarseNet</cell><cell>CoarseNet+FineNet</cell></row><row><cell>1.81</cell><cell>2.11</cell><cell>2.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Comparisons of testing errors under different metrics.</figDesc><table><row><cell>Metrics</cell><cell>[42]</cell><cell>Our method</cell></row><row><cell>Lpose in Eq. (13)</cell><cell>26.35</cell><cell>7.69</cell></row><row><cell>Lgeo in Eq. (14)</cell><cell>5.53</cell><cell>4.23</cell></row><row><cell>MSE (pose parameters)</cell><cell>1.91</cell><cell>0.56</cell></row><row><cell>Mean vertex distance (geometry parameters)</cell><cell>5.18</cell><cell>4.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Quantitative Comparison. Our method outperformes<ref type="bibr" target="#b60">[61]</ref> and<ref type="bibr" target="#b59">[60]</ref> in terms of RMSE and MAE.</figDesc><table><row><cell>Method</cell><cell>RMSE [mm]</cell><cell>MAE [mm]</cell></row><row><cell>[61]</cell><cell>5.946</cell><cell>4.420</cell></row><row><cell>[60]</cell><cell>5.367</cell><cell>3.923</cell></row><row><cell>Ours</cell><cell>4.915</cell><cell>3.846</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/waps101/3DMM edges</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/AaronJackson/vrn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/unibas-gravis/basel-face-pipeline</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/unibas-gravis/scalismo-faces</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Thomas Vetter</rs> et al. and <rs type="person">Kun Zhou</rs> et al. for allowing us to use their <rs type="grantNumber">3D</rs> face datasets. This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2016YFC0800501</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61672481</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XUyp5cp">
					<idno type="grant-number">3D</idno>
				</org>
				<org type="funding" xml:id="_SqSBcsX">
					<idno type="grant-number">2016YFC0800501</idno>
				</org>
				<org type="funding" xml:id="_mzrNbmq">
					<idno type="grant-number">61672481</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverse rendering of faces with a 3d morphable model</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1080" to="1093" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reconstructing high quality face-surfaces using model based stereo</title>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fitting a 3d morphable model to edges: A comparison between hard and soft correspondences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="377" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;99</title>
		<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online modeling for realtime facial animation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time high-fidelity facial performance capture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d shape regression for real-time facial animation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iteratively reweighted algorithms for compressive sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on Acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3869" to="3872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video face replacement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Occlusion-aware 3d morphable face models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Occlusion-aware 3d morphable models and an illumination prior for face image analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic face reenactment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rehmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thormahlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4217" to="4224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reconstructing detailed dynamic face geometry from monocular video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3d face rigs from monocular video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging motion capture and 3d scanning for high-fidelity facial performance acquisition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiresolution 3d morphable face model and fitting framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</title>
		<meeting>the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic 3d avatar creation from hand-held video input</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ichim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">3d face reconstruction with geometry details from a single image</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1702.05619</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4188" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d face reconstruction from a single image using a single reference face shape</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="405" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Being john malkovich</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inversefacenet: Deep single-shot inverse face rendering from A single image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>CoRR, abs/1703.10956</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Realtime facial animation with on-the-fly correctives</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013-07">2013. July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spherical harmonics</title>
		<author>
			<persName><forename type="first">C</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced video and signal based surveillance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shape from shading</title>
		<author>
			<persName><forename type="first">E</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of mathematical models in computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="375" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques, SIGGRAPH</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques, SIGGRAPH</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time facial segmentation and performance capture from rgb input</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="244" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo for automated face image analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="183" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The first facial landmark tracking in-the-wild challenge: Benchmark and results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshop (ICCVW), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic acquisition of highfidelity facial performances using monocular videos</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shape reconstruction of 3d bilaterally symmetric surfaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="110" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the early history of the singular value decomposition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="566" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-time expression transfer for facial reenactment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lightweight binocular facial performance capture under uncontrolled lighting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Realtime performance-based facial animation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Performance-driven facial animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Illumination-insensitive face recognition using symmetric shape-from-shading</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Symmetric shape-from-shading using image</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
