<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Denoising, Deblocking, and Enhancement Through Separable 4-D Nonlocal Spatiotemporal Transforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-08-22">August 22, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Maggioni</surname></persName>
							<email>matteo.maggioni@tut.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Signal Processing</orgName>
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postCode>33720</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
							<email>boracchi@elet.polimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Signal Processing</orgName>
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postCode>33720</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Signal Processing</orgName>
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postCode>33720</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
							<email>alessandro.foi@tut.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Signal Processing</orgName>
								<orgName type="institution">Tampere University of Technology</orgName>
								<address>
									<postCode>33720</postCode>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Denoising, Deblocking, and Enhancement Through Separable 4-D Nonlocal Spatiotemporal Transforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-08-22">August 22, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">47A930F4F1B8B0FB2F16CD3DD9802FA4</idno>
					<idno type="DOI">10.1109/TIP.2012.2199324</idno>
					<note type="submission">received January 11, 2011; revised October 7, 2011; accepted December 13, 2011. Date of publication May 15, 2012; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive transforms</term>
					<term>motion estimation</term>
					<term>nonlocal methods</term>
					<term>video deblocking</term>
					<term>video denoising</term>
					<term>video enhancement</term>
					<term>video filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a powerful video filtering algorithm that exploits temporal and spatial redundancy characterizing natural video sequences. The algorithm implements the paradigm of nonlocal grouping and collaborative filtering, where a higher dimensional transform-domain representation of the observations is leveraged to enforce sparsity, and thus regularize the data: 3-D spatiotemporal volumes are constructed by tracking blocks along trajectories defined by the motion vectors. Mutually similar volumes are then grouped together by stacking them along an additional fourth dimension, thus producing a 4-D structure, termed group, where different types of data correlation exist along the different dimensions: local correlation along the two dimensions of the blocks, temporal correlation along the motion trajectories, and nonlocal spatial correlation (i.e., self-similarity) along the fourth dimension of the group. Collaborative filtering is then realized by transforming each group through a decorrelating 4-D separable transform and then by shrinkage and inverse transformation. In this way, the collaborative filtering provides estimates for each volume stacked in the group, which are then returned and adaptively aggregated to their original positions in the video. The proposed filtering procedure addresses several video processing applications, such as denoising, deblocking, and enhancement of both grayscale and color data. Experimental results prove the effectiveness of our method in terms of both subjective and objective visual quality, and show that it outperforms the state of the art in video denoising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>applications involving digital videos has motivated a significant interest in restoration or enhancement solutions, and the literature contains a plethora of such algorithms (see <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> for a comprehensive overview).</p><p>At the moment, the most effective approach in restoring images or video sequences exploits the redundancy given by the nonlocal similarity between patches at different locations within the data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Algorithms based on this approach have been proposed for various signal-processing problems, and mainly for image denoising <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Specifically, in <ref type="bibr" target="#b6">[7]</ref> an adaptive pointwise image filtering strategy has been introduced, called nonlocal means, where the estimate of each pixel is obtained as an average of, in principle, every other pixel in the noisy image, weighted proportionally to the similarity between the local neighborhoods surrounding them. Until now, the most effective image-denoising algorithm, termed BM3D <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, relies on the so-called grouping and collaborative filtering paradigm: at first mutually similar 2-D image blocks are stacked into a 3-D group (grouping), then the group is filtered through a transform-domain shrinkage (collaborative filtering) which simultaneously provides individual estimates for each grouped block. These estimates are returned to their respective locations and eventually aggregated to produce the final denoised image. In doing so, BM3D leverages both the nonlocal and local spatial correlation of natural images, exploiting the abundance of mutually similar patches and the high correlation of image data within each patch, respectively. The BM3D filtering scheme has been successfully applied to video denoising with the V-BM3D algorithm <ref type="bibr" target="#b10">[11]</ref>, as well as to several other applications including image and video super-resolution <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, image sharpening <ref type="bibr" target="#b12">[13]</ref>, and image deblurring <ref type="bibr" target="#b16">[17]</ref>.</p><p>In V-BM3D, groups are 3-D arrays of mutually similar blocks extracted from a set of consecutive frames in the video sequence. V-BM3D does not distinguish whether the blocks are grouped due to their temporal or nonlocal (spatial) similarity, even though it is typically along the temporal dimension that the highest block correlation is preserved. We recognize this as a conceptual as well as practical weakness of the V-BM3D algorithm. In fact, the simple experiments reported in Section VIII demonstrate that the denoising performance does not necessarily increase with the number of spatially self-similar blocks in each group, whereas it is always improved by exploiting the temporal redundancy. Furthermore, in <ref type="bibr" target="#b11">[12]</ref>, it is shown that, even when fast motion is present, the similarity along the motion trajectories is much stronger than the nonlocal similarity existing within an individual frame. As a matter of fact, nearly all modern motion-compensated video-coding techniques exploit the smoothness of videos along the temporal axis <ref type="bibr" target="#b17">[18]</ref>.</p><p>This paper proposes V-BM4D, a novel video-filtering approach that overcomes the above weaknesses by separately exploiting the temporal and nonlocal correlation of the video. The core element of V-BM4D is the spatiotemporal volume, a 3-D structure formed by a sequence of blocks following a specific trajectory obtained, for example, by concatenating motion vectors along time <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Thus, as opposed to V-BM3D, V-BM4D does not group blocks, but mutually similar spatiotemporal volumes according to a nonlocal search procedure. Hence, groups in V-BM4D are 4-D stacks of 3-D volumes, and the collaborative filtering is then performed via a separable 4-D spatiotemporal transform. Such transform leverages three types of correlation: local spatial correlation between pixels in each block of a volume, local temporal correlation between blocks of each volume, and nonlocal spatial and temporal correlation between volumes of the same group. The 4-D group spectrum is thus highly sparse, which makes the shrinkage more effective than that of V-BM3D, yielding superior performance in terms of noise reduction.</p><p>In this paper, we extend the basic implementation of V-BM4D as a grayscale denoising filter introduced in the conference paper <ref type="bibr" target="#b0">[1]</ref> presenting its modifications for the deblocking and deringing of compressed videos, as well as for the enhancement (sharpening) of low-contrast videos. Then, leveraging the approach presented in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b20">[21]</ref>, we generalize V-BM4D to perform collaborative filtering of color (multi-channel) data. We also empirically analyze the impact of the temporal and nonlocal correlation on the final filtering quality.</p><p>This paper is organized as follows. Section II formally defines the V-BM4D filtering scheme. Section III discusses the implementation aspects, with particular emphasis on the computation of motion vectors. The application of V-BM4D to deblocking and deringing is given in Section IV, whereas video enhancement (sharpening) is presented in Section V. The experimental evaluation and the computational complexity analysis of V-BM4D are reported in Sections VI and VII, respectively. Concluding remarks are given in Sections VIII and IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BASIC ALGORITHM</head><p>The purpose of the proposed algorithm is to provide an estimate of the original video from the observed data. For the algorithm design, we assume the common additive white Gaussian noise model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observation Model</head><p>We consider the observed video as a noisy image sequence z : X × T → R defined as z(x, t) = y(x, t) + η(x, t),</p><p>x ∈ X, t ∈ T (1) where y is the original (unknown) video, η(•, •) ∼ N (0, σ 2 ) is independent identically distributed white Gaussian noise, and (x, t) are the 3-D spatiotemporal coordinates belonging to the spatial domain X ⊂ Z 2 and time domain T ⊂ Z, respectively. The frame of the video z at time t is denoted by z(X, t). The V-BM4D algorithm comprises three fundamental steps inherited from the BM3D paradigm, specifically grouping (Section II-C), collaborative filtering (Section II-D), and aggregation (Section II-E). These steps are performed for each spatiotemporal volume of the video (Section II-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatiotemporal Volumes</head><p>Let B z (x 0 , t 0 ) denote a square block of fixed size N × N extracted from the noisy video z; without loss of generality, the coordinates (x 0 , t 0 ) identify the top-left pixel of the block in the frame z(X, t 0 ). A spatiotemporal volume is a 3-D sequence of blocks built following a specific trajectory along time, which is supposed to follow the motion in the scene. Formally, the trajectory associated to (x 0 , t 0 ) is defined as</p><formula xml:id="formula_0">Traj(x 0 , t 0 ) = (x j , t 0 + j ) h + j =-h - (2)</formula><p>where the elements (x j , t 0 + j ) are time-consecutive coordinates, each of these representing the position of the reference block B z (x 0 , t 0 ) within the neighboring frames z(X, t 0 + j ), j = -h -, . . . , h + . For the sake of simplicity, in this section it is assumed</p><formula xml:id="formula_1">h -= h + = h for all (x, t) ∈ X × T .</formula><p>The trajectories can be either directly computed from the noisy video or when a coded video is given, they can be obtained by concatenating motion vectors. In what follows we assume that, for each (x 0 , t 0 ) ∈ X × T , a trajectory Traj(x 0 , t 0 ) is given and thus the 3-D spatiotemporal volume associated to (x 0 , t 0 ) can be determined as</p><formula xml:id="formula_2">V z (x 0 , t 0 ) = B z (x i , t i ) : (x i , t i ) ∈ Traj(x 0 , t 0 ) (3)</formula><p>where the subscript z specifies that the volumes are extracted from the noisy video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grouping</head><p>The groups are stacks of mutually similar spatiotemporal volumes and constitute the nonlocal element of V-BM4D. The mutually similar volumes are determined by a nonlocal search procedure as in <ref type="bibr" target="#b9">[10]</ref>. Specifically, let Ind(x 0 , t 0 ) be the set of indices identifying the volumes similar to the reference volume V z (x 0 , t 0 ), according to a distance operator δ v . Formally</p><formula xml:id="formula_3">Ind(x 0 , t 0 ) = (x i , t i ) : δ v (V z (x 0 , t 0 ), V z (x i , t i )) &lt; τ match</formula><p>where the parameter τ match &gt; 0 controls the minimum degree of similarity among volumes with respect to the distance δ v , e.g., measured as the 2 -norm of the difference between two volumes.</p><p>The group associated to the reference volume V z (x 0 , t 0 ) is then</p><formula xml:id="formula_4">G z (x 0 , t 0 ) = V z (x i , t i ) : (x i , t i ) ∈ Ind(x 0 , t 0 ) . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>In ( <ref type="formula" target="#formula_4">4</ref>), we implicitly assume that the 3-D volumes are stacked along a fourth dimension; hence, the groups are 4-D data structures. The order of the spatiotemporal volumes in the 4-D stacks is based on their similarity with the reference volume. Note that since δ v (V z , V z ) = 0, every group G z (x 0 , t 0 ) contains, at least, V z (x 0 , t 0 ). Fig. <ref type="figure" target="#fig_0">1</ref> shows an example of volumes and group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Collaborative Filtering</head><p>According to the general formulation of the grouping and collaborative-filtering approach for a d-dimensional signal <ref type="bibr" target="#b9">[10]</ref>, the groups are (d + 1)-dimensional structures of similar d-dimensional elements, which are then jointly filtered. In particular, each of the grouped elements influences the filtered output of all the other elements of the group: this is the basic idea of collaborative filtering. This is typically realized through the following steps: first a (d +1)-dimensional separable linear transform is applied to the group, second the transformed coefficients are shrunk, for example, by hard thresholding or by Wiener filtering, and finally the (d + 1)-dimensional transform is inverted to obtain an estimate for each grouped element.</p><p>The collaborative filtering applies a 4-D separable linear transform T 4D on each 4-D group G z (x 0 , t 0 ), thus providing the group estimate</p><formula xml:id="formula_6">Ĝ y (x 0 , t 0 ) = T -1 4D ϒ (T 4D (G z (x 0 , t 0 )))</formula><p>where ϒ denotes a generic shrinkage operator. The fil-</p><formula xml:id="formula_7">tered 4-D group Ĝ y (x 0 , t 0 ) is composed of volumes Vy (x, t) Ĝ y (x 0 , t 0 ) = Vy (x i , t i ) : (x i , t i ) ∈ Ind(x 0 , t 0 )</formula><p>with each Vy being an estimate of the corresponding unknown volume V y in the original video y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Aggregation</head><p>The groups Ĝ y constitute a very redundant representation of the video because, in general, the volumes Vy overlap. Consequently, collaborative filtering provides multiple estimates for the voxels within the overlapping parts. Such estimates are aggregated through a convex combination with adaptive weights defined as</p><formula xml:id="formula_8">ŷ = (x 0 ,t 0 )∈X ×T (x i ,t i )∈Ind(x 0 ,t 0 ) w (x 0 ,t 0 ) Vy (x i , t i ) (x 0 ,t 0 )∈X ×T (x i ,t i )∈Ind(x 0 ,t 0 ) w (x 0 ,t 0 ) χ (x i ,t i ) (5)</formula><p>where we assume Vy (x i , t i ) to be zero padded outside its domain, being χ (x i ,t i ) : X ×T → {0, 1} the characteristic function (indicator) of the support of the volume Vy (x i , t i ), and the aggregation weights w (x 0 ,t 0 ) are different for different groups. The aggregation weights depend on the result of the shrinkage in the collaborative filtering, as they are typically defined to be inversely proportional to the total sample variance of the estimate of the corresponding groups <ref type="bibr" target="#b9">[10]</ref>. Intuitively, the sparser the shrunk 4-D spectrum Ĝ y (x 0 , t 0 ), the larger the corresponding weight w (x 0 ,t 0 ) . Such aggregation is a wellestablished procedure to obtain a global estimate from different overlapping local estimates <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPLEMENTATION ASPECTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computation of the Trajectories</head><p>In our implementation of V-BM4D, we construct the trajectories by concatenating motion vectors along time.</p><p>1) Location Prediction: The motion vector can be computed as v(x i , t i ) = x i-1x i , where (x i-1 , t i -1) and (x i , t i ) are two consecutive spatiotemporal locations. Hence, under the assumption of smooth motion, we can predict the position xi (t i + 1) of a block in the frame z (X, t i + 1) as</p><formula xml:id="formula_9">xi (t i + 1) = x i + γ p • v(x i , t i ) (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where γ p ∈ [0, 1] is a weighting factor of the prediction. In the case (x i-1 , t i -1) is not available, we consider the lack of motion as the most likely situation and we set xi (t i + 1) = x i . Analogous predictions can be made when looking for precedent blocks in the sequence.</p><p>2) Similarity Criterion: The motion of a block is generally tracked by identifying the most similar block in the subsequent or precedent frame. However, in the presence of noise, it is advisable to enforce a motion-smoothness prior to improve the tracking. In particular, given the predicted future xi (t i + 1) or past xi (t i -1) positions of the block B z (x i , t i ), we define the similarity between B z (x i , t i ) and B z (x j , t i ± 1), through a penalized quadratic difference defined as</p><formula xml:id="formula_11">δ b B z (x i , t i ), B z (x j , t i ± 1) = B z (x i , t i ) -B z (x j , t i ± 1) 2 2 +γ d xi (t i ± 1) -x j 2 (7)</formula><p>where xi (t i ± 1) is defined as in <ref type="bibr" target="#b5">(6)</ref>, and γ d ∈ R + is the penalization parameter. Observe that the blocks are tracked independently in the frames at times t i + 1 and t i -1.</p><p>V-BM4D constructs the trajectory (2) by repeatedly minimizing <ref type="bibr" target="#b6">(7)</ref>. Formally, the motion of B z (x i , t i ) from time t i to t i ± 1 is determined by the position x i±1 that minimizes <ref type="bibr" target="#b6">(7)</ref> as</p><formula xml:id="formula_12">x i±1 = arg min x k ∈N i δ b B z (x i , t i ), B z (x k , t i ± 1)</formula><p>where N i is an adaptive search neighborhood within the frame z(X, t i ± 1) (further details are given in Section III-A3). Even though such x i±1 can be always found, we stop the trajectory construction whenever the corresponding minimum distance δ b exceeds a fixed parameter τ traj ∈ R + , which imposes a minimum amount of similarity along the spatiotemporal volumes. This allows V-BM4D to effectively resolve those situations, such as occlusions and scene changes, where consistent blocks in terms of both similarity and motion smoothness cannot be found.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> illustrates two trajectories estimated using different penalization parameters γ d . Observe that the penalization term becomes essential when blocks are tracked within flat areas or homogeneous textures in the scene. In fact, the right image of Fig. <ref type="figure" target="#fig_1">2</ref> shows that without a position-dependent distance metric the trajectories would be mainly determined by the noise. As a consequence, the collaborative filtering would be less effective because of the badly conditioned temporal correlation of the data within the volumes.</p><p>3) Search Neighborhood:</p><p>The penalty term γ d xi (t i ± 1)x j 2 likely leads the minimizer of <ref type="bibr" target="#b6">(7)</ref> to be close to xi (t i ± 1). Thus, we can rightly restrict the minimization of (7) to a spatial search neighborhood N i centered at xi (t i ± 1). We experienced that it is convenient to make the search-neighbor size, N P R × N P R , adaptive on the velocity of the tracked block (magnitude of motion vector) by setting</p><formula xml:id="formula_13">N P R = N S • 1 -γ w • e - ||v(x i ,t i )|| 2 2 2•σ 2 w</formula><p>where N S is the maximum size of N i , γ w ∈ [0, 1] is a scaling factor, and σ w &gt; 0 is a tuning parameter. As the velocity v increases, N P R approaches N S accordingly to σ w ; conversely, when the velocity is zero N P R = N S (1 -γ w ). By setting a proper value of σ w we can control the decay rate of the exponential term as a function of v or, in other words, how permissive the window contraction is with respect to the velocity of the tracked block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subvolume Extraction</head><p>So far, the number of frames spanned by all the trajectories has been assumed fixed and equal to h. However, because of occlusions, scene changes, or heavy noise, any trajectory Traj(x i , t i ) can be interrupted at any time, i.e., whenever the distance between consecutive blocks falls below the threshold τ traj . Consequently, if</p><formula xml:id="formula_14">[t i -h - i , t i +h + i ]</formula><p>is the temporal extent of the trajectory Traj(x i , t i ), in general we have 0 ≤ h - i ≤ h and 0 ≤ h + i ≤ h, being h the maximum forward and backward temporal extent allowed in the algorithm.</p><p>In principle, V-BM4D may stack together volumes having different lengths, however, in practice, because of the separability of the transform T 4D , every group G z (x i , t i ) has to be composed of volumes having the same length. Thus, for each reference volume V z (x 0 , t 0 ), we only consider the volumes V z (x i , t i ) such that t i = t 0 , h - i ≥ h - 0 , and h + i ≥ h + 0 . Then, we extract from each V z (x i , t i ) the subvolume having temporal extent [t 0 -h - 0 , t 0 +h + 0 ], denoted as E L 0 V z (x i , t i ) . Among all the possible criteria for extracting a subvolume of length L 0 = h - 0 +h + 0 +1 from a longer volume, our choice aims at limiting the complexity yet maintaining a high data correlation. In fact, we can reasonably assume that similar objects at different positions are represented by similar volumes along time.</p><p>In the grouping, we set as distance operator δ v the 2 -norm of the difference between time-synchronous volumes normalized with respect to their lengths</p><formula xml:id="formula_15">δ v V z (x 0 , t 0 ), V z (x i , t i ) = V z (x 0 , t 0 ) -E L 0 V z (x i , t i ) 2 2 L 0 . (<label>8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) C. Two-Stage Implementation With Collaborative Wiener Filtering</head><p>The general procedure described in Section II is implemented by two cascading stages, each composed of the grouping, collaborative filtering, and aggregation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Hard-Thresholding Stage:</head><p>In the first stage, volumes are extracted from the noisy video z, and groups are then formed using the δ v -operator <ref type="bibr" target="#b7">(8)</ref> with the predefined threshold τ ht match . Collaborative filtering is realized by hard thresholding each group</p><formula xml:id="formula_16">G z (x, t) in 4-D transform domain Ĝht y (x 0 , t 0 ) = T ht -1 4D ϒ ht T ht 4D (G z (x 0 , t 0 ))</formula><p>where T ht 4D is the 4-D transform and ϒ ht is the hard-threshold operator with threshold σ λ 4D .</p><p>The outcome of the hard-thresholding stage ŷht is obtained by aggregating with a convex combination all the estimated groups Ĝht y (x 0 , t 0 ), as defined in <ref type="bibr" target="#b4">(5)</ref>. The adaptive weights used in this combination are inversely proportional to the number N ht (x 0 ,t 0 ) of nonzero coefficients of the corresponding hardthresholded group Ĝht y (x 0 , t 0 ): that is w ht (x 0 ,t 0 ) = 1/N ht (x 0 ,t 0 ) , which provides an estimate of the total variance of Ĝht y (x 0 , t 0 ). In such a way, we assign larger weights to the volumes belonging to groups having sparser representation in T 4D domain.</p><p>2) Wiener-Filtering Stage: In the second stage, the motion estimation is improved by extracting new trajectories Traj ŷht from the basic estimate ŷht , and the grouping is performed on the new volumes V ŷht . Volume matching is still performed through the δ v -distance, but using a different threshold τ wie match . The indices identifying similar volumes Ind ŷht (x 0 , t 0 ) are used to construct both groups G z and G ŷht , composed by volumes extracted from the noisy video z and from the estimate y ht , respectively. Collaborative filtering is hence performed using an empirical Wiener filter in T wie 4D transform domain. Shrinkage is realized by scaling the 4-D transform coefficients of each group G z (x 0 , t 0 ), extracted from the noisy video z, with the Wiener attenuation coefficients</p><formula xml:id="formula_17">W(x 0 , t 0 ) = T wie 4D G ŷht (x 0 , t 0 ) 2 T wie 4D G ŷht (x 0 , t 0 ) 2 + σ 2<label>(9)</label></formula><p>that are computed from the energy of the 4-D spectrum of the group G ŷht (x 0 , t 0 ). Eventually, the group estimate is obtained by inverting the 4-D transform as Ĝwie y (x 0 , t 0 ) = T wie -1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D</head><p>W(x 0 , t 0 ) • T wie 4D (G z (x 0 , t 0 )) where • denotes the element-wise product. The final global estimate ŷwie is computed by the aggregation (5), using the weights w wie (x 0 ,t 0 ) = ||W(x 0 , t 0 )|| -2 2 , which follow from considerations similar to those underlying the adaptive weights used in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Settings</head><p>The parameters involved in the motion estimation and in the grouping, that is γ d , τ traj , and τ match , depend on the noise standard deviation σ . Intuitively, the thresholds defining the minimum amount of similarity between blocks and volumes should increase with σ to compensate the effects of the noise. For the sake of simplicity, we model such dependences as second-order polynomials in σ : γ d (σ ), τ traj (σ ), and τ match (σ ). The nine coefficients required to describe the three polynomials are jointly optimized using the Nelder-Mead simplex direct search algorithm <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. As an optimization criterion, we maximize the sum of the restoration performance (PSNR) of V-BM4D applied to the test sequences Salesman, Tennis, Flower Garden, Miss America, Coastguard, Foreman, Bus, and Bicycle corrupted by white Gaussian noise having values of standard deviation σ varying from 5 to 70 ([0,255] being the intensity range of the videos). The resulting polynomials are</p><formula xml:id="formula_18">γ d (σ ) = 0.1275 • σ 2 -1.5045 • σ + 10.2 (10) τ traj (σ ) = 0.0047 • σ 2 + 0.0676 • σ + 0.4564 (11) τ match (σ ) = 0.0171 • σ 2 + 0.4520 • σ + 47.9294. (12)</formula><p>The solid lines in Fig. <ref type="figure" target="#fig_2">3</ref> show the above functions. We also plot, using different markers, the optimum values of the three parameters obtained by unconstrained and independent optimizations of V-BM4D for each test sequence and σ . Empirically, the polynomials demonstrate a good approximation of the optimum (γ d , τ traj , τ match ). Within the considered σ range, the curve <ref type="bibr" target="#b9">(10)</ref> is "practically" monotone increasing despite its negative first-degree coefficient. We refrain from introducing additional constraints to the polynomials as well as from considering additional σ values smaller than 5, because the resulting sequences would be mostly affected by the noise and quantization artifacts intrinsic in the original test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEBLOCKING</head><p>Most video compression techniques, such as MPEG-4 <ref type="bibr" target="#b25">[26]</ref> or H.264 <ref type="bibr" target="#b26">[27]</ref>, make use of block-transform coding and thus may suffer, especially at low bitrates, from several compression artifacts such as blocking, ringing, mosquito noise, and flickering. These artifacts are mainly due to the motion compensation and the coarse quantization of the blocktransform coefficients. Moreover, since each block is processed separately, the correlation between pixels at the borders of neighboring blocks is typically lost during the compression, resulting in false discontinuities in the decoded video (such as those shown in the blocky frames in Fig. <ref type="figure" target="#fig_6">7</ref>).</p><p>A large number of deblocking filters have been proposed in the past decade; among them we mention frame-based enhancement using a linear low-pass filter in spatial or transform domain <ref type="bibr" target="#b27">[28]</ref>, projection onto convex sets methods <ref type="bibr" target="#b28">[29]</ref>, spatial block boundary filter <ref type="bibr" target="#b29">[30]</ref>, statistical modeling methods <ref type="bibr" target="#b30">[31]</ref>, or shifted thresholding <ref type="bibr" target="#b31">[32]</ref>. Additionally, most of modern video coding block-based techniques, such as H.264 or MPEG-4, embed an in-loop deblocking filter as an additional processing step in the decoder <ref type="bibr" target="#b25">[26]</ref>.</p><p>Inspired by <ref type="bibr" target="#b32">[33]</ref>, we treat the blocking artifacts as additive noise. This choice allows us to model the compressed video z as in <ref type="bibr" target="#b0">(1)</ref>, where y now corresponds to the original uncompressed video and η represents the compression artifacts. In what follows, we focus our attention on MPEG-4 compressed videos. In this way, the proposed filter can be applied reliably over different types of data degradations with little need of adjustment or user intervention.</p><p>In order to use V-BM4D as a deblocking filter, we need to determine a suitable value of σ to handle the artifacts in a compressed video. For this purpose, we proceed as in the previous section and we identify the optimum value of σ for a set of test sequences compressed at various rates. Fig. <ref type="figure" target="#fig_3">4</ref> shows these optimum values plotted against the average bit-per-pixel (bpp) rate of the compressed video and the parameter q that controls the quantization of the block-transform coefficients <ref type="bibr" target="#b25">[26]</ref> [Fig. <ref type="figure" target="#fig_3">4(a)</ref>]. Let us observe that both the bpp and q parameters are easily accessible from any given MPEG-4 coded video. These plots suggest that a power law may conveniently explain the relation between the optimum value of σ and both the bpp rate and q. Hence, we fit such bivariate function to the optimum values via least-squares regression, obtaining the adaptive value of σ for the V-BM4D deblocking filter as</p><formula xml:id="formula_19">σ (bpp, q) = 0.09 • q 1.11 • bpp -0.46 + 3.37. (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>The function σ (bpp, q) is shown in Fig. <ref type="figure" target="#fig_3">4</ref> (right). Note that in MPEG-4 the parameter q ranges from 2 to 31, where higher values correspond to a coarser quantization and consequently lower bitrates. As a matter of fact, when q increases and/or bpp decreases, the optimum σ increases, in order to effectively cope with stronger blocking artifacts. Clearly, a much larger value of σ could result in oversmoothing, while much smaller values may not suffice for effectively reducing the compression artifacts. While in this paper, we mostly deal with short test sequences, and we compute the bpp as the average rate over the whole sequence, we argue that in practice this rate should be computed as the average over a limited set of frames, namely the so-called group of pictures built around each intracoded frame. In principle, one could learn a model for σ together with all the remaining V-BM4D parameters at once (possibly achieving better results), but this would increase the risk of overfitting many parameters to the peculiarities of this compression method, and would complicate the optimization task.</p><p>Let us remark that V-BM4D deblocking can be straightforwardly applied also to videos compressed by other encoders than MPEG-4, because the q parameter can be both estimated as a subjective quality metric for compressed videos, or as an objective measurement on the compression artifacts <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ENHANCEMENT</head><p>Enhancement is used to improve the video quality so that the filtered video becomes more pleasing to human subjective judgment and/or better suited for subsequent automatic interpretation tasks, as segmentation or pattern recognition. In particular, by enhancement we refer to the sharpening of degraded details within images (frames) characterized by low contrast.</p><p>Among the existing enhancement techniques, we mention methods based on histogram manipulation <ref type="bibr" target="#b34">[35]</ref>, linear and nonlinear unsharp masking <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>, fuzzy logic <ref type="bibr" target="#b38">[39]</ref>, and weighted median filter <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Transform-domain methods generally apply a nonlinear operator to the transform coefficients of the processed image/video in order to accentuate specific portions of the spectrum, which eventually results in sharpening of details <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. One of the most popular techniques is alpha-rooting <ref type="bibr" target="#b41">[42]</ref>, which raises the magnitude of each transform coefficient φ i of the processed spectrum to a power 1/α, with α &gt; 1 as where φ 0 is the DC term and φi is the resulting sharpened coefficients. Observe that α &gt; 1 induces sharpening, as it scales the large coefficients relatively to the small ones, i.e., those carrying high-frequency information <ref type="bibr" target="#b41">[42]</ref>. Although <ref type="bibr" target="#b13">(14)</ref> assumes real-valued transform coefficients, it can be generalized to complex-valued ones, observing that alpharooting preserves the sign in the former case, and the phase in the latter.</p><formula xml:id="formula_21">φi = ⎧ ⎨ ⎩ sign φ i φ 0 φ i φ 0 1 α , if φ 0 = 0 φ i , otherwise<label>(14)</label></formula><p>A critical issue in image/video enhancement is the amplification of the noise together with the sharpening of image details <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, an effect that becomes more severe as the amount of applied sharpening increases. In order to cope with this problem, a joint application of a denoising and sharpening filter is often recommendable, and in particular this practice has been investigated in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b38">[39]</ref>.</p><p>Enhancement of digital videos, following the approach proposed in <ref type="bibr" target="#b12">[13]</ref>, can be easily performed by combining the V-BM4D filter with the alpha-rooting operator <ref type="bibr" target="#b13">(14)</ref>, in order to simultaneously reduce the noise and sharpen the original signal. The V-BM4D sharpening algorithm still comprises the grouping, collaborative filtering, and aggregation steps, and it is carried out through the hard-thresholding stage only. The alpha-rooting operator is applied on the thresholded coefficients within the collaborative filtering step, before the inversion of the 4-D transform. Note that since the alpharooting amplifies the group coefficients, the total variance of the filtered group changes; thus, the aggregation weights cannot be estimated from the number of retained nonzero coefficients N har (x 0 ,t 0 ) . A simple estimator is devised in <ref type="bibr" target="#b12">[13]</ref>, and can be used to define the weights of (5) as</p><formula xml:id="formula_22">w har (x 0 ,t 0 ) = 1 (i) =0 w i σ 2 having w i = 1 - 1 α 2 |φ 0 | -2 α |φ i | 2 α + 1 α 2 |φ i | 2 α -2 |φ 0 | 2-2 α</formula><p>where is the transformed spectrum of the group G ht z (x 0 , t 0 ) resulting from hard thresholding, and φ 0 is its corresponding DC coefficient. The DC term is not alpha-rooted; thus, its contribution to the total variance of the sharpened group should be σ 2 . However, in order to avoid completely flat blocks being awarded with excessively large weights, the weight for the DC term is set equal to the weight of the smallest retained coefficients, i.e., those having magnitude σ λ 4D as</p><formula xml:id="formula_23">w 0 = 1 - 1 α 2 |φ 0 | -2 α |σ λ 4D | 2 α + 1 α 2 |σ λ 4D | 2 α -2 |φ 0 | 2-2 α .</formula><p>The separability of the 4-D transform can be exploited to extend this approach, by treating in a different way different portions of the thresholded 4-D spectrum. Remember that the 4-D spectrum is structured according to the four dimensions of the corresponding group, i.e., two local spatial, one local temporal, and one for the nonlocal similarity. In particular, it includes a 2-D surface (face) corresponding to the DC terms of the two 1-D transforms used for decorrelating the temporal and nonlocal dimensions of the group, and 3-D volume corresponding to the DC term of the 1-D temporal transform. Hence, the value of α can be decreased for the coefficients that do not belong to this 3-D volume, in order to attenuate the temporal flickering artifacts. Likewise, the portion of spectrum in the 2-D surface can be used to characterize the group content as proposed in <ref type="bibr" target="#b44">[45]</ref>, for example, by using lower values of α on flat regions to avoid noise accentuation.</p><p>We introduce the sharpening operator in the first stage (hard thresholding) only, as this guarantees excellent subjective results, and we plan to address the application of alpha-rooting during Wiener filtering in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>In this section, we present the experimental results obtained with a C/MATLAB implementation of the V-BM4D algorithm. The filtering performance is measured using the PSNR, computed on the whole sequence as PSNR( ŷ, y) = 10 log 10</p><formula xml:id="formula_24">D 2 |X||T | (x,t )∈X ×T y((x, t))-ŷ(x, t) 2</formula><p>where |X| and |T | stand for the cardinality of X and T , respectively, and D is the maximum value of y. Additionally, we measure the performance of V-BM4D by means of the MOVIE index <ref type="bibr" target="#b45">[46]</ref>, a recently introduced video quality assessment (VQA) metric that is expected to be closer to the human visual judgement than the PSNR. We recall that the test sequences, as well as the noise standard deviation σ , are considered to be in the range [0, 255]; thus, D = 255. The transforms employed in the collaborative filtering are similar to those in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>: T ht 4D (used in the hardthresholding stage) is a 4-D separable composition of 1-D biorthogonal wavelet in both spatial dimensions, 1-D discrete cosine transform (DCT) in the temporal dimension and 1-D Haar wavelet in the fourth (grouping) dimension, while T wie 4D (used in the Wiener-filtering stage) differs from T ht 4D as in the spatial dimension it performs a 2-D DCT. Note that, because of the Haar transform, the cardinality M of each group is required to be a power of 2. To reduce the complexity of the grouping phase, we restrict the search of similar volumes within an N G × N G neighborhood centered The PSNR (dB) and MOVIE index <ref type="bibr" target="#b45">[46]</ref> (the lower the better) values are reported in the left and right part of each cell, respectively. In order to enhance the readability of the results, every MOVIE index has been multiplied by 10 3 . The test sequences are corrupted by white gaussian noise with different values of standard deviation σ . around the coordinates of the reference volume, and we introduce a step of N step ∈ N pixels in both horizontal and vertical directions between each reference volume. Although we set N step &gt; 1, we have to compute beforehand the trajectory of every possible volume in the video, since each volume is a potential candidate element of any group. Table I provides a complete overview of the parameters setting in V-BM4D.</p><p>The remaining part of this section presents the results of experiments concerning grayscale denoising (Section VI-A), deblocking (Section VI-B), enhancement (Section VI-C), and color filtering (Section VI-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grayscale Denoising</head><p>We compare the proposed filtering algorithm against V-BM3D <ref type="bibr" target="#b10">[11]</ref>, as this represents the state of the art in video denoising and we refer the reader to <ref type="bibr" target="#b10">[11]</ref> for comparisons with other methods that are less effective than V-BM3D. Table II reports the denoising performance of V-BM3D and V-BM4D in terms of PSNR and MOVIE index.</p><p>In our experiments, the two algorithms are applied to a set of test sequences corrupted by white Gaussian noise with different levels of standard deviation σ . The observations z are obtained by synthetically adding Gaussian noise to the test sequences, according to <ref type="bibr" target="#b0">(1)</ref>. Further details concerning the original sequences, such as the resolution and number of frames, are reported in the header of th tables.</p><p>As one can see, V-BM4D outperforms V-BM3D in nearly all the experiments, with PSNR improvement of almost 1 dB. It is particularly interesting to observe that V-BM4D effectively handles the sequences characterized by rapid motions and frequent scene changes, especially under heavy noise, such as Tennis, Flower Garden, Coastguard, and Bus. Fig. <ref type="figure" target="#fig_4">5</ref> shows that, as soon as the sequence presents a significant change in the scene, the denoising performances decrease significantly for both algorithms, but in these situations, V-BM4D requires fewer frames to recover the previous PSNR values, as shown by the lower peaks at about frames 25, 90, and 150 of Tennis, and the general superior performance The PSNR (dB) and MOVIE index <ref type="bibr" target="#b45">[46]</ref> (the lower the better) values are reported in the left and right part of each cell, respectively. In order to enhance the readability of the results, every MOVIE index has been multiplied by 10 3 . The parameter q controls the quantization matrix of the MPEG-4 encoder and bpp denotes the average bit-per-pixel rate of the compressed video. As a reference, we also show the PSNR and MOVIE index of the unfiltered compressed (Compr.) videos.</p><p>in Bus which is a sequence characterized by a high motion activity. Finally, Fig. <ref type="figure" target="#fig_5">6</ref> offers a visual comparison of the performance of the two algorithms applied to the test sequence Flower Garden corrupted by white Gaussian noise with standard deviation σ = 40. As a subjective quality assessment, V-BM4D better preserves textures, without introducing disturbing artifacts in the restored video: this is clearly visible from the excellent reconstruction of the roof pattern, and of the bark of the tree. Such improvement substantiates the improvements in terms of both PSNR and MOVIE index reported in Table <ref type="table" target="#tab_0">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deblocking</head><p>Table III compares, in terms of objective measurements, the V-BM4D deblocking filter against the MPlayer accurate deblocking filter, 1 as, to the best of our knowledge, it is one of the most effective deblocking algorithms. Eight test sequences compressed by the MPEG-4 encoder with different values of the quantization parameter q have been considered: additional details and the average bit-per-pixel (bpp) rates concerning these sequences are reported in the table. 1 Source code and documentation is available at http://sourceforge.net/ projects/ffdshow-tryout/ and http://www.mplayerhq.hu/.</p><p>Numerical results show that V-BM4D outperforms Mplayer in all the experiments, with improvement peaks of almost 2 dB in terms of PSNR. For the sake of completeness, we also report the MOVIE index. Observe that MOVIE often prefers the compressed observation rather than the filtered sequences, thus showing a general preference toward piecewise smooth images. However, let us observe that such results do not conform to the visual quality of the deblocked videos.</p><p>Fig. <ref type="figure" target="#fig_6">7</ref> shows the results of V-BM4D deblocking on the test sequence Tennis encoded using an aggressive compression level (q = 25). The visual quality of the deblocked sequence has been significantly improved, as the compression artifacts, such as blocking and ghosting, have been successfully filtered without losing fine image details. In particular, we can note how the player and the white poster sharply emerge from their blocky counterparts, whereas almost-uniform areas, such as the table and the wall, have been pleasingly smoothed without introducing blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Enhancement</head><p>In the enhancement experiments, we use the same settings reported in Table <ref type="table" target="#tab_0">I</ref>, testing two values of α, i.e., the parameter that controls the amount of sharpening in the alpha-rooting.     <ref type="figure">9</ref>. Joint V-BM4D denoising, enhancement and deflickering of the test sequence Miss America<ref type="foot" target="#foot_0">2</ref> corrupted by white Gaussian noise with standard deviation σ = 10. The bottom row shows the differences between two consecutive frames of the original, noisy, and enhanced sequences. The right-most column shows the sharpening result using different α {DC,AC} in the temporal DC and AC coefficients of the groups spectra. The images in the bottom row are all drawn with respect to the same gray colormap, which is stretched four times in order to improve the visualization. Fig. <ref type="figure" target="#fig_7">8</ref> presents the results of the V-BM4D enhancement filter applied to the test sequence Foreman corrupted by white Gaussian noise having standard deviation σ = 25, and sharpened using α = 1.1 and α = 1.25. The combination of V-BM4D with the alpha-rooting enables an effective detail preservation together with a fairly good noise suppression. In particular, V-BM4D sharpens and reveals barely visible information hidden in the noisy videos, such as the fine details of the wall in the background. The proposed enhancement filter is minimally susceptible to noise even when strong sharpening is performed (i.e., α = 1.25), as shown by the smooth reconstruction of flat areas such as the hat of Foreman.</p><p>As explained in Section V, the temporal artifacts can be attenuated by treating differently the different 4-D spectrum coefficients along the temporal dimension. Fig. <ref type="figure">9</ref> shows the enhancement results of V-BM4D applied to the test sequence Miss America corrupted by white Gaussian noise with standard deviation σ = 10. In this experiment, we compare two different settings of the sharpening parameter α. In the first case, a constant value of α {DC,AC} = 1.25 is used for the sharpening of the whole 4-D spectrum, whereas in the second case two values α DC = 1.25 and α AC = 0.625 are used to apply a different level of sharpening to the temporal DC and AC coefficients, respectively. The bottom row of Fig. <ref type="figure">9</ref> shows the difference of two consecutive frames in the original, noisy, and filtered test sequence Miss America. One can clearly notice that the sequence processed using α DC = α AC is affected by flickering artifacts because the intensities of the background in the temporal difference are highly irregular; in contrast, such intensities are extremely smooth when α DC = α AC . Thus, a nonuniform sharpening of the 4-D spectrum allows V-BM4D to significantly attenuate the flickering, enabling a better temporal consistency among consecutive frames, yet maintaining excellent enhancement (sharpening) and noise reduction properties. As a matter of fact, the V-BM4D enhancement filter has been applied in biomedical imaging to facilitate the tracking of microtubules in RFP-EB3 time-lapse videomicroscopy sequences corrupted by both heavy noise and flickering artifacts <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Color Filtering</head><p>The proposed V-BM4D algorithm can be extended to color filtering using the same approach of the Color-BM3D image denoising algorithm <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>. We consider the denoising of noisy color (e.g., RGB) videos, having each channel independently corrupted by white Gaussian noise with standard deviation σ .</p><p>The algorithm proceeds as follows. At first, the RGB noisy video is transformed to a luminance-chrominance color space, then both the motion estimation and the grouping are computed within the luminance channel only, as this usually has the highest SNR and carries most of the significant information. Since the image structures do not typically vary among different channels, the results of the motion estimation and the grouping within the luminance can be also directly applied to the chrominance channels as well. Once the groups are formed, the three channels undergo the collaborative filtering and aggregation independently, thus producing three individual estimates. Eventually, the final denoised RGB video is obtained by inverting the color space transformation. Such approach is a reasonable tradeoff between the achieved denoising quality and the required computational complexity. Fig. <ref type="figure" target="#fig_9">10</ref> compares the denoising performances of V-BM4D against the state-of-the-art V-BM3D filter, on the color sequence Bus corrupted by white Gaussian noise having standard deviation σ = 40. As a subjective assessment, V-BM4D better preserves fine details, such as the road sign and the leaves of the trees in the background. Table <ref type="table" target="#tab_3">IV</ref> shows that, according to the objective metric considered, V-BM4D outperforms V-BM3D in every experiment, with PSNR gains of up to 1.5 dB.</p><p>The MOVIE index confirms the superior performances of V-BM4D, especially when the observations are corrupted with high level of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. COMPLEXITY</head><p>In our analysis, the complexity of the algorithm is measured as the number of basic arithmetic operations performed; other factors that may also influence the execution time, such as the number of memory accesses or memory consumption, have not been considered.</p><p>Each run of V-BM4D involves the execution of the hardthresholding stage (whose complexity is C ht V-BM4D ), of the Wiener-filtering stage (whose complexity is C wie V-BM4D ), and two runs of the motion estimation algorithm (whose complexity is C CT ). Hence, the V-BM4D overall complexity is</p><formula xml:id="formula_25">C V-BM4D = 2C CT + C ht V-BM4D + C wie V-BM4D .<label>(15)</label></formula><p>V-BM3D does not require any motion estimation, and thus, its complexity (C V-BM3D ) is given by the sum of the complexity of its hard-thresholding (C ht V-BM3D ) and Wiener-filtering (C wie V-BM3D ) stages</p><formula xml:id="formula_26">C V-BM3D = C ht V-BM3D + C wie V-BM3D . (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>To provide a fair comparison, we assume that the number of blocks in any spatiotemporal volume (2h+1) in V-BM4D coincides with the size of temporal search window (2N F R + 1) in V-BM3D; similarly, we assume that the number of grouped volumes in V-BM4D (referred to as M) corresponds to the number of grouped blocks in V-BM3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Computation of the Trajectory</head><p>The trajectory of a block is defined by the sequence of the most similar blocks in the preceding h and following h frames. In particular, each block is found from a full search within an adaptive search window of size up to N S × N S , and the similarity between two N × N blocks is tested using the 2 -norm of their difference. The calculation of the norm requires 3N 2 operations, i.e., two additions and one multiplication for each pair of corresponding pixels. Such trajectories are constructed for every block, and thus every voxel of the video. Therefore the total cost is</p><formula xml:id="formula_28">C CT = |X||T |( h -1)N 2 S 3N 2<label>(17)</label></formula><p>where |X| and |T | are the cardinalities of X and T , respectively; for the sake of notation simplicity, the temporal extent of the volumes is denoted by h = 2h + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hard-Thresholding Stage</head><p>In the hard-thresholding stage, for each processed volume, at first all volumes belonging to a search window of size N G × N G are tested for similarity, then the M most similar ones are stacked together in a 4-D group which is consequently transformed by a separable 4-D transform. The hard thresholding in transform domain is performed via element-wise comparisons, which require one arithmetical operation per element. At last, the basic estimate is obtained by aggregating the inverse 4-D transform of the filtered groups. Thus, we obtain</p><formula xml:id="formula_29">C ht V-BM4D = |X| N 2 step |T | N 2 G 3 h N 2 Grouping + M h N 2 Thresholding + M h N 2 Aggregation + 2 2M hC (N,N,N) + MC ( h, h,N 2 ) + C (M,M, hN 2 )</formula><p>Forward and Inverse Transformations <ref type="bibr" target="#b17">(18)</ref> where N step is the number of pixels separating the processed volumes, C (m, p,n) denotes the cost of a multiplication between matrices of size m × p and p × n (i.e., the cost of a linear transformation), and the factor 3 in the grouping complexity is due to the computation of the 2 -norm between two 3-D volumes of size N × N × h. The cost of 4-D transformation, being linear and separable, is the sum of four matrix multiplications, one for each dimension of the group. In V-BM3D, the grouping is obtained by a predictivesearch block matching <ref type="bibr" target="#b10">[11]</ref>. At first, this technique extracts the N B most similar blocks within an N G × N G window centered around the current processed block, then in the h preceding and h following frames, it inductively searches for other N B most similar blocks within N B windows of size N P R × N P R (with N P R N G ) centered around the position of the previous N B matched blocks. Once a group of M blocks is formed, the algorithm first applies the 3-D transformation, and then performs a coefficients shrinkage through hard thresholding. Thus, the complexity is</p><formula xml:id="formula_30">C ht V-BM3D = |X| N 2 step |T | N 2 G +N B h N 2 P R 3N 2 Grouping + M N 2 Thresholding + M N 2 Aggregation + 2 2MC (N,N,N) + C (M,M,N 2 )</formula><p>Forward and inverse transformations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Wiener-Filtering Stage</head><p>In the Wiener-filtering stage, the grouping is performed within the basic estimate obtained from the first stage, and two groups are subsequently formed. Both groups first undergo the forward transformation, then the shrinkage of the coefficients of the noisy group, implemented as an empirical Wiener filter, is realized via element-wise multiplication using the weights defined in <ref type="bibr" target="#b8">(9)</ref>. The calculation of (9) requires six arithmetic operations per group element. After the shrinkage, the inverse transformation is only applied to the filtered group. In conclusion, the overall cost for the V-BM4D algorithm is</p><formula xml:id="formula_32">C wie V-BM4D = |X| N 2 step |T | N 2 G 3 h N 2 Grouping + 6M h N 2 Shrinkage + M h N 2 Aggregation + 4 2M hC (N,N,N) + MC ( h, h,N 2 ) + C (M,M, h N 2 )</formula><p>Forward and Inverse Transformations .</p><p>Analogously, in V-BM3D the complexity of Wiener-filtering stage is</p><formula xml:id="formula_34">C wie V-BM3D = |X| N 2 step |T | N 2 G +N B h N 2 P R 3N 2 Grouping +6M N 2 Shrinkage + M N 2 Aggregation + 4 2MC (N,N,N) + C (M,M,N 2 )</formula><p>Forward and Inverse Transformations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparative Analysis</head><p>The complexities of V-BM3D and V-BM4D are O(|X||T |); thus, both algorithms scale linearly with the number of processed voxels. In our analysis, we compare the terms in <ref type="bibr" target="#b17">(18)</ref> and ( <ref type="formula" target="#formula_33">20</ref>) against the corresponding ones in ( <ref type="formula" target="#formula_31">19</ref>) and ( <ref type="formula" target="#formula_35">21</ref>), assuming that V-BM3D and V-BM4D share the same parameters. At first, we observe that the cost of the grouping is similar in both algorithms, and therefore, for the sake of our analysis, it can be neglected. Differently, the coefficients shrinkage, the aggregation, and the transformation steps (in both the Wiener-filtering and hard-thresholding stages) require h times more operations in V-BM4D than those in V-BM3D, as the filtering in V-BM4D involves the additional (fourth) dimension. Moreover, V-BM4D is burdened by the motion estimation step, whose complexity is defined in <ref type="bibr" target="#b16">(17)</ref>. However, this cost can be reduced when the input video is encoded with a motion-compensated algorithm, such as MPEG-4 or H.264, since the motion vectors required to build the spatiotemporal volumes can be directly extracted from the encoded data. In conclusion, under these assumptions, V-BM4D is at least h times computationally more demanding than V-BM3D. We compare the proposed adaptive full search (described in Section III-A) with a fast diamond search <ref type="bibr" target="#b46">[47]</ref> modified in order to incorporate the penalty term described in Section III-A.2 into the block matching. The time required to filter a single frame, and (in parenthesis) the time solely spent during the motion estimation are reported in the last column. The test sequence Tennis is corrupted by white gaussian noise having σ = 25.</p><p>Table V reports the PSNR values and the corresponding seconds per frame (1/frames/s) required by a single-threaded implementation of V-BM4D to process the test sequence Tennis (CIF resolution) on a 3-GHz core. We quantify the computational load of the grouping and the filtering, by modifying in both stages the size of the search window N G and the number of grouped volumes M, respectively. Then, we compare two different motion-estimation strategies, specifically the predictive search described in Section III-A and the fast diamond search algorithm presented in <ref type="bibr" target="#b46">[47]</ref> modified to incorporate in the block matching the penalty term described in Section III-A2. Finally, we fix N step = 6 in both stages to keep the average frame-per-second (frames/s) count unbiased. All the remaining V-BM4D parameters are set as in Table <ref type="table" target="#tab_0">I</ref>. We observe that the speed-ups induced by the fast motion estimation algorithm (8×), the smaller search window (15×), or the smaller group size (2.5×) correspond to marginal PSNR losses, thus demonstrating the good scalability properties of the proposed V-BM4D. Let us also remark that when the nonlocality features are disabled (i.e., M = 1 and N G = 1), every group is composed by its reference volume only. Thus, the motion estimation for the all the non-reference volumes can be safely skipped, enabling an additional speedup of 12× without compromising the final quality of the filtered sequence. Under such conditions, V-BM4D is able to process nearly 4 frames/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION</head><p>As anticipated in Section I, a severe limitation of V-BM3D lies in the grouping step, because the temporal and nonlocal spatial correlation properties are not distinguished in the grouped data. The improved effectiveness of V-BM4D indicates the importance of separately treating such different types of correlation, and, in particular, of explicitly accounting the motion information. In what follows, we analyze the PSNR performances of the two algorithms when a temporal-based or nonlocal-based grouping is encouraged. In order to do so, we vary the parameters that control the grouping strategy, namely (M, h) in V-BM4D and (N B , N F R ) in V-BM3D, in both the hard-thresholding and Wiener-filtering stages. In our experiments, we consider the test sequences Salesman and Tennis, being representative of videos characterized by lowand high-motion activity, respectively.</p><p>We recall that for a given pair (M, h) V-BM4D builds volumes having temporal extent up to 2h + 1 and stacks up to M of such volumes in the 4-D groups. In this analysis, we consider the pairs (M, h) = (1, 7), which yields groups composed of a single volume having temporal extent 15, and (M, h) = (16, 0), which yields groups composed of 16 volumes of extent having temporal extent 1. These settings correspond to a temporal-based grouping strategy in the former case, and to a nonlocal-based grouping strategy in the latter. Fig. <ref type="figure" target="#fig_10">11</ref> shows that, although the temporal-based groups are smaller than the nonlocal-based ones, they still yield a PSNR improvement of about 17% in Salesman and 13% in Tennis with respect to the basic configuration (M, h) = (1, 0). In contrast, the PSNR improvement induced by nonlocal-based groups is only about 4% in Salesman and 3% in Tennis. Note that the size of the groups in V-BM4D can be reduced down to 1, somehow resembling V-BM3D, without suffering from a substantial loss in terms of restoration quality. As a matter of fact, the PSNR values are only marginally worse than the corresponding results reported in Table <ref type="table" target="#tab_0">II</ref>, which are obtained using larger values of M. Interestingly, the performance for the test sequence Salesman decreases regularly when h ≥ 3 as the size M of the groups increases. Thus, for nearly stationary videos, the nonlocal correlation properties actually worsen the filtering outcome.</p><p>To reproduce the nonlocal-based grouping strategy in V-BM3D, we increase the parameter N B , controlling the number of self-similar blocks to be followed in the adjacent frames, and further we set d s = 0 to give no preference toward blocks belonging to different frames (i.e., blocks having the same coordinates of the reference one <ref type="bibr" target="#b10">[11]</ref>). Additionally, we fix the maximum size of the groups to N 2 = 16, so that bigger groups can be formed as N F R and/or N B increase. We stress that the composition of a group in V-BM3D is not known when N B × N F R &gt; N 2 , since the number of potential block candidates is greater than the maximum size of the group, and such candidates are unpredictably extracted from both the temporal and nonlocal dimensions. Fig. <ref type="figure" target="#fig_10">11</ref> illustrates the observed V-BM3D denoising performance. Similarly to V-BM4D, the plots show a consistent PSNR improvement along the temporal dimension (i.e., as N F R increases), and an almost regular loss along the nonlocal dimension (i.e., as N B becomes larger).</p><p>This analysis empirically demonstrates that: 1) in our framework, the nonlocal spatial correlation within the data does not dramatically affect the global PSNR of the restored video, although it becomes crucial in sequences in which the temporal correlation cannot be exploited (e.g., having frequent occlusions and scene changes) and 2) a grouping based only on temporal-correlated data always guarantees, both in V-BM4D and V-BM3D, higher performance than a grouping that only exploits the nonlocal spatial correlation. We also observe that if the volumes are composed by blocks having the same spatial coordinate (i.e., zero-motion assumption, or equivalently γ d = ∞), the denoising quality significantly decreases. For example, under such zero-motion assumption, we observe a PSNR loss of about 2.5 dB in the denoising of the test sequence Flower Garden corrupted by white Gaussian noise with standard deviation σ = 25 with respect to the corresponding experiment in Table <ref type="table" target="#tab_0">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>Experiments show that V-BM4D outperforms V-BM3D both in terms of objective performance (as reported in Tables II and IV), and of visual appearance (as shown in Figs. 6 and 10), thus achieving state-of-the-art results in video denoising. In particular, V-BM4D can restore fine image details much better than V-BM3D, even in sequences corrupted by heavy noise (σ = 40). However, the computational complexity of V-BM4D is obviously higher than that of V-BM3D, because of the need to perform the motion estimation, and the need to process higher dimensional data. Then, our analysis of the V-BM4D and V-BM3D frameworks highlighted that the temporal correlation is a key element in video denoising, as it represents an effective prior that has to be exploited when designing nonlocal video restoration algorithms. In conclusion, V-BM4D can be a viable alternative to V-BM3D especially in applications where the highest restoration quality is paramount, or when the separation of the four dimensions is essential.</p><p>V-BM4D has been also proven to be an effective deblocking and sharpening (enhancement) filter, providing excellent performances from both a subjective and objective point of view. Additionally, by exploiting the separability of the 4-D transform, we are able to alleviate the spatiotemporal artifacts (such as flickering) by acting differently on different transform coefficients. Furthermore, we remark that V-BM4D can be extended to color data filtering in each of its applications, namely denoising, deblocking, and sharpening.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of a spatiotemporal volume (left), and a group of mutually similar volumes (right).</figDesc><graphic coords="2,317.15,54.77,116.06,93.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Effect of different penalties γ d = 0.025 (left) and γ d = 0 (right) on the background textures of the sequence Tennis corrupted by Gaussian noise with σ = 20. The block positions at time t = 1 are the same in both experiments.</figDesc><graphic coords="4,54.47,56.57,115.94,92.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. From left to right, the second-order polynomials (10)-<ref type="bibr" target="#b11">(12)</ref> describing the relation between the parameters γ d , τ traj , and τ match and the noise standard deviation σ . The nine coefficients of the three polynomials have been determined by maximizing the sum of the PSNR of the denoised test sequences Salesman (+), Tennis (•), Flower Garden ( * ), Miss America (×), Coastguard ( ), Foreman (♦), Bus ( ), and Bicycle ( ), corrupted by white Gaussian noise having σ ranging between 5 and 70. As a reference, we superimpose the optimum parameters values for each test sequence and σ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Test sequences used in the optimization are Salesman (+), Tennis (•), Flower Garden ( * ), Miss America (×), Coastguard ( ), Foreman (♦), Bus ( ), and Bicycle ( ). (a) Optimum values of σ plotted against the bit-per-pixel (bpp) rate (top-left) and quantization parameter q (top-right). The bottom plots show the corresponding residual absolute errors. (b) Fitted curve of the optimum values of σ as a bivariate function σ (bpp, q) (13) used by the V-BM4D deblocking filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Frame-by-frame PSNR (dB) output of the test sequences Tennis (left) and Bus (right) corrupted by white Gaussian noise with standard deviation σ = 40 denoised by V-BM4D (thick line) and V-BM3D (thin line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visual comparison of the V-BM4D and V-BM3D denoising performance on the test sequence Flower Garden corrupted by white Gaussian noise with standard deviation σ = 40.</figDesc><graphic coords="10,71.87,179.33,108.14,65.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual comparison of the V-BM4D and MPlayer deblocking performance on the test sequence Tennis compressed by the MPEG-4 encoder with quantization parameter q = 25.</figDesc><graphic coords="10,71.87,302.09,108.14,66.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visual comparison of the V-BM4D joint denoising and sharpening performance on the test sequence (Foreman corrupted by white Gaussian noise with standard deviation σ = 25). Different settings of the sharpening parameter α are compared.</figDesc><graphic coords="10,71.87,425.81,108.14,66.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.</head><label></label><figDesc>Fig.9. Joint V-BM4D denoising, enhancement and deflickering of the test sequence Miss America 2 corrupted by white Gaussian noise with standard deviation σ = 10. The bottom row shows the differences between two consecutive frames of the original, noisy, and enhanced sequences. The right-most column shows the sharpening result using different α {DC,AC} in the temporal DC and AC coefficients of the groups spectra. The images in the bottom row are all drawn with respect to the same gray colormap, which is stretched four times in order to improve the visualization.</figDesc><graphic coords="10,71.87,495.29,108.14,66.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Visual comparison of the V-BM3D and V-BM4D color denoising performance on the test sequence Bus corrupted by white Gaussian noise with standard deviation σ = 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. PSNR (dB) denoising performances of V-BM3D and V-BM4D tuned with different combinations of the parameters (N B , N F R ) and (M, h), respectively. In V-BM3D, we set the size of the 3-D groups to N 2 = 16, and the distance penalty to d s = 0. The test sequences Salesman and Tennis are corrupted by white Gaussian noise with standard deviation σ = 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PARAMETER</head><label>I</label><figDesc>SETTINGS OF V-BM4D FOR THE FIRST (HARD-THRESHOLDING) AND THE SECOND (WIENER-FILTERING) STAGE. IN THE HARD-THRESHOLDING STAGE, THE THREE PARAMETERS γ d , τ TRAJ , AND τ MATCH VARY ACCORDING TO THE NOISE STANDARD DEVIATION</figDesc><table><row><cell>Stage</cell><cell>N</cell><cell>N S</cell><cell>N G</cell><cell>h</cell><cell>M</cell><cell>λ 4D</cell><cell>γ p</cell><cell>γ w</cell><cell>σ w</cell><cell>N step</cell><cell>γ d</cell><cell>τ traj</cell><cell>τ match</cell></row><row><cell>Hard thr. Wiener filt.</cell><cell>8 7</cell><cell>11</cell><cell>19 27</cell><cell>4</cell><cell>32 8</cell><cell>2.7 Does not apply</cell><cell>0.3</cell><cell>0.5</cell><cell>1</cell><cell>6 4</cell><cell>γ d (σ ) 0.005</cell><cell>τ traj (σ ) 1</cell><cell>τ match (σ ) 13.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III DEBLOCKING</head><label>III</label><figDesc>PERFORMANCE OF V-BM4D AND MPLAYER ACCURATE DEBLOCKING FILTER</figDesc><table><row><cell></cell><cell>Video:</cell><cell>Salesm.</cell><cell>Tennis</cell><cell>Fl. Gard.</cell><cell>Miss Am.</cell><cell>Coastg.</cell><cell>Foreman</cell><cell>Bus</cell><cell>Bicycle</cell></row><row><cell>q</cell><cell>Res.:</cell><cell>288×352</cell><cell>240×352</cell><cell>240×352</cell><cell>288×360</cell><cell>144×176</cell><cell>288×352</cell><cell>288×352</cell><cell>576×720</cell></row><row><cell></cell><cell>Frames:</cell><cell>50</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>300</cell><cell>300</cell><cell>150</cell><cell>30</cell></row><row><cell></cell><cell>bpp</cell><cell>0.3232</cell><cell>0.5323</cell><cell>1.4824</cell><cell>0.0884</cell><cell>0.4609</cell><cell>0.3005</cell><cell>0.7089</cell><cell>0.4315</cell></row><row><cell>5</cell><cell>V-BM4D Mplayer</cell><cell>35.95|0.16 35.14|0.17</cell><cell>34.41|0.18 33.79|0.17</cell><cell>33.54|0.05 32.73|0.07</cell><cell>39.51|0.15 38.58|0.14</cell><cell>34.75|0.13 34.00|0.13</cell><cell>36.49|0.16 35.60|0.14</cell><cell>35.05|0.13 34.36|0.10</cell><cell>38.01|0.08 36.53|0.11</cell></row><row><cell></cell><cell>Compr.</cell><cell>35.28|0.17</cell><cell>33.87|0.17</cell><cell>32.81|0.07</cell><cell>39.03|0.13</cell><cell>34.12|0.13</cell><cell>35.70|0.14</cell><cell>34.45|0.10</cell><cell>36.71|0.11</cell></row><row><cell></cell><cell>bpp</cell><cell>0.1319</cell><cell>0.2249</cell><cell>0.7288</cell><cell>0.0399</cell><cell>0.1926</cell><cell>0.1276</cell><cell>0.3285</cell><cell>0.2076</cell></row><row><cell>10</cell><cell>V-BM4D Mplayer</cell><cell>32.12|0.87 31.66|1.08</cell><cell>30.39|0.83 29.87|0.89</cell><cell>27.93|0.26 27.40|0.31</cell><cell>37.30|0.48 36.61|0.53</cell><cell>30.75|0.50 30.23|0.53</cell><cell>32.91|0.49 32.16|0.52</cell><cell>30.69|0.43 30.11|0.41</cell><cell>33.54|0.36 32.45|0.46</cell></row><row><cell></cell><cell>Compr.</cell><cell>31.54|0.86</cell><cell>29.84|0.78</cell><cell>27.41|0.29</cell><cell>36.66|0.46</cell><cell>30.19|0.51</cell><cell>32.09|0.48</cell><cell>30.07|0.36</cell><cell>32.37|0.46</cell></row><row><cell></cell><cell>bpp</cell><cell>0.0865</cell><cell>0.1326</cell><cell>0.4470</cell><cell>0.0318</cell><cell>0.1184</cell><cell>0.0812</cell><cell>0.2039</cell><cell>0.1333</cell></row><row><cell>15</cell><cell>V-BM4D Mplayer</cell><cell>30.06|1.89 29.65|2.39</cell><cell>28.48|1.49 28.03|1.52</cell><cell>25.15|0.58 24.68|0.68</cell><cell>36.13|0.82 35.59|0.90</cell><cell>28.73|1.01 28.30|1.10</cell><cell>31.10|0.90 30.36|0.98</cell><cell>28.48|0.85 27.89|0.83</cell><cell>31.16|0.79 30.12|0.95</cell></row><row><cell></cell><cell>Compr.</cell><cell>29.48|1.78</cell><cell>27.97|1.39</cell><cell>24.67|0.63</cell><cell>35.41|0.81</cell><cell>28.18|1.03</cell><cell>30.27|0.90</cell><cell>27.83|0.71</cell><cell>30.00|0.98</cell></row><row><cell></cell><cell>bpp</cell><cell>0.0661</cell><cell>0.0943</cell><cell>0.3058</cell><cell>0.0280</cell><cell>0.0852</cell><cell>0.0625</cell><cell>0.1453</cell><cell>0.0985</cell></row><row><cell>20</cell><cell>V-BM4D Mplayer</cell><cell>28.66|3.03 28.31|3.76</cell><cell>27.24|2.07 26.82|2.12</cell><cell>23.34|0.95 22.90|1.12</cell><cell>35.02|1.21 32.93|1.58</cell><cell>27.42|1.73 27.04|1.96</cell><cell>29.85|1.38 29.12|1.55</cell><cell>26.96|1.38 26.42|1.42</cell><cell>29.52|1.26 28.60|1.56</cell></row><row><cell></cell><cell>Compr.</cell><cell>28.11|2.71</cell><cell>26.76|1.93</cell><cell>22.88|1.02</cell><cell>34.21|1.21</cell><cell>26.90|1.73</cell><cell>29.03|1.37</cell><cell>26.35|1.16</cell><cell>28.43|1.58</cell></row><row><cell></cell><cell>bpp</cell><cell>0.0546</cell><cell>0.0710</cell><cell>0.2225</cell><cell>0.0257</cell><cell>0.0679</cell><cell>0.0523</cell><cell>0.1121</cell><cell>0.0846</cell></row><row><cell>25</cell><cell>V-BM4D Mplayer</cell><cell>27.63|4.19 27.30|5.09</cell><cell>26.34|2.55 25.96|2.57</cell><cell>22.07|1.38 21.63|1.64</cell><cell>34.31|1.54 33.66|1.70</cell><cell>26.47|2.53 26.11|2.95</cell><cell>29.01|1.87 28.25|2.13</cell><cell>25.93|1.96 25.38|2.04</cell><cell>28.32|1.78 27.35|2.18</cell></row><row><cell></cell><cell>Compr.</cell><cell>27.07|3.63</cell><cell>25.85|2.38</cell><cell>21.62|1.49</cell><cell>33.45|1.57</cell><cell>25.98|2.45</cell><cell>28.10|1.86</cell><cell>25.27|1.66</cell><cell>27.22|2.20</cell></row><row><cell></cell><cell>bpp</cell><cell>0.0477</cell><cell>0.0604</cell><cell>0.1697</cell><cell>0.0244</cell><cell>0.0584</cell><cell>0.0480</cell><cell>0.0921</cell><cell>0.0676</cell></row><row><cell>30</cell><cell>V-BM4D Mplayer</cell><cell>26.84|5.38 26.51|6.31</cell><cell>25.59|2.99 25.26|3.02</cell><cell>21.08|1.86 20.65|2.24</cell><cell>33.25|1.90 32.80|2.08</cell><cell>25.72|3.53 25.38|4.20</cell><cell>28.30|2.33 27.57|2.68</cell><cell>25.06|2.57 24.55|2.70</cell><cell>27.40|2.34 26.54|2.88</cell></row><row><cell></cell><cell>Compr.</cell><cell>26.28|4.59</cell><cell>25.11|2.77</cell><cell>20.64|1.99</cell><cell>32.39|1.97</cell><cell>25.25|3.31</cell><cell>27.37|2.31</cell><cell>24.41|2.19</cell><cell>26.35|2.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COLOR</head><label>IV</label><figDesc>DENOISING PERFORMANCE OF V-BM3D AND V-BM4D IN TERMS OF PSNR (dB) AND MOVIE INDEX [46] (THE LOWER THE BETTER) VALUES ARE REPORTED IN THE LEFT AND RIGHT PART OF EACH CELL, RESPECTIVELY In order to enhance the readability of the results, every MOVIE index has been multiplied by 10 3 . The test sequences are corrupted by white gaussian noise with different values of standard deviation σ.</figDesc><table><row><cell></cell><cell>Video:</cell><cell>Tennis</cell><cell>Coastg.</cell><cell>Foreman</cell><cell>Bus</cell></row><row><cell>σ</cell><cell>Res.:</cell><cell>240×352</cell><cell>144×176</cell><cell>288×352</cell><cell>288×352</cell></row><row><cell></cell><cell>Frames:</cell><cell>150</cell><cell>300</cell><cell>300</cell><cell>150</cell></row><row><cell>5</cell><cell>V-BM4D V-BM3D</cell><cell>39.98|0.01 39.45|0.01</cell><cell>41.13|0.01 40.18|0.01</cell><cell>41.38|0.01 40.56|0.01</cell><cell>40.21|0.01 39.07|0.01</cell></row><row><cell>10</cell><cell>V-BM4D V-BM3D</cell><cell>36.42|0.04 36.04|0.04</cell><cell>37.28|0.03 36.82|0.03</cell><cell>37.92|0.05 37.52|0.04</cell><cell>36.23|0.05 34.96|0.07</cell></row><row><cell>20</cell><cell>V-BM4D V-BM3D</cell><cell>32.88|0.17 32.54|0.18</cell><cell>33.61|0.13 33.39|0.14</cell><cell>34.62|0.15 34.49|0.16</cell><cell>32.27|0.20 31.03|0.32</cell></row><row><cell>40</cell><cell>V-BM4D V-BM3D</cell><cell>29.52|0.70 29.20|0.82</cell><cell>30.00|0.42 29.99|0.63</cell><cell>31.30|0.44 31.17|0.56</cell><cell>28.32|0.70 27.34|1.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PSNR</head><label>V</label><figDesc>(dB) AND TIME (S) DENOISING PERFORMANCE OF V-BM4D TUNED WITH DIFFERENT COMBINATIONS OF THE PARAMETERS M AND N G USING DIFFERENT MOTION ESTIMATION ALGORITHMS</figDesc><table><row><cell>Mot. est.</cell><cell>M</cell><cell>N G</cell><cell>PSNR</cell><cell cols="2">1/frames/s</cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>29.88</cell><cell>3.07</cell><cell>(2.8)</cell></row><row><cell>Mod. [47]</cell><cell>1</cell><cell>19</cell><cell>29.88</cell><cell>7.36</cell><cell>(2.8)</cell></row><row><cell></cell><cell>32</cell><cell>19</cell><cell>30.17</cell><cell>14.57</cell><cell>(2.8)</cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>30.07</cell><cell>22.42</cell><cell>(22.1)</cell></row><row><cell>Sec. III-A</cell><cell>1</cell><cell>19</cell><cell>30.07</cell><cell>26.76</cell><cell>(22.1)</cell></row><row><cell></cell><cell>32</cell><cell>19</cell><cell>30.32</cell><cell>33.99</cell><cell>(22.1)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Supplementary downloadable material available at http://ieeexplore.ieee.org, provided by the authors. This includes an uncompressed AVI format movie clip showing the joint denoising, enhancement and deflickering of V-BM4D. This material is 62.2 MB in size.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the Academy of Finland under Project 213462, the Finnish Programme for Centers of Excellence in Research 2006-2011 and Project 252547, the Academy Research Fellow 2011-2016 and Project 129118, the Post-Doctoral Researchers 2009-2011, and by Tampere Graduate School in Information Science and Engineering (TISE)</p><p>. This paper is based on and extends the authors' preliminary conference publications <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The associate editor coordinating the review of this manuscript and approving it for publication was F. A. Baqai. M. Maggioni, A. Foi, and K. Egiazarian are with the</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His current research interests include mathematical and statistical methods for signal processing, functional and harmonic analysis, and computational modeling of the human visual system. He is currently an Academy Research Fellow with the Department of Signal Processing, Academy of Finland, Tampere University of Technology. His recent work focuses on spatially adaptive (anisotropic, nonlocal) algorithms for the restoration and enhancement of digital images, on noise modeling for imaging devices, and on the optimal design of statistical transformations for the stabilization, normalization, and analysis of random data.</p><p>Dr. Foi is an Associate Editor for the IEEE TRANSACTIONS ON IMAGE PROCESSING.</p><p>Karen Egiazarian (SM'96) was born in Yerevan, Armenia, in 1959. He received the M.Sc. degree in mathematics from Yerevan State University, Yerevan, Armenia, the Ph.D. degree in physics and mathematics from Moscow State University, Moscow, Russia, and the D.Tech. degree from the Tampere University of Technology (TUT), <ref type="bibr">Tampere, Finland, in 1981</ref><ref type="bibr">, 1986</ref><ref type="bibr">, and 1994, respectively.</ref> He is a Professor of signal processing with the Computational Imaging and Transforms Group, TUT. He has published more than 500 refereed journals and conference articles. His current research interests include computational imaging, image and video denoising, image compression, and digital logic.</p><p>Dr. Egiazarian is an Associate Editor of several journals, such as SPIE Journal of Electronic Imaging and Research Letters in Signal Processing. He is a member of the DSP Technical Committee of the IEEE Circuits and Systems Society.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video denoising using separable 4D nonlocal spatiotemporal transforms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2011-01">Jan. 2011</date>
			<biblScope unit="volume">7870</biblScope>
			<biblScope unit="page" from="787003" to="787004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Four-dimensional collaborative denoising and enhancement of timelapse imaging of mCherry-EB3 in hippocampal neuron growth cones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BioPhoton. Imag. Conf</title>
		<meeting>BioPhoton. Imag. Conf</meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image sequence denoising via sparse and redundant representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonlocal video denoising, simplification and inpainting using discrete regularization on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghoniem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmoataz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2445" to="2455" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noise reduction through detection of signal redundancy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><surname>Bonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rethinking Artificial Intelligence</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From local kernel to nonlocal multiple-model image denoising</title>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocal image and movie denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="139" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Patch-based video processing: A variational bayesian approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08">Aug. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video denoising by sparse 3D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Eur. Signal Process. Conf</title>
		<meeting>15th Eur. Signal ess. Conf<address><addrLine>Poznan, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="page" from="145" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiframe raw-data denoising based on blockmatching and 3-D filtering for low-light imaging and stabilization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Local Non-Local Approx. Image Process</title>
		<meeting>Int. Workshop Local Non-Local Approx. Image ess</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint image sharpening and denoising by 3D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. TICSP Workshop Spectral Methods Multirate Signal Process</title>
		<meeting>Int. TICSP Workshop Spectral Methods Multirate Signal ess</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image and video super-resolution via spatially adaptive block-matching filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Danielyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Local Non-Local Approx. Image Process</title>
		<meeting>Int. Workshop Local Non-Local Approx. Image ess</meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image upsampling via spatially adaptive block-matching filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Danielyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Eur. Signal Process. Conf</title>
		<meeting>16th Eur. Signal ess. Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Danielyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<title level="m">Spatially Adaptive Filtering as Regularization in Inverse Imaging: Compressive Sensing, Upsampling, and Super-Resolution</title>
		<imprint/>
	</monogr>
	<note>in Super-Resolution Imaging</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image restoration by sparse 3D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
			<biblScope unit="volume">6812</biblScope>
			<biblScope unit="page" from="681207" to="681208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motion estimation for video coding standards</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Signal Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A survey of spatio-temporal grouping techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Megret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dementhon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Content based video matching using spatiotemporal volumes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="377" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3D collaborative filtering with grouping constraint luminance-chrominance space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local adaptive transform based image denoising with varying window size</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oktem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted overcomplete denoising</title>
		<author>
			<persName><forename type="first">O</forename><surname>Guleryuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Rec. 37th Asilomar Conf. Signals, Syst. Comput</title>
		<meeting>Conf. Rec. 37th Asilomar Conf. Signals, Syst. Comput</meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1992" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simplex method for function minimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convergence properties of the Nelder-Mead simplex method in low dimensions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lagarias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Opt</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="147" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The MPEG-4 video standard verification model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the H.264/AVC video coding standard</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="560" to="576" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive postfiltering of transform coefficients for the reduction of blocking artifacts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="594" to="602" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiframe blockingartifact reduction for transform-coded video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gunturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altunbasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="276" to="282" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple algorithm for removing blocking artifacts in block-transform coded images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="35" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Subband adaptive regularization method for removing blocking effect</title>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="1995-10">Oct. 1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="523" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deblocking of block-transform compressed images using phase-adaptive shifted thresholding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Multimedia</title>
		<meeting>IEEE Int. Symp. Multimedia</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="97" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Evan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transform coefficient histogram-based image enhancement algorithms using contrast entropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="741" to="758" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Polynomial and rational operators for image processing and analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Image Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="203" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear unsharp masking methods for image contrast enhancement</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="1996-07">Jul. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image enhancement via adaptive unsharp masking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Polesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="505" to="510" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An image enhancement technique combining sharpening and noise reduction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Instrum</title>
		<meeting>IEEE Instrum</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1921" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quadratic weighted median filters for edge enhancement of noisy images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Aysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3294" to="3310" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image sharpening using permutation weighted medians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Signal Process. Conf</title>
		<meeting>10th Eur. Signal ess. Conf<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">Sep. 2000</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transform image enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aghagolzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Ersoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="626" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wavelet based fingerprint image enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hatami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamarei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst</title>
		<meeting>IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4610" to="4613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Artifacts in alpha-rooting of images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcclellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="1980-04">Apr. 1980</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="449" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A perceptual model for JPEG applications based on block classification, texture masking, and luminance masking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="428" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced efficient diamond search algorithm for fast block motion estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcneelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bayoumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Circuits Syst</title>
		<imprint>
			<biblScope unit="page" from="3198" to="3201" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
