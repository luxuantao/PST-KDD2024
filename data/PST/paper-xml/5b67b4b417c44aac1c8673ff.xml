<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Batch Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carla</forename><surname>Gomes</surname></persName>
							<email>gomes@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
							<email>selman@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Batch Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F873F1EE973899C9A825F4A9FD733E0A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Normalizing the input data of neural networks to zero-mean and constant standard deviation has been known for decades <ref type="bibr" target="#b28">[29]</ref> to be beneficial to neural network training. With the rise of deep networks, Batch Normalization (BN) naturally extends this idea across the intermediate layers within a deep network <ref type="bibr" target="#b22">[23]</ref>, although for speed reasons the normalization is performed across mini-batches and not the entire training set. Nowadays, there is little disagreement in the machine learning community that BN accelerates training, enables higher learning rates, and improves generalization accuracy <ref type="bibr" target="#b22">[23]</ref> and BN has successfully proliferated throughout all areas of deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref>. However, despite its undeniable success, there is still little consensus on why the benefits of BN are so pronounced. In their original publication <ref type="bibr" target="#b22">[23]</ref> Ioffe and Szegedy hypothesize that BN may alleviate "internal covariate shift" -the tendency of the distribution of activations to drift during training, thus affecting the inputs to subsequent layers. However, other explanations such as improved stability of concurrent updates <ref type="bibr" target="#b12">[13]</ref> or conditioning <ref type="bibr" target="#b41">[42]</ref> have also been proposed.</p><p>Inspired by recent empirical insights into deep learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57]</ref>, in this paper we aim to clarify these vague intuitions by placing them on solid experimental footing. We show that the activations and gradients in deep neural networks without BN tend to be heavy-tailed. In particular, during an early on-set of divergence, a small subset of activations (typically in deep layer) "explode". The typical practice to avoid such divergence is to set the learning rate to be sufficiently small such that no steep gradient direction can lead to divergence. However, small learning rates yield little progress along flat directions of the optimization landscape and may be more prone to convergence to sharp local minima with possibly worse generalization performance <ref type="bibr" target="#b24">[25]</ref>.</p><p>BN avoids activation explosion by repeatedly correcting all activations to be zero-mean and of unit standard deviation. With this "safety precaution", it is possible to train networks with large learning rates, as activations cannot grow incrontrollably since their means and variances are normalized. SGD with large learning rates yields faster convergence along the flat directions of the optimization landscape and is less likely to get stuck in sharp minima.</p><p>We investigate the interval of viable learning rates for networks with and without BN and conclude that BN is much more forgiving to very large learning rates. Experimentally, we demonstrate that the activations in deep networks without BN grow dramatically with depth if the learning rate is too large. Finally, we investigate the impact of random weight initialization on the gradients in the network and make connections with recent results from random matrix theory that suggest that traditional initialization schemes may not be well suited for networks with many layers -unless BN is used to increase the network's robustness against ill-conditioned weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Batch Normalization Algorithm</head><p>As in <ref type="bibr" target="#b22">[23]</ref>, we primarily consider BN for convolutional neural networks. Both the input and output of a BN layer are four dimensional tensors, which we refer to as I b,c,x,y and O b,c,x,y , respectively. The dimensions corresponding to examples within a batch b, channel c, and two spatial dimensions x, y respectively. For input images the channels correspond to the RGB channels. BN applies the same normalization for all activations in a given channel,</p><formula xml:id="formula_0">O b,c,x,y ← γ c I b,c,x,y -µ c σ 2 c + + β c ∀ b, c, x, y.<label>(1)</label></formula><p>Here, BN subtracts the mean activation µ c = 1 |B| b,x,y I b,c,x,y from all input activations in channel c, where B contains all activations in channel c across all features b in the entire mini-batch and all spatial x, y locations. Subsequently, BN divides the centered activation by the standard deviation σ c (plus for numerical stability) which is calculated analogously. During testing, running averages of the mean and variances are used. Normalization is followed by a channel-wise affine transformation parametrized through γ c , β c , which are learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Experimental Setup</head><p>To investigate batch normalization we will use an experimental setup similar to the original Resnet paper <ref type="bibr" target="#b16">[17]</ref>: image classification on CIFAR10 <ref type="bibr" target="#b26">[27]</ref> with a 110 layer Resnet. We use SGD with momentum and weight decay, employ standard data augmentation and image preprocessing techniques and decrease learning rate when learning plateaus, all as in <ref type="bibr" target="#b16">[17]</ref> and with the same parameter values. The original network can be trained with initial learning rate 0.1 over 165 epochs, however which fails without BN. We always report the best results among initial learning rates from {0.1, 0.003, 0.001, 0.0003, 0.0001, 0.00003} and use enough epochs such that learning plateaus. For further details, we refer to Appendix B in the online version <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Disentangling the benefits of BN</head><p>Without batch normalization, we have found that the initial learning rate of the Resnet model needs to be decreased to α = 0.0001 for convergence and training takes roughly 2400 epochs. We refer to this architecture as an unnormalized network. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref> this configuration does not attain the accuracy of its normalized counterpart. Thus, seemingly, batch normalization yields faster training, higher accuracy and enable higher learning rates. To disentangle how these benefits are related, we train a batch normalized network using the learning rate and the number of epochs of an unnormalized network, as well as an initial learning rate of α = 0.003 which requires 1320 epochs for training. These results are also illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, where we see that a batch normalized networks with such a low learning schedule performs no better than an unnormalized network. Additionally, the train-test gap is much larger than for normalized networks using lr α = 0.1, indicating more overfitting. A learning rate of α = 0.003 gives results in between these extremes. This suggests that it is the higher learning rate that BN enables, which mediates the majority of its benefits; it improves regularization, accuracy and gives faster convergence. Similar results can be shown for variants of BN, see Table <ref type="table">4</ref> in Appendix K of the online version <ref type="bibr" target="#b3">[4]</ref>. We used a 110-layer Resnet with three distinct learning rates 0.0001, 0.003, 0.1. The smallest, 0.0001 was picked such that the network without BN converges. The figure shows that with matching learning rates, both networks, with BN and without, result in comparable testing accuracies (red and green lines in right plot). In contrast, larger learning rates yield higher test accuracy for BN networks, and diverge for unnormalized networks (not shown). All results are averaged over five runs with std shown as shaded region around mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning rate and generalization</head><p>To explain these observations we consider a simple model of SGD; the loss function (x) is a sum over the losses of individual examples in our dataset (x) = 1 N N i=1 i (x). We model SGD as sampling a set B of examples from the dataset with replacements, and then with learning rate α estimate the gradient step as α∇ SGD (x) = α |B| i∈B ∇ i (x). If we subtract and add α∇ (x) from this expression we can restate the estimated gradient ∇ SGD (x) as the true gradient, and a noise term</p><formula xml:id="formula_1">α∇ SGD (x) = α∇ (x) gradient + α |B| i∈B ∇ i (x) -∇ (x) error term</formula><p>.</p><p>We note that since we sample uniformly we have E α |B| i∈B ∇ i (x) -∇ (x) = 0. Thus the gradient estimate is unbiased, but will typically be noisy. Let us define an architecture dependent noise quantity C of a single gradient estimate such that C = E ∇ i (x) -∇ (x)|| 2 . Using basic linear algebra and probability theory, see Apppendix D, we can upper-bound the noise of the gradient step estimate given by SGD as</p><formula xml:id="formula_2">E α∇ (x) -α∇ SGD (x) 2 ≤ α 2 |B| C.<label>(2)</label></formula><p>Depending on the tightness of this bound, it suggests that the noise in an SGD step is affected similarly by the learning rate as by the inverse mini-batch size 1 |B| . This has indeed been observed in practice in the context of parallelizing neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref> and derived in other theoretical models <ref type="bibr" target="#b23">[24]</ref>. It is widely believed that the noise in SGD has an important role in regularizing neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b56">57]</ref>. Most pertinent to us is the work of Keskar et al. <ref type="bibr" target="#b24">[25]</ref>, where it is empirically demonstrated that large mini-batches lead to convergence in sharp minima, which often generalize poorly. The intuition is that larger SGD noise from smaller mini-batches prevents the network from getting "trapped" in sharp minima and therefore bias it towards wider minima with better generalization. Our observation from (2) implies that SGD noise is similarly affected by the learning rate as by the inverse mini-bath size, suggesting that a higher learning rate would similarly bias the network towards wider minima. We thus argue that the better generalization accuracy of networks with BN, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, can be explained by the higher learning rates that BN enables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Batch Normalization and Divergence</head><p>So far we have provided empirical evidence that the benefits of batch normalization are primarily caused by higher learning rates. We now investigate why BN facilitates training with higher learning rates in the first place. In our experiments, the maximum learning rates for unnormalized networks have been limited by the tendency of neural networks to diverge for large rates, which typically happens in the first few mini-batches. We therefore focus on the gradients at initialization. When comparing the gradients between batch normalized and unnormalized networks one consistently finds that the gradients of comparable parameters are larger and distributed with heavier tails in unnormalized networks. Representative distributions for gradients within a convolutional kernel are illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. A natural way of investigating divergence is to look at the loss landscape along the gradient direction during the first few mini-batches that occur with the normal learning rate (0.1 with BN, 0.0001 without). In Figure <ref type="figure">3</ref> we compare networks with and without BN in this regard. For each network we compute the gradient on individual batches and plot the relative change in loss as a function of the step-size (i.e. new_loss/old_loss). (Please note the different scales along the vertical axes.) For unnormalized networks only small gradient steps lead to reductions in loss, whereas networks with BN can use a far broader range of learning rates.</p><p>Let us define network divergence as the point when the loss of a mini-batch increases beyond 10 3 (a point from which networks have never managed to recover to acceptable accuracies in our experiments). With this definition, we can precisely find the gradient update responsible for divergence. It is interesting to see what happens with the means and variances of the network activations along a 'diverging update'. Figure <ref type="figure" target="#fig_2">4</ref> shows the means and variances of channels in three layers <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr">80)</ref> during such an update (without BN). The color bar reveals that the scale of the later layer's activations and variances is orders of magnitudes higher than the earlier layer. This seems to suggest that the divergence is caused by activations growing progressively larger with network depth, with the network output "exploding" which results in a diverging loss. BN successfully mitigates this phenomenon by correcting the activations of each channel and each layer to zero-mean and unit standard deviation, which ensures that large activations in lower levels cannot propagate uncontrollably upwards. We argue that this is the primary mechanism by which batch normalization enables higher learning rates. This explanation is also consistent with the general folklore observations that shallower networks allow for larger learning rates, which we verify in Appendix H. In shallower networks there aren't as many layers in which the activation explosion can propagate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Batch Normalization and Gradients</head><p>Figure <ref type="figure" target="#fig_2">4</ref> shows that the moments of unnormalized networks explode during network divergence and Figure <ref type="figure" target="#fig_3">5</ref> depicts the moments as a function of the layer depth after initialization (without BN) in log-scale. The means and variances of channels in the network tend to increase with the depth of the network even at initialization time -suggesting that a substantial part of this growth is data independent. In Figure <ref type="figure" target="#fig_3">5</ref> we also note that the network transforms normalized inputs into an output that reaches scales of up to 10 2 for the largest output channels. It is natural to suspect that such a dramatic relationship between output and input are responsible for the large gradients seen in Figure <ref type="figure" target="#fig_1">2</ref>. To test this intuition, we train a Resnet that uses one batch normalization layer only at the very last layer of the network, normalizing the output of the last residual block but no intermediate activation. Such an architecture allows for learning rates up to 0.03 and yields a final test accuracy of 90.1%, see Appendix E -capturing two-thirds of the overall BN improvement (see Figure <ref type="figure" target="#fig_0">1</ref>). This suggests that normalizing the final layer of a deep network may be one of the most important contributions of BN.</p><p>For the final output layer corresponding to the classification, a large channel mean implies that the network is biased towards the corresponding class. In Figure <ref type="figure" target="#fig_4">6</ref> we created a heatmap of ∂L b ∂O b,j after  initialization, where L b is the loss for image b in our mini-batch, and activations j corresponds to class j at the final layer. A yellow entry indicates that the gradient is positive, and the step along the negative gradient would decrease the prediction strength of this class for this particular image. A dark blue entry indicates a negative gradient, indicating that this particular class prediction should be strengthened. Each row contains one dark blue entry, which corresponds to the true class of this particular image (as initially all predictions are arbitrary). A striking observation is the distinctly yellow column in the left heatmap (network without BN). This indicates that after initialization the network tends to almost always predict the same (typically wrong) class, which is then corrected with a strong gradient update. In contrast, the network with BN does not exhibit the same behavior, instead positive gradients are distributed throughout all classes. Figure <ref type="figure" target="#fig_4">6</ref> also sheds light onto why the gradients of networks without BN tend to be so large in the final layers: the rows of the heatmap (corresponding to different images in the mini-batch) are highly correlated. Especially the gradients in the last column are positive for almost all images (the only exceptions being those image that truly belong to this particular class label). The gradients, summed across all images in the minibatch, therefore consist of a sum of terms with matching signs and yield large absolute values. Further, these gradients differ little across inputs, suggesting that most of the optimization work is done to rectify a bad initial state rather than learning from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gradients of convolutional parameters</head><p>We observe that the gradients in the last layer can be dominated by some arbitrary bias towards a particular class. Can a similar reason explain why the gradients for convolutional weights are larger for unnormalized networks. Let us consider a convolutional weight K o,i,x,y , where the first two dimensions correspond to the outgoing/ingoing channels and the two latter to the spatial dimensions.</p><formula xml:id="formula_3">a = bxy |d bxy cociij | b = | bij d bxy cociij | a/b</formula><p>For notational clarity we consider 3-by-3 convolutions and define S = {-1, 0, 1} × {-1, 0, 1} which indexes into K along spatial dimensions. Using definitions from section 1.1 we have</p><formula xml:id="formula_4">O b,c,x,y = c x ,y ∈S I b,c ,x+x ,y+y K c,c ,x ,y<label>(3)</label></formula><p>Now for some parameter K o,i,x,y inside the convolutional weight K, its derivate with respect to the loss is given by the backprop equation <ref type="bibr" target="#b39">[40]</ref> and (3) as</p><formula xml:id="formula_5">∂L ∂K o,i,x ,y = b,x,y d bxy o,i,x ,y , where d bxy o,i,x ,y = ∂L ∂ O b,o,x,y I b,i,x+x ,y+y .<label>(4)</label></formula><p>The gradient for K o,i,x,y is the sum over the gradients of examples within the mini-batch, and over the convoluted spatial dimensions. We investigate the signs of the summands in (4) across both network types and probe the sums at initialization in Table <ref type="table" target="#tab_0">1</ref>. For an unnormalized networks the absolute value of (4) and the sum of the absolute values of the summands generally agree to within a factor 2 or less. For a batch normalized network, these expressions differ by a factor of 10 2 , which explains the stark difference in gradient magnitude between normalized and unnormalized networks observed in Figure <ref type="figure" target="#fig_1">2</ref>. These results suggest that for an unnormalized network, the summands in (4) are similar across both spatial dimensions and examples within a batch. They thus encode information that is neither input-dependent or dependent upon spatial dimensions, and we argue that the learning rate would be limited by the large input-independent gradient component and that it might be too small for the input-dependent component. We probe these questions further in Appendix J, where we investigate individual parameters instead of averages.</p><p>Table <ref type="table" target="#tab_0">1</ref> suggests that for an unnormalized network the gradients are similar across spatial dimensions and images within a batch. It's unclear however how they vary across the input/output channels i, o.</p><p>To study this we consider the matrix M i,o = xy | bxy d bxy oixy | at initialization, which intuitively measures the average gradient magnitude of kernel parameters between input channel i and output channel o. Representative results are illustrated in Figure <ref type="figure" target="#fig_5">7</ref>. The heatmap shows a clear trend that some channels constantly are associated with larger gradients while others have extremely small gradients by comparison. Since some channels have large means, we expect in light of (4) that weights outgoing from such channels would have large gradients which would explain the structure in Figure <ref type="figure" target="#fig_5">7</ref>. This is indeed the case, see Appendix G in the online version <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Random initialization</head><p>In this section argue that the gradient explosion in networks without BN is a natural consequence of random initialization. This idea seems to be at odds with the trusted Xavier initialization scheme <ref type="bibr" target="#b11">[12]</ref> which we use. Doesn't such initialization guarantee a network where information flows smoothly between layers? These initialization schemes are generally derived from the desiderata that the variance of channels should be constant when randomization is taken over random weights. We argue that this condition is too weak. For example, a pathological initialization that sets weights to 0 or 100 with some probability could fulfill it. In <ref type="bibr" target="#b11">[12]</ref> the authors make simplifying assumptions that essentially result in a linear neural network. We consider a similar scenario and connect them with recent results in random matrix theory to gain further insights into network generalization. Let us consider a simple toy model: a linear feed-forward neural network where A t ... A 2 A 1 x = y, for weight matrices A 1 , A 2 ...A n . While such a model clearly abstracts away many important points they have proven to be valuable models for theoretical studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">56]</ref>. CNNs can, of course, be flattened into fully-connected layers with shared weights. Now, if the matrices are initialized randomly, the network can simply be described by a product of random matrices. Such products have recently garnered attention in the field of random matrix theory, from which we have the following recent result due to <ref type="bibr" target="#b29">[30]</ref>.</p><p>Theorem 1 Singular value distribution of products of independent Gaussian matrices <ref type="bibr" target="#b29">[30]</ref>. Assume that X = X 1 X 2 ...X M , where X i are independent N × N Gaussian matrices s.t. E[X i, jk ] = 0 and E[X 2 i, jk ] = σ 2 i /N for all matrices i and indices j, k. In the limit N → ∞, the expected singular value density ρ M (x) of X for x ∈ 0, (M + 1) M +1 /M M is given by ρ M (x) = 1 πx sin((M + 1)ϕ) sin(M ϕ) sin ϕ, where x = sin((M + 1)ϕ)       A closer look at (5) reveals that the distribution blows up as x -M/(M +1) nears the origin, and that the largest singular value scales as O(M ) for large matrices. In Figure <ref type="figure" target="#fig_13">9</ref> we investigate the singular value distribution for practically sized matrices. By multiplying more matrices, which represents a deeper linear network, the singular values distribution becomes significantly more heavy-tailed. Intuitively this means that the ratio between the largest and smallest singular value (the condition number) will increase with depth, which we verify in Figure <ref type="figure" target="#fig_1">20</ref>   κ has the following effects on solving a linear system with gradient descent: 1) convergence becomes slower, 2) a smaller learning rate is needed, 3) the ratio between gradients in different subspaces increases <ref type="bibr" target="#b2">[3]</ref>. There are many parallels between these results from numerical optimization, and what is observed in practice in deep learning. Based upon Theorem 1, we expect the conditioning of a linear neural network at initialization for more shallow networks to be better which would allow a higher learning rate. And indeed, for an unnormalized Resnet one can use a much larger learning if it has only few layers, see Appendix H. An increased condition number also results in different subspaces of the linear regression problem being scaled differently, although the notion of subspaces are lacking in ANNs, Figure <ref type="figure" target="#fig_3">5</ref> and 7 show that the scale of channels differ dramatically in unnormalized networks. The Xavier <ref type="bibr" target="#b11">[12]</ref> and Kaming initialization schemes <ref type="bibr" target="#b15">[16]</ref> amounts to a random matrix with iid entries that are all scaled proportionally to n -1/2 , the same exponent as in Theorem 1, with different constant factors. Theorem 1 suggests that such an initialization will yield ill-conditioned matrices, independent of these scale factors. If we accept these shortcomings of Xavier-initialization, the importance of making networks robust to initialization schemes becomes more natural.</p><formula xml:id="formula_6">M +1 sin ϕ(sin(M ϕ)) M (5) ⇢ M (x) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 3 T d D I + 2 y s c O Z S 7 k r 7 1 R C Z 9 2 d F Q = " &gt; A A A B 8 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i 1 E 2 Z E U G X R T d u h A r 2 I u 1 Q M m n a h u Y y J B m x D H 0 K N y 4 U c e v j u P N t T N t Z a O s P g Y / / n E P O + a O Y M 2 N 9 / 9 v L r a y u r W / k N w t b 2 z u 7 e 8 X 9 g 4 Z R i S a 0 T h R X u h V h Q z m T t G 6 Z 5 b Q V a 4 p F x G k z G l 1 P 6 8 1 H q g 1 T 8 t 6 O Y x o K P J C s z w i 2 z n r o 6 K H q 3 p a f T r v F k l / x Z 0 L L E G R Q g k y 1 b v G r 0 1 M k E V R a w r E x 7 c C P b Z h i b R n h d F L o J I b G m I z w g L Y d S i y o C d P Z w h N 0 4 p w e 6 i v t n r R o 5 v 6 e S L E w Z i w i 1 y m w H Z r F 2 t T 8 r 9 Z O b P 8 y T J m M E 0 s l m X / U T z i y C k 2 v R z 2 m K b F 8 7 A A T z d y u i A y x x s S 6 j A o u h G D x 5 G V o n F U C x 3 f n p e p V F k c e j u A Y y h D A B V T h B m p Q B w I C n u E V 3 j z t v X j v</formula><formula xml:id="formula_7">c O Z S 7 k r 7 1 R C Z 9 2 d F Q = " &gt; A A A B 8 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i 1 E 2 Z E U G X R T d u h A r 2 I u 1 Q M m n a h u Y y J B m x D H 0 K N y 4 U c e v j u P N t T N t Z a O s P g Y / / n E P O + a O Y M 2 N 9 / 9 v L r a y u r W / k N w t b 2 z u 7 e 8 X 9 g 4 Z R i S a 0 T h R X u h V h Q z m T t G 6 Z 5 b Q V a 4 p F x G k z G l 1 P 6 8 1 H q g 1 T 8 t 6 O Y x o K P J C s z w i 2 z n r o 6 K H q 3 p a f T r v F k l / x Z 0 L L E G R Q g k y 1 b v G r 0 1 M k E V R a w r E x 7 c C P b Z h i b R n h d F L o J I b G m I z w g L Y d S i y o C d P Z w h N 0 4 p w e 6 i v t n r R o 5 v 6 e S L E w Z i w i 1 y m w H Z r F 2 t T 8 r 9 Z O b P 8 y T J m M E 0 s l m X / U T z i y C k 2 v R z 2 m K b F 8 7 A A T z d y u i A y x x s S 6 j A o u h G D x 5 G V o n F U C x 3 f n p e p V F k c e j u A Y y h D A B V T h B m p Q B w I C n u E V 3 j z t v X j v 3 s e 8 N e d l M 4 f w R 9 7 n D x k 5 j + 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 3 T d D I + 2 y s c O Z S 7 k r 7 1 R C Z 9 2 d F Q = " &gt; A A A B 8 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i 1 E 2 Z E U G X R T d u h A r 2 I u 1 Q M m n a h u Y y J B m x D H 0 K N y 4 U c e v j u P N t T N t Z a O s P g Y / / n E P O + a O Y M 2 N 9 / 9 v L r a y u r W / k N w t b 2 z u 7 e 8 X 9 g 4 Z R i S a 0 T h R X u h V h Q z m T t G 6 Z 5 b Q V a 4 p F x G k z G l 1 P 6 8 1 H q g 1 T 8 t 6 O Y x o K P J C s z w i 2 z n r o 6 K H q 3 p a f T r v F k l / x Z 0 L L E G R Q g k y 1 b v G r 0 1 M k E V R a w r E x 7 c C P b Z h i b R n h d F L o J I b G m I z w g L Y d S i y o C d P Z w h N 0 4 p w e 6 i v t n r R o 5 v 6 e S L E w Z i w i 1 y m w H Z r F 2 t T 8 r 9 Z O b P 8 y T J m M E 0 s l m X / U T z i y C k 2 v R z 2 m K b F 8 7 A A T z d y u i A y x x s S 6 j A o u h G D x 5 G V o n F U C x 3 f n p e p V F k c e j u A Y y h D A B V T h B m p Q B w I C n u E V 3 j z t v X j v 3 s e 8 N e d l M 4 f w R 9 7 n D x k 5 j + 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 3 T d D I + 2 y s c O Z S 7 k r 7 1 R C Z 9 2 d F Q = " &gt; A A A B 8 H i c b Z D L S g M x F I b P 1 F u t t 6 p L N 8 E i 1 E 2 Z E U G X R T d u h A r 2 I u 1 Q M m n a h u Y y J B m x D H 0 K N y 4 U c e v j u P N t T N t Z a O s P g Y / / n E P O + a O Y M 2 N 9 / 9 v L r a y u r W / k N w t b 2 z u 7 e 8 X 9 g 4 Z R i S a 0 T h R X u h V h Q z m T t G 6 Z 5 b Q V a 4 p F x G k z G l 1 P 6 8 1 H q g 1 T 8 t 6 O Y x o K P J C s z w i 2 z n r o 6 K H q 3 p a f T r v F k l / x Z 0 L L E G R Q g k y 1 b v G r 0 1 M k E V R a w r E x 7 c C P b Z h i b R n h d F L o J I b G m I z w g L Y d S i y o C d P Z w h N 0 4 p w e 6 i v t n r R o 5 v 6 e S L E w Z i w i 1 y m w H Z r F 2 t T 8 r 9 Z O b P 8 y T J m M E 0 s l m X / U T z i y C k 2 v R z 2 m K b F 8 7 A A T z d y u i A y x x s S 6 j A o u h G D x 5 G V o n F U C x 3 f n p e p V F k c e j u A Y y h D A B V T h B m p Q B w I C n u E V 3 j z t v X j v</formula><formula xml:id="formula_8">S E f R Y 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Y Q n + B F w + K e P U n e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 i A R X B v X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V D F s s l j E q h N Q j Y J L b B p u B H Y S h T Q K B L a D 8 e 2 s 3 n 5 E p X k s 7 8 0 k Q T + i Q 8 l D z q i x V u O p X 6 6 4 V X c u s g p e D h X I V e + X v 3 q D m K U R S s M E 1 b r r u Y n x M 6 o M Z w K n p V 6 q M a F s T I f Y t S h p h N r P 5 o t O y Z l 1 B i S M l X 3 S k L n 7 e y K j k d a T K L C d E T U j v V y b m f / V u q k J r / 2 M y y Q 1 K N n i o z A V x M R k d j U Z c I X M i I k F y h S</formula><formula xml:id="formula_9">S E f R Y 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Y Q n + B F w + K e P U n e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 i A R X B v X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V D F s s l j E q h N Q j Y J L b B p u B H Y S h T Q K B L a D 8 e 2 s 3 n 5 E p X k s 7 8 0 k Q T + i Q 8 l D z q i x V u O p X 6 6 4 V X c u s g p e D h X I V e + X v 3 q D m K U R S s M E 1 b r r u Y n x M 6 o M Z w K n p V 6 q M a F s T I f Y t S h p h N r P 5 o t O y Z l 1 B i S M l X 3 S k L n 7 e y K j k d a T K L C d E T U j v V y b m f / V u q k J r / 2 M y y Q 1 K N n i o z A V x M R k d j U Z c I X M i I k F y h S</formula><formula xml:id="formula_10">S E f R Y 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Y Q n + B F w + K e P U n e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 i A R X B v X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V D F s s l j E q h N Q j Y J L b B p u B H Y S h T Q K B L a D 8 e 2 s 3 n 5 E p X k s 7 8 0 k Q T + i Q 8 l D z q i x V u O p X 6 6 4 V X c u s g p e D h X I V e + X v 3 q D m K U R S s M E 1 b r r u Y n x M 6 o M Z w K n p V 6 q M a F s T I f Y t S h p h N r P 5 o t O y Z l 1 B i S M l X 3 S k L n 7 e y K j k d a T K L C d E T U j v V y b m f / V u q k J r / 2 M y y Q 1 K N n i o z A V x M R k d j U Z c I X M i I k F y h S</formula><formula xml:id="formula_11">I A r Q p D G 4 G w 7 A x + 5 W r i 9 C Z W K D 4 B o = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Y Q n + B F w + K e P U n e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 i A R X B v X / X Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V D F s s l j E q h N Q j Y J L b B p u B H Y S h T Q K B L a D 8 e 2 s 3 n 5 E p X k s 7 8 0 k Q T + i Q 8 l D z q i x V u O p X 6 6 4 V X c u s g p e D h X I V e + X v 3 q D m K U R S s M E 1 b r r u Y n x M 6 o M Z w K n p V 6 q M a F s T I f Y t S h p h N r P 5 o t O y Z l 1 B i S M l X 3 S k L n 7 e y K j k d a T K L C d E T U j v V y b m f / V u q k J r / 2 M y y Q 1 K N n i o z A V x M R k d j U Z c I X M i I k F y h S 3 u x I 2 o o o y Y 7 M p 2 R C 8 5 Z N X o X V R 9 S w 3 L i u 1 m z y O I p z A K Z y D B 1 d Q g z u o Q x M Y I D z D K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The original batch normalization paper posits that internal covariate explains the benefits of BN <ref type="bibr" target="#b22">[23]</ref>. We do not claim that internal covariate shift does not exist, but we believe that the success of BN can be explained without it. We argue that a good reason to doubt that the primary benefit of BN is eliminating internal covariate shift comes from results in <ref type="bibr" target="#b33">[34]</ref>, where an initialization scheme that ensures that all layers are normalized is proposed. In this setting, internal covariate shift would not disappear. However, the authors show that such initialization can be used instead of BN with a relatively small performance loss. Another line of work of relevance is <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b46">[47]</ref>, where the relationship between various network parameters, accuracy and convergence speed is investigated, the former article argues for the importance of batch normalization to facilitate a phenomenon dubbed 'super convergence'. Due to space limitations, we defer discussion regarding variants of batch normalization, random matrix theory, generalization as well as further related work to Appendix A in the online version <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have investigated batch normalization and its benefits, showing how the latter are mainly mediated by larger learning rates. We argue that the larger learning rate increases the implicit regularization of SGD, which improves generalization. Our experiments show that large parameter updates to unnormalized networks can result in activations whose magnitudes grow dramatically with depth, which limits large learning rates. Additionally, we have demonstrated that unnormalized networks have large and ill-behaved outputs, and that this results in gradients that are input independent. Via recent results in random matrix theory, we have argued that the ill-conditioned activations are natural consequences of the random initialization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The training (left) and testing (right) accuracies as a function of progress through the training cycle.We used a 110-layer Resnet with three distinct learning rates 0.0001, 0.003, 0.1. The smallest, 0.0001 was picked such that the network without BN converges. The figure shows that with matching learning rates, both networks, with BN and without, result in comparable testing accuracies (red and green lines in right plot). In contrast, larger learning rates yield higher test accuracy for BN networks, and diverge for unnormalized networks (not shown). All results are averaged over five runs with std shown as shaded region around mean.</figDesc><graphic coords="3,108.00,72.31,186.14,127.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histograms over the gradients at initialization for (midpoint) layer 55 of a network with BN (left) and without (right). For the unnormalized network, the gradients are distributed with heavy tails, whereas for the normalized networks the gradients are concentrated around the mean. (Note that we have to use different scales for the two plots because the gradients for the unnormalized network are almost two orders of magnitude larger than for the normalized on.)</figDesc><graphic coords="4,108.00,161.59,191.28,124.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Heatmap of channel means and variances during a diverging gradient update (without BN). The vertical axis denote what percentage of the gradient update has been applied, 100% corresponds to the endpoint of the update. The moments explode in the higher layer (note the scale of the color bars).</figDesc><graphic coords="5,222.57,179.58,164.10,123.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average channel means and variances as a function of network depth at initialization (error bars show standard deviations) on log-scale for networks with and without BN. The batch normalized network the mean and variances stays relatively constant throughout the network. For an unnormalized network, they seem to grow almost exponentially with depth.</figDesc><graphic coords="6,187.20,72.00,239.61,150.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: A heat map of the output gradients in the final classification layer after initialization. The columns correspond to a classes and the rows to images in the mini-batch. For an unnormalized network (left), it is evident that the network consistently predicts one specific class (very right column), irrespective of the input. As a result, the gradients are highly correlated. For a batch normalized network, the dependence upon the input is much larger.</figDesc><graphic coords="6,147.60,295.17,146.91,103.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Average absolute gradients for parameters between in and out channels for layer 45 at initialization. For an unnormalized network, we observe a dominant low-rank structure. Some in/out-channels have consistently large gradients while others have consistently small gradients. This structure is less pronounced with batch normalization (right).</figDesc><graphic coords="8,139.86,68.93,173.23,129.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 s e 8 N</head><label>8</label><figDesc>e d l M 4 f w R 9 7 n D x k 5 j + 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 3 T d D I + 2 y s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 s e 8 N</head><label>8</label><figDesc>e d l M 4 f w R 9 7 n D x k 5 j + 4 = &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I A r Q p D G 4 G w 7 A x + 5 W r i 9 C Z W K D 4 B o = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>3 u x I 2 o o o y Y 7 M p 2 R C 8 5 Z N X o X V R 9 S w 3 L i u 1 m z y O I p z A K Z y D B 1 d Q g z u o Q x M Y I D z D K 7 w 5 D 8 6 L 8 + 5 8 L F o L T j 5 z D H / k f P 4 A 5 j u M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I A r Q p D G 4 G w 7 A x + 5 W r i 9 C Z W K D 4 B o = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>3 u x I 2 o o o y Y 7 M p 2 R C 8 5 Z N X o X V R 9 S w 3 L i u 1 m z y O I p z A K Z y D B 1 d Q g z u o Q x M Y I D z D K 7 w 5 D 8 6 L 8 + 5 8 L F o L T j 5 z D H / k f P 4 A 5 j u M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I A r Q p D G 4 G w 7 A x + 5 W r i 9 C Z W K D 4 B o = " &gt; A A A B 6 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>3 u x I 2 o o o y Y 7 M p 2 R C 8 5 Z N X o X V R 9 S w 3 L i u 1 m z y O I p z A K Z y D B 1 d Q g z u o Q x M Y I D z D K 7 w 5 D 8 6 L 8 + 5 8 L F o L T j 5 z D H / k f P 4 A 5 j u M / A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>7 w 5 DFigure 8 :</head><label>58</label><figDesc>Figure 8: Distribution of singular values according to theorem 1 for some M . The theoretical distribution becomes increasingly heavy-tailed for more matrices, as does the empirical distributions of Figure 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8</head><label>8</label><figDesc>Figure 8  illustrates some density plots for various values of M and θ. A closer look at<ref type="bibr" target="#b4">(5)</ref> reveals that the distribution blows up as x -M/(M +1) nears the origin, and that the largest singular value scales as O(M ) for large matrices. In Figure9we investigate the singular value distribution for practically sized matrices. By multiplying more matrices, which represents a deeper linear network, the singular values distribution becomes significantly more heavy-tailed. Intuitively this means that the ratio between the largest and smallest singular value (the condition number) will increase with depth, which we verify in Figure20in Appendix K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An illustration of the distributions of singular values of random square matrices and product of independent matrices. The matrices have dimension N=1000 and all entries independently drawn from a standard Gaussian distribution. Experiments are repeated ten times and we show the total number of singular values among all runs in every bin, distributions for individual experiments look similar. The left plot shows all three settings. We see that the distribution of singular values becomes more heavy-tailed as more matrices are multiplied together.</figDesc><graphic coords="9,367.54,66.04,144.23,108.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Gradients of a convolutional kernel as described in (4) at initialization. The table compares the absolute value of the sum of gradients, and the sum of absolute values. Without BN these two terms are similar in magnitude, suggesting that the summands have matching signs throughout and are largely data independent. For a batch normalized network, those two differ by about two orders of magnitude.</figDesc><table><row><cell>Layer 18, with BN</cell><cell>7.5e-05</cell><cell>3.0e-07</cell><cell>251.8</cell></row><row><cell>Layer 54, with BN</cell><cell>1.9e-05</cell><cell>1.7e-07</cell><cell>112.8</cell></row><row><cell>Layer 90, with BN</cell><cell>6.6e-06</cell><cell>1.6e-07</cell><cell>40.7</cell></row><row><cell>Layer 18, w/o BN</cell><cell>6.3e-05</cell><cell>3.6e-05</cell><cell>1.7</cell></row><row><cell>Layer 54, w/o BN</cell><cell>2.2e-04</cell><cell>8.4e-05</cell><cell>2.6</cell></row><row><cell>Layer 90, w/o BN</cell><cell>2.6e-04</cell><cell>1.2e-04</cell><cell>2.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in Appendix K. Consider min Ai, i=1,2 ... t A t ... A 2 A 1 x-y 2 ,this problem is similar to solving a linear system min</figDesc><table /><note><p>x Axy 2 if one only optimizes over a single weight matrix A i . It is well known that the complexity of solving min x Axy via gradient descent can be characterized by the condition number κ of A, the ratio between largest σ max and smallest singular value σ min . Increasing</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Yexiang Xue, Guillaume Perez, Rich Bernstein, Zdzislaw Burda, Liam McAllister, Yang Yuan, Vilja Järvi, Marlene Berke and Damek Davis for help and inspiration. This research is supported by NSF Expedition CCF-1522054 and Awards FA9550-18-1-0136 and FA9550-17-1-0292 from AFOSR. KQW was supported in part by the III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822 grants from the National Science Foundation, and generous support from the Bill and Melinda Gates Foundation, the Office of Naval Research, and SAP America Inc.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k i i 0 + C l N J M C a z 2 8 l A a M 5 Q T i x Q p o X d l b A R 1 Z S h T a h k Q / C W T 1 6 F V q 3 q W b 6 / r N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k i i 0 + C l N J M C a z 2 8 l A a M 5 Q T i x Q p o X d l b A R 1 Z S h T a h k Q / C W T 1 6 F V q 3 q W b 6 / r N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k i i 0 + C l N J M C a z 2 8 l A a M 5 Q T i x Q p o X d l b A R 1 Z S h T a h k Q / C W T 1 6 F V q 3 q W b 6 / r N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k i i 0 + C l N J M C a z 2 8 l A a M 5 Q T i x Q p o X d l b A R 1 Z S h T a h k Q / C W T 1 6 F V q 3 q W b 6 / r N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt;</p><p>10 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X Q V Z F w m q W W S L j 6 K U o G t w r P T 8 Y B r R q 2 Y O C B U c 7 c r p i O i C b U u o J I L w V 8 + e R V a t a r v + O 6 i U r / O 4 y j C C Z z C O f h w C X W 4 h Q Y 0 g c I j P M M r v C G F X t A 7 + l i 0 F l A + c w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A</p><p>w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3</p><p>w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3</p><p>y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>10 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T</p><p>w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T</p><p>w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T</p><p>w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T</p><p>w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3</p><p>w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3</p><p>y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>&lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a c C U N 7 y P L G q t r l f K</p><p>10 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt;</p><p>10 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X Q V Z F w m q W W S L j 6 K U o G t w r P T 8 Y B r R q 2 Y O C B U c 7 c r p i O i C b U u o J I L w V 8 + e R V a t a r v + O 6 i U r / O 4 y j C C Z z C O f h w C X W 4 h Q Y 0 g c I j P M M r v C G F X t A 7 + l i 0 F l A + c w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X Q V Z F w m q W W S L j 6 K U o G t w r P T 8 Y B r R q 2 Y O C B U c 7 c r p i O i C b U u o J I L w V 8 + e R V a t a r v + O 6 i U r / O 4 y j C C Z z C O f h w C X W 4 h Q Y 0 g c I j P M M r v C G F X t A 7 + l i 0 F l A + c w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X Q V Z F w m q W W S L j 6 K U o G t w r P T 8 Y B r R q 2 Y O C B U c 7 c r p i O i C b U u o J I L w V 8 + e R V a t a r v + O 6 i U r / O 4 y j C C Z z C O f h w C X W 4 h Q Y 0 g c I j P M M r v C G F X t A 7 + l i 0 F l A + c w x / h D 5 / A N I r j p 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A / I t / q c 3 3 V O 3 m a O 2 U Z A 8 a w I 9 M g o = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J W Z I u i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X Q V Z F w m q W W S L j 6 K U o G t w r P T 8 Y B r R q 2 Y O C B U c 7 c r p i O i C b U u o J I L w V 8 h V N G 2 l + 8 / n g 3 / U U q T n R s n H g M v 1 c = " &gt; A A A B 7 X i c b Z D L S g M x F I Z P 6 q 3 W W 9 W l m 2 A R X J U Z K e i y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X</p><p>y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X</p><p>y 6 M Z l B X u B d i y Z N N P G Z p I h y Q h l 6 D u 4 c a G I W 9 / H n W 9 j 2 s 5 C W 3 8 I f P z n H H L O H y a C G + t 5 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W k a l m r I m V U L p T k g M E 1 y y p u V W s E 6 i G Y l D w d r h + G Z W b z 8 x b b i S 9 3 a S s C A m Q 8 k j T o l 1 V s v 3 H r L a t F + u e F V v L r w K f g 4 V y N X o l 7 9 6 A 0 X T m E l L B T G m 6 3 u J D T K i L a e C T U u 9 1 L C E 0 D E Z s q 5 D S W J m g m y + 7 R S f O W e A I 6 X d k x b P 3 d 8 T G Y m N m c S h 6 4 y J H Z n l 2 s z 8 r 9 Z N b X</p><p>10 4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g D x b t 4 I 0 h h Q 6 G j 4 Y u y W o d N S d V / Q = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u a t N + u e J W 3 b n I K n g 5 V C B X o 1 / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X o q Q R a j + b r z s l Z 9 Y Z k D B W 9 k l D 5 u 7 v i Y x G W k + i w H Z G 1 I z 0 c m 1 m / l f r p i a 8 9 j M u k 9 S g Z I u P w l Q Q E 5 P Z 7 W T A F T I j J h Y o U 9 z u S t i I K s q M T a h k Q / C W T 1 6 F 1 m X V s 3 x f q 9 R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c / + o 7 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g D x b t 4 I 0 h h Q 6 G j 4 Y u y W o d N S d V / Q = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u a t N + u e J W 3 b n I K n g 5 V C B X o 1 / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X o q Q R a j + b r z s l Z 9 Y Z k D B W 9 k l D 5 u 7 v i Y x G W k + i w H Z G 1 I z 0 c m 1 m / l f r p i a 8 9 j M u k 9 S g Z I u P w l Q Q E 5 P Z 7 W T A F T I j J h Y o U 9 z u S t i I K s q M T a h k Q / C W T 1 6 F 1 m X V s 3 x f q 9 R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c / + o 7 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g D x b t 4 I 0 h h Q 6 G j 4 Y u y W o d N S d V / Q = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u a t N + u e J W 3 b n I K n g 5 V C B X o 1 / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X o q Q R a j + b r z s l Z 9 Y Z k D B W 9 k l D 5 u 7 v i Y x G W k + i w H Z G 1 I z 0 c m 1 m / l f r p i a 8 9 j M u k 9 S g Z I u P w l Q Q E 5 P Z 7 W T A F T I j J h Y o U 9 z u S t i I K s q M T a h k Q / C W T 1 6 F 1 m X V s 3 x f q 9 R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c / + o 7 Y &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g D x b t 4 I 0 h h Q 6 G j 4 Y u y W o d N S d V / Q = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u a t N + u e J W 3 b n I K n g 5 V C B X o 1 / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X o q Q R a j + b r z s l Z 9 Y Z k D B W 9 k l D 5 u 7 v i Y x G W k + i w H Z G 1 I z 0 c m 1 m / l f r p i a 8 9 j M u k 9 S g Z I u P w l Q Q E 5 P Z 7 W T A F T I j J h Y o U 9 z u S t i I K s q M T a h k Q / C W T 1 6 F 1 m X V s 3 x f q 9 R v 8 j i K U K b w S 1 I e 9 9 A P c h L 9 3 w Q r L + u t z 3 o = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u v G m / X H G r 7 l x k F b w c K p C r 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r U d I I t Z / N 1 5 2 S M + s M S B g r + 6 Q h c / f 3 R E Y j r S d R Y D s j a k Z 6 u T Y z / 6 t 1 U x N e + x m X S W p Q s s V H Y S q I i c n s d j L g C p k R E w u U K W 5 3 J W x E F W X G J l S y I X j L J 6 9 C 6 7 L q W b 6 v V e o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w z O 8 w p u T O C / O u / O x a C 0 4 + c w x / J H z + Q M 7 a 4 7 V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U K b w S 1 I e 9 9 A P c h L 9 3 w Q r L + u t z 3 o = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u v G m / X H G r 7 l x k F b w c K p C r 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r U d I I t Z / N 1 5 2 S M + s M S B g r + 6 Q h c / f 3 R E Y j r S d R Y D s j a k Z 6 u T Y z / 6 t 1 U x N e + x m X S W p Q s s V H Y S q I i c n s d j L g C p k R E w u U K W 5 3 J W x E F W X G J l S y I X j L J 6 9 C 6 7 L q W b 6 v V e o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w z O 8 w p u T O C / O u / O x a C 0 4 + c w x / J H z + Q M 7 a 4 7 V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U K b w S 1 I e 9 9 A P c h L 9 3 w Q r L + u t z 3 o = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u v G m / X H G r 7 l x k F b w c K p C r 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r U d I I t Z / N 1 5 2 S M + s M S B g r + 6 Q h c / f 3 R E Y j r S d R Y D s j a k Z 6 u T Y z / 6 t 1 U x N e + x m X S W p Q s s V H Y S q I i c n s d j L g C p k R E w u U K W 5 3 J W x E F W X G J l S y I X j L J 6 9 C 6 7 L q W b 6 v V e o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w z O 8 w p u T O C / O u / O x a C 0 4 + c w x / J H z + Q M 7 a 4 7 V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U K b w S 1 I e 9 9 A P c h L 9 3 w Q r L + u t z 3 o = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J p K D H o h e P F e w H t L F s t p N 2 6 W Y T d j d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 5 a O U 8 W w y W I R q 0 5 A N Q o u s W m 4 E d h J F N I o E N g O x r e z e v s J l e a x f D C T B P 2 I D i U P O a P G W m 3 P f c w u v G m / X H G r 7 l x k F b w c K p C r 0 S 9 / 9 Q Y x S y O U h g m q d d d z E + N n V B n O B E 5 L v V R j Q t m Y D r F r U d I I t Z / N 1 5 2 S M + s M S B g r + 6 Q h c / f 3 R E Y j r S d R Y D s j a k Z 6 u T Y z / 6 t 1 U x N e + x m X S W p Q s s V H Y S q I i c n s d j L g C p k R E w u U K W 5 3 J W x E F W X G J l S y I X j L J 6 9 C 6 7 L q W b 6 v V e o 3 e R x F O I F T O A c P r q A O d 9 C A J j A Y w z O 8 w p u T O C / O u / O x a C 0 4 + c w x / J H z + Q M 7 a 4 7 V &lt; / l a t e x i t &gt; 10 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k</p><p>A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d C C f 0 R X j w o 4 t X f 4 8 1 / 4 7 b N Q V t f W H h 4 Z 4 a d e Y N E C o O u + + 0 U 1 t Y 3 N r e K 2 6 W d 3 b 3 9 g / L h U c v E q W a 8 y W I Z 6 0 5 A D Z d C 8 S Y K l L y T a E 6 j Q P J 2 M L 6 d 1 d t P X B s R q w e c J N y P 6 F C J U D C K 1 m p 7 7 m N 2 U Z v 2 y x W 3 6 s 5 F V s H L o Q K 5 G v 3 y V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x a 6 q W G J 5 S N 6 Z B 3 L S o a c e N n 8 3 W n 5 M w 6 A x L G 2 j 6 F Z O 7 + n s h o Z M w k C m x n R H F k l m s z 8 7 9 a N 8 X w 2 s + E S l L k i i 0 + C l N J M C a z 2 8 l A a M 5 Q T i x Q p o X d l b A R 1 Z S h T a h k Q / C W T 1 6 F V q 3 q W b 6 / r N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; 10 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m E P s d Y E E e w U Z B a 9 x C d o u Y d v C G 4 = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J V N B j 0 Y v H C v Y D 2 l g 2 2 0 2 7 d L M J u x O h h P 4 I L x 4 U 8 e r v 8 e a / c d v m o K 0 v L D y 8 M 8 P O v E E i h U H X / X Y K K 6 t r 6 x v F z d L W 9 s 7 u X n n / o G n i V D P e Y L G M d T u g h k u h e A M F S t 5 O N K d R I H k r G N 1 O 6 6 0 n r o 2 I 1 Q O O E + 5 H d K B E K B h F a 7 U 8 9 z E 7 u 5 j 0 y h W 3 6 s 5 E l s H L o Q K 5 6 r 3 y V 7 c f s z T i C p m k x n Q 8 N 0 E / o x o F k 3 x S 6 q a G J 5 S N 6 I B 3 L C o a c e N n s 3 U n 5 M Q 6 f R L G 2 j 6 F Z O b + n s h o Z M w 4 C m x n R H F o F m t T 8 7 9 a J 8 X w 2 s + E S l L k i s 0 / C l N J M C b T 2 0 l f a M 5 Q j i 1 Q p o X d l b A h 1 Z S h T a h k Q / A W T 1 6 G 5 n n V s 3 x / W a n d 5 H E U 4 Q i O 4 R Q 8 u I I a 3 E E d G s B g B M / w C m 9 O 4 r w 4 7 8 7 H v L X g 5 D O H 8 E f O 5 w 8 + d Y 7 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m E P s d Y E E e w U Z B a 9 x C d o u Y d v C G 4 = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J V N B j 0 Y v H C v Y D 2 l g 2 2 0 2 7 d L M J u x O h h P 4 I L x 4 U 8 e r v 8 e a / c d v m o K 0 v L D y 8 M 8 P O v E E i h U H X / X Y K K 6 t r 6 x v F z d L W 9 s 7 u X n n / o G n i V D P e Y L G M d T u g h k u h e A M F S t 5 O N K d R I H k r G N 1 O 6 6 0 n r o 2 I 1 Q O O E + 5 H d K B E K B h F a 7 U 8 9 z E 7 u 5 j 0 y h W 3 6 s 5 E l s H L o Q K 5 6 r 3 y V 7 c f s z T i C p m k x n Q 8 N 0 E / o x o F k 3 x S 6 q a G J 5 S N 6 I B 3 L C o a c e N n s 3 U n 5 M Q 6 f R L G 2 j 6 F Z O b + n s h o Z M w 4 C m x n R H F o F m t T 8 7 9 a J 8 X w 2 s + E S l L k i s 0 / C l N J M C b T 2 0 l f a M 5 Q j i 1 Q p o X d l b A h 1 Z S h T a h k Q / A W T 1 6 G 5 n n V s 3 x / W a n d 5 H E U 4 Q i O 4 R Q 8 u I I a 3 E E d G s B g B M / w C m 9 O 4 r w 4 7 8 7 H v L X g 5 D O H 8 E f O 5 w 8 + d Y 7 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m E P s d Y E E e w U Z B a 9 x C d o u Y d v C G 4 = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J V N B j 0 Y v H C v Y D 2 l g 2 2 0 2 7 d L M J u x O h h P 4 I L x 4 U 8 e r v 8 e a / c d v m o K 0 v L D y 8 M 8 P O v E E i h U H X / X Y K K 6 t r 6 x v F z d L W 9 s 7 u X n n / o G n i V D P e Y L G M d T u g h k u h e A M F S t 5 O N K d R I H k r G N 1 O 6 6 0 n r o 2 I 1 Q O O E + 5 H d K B E K B h F a 7 U 8 9 z E 7 u 5 j 0 y h W 3 6 s 5 E l s H L o Q K 5 6 r 3 y V 7 c f s z T i C p m k x n Q 8 N 0 E / o x o F k 3 x S 6 q a G J 5 S N 6 I B 3 L C o a c e N n s 3 U n 5 M Q 6 f R L G 2 j 6 F Z O b + n s h o Z M w 4 C m x n R H F o F m t T 8 7 9 a J 8 X w 2 s + E S l L k i s 0 / C l N J M C b T 2 0 l f a M 5 Q j i 1 Q p o X d l b A h 1 Z S h T a h k Q / A W T 1 6 G 5 n n V s 3 x / W a n d 5 H E U 4 Q i O 4 R Q 8 u I I a 3 E E d G s B g B M / w C m 9 O 4 r w 4 7 8 7 H v L X g 5 D O H 8 E f O 5 w 8 + d Y 7 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l m E P s d Y E E e w U Z B a 9 x C d o u Y d v C G 4 = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J V N B j 0 Y v H C v Y D 2 l g 2 2 0 2 7 d L M J u x O h h P 4 I L x 4 U 8 e r v 8 e a / c d v m o K 0 v L D y 8 M 8 P O v E E i h U H X / X Y K K 6 t r 6 x v F z d L W 9 s 7 u X n n / o G n i V D P e Y L G M d T u g h k u h e A M F S t 5 O N K d R I H k r G N 1 O 6 6 0 n r o 2 I 1 Q O O E + 5 H d K B E K B h F a 7 U 8 9 z E 7 u 5 j 0 y h W 3 6 s 5 E l s H L o Q K 5 6 r 3 y V 7 c f s z T i C p m k x n Q 8 N 0 E / o x o F k 3 x S 6 q a G J 5 S N 6 I B 3 L C o a c e N n s 3 U n 5 M Q 6 f R L G 2 j 6 F Z O b + n s h o Z M w 4 C m x n R H F o F m t T 8 7 9 a J 8 X w 2 s + E S l L k i s 0 / C l N J M C b T 2 0 l f a M 5 Q j i 1 Q p o X d l b A h 1 Z S h T a h k Q / A W T 1 6 G 5 n n V s 3 x / W a n d 5 H E U 4 Q i O 4 R Q 8 u I I a 3 E E d G s B g B M / w C m 9 O 4 r w 4 7 8 7 H v L X g 5 D O H 8 E f O 5 w 8 + d Y 7 X &lt; / l a t e x i t &gt;</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e X E E d 7 q A B T W A w h m d 4 h T c n c V 6 c d + d j 0 V p w 8 p l j + C P n 8 w c 8 8 I 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T h 8 n D r 9 6 + m O b T E C M D 5 I L V X J A + h Y = " &gt; A A A B 7 n i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 G J J i q D H o h e P F e w H t L F s t p t 2 6 W Y T d i d</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Scientific</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex optimization algorithms</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02375</idno>
		<title level="m">Understanding batch normalization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01838</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11029</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4470" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<title level="m">Recurrent batch normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Advanced machine learning systems : lecture notes</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sa</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eigenvalues and condition numbers of random matrices</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="560" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04231</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Untersuchungen zu dynamischen neuronalen netzen</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diploma, Technische Universität München</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1729" to="1739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batchnormalized models</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzkebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04623</idno>
		<title level="m">Three factors influencing minima in sgd</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philémon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2657" to="2661" />
		</imprint>
	</monogr>
	<note>2016 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
	<note>Efficient backprop</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bulk and soft-edge universality for singular values of products of ginibre random matrices</title>
		<author>
			<persName><forename type="first">Dang-Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l&apos;Institut Henri Poincaré, Probabilités et Statistiques</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1734" to="1762" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Institut Henri Poincaré</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Cosme</forename><surname>Louart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Couillet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05419</idno>
		<title level="m">A random matrix approach to neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Depth creates no bad local minima</title>
		<author>
			<persName><forename type="first">Haihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08580</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting small batch training for deep neural networks</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07612</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Solving internal covariate shift in deep learning with linked neurons</title>
		<idno type="arXiv">arXiv:1712.02609</idno>
		<editor>Carles Roger Riera Molina and Oriol Pujol Vila</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5949" to="5958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geometry of neural network loss surfaces via random matrix theory</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2798" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nonlinear random matrix theory for deep learning</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Worah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Incorporating condition measures into the complexity theory of linear programming</title>
		<author>
			<persName><forename type="first">James</forename><surname>Renegar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="524" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ill-conditioning in neural network training problems</title>
		<author>
			<persName><forename type="first">Sirpa</forename><surname>Saarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall</forename><surname>Bramley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="693" to="714" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The effect of batch normalization on deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Schilling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01232</idno>
		<title level="m">Deep information propagation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of residual networks using large learning rates</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07120</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Don&apos;t decay the learning rate, increase the batch size</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A bayesian perspective on generalization and stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Solving the ill-conditioning in neural network learning</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Der</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">L1-norm batch normalization for efficient training of deep neural networks</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09769</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08494</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Group normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05393</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Global optimality conditions for deep neural networks</title>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02444</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Condition number theorems in optimization</title>
		<author>
			<persName><forename type="first">Tullio</forename><surname>Zolezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="507" to="516" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
