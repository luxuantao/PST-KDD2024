<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Principal Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Sch Olkopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max-Planck-Institut f. biol. Kybernetik</orgName>
								<address>
									<addrLine>Spemannstr. 38, 72076 T ubingen</addrLine>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GMD FIRST</orgName>
								<address>
									<addrLine>Rudower Chaussee 5</addrLine>
									<postCode>12489</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><surname>Robert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GMD FIRST</orgName>
								<address>
									<addrLine>Rudower Chaussee 5</addrLine>
									<postCode>12489</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel Principal Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Principal Component Analysis PCA is a basis transformation to diagonalize an estimate of the covariance matrix of the data x k , k = 1; : : : ; , x k 2 R N , P k=1 x k = 0, de ned as C = 1 `X j=1</p><p>x j x j :</p><p>1</p><p>The new coordinates in the Eigenvector basis, i.e. the orthogonal projections onto the Eigenvectors, are called principal components.</p><p>In this paper, we generalize this setting to a nonlinear one of the following kind. Suppose we rst map the data nonlinearly into a feature space F by : R N ! F; x 7 ! X: 2</p><p>We will show that even if F has arbitrarily large dimensionality, for certain choices of , w e can still perform PCA in F. This is done by the use of kernel functions known from Support Vector Machines Boser, Guyo n , &amp; V apnik, 1992.</p><p>2 Kernel PCA Assume for the moment that our data mapped into feature space, x 1 ; : : : ; x `, is centered, i.e. CV: Substituting 3, we note that all solutions V lie in the span of x 1 ; : : : ; x `. This implies that we m a y consider the equivalent system</p><formula xml:id="formula_0">x k V = x k</formula><p>CV for all k = 1 ; : : : ; ;</p><p>and that there exist coe cients 1 ; : : : ; `such that V = X i=1 i x i : 5 Substituting 3 and 5 into 4, and de ning an ` `matrix K by K ij := x i x j ; 6 we arrive a t `K = K 2 ; 7 where denotes the column vector with entries 1 ; : : : ; `. T o nd solutions of 7, we solve the Eigenvalue problem ` = K 8 for nonzero Eigenvalues. Clearly, all solutions of 8 do satisy 7. Moreover, it can be shown that any additional solutions of 8 do not make a di erence in the expansion 5 and thus are not interesting for us.</p><p>We normalize the solutions k belonging to nonzero Eigenvalues by requiring that the corresponding vectors in F be normalized, i.e. V k V k = 1 : By virtue of 5, 6 and 8, this translates into</p><formula xml:id="formula_1">1 = X i;j=1 k i k j x i x j = k K k = k k k : 9</formula><p>For principal component extraction, we compute projections of the image of a test point x o n to the Eigenvectors V k in F according to</p><formula xml:id="formula_2">V k x = X i=1 k i x i x:</formula><p>10 Note that neither 6 nor 10 requires the x i in explicit form | they are only needed in dot products. Therefore, we are able to use kernel functions for computing these dot products without actually performing the map <ref type="bibr">Aizerman, Braverman, &amp; Rozonoer, 1964;</ref><ref type="bibr">Boser, Guyo n , &amp; V apnik, 1992:</ref> for some choices of a kernel kx; y, it can be shown by methods of functional analysis that there exists a map into some dot product space F possibly of in nite dimension such that k computes the dot product in F. Kernels which h a ve successfully been used in Support Vector Machines Sch olkopf, Burges, &amp; Vapnik, 1995 include polynomial kernels kx; y = x y d ; 11 radial basis functions kx; y = exp , ,kx , yk 2 =2 2 , and sigmoid kernels kx; y = tanhx y + . It can be shown that polynomial kernels of degree d correspond to a map into a feature space which is spanned by all products of d entries of an input pattern, e.g., for the case of N = 2 ; d = 2 ,</p><p>x y 2 = x 2 1 ; x 1 x 2 ; x 2 x 1 ; x 2 2 y 2 1 ; y 1 y 2 ; y 2 y 1 ; y 2 2 :</p><formula xml:id="formula_3">linear PCA R 2 F Φ kernel PCA k(x,y) = (x . y) k(x,y) = (x . y) d x x x x x x x x x x x x x x x x x x x x x x x x R 2 x x x x x x x x x x x x</formula><p>Fig. <ref type="figure">1</ref>. Basic idea of kernel PCA: by using a nonlinear kernel function k instead of the standard dot product, we implicitly perform PCA in a possibly high dimensional space F which is nonlinearly related to input space. The dotted lines are contour lines of constant feature value.</p><p>If the patterns are images, we can thus work in the space of all products of d pixels and thereby take i n to account higher order statistics when doing PCA.</p><p>Substituting kernel functions for all occurences of xy, we obtain the following algorithm for kernel PCA Fig. <ref type="figure">1</ref>: we compute the dot product matrix cf. Eq. 6 K ij = kx i ; x j ij , solve 8 by diagonalizing K, normalize the Eigenvector expansion coe cients n by requiring Eq. 9, and extract principal components corresponding to the kernel k of a test point x by computing projections onto Eigenvectors Eq. 10, Fig. <ref type="figure">2</ref>.</p><p>We should point out that in practice, our algorithm is not equivalent t o t h e form of nonlinear PCA obtainable by explicitly mapping into the feature space F: e v en though the rank of the dot product matrix will be limited by the sample size, we m a y not even be able to compute this matrix, if the dimensionality i s prohibitively high. For instance, 16 16 pixel input images and a polynomial degree d = 5 yield a dimensionality o f 1 0 10 . Kernel PCA deals with this problem by automatically choosing a subspace of F with a dimensionality given by the rank of K, and by providing a means of computing dot products between vectors in this subspace. This way, w e h a ve t o e v aluate `kernel functions in input space rather than a dot product in a 10 10 dimensional space.</p><p>To conclude this section, we brie y mention the case where we drop the assumption that the x i are centered in F. Note that we cannot in general center the data, as we cannot compute the mean of a set of points that we d o not have in explicit form. Instead, we have to go through the above algebra using x i : = x i , 1=` P ì=1 x i : It turns out that the matrix that we have t o diagonalize in that case, call it K, can be expressed in terms of K as Kij = K , 1 `K , K1 `+ 1 `K 1 `; using the shorthand 1 `ij := 1=`for details, see Sch olkopf, Smola, &amp; M uller, 1996<ref type="foot" target="#foot_0">3</ref> .</p><formula xml:id="formula_4">Σ (Φ(x) . V) = Σ α i k (x,x i ) input vector x sample x 1 , x 2 , x 3 ,... comparison: k(x,x i ) feature value weights (Eigenvector coefficients) α 1 α 2 α 3 α 4 k k k k</formula><p>3 Experiments on Feature Extraction Using polynomial kernels 11 of degrees d = 1 ; : : : ; 6, and extracting the rst 2 n n = 6 ; 7; : : : ; 11 principal components, we found the following. In the case of linear PCA d = 1, the best classi cation performance 8.6 error is attained for 128 components. Extracting the same number of nonlinear components d = 2 ; : : : ; 6 in all cases lead to superior performance around 6 error.</p><p>Moreover, in the nonlinear case, the performance can be further improved by using a larger number of components note that there exist more higher order features than there are pixels in an image. Using d 2 and 2048 components, we obtained around 4 error, which coincides with the best result reported for standard nonlinear Support Vector Machines Sch olkopf, <ref type="bibr" target="#b5">Burges, &amp; Vapnik, 1995.</ref> This result is competitive with convolutional 5 layer neural networks 5.0 were reported by <ref type="bibr">LeCun et al., 1989</ref>; it is much better than linear classi ers operating directly on the image data a linear Support Vector Machine achieves 8.9; Sch olkopf, Burges, &amp; Vapnik, 1995. These ndings have been con rmed on an object recognition task, the MPI chair data base for details on all experiments, see Sch olkopf, Smola, &amp; M uller, 1996. We should add that our results were obtained without using any prior knowledge about symmetries of the problem at hand. This explains why the performance is inferior to Virtual Support Vector classi ers 3.2, Sch olkopf, Burges, <ref type="bibr">&amp;</ref><ref type="bibr" target="#b6">Vapnik, 1996, and</ref><ref type="bibr">Tangent Distance Nearest Neighbour classi ers 2.6, Simard, LeCun, &amp;</ref><ref type="bibr" target="#b8">Denker, 1993.</ref> We b elieve that adding e.g. local translation invariance, be it by generating virtual" translated examples or by c hoosing a suitable kernel, could further improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>This paper was devoted to the exposition of a new technique for nonlinear principal component analysis. To develop this technique, we made use of a kernel method which so far only had been used in supervised learning <ref type="bibr">Vapnik, 1995.</ref> Clearly, the kernel method can be applied to any algorithm which can be formulated in terms of dot products exclusively, including for instance k-means and independent component analysis cf. <ref type="bibr" target="#b7">Sch olkopf, Smola, &amp; M uller, 1996.</ref> In experiments comparing the utility of kernel PCA features for pattern recognition using a linear classi er, we found two advantages of nonlinear kernel PCA: rst, nonlinear principal components a orded better recognition rates than corresponding numbers of linear principal components; and second, the performance for nonlinear components can be further improved by using more components than possible in the linear case.</p><p>The computational complexity o f k ernel PCA does not grow with the dimensionality of the feature space that we are implicitely working in. This makes it possible to work for instance in the space of all possible d-th order products between pixels of an image. As in the variant of standard PCA which diagonalizes the dot product matrix e.g. <ref type="bibr" target="#b3">Kirby &amp; Sirovich, 1990</ref>, we have to diagonalize an ` `matrix `being the number of examples, or the size of a representative subset, with a comparable computational complexity | we only need to compute kernel functions rather than dot products. If the dimensionality of input space is smaller than the number of examples, kernel principal component extraction is computationally more expensive than linear PCA; however, this investment can pay back afterwards: we h a ve presented results indicating that in pattern recognition, it is su cient to use a linear classi er, as long as the features extracted are nonlinear. The main advantage of linear PCA up to date, however, consists in the possibility to reconstruct the patterns from their principal components.</p><p>Compared to other methods for nonlinear PCA, as autoassociative MLPs with a bottleneck hidden layer e.g. Diamantaras &amp; Kung, 1996 or principal curves Hastie &amp; Stuetzle, 1989, kernel PCA has the advantage that no nonlinear optimization is involved | we only need to solve an Eigenvalue problem as in the case of standard PCA. Therefore, we are not in danger of getting trapped in local minima during during training. Compared to most neural network type generalizations of PCA e.g. <ref type="bibr" target="#b4">Oja, 1982</ref>, kernel PCA moreover has the advantage that it provides a better understanding of what kind of nonlinear features are extracted: they are principal components in a feature space which i s xed a priori by c hoosing a kernel function. In this sense, the type of nonlinearities that we are looking for are already speci ed in advance, however this speci cation is a very wide one, it merely selects the high dimensional feature space, but not the relevant feature subspace: the latter is done automatically. In this respect it is worthwhile to note that by using sigmoid kernels Sec. 2 we can in fact also extract features which are of the same type as the ones extracted by MLPs cf. Fig. <ref type="figure">2</ref>, and the latter is often considered a nonparametric technique. With its rather wide class of admissible nonlinearities, kernel PCA forms a framework comprising various types of feature extraction systems. A number of di erent k ernels have already been used in Support Vector Machines, of polynomial, Gaussian, and sigmoid type. They all led to high accuracy classi ers, and constructed their decision boundaries, which are hyperplanes in di erent feature spaces, from almost the same Support Vectors Sch olkopf, Burges, <ref type="bibr">&amp; Vapnik, 1995.</ref> The general question of how t o c hoose the best kernel for a given problem is yet unsolved, both for Support Vector Machines and for kernel PCA.</p><p>PCA feature extraction has found application in many areas, including noise reduction, pattern recognition, regression estimation, and image indexing. In all cases where taking into account nonlinearities might be bene cial, kernel PCA provides a new tool which can be applied with little computational cost and possibly substantial performance gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Kernel PCA feature extraction for an OCR task test point x, Eigenvector V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 shows the rst principal component o f a toy data set, extracted by polynomial kernel PCA. For an investigation of the utility of kernel PCA features for a realistic pattern recognition problem, we trained a separating hyperplane classi er Vapnik &amp; Chervonenkis, 1974; Cortes &amp; Vapnik, 1995 on nonlinear features extracted from the US postal service USPS handwritten digit data base by k ernel PCA. This database contains 9300 examples of dimensionality 256; 2000 of them make up the test set. For computational reasons, we used only a subset of 3000 training examples for the dot product matrix.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">This paper, along with several Support Vector publications, can be downloaded from http: www.mpik-tueb.mpg.de people personal bs svm.html.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. BS is supported by the Studienstiftung des Deutschen Volkes. AS is supported by a grant of the DFG JA 379 51. This work pro ted from discussions with V. Blanz, L. Bottou, C. Burges, S. Solla, and V. Vapnik. Thanks to AT&amp;T and Bell Labsoratories for the possibility of using the USPS database.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theoretical foundations of the potential function method in pattern recognition learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rozono Er</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation and Remote Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="821" to="837" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classi ers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Apnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Annual Workshop on COLT</title>
				<meeting><address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1989">1995. 1989</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Application of the Karhunen Lo eve procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simpli ed neuron model as a principal component analyzer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="267" to="273" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting support data for a given task</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Olkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, First International Conference on Knowledge Discovery &amp; Data Mining</title>
				<editor>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<meeting>First International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating invariances in support vector learning machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Olkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN&apos;96</title>
				<editor>
			<persName><forename type="first">C</forename><forename type="middle">V D</forename><surname>Malsburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Seelen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbr Uggen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">B</forename><surname>Sendho</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1112</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nonlinear component analysis as a k ernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Olkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp; K.-R</forename><surname>Uller</surname></persName>
		</author>
		<idno>44</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Max Planck Institut f ur biologische Kybernetik</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Submitted to Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">E cient pattern recognition using a new transformation distance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 5</title>
				<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Theory of Pattern Recognition in Russian</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theorie der Zeichenerkennung, A k ademie Verlag</title>
				<editor>
			<persName><forename type="first">:</forename><forename type="middle">W</forename><surname>German Translation</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Wapnik</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tscherwonenkis</surname></persName>
		</editor>
		<meeting><address><addrLine>Moscow; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Nauka</publisher>
			<date type="published" when="1974">1974. 1979</date>
		</imprint>
	</monogr>
	<note>This article was processed using the L A T E X macro package with LLNCS style</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
