<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Range Loss for Deep Face Recognition with Long-tail</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-11-28">28 Nov 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<email>zhangxiao1688@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Software</orgName>
								<address>
									<country>Tianjin University</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
							<email>fangzy@mail.sustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Southern University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
							<email>yandongw@andrew.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<email>zhifeng.li@siat.ac.cn</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<orgName type="institution">Shenzhen Institutes of Advanced Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Range Loss for Deep Face Recognition with Long-tail</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-28">28 Nov 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">DF5D7A990D123937A89D63913DD400A7</idno>
					<idno type="arXiv">arXiv:1611.08976v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks have achieved great improvement on face recognition in recent years because of its extraordinary ability in learning discriminative features of people with different identities. To train such a welldesigned deep network, tremendous amounts of data is indispensable. Long tail distribution specifically refers to the fact that a small number of generic entities appear frequently while other objects far less existing. Considering the existence of long tail distribution of the real world data, large but uniform distributed data are usually hard to retrieve. Empirical experiences and analysis show that classes with more samples will pose greater impact on the feature learning process <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b19">19]</ref> and inversely cripple the whole models feature extracting ability on tail part data. Contrary to most of the existing works that alleviate this problem by simply cutting the tailed data for uniform distributions across the classes, this paper proposes a new loss function called range loss to effectively utilize the whole long tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarging inter-personal differences within one mini-batch simultaneously when facing even extremely unbalanced data. The optimization objective of range loss is the k greatest range's harmonic mean values in one class and the shortest inter-class distance within one batch. Extensive experiments on two famous and challenging face recognition benchmarks (Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b12">[12]</ref> and YouTube Faces (YTF) <ref type="bibr" target="#b32">[31]</ref>) not only demonstrate the effectiveness of the proposed approach in overcoming the long tail effect but also show the good generalization ability of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have witnessed great improvement on a series of vision tasks such as object classification <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b9">9]</ref> , scene understanding <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b36">35]</ref>, and action recognition <ref type="bibr" target="#b14">[14]</ref>. As for the face recognition task, CNNs like DeepID2+ <ref type="bibr" target="#b26">[26]</ref> by <ref type="bibr">Yi Sun,</ref><ref type="bibr">FaceNet[23]</ref>, DeepFace <ref type="bibr" target="#b28">[28]</ref>, Deep FR <ref type="bibr" target="#b20">[20]</ref>, have even proven to outperform humans on some benchmarks.</p><p>To train a robust deep model, abundant training data <ref type="bibr" target="#b2">[3]</ref> and well-designed training strategies are indispensable. It is also worth to point out that, most of the existing training data sets like LSVRC's object detection task <ref type="bibr" target="#b21">[21]</ref>, which contains 200 basic-level categories, were carefully filtered so that the number of each object instance is kept similar to avoid the long tailed distribution.</p><p>More specifically, long tail property refers to the condition where only limited number of object classes appear frequently, while most of the others remain relatively rarely. If a model was trained under such an extremely imbalanced distributed dataset (in which only limited and deficient training samples are available for most of the classes), it would be very difficult to obtain good performance. In other words, insufficient samples in poor classes/identities will result in the intra-class dispension in a relatively large and loose area, and in the same time compact the interclasses dispension <ref type="bibr" target="#b30">[30]</ref>.</p><p>In <ref type="bibr" target="#b22">[22]</ref>, Bengio gave the terminology called "representation sharing": human possess the ability to recognize objects we have seen only once or even never as representation sharing. Poor classes can be beneficial for knowledge learned from semantically similar but richer classes. While in practice, other than learning the transfer feature from richer classes, previous work mainly cut or simply replicate some of the data to avoid the potential risk long tailed distribution may cause. According to <ref type="bibr" target="#b19">[19]</ref>'s verification, even only 40% of positive samples are left out for feature learning, detection performance will be improved a bit if the samples are more uniform. Such disposal method's flaw is obvious: To simply abandon the data partially, information contained in these identities may also be omitted.</p><p>In this paper, we propose a new loss function, namely range loss to effectively enhance the model's learning ability towards tailed data/classes/identities. Specifically, this loss identifies the maximum Euclidean distance between all sample pairs as the range of this class. During the iteration of training process, we aim to minimize the range of each class within one batch and recompute the new range of this subspace simultaneously.</p><p>The main contributions of this paper can be summarized as follows:</p><p>1. We extensively investigate the long tail effect in deep face recognition, and propose a new loss function called range loss to overcome this problem in deep face recognition. To the best of our knowledge, this is the first work in the literature to discuss and address this important problem.</p><p>2. Extensive experiments have demonstrated the effectiveness of our new loss function in overcoming the long tail effect. We further demonstrate the excellent generalizability of our new method on two famous face recognition benchmarks (LFW and YTF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning is proved to own a great ability of feature learning and achieve great performances in a series of vision tasks like object detection <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b27">27]</ref>, face recognition <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref>, and so forth. By increasing the depth of the deep model to 16-19 layers, VGG <ref type="bibr" target="#b25">[25]</ref> achieved a significant improvement on the VOC 2012 <ref type="bibr" target="#b3">[4]</ref> and Caltech 256 <ref type="bibr" target="#b4">[5]</ref>. Based on the previous work, Residual Network, proposed by Kaiming He et al, present a residual learning framework to ease the training of substantially deeper networks <ref type="bibr" target="#b9">[9]</ref>. In <ref type="bibr" target="#b30">[30]</ref>, the authors propose a new supervision signal, called center loss, for face recognition task. Similar to our range loss's main practice, center loss minimizes the distances between the deep features and their corresponding class centers ( Defined as arithmetic mean values).</p><p>Long tailed distribution of the data has been involved and studied in scene parsing <ref type="bibr" target="#b33">[32]</ref>, and zero-shot learning <ref type="bibr" target="#b18">[18]</ref>. In a workshop talk 2015, Bengio described the long tail distribution as the enemy of machine learning <ref type="bibr" target="#b22">[22]</ref>. In <ref type="bibr" target="#b33">[32]</ref>, a much better super-pixel classification results are achieved by the expanding the poor classes' samples. In <ref type="bibr" target="#b19">[19]</ref>, this paper investigates many factors that influence the performance in fine-tune for object detection with long tailed distribution of samples. Their analysis and empirical results indicate that classes with more samples will pose greater impact on the feature learning. And it is better to make the sample number more uniform across classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>In this section, we firstly elaborate our exploratory experiments implemented with VGG on LFW's face verification task, which give us an intuitive understanding of the potential effects by long tailed data. Based on the conclusion drew from these two experiments, we propose a new loss function namely, range loss to improve model's endurance and utilization rate toward highly imbalanced data follow by some discussions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>In statistics, a long tail of certain distributions is the portion of the distribution having a large number of occurrences far from the "head" or central part of the distribution <ref type="bibr" target="#b0">[1]</ref>. To investigate the long-tail property deeply and thoroughly in the context of deep learning face recognition, we first trained several VGG-16 models <ref type="bibr" target="#b25">[25]</ref> with softmax loss function on data sets with extremely imbalanced distribution ( the distribution of our training data is illustrated in 2. ) We constructed our long tail distributed training set from MS-Celeb-1M <ref type="bibr" target="#b5">[6]</ref> and CASIA-WebFace <ref type="bibr" target="#b34">[33]</ref> data set, which consists of 1.7 million face images with almost 100k identities included in the training data set. Among this set, there are 700k images for roughly 10k of the identities, and 1 million images for the remaining 90k identities. To better understand the potential effect of long tailed data on the extracted identical representation features, we slice the raw data into several groups according to different proportions in Table <ref type="table">1</ref>. As we can see in <ref type="bibr">Fig 2,</ref><ref type="bibr"></ref> classes that contain less than 20 images are defined as poor classes (tailed data). As is shown in Table1, group A-0 is the raw training set. 20%, 50%, 70%, 100% of the poor classes in A-0 is cut to construct group A-1, A-2, A-3 and A-4 respectively. We conduct our experiments on LFW's face verification task and the accuracy are compared in Table <ref type="table" target="#tab_1">2</ref>. As is shown in Table <ref type="table" target="#tab_1">2</ref>, group A-2 achieves the highest accuracy rate in series A. With the growth of the tail, group A-1 and A-0 get lower performances though they contain more identities and images.</p><p>These results indicate that, tailed data stand a great chance to pose a negative effect on the trained model's ability. Based on the above findings, we come to analyze the distinct characteristics of Long-tail effect that, conventional visual deep models do not always benefit as much from larger data set with long-tailed property as it does for a uniform distributed larger data set. Moreover, long tailed data set, if cut and remained in a specific proportion (50% in here), will contribute to deep models' training.</p><p>In fact, there are some different features in face recognition task: the intra-class variation is large because the face image can be easily influenced by the facing directions, lighting conditions and original resolutions. On the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Groups</head><p>Acc. on LFW A-0 (with long-tail) 97.87% A-1 (cut 20% tail) 98.03% A-2 (cut 50% tail) 98.25% A-3 (cut 70% tail) 97.18% A-4 (cut 100% tail) 95.97% other hand, compared with other recognition tasks, the inter class variation in face recognition is much smaller. As the growth of the number of identities, it is possible to include two identities with similar face. Worse still, their face images are so few that can not give a good description to their own identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Study of VGG Net with Contrastive and Triplet Loss on Subsets of Object Classes</head><p>Considering the characteristics of long tailed distributions: a small number of generic objects/entities appear very often while most others exist much more rarely. People will naturally think the possibility to utilize the contrastive loss <ref type="bibr" target="#b26">[26]</ref> or the triplet loss <ref type="bibr" target="#b23">[23]</ref> to solve the long tail effect because of its pair training strategy.</p><p>The contrastive loss function consists of two types of samples: positive samples of similar pairs and negative samples of dissimilar pairs. The gradients of the loss function act like a force that pulls together positive pairs and pushes apart in negative pairs. Triplet loss minimizes the distance between an anchor and a positive sample, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.</p><p>In this section, we apply the contrastive loss and triplet loss on VGG-16 with the same constructed long tailed distributed data. The goal of this experiment, on some level, is to gain insights on the contrastive loss and triplet loss's processing capacity of long tailed data. We conduct the LFW's face verification experiment on the most representative groups A-0 and group A-2 with full and half of the long tailed data. As for the training pairs, we depart all identities into two parts with same number of identities firstly. The former part contains only richer classes and the later poor classes. Positive pairs (images of the same person) are randomly selected from the former part and negative pairs are generated in the latter part data of different identities. After training, we got the contrastive and triplet's results shown in Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref> respectively. From these tables, we can clearly see that long tail effect still exist on models trained with contrastive loss and triplet loss: with 291,277 more tailed images in group A-0's training set, contrary to promoting the verification performances, accuracy is reduced by 0.15%. Moreover, contrastive loss improves the accu-Training Groups Acc. on LFW A-0 (with long-tail) 98.35% A-2 (cut 50% of tail) 98.47% racy by 0.46% and 0.21% comparing to VGG-16 with softmax loss. Probable causes of long tail effect's existence in contrastive loss may lie that: though pair training and triplet training strategy can avoid the direct negative effect long tail distribution may brought, classes in the tail are more like to be selected in the training pairs' construction (poor classes are accounted for 90% of the classes). Because the massive classes with rare samples piled up in the tail, pairs contain the pictures of one person are extremely limited in a small amount, thus resulting in the lack of enough descriptions toward intra-class's invariation. Inspired by contrastive and triplet loss's defect and deficiency, we find the necessity to propose our loss function specially-costumed to be integrated into training data with long tail distribution. Such loss function is designed primarily for better utilizing the tailed data, which we believe has been submerged by the richer classes' information and poses not only almost zero impact to the model, but a negative resistance to model's effectiveness in learning discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Range Loss</head><p>Intrigued by the experiment results above that long tail effect does exist in models trained with contrastive loss and triplet loss, we delve deeper into this phenomenon, give a qualitative explanation of the necessity to propose our new loss toward this problem and further discuss the merits and disadvantages of the existing methods.</p><p>In long tail distributed data, samples of the tailed data are usually extremely rare, there are only very limited images for each person in our dataset. Contrastive loss optimizes the model in such a way that neighbors are pulled together and non-neighbors are pushed apart. To construct such a training set consists of similar pairs and negative examples of dissimilar pairs, sufficient pairs of the same person is indispensable but out of the question to be achieved on long tailed data.</p><p>Moreover, as we discussed in the previous section, richer classes will pose greater impact on the model's training. Ways to leverage the imbalanced data should be considered.</p><p>The the objective of designing range loss is summarized as:</p><p>• Range loss should be able to strengthen the tailed data's impact in the training process to prevent poor classes from being submerged by the rich classes.</p><p>• Range loss should penalize those sparse samples' dispension brought by poor classes.</p><p>• Enlarge the inter-class distance at the same time.</p><p>Inspired by the contrastive loss, we design the Range Loss in a form that reduces intra-personal variations while enlarge the inter-personal differences simultaneously. But contrary to contrastive loss function's optimizing on positive and negative pairs, the range loss function will calculate gradients and do back propagation based on the overall distance of classes within one minibatch. In other words, statistical value over the whole class substituted the single sample's value on pairs. As to the second goal, the author in <ref type="bibr" target="#b11">[11]</ref> use the hard negative mining idea to deal with these samples. For those sparse training samples in poor classes, features located in the feature space's spatial edge(edge feature) can be viewed as the points that enlarge the intraclass's invariation most. These samples, to a certain degree, can also be viewed as the hard negative samples. In-spired by this idea, range loss should be designed to minimize those hard negative samples' distance thus lessen the exaggerated intra-class invariation by tailed data. Based on this, we calculate k greatest range's harmonic mean value over the feature set extracted in the last FC layer as the interclass loss in our function. The range value can be viewed as the intra-class's two most hard negative samples. For the inter-class loss, the shortest distance of class feature centers will be the supervision.</p><p>To be more specifically, range loss can be formulated as:</p><formula xml:id="formula_0">L R = αL Rintra + βL Rinter<label>(1)</label></formula><p>Where α and β are two weight of range loss and in which L Rintra denotes the intra-class loss that penalizes the maximum harmonic range of each class:</p><formula xml:id="formula_1">L Rintra = i⊆I L i Rintra = i⊆I k k j=1 1 Dj<label>(2)</label></formula><p>Where I denotes the complete set of classes/identities in this mini-batch. D j is the j-th largest distance. For example, we define L Rinter represents the inter-class loss that</p><formula xml:id="formula_2">D 1 = x 1 -x 2</formula><formula xml:id="formula_3">L Rinter = max(m -D Center , 0) = max(m -x Q -x R 2 2 , 0)<label>(3)</label></formula><p>where, D Center is the shortest distance between class centers, that are defined as the arithmetic mean of all output features in this class. In a mini-batch, the distance between the center of class Q and class R is the shortest distance for all class centers. m denotes a super parameter as the max optimization margin that will exclude D Center greater than this margin from the computation of the loss.</p><p>In order to prevent the loss being degraded to zeros <ref type="bibr" target="#b30">[30]</ref> during the training, we use our loss joint with the softmax loss as the supervisory signals. The final loss function can be formulated as:</p><formula xml:id="formula_4">L = L M + λL R = - M i=1 log e W T y i xi+by i n j=1 e W T j xi+bj + λL R (4)</formula><p>In the above expression, M refers to the mini-batch size and n is the number of identities within the training set. x i  denotes the features of identity y i extracted from our deep model's last fully connected layers. W j and b j are the parameters of the last FC layer. λ is inserted as a scaler to balance the two supervisions. If set to 0, the overall loss function can be seen as the conventional softmax loss. According to the chain rule, gradients of the range loss with respect to x i can be computed as:</p><formula xml:id="formula_5">∂L R ∂x i = α ∂L Rintra ∂x i + β ∂L Rinter ∂x i<label>(5)</label></formula><p>For a specific identity, let S =</p><formula xml:id="formula_6">k i=1<label>1</label></formula><p>Di , D j is a distance of x j1 and x j2 , two features in the identity.</p><formula xml:id="formula_7">∂L Rintra ∂x i = 2k (D j S) 2    |x j1 -x j2 | , x i = x j1 |x j2 -x j1 | , x i = x j2 0, x i = x j1 , x j2<label>(6)</label></formula><formula xml:id="formula_8">∂L Rinter ∂x i =        ∂L ∂x Q = 1 2n R x R n R -x Q n Q ∂L ∂x R = 1 2n Q x Q n Q -x R n R 0, x i = x Q , x R<label>(7)</label></formula><p>Where n i denotes the total number of samples in class i. And we summarize the loss value and gradient value's computation process in Algorithm 1. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussions on Range Loss's Effectiveness</head><p>Generally speaking, range loss adopts two stronger identifiability statistical parameters than contrastive loss and   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our range loss based models on two well known face recognition benchmarks, LFW and YTF data sets. We firstly implemented our range loss with VGG's <ref type="bibr" target="#b25">[25]</ref> architecture and train on 50% and 100% long tailed data to measure its performances on face verification task. More than that, based on <ref type="bibr" target="#b30">[30]</ref>'s recent proposed center loss which achieves the state-of-art performances on LFW and YTF, we implement our range loss with the same network's structure to see whether the range loss is able to handle the long tailed data better than other loss function in a more general CNN's structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details of VGG with Range Loss</head><p>Training Data and Preprocessing: To get a high-quality training data, we compute a mean feature vector for all identities according to their own pictures in data set. For a specific identity, images whose feature vector is far from the identity's feature vector will be removed. After carefully filtering and cleaning the MS-Celeb-1M <ref type="bibr" target="#b5">[6]</ref> and CASIA-WebFace <ref type="bibr" target="#b34">[33]</ref> data set, we obtain a dataset which contains 5M images with 100k unique identities. We use the new proposed multi-task cascaded CNN in <ref type="bibr" target="#b35">[34]</ref> to conduct the face detection and alignment. Training images are cropped to the size of 224×224 and 112×94 RGB 3-channel images for VGG and our CNN model's input, respectively. In this process, to estimate a reasonable mini-batch size is of crucial importance. By our experiences, it's better to construct such a mini-batch that contains multiple classes and same number of samples within each class. For examples, we set mini-batch size at 32 in our experiment, and 4 different identities in one batch with 8 images for each identity. For those small scale nets, it's normal to set 256 as the batch size, with 16 identities in one batch and 16 images per identities. Generally speaking, more identities being included in one mini-batch will contribute to both the softmax loss's supervising and the range loss's inter-class part.</p><p>VGG's settings: The VGG net is a heavy convolutional neural networks model, especially when facing a training set with large amounts of identities. For 100k identities, according to our experiences, the mini-batch size can never exceed 32 because of the limitation of the GPU memory. The net is initialized by Gaussian distribution. The loss weight of the inter-class part of range loss is 10 -4 while the intra-class part of range loss is 10 -5 . The parameter margin is set 2 × 10 4 . Initial learning rate is set at 0.1 and reduce by half every 20, 000 iterations. We extract each of the testing sample's feature in the last fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performances on LFW and YTF Data sets</head><p>LFW is a database of face photographs designed for unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. 1680 of the people have two or more distinct photo's in this data set <ref type="bibr" target="#b12">[12]</ref>.</p><p>YouTube faces database is a database of face videos designed for studying the problem of unconstrained face recognition in videos. The data set contains 3,425 videos of 1,595 different people. All the videos were downloaded from YouTube. An average of 2.15 videos are available for each subject <ref type="bibr" target="#b32">[31]</ref>. We implement the CNN model using the Caffe <ref type="bibr" target="#b13">[13]</ref> library with our customized range loss layers. For comparison, we train three models under the supervision of softmax loss (model A), joint contrastive loss   Secondly, the integration of range loss to the model enables the latter 50% tailed data to contribute to model's learning. This shows that, the original drawback that tailed data may bring, has been more than eliminated, but converted into notably contribution. This shows the advantage of our proposed range loss in dealing with long tailed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance of Range Loss on other CNN structures</head><p>To measure the performances and impact by the range loss and comprehensively and thoroughly, we further adopt residual CNN <ref type="bibr" target="#b9">[9]</ref> supervised by the joint signals of range loss and softmax. Deep residual net in recent years have been proved to show good generalization performance on recognition tasks. It presents a residual learning framework that ease the training of networks substantially deeper than those used previously and up to 152 layers on the Imga-geNet dataset. That we choose this joint signals can be largely ascribed to the softmax's strong ability to give a discriminative boundaries among classes. Different to our previous practice, the model is trained under 1.5M filtered data from MS-Celeb-1M <ref type="bibr" target="#b5">[6]</ref> and CASIA-WebFace <ref type="bibr" target="#b34">[33]</ref>, which is of smaller scale size of the original long tail dataset with a more uniform distribution. The intention of this experiment lies that: apart from the ability to utilize amounts of imbalanced data, we want to verify our loss function's generalization ability to train universal CNN model and to achieve the state-of-art performances. We evaluate the range loss based residual net's performances on LFW and YTF's face verification task. The model's architecture is illustrated in Fig. <ref type="figure">7</ref>. In Table <ref type="table" target="#tab_6">6</ref>, we compare our method against many existing models, including DeepID-2+ <ref type="bibr" target="#b26">[26]</ref>, FaceNet <ref type="bibr" target="#b23">[23]</ref>, Baidu <ref type="bibr" target="#b17">[17]</ref>, DeepFace <ref type="bibr" target="#b28">[28]</ref> and our baseline model D (Our residual net structure supervised by softmax loss). From the results in Table <ref type="table" target="#tab_6">6</ref>, we have the following observations. Firstly, our model E (supervised by softmax and range loss) beats the baseline model D (supervised by softmax only) by a significant margin (from 98.27% to 99.52% in LFW, and 93.10% to 93.70% in YTF). This represents the joint supervision of range loss and softmax loss can notablely enhance the deep neural models' ability to extract discriminative features. Secondly, residual network integrated with range loss was non-inferior to the existing famous networks and even outperforms most of them. This shows our loss function's generalization ability to train universal CNN model and to achieve the state-of-art performances. Lastly, our proposed networks are trained under a database far less than other's(shown in Table <ref type="table" target="#tab_6">6</ref>), this indicates the advantages of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we deeply explore the potential effects the long tail distribution may pose to the deep models training. Contrary to our intuitiveness, long tailed data, if tailored properly, can contribute to the model's training. We proposed a new loss function, namely range loss. By combining the range loss with the softmax loss to jointly supervise the learning of CNNs, it is able to reduce the intra-class variations and enlarge the inter-class distance under imbalanced long tailed data effectively. Therefore, the optimization goal towards the poor classes should be focused on these thorny samples within one class. Its performance on several largescale face benchmarks has convincingly demonstrated the effectiveness of the proposed approach.   <ref type="figure">7</ref>. Residual Network's structure adopted in our experiment. All the convolutional filters' size are 3×3 with stride 1. Activation units ReLu layers are added after each convolutional layers. The number of the feature maps are 32 from the front layers to 512 in the last layers. We set the max-pooling's kernel size as 2×2 with stride 2. Features in the last convolutional layer and the penultimate convolutional layer are extracted and concatenated as the input of the last fully connected layers. The whole CNN is trained under the joint supervisory signals of soft-max and our range loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Long tail distributed data set for human faces(Selected from MS-Celeb-1M[6]). Number of face images per person falls drastically, and only a small part of persons have large number of images. Cutting line in red represents the average number of images per person.</figDesc><graphic coords="2,50.11,72.00,236.25,193.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Our Constructed Data set with Long-tailed Distributions. The Cutting lines in the above figure represent the division proportions we used to construct subsets of object classes.</figDesc><graphic coords="2,308.86,71.99,236.25,165.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An simulated 2-D feature distribution graph in one minibatch. There are 4 classes in this mini-batch, and Class B represents one typical poor class. D1 denotes Class B's greatest intraclass range. L2 between Class D and Class A represents the center distance of these two classes. The objective of range loss can be seen as the shortest center distances( L2 in these 4 classes) and the harmonic mean value of the k greatest ranges( D1 as for Class B) in each class. (Best viewed in color.)</figDesc><graphic coords="4,308.86,72.00,236.26,207.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 2 2 .</head><label>22</label><figDesc>and D 2 = x 3 -x 4 2 D 1 and D 2 are the largest and second largest Euclidean range for a specific identity i respectively. Input x 1 and x 2 denoted two face samples with the longest distance, and similarly, input x 3 and x 4 are samples with of the second longest distance. Equivalently, the overall cost is the harmonic mean of the first k-largest range within each class. Experience shows that k = 2 bring a good performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Some common face images in LFW.</figDesc><graphic coords="5,308.86,72.00,236.26,119.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Some common face images in YTF.</figDesc><graphic coords="5,308.86,217.33,236.24,102.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Training algorithm with range loss Require: Feature set {x i } extracted from the last fully connected layer. Hyper parameter m and λ. Ensure: The intra-class part of range loss L Rintra and the inter-class part of range loss L Rinter . The gradient of intra-class ∂L R intra ∂xi and inter-class ∂L R inter ∂xi . for each class i ⊆ I in one mini-batch do Compute the arithmetic mean feature as feature center c i of class i. Compute the k largest Euclidean distances {D j } among features {x i } of class i. Compute the harmonic mean of {D j } as the intra-class loss of class i, L i R = k k j=1 Dj . end for Compute the intra-class loss L Rintra = i⊆I L i R = i k k j=1 Dj . Compute the intra-class gradient ∂L R intra ∂xi . Compute the shortest distances D center among all feature centers {c P }. if m -D min &gt; 0 then Output the inter-class gradient ∂L R inter ∂xi . else∂L R inter ∂xi = 0. end if others: distance of the peripheral points in the intra-class subspace, and the center distance of the classes. Both the range value and the center value is calculated based on groups of samples. Statistically speaking, range loss utilizes those training samples of one mini-batch in a joint way instead of individually or pairly, thus ensure the model's optimization direction comparatively balanced. To give an intuitive explanations of the range loss, we have simulated a 2-D feature distribution graph in one mini-batch with 4 classes (see Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An Overview of Our Filtered and Cropped Face Database. Images in the first row are raw images before alignment and cropping. Corresponding images are listed below the raw images. Some common faces in our training set are presented in the last row.</figDesc><graphic coords="10,50.11,146.17,495.00,187.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure</head><label></label><figDesc>Figure 7. Residual Network's structure adopted in our experiment. All the convolutional filters' size are 3×3 with stride 1. Activation units ReLu layers are added after each convolutional layers. The number of the feature maps are 32 from the front layers to 512 in the last layers. We set the max-pooling's kernel size as 2×2 with stride 2. Features in the last convolutional layer and the penultimate convolutional layer are extracted and concatenated as the input of the last fully connected layers. The whole CNN is trained under the joint supervisory signals of soft-max and our range loss.</figDesc><graphic coords="10,50.11,513.66,495.00,66.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>VGG Net with Softmax Loss's performances on LFW with Long-tail Effect.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>VGG Net with Softmax+Contrastive Loss's performances on LFW with Long-tail Effect.</figDesc><table><row><cell>Training Groups</cell><cell>Acc. on LFW</cell></row><row><cell>A-0 (with long-tail)</cell><cell>98.10%</cell></row><row><cell>A-2 (cut 50% of tail)</cell><cell>98.40%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>VGG Net with Softmax+Triplet Loss's performances on LFW with Long-tail Effect.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Verification Accuracy of different loss combined with VGG on LFW and YTF data sets. Model A is using the softmax loss only. Model B is using the contrastive loss with softmax loss and Model C is using the range loss with softmax loss.</figDesc><table><row><cell>Methods</cell><cell>Images</cell><cell>LFW</cell><cell>YTF</cell></row><row><cell>DeepID-2+ [26]</cell><cell>-</cell><cell cols="2">99.47% 93.20%</cell></row><row><cell>FaceNet [23]</cell><cell>200M</cell><cell cols="2">99.63% 95.10%</cell></row><row><cell>Baidu [17]</cell><cell>1.3M</cell><cell>99.13%</cell><cell>-</cell></row><row><cell>Deep FR [20]</cell><cell>2.6M</cell><cell cols="2">98.95% 97.30%</cell></row><row><cell>DeepFace [28]</cell><cell>4M</cell><cell cols="2">97.35% 91.40%</cell></row><row><cell>Model D</cell><cell>1.5M</cell><cell cols="2">98.27% 93.10%</cell></row><row><cell>Model E</cell><cell>1.5M</cell><cell cols="2">99.52% 93.70%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Compare Verification Accuracy of different CNN model on LFW and YTF datasets with our proposed CNN Networks. Model D is our adopted residual net with softmax loss only. Model E is the same net using range loss.</figDesc><table><row><cell>and softmax loss (model B), and softmax combined with</cell></row><row><cell>range loss (model C). From the results shown in Table 5,</cell></row><row><cell>we can see that Model C (jointly supervised by the range</cell></row><row><cell>loss and softmax loss) beats the baseline model A (super-</cell></row><row><cell>vised by only softmax loss) by a large gap: from 97.87% to</cell></row><row><cell>98.53% in LFW. Contrary to our previous experimental re-</cell></row><row><cell>sult that models trained with complete long tailed data reach</cell></row><row><cell>a lower accuracy, our model's (Model C) performances on</cell></row><row><cell>complete long tail exceed the 50% long tail group's result</cell></row><row><cell>by 0.43%. This shows that, firstly, comparing to soft-max</cell></row><row><cell>loss and contrastive loss, range loss's capacity of learning</cell></row><row><cell>discriminative feature from long tailed data performed best.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The long tail of expertise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spradlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2002">June 2010. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2016. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2007">2015. 1, 2, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond hard negative mining: Efficient detector learning via block-circulant decomposition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2760" to="2767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<title level="m">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007">2015. 1, 2, 7</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The battle against the long tail</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Talk on Workshop on Big Data and Statistical Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2015. 1, 2, 3, 7</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2006">2014. 1, 2, 3, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1988">1988-1996, 2014. 1, 2, 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent factor guided convolutional neural networks for age-invariant face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4893" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2016. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3294" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02878</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Naive-deep face recognition: Touching the limit of lfw benchmark or not</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
