<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The security of machine learning in an adversarial setting: a survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xianmin</forename><surname>Wang</surname></persName>
							<email>xianmin@gzhu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangzhou University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
							<email>lijing@gzhu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangzhou University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangzhou University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohui</forename><surname>Kuang</surname></persName>
							<email>xiaohui_kuang@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">National Key Laboratory of Science and Technology on Information System Security</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-An</forename><surname>Tan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The security of machine learning in an adversarial setting: a survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7497ED512B6F59C03B499C9BD2AA5208</idno>
					<idno type="DOI">10.1016/j.jpdc.2019.03.003</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>adversarial setting</term>
					<term>adversarial attack</term>
					<term>adversarial example</term>
					<term>security model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection.</p><p>Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed</p><p>that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for $ Fully documented templates are available in the elsarticle package on CTAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past decade, machine learning (ML) <ref type="bibr" target="#b0">[1]</ref> has powered many aspects of practical tasks, e.g., natural language understanding, object recognition, target detection, and failure localization <ref type="bibr" target="#b1">[2]</ref>. ML methods are representation-learning methods that map an input domain to an output domain by observing sample pairs of inputs and outputs. A distinguishing characteristic of ML is that the programmer designs a way to discover the solution to a problem using instances of solved problems, instead of deterministically describing the solution via coding. Because the features computed by machines are usually more robust and representative than hand-crafted features, ML methods can achieve satisfactory results in many applications.</p><p>ML methods are traditionally developed under the assumption that both the training and evaluating process in ML are implemented under a benign setting. As a result, the statistical characteristics of the data employed to make predictions are the same statistical characteristics of the data used for training the ML model. However, this assumption usually does not hold in the case whereby the ML model is designed under adversarial settings because a capable adversary is able to modify some statistical properties of the source data. Fig. <ref type="figure" target="#fig_0">1</ref> demonstrates the two settings whereby an ML model is deployed. It implies that the source data of the traditional statistical model of ML algorithms under a benign setting does not depend on the classifier, and the noise affecting the data is stochastic (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). In contrast, the source data under an adversarial setting are not neutral, and the adversarial noise is carefully crafted to maximize the classifier's error (Fig. <ref type="figure" target="#fig_0">1(b)</ref>).</p><p>Because of the continuing advancement of deep learning (DL) methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and the high accessibility of hardware for training complex models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, ML   <ref type="bibr" target="#b9">[10]</ref>. The adversarial example is formulated using the origin image and the carefully crafted perturbations such that a benign tumor is misclassified as a malignant tumor with 100% confidence.</p><p>methods have advanced to a new stage: when provided with naturally occurring inputs, they can outperform human beings on several tasks. We have also reached the point whereby ML performs satisfactorily but may easily be broken by adversarial examples that are carefully crafted toward training or test phases <ref type="bibr" target="#b6">[7]</ref>. Specifically, Szegedy et al. revealed some interesting properties of neural networks that contradict commonly held beliefs <ref type="bibr" target="#b7">[8]</ref>. These discoveries lead to a quite astonishing view: there exists a "blind point" in every deep neural network in the sense that some input perturbations that are imperceivable to human eyes can completely fool the ML model. This small perturbation can be carefully crafted to formulate adversarial examples, forcing a network to produce incorrect predictions with high confidence <ref type="bibr" target="#b8">[9]</ref>.</p><p>As ML methods are further applied in cases where adversaries have incentives to disturb the operation of the ML system, there has been increasing interest in studying the adversarial attack behavior and protection strategies under adversarial settings. Especially in recent years, the discovery of adversarial examples has raised substantial concerns in various practical applications, e.g. Internet of Things (IoT), smart home systems, wireless sensor network, voice and video traffics, mobile platform and information security <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. For example, in the area of medical imaging <ref type="bibr" target="#b9">[10]</ref>, a benign tumor can be made to be misclas-sified as a malignant tumor by an ML-based tumor detection system by adding carefully designed distortions to the origin images (Fig. <ref type="figure" target="#fig_1">2</ref>). For PDF malware classification systems, such as PDFrate <ref type="bibr" target="#b14">[15]</ref> and Hidost <ref type="bibr" target="#b15">[16]</ref>, some elaborate maleware variants can preserve their malicious behavior while also being classified as benign by the classifier <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. In addition, an adversary can even fool an automated driving system by designing a perturbed image that looks like a stop sign to human eyes but is detected as a passing sign by the ML system <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this work, we present a comprehensive survey regarding with the security of ML under adversarial settings in the context of various applications. We briefly introduce the taxonomy of ML methods in Section 2 and then give a framework for ML security under adversarial settings by analyzing the ML security model in Section 3. Section 4 reviews the adversarial attacks according to the ML implementation phases, while Section 5 discusses various defense approaches against the adversarial attacks. We finally discuss future research challenges and give a wide viewpoint of the research direction in the conclusion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of machine learning</head><p>ML is an interdisciplinary area of computer science, statistics, probability and brain-like technology, therein playing a vital role in AI research. ML algorithms are used in designing and implementing an automatic knowledge acquisition system by imitating human behavior. Based on the data characteristics, ML methods can be classified into three categories <ref type="bibr" target="#b19">[20]</ref>, i.e., supervised learning (SL) methods, unsupervised learning (USL) methods and reinforcement learning (RL) methods.</p><p>In the training phase of SL methods, the input data are labeled with their corresponding output. Typical instances of SL methods include decision trees (DTs) <ref type="bibr" target="#b20">[21]</ref>, support vector machine (SVM) <ref type="bibr" target="#b21">[22]</ref>, neural networks (NNs) <ref type="bibr" target="#b22">[23]</ref> and linear regression (LR) <ref type="bibr" target="#b23">[24]</ref>. Many practical tasks are based on SL methods, e.g., natural language processing (NLP) <ref type="bibr" target="#b24">[25]</ref>, object recognition in images <ref type="bibr" target="#b25">[26]</ref>,</p><p>Attitude Estimation <ref type="bibr" target="#b26">[27]</ref> and Computer Aided Translation (CAT) <ref type="bibr" target="#b27">[28]</ref>. In an adversarial setting, various of adversarial attacks are ascertained on the real world <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The main characteristic of USL methods is that the training inputs are unlabeled. USL methods consist of model pre-training <ref type="bibr" target="#b30">[31]</ref>, clustering points <ref type="bibr" target="#b31">[32]</ref> and dimensionality reduction <ref type="bibr" target="#b32">[33]</ref>. Some literatures have reported that the USL methods also suffer from adversarial attack in an malicious setting in practice <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, although adversarial examples are mainly designed against SL methods. RL methods discover the optimal agent behavior by a trial-and-error process based on the actions, observations and rewards of the agent <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. It is the subfield of controlling and scheming. Combined with SL and USL methods, RL enabled a computer to defeat a human Go champion in 2016 <ref type="bibr" target="#b37">[38]</ref>. Since RL methods are usually associated with SL and USL techniques, they are susceptible to adversarial examples in adversarial setting <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Security model of machine learning</head><p>The security model of ML includes an attack surface, threat model and adversarial model. This security model provides a blueprint for studying attack and defense mechanics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Machine learning attack surface</head><p>To measure the security of the ML system, Paperno et al. formulated a system's attack surface consisting of an ML model and dataset based on the adversarial's attack objective <ref type="bibr" target="#b40">[41]</ref>. Fig. <ref type="figure" target="#fig_2">3</ref> shows the attack surface of ML systems, which is constructed to describe the features of available attacks under an adversarial setting. Note that the top line is a generalized data processing pipeline for ML, whereas the bottom line represents the various attacks from which the ML model suffers according to the data processing pipeline. As shown, in the data collection process, the adversary can access the ML model by manipulating the data acquisition process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Threat model of machine learning</head><p>For systematically researching the ML threat model, Dalvi first studied adversarial attacks for ML-based spam detection systems <ref type="bibr" target="#b41">[42]</ref> in 2004. Then, Lowd et al. proposed the idea of adversarial learning <ref type="bibr" target="#b42">[43]</ref> motivated by Dalvi's work.</p><p>In 2010, Barreno et al. explicitly presented the concept of ML security, as well as the taxonomy and modeling of the attack behavior <ref type="bibr" target="#b43">[44]</ref>. In 2014, they further improved the ML security framework <ref type="bibr" target="#b44">[45]</ref>. In addition, <ref type="bibr">Kurakin et al. discussed</ref> the label leaking phenomenon of ML under an adversarial setting in 2016 <ref type="bibr" target="#b28">[29]</ref>.</p><p>Based on these studies, the taxonomy categorizing the ML threat model can be classified along three directions, as shown in Fig. <ref type="figure" target="#fig_17">4</ref>.</p><p>( integrity of the ML model is compromised, deviations of the prediction results may be generated <ref type="bibr" target="#b45">[46]</ref>. For example, junk email can be modified with some good words to fool spam filters (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b)). Availability attacks are usually caused by false positives, e.g., denial-of-service (DoS) attacks. For instance, if an adversary puts a malicious object that is difficult to identify by ML systems on a busy road, autonomous vehicles may be forced to stop along the side of a road. Confidentiality attacks attempt to steal sensitive information from a dataset and the parameters of the ML model. A well-designed ML system should protect important information from unauthorized users. For example, an ML-based medical diagnosis system is required to prevent adversaries from analyzing the model and recovering infirmation about the patient.</p><p>(3) In the direction of attack specification, the threat model can be divided into discriminate attacks and indiscriminate attacks. Discriminate attacks primarily concern a particular instance, whereas indiscriminate attacks encompass a broad range of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial model of machine learning</head><p>The ML adversarial model includes the adversarial goal, adversarial capabilities, adversarial knowledge and adversarial strategy.</p><p>(1) Adversarial goal. The adversarial goals can be defined from two perspectives, i.e., the level of destruction caused by the adversary and the attack specificality, discussed in the above section. In the former, the adversary impacts the confidentiality, integrity, availability and privacy of the ML model <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. In the latter, discriminate and non-discriminate attacks are produced. For example, the adversary can maximize the misclassification probability for an ML system using a discriminate and integrity attack and can also obtain confidential information about a guest via a targeted attack.</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The adversarial attack</head><p>Research on ML security mainly focuses on supervised ML methods. Many adversarial attacks have been discovered in early supervised-based security detection systems. For example, Wittel et al. proposed to inject malicious data into NB-based spam detection systems to obfuscate the model's predictions <ref type="bibr" target="#b49">[50]</ref>. Srndic et al. broke an SVM-based malicious PDF detection system by crafting some malicious PDF variants <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>In addition, some unsupervised ML methods also face security issues under adversarial settings <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>. For instance, the adversary can inject malicious examples into the clustering system to impact the clustering results. Moreover, some obfuscation attacks can hide the adversarial examples while keeping the clustering results unchanged <ref type="bibr" target="#b55">[56]</ref>.</p><p>Currently, deep learning (DL) techniques are increasingly raising concerns in academic and industrial communities <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. As an important part of DL, deep neural networks (DNNs) achieve remarkable results in many multimode recognition tasks. In most cases, DL methods outperform traditional ML methods in terms of prediction accuracy; however, Szegedy et al. revealed two characteristics with regard to how DNNs work <ref type="bibr" target="#b7">[8]</ref>. The first observation refers to the fact that a single feature of a DNN is uninterpretable. This interesting finding leads to the other more astonishing aspect: there exist special inputs that are close to correctly classified samples but that are thoroughly misclassified by  </p><formula xml:id="formula_2">min µ H |µ| + L (I c + µ, l)<label>(2)</label></formula><p>Equation 2 should satisfy</p><formula xml:id="formula_3">I c + µ ∈ [0, 1]</formula><p>m and H (I c + µ) = l, where L is the loss function of the classification.</p><p>• Fast gradient sign method (FGSM)</p><p>FGSM is an efficient adversarial generation method proposed by Goodfellow <ref type="bibr" target="#b66">[67]</ref>. This method creates adversarial examples by appending noise </p><p>where ∇ς is the cost gradient with regard to I c under the parameters θ, the sign symbol represents the sign operator, and the scaler value ε is used to confine the volume of the perturbations. Note that in higher dimensional space, the FGSM examples present some "linearity" characteristics for the DNN model, whereas the DNN models are generally considered to be highly non-linear. Fig. <ref type="figure" target="#fig_5">7</ref> demonstrates an example of an FGSM adversarial on the ImageNet dataset.</p><p>• Universal adversarial perturbations (UAP)</p><p>The FGSM and L-BFGS methods employ adversarial examples to attack the DNN model using a single image. Attacks exhibiting promising results on one image usually fail to achieve favorable results on other images.</p><p>To deceive a DNN model on every image at high confidence, Moosavi-Dezfooli et al. developed a universal attack approach in which "universal" adversarial perturbations are generated <ref type="bibr" target="#b67">[68]</ref>. Suppose that the original images are collected with a distribution D. The "universal" perturbation µ is determined by</p><formula xml:id="formula_5">P Ic∈D (H (I c ) = H (I c + µ)) ≥ δ<label>(4)</label></formula><p>where δ represents the attack ratio, validated the effectiveness of the proposed method only on ResNet.</p><formula xml:id="formula_6">µ p ≤ ζ, • p is the l p -norm,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• UPSET and ANGR method</head><p>Two block-box attack methods, UPSET and ANGR, were proposed by Sarkar et al <ref type="bibr" target="#b68">[69]</ref>. For a DL model with n classes, the UPSET method attempts to generate k image-unconvinced perturbations such that, when appending these perturbations to an image with an untargeted class, the network can still classify the distorted image as the corresponding target class. This process can be interpreted as the following optimization equation.</p><formula xml:id="formula_7">I µ = max (min (kN (i) + I c , 1) , 1)<label>(5)</label></formula><p>where N (i) denotes the perturbations generated by a residual generating network N with input data i. In contrast to the UPSET method, the ANGRI algorithm computes specific perturbations based on the original work. Both methods have been reported to achieve favorable performance for fooling the DL model on the CIFAR-10 and MNIST datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• C&amp;W attack method</head><p>Carlini and Wagner introduced a powerful adversarial attack, also known as the C&amp;W attack, which achieves impressive results on distilled and undistilled DNN models <ref type="bibr" target="#b69">[70]</ref>. These attacks apply three distance mea- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Defense strategy</head><p>The defense strategy under the adversarial setting includes four main parts:</p><p>1) the security evaluation mechanism for the ML model, 2) the defense strategy during the training phase, 3) the defense strategy during the prediction/test phase, and 4) the privacy protection method for the ML system. Moreover, Table .2 summarizes the main attributes of the defense methods presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Security evaluation mechanism</head><p>Traditional security evaluation methods primarily focus on the classification accuracy of the ML model, and they fail to consider the security of such model.</p><p>To address this problem, various studies have proposed a number of methods for evaluating the security and robustness of ML models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b103">104]</ref>. The motivation of the security evaluation is to simulate various realistic attack scenarios  <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102]</ref>, <ref type="bibr" target="#b102">[103]</ref> and highlight the most critical vulnerabilities by analyzing the influence of the adversarial attacks on a given ML model. Characterized by the defense pattern, the mechanisms underlying these evaluation methods can be categorized into reactive defense and proactive defense, as shown in Fig. <ref type="figure" target="#fig_8">8</ref>.</p><p>Reactive defense refers to an arms race in which both the ML model designer and the adversary are involved. For each iteration, the adversary first analyzes the ML defenses strategy and develops an attack approach against the model.</p><p>Then, the designer of the ML model responds to this action based on this proposed attack type. Finally, the designer adds features that can defend from the novel attack to update the ML system (Fig. <ref type="figure" target="#fig_8">8(a)</ref>). In contrast to reactive defense, the main agent of this arms race in proactive defense is the designer of the ML model. Before deploying the ML system, the designer develops the defense strategies by analyzing the possible deficiencies and threats to the ML model based on the adversarial capability, attack strategy, etc. (Fig. <ref type="figure" target="#fig_8">8(b)</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Defense strategy during training phase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Defense strategy on prediction/test phase</head><p>Defenses against adversarial attacks during the prediction/test phase follow two main directions:</p><p>(1) Modifying ML model, e.g. via transforming the training data/process, adjusting the network layers, and changing activation functions. These methods change the ML model parameters.</p><p>(2) For external models, these methods use additional modules to detect or suppress adversarial attacks, thereby keeping the ML model parameters unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Modifying ML model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Adversarial training method</head><p>Adversarial training was the first approach against adversarial attacks.</p><p>Since the discovery of adversarial attacks on ML models, there has been proposed to use Discrete Cosine Transform (DCT) compression to defend against universal perturbations <ref type="bibr" target="#b85">[86]</ref>. The main limitation of the data compression strategy is that a higher compression rate leads to a loss of ML classification accuracy, whereas a low compression rate cannot effectively counter adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Foveation-based method</head><p>Boix et al. demonstrated that the influence of adversarial attacks can be mitigated by a foveation mechanism in different image regions <ref type="bibr" target="#b86">[87]</ref>.</p><p>They observed that the DNN model is robust to scale and transformation changes over the original images enforced by the foveation approach, whereas this property does not generalize to adversarial patterns. As a result, the foveation can be used as an alternative solution for alleviating the impact of adversarial attacks. However, the effectiveness of the foveation against more powerful attacks has not been validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Gradient masking method</head><p>Gradient masking methods enhance ML robustness by modifying the gradients of the input data, loss/actication function, etc. For example, Lyu s are updating <ref type="bibr" target="#b88">[89]</ref>. Moveover, Nguyen and Sinha developed a gradient masking method to defend against C&amp;W attacks, in which the noise is appended to the network logit layer <ref type="bibr" target="#b89">[90]</ref>. In addition, Ross and Doshi-Velez analyzed the gradient regularization of inputs and then proposed a method for improving network robustness <ref type="bibr" target="#b90">[91]</ref>. Because small adversarial perturbations cannot drastically vary the outputs of the deployed model, this method trains the model by penalizing the input variation degree.</p><p>• Defensive distillation method distillation, to resist against adversarial perturbations used to attack DNN models <ref type="bibr" target="#b92">[93]</ref>. In contrast to the knowledge transferring pattern of the original distillation, the defensive distillation method extracts the knowledge from its own structures to enhance resistance to adversarial attacks. Fig. <ref type="figure" target="#fig_12">9</ref> demonstrates the framework of the defensive distillation method. As depicted in Fig. <ref type="figure" target="#fig_12">9</ref>, the DNN model first derives the knowledge based on its outputs, and then, it retrains the model using the obtained knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• DeepCloak method</head><p>The DeepCloak method enhances the robustness of the DNN model by removing useless features <ref type="bibr" target="#b93">[94]</ref>. The flowchart of DeepCloak is illustrated in Fig. <ref type="figure" target="#fig_0">10</ref>. First, this method inserts a masking layer before the network decision layer. Then, the added layer is trained using the original and adversarial image pairs. Finally, the feature differences of the decision layer and added layer are encoded. Because the most prominent features </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv Pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deepcloak Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction Classifier</head><p>Extracted features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fuuly conected Softmax</head><p>Figure <ref type="figure" target="#fig_0">10</ref>: The main concept of the DeepCloak technique <ref type="bibr" target="#b93">[94]</ref>.</p><p>have the dominant weights, the prominent features can be removed by masking the dominant weights for the added layer. Compared to the above-mentioned defense algorithms, DeepCloak method is more efficient because it eliminates the unnecessary features without retraining the model. as well as the contaminated images (Fig. <ref type="figure" target="#fig_14">11</ref>). Moreover, motivated by the  • Feature squeezing method Evans et al. presented a novel approach to detect malicious perturbations of an image via feature squeezing <ref type="bibr" target="#b96">[97]</ref>. This method first decreases the color depth of an image by external modules. Then, it compares the model classification accuracies obtained on the original images and on the compressed images. If there exist substantial differences between the two accuracies, the target image is regarded as an adversarial example. The framework of the feature squeezing method is depicted in Fig. <ref type="figure" target="#fig_15">12</ref>.</p><p>• Universal perturbation defense method The goal of a universal perturbation defense strategy is to rectify the contaminated inputs by a perturbation rectifying network (PRN) before the input layer. Depending upon the PRN, the rectified input data and the origin input data are characterized by the same distributions <ref type="bibr" target="#b97">[98]</ref>.</p><p>The PRN is a pre-input layer and is trained without having to change the parameters of the targeted network. Fig. <ref type="figure" target="#fig_16">13</ref> demonstrates the framework of the universal perturbation defense method based on the PRN. As depicted in Fig. <ref type="figure" target="#fig_16">13</ref>, during the data rectification process, this method first feeds the input images into the PRN, and then, it detects possible perturbations based on the output features of the PRN.</p><p>In summary, the existing defense measures are reported to be successful to resist the adversarial examples on ML prediction phase to some extend. Some approaches (e.g., adversarial training <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b81">82]</ref>, data compression <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref>,</p><p>defensive distillation <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b92">93]</ref>) validate they can defend against different kinds of adversarial attacks via carrying out extensive experiments. However, these methods are probably to decrease the predictive accuracy rate on the benign examples. Some approaches (e.g., <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>) can provide favorable results on a kind of specific adversarial attacks, whereas these methods are usually vulnerable to the same adversarial examples due to their transferability. Therefore, devising robust and universal defense strategies in the absence of reducing the performance of the origin ML model is a challenge task.  (2) Data privacy is important in ML security. Although the DP and HE techniques provide a powerful guarantee for ML methods, they remain inefficient because of the high computational complexity. Hence, it is worth designing effective and efficient encryption methods to ensure ML model privacy.</p><p>(3) The security evaluation of ML methods under adversarial settings should be highlighted in research. Because of the continuous emergence of new security threats, ML designers should place greater importance on the security evaluation of ML models. However, most evaluation methods are under exploration and discussion. Therefore, establishing additional integrity and authority evaluation mechanics is an urgent ploblem that must be solved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The environments whereby ML models are deployed. (a) represents the benign setting, and (b) represents the adversarial setting.</figDesc><graphic coords="4,139.63,397.85,315.30,123.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An adversarial example in the medicine imaging field<ref type="bibr" target="#b9">[10]</ref>. The adversarial example is formulated using the origin image and the carefully crafted perturbations such that a benign tumor is misclassified as a malignant tumor with 100% confidence.</figDesc><graphic coords="5,139.63,157.03,315.29,136.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The attack surface of ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>)( 3 )( 4 )Figure 5 :</head><label>345</label><figDesc>Figure 5: Adversarial capability [49].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 Figure 6 : 4 . 2 .</head><label>6642</label><figDesc>Figure 6: An illustration of a poisoning attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An adversarial example created by the FGSM method on ImageNet [67].</figDesc><graphic coords="15,130.03,157.03,350.33,134.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>ζ is a predefined constant. A smaller ζ leads to less perceivable perturbations in the images. Note that the perturbation computed by this algorithm can be generalized well to various networks, whereas Moosavi-Dezfooli et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>During the ML training phase, the adversary can deploy a poisoning attack to modify the statistical characteristics of the training data. To resist poisoning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A conceptual representation of reactive defense and proactive defense [105], where (a) is reactive defense and (b) is proactive defense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>a•</head><label></label><figDesc>general consensus that adversarial training can enhance the robustness against adversarial examples. In contrast to the general ML training process, adversarial training incorporates a hybrid dataset including adversarial examples and the original samples. In essence, adversarial training is a robust generalization method for the ML training phase [61, 81]. However, it is a non-adaptive strategy in the sense that the ML model is required to be trained on relevant adversarial examples to resist a particular type of attack. Moreover, a new study suggests that the adversarially trained model can be again attacked by effective adversarial examples[82]. Data compression method Ghahramani et al. observed that most widely used ML model datasets are comprised as JPG format images. They performed research with regard to the impact of JPG compression [83]. According to their studies, JPG compression can suppress the decrease in performance to a large extent in terms of classification accuracy under FGSM perturbations. Moreover, Guo et al. and Das et al. studied the image JPEG compression effect to mitigate the impact of adversarial images[84, 85]. These methods employ an ensemble-based technique to counter FGSM attacks by removing the high-frequency components from the images. In addition, Moosavi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>et al. proposed to mitigate against L-BFGS and FGSM attacks by penalizing the gradient of the loss function of the networks [88]. Shaham et al. attempted to increase the local stability of neural networks by minimizing the loss function over adversarial examples when the model parameter-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Hinton et al. observed that the knowledge of a large DDN model can be transferred to a small model by the distillation technique [92]. This means that distillation can decrease the dimensionality of the DNN. Motivated by this work, Papernot et al. formulated a distillation variant, defensive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An illustration of the defensive distillation strategy against adversarial attacks [92].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>445 5 . 3 . 2 .</head><label>532</label><figDesc>Appending external model • GAN-based defense method Goodfellow et al. first proposed an intriguing DL model, i.e., Generative Adversarial Networks (GANs), to enhance and semantically segment imagery [106]. Then, Lee et al. used this popular GAN platform to con-450 struct a robust model that resists FGSM attacks[95]. Based on the GAN training, the deployed model can successfully classify the original images,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The defense strategy based on GAN [106].</figDesc><graphic coords="24,157.14,332.04,280.26,96.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The framework of the feature squeezing method [97].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: The framework of the universal perturbation defense method<ref type="bibr" target="#b97">[98]</ref>.</figDesc><graphic coords="25,157.14,157.03,280.26,106.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>5. 4 .</head><label>4</label><figDesc>Privacy protection for ML modelTo construct an effective ML model, a large amount of training data is required. Generally, most such training data are collected using crowd sourcing techniques. Because these training data usually contain a large amount of private and sensitive information with regard to the users, ensuring training data privacy has become a crucial issue for ensuring the security of ML technologies. In addation, some investigations have demonstrated that the important parameters in the ML model may also be hidden. For instance, the adversary can perform an inversion attack against the ML system to obtain private data inside the ML model<ref type="bibr" target="#b106">[107]</ref>. To ensure the data privacy of the ML model, two popular defense strategies are usually employed, i.e., deferential privacy (DP)-based methods and homomorphic encryption (HE)-based methods. 5.4.1. Deferential-privacy-based methods Dwork et al. proposed the Deferential privacy (DP) technique in 2006 and then proved its robustness and security in theory[108]. The DP technique is widely accepted as a means of ensuring model privacy, therein aiming at obscuring the inputs by adding noise to the original data model. Based on DP, Erlingsson et al. proposed a randomized aggregative privacy-preserving ordinal response (RAPPOR) method to achieve strong security in a crowd sourcing process from a user terminal. This method combines a randomized response mechanic and DP technique to obtain a high performance. Recently, Papernot et al. proposed a promising approach, PATE (Private Aggregation of Teacher Ensembles)[99], to ensure the privacy of the ML model. This method constructs a teacher model by training some disjoint subsets of the data and then formulating a student model based on the outputs of the teacher system. Note that the training process of the student model depends on the prediction results of the teacher model, instead of the intrinsic parameters. Fig.14 presents an overview of PATE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 14 :( 1 )</head><label>141</label><figDesc>Figure 14: An overview of PATE algorithm [99].</figDesc><graphic coords="27,130.03,157.03,350.32,119.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The dataset training, feature extraction and</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Machine learning process</cell><cell></cell><cell></cell></row><row><cell>Data Collection</cell><cell>Training/Testing Dataset</cell><cell>Feature Extraction</cell><cell>Learning Algorithm</cell><cell>Detection Model</cell><cell>Retraining</cell></row><row><cell>Stealthy Channel</cell><cell>Mimicry Attack</cell><cell>Polymorphic Metamorphic</cell><cell>Gradient Descent Attack</cell><cell>Model Stealing</cell><cell>Poisoning Attack</cell></row><row><cell></cell><cell></cell><cell cols="2">Attack surface</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>The taxonomy of ML threaten model Causative attacks Exploratory attacks Integrity attacks Availability attacks Targeted attacks Model stealth attacks Indiscriminate attacks Attack influence Security violation Attack specification</head><label></label><figDesc></figDesc><table><row><cell>Figure 4: Taxonomy of threat model for ML.</cell></row><row><cell>) In the direction of the attack influence, the threat model of ML is di-</cell></row><row><cell>vided into causative attacks and exploratory attacks. Causative attacks</cell></row><row><cell>affect the ML model by controlling training data, and exploratory attacks</cell></row><row><cell>produce misclassified results but do not influence the training data.</cell></row><row><cell>(2) In the direction of security violations, the threat model of ML can be clas-</cell></row><row><cell>sified into integrity attacks, availability attacks and confidentiality attacks.</cell></row><row><cell>Integrity attacks compromise the model assets via false negatives. If the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The summary of the main attributes of different attacks in this paper.</figDesc><table><row><cell></cell><cell cols="4">Attacking phase Black/White box Learning method Adversarial goal</cell></row><row><cell>Poisioning</cell><cell>Training</cell><cell>White box</cell><cell>One shot</cell><cell>Integrity</cell></row><row><cell>L-BFGS</cell><cell>predicting</cell><cell>White box</cell><cell>One shot</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p>sarial examples that take the form of small perturbations of the input images to deceive the DNN model</p><ref type="bibr" target="#b7">[8]</ref></p>. Subsequently, other research teams have also developed numerous attack approaches against some popular DL-based systems, e.g., navigational systems</p><ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref></p>. Note that earlier work primarily concerned traditional learning methods, e.g., SVM, NB and DT, whereas most current work focuses on DL methods. Table</p>.</p>1 provides a summary of the main attributes of the attacks mentioned in this paper.</p>4.1. Adversarial attack during training phase</p>Some adversaries attempt to compromise the ML model by attacking the original data during the ML training phase. A poisoning attack is a typical type of attack that attempts to modify the statistical characteristics of the training dataset. A poisoning attack is regarded as a causative attack that breaks the integrity and availability of the ML model. In most cases, the original dataset used by the ML system is confidential and cannot be easily modified by attackers. However, in some applications (e.g., biometric facial recognition, malware detection and spam email filtering), the ML model is usually required to be retrained because the training dataset may degenerate as the environment changes. This fact provides an opportunity for the attacker to manipulate the ML training data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The summary of the main attributes of different defense methods in this paper.</figDesc><table><row><cell></cell><cell>Defense Strategies</cell><cell cols="2">Reactive/Proactive Representative Studies</cell></row><row><cell>Defense strategy</cell><cell>Data sanitization</cell><cell>Reactive</cell><cell>[77]</cell></row><row><cell>on training phase</cell><cell>Generalization enhancing</cell><cell>Proactive</cell><cell>[78, 79],[80]</cell></row><row><cell>Defense strategy on prediction phase</cell><cell>Adversarial training Data compression Foveation-based methods</cell><cell>Proactive Reactive Reactive</cell><cell>[61, 81],[82] [83],[84, 85],[86] [87]</cell></row><row><cell></cell><cell>Gradient masking</cell><cell>Reactive</cell><cell>[88],[89],[90],[91]</cell></row><row><cell></cell><cell>Defensive distillation</cell><cell>Proactive</cell><cell>[92],[93]</cell></row><row><cell></cell><cell>DeepCloak</cell><cell>Reactive</cell><cell>[94]</cell></row><row><cell></cell><cell>GAN-based methods</cell><cell>Proactive</cell><cell>[95],[96]</cell></row><row><cell></cell><cell>Feature squeezing</cell><cell>Reactive</cell><cell>[97]</cell></row><row><cell></cell><cell>Universal defense methods</cell><cell>Proactive</cell><cell>[98]</cell></row><row><cell>Privacy protection</cell><cell>DP-based methods</cell><cell>Proactive</cell><cell>[99]</cell></row><row><cell>strategy</cell><cell>HE-based methods</cell><cell>Proactive</cell><cell>[100],</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (Nos. 61472091 and 61370194), the Natural Science Foundation of Guangdong Province for Distinguished Young Scholars (No. 2014A030306020), the Guangzhou Scholars Project for Universities of Guangzhou (No. 1201561613), the Science and Technology Planning Project of Guangdong Province, China (No. 2015B010129015), the National Natural Science Foundation for Outstanding Youth Foundation (No. 61722203), the National Key R&amp;D Program of China (No. 2016YFN0800602), the Shandong Provincial Key R&amp;D Program of China (No. 2018CXGC0701) and JSPS KAKENHI Grant No. JP15K00028.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Availability FGSM predicting White box Iterative Availability UAP predicting White box Iterative Availability UPSET/ANGR predicting Black box Iterative Availability C&amp;W predicting White box Iterative Availability the model. Based on these facts, Szegedy et al. proposed to construct adver-</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rna splicing. the human splicing code reveals new insights into the genetic determinants of disease</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bretschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gueroussov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="issue">6218</biblScope>
			<biblScope unit="page">1254806</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards ultra-high performance and energy efficiency of deep learning systems: An algorithm-hardware co-optimization framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memristive boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Bojnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High PERFORMANCE Computer Architecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<title level="m">Adversarial Examples for Malware Detection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Practical black-box attacks against deep learning systems using adversarial examples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial attacks against medical deep learning systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05296</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Publicly verifiable privacypreserving aggregation and its application in iot</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="39" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Secure data uploading scheme for a smart home system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An authenticated asymmetric group key agreement based on attribute encryption</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A payloaddependent packet rearranging covert channel for mobile voip traffic</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page" from="162" to="173" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Malicious pdf detection using metadata and structural features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Smutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stavrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security Applications Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection of malicious pdf files based on hierarchical document structure</title>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Network &amp; Distributed System Security Symposium</title>
		<meeting>the 20th Annual Network &amp; Distributed System Security Symposium</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers: A case study on pdf malware classifiers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed System Security Symposium</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Significant permission identification for machine learning based android malware detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Srisa-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Badnets: Identifying vulnerabilities in the machine learning model supply chain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1708.06733</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<title level="m">Machine learning, a probabilistic perspective</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<title level="m">Support vector machines, IEEE Intelligent Systems and their applications</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<title level="m">Neural Network: A Comprehensive Foundation</title>
		<imprint>
			<publisher>Prentice Hall PTR</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Su</surname></persName>
		</author>
		<title level="m">Linear regression analysis</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object recognition using hierarchical temporal memory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fallas-Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Torres-Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Computing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attitude estimation based on arrays of passive rfid tags</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alvareznarciandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laviada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lasheras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Antennas &amp; Propagation</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Applications of speechto-text recognition and computer-aided translation for facilitating crosscultural learning through a learning activity: issues and their solutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shadiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Technology Research &amp; Development</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Robust physicalworld attacks on deep learning visual classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning for cell-level visual representation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical &amp; Health Informatics PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of htns in complex adversarial domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName><forename type="first">Junling</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wellman</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust adversarial reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adversarial deep reinforcement learning in portfolio management</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards the science of security and privacy in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wellman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The security of machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluation: From precision, recall and f-factor to roc, informedness, markedness &amp; correlation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2229" to="3981" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Preserving privacy with probabilistic indistinguishability in weighted social networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel &amp; Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1417" to="1429" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Effective query grouping strategy in clouds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Characterizing the limits and defenses of machine learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On attacking statistical spam filters</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Wittel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Email &amp; Anti-Spam</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Randomizing svm against adversarial attacks under uncertainty</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Is data clustering in adversarial settings secure?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM workshop on Artificial intelligence and security</title>
		<meeting>the 2013 ACM workshop on Artificial intelligence and security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Poisoning behavioral malware clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wressnegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 workshop on artificial intelligent and security workshop</title>
		<meeting>the 2014 workshop on artificial intelligent and security workshop</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sampling attack against active learning in adversarial environment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Modeling Decisions for Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Building packet length covert channel over mobile voip traffics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>-A. Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="144" to="153" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is data clustering in adversarial settings secure?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM workshop on Artificial intelligence and security</title>
		<meeting>the 2013 ACM workshop on Artificial intelligence and security</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A packetreordering covert channel over volte voice and video traffics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">M-sse: an effective searchable symmetric encryption with enhanced security for mobile devices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="38860" to="38869" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An identity-based anti-quantum privacy-preserving blind authentication in wireless sensor networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>-A. Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1663</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavidezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Vulnerabilities in biometric encryption systems, in: International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adversarial label flips attack on support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="870" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Upset and angri : Breaking high performance image classifiers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Security and Privacy</publisher>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cifar10-dvs: An event-stream dataset for object classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">309</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pixeldefend</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mdry</surname></persName>
		</author>
		<title level="m">Adversarially robust generalization requires more data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misleading</forename><surname>Learners</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Co-opting Your Spam Filter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for robust classifier design in adversarial environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning &amp; Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Bagging Classifiers for Fighting Poisoning Attacks in Adversarial Classification Tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Antidote:understanding and defending against poisoning of anomaly detectors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM Conference on Internet Measurement</title>
		<meeting><address><addrLine>Chicago, Illinois, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">2009. November, 2009</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Regularizing deep networks using efficient layerwise adversarial training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">A study of the effect of jpg compression on adversarial images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavidezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Foveation-based mechanisms alleviate adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A unified gradient regularization family for adversarial examples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="301" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Understanding adversarial training: Increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A learning and masking approach to secure learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshivelez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Masking deep neural network models for robustness against adversarial samples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><surname>Deepcloak</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Generative adversarial trainer: Defense to adversarial perturbations with gan</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ape-Gan</surname></persName>
		</author>
		<imprint>
			<publisher>Adversarial perturbation elimination with gan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed System Security Symposium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Defense against universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>lfar Erlingsson, Scalable private learning with pate</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">On data banks and privacy homomorphisms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Dertouzos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Secure Computation</title>
		<imprint>
			<biblScope unit="page" from="169" to="179" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Achieving reliable and secure services in cloud computing environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Dynamic access policy in cloud-based personal health record (phr) systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jie</surname></persName>
		</author>
		<idno>2016) S0020025516304571</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Wernsing, Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dowlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Radio and Wireless Symposium</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="76" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Security evaluation of pattern classifiers under attack</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pougetabadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial networks, Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Differential privacy</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Anonymizing popularity in online social networks with full utility</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno>S0167739X16301170</idno>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
