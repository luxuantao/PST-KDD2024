<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-10">10 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minhao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
							<email>xiangchs@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
							<email>jieyuz2@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">The Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<title level="a" type="main">TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-10">10 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511935</idno>
					<idno type="arXiv">arXiv:2202.04887v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Taxonomy Completion</term>
					<term>Self-supervised Learning</term>
					<term>Knowledge Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Taxonomies are fundamental to many real-world applications in various domains, serving as structural representations of knowledge. To deal with the increasing volume of new concepts needed to be organized as taxonomies, researchers turn to automatically completion of an existing taxonomy with new concepts. In this paper, we propose TaxoEnrich, a new taxonomy completion framework, which effectively leverages both semantic features and structural information in the existing taxonomy and offers a better representation of candidate position to boost the performance of taxonomy completion. Specifically, TaxoEnrich consists of four components:</p><p>(1) taxonomy-contextualized embedding which incorporates both semantic meanings of concept and taxonomic relations based on powerful pretrained language models; ( <ref type="formula">2</ref>) a taxonomy-aware sequential encoder which learns candidate position representations by encoding the structural information of taxonomy; (3) a query-aware sibling encoder which adaptively aggregates candidate siblings to augment candidate position representations based on their importance to the query-position matching; (4) a query-position matching model which extends existing work with our new candidate position representations. Extensive experiments on four large real-world datasets from different domains show that TaxoEnrich achieves the best performance among all evaluation metrics and outperforms previous state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Information extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Taxonomies have been widely used for organizing concepts structurally <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, to capture the "is-a" relationship between concept pairs, people often formulate the taxonomy into tree or directed acyclic graph (DAG) structure. Example applications could be found in e-commerce, where Amazon leverages product taxonomies for personalized recommendations and product navigation, and fine-grained named entity recognition where people rely on concept taxonomies (e.g., MeSH) <ref type="bibr" target="#b8">[9]</ref> to extract and label useful information from massive corpus.</p><p>However, the construction of taxonomies usually requires a substantial amount of human curation. Such process is time-consuming and labor-intensive. Thus, it is extremely hard to handle the large number of emerging new concepts in downstream tasks, which is fairly common nowadays with the rising tide of big data. To tackle this issue, recent work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> turns to the tasks of the automatic expansion and completion of the existing taxonomy. Previous taxonomy construction methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> construct taxonomies from scratch, and highly rely on annotated hypernym pairs, which are expensive and sometimes inaccessible in practice. Therefore, automatic taxonomy expansion based on existing taxonomies is in great need and has gained increasing attention.</p><p>The recent studies on taxonomy expansion and completion achieved noticeable progress, which mainly contribute from two directions. (1) extract hierarchical information from the existing taxonomy, and utilize different ways to model the structural information in the existing taxonomy, such as local egonet <ref type="bibr" target="#b18">[19]</ref>, parentquery-child triplet <ref type="bibr" target="#b33">[34]</ref>, and mini-paths <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b1">(2)</ref> leverage the supporting corpus to generate the embeddings of concepts directly. They either only used implicit relational semantics <ref type="bibr" target="#b11">[12]</ref>, or only relied on corpus to construct limited seed-guided taxonomy <ref type="bibr" target="#b4">[5]</ref>. Very recently, <ref type="bibr" target="#b30">[31]</ref> combines the representations from semantic sentences and local-subgraph encoding as the features of concepts. However, they only utilized light-weight multi-layer perceptron (MLP) for matching, which suffers from the limited representation power. In this paper, we follow <ref type="bibr" target="#b33">[34]</ref> to focus on taxonomy completion, which aims to predict the most likely ⟨query, hypernym, hyponym⟩ triplet for a given query concept. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, when considering the query "Integrated Circuit", we aim to find its true parent "Hardware" and child "GPU ".</p><p>To effectively leverage both semantic and structural information for better taxonomy completion performance, in this work, we propose TaxoEnrich, which aims to learn better representations for each candidate position and render new state-of-the-art taxonomy completion performance. Specifically, TaxoEnrich consists of four carefully-designed components. First, we propose a taxonomycontextualized embedding generation process based on pseudo sentences extracted from existing taxonomy. The two types of pseudo sentences, i.e., ancestral and descendant pseudo sentences, capture taxonomic relations from two directions respectively. Then, the powerful pretrained language models are utilized to produce the taxonomy-contextualized embedding based on the extracted sentences. Secondly, to encode the structural information of the existing taxonomy in both vertical and horizontal views, we develop two novel encoders: a sequential feature encoder based on the pseudo sentences and a query-aware sibling encoder base on the importance of candidate siblings to the matching task. The former aims to learn a taxonomy-aware candidate position representations, while the latter further augments the position representations with adaptively aggregated candidate siblings information. Finally, we develop an effective query-position matching model by extending previous work <ref type="bibr" target="#b33">[34]</ref> to incorporate our novel candidate position representations. Specifically, it takes into consideration both finegrained (query to candidate parent) and coarse-grained (query to candidate position) relatedness for better taxonomy completion performance.</p><p>We conducted extensive experiments on four real-world taxonomies from different domains to test the performance of Tax-oEnrich framework. Further more, we designed two variations of the framework, TaxoEnrich and TaxoEnrich-S to conduct ablation experiments to explore the utilization of different information under different datasets, along with studies to examine the effectiveness of each sub-module of the framework. Our results show that TaxoEnrich can more accurately capture the correct positions of query nodes than previous methods and achieve state-of-the-art performance on both taxonomy completion and expansion tasks.</p><p>To summarize, our major contributions include:</p><p>• We propose an effective embedding generation approach which can be applied generally for learning contextualized embedding for each concept node in a given taxonomy. • We introduce the sequential feature encoders to capture vertical structural information of candidate positions in the taxonomy. • We design an effective query-aware sibling encoder to incorporate horizontal structural information in the taxonomy.</p><p>• Extensive experiments demonstrate that our developed framework enhances the performance on both taxonomy completion and expansion task by a large margin over the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>In this section, we formally define the taxonomy completion task studied in the paper. Taxonomy. Follow <ref type="bibr" target="#b18">[19]</ref>, we formulate a taxonomy T 0 = {N 0 , E 0 } as a directed acyclic graph where each node 𝑛 ∈ N 0 represents a concept and each directed edge ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩ ∈ E 0 represents a taxonomic relationship between two concepts. Taxonomy Completion. The taxonomy completion task <ref type="bibr" target="#b33">[34]</ref> is defined as following: given an existing taxonomy T 0 and a set of new concepts C, assuming that each concept in C is in the same semantic domain as T 0 , we aim to automatically find the most possible ⟨ hypernym, hyponym⟩ pairs for each new concept to complete the taxonomy. The output is T = {N, E ′ } where N = N 0 ∪ C and E ′ is the updated edges set after inserting new concepts. Candidate Positions. In the taxonomy completion task, we define a valid candidate position as a pair of concept nodes in the existing taxonomy ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩ where 𝑛 𝑝 is a parent of 𝑛 𝑐 . Note that one of 𝑛 𝑝 and 𝑛 𝑐 could be a pseudo placeholder node in case that the concepts needed to be inserted as root or leaf nodes. The goal of taxonomy completion is to enrich the existing taxonomy by inserting new concepts. These new concepts are generally extracted from text corpus using entity extraction tools. Since this process is not the focus of the paper, we assume that the set of new concepts C is given, as well as their embedding, which is denoted by 𝑒 𝑞 for new concept 𝑛 𝑞 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE TAXOENRICH FRAMEWORK</head><p>In this section, we introduce the TaxoEnrich framework in details. We first introduce the taxonomy-contextualized embedding generation for each concept node in the existing taxonomy. Then, given the extracted taxonomy-contextualized embedding, we develop two encoders to learn the representation of candidate positions from vertical and horizontal views of the taxonomy respectively. Finally, we propose a query-to-position matching model which leverages various structural information and takes into consideration both fine-and coarse-grained relatedness to boost the matching performance. The overall framework of TaxoEnrich is in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Taxonomy-Contextualized Embedding</head><p>Here, we describe the generation process of taxonomy-contextualized embedding for each node in the taxonomy. Different from prior work which leverages static word embedding, such as Word2Vec and FastText <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, or contextualized embedding solely based on an additional text corpus <ref type="bibr" target="#b29">[30]</ref>, we generate taxonomy-contextualized embedding based on taxonomy structure and concept surface name. The reason is that neighboring concepts in taxonomy are likely to share similar semantic meaning and it is hard to distinguish them based on predefined general-purpose embedding. With similar spirit, <ref type="bibr" target="#b30">[31]</ref> also leverage pretrained language models to produce contextualized embedding based on limited number of taxonomy neighbor and the surface names is implicitly utilized in fine-tuning the pretrained language models. In contrast, we aim to fuse the information of all the descendant/ancestral concepts of the given concept, without fine-tuning a huge pretrained language model. Specifically, given a concept node, we build pseudo sentences based on Hearst patterns <ref type="bibr" target="#b17">[18]</ref> to represent both the positional and semantic information. We separately consider the descendant/ancestral information by constructing descendant/ancestral pseudo sentences respectively as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Formally, given a taxonomy T 0 = (V 0 , E 0 ) and the candidate concept node 𝑣, we extract the following two types of pseudo sentences that represent taxonomic relationships: (1) Ancestral Pseudo Sentences: We first extract paths connecting root and the candidate node 𝑣 without duplicate nodes.</p><p>In the extracted path (𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑙 ), 𝑝 1 is the root node and 𝑝 𝑙 = 𝑣. We denote the 𝑖-th extracted path for node 𝑣 as 𝑃 𝑎 (𝑣) 𝑖 = (𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑙−1 ). Along each path, we generate the ancestral pseudo sentence as below:</p><p>"𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑙−1 is a superclass of 𝑣" or "𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑙−1 is an ascendant of 𝑣" All such words like "superclass" or "ascendant" that can represent the hierarchical relationship between the path and candidate nodes can be used for sentence generation. We denote the collection of such ancestral paths as 𝑃 𝑎 (𝑣) and the generated sentences as 𝑆 𝑎 (𝑣) (2) Descendant Pseudo Sentences: Similarly, the paths without duplicated nodes starting from the candidate node 𝑣 to leaf nodes are extracted, denoted as (𝑝 1 , 𝑝 2 , . . . , 𝑝 𝑙 ) where 𝑝 1 = 𝑣 and 𝑝 𝑙 is a leaf node. In this case, along each path, we generate the sentence as below "𝑝 2 , 𝑝 3 , . . . , 𝑝 𝑙 is a subclass of 𝑣" or "𝑝 2 , 𝑝 3 , . . . , 𝑝 𝑙 is a descendant of 𝑣"</p><p>We denote the collection of such descendants paths as 𝑃 𝑑 (𝑣) and the generated sentences as 𝑆 𝑑 (𝑣)</p><p>Then, the set of all the generated pseudo sentences is 𝑆 (𝑣) = 𝑆 𝑑 (𝑣) ∪ 𝑆 𝑎 (𝑣). As visualized in Figure <ref type="figure" target="#fig_2">3</ref>, aiming to generate embeddings of the concept node "Disk", we only consider the ancestral and descendant paths, such as "Electronic Devices, Smart Phone is a superclass of Disk". Note that if the candidate node is leaf node or root, we will only consider one side pseudo sentences. Given the generated pseudo sentences, we apply a pretrained language models to generate taxonomy-contextualized embedding for each node. Specifically, we feed the pseudo sentences to the pretrained language model and collect the last hidden state representations of the concept node 𝑣, which is averaged as the final taxonomy-contextualized embedding x 𝑣 ∈ R 𝑑 . In our preliminary experiments, we found that SciBERT is better than other models in representing concept representations.</p><p>Hence, in this paper, we choose SciBERT <ref type="bibr" target="#b1">[2]</ref> following <ref type="bibr" target="#b30">[31]</ref>. And the comparison between different pre-trained language models in terms of performance are discussed in 7. Note that the taxonomycontextualized embeddings are pre-computed and fixed for the following modules, which means we do not fine-tune the large pretrained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequential Feature Encoder</head><p>Given the taxonomy-contextualized embedding x 𝑣 for existing node 𝑣, we develop a learnable sequential feature encoder 𝑔(𝑣) to encode the structural information of candidate positions in a vertical view of taxonomy. For a candidate position ⟨𝑝, 𝑐⟩ consisting of candidate parent 𝑝 and child 𝑐, we produce parent embedding 𝑔(𝑝) and child embedding 𝑔(𝑐) respectively. Specifically, for candidate parent 𝑝 and its corresponding ancestral paths 𝑃 𝑎 (𝑝), we randomly sample a path 𝑝 𝑝 from 𝑃 𝑎 (𝑝) and apply a LSTM sequential encoder which inputs the sampled pseudo sentence and the taxonomy-contextualized embedding. Then, we concatenate the final hidden state of the LSTM encoder and the taxonomy-contextualized embedding x 𝑝 as parent embedding. Formally,</p><formula xml:id="formula_0">𝑔(𝑝) = x 𝑝 ⊕ LSTM(𝑝 𝑝 ; Θ 1 )<label>(1)</label></formula><p>where Θ 1 is the learnable parameters of the LSTM encoder and ⊕ represents the concatenation operation. Similarly, we generate the child embedding 𝑔(𝑐) based on taxonomy-contextualized embedding x 𝑐 and descendant paths 𝑝 𝑐 from 𝑃 𝑑 (𝑐):</p><formula xml:id="formula_1">𝑔(𝑐) = x 𝑐 ⊕ LSTM(𝑝 𝑐 ; Θ 2 )<label>(2)</label></formula><p>where, Θ 2 represents the learnable parameters of the LSTM encoder. The output 𝑔(𝑢), 𝑔(𝑣) ∈ R ℎ will be used as the embedding for candidate position nodes. Through this sequential feature encoder, we are able to fuse the structural information of candidate position in a vertical view. This allows the candidate position representations to be aware of the "depth" information of the candidate position, i.e., whether the candidate position is in the top-level of taxonomy close to the root or in the bottom-level close to leave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query-Aware Siblings Encoder</head><p>The aforementioned sequential feature encoder incorporates the taxonomy structural information in a vertical view, however, it is of great importance to also encode the horizontal local information of the candidate position. Thus, we develop another encoder to incorporate the structural information in a horizontal view. Specifically, in addition to the candidate parent and child, we consider the candidate siblings, i.e., the children of candidate parent, of the query node.</p><p>However, incorporating candidate siblings is challenging than candidate parent and child. The reasons are twofold. First, compared to the candidate parent and child which compose the candidate position, candidate siblings could introduce noisy information and thus lead to sub-optimal results. For example, for the top-level of taxonomy, the candidate siblings could have quite diverse semantic meanings, which hinder good matching between candidate position and query node. Secondly, since some candidate parent could have substantial amount of children (candidate siblings), it is infeasible to incorporate all the candidate siblings without strategic selection.</p><p>To tackle these issues, we develop a query-aware siblings encoder, which adaptively selects part of the candidate siblings. Specifically, we measure the relatedness of a given query embedding 𝑒 𝑞 and each candidate sibling condition on the representation of candidate parent-child pair. Such relatedness is in turn used to aggregate the sibling information into a single siblings embedding. Mathematically, given candidate position ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩ with corresponding embedding 𝑔(𝑝), 𝑔(𝑐) and the set of candidate siblings 𝐶 (𝑛 𝑝 ) = {𝑠 1 , 𝑠 2 , . . . , 𝑠 𝑡 }, we use a learnable bilinear matrix W Sib ∈ R 𝑑×(2ℎ+𝑑)  to calculate the relatedness of query and candidate sibling 𝑠 𝑖 as</p><formula xml:id="formula_2">𝜙 𝑠 𝑖 = 𝑒 𝑇 𝑞 W Sib 𝑔(𝑝), 𝑔(𝑐), x 𝑠 𝑖<label>(3)</label></formula><p>Then the relatedness score 𝜙 𝑠 𝑖 is normalized over the set of candidate siblings by a softmax function:</p><formula xml:id="formula_3">𝛼 𝑠 𝑖 = 𝜎 softmax (𝜙 𝑠 𝑖 ) = exp(𝜙 𝑠 𝑖 ) 𝑠 𝑗 ∈𝐶 (𝑛 𝑝 ) exp(𝜙 𝑠 𝑗 ) (4)</formula><p>The normalized score 𝛼 𝑠 𝑖 captures the importance of candidate sibling 𝑠 𝑖 for the specific query-position matching. In other words, it highlights the siblings relevant to the query condition on the candidate position while lessen the effect of irrelevant siblings. Finally, the sibling embeddings are aggregate based on the normalized score as</p><formula xml:id="formula_4">𝑎(𝑝) = ∑︁ 𝑠 𝑖 ∈𝐶 (𝑛 𝑝 ) 𝛼 𝑠 𝑖 x 𝑠 𝑖<label>(5)</label></formula><p>where 𝑎(𝑝) ∈ R 𝑑 . During experiments, we found that such a query-aware siblings encoder renders good performance when only a subset of siblings are considered, which alleviates the heavy burden of aggregate over the potentially large amount of candidate siblings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query-Position Matching Model</head><p>Finally, given the representation of candidate parent 𝑔(𝑝), child 𝑔(𝑐) and siblings 𝑎(𝑝) as well as the given query embedding 𝑒 𝑞 , we are ready to present our final matching module, which outputs the matching score of query and candidate position for taxonomy completion task. In particular, we seek to learn a matching model 𝑠 that outputs the desired relatedness score:</p><formula xml:id="formula_5">𝑠 (𝑛 𝑞 , ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩) = 𝑓 (𝑒 𝑞 , 𝑔(𝑝), 𝑔(𝑐), 𝑎(𝑝))<label>(6)</label></formula><p>where 𝑓 is a parametrized scoring function. The previous study <ref type="bibr" target="#b33">[34]</ref> showed that the simple matching model that learns one-to-one relatedness between the query node and the position pair ignores fine-grained relatedness between query and position component, i.e., the relatedness between ⟨𝑛 𝑞 , 𝑛 𝑝 ⟩ and ⟨𝑛 𝑞 , 𝑛 𝑐 ⟩. Therefore, inspired by <ref type="bibr" target="#b33">[34]</ref>, we propose a new matching model which incorporates the additional siblings embedding and learn more precise matching based on both fine-grained (query to candidate parent/child/siblings) and coarse-grained relatedness (query to position).</p><p>To learn both the fine-grained and coarse-grained relatedness between the query node and the candidate positions, we construct multiple auxiliary scorers that separately focus on the relationship between the query node and the candidate parent, the candidate child, the candidate siblings and the candidate position, respectively. We adopt the Neural Tensor Network (NTN) <ref type="bibr" target="#b21">[22]</ref> as the base models. Given vectors 𝑢 ∈ R 𝑑 𝑢 , 𝑣 ∈ R 𝑑 𝑣 , an NTN can be defined as NTN(𝑢, 𝑣) = w 𝑇 𝜎 tanh (ℎ(𝑢, 𝑣)) </p><formula xml:id="formula_7">ℎ(𝑢, 𝑣) = 𝑢 𝑇 W [1:𝑘 ] 𝑣 + V 𝑢 𝑣 + b<label>(8)</label></formula><p>where 𝜎 tanh is a tanh function and</p><formula xml:id="formula_8">w ∈ R 𝑘 , W [1:𝑘 ] ∈ R 𝑑 𝑢 ×𝑑 𝑣 ×𝑘 , V ∈ R 𝑘×(𝑑 𝑢 +𝑑 𝑣</formula><p>) and b ∈ R 𝑘 are learnable parameters. Note that 𝑘 is a hyperparameter in NTN.</p><p>Then our multiple scorer can be defined as</p><formula xml:id="formula_9">𝑆 1 (𝑛 𝑞 , 𝑛 𝑝 ) = w 𝑇 1 𝜎 tanh (ℎ 1 (𝑒 𝑞 , 𝑔(𝑝)))<label>(9)</label></formula><formula xml:id="formula_10">𝑆 2 (𝑛 𝑞 , 𝑛 𝑐 ) = w 𝑇 2 𝜎 tanh (ℎ 2 (𝑒 𝑞 , 𝑔(𝑐)))<label>(10)</label></formula><formula xml:id="formula_11">𝑆 3 (𝑛 𝑞 , 𝐶 (𝑛 𝑝 )) = w 𝑇 3 𝜎 tanh (ℎ 3 (𝑒 𝑞 , 𝑎(𝑝)))<label>(11)</label></formula><formula xml:id="formula_12">𝑆 4 (𝑛 𝑞 , ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩) = w 𝑇 4 𝜎 tanh (ℎ 4 (𝑒 𝑞 , 𝑔(𝑝), 𝑔(𝑐), 𝑎(𝑝) ))<label>(12)</label></formula><p>We omit the learnable parameters inside each ℎ for notation convenience. In this formulation, 𝑆 1 , 𝑆 2 , 𝑆 3 aim to learn the fine-grained relatedness for ⟨𝑛 𝑞 , 𝑛 𝑝 ⟩, ⟨𝑛 𝑞 , 𝑛 𝑐 ⟩, ⟨𝑛 𝑞 , 𝐶 (𝑛 𝑝 )⟩ separately by predicting whether the 𝑛 𝑝 , 𝑛 𝑐 , and 𝐶 (𝑛 𝑝 ) is the reasonable parent, child, and siblings, respectively. Differently, 𝑆 4 is designed for coarsegrained relatedness between the query node and the candidate position. Eventually we construct a primal scorer which incorporates the all the auxiliary scorers.</p><formula xml:id="formula_13">𝑆 𝑝 (𝑛 𝑞 , ⟨𝑛 𝑝 , 𝑛 𝑐 ⟩) = u 𝑇 𝑝 𝜎 tanh ( ℎ 1 , ℎ 2 , ℎ 3 , ℎ 4 ))<label>(13)</label></formula><p>We omit the input of each function ℎ for simplicity. In this case, even though 𝑆 𝑝 and 𝑆 4 share the same supervision signal, the concatenation of internal representations of other auxiliary scorers in 𝑆 𝑝 will allow it to capture accurate matching information based on 𝑆 1 , 𝑆 2 , 𝑆 3 when 𝑆 4 cannot learn correct coarse-grained relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Learning Objectives.</head><p>For each auxiliary scorers, since the model is trained for binary classification task to calculate the relatedness between the query node and the target objective, we adopt the binary cross-entropy loss. Thus, the learning objective for each scorer can be formulated as</p><formula xml:id="formula_14">L 𝑘 = − 1 |D| ∑︁ (X 𝑖 ,𝑦 𝑖 ) ∈D 𝑦 𝑖 •log(𝑆 𝑖 (X 𝑖 )) + (1−𝑦 𝑖 ) •log(1−𝑆 𝑖 (X 𝑖 )) (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>where D is the dataset formulated by the self-supervised generation following similar methods proposed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, and (X 𝑖 , 𝑦 𝑖 ) is the generated data pair in the dataset, and 𝑘 ∈ {𝑝, 1, 2, 3, 4} represents each scorer. In this case, the final learning objective L (Θ) that focuses on the primal task will naturally be defined as</p><formula xml:id="formula_16">L (Θ) = L 𝑝 + 𝜆 1 L 1 + 𝜆 2 L 2 + 𝜆 3 L 3 + 𝜆 4 L 4<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experiment Setup</head><p>Dataset. We evaluate the performance of TaxoEnrich framework on the following four real-world large-scale datasets. The statistics of each dataset are listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>• Microsoft Academic Graph (MAG): This public Field-of-Study (FoS) taxonomy contains over 660 thousand scientific concepts and more than 700 thousand taxonomic relations. We follow the data preprocessing in <ref type="bibr" target="#b18">[19]</ref> to only select partial taxonomies under the computer science (MAG-CS) and psychology (MAG-PSY) domain <ref type="bibr" target="#b20">[21]</ref>. • WordNet: We collect the concepts and taxonomic relations from verbs and nouns sub-taxonomies based on WordNet 3.0 (WordNet-Noun, WordNet-Verb). These two sub-fields are the only parts that have fully-developed taxonomies in WordNet. In practice, due to the scarcity in the dataset, i.e. there are many disconnected components in the both taxonomies, we added a pseudo root named "Noun" and "Verb" and connect this root to the head of each connected components in the taxonomies for generate a more complete taxonomic structure.</p><p>Follow the dataset splitting settings used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, we sample 1000 nodes for validation and test respectively in each dataset. Then we use the remaining nodes to construct the initial taxonomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>To fully understand the performance of our framework, we compare our model with the following methods.</p><p>• Bilinear Model <ref type="bibr" target="#b23">[24]</ref> incorporates the interaction of two concept embeddings, i.e., ⟨parent, child⟩ entity pair embeddings, through a simple bilinear form. This method serves as a baseline result to check the comparable performance of each framework. • TaxoExpan <ref type="bibr" target="#b18">[19]</ref> is a state-of-the-art taxonomy expansion framework, which leverages the positional-enhanced graph neural network to capture the relationship between query nodes and local egonet, along with InfoNCE loss <ref type="bibr" target="#b15">[16]</ref> to increase the robustness of the model. • ARBORIST <ref type="bibr" target="#b11">[12]</ref> is a state-of-the-art taxonomy expansion framework which aims for taxonomies with heterogeneous edge semantics and optimizes a large margin ranking loss with a dynamic margin function.</p><p>• TMN <ref type="bibr" target="#b33">[34]</ref> is a state-of-the-art taxonomy completion framework and also the first framework that proposed the completion task, and computed the matching score between the query concept and ⟨ hypernym, hyponym⟩ pairs. • GenTaxo <ref type="bibr" target="#b30">[31]</ref> is a state-of-the-art taxonomy completion framework using both sentence-based and subgraph-based encodings of the nodes to perform the matching. Since part of the framework concentrates on concept name generation tasks, which is not the focus of this paper, we adopt the GenTaxo++ assuming the newly added nodes are given. <ref type="foot" target="#foot_0">1</ref>We also include two variants of TaxoEnrich in experiments for ablation study: • TaxoEnrich-S: In this version, we exclude the sibling information from the matching model, since in sparse taxonomies, such as WordNet, the siblings cannot represent the precise candidate positions, and might still introduce noisy information when computing the relateness between query node and candidate position. • TaxoEnrich: In this version, we adopt the full framework of TaxoEnrich as described above. We will examine the difference between two variants through further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Since the result from the model's output is a ranking list of candidate positions for each query node, following the guidelines in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, we utilize the following rank-based metrics to evaluate the performance our framework and the comparison methods.</p><p>• Mean Rank (MR). This metric measures the average rank position of a query concept's true position among all candidate positions. For queries with multiple correct positions, we first calculate the rank position of each individual triplet and then take the average of all rank positions. Smaller value in this metric indicates the better performance of the model. • Mean Reciprocal Rank (MRR). We follow <ref type="bibr" target="#b28">[29]</ref> to compute the reciprocal rank of a query concept's true positions using a scaled MRR. In the evaluation, we scale the MRR by 10 to enlarge the difference between different models clearly. • Recall@𝑘 measures the number of query concepts' true positions ranked in the top 𝑘, divided by the total number of true positions of all query concepts. • Precision@𝑘 measures the number of query concepts' true positions ranked in the top 𝑘, divided by the total number of queries times 𝑘.</p><p>For all the evaluation metrics listed above except for MR, the larger value indicates better performance of the model. During the evaluation, since MR and MRR are the only metrics that concentrates on the performance of all predictions in the taxonomy in general, we consider them as the most important metric for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section we will first discuss the experiment results on both taxonomy completion and expansion tasks which demonstrated the superiority of our TaxoEnrich method. Then to further understand the contributions from each of our model design, we conduct ablation studies. Finally we performed case studies to further illustrate the effectiveness of TaxoEnrich.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance on Taxonomy Completion</head><p>The overall performance of compared methods and the proposed framework is indicated in Table <ref type="table" target="#tab_1">2</ref>. First, we can see that the performance of the framework tends to become better when the complexity of local structure increases, from the one-to-one matching in TaxoExpan to triplet in TMN , and the neighboring paths and subgraph encoding in GenTaxo. Second, we can generally observe the power of pre-trained language models in the representations of concept nodes in the taxonomy. The frameworks including GenTaxo and TaxoEnrich utilizing language models have generally better performance in the precision@𝑘 and recall@𝑘 metrics.</p><p>In terms of MR , we can see that TaxoEnrich obtained most performance improvement in MAG-CS dataset since the computer science taxonomy has the most complete taxonomic structure compared with other datasets, allowing for more accurate taxonomycontextualized embeddings generated by Section 3.1. And in Word-Net datasets the performance in MR metric is improved by a relatively large margin while all frameworks do not perform as well as in MAG datasets. In terms of precision@𝑘 and recall@𝑘, our method also shows noticeable improvement over baseline models.</p><p>In the previous methods, the static embedding method failed to capture the similar semantic meaning between different concept nodes. And we can see GenTaxo renders competing performance on these two metrics, but tends to be unstable and perform not well in ranking metrics. The primary reason for this observation is that while the language-based embeddings can provide pretty accurate positional information, its light-weight MLP matching module prevents it from capturing useful relatedness between query node and candidate position.</p><p>For two WordNet datasets, we can see that other frameworks are inclined to have similarly poor performance due to the scarcity of taxonomies. The non-connectivity causes the matching module difficult to extract the relations during the training. Therefore, the manually added pseudo root for sentence generation would maintain the taxonomic structure information in the representations of concept node and candidate positions, allowing the framework to capture both the structure and semantic information of each node.</p><p>In the comparison between TaxoEnrich-S and TaxoEnrich, we can observe that, in two MAG datasets, the incorporation of sibling information in TaxoEnrich would have better performance. However, it will also cause a drop in MR metric except for MAG-PSY and WordNet-Noun datasets since the randomly extracted siblings will still introduce noisy information in the matching module. In the WordNet datasets, the performance of two methods are very similar. This is because with the scarcity in the taxonomies, i.e., the lack of siblings, will mislead the model to incorporate inaccurate sibling information, causing a clear difference for MR metric. On the other hand, the precision in TaxoEnrich is still better than TaxoEnrich-S, which illustrates the effectiveness of siblings in representing the positional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on Taxonomy Expansion</head><p>Taxonomy expansion is a special case of the taxonomy completion task where new concepts are all leaf nodes. In this case, we would like to further explore the performance of TaxoEnrich on taxonomy expansion task on MAG-CS and WordNet-Verb dataset, compared with TaxoExpan, TMN, and GenTaxo framework. As indicated in Table <ref type="table" target="#tab_2">3</ref>, we can also observe that TaxoEnrich outperforms other methods by a large margin in all metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>In this section, we conduct the ablation studies on the major components of TaxoEnrich framework: 1) Incorporation of sibling information separated from embedding generation; 2) The implementations of different feature encoder models to capture different structural information. Note that, in the ablation study experiment, we used TaxoEnrich-S model for more direct and simpler comparison between the embeddings and modules. Additional ablation studies about hyperparameters are presented in appendix. 5.3.1 The Effectiveness of Query-Aware Sibling Encoder. In this section, we further discuss the effectiveness of approaches of incorporating sibling information in our framework. We argue that simple including sibling information in embeddings would actually introduce noisy information to the framework. In many cases, some high-level concepts, such as "Artificial Intelligence" or "Machine Learning" in MAG-CS taxonomy, have thousands of children. Therefore, it is unrealistic to consider all siblings, or unreasonable to randomly consider some of them. Thus, we conduct experiments to verify this assumption by randomly selecting at most 5 siblings in the process of embedding generation. However, as shown in Table <ref type="table" target="#tab_3">4</ref>, such operation would not only prevent the framework from recognizing correct positions, but increase the embedding generation time to the three times of the original in the implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Different</head><p>Feature Encoders in TaxoEnrich . We will continue to examine the superior performance of TaxoEnrich with different feature encoders and the effectiveness of such methods on other frameworks. The techniques of encoding features before matching have been experimented by previous methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>, showing that the neighboring terms of the candidate position would better utilize the structural information. We implement different feature encoding: the raw embeddings, LSTM encoding, PGAT encoding, and the combinations of these three, i.e., concatenating the output of encoders as input for matching module, for comparison in this section. Through experiments in Table <ref type="table" target="#tab_4">5</ref>, we can see that, the encoded features will improve the performance of all frameworks by a large margin, and TaxoEnrich can still outperform other methods regardless of encoded embeddings of concept nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Studies</head><p>We demonstrate the effectiveness of TaxoEnrich framework by predicting true positions of several query concepts in MAG-CS datasets in Table <ref type="table">6</ref>. For high-level concepts like "heap", TaxoEnrich ranks all the true positions at top 5, while TMN can only identify part of the true positions' information, like ("algorithm, leaf ") for "heap". And for those leaf nodes, such as "all pairs testing" and "sensor hub", we can observe that TMN will have much better performance. However, it will include some coarse high-level concepts such as "machine learning" and "artificial intelligence". In general, we can see TaxoEnrich works better than baselines for recovering true positions, and the top predictions by TaxoEnrich generally follow reasonable consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Automatic taxonomy construction is a long-standing task in the literature. Existing taxonomy construction methods leverage lexical features from the resource corpus such as lexical-patterns <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> or distributional representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref> to construct a taxonomy from scratch. However, in many real-world applications, some existing taxonomies may have already been laboriously curated and are deployed in online systems, which calls for solutions to the taxonomy expansion problem. To this end, multitudinous methods have been proposed recently to solve the taxonomy expansion problem <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. For example, ARBORIST <ref type="bibr" target="#b11">[12]</ref> studies expanding taxonomies by jointly learning latent representations for edge semantics and taxonomy concepts; TaxoExpan <ref type="bibr" target="#b18">[19]</ref> proposes position-enhanced graph neural networks to encode the relative position of terms and a robust InfoNCE loss <ref type="bibr" target="#b15">[16]</ref>; STEAM <ref type="bibr" target="#b29">[30]</ref> re-formulates the taxonomy expansion task as a mini-path-based prediction task and proposes to solve it through a multi-view co-training objective. Some other methods were proposed for taxonomy completion, such as TMN <ref type="bibr" target="#b33">[34]</ref> focuses on taxonomy completion task with channel-gating mechanism and triplet matching network; and GenTaxo <ref type="bibr" target="#b30">[31]</ref> collects information from complex local-structure information and learns to generate concept's full name from corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we proposed TaxoEnrich to enhance taxonomy completion task with self-supervision. It captures the hierarchical and semantic information of concept nodes based on the taxonomic relations in the existing taxonomy. Additionally, the selective queryaware attention module and elaborately designed matching module further improves the performance of learning relatedness between query node and candidate position. Extensive experimental results elucidated the effectiveness of TaxoEnrich by showing that it largely outperforms the previous methods achieving state-of-the-art performance on both taxonomy completion and expansion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS A.1 Baseline Models</head><p>TaxoExpan, ARBORIST <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> were designed for taxonomy expansion task. We follow the implementations in <ref type="bibr" target="#b33">[34]</ref> to calculate the ranking of candidate positions from the single score output of their matching model, so that we can have similar output for evaluations. For TaxoExpan <ref type="bibr" target="#b18">[19]</ref>, we implemented the full framework with PGAT propagation method and InfoNCE Loss <ref type="bibr" target="#b15">[16]</ref>. In comparison experiments in <ref type="bibr" target="#b33">[34]</ref>, all methods only leveraged the initial embeddings without any distribution models for matching model comparison. For TMN , we implemented with Raw embedding + LSTM and PGAT encoders for the full comparison, based on the original triplet matching network. And for GenTaxo, we used the same distribution model as in TaxoEnrich.</p><p>Note that in the previous methods, such as TaxoExpan, AR-BORIST and TMN , the generation of the initial embeddings was from the static word embedding method. In MAG datasets, the embeddings of each concept node is computed using Word2Vec method to generate a 250-dimensional vectors. And in WordNet datasets, the embeddings were generated using FastText as a 300dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Settings</head><p>In the implementation of TaxoEnrich, we use Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with learning rate 0.001. We applied a scheduler which multiplies the learning rate by a factor of 0.5 after 10 epochs of non-improving metrics. The hidden dimension for LSTM encoders is set as 500 and the number of bilinear models 𝑘 in the matching module is 10. The number of siblings selected in the attention module 𝑡 is set as 5 for training and 20 for testing. The model is then trained with 200 epochs with early stop if the MR metric has not improved for more than 10 epochs on validation dataset. For other hyperparameters, we set 𝑙 1 , 𝑙 2 , 𝑙 3 , 𝑙 4 = 1.0, 1.0, 1.0, 0.2 in all datasets to avoid heavy parameter tuning, batch size as 16 for both TaxoEnrich and TaxoEnrich-S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL ABLATION STUDIES B.1 Hyperparameter Tuning</head><p>We conduct additional ablation studies on hyperparameter searching. We examine the influence of batch size of TaxoEnrich-S framework. It turns out that the batch size with 16 tends to be better than others.</p><p>Experiments on the learning rate of the newly incorporated sibling loss is also explored. We can observe that 𝑙 4 = 0.2 and 0.5 will result in slightly better performance for MAG-CS datasets, as the information in siblings will still introduce noises if we treat is equally with parent and children relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sentence Encoder Studies</head><p>The comparison between different pretrained language models for sentence encoders is also studied under settings described above. And the results are shown in Table <ref type="table" target="#tab_5">7</ref>. We can see that SciBERT achieves the best performance among all language models, and Transformer has very similar results. And BERT has relatively poor performance. The reason may be that BERT is pretrained on general domain, making it less accurate in representing scientific domainspecific concepts in MAG-CS datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of inserting new concepts into an existing taxonomy of computer science terms. For each new concept, we aim to find the relatedness between the concept and each candidate position.</figDesc><graphic url="image-1.png" coords="1,317.96,403.09,252.20,248.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The complete architecture of TaxoEnrich . The figure describes the workflow of TaxoEnrich, and the details are discussed in the corresponding section.</figDesc><graphic url="image-2.png" coords="3,53.80,83.68,504.37,183.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The visualization of taxonomy-contextualized generation process. In this example, we aim to extract the embedding of the concept node "Disk".</figDesc><graphic url="image-3.png" coords="3,53.80,574.75,252.20,87.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="10,53.80,144.51,252.20,199.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="10,53.80,400.38,252.20,202.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics. |N | represents the number of nodes in the taxonomy and |E | represents the number of edges in the taxonomy. |D| indicates the taxonomy depth. # of Sentences denotes the number of pseudo sentences generated by the embedding generation module in each taxonomy.</figDesc><table><row><cell>Dataset</cell><cell>|N |</cell><cell>|E |</cell><cell cols="2">|D| # of Sentences</cell></row><row><cell>MAG-CS</cell><cell cols="2">24,754 42,329</cell><cell>6</cell><cell>227,609</cell></row><row><cell>MAG-PSY</cell><cell cols="2">23,187 30,041</cell><cell>6</cell><cell>111,194</cell></row><row><cell cols="4">WordNet-Noun 83,073 76,812 20</cell><cell>236,454</cell></row><row><cell cols="4">WordNet-Verb 13,936 13,403 13</cell><cell>34,654</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall results of Taxonomy Completion task on the four large-scale datasets. ** indicates the results are from<ref type="bibr" target="#b33">[34]</ref>.</figDesc><table><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of Taxonomy Expansion task on the MAG-CS and WordNet-Verb datasets.</figDesc><table><row><cell>Method</cell><cell>MR</cell><cell cols="2">MAG-CS MRR Recall@1</cell><cell>Precision@1</cell></row><row><cell>TaxoExpan</cell><cell cols="2">197.776 ± 16.038 0.562 ± 0.023</cell><cell>0.100 ± 0.011</cell><cell>0.163 ± 0.018</cell></row><row><cell>TMN</cell><cell>118.963 ± 6.307</cell><cell>0.689 ± 0.005</cell><cell>0.174 ± 0.002</cell><cell>0.283 ± 0.004</cell></row><row><cell>GenTaxo</cell><cell cols="2">140.262 ± 40.398 0.634 ± 0.044</cell><cell>0.149 ± 0.020</cell><cell>0.294 ± 0.096</cell></row><row><cell>TaxoEnrich-S</cell><cell cols="4">67.947 ± 1.121 0.721 ± 0.008 0.182 ± 0.005 0.304 ± 0.008</cell></row><row><cell>Method</cell><cell>MR</cell><cell cols="2">WordNet-Verb MRR Recall@1</cell><cell>Precision@1</cell></row><row><cell>TaxoExpan</cell><cell>665.409 ± 137.250</cell><cell>0.406 ± 0.056</cell><cell>0.085 ± 0.018</cell><cell>0.095 ± 0.004</cell></row><row><cell>TMN</cell><cell>615.021 ± 166.375</cell><cell>0.423 ± 0.056</cell><cell>0.110 ± 0.021</cell><cell>0.124 ± 0.009</cell></row><row><cell>GenTaxo</cell><cell cols="2">6046.363 ± 439.305 0.155 ± 0.010</cell><cell>0.094 ± 0.019</cell><cell>0.141 ± 0.079</cell></row><row><cell>TaxoEnrich-S</cell><cell>217.842 ± 5.230</cell><cell cols="3">0.481 ± 0.071 0.162 ± 0.082 0.294 ± 0.031</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the incorporation of siblings in embeddings on MAG-CS dataset. The framework that randomly incorporates sibling information in embedding generation module is denoted as TaxoEnrich-Sib for simplicity.</figDesc><table><row><cell>Method</cell><cell>MR</cell><cell>MRR</cell><cell cols="2">MAG-CS Recall@1</cell><cell>Precision@1</cell></row><row><cell cols="4">TaxoEnrich-Sib 122.144 ± 3.219 0.513 ± 0.006</cell><cell>0.138 ± 0.000</cell><cell>0.224 ± 0.001</cell></row><row><cell>TaxoEnrich-S</cell><cell cols="5">73.680 ± 1.346 0.545 ± 0.002 0.154 ± 0.006 0.251 ± 0.016</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies on MAG-CS dataset with different feature encoders. Some results are from the main table.</figDesc><table><row><cell>Method</cell><cell>Distribution Model</cell><cell>MR</cell><cell>MAG-CS Recall@1</cell><cell>Precision@1</cell></row><row><cell>TaxoExpan</cell><cell>Raw Embedding</cell><cell>3360.343 ± 6.126</cell><cell>0.000 ± 0.000</cell><cell>0.001 ± 0.001</cell></row><row><cell>TaxoExpan</cell><cell>Raw + PGAT</cell><cell cols="2">823.075 ± 114.638 0.030 ± 0.002</cell><cell>0.132 ± 0.010</cell></row><row><cell>TMN</cell><cell>Raw Embedding</cell><cell>636.254 ± 36.465</cell><cell>0.036 ± 0.005</cell><cell>0.156 ± 0.008</cell></row><row><cell>TMN</cell><cell cols="2">Raw + LSTM + PGAT 436.319 ± 13.128</cell><cell>0.056 ± 0.001</cell><cell>0.245 ± 0.006</cell></row><row><cell>TaxoEnrich-S</cell><cell>Raw Embedding</cell><cell>103.016 ± 6.589</cell><cell>0.145 ± 0.004</cell><cell>0.236 ± 0.010</cell></row><row><cell>TaxoEnrich-S</cell><cell>Raw + LSTM</cell><cell>73.680 ± 1.346</cell><cell cols="2">0.154 ± 0.006 0.251 ± 0.024</cell></row><row><cell cols="2">TaxoEnrich-S Raw + LSTM + PGAT</cell><cell>100.188 ± 2.214</cell><cell>0.150 ± 0.004</cell><cell>0.244 ± 0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Ablation studies of TaxoEnrich-S on MAG-CS for the comparison between different pseudo sentence encoders.</figDesc><table><row><cell>Method</cell><cell>MR</cell><cell cols="2">MAG-CS Recall@1 Precision@1</cell></row><row><cell cols="2">Transformer 74.132</cell><cell>0.149</cell><cell>0.248</cell></row><row><cell>SciBERT</cell><cell>73.680</cell><cell>0.154</cell><cell>0.251</cell></row><row><cell>BERT</cell><cell>253.221</cell><cell>0.082</cell><cell>0.173</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that since the implementation code of GenTaxo<ref type="bibr" target="#b30">[31]</ref> is not released, we implemented the framework based on the description in the paper.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Snowball: extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
		<idno>DL &apos;00</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676[cs.CL]</idno>
		<title level="m">SciBERT: A Pretrained Language Model for Scientific Text</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Taxonomy induction using hypernym subsequences</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th international conference on computational linguistics</title>
				<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Coling</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corel: Seed-guided topical taxonomy construction by concept learning and relation transferring</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1928" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MetaPAD: Meta Pattern Discovery from Massive Text Corpora</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Medical subject headings (MeSH)</title>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">E</forename><surname>Lipscomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic taxonomy construction from keywords</title>
		<author>
			<persName><forename type="first">Xueqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1433" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network</title>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expanding Taxonomies with Implicit Edge Semantics</title>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhananjay</forename><surname>Shrouty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2044" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Endto-end reinforcement learning for automatic taxonomy induction</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04044</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403274</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403274" />
		<title level="m">Octet. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2020-07">2020. Jul 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PATTY: A Taxonomy of Relational Patterns with Semantic Types</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inclusive yet Selective: Supervised Distributional Hypernymy Detection</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2057</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-2057" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TaxoExpan: Self-supervised taxonomy expansion with positionenhanced graph neural network</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="486" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10877[cs.IR]</idno>
		<title level="m">Entity Set Search of Scientific Literature: An Unsupervised Ranking Approach</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03682</idno>
		<title level="m">Who Should Go First? A Self-Supervised Concept Sorting Model for Improving Taxonomy Expansion</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wikidata: A New Platform for Collaborative Data Collection</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<idno type="DOI">10.1145/2187980.2188242</idno>
		<ptr target="https://doi.org/10.1145/2187980.2188242" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
				<meeting>the 21st International Conference on World Wide Web<address><addrLine>Lyon, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
	<note>WWW &apos;12 Companion)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A phrase mining framework for recursive construction of a topical hierarchy</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihit</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thrivikrama</forename><surname>Taula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="437" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Characterising Measures of Lexical Distributional Similarity</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Co-embedding network nodes and hierarchical labels with taxonomy based generative adversarial networks</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1026" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02974</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09551</idno>
		<title level="m">TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TaxoGen: Constructing Topical Concept Taxonomy by Adaptive Term Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">T</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Taxonomy Completion via Triplet Matching Network</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4662" to="4670" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
