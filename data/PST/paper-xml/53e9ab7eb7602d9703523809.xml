<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">6D Hands: Markerless Hand Tracking for Computer Aided Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
							<email>rywang@threegear.com</email>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
							<email>sparis@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Systems, Inc</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jovan</forename><surname>Popović</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Systems, Inc</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Univ. of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 3Gear Systems</addrLine>
									<settlement>Foster City</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">UIST&apos;11</orgName>
								<address>
									<addrLine>October 16-19</addrLine>
									<postCode>2011</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">6D Hands: Markerless Hand Tracking for Computer Aided Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">681770F6C80178537553AB38F205EBF3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H5.2 [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies Design</term>
					<term>Human Factors</term>
					<term>Algorithms computer aided design</term>
					<term>hand-tracking</term>
					<term>3D object manipulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Computer Aided Design (CAD) is an essential component of modern mechanical engineering, architectural design and visual effects design. 3D CAD software enables a user to digitally specify shapes, positions and orientations of objects in a virtual 3D scene. Formally, most of these tasks require users to specify 3 or more variables, for instance, the xyz compo- nents of a translation or the three angles of a rotation. However, a 2D mouse is still the predominate method by which users interact with CAD software. It is typically controlled unimanually by the dominant (right) hand and used for both adjusting the camera perspective and moving objects in a 3D scene. In this paper, we propose a new input method that is complimentary to the mouse and better matches the dimensionality of 3D assembly tasks.</p><p>We facilitate more efficient interactions for CAD by enabling the user to manipulate the camera perspective and objects in the scene with both hands in 3D. We can track 6 degrees of freedom for each hand and a pinching gesture for selection. To avoid wrist-strain, we primarily use the three translational degrees of freedom of each hand tracked at the tip of the thumb. We propose comfortable, memorable and efficient gestures that map unimanual translation to 3D translation in the virtual world and bimanual translation to 3D rotation.</p><p>A complete CAD software system typically includes functionality for modeling 3D parts, assembling the parts into Paper Session: 3D</p><p>UIST <ref type="bibr">'11, October 16-19, 2011</ref>, Santa Barbara, CA, USA a whole, and, in the case of engineering CAD, laying out views of the assembly on a paper blueprint. In this work, we focus on the assembly component of CAD, which primarily involves the 3D positioning of parts, a task well suited for direct hand manipulation.</p><p>A major feature of our system is that it tracks the hands without gloves or markers, leaving them unencumbered to use the keyboard and mouse. This enables users to transition seamlessly to existing functionality in the CAD software such as navigating a menu with the mouse and typing coordinates on the keyboard. By facilitating these mixed-mode operations, our system serves as a practical complement to the mouse and keyboard for 3D assembly interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Many methods have been proposed for markerless or gloveless hand tracking, but they are either too slow for interactive applications, e.g. <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b6">7]</ref>, or the range of poses that they can detect do not permit the precise selection required in CAD applications, e.g. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b20">20]</ref>. In comparison, our system achieves bimanual 6-DOF pose estimation at interactive rates and reliably detects poses suited for discrete selection such as pinching and pointing.</p><p>Glove tracking has been proposed to ease and speed up the problem of hand tracking, e.g. <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b25">25]</ref>. However, gloves are a significant drawback if one wants to also use the keyboard and mouse. Users may be reluctant to put on a glove when switching from a 2D task such as menu navigation to a 3D task such as object assembly. Wearing a glove may also become uncomfortable during long work sessions.</p><p>Dedicated 6-DOF rate-control devices such as the 3DConnexion SpaceNavigator are used for smooth camera control, but are generally not suited for or used for object selection or manipulation in CAD. We propose a set of hand gestures that works well for both camera adjustment and object manipulation.</p><p>Recently Microsoft introduced and deployed the Kinect motion capture system. While successful results have been shown for whole body tracking <ref type="bibr" target="#b21">[21]</ref>, it is unclear if the Kinect system can be used to track hands. In particular, occlusions of the fingers are difficult to resolve using a single viewpoint.</p><p>Pinching has been shown to be an effective gesture for "clicking" in 3D space. Hilliges and colleagues use a single depth camera to detect pinches above a table top <ref type="bibr" target="#b8">[9]</ref>. Benko and Wilson <ref type="bibr" target="#b3">[4]</ref> track pinches using an infrared camera above a projector. Wilson uses a webcam to detect pinches above the keyboard <ref type="bibr" target="#b26">[26]</ref>. However, all three approaches rely on a single-view pinch detection technique that suffers from occlusions, restricting the hand orientations that can be tracked.</p><p>A unique feature of our approach is the use of two widebaseline viewpoints. Our two-view approach resolves occlusions from one view using information from the other, enabling robust gesture (e.g. pinch) detection. Our contribution is independent of the particular type of camera used (depth or RGB).</p><p>Recent work on 3D assembly by Kin and colleagues <ref type="bibr" target="#b10">[11]</ref> addressed the construction of organic sets on a multi-touch surface. In organic set dressing, the artistic look and feel of a scene is more important than precise placement of parts.</p><p>In comparison, we focus primarily on 3D assembly for mechanical engineering where exact relationships between parts is crucial.</p><p>A large body of HCI research has used various forms of 3D input for virtual reality applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref> but studies have shown that 3D input usability is often inferior to the mouse <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23]</ref>. Based on this observation, we designed our system to limit the duration of 3D input sessions and to be complementary to existing input devices so that users can fall back to standard mouse+keyboard interaction at any time. We envision that our hand tracking will be used only for 3D assembly tasks where it offers a natural and intuitive three-dimensional interaction, while other tasks such as menu navigation will continue to be done with the mouse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRADITIONAL 3D ASSEMBLY WORKFLOW</head><p>3D assembly refers to positioning a set of 3D parts to create a larger whole. It is used to construct a set from a collection of props or to assemble mechanical pieces into a machine. It is performed in two main ways.</p><p>Coarse 3D placement is used primarily in the context of computer animation or special effects, where the location of an object is only important so much as it generates a convincing image or effect. For such placement, users mostly rely on the translation manipulator to specify position in 3D space with a 2D mouse. A translation manipulator multiplexes 3D translation onto three 1D components projected as axes on the screen (Figure <ref type="figure" target="#fig_1">2</ref>). The user selects an arrow and drags along each axis, one at a time, to move an object to its desired location. CAD software also lets users drag objects freely in the image plane. These manipulators are good at specifying axis-aligned and in-plane translations, but require more effort for other directions. Equivalent metaphors with similar pros and cons exist for rotations. The user enters the x translation mode by dragging along the projected x-axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X Z Y</head><p>These manipulators provide a continuous and thus imprecise translation control. For precise placement of mechanical parts, a process of specifying constraints between faces and boundaries (or "mates") is used to define the position of a part exactly. Such positioning is crucial in manufacturing, and mating is the primary mode for 3D assembly in all mechanical engineering CAD software (e.g. SolidWorks, Autodesk Inventor, CATIA). A mate is specified with the mouse by selecting two of the features to align, and defining the Paper Session: 3D</p><p>UIST <ref type="bibr">'11, October 16-19, 2011</ref>, Santa Barbara, CA, USA relationship between them (Figure <ref type="figure">3</ref>). For instance, if two circular boundaries are required to lie along the same axis, a user would click each boundary successively and select the concentric mate option. Specifying mates requires significant adjustments to the camera perspective because mates often involve occluded features. For instance, when specifying that two faces should be coincident, one of the faces is often facing away from the user, and thus not directly selectable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select first face Rotate camera</head><p>Select second face Coincident mate 1 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">4</head><p>Figure <ref type="figure">3</ref>: To specify a constraint or "mate" using a traditional 2D mouse, the user clicks on one face, rotates the camera, clicks on the other face, and indicates that they should be coincident.</p><p>In addition to manipulating objects, users also adjust the camera perspective to visualize the assembly from different view points or to select occluded features. This is classically done with the mouse using an arcball rotation control. This tool maps x and y mouse translations to the rotations around the y and x screen axes respectively. z-axis rotation, when available, is modal and involves either a different mouse button or a modifier key.</p><p>We observed several mechanical engineers using the Solidworks 2009 CAD software. Other functions often used in 3D assembly include importing parts from the file system into the scene, hiding or isolating parts in complex assemblies, and performing specialized shape modifications to a part (e.g. adding threads to a screw). These actions are accessed through mouse-driven menus, and would be more difficult to map to hand tracking.</p><p>We contribute a system that uses hand gestures for the positioning of parts for 3D assembly while other tasks such as entry of numerical values, annotations, or menu navigation are still performed with the keyboard and the mouse. Our gestures allow users to efficiently reposition pieces in 3D by selecting and moving them with their hands. Users can also adjust the camera perspective to access pieces and explore the 3D scene. Furthermore, our system enables modeless specification of exact relationships, making it suitable for the assembly of mechanical parts for engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAND GESTURES FOR 3D ASSEMBLY</head><p>Based on the observations made in the previous section, we first describe the design principles that guided how we built our system. Then, we propose a set of gestures that follow these guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design Principles</head><p>Before describing the gestures that we implemented in our system, we first discuss the challenges that we faced while we designed our system and the high-level principles that we followed to address them.</p><p>A small number of simple hand poses Although a human hand has 27 DOFs, only a few poses are comfortable and can be reproduced without training. Guided by this idea, we built our system mostly on the pinch pose inspired by Andrew Wilson's <ref type="bibr" target="#b26">[26]</ref> Thumb and Fore-Finger Interface. We use pinching as an analog of the mouse click to indicate the selection of an object. We also explore pointing for specifying remote locations, and touching the desktop with fingers to turn the desk surface into a multi-touch interface.</p><p>Use precise and memorable gestures To create precise and memorable gestures, we use metaphors that correspond closely to physical actions. We directly map 3D physical positions to virtual positions. Once a user understands a virtual scene, reaching for the object leverages his physical intuition to reach for a point in 3D space. We also adopt a physicallybased mental model to design hand gestures for rotation. Motivated by work showing that users tend to perform rotation and translation separately <ref type="bibr" target="#b15">[15]</ref>, we decouple camera rotation and camera translation as two distinct gestures to provide precise and physically-based control for both.</p><p>Limited hand motion Unrestricted 3D interactions and large movements are tiring and only useful for short periods of time. We exploit the desktop environment to address the fatigue issue and design our system such that users can rest their elbows or forearms on the desk most of the time. Inspired by the Eden system, <ref type="bibr" target="#b10">[11]</ref>, we also allow the user to pass objects between the hands (throw-and-catch) to minimize dragging. We also amplify the user's 3D motion so that only small gestures are needed, e.g. we map a 10 • hand rotation to 20 • in the modeler.</p><p>Specification of exact constraints Exact constraints, or mates, are crucial in mechanical engineering applications to align pieces or put objects in exact contact. Specifying these mates explicitly involves selecting small features such as boundaries and faces. This is already challenging in 2D with a mouse, and even more difficult with 3D selection. Instead, we "snap" a part into place when it is sufficiently close to satisfying a concentric or a coincident mate. This feature enables our prototype CAD system to facilitate the precise alignment and contact specification required in mechanical engineering.</p><p>Concentric and coincident mates account for a large proportion of mates between mechanical parts. For instance, any mechanical assembly held together by screws and nuts use concentric and coincident mates. However, we hope to address more general mates (e.g. distance mates, curved surfaces) in future work. Presently, we fall back to traditional mouse and keyboard input for more complex mates.</p><p>Cues for 3D positioning We need to provide visual cues that relate the physical space where the user's hands are with the virtual space where modeling occurs. Since we do not Paper Session: 3D UIST'11, October 16-19, 2011, Santa Barbara, CA, USA want to assume that a 3D display is available, a simple "3D pointer", e.g. a small 3D arrow, is not enough because of the depth ambiguity inherent in 2D displays that makes it difficult to know if the pointer is at the same depth as another object. We address this challenge with a shadow metaphor that provides unambiguous depth cues in addition to the main 3D view. We render a virtual ground plane on which each piece projects a shadow, and we augment our pointer with a stem and base that shows its projection on the same plane <ref type="bibr" target="#b7">[8]</ref>, thereby resolving the depth ambiguity. Further, we also highlight the closest object to each hand to show which object can be selected in the current configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seamless transition between tracking and other modalities</head><p>3D assembly in CAD software is performed in conjunction with other activities that require the use of the keyboard and mouse such as structural analysis, reading documentation, or even just checking e-mail. Hand tracking should not interfere with other input modalities and should be an addition to them, not a replacement. We designed our system so that it can track bare hands since gloves and markers would impede the users' ability to type and use a mouse comfortably. We also automatically stop tracking when the hands approach the keyboard or the mouse, which allows the user to seamlessly go back to a classical keyboard-and-mouse interaction at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gestures</head><p>We support several gestures for object and camera manipulation (Figure <ref type="figure" target="#fig_3">4</ref>). These are designed according the guidelines previously discussed.</p><p>Object translation When the user pinches with one hand, we select the closest object, and translate according to the pinching hand. If the user drags the object sufficiently close to another object, we examine the shapes of both objects and consider snapping the dragged object to generate a mate.</p><p>In our prototype CAD system, we focus on two common mates, concentric mates between circular boundaries and coincident mates between two faces. We detect approximate alignment of circular boundaries <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b12">12]</ref> to snap the object into an exact alignment. Similarly, we snap together parallel faces that are almost touching to generate a coincident mate. Our 3D snapping generalizes snapping with 2D input devices <ref type="bibr" target="#b17">[17]</ref> by generating the same mates used in CAD software.</p><p>Long-distance object translation Dragging objects is suitable for small translations but becomes tedious for large displacements, for example to go from one side of the workspace to the other. We address this with a throw-and-catch gesture akin to the Eden system <ref type="bibr" target="#b10">[11]</ref>. The user pinches with one hand next to an object, which selects it, then pinches with the other hand where the object should go, and finally opens again the first hand, i.e. stops pinching, which transports the object where the second hand is. This gesture may be better seen in the companion video.</p><p>Camera translation When the user double-pinches with the two hands, we "attach" the camera to the two hands and translate it according to the average hand motion.</p><p>Camera and object rotation The user can control rotation by pinching with both hands, using a new sheet-of-paper metaphor. The motion is inspired by the grab-and-twirl gesture developed by Cutler and colleagues <ref type="bibr" target="#b5">[6]</ref>, and reproduces what users would experience if they were handling a sheet of paper as shown in Figure <ref type="figure" target="#fig_4">5</ref>. Compared to grab-and-twirl, we remove the dependence on the hand orientation completely.</p><p>Because we can track the 3D pinch points more accurately than hand orientation, we can amplify the rotated angles to minimize motion. We detail the formulas to derive the rotation angles from these gestures in the appendix.</p><p>When nothing is selected, a two-handed pinch rotates the viewpoint direction. When an object is selected with one of the hands, the rotation affects that object. Because CAD models are almost always axis-aligned, we snap object rotations to 90 degree increments.</p><p>Discussion We also experimented with a direct mapping of hand orientation to camera and object rotation. Direct orientation mapping worked well for small rotations, but large rotations led to significant strain on the wrists. This was problematic because camera viewpoint changes and object reorientations in CAD typically involve rotations of 90 degrees or more. In comparison, our two-handed pinching gestures do not require uncomfortable extreme wrist rotations and distributes effort across the larger muscles of the arm. Overall, we found our sheet-of-paper metaphor as easy-to-learn as a direct mapping while being more accurate and less straining on the wrists.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Translation Throw and Catch</head><note type="other">Object Placement</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MARKERLESS HAND-TRACKING</head><p>The main technical contribution of this work is a markerless hand tracking system that accurately and robustly recognizes the gestures previously described. As discussed in the previous work section, real-time markerless hand tracking is still an unsolved problem, and our approach leverages a wide-baseline camera setup, our constrained set of gestures, and the calibrated desktop environment to make this problem tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Setup</head><p>We designed our setup with several factors in mind. We ensure that both cameras can see the hands over a large 3D  capture volume. Our pose estimation algorithm benefits from two significantly different viewpoints to maximize information and minimize self-occlusion of the hand. The cameras are placed so that one hand does not occlude the other from either view. In particular, this could be a problem when one hand is raised above the other (Figure <ref type="figure" target="#fig_5">6</ref>). Given these constraints, we propose a configuration such that a 34cm × 46cm × 24cm rectangular operating region is completely visible from both cameras. The cameras are placed so that their principal axes differ by an angle of 45 • , which yields significantly different viewpoints. Each camera also forms a steep 67 • angle with the ground plane. Given a spacing of 20 cm between the two hands, this allows one hand to be 10 cm above the other without inter-hand occlusion.</p><note type="other">Bimanual Rotation</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markerless Pose Estimation</head><p>Our technique applies a data-driven pose estimation algorithm inspired by the marker-based technique of Wang et  al. <ref type="bibr" target="#b25">[25]</ref>. However, using Wang's color glove would greatly impede users' interaction with other input devices such as a mouse or a keyboard, and we opted for a markerless approach, which makes the tracking problem much more challenging. We extend Wang's technique in several ways to address our needs and leverage our specific configuration. First, we sample only the limited set of hand poses relevant to our gestures rather than arbitrary configurations of the hand. We also consider only comfortable hand orientations and positions within our rectangular area. Finally, we modify the pose estimation algorithm to use two cameras rather than one.</p><p>Image Pair (Our Method) Two Camera Pose Estimation Our system builds upon recent work in data-driven pose estimation that uses a precom-puted database to map image features to poses, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19]</ref>. Specifically we adapt a technique designed to track color gloves using a database that maps tiny 40 × 40 pixel images of the hand to their associated 3D poses <ref type="bibr" target="#b25">[25]</ref>.</p><note type="other">Single Image Ground Truth Database NN Resulting Pose Database Query</note><p>For background subtraction, we build a Gaussian mixture model for the background <ref type="bibr" target="#b22">[22]</ref> and segment the two largest connected skin-toned regions (from the left and right hand). We encode the hand regions from each camera as a pair of tiny images, and query for the nearest neighbors in a precomputed database mapping hand image pairs to their associated 3D poses. Finally, we solve a 6-DOF inverse kinematics (IK) problem to place the resulting 3D hand pose to match the hand region locations (Figure <ref type="figure">9</ref>).</p><p>A pair of hand images from different viewpoints provides much more information than a single image, but storing a database of image pairs requires twice as much memory for the same density of hand poses. To reduce memory usage, we exploit the redundancy of hand images of the image pair set. Given a set of N = 2.1 × 10 6 relevant hand poses, we render these poses from two camera viewpoints to obtain tiny image pairs {(r 0 i , r 1 i )} 1...N . We then sample K = 30,000 of the most different (i.e. least redundant) images from these image pairs { rk } 1...K using low dispersion sampling. These K images approximate the original set of image pairs, mapping them to a much smaller set of approximations {(r 0 i , r1 i )} i , thus making image-pair-based pose estimation efficient. Figure <ref type="figure" target="#fig_0">10</ref>: We compared our proposed image sampling approach that exploits redundancy between cameras and decouples sampling with a naive approach that samples pairs of images. Our approach yields a savings of 27% for equal quality at 30,000 samples.</p><p>We compared our sampling approach to a naive image-pair sampling approach that does not exploit redundancy of images between cameras (See Figure <ref type="figure" target="#fig_0">10</ref>). Both curves converge slowly, but to achieve equal quality with 30,000 samples from the naive approach, our approach only requires 22,000 samples. To achieve equality quality with 30,000 samples taken with our approach, the naive approach would require 50% more samples (45,500).</p><p>Gesture and Desktop-Specific Sampling Because our system relies on only a few gestures, we only need to track a small set of hand poses. Hand poses outside of this relevant set are ignored during tracking because they do not cor-respond to actions in our CAD system. While the work of Wang and colleagues sampled a large variety of hand poses for general purpose tracking, we restrict our sampling to only the set of relevant hand poses for our gestures and for our particular desktop environment.</p><p>Our technique concentrates on the finger configurations required by our gestures. We use a relaxed hand pose, a pinching hand pose, and a pointing hand pose as a finger configuration basis {q i }. We then take pairwise blends between each of these poses {q|q = αq i + (1 -α)q j } for α ∈ [0, 1] to generate a dense set of finger configuration transitions between basis poses.</p><p>Our model assumes that the hand is in a 34cm×46cm×24cm box above the keyboard, and we only sample hand positions in this region. The principal axis of the hand can move in a cone subtending an angle of θ = 60 • and the hand can twist about that axis φ = 130 • . While most people can twist their hands more, these ranges are sufficient to cover the set of comfortable poses. With this restrictive sampling, we are able to make markerless hand-tracking feasible. In Figure <ref type="figure" target="#fig_9">11</ref>, we quantify the benefits of our gesture-specific, desktop-specific sampling, compared to sampling all hand configurations and all orientations, many of which are impossible given the user and camera placement. A gestureagnostic sampling requires 30,000 samples to achieve equal quality with 5,000 samples taken with our approach. A desktop-agnostic sampling requires 30,000 samples to achieve equal quality with 15,000 samples from our approach.</p><p>Pinch / Click Detection A robust pinch detector is the basis of our gestures, and we address it separately from 3D tracking. Our pinch detection is based on detecting separation of the tips of the index finger and thumb in at least one of the two camera images. First, we check for extrema <ref type="bibr" target="#b18">[18]</ref> of the silhouette close to the predicted locations of the index finger and thumb from our 3D pose estimate. Thumb-index separation is detected if the geodesic distance between the extrema is longer than the Euclidean distance. If no separation is detected in either view, we register a pinch (Figure <ref type="figure" target="#fig_1">12</ref>). Figure <ref type="figure">9</ref>: We segment the hands using background subtraction and skin-tone detection. We take the resulting hand regions and encode them as tiny images. We use a pair of tiny images to query the database, blend the nearest neighbors, and solve a 6-DOF IK problem to obtain the 3D hand pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Hand Estimate Predicted Tips Nearby Extrema</head><p>Figure <ref type="figure" target="#fig_1">12</ref>: We look for extrema (red points) in the silhouette that match the predicted locations of the thumb and index finger tips (blue points). If the geodesic distance (orange line) between the extrema is larger than the Euclidean distance (green line), we detect a separation.</p><p>To evaluate our approach, we recorded 8 sequences from 4 people consisting of pinching gestures performed at various locations in the capture volume. We found that we could track pinching with about 99% accuracy. Further analysis, showed that the occasional missed detections and false pinch detections mostly occur at extreme poses or at extreme positions of our capture volume. As demonstrated in our companion video, our robust detection mechanism enables complex interaction.</p><p>Limitations and User Calibration Our computer-vision-based approach is currently limited to environments where the hands can be segmented from the background with skin-tone detection and background subtraction. Presently, we also ask the user to wear a long-sleeved top because we do not model the skin-toned arms. Both of these issues can be resolved with the use of depth cameras, and we hope to explore two-depthcamera setups in future work.</p><p>Unlike the technique of Wang and Popović, our database sampling is specific to the camera setup and needs to be regenerated when the cameras are moved. This requires approximately 30 minutes.</p><p>For best results, we also manually calibrate three hand poses of the user. Differently shaped hands tend to pinch and point differently. We kinematically adjust the joint angles of our basis poses to reflect the user-specific pinching, pointing and resting gestures. This takes approximately ten minutes of manual calibration, and we hope to incorporate a more accurate and automatic technique in the future <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>We tested our system on an Intel Core i7 desktop computer with two PlayStation 3 Eye cameras. Our hand-tracking system runs at interactive rates (20 Hz) on two threads of the processor. Our complete 3D assembly system runs at a slower rate of 17 Hz on the models we tested.</p><p>We measured precision (jitter) of the tracked thumb tips to be approximately 3mm. Using known rotation sequences, we also estimated the rotation precision to be between 1 and 3 degrees around the x and y axes, and about 6 degrees around the z axis.</p><p>We tested our assembly system on several models used to teach 3D assembly included as part of the SolidWorks CAD software. We show that these models can be efficiently assembled using hand gestures alone. In the hands of an expert user of both our system and a traditional CAD mating interface, we have observed time savings of up to 40%. This comes from significantly fewer modal transitions from selecting parts to adjusting the camera. We also evaluated our bimanual rotation gestures on a rotation-matching task.</p><p>While not quite as efficient as an expert arcball user, our bimanual gestures provides a comfortable and easy-to-use rotational mapping. In another sequence, we show that our system works well alongside a mouse, keyboard and 3DConnexion SpaceNavigator.</p><p>In an early pilot study with three novice users, we found that due to the lack of tactile feedback, visual feedback is crucial Paper Session: 3D UIST'11, October 16-19, 2011, Santa Barbara, CA, USA for keeping the user in sync with the state of the assembly system. Users would often mis-select objects or confuse being in object translation mode versus camera rotation mode.</p><p>In response, we gave each assembly action, including camera rotation, camera translation, object selection, and object release, a distinct visual cue. For instance, we draw a bounding box around the scene and a 3D manipulator for camera rotation and translation. We higlight the active object with a golden halo when it has been selected for manipulation.</p><p>We also found that shadows were an insufficient depth cue for 3D object selection on a 2D display. Users would often select a different object than the one they had intended. We addressed this by brightening objects near each hand and always outlining the closest object that would be selected if the user were to pinch. These cues inform the user about the relative position of his hands in the scene without having to look down at shadows before selecting an object, reducing the incidence of mis-selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extensions</head><p>In addition to 3D CAD assembly, our hand-tracking system can be used to turn the desk plane into a touch-sensitive device. We use our two-camera setup to triangulate the 3D location of the index finger and detect intersection with the calibrated desk plane. We demonstrated this system on a 2D multi-touch photo rotation task. While our touch detection is only accurate within five millimeters, this is sufficient for simple multi-touch applications at the desktop. We detect touch with a calibrated desk to enable a virtual multi-touch surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We have developed a hand-tracking system tailored for a fundamental task in computer aided design, 3D assembly. Our system relies on a small set of gestures that are comfortable to use, precise, and easy to remember. To recognize these gestures, we built a data-driven pose-estimation system that uses a wide-baseline camera setup and an efficiently sampled database to track the hands without markers, alongside the keyboard and mouse. Furthermore, our prototype CAD system generates the exact positioning constraints used in traditional CAD for mechanical engineering. In summary, we have developed a complementary user input device for efficient 3D assembly of mechanical parts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose a markerless hand-tracking system using two webcams for 3D assembly in the context of Computer Aided Design.</figDesc><graphic coords="1,317.69,321.89,240.02,121.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A traditional translation manipulator multiplexes 3D translation onto three 1-D translation modes.</figDesc><graphic coords="2,415.09,481.09,143.42,95.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: We support a few simple gestures for 3D manipulation and camera adjustment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We mimic the physical actions of rotating an imaginary piece of paper with two hands to define our gestures for bimanual rotation. Rotating the sheet about the y or z-axis involves moving the hands in opposite directions along the xz or xy-plane respectively. To rotate the paper about the x-axis, one lifts or lowers the hands while bending the wrists (resulting in a translation along the y-axis about the elbow pivot)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The most common occlusion case is when one hand (approximated as the circle on the right) is raised above another (the left circle). A steeper camera angle θ 2 &gt; θ 1 allows a larger vertical separation h 2 &gt; h 1 between the hands without occlusion.</figDesc><graphic coords="5,343.89,533.19,50.32,50.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Our cameras are arranged so that both cameras observe the hand in a rectangular operating region (green).</figDesc><graphic coords="5,317.69,68.99,164.62,138.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Given only silhouette data, a single camera view does not provide information to resolve ambiguities in orientation and finger configuration. Two cameras from different view points provides much less ambiguous data.</figDesc><graphic coords="5,343.89,587.09,50.32,50.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Our gesture-specific and desktop-specific sampling allows us to use as much as 80% fewer samples to achieve equal sampling quality with a gesture and desktop agnostic approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: We detect touch with a calibrated desk to enable a virtual multi-touch surface.</figDesc><graphic coords="8,54.39,419.79,239.62,150.72" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Formulas for camera rotation</head><p>Given previous click points (p 0 , p 1 ) and current click points (p ′ 0 , p ′ 1 ), the bimanual rotation is specified by R 1 R 2 . The rotation R 1 corresponds to the elbow rotation about the xaxis of arctan((((p ′ 0 + p ′ 1 ) -(p 0 + p 1 ))/2) y , f ), where f is the forearm length. The rotation R 2 corresponds to rotating the sheet about its center axis. Let the normalized vector between the click points be n = (p 1p 0 )/ p 1p 0 and</p><p>Then R 2 is a rotation about the sheet's center axis n × n ′ with an angle of arctan( n × n ′ , n • n ′ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper Session: 3D</head><p>UIST <ref type="bibr">'11, October 16-19, 2011</ref>, Santa Barbara, CA, USA</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The two-user responsive workbench: Support for collaboration through independent views of a shared space</title>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Mcdowall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bolas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 97</title>
		<meeting>SIGGRAPH 97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating 3D hand pose from a cluttered image</title>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Depthtouch: Using depth-sensing camera to enable freehand interactions on and above the interactive surface</title>
		<author>
			<persName><forename type="first">Hrvoje</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
		<idno>MSR-TR-2009-23</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-point interactions with immersive omnidirectional visualizations in a dome</title>
		<author>
			<persName><forename type="first">Hrvoje</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interactive Tabletops and Surfaces (ITS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Francois</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchel</forename><surname>Benovoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalia</forename><surname>El-Shimy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cooperstock</surname></persName>
		</author>
		<title level="m">Did &quot;Minority Report&quot; Get it Wrong? Superiority of the Mouse over 3D Input Devices in a 3D Placement Task. Human-Computer Interaction-INTERACT 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="400" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-handed direct manipulation on the responsive workbench</title>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interactive 3D Graphics (I3D)</title>
		<meeting>of Interactive 3D Graphics (I3D)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-Based Hand Tracking with Texture, Shading and Self-occlusions</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Gorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiscale 3D reference visualization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keenan</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interactive 3D Graphics and Games (I3D)</title>
		<meeting>of Interactive 3D Graphics and Games (I3D)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
	<note>Andres Rutnik, and Azam Khan</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactions in the air: adding further depth to interactive tabletops</title>
		<author>
			<persName><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Garcia-Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Butz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Interface Software and Technology (UIST)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Artistic collaboration in designing vr visualizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><forename type="middle">L</forename><surname>Karelitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Vote</surname></persName>
		</author>
		<author>
			<persName><surname>Laidlaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="18" to="23" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eden: A professional multitouch tool for constructing virtual organic environments</title>
		<author>
			<persName><forename type="first">Kenrick</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Bollensdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">Paper Session: 3D UIST&apos;11</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">October 16-19, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated generation of interactive 3D exploded view diagrams</title>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual touchpad: a two-handed gestural input device</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laszlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimodal interfaces</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time hand tracking and gesture recognition for human-computer interaction</title>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Manresa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Varona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Perale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the allocation of control in a 6 degree-of-freedom docking experiment</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Masliah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illustrating how mechanical assemblies work</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Liang</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilmot</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moving objects with 2D input devices in cad systems and desktop virtual environments</title>
		<author>
			<persName><forename type="first">Ji-Young</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Stuerzlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Interface</title>
		<meeting>of Graphics Interface</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real time identification and localization of body parts from depth images</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hands in action: real-time 3D reconstruction of hands in interaction with objects</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedvig</forename><surname>Kjellstrm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient bimanual symmetric 3D manipulation for markerless hand-tracking</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Schlattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Virtual Reality International Conference (VRIC)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-Time Human Pose Recognition in Parts from Single Depth Images</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Value of Constraints for 3D User Interfaces</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chadwick</forename><surname>Wingrave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dagstuhl Seminar on VR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed occlusion reasoning for tracking with nonparametric belief propagation</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1369" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time handtacking with a color glove</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jovan</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust computer vision-based detection of pinching for one and two-handed gesture input</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Interface Software and Technology (UIST)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hand gesture interface device</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaron</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Lanier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><surname>Harvill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
