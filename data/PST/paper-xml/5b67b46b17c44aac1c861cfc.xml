<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Code Comment Generation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<country>Ministry of Education</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<country>Ministry of Education</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
							<email>lige@pku.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<country>Ministry of Education</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<country>Ministry of Education</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>xin.xia@monash.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Lo</surname></persName>
							<email>davidlo@smu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<email>zhijin@pku.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<country>Ministry of Education</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution" key="instit1">EECS</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Zhi</surname></persName>
						</author>
						<title level="a" type="main">Deep Code Comment Generation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A30FDBE560ED6C82AC518757728CD34</idno>
					<idno type="DOI">10.1145/3196321.3196334</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>program comprehension</term>
					<term>comment generation</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In software development and maintenance, developers spend around 59% of their time on program comprehension activities <ref type="bibr" target="#b44">[45]</ref>. Previous studies have shown that good comments are important to program comprehension, since developers can understand the meaning of a piece of code by using the natural language description of the comments <ref type="bibr" target="#b34">[35]</ref>. Unfortunately, due to tight project schedule and other reasons, code comments are often mismatched, missing or outdated in many projects. Automatic generation of code comments can not only save developers' time in writing comments, but also help in source code understanding.</p><p>Many approaches have been proposed to generate comments for methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref> and classes <ref type="bibr" target="#b24">[25]</ref> of Java, which is the most popular programming language in the past 10 years <ref type="foot" target="#foot_0">1</ref> . Their techniques vary from the use of manually-crafted <ref type="bibr" target="#b24">[25]</ref> to Information Retrieval (IR) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Moreno et al. <ref type="bibr" target="#b24">[25]</ref> defined heuristics and stereotypes to synthesize comments for Java classes. These heuristics and stereotypes are used to select information that will be included in the comment. Haiduc et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> applied IR approaches to generate summaries for classes and methods. IR approaches such as Vector Space Model (VSM) and Latent Semantic Indexing (LSI) usually search comments from similar code snippets. Although promising, these techniques have two main limitations: First, they fail to extract accurate keywords used for identifying similar code snippets when identifiers and methods are poorly named. Second, they rely on whether similar code snippets can be retrieved and how similar the snippets are.</p><p>Recent years have seen an emerging interest in building probabilistic models for large-scale source code. Hindle et al. <ref type="bibr" target="#b16">[17]</ref> have addressed the naturalness of software and demonstrated that code can be modeled by probabilistic models. Several subsequent studies have developed various probabilistic models for different software tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. When applied to code summarization, different from IR-based approaches, existing probabilistic-model-based approaches usually generate comments directly from code instead of synthesizing them from keywords. One of such probabilistic-modelbased approaches is by Iyer et al. <ref type="bibr" target="#b18">[19]</ref> who propose an attentionbased Recurrent Neural Network (RNN) model called CODE-NN. It builds a language model for natural language comments and aligns the words in comments with individual code tokens directly by attention component. CODE-NN recommends code comments given source code snippets extracted from Stack Overflow. Experimental results demonstrate the effectiveness of probabilistic models on code summarization. These studies provide principled methods for probabilistically modeling and resolving ambiguities both in natural language descriptions and in the source code.</p><p>In this paper, to utilize the advantage of deep learning techniques, we propose a novel approach DeepCom to generate descriptive comments for Java methods which are functional units of Java language. DeepCom builds upon advances in Neural Machine Translation (NMT). NMT aims to automatically translate from one language (e.g., Chinese) to another language (e.g., English) and it has been shown to achieve great success for natural language corpora <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>. Intuitively, generating comments can be considered as a variant of the NMT problem, where source code written in a programming language needs to be translated to text in natural language. Compared to CODE-NN which only builds a language model for comments, the NMT model builds language models for both source code and comments. The words in comments align with the RNN hidden states which involve the semantics of code tokens. Deep-Com generates comments by automatically learning from features (e.g., identifier names, formatting, semantics, and syntax features) extracted from a large-scale Java corpus. Different from traditional machine translation, our task is challenging since:</p><p>(1) Source code is structured: In contrast to natural language text which is weakly structured, programming languages are formal languages and source code written in them are unambiguous and structured <ref type="bibr" target="#b2">[3]</ref>. Many probabilistic models used in NMT are sequence-based models that need to be adapted to structured code analysis. The main challenge and opportunity is how to take advantage of rich and unambiguous structure information of source code to boost effectiveness of existing NMT techniques. (2) Vocabulary: In natural language (NL) corpora normally used for NMT, the vocabulary is usually limited to the most common words, e.g., 30,000 words, and words outside the vocabulary are treated as unknown words -often marked as UNK . It is effective for such NL corpora because words outside the dominant vocabulary are so rare. In code corpora, the vocabulary consists of keywords, operators, and identifiers. It is common for developers to define various new identifiers, and thus they tend to proliferate. In our dataset, we get 794,711 unique tokens after replacing numerals and strings with generic tokens NUM and STR . In a codebase used to build probabilistic models, there are likely to be many out-of-vocabulary identifiers. As Table <ref type="table" target="#tab_1">1</ref> illustrates, there are 794,621 unique identifiers in our dataset. If we use most common 30,000 tokens as the code vocabulary, about 95 % identifiers will be regarded as UNK . Hellendoorn and Devanbu <ref type="bibr" target="#b15">[16]</ref> have demonstrated that it is unreasonable for source code to use such a vocabulary.</p><p>To address these issues, DeepCom customizes a sequence-based language model to analyze Abstract Syntax Trees (AST) which capture structures and semantics of Java methods. The ASTs are converted into sequences before they are fed into DeepCom. It is generally accepted that a tree cannot be restored from a sequence generated by a classical traversal method such as pre-order traversal and post-order traversal. To better present the structure of ASTs, and keep the sequences unambiguous, we propose a new structure-based traversal (SBT) method to traverse ASTs. Using The brackets represent the structure of the AST and we can restore a tree unambiguously from a sequence generated using SBT. Moreover, to address the vocabulary challenge, we propose a new method to represent unknown tokens. The tokens in AST sequences include terminal nodes, non-terminal nodes, and brackets in our work. The unknown tokens come from the terminal tokens of ASTs. We replace the unknown tokens with their types instead of a universal special UNK token.</p><p>DeepCom generates comments word-by-word from AST sequences. We train and evaluate DeepCom on the Java dataset that consists of 9,714 Java projects from GitHub. The experimental results show that DeepCom can generate informative comments. Additionally, the results show that DeepCom achieves the best performance when compared with a number of baselines including the state-of-the-art approach by Iyer et al. <ref type="bibr" target="#b18">[19]</ref>.</p><p>The main contributions of this paper are as follows:</p><p>• We formulate code comments generation task as a machine translation task. • We customize a sequence-based model to process structural information extracted from source code to generate comments for Java methods. In particular, we propose a new AST traversal method (namely structure-based traversal) and a domain-specific method to deal with out-of-vocabulary tokens better.</p><p>Paper organization. The remainder of this paper is organized as follows. Section II presents background materials on language models and NMT. Section III elaborates on the details of DeepCom.</p><p>Section IV and Section V present the experiment setup and results. Section VI discusses strengths of DeepCom, and threats to validity. Section VII surveys the related work. Finally, Section VIII concludes the paper and points out potential future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Language Models</head><p>Our work is inspired by the machine translation problem in the NLP field. We exploit the language models learning from a large-scale source code corpus. The models generate code comments from the learned features. The language models learn the probabilistic distribution over sequences of words. They work tremendously well on a large variety of problem (e.g., machine translation <ref type="bibr" target="#b5">[6]</ref>, speech recognition <ref type="bibr" target="#b8">[9]</ref>, and question answering <ref type="bibr" target="#b45">[46]</ref>). For a sequence x = (x 1 , x 2 , ..., x n ) (e.g., a statement), the language model aims to estimate the probability of it. The probability of a sequence is computed via each of its tokens. That is,</p><formula xml:id="formula_0">&amp;# $ % # % # # % $ ! " ! )! %# % ' % % # % # !&amp; # !&amp; ! )! ! %# ! %# ! )! # # % ! )! ! % # + + + + + (%# %$ # "&amp; $% % # + (%# %$ # "&amp; $% % %&amp;# &amp; % % $</formula><formula xml:id="formula_1">P (x ) = P (x 1 )P (x2|x 1 )...P (x n |x 1 ...x n-1 )<label>(1)</label></formula><p>In this paper, we adopt a language model based on the deep neural network called Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>. LSTM is one of the state-of-the-art RNNs. LSTM outperforms general RNN because it is capable of learning long-term dependencies. It is a natural model to use for source code which has long dependencies (e.g., a class is used far away from its import statement). The details of RNN and LSTM are shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Recurrent Neural Networks.</head><p>RNNs are intimately related to sequences and lists because of their chain-like natures. It can in principle map from the entire history of previous inputs to each output. At each time step t, the unit in the RNN takes not only the input of the current step but also the hidden state outputted by its previous time step t -1. As Figure <ref type="figure" target="#fig_0">1</ref>(a) illustrates, the hidden state of time step t is updated according to the input vector x t and its previous hidden state h t -1 , namely, h t = tanh(W x t + U h t -1 + b) where W , U , and b are the trainable parameters which are updated while training, and tanh is the activation function: tanh(z) = (e ze -z )/(e z + e (-z ) ).</p><p>A prominent drawback of the standard RNN model is that gradients may explode or vanish during the back-propagation. These phenomena often appear when long dependencies exist in the sequences. To address these problems, some researchers have proposed several variants to preserve long-term dependencies. These variants include LSTM and Gated Recurrent Unit (GRU). In this paper, we adopt the LSTM which has achieved success on many NLP tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Long Short-Term</head><p>Memory. LSTM introduces a structure called the memory cell to solve the problem that ordinary RNN is difficult to learn long-term dependencies in the data. The LSTM is trained to selectively "forget" information from the hidden states, thus allowing room to take in more important information <ref type="bibr" target="#b17">[18]</ref>. LSTM introduces a gating mechanism to control when and how to read previous information from the memory cell and write new information. The memory cell vector in the recurrent unit preserves long-term dependencies. In this way, LSTM handles long-term dependencies more effectively than vanilla RNN. LSTM has been widely used to solve semantically related tasks and has achieved convincing performance. These advantages motivate us to exploit LSTM for building models for source code and comments. Figure <ref type="figure" target="#fig_0">1</ref>(b) illustrates a typical LSTM unit and for more details of LSTM, please refer to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Machine Translation</head><p>NMT <ref type="bibr" target="#b43">[44]</ref> is an end-to-end learning approach for automated translation. It is a deep learning based approach and has made rapid progress in recent years. NMT has shown impressive results surpassing those of phrase-based systems while addressing shortcomings such as the need for hand engineered features. Its architecture typically consists of two RNNs, one to consume the input text sequences and the other one to generate the translated output sequences. It is often accompanied by an attention mechanism that aligns target with source tokens <ref type="bibr" target="#b5">[6]</ref>.</p><p>NMT bridges the gap between different natural languages. Generating comments from the source code is a variant of machine translation problem between the source code and the natural language. We explore whether the NMT approach can be applied to comments generation. In this paper, we follow the common Sequence-to-Sequence (Seq2Seq) <ref type="bibr" target="#b36">[37]</ref> learning framework with attention <ref type="bibr" target="#b5">[6]</ref> which helps cope effectively with the long source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>The transition process between source code and comments is similar to the translation process between different natural languages. Existing research has applied machine translation methods translating code from one source language (e.g., Java) to another (e.g., C#) <ref type="bibr" target="#b12">[13]</ref>. A few studies adopt machine translation method for generating natural language descriptions from the source code. Oda et al. <ref type="bibr" target="#b29">[30]</ref> present a machine translation approach to generate natural language Pseudo-code of the source code at the statement level. In this paper, DeepCom translates the source code to a high-level description at the method level.</p><p>The overall framework of DeepCom is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. DeepCom mainly consists of three stages: data processing, model training, and online testing. The source code we obtained from GitHub is parsed and preprocessed into a parallel corpus of Java methods and their corresponding comments. In order to learn the structural information, the Java methods are converted into AST sequences by a special traversal approach before input into the model. With the parallel corpus of AST sequences and comments, we build and train generative neural models based on the idea of NMT. There are two challenges during training process: • How to represent ASTs to store the structural information and keep the representation unambiguous while traversing the ASTs? • How to deal with out-of-vocabulary tokens in source code?</p><p>In the following paragraphs, we will introduce the details of the model and the approaches we propose to resolve the abovementioned challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence-to-Sequence Model</head><p>In this paper, we apply a Sequence-to-Sequence (Seq2Seq) model to learn source code and generate comments. Seq2Seq model is widely used for machine translation <ref type="bibr" target="#b36">[37]</ref>, text summarization <ref type="bibr" target="#b33">[34]</ref>, dialogue system <ref type="bibr" target="#b38">[39]</ref>, etc. The model consists of three components, an Encoder, a Decoder, and an Attention component, in which the Encoder and Decoder are both LSTMs. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the detailed Seq2Seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder.</head><p>The encoder is an LSTM we describe in Section 2 and responsible for learning the source code. At each time step t, it reads one token x t of the sequence, then updates and records the current hidden state s t , namely,</p><formula xml:id="formula_2">s t = f (x t , s t -1 )<label>(2)</label></formula><p>where f is an LSTM unit that maps a word of source language x t into a hidden state s t . The encoder learns latent features from source code, and the features are encoded into the context vector c. These latent features include the identifiers naming conventions, control structures, and etc. In this paper, DeepCom adopts the attention mechanism to compute the context vector c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Attention.</head><p>Attention mechanism is a recent model that selects the important parts from the input sequence for each target word. For example, the token "whether" in comments usually aligns with the "if" statements in the source code. The generation of each word is guided by a classic attention method proposed by Bahdanau et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>It defines individual c i for predicting each target word y i as a weighted sum of all hidden states s 1 , .., s m in encoder and computed as</p><formula xml:id="formula_3">c i = m j=1 α i j s j (3)</formula><p>The weight α i j of each hidden state s j is computed as</p><formula xml:id="formula_4">α i j = exp(e i j ) m k =1 exp(e ik )<label>(4)</label></formula><p>and</p><formula xml:id="formula_5">e i j = a(h i-1 , s j )<label>(5)</label></formula><p>is an alignment model which scores how well the inputs around position j and the output at position i match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Decoder.</head><p>The Decoder aims to generate the target sequence y by sequentially predicting the probability of a word y i conditioned on the context vector c i and its previous generated words y 1 , ..., y i-1 , i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(y</head><formula xml:id="formula_6">i |y 1 , ..., y i-1 , x ) = д(y i-1 , h i , c i ) (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where д is used to estimate the probability of the word y i . The goal of the model is to minimize the cross-entropy, i.e., minimize the following objective function:</p><formula xml:id="formula_8">H (y) = - 1 N N i=1 n j=1 loдp(y (i ) j )<label>(7)</label></formula><p>where N is the total number of training instances, and n is the length of each target sequence. y (i ) j means the jth word in the ith instance. Through optimizing the objective function using optimization algorithms such as gradient descendant, the parameters can be estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Abstract Syntax Tree with SBT traversal</head><p>Translation between source code and NL is challenging due to the structure of source code. One simple way to model source code is to just view it as plain text. However, in such way, the structure information will be omitted, which will cause inaccuracies in the generated comments. To learn the semantic and syntactic information at the same time, we convert the ASTs into specially formatted sequences by traversing the ASTs. Sequences obtained by classical traversal methods (e.g., pre-order traversal) are lossy since the original ASTs cannot unambiguously be reconstructed back from them. This ambiguity may cause different Java methods (each with different comments) to be mapped to the same sequence representation. It is confusing for the neural network if there are multiple labels (in our setting, comments) given to a specific input. For addressing this problem, we propose a Structure-based Traversal (SBT) method to traverse the AST. The details are presented in Algorithm 1. Figure <ref type="figure" target="#fig_3">4</ref> illustrates a simple example of SBT to traverse a tree and the detailed procedure is as follows:</p><p>• From the root node, we first use a pair of brackets to represent the tree structure and put the root node itself behind the right bracket, that is (1)1, shown in seq ← ∅ seq is the sequence of a tree after traversal for c in childs do 8:</p><formula xml:id="formula_9">seq ← seq + SBT (c) 9:</formula><p>seq ← seq+)r Add right bracket for non-terminal nodes after traversing all their children 10:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>return seq</head><p>DeepCom processes each AST into a sequence following the SBT algorithm. For example, the AST sequence of the following Java method extracted from project Eclipse Che 2 is shown in Figure <ref type="figure" target="#fig_5">5</ref>: public String extractFor(Integer id){ LOG.debug("Extracting method with ID:{}", id); return requests.remove(id); }</p><p>The left part of Figure <ref type="figure" target="#fig_5">5</ref> is the AST of the method. The non-terminal nodes (those without boxes) illustrate the structural information of source code. They have the feature "type" which is a fixed set (e.g., IfStatement, Block, and ReturnStatement). The terminal nodes (those within boxes) not only have "type" but also have "value" (token within brackets). The "value" is the concrete token occurring in the source code and "type" indicates the type of the token. The right part of the figure is the sequence constructed by traversing the AST. The terminal nodes are represented by their "type" and "value" (connected by "_"), such as "log" is represented by "Sim-pleName_Log". The non-terminal nodes are represented by their "type". A subtree is included in a pair of brackets and we can restore the AST from the given sequence easily. In this way, we can keep the structural information and make the representation losslessthe original AST can be unambiguously reconstructed from the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Out-of-vocabulary tokens</head><p>Vocabulary is another challenge to model source code <ref type="bibr" target="#b15">[16]</ref>. In NL, studies usually limit vocabulary to the most common words (e.g., 2 https://github.com/eclipse/che top 30,000) during data processing. The out-of-vocabulary tokens are replaced by a special unknown token, e.g., UNK . It is effective for NLP because words outside vocabulary are so rare. However, this method is arguably inappropriate when it comes to source code.</p><formula xml:id="formula_10">&amp; ! $ &amp; ! ! $ "' " +" " &amp;$ $ $ &amp; ! " +" " " &amp; $ ! *"$ %% ! &amp; &amp; &amp; &amp; ! (! &amp; ! " ! " ' " *&amp;$ &amp; &amp; ! ) &amp; ,- " &amp;'$ &amp; &amp; &amp; &amp; ! (! &amp; ! " $ #' %&amp; " $ !( " " *&amp;$ &amp; !$ &amp; ! $ &amp; ! ! $ "' ! $ "' " +" " &amp;$ " &amp;$ " +" $ $ &amp; ! " +" " &amp; $ " &amp; $ " +" " " $ $ &amp; ! ! *"$ %% ! &amp; &amp; &amp; &amp; ! (! &amp; ! " " " ' " ' " *&amp;$ &amp; &amp; ! ) &amp; ,- " *&amp;$ &amp; &amp; ! ) &amp; ,- " " &amp; ! (! &amp; ! *"$ %% ! &amp; &amp; &amp; &amp;'$ &amp; &amp; &amp; &amp; ! (! &amp; ! " $ #' %&amp; " $ #' %&amp; " $ !( " $ !( " " &amp; ! (! &amp; ! &amp;'$ &amp; &amp; &amp; ! " *&amp;$ &amp; !$ " *&amp;$ &amp; !$ &amp; ! $ &amp; !</formula><p>In addition to fixed operators and keywords, there are user-defined identifiers which take up the majority of code tokens <ref type="bibr" target="#b6">[7]</ref>. These identifiers have a substantial influence on the vocabulary of language models. If we keep a regular vocabulary size for source code, there will be many unknown tokens. If we want the occurrences of UNK tokens to be as few as possible, the vocabulary size will increase a lot. A large vocabulary size will make it difficult to train a deep learning model since it requires more training data, time, and memory. To achieve optimal and stable results, models need to run a larger number of iterations to tune the parameters for each word in the vocabulary. Hence, we propose a new method to represent the out-of-vocabulary tokens for source code. In AST, the non-terminal nodes have "type" feature, and terminal nodes not only have "type" feature, but also have "value" feature. DeepCom takes the AST sequences as inputs, the vocabulary consists of brackets, all "type" of nodes (including non-terminal nodes T non and terminal nodes T term ), and partial type-value pairs of terminal tokens. We keep the tokens which appear in the most frequent 30,000 tokens as the AST sequences vocabulary. For the type-value pairs outside the vocabulary, Deep-Com uses their "type" T term instead of the UNK token to replace them. For example, for the terminal nodes "extractFor" and "id" in the code presented above, their types are both "SimpleName" as shown in Figure <ref type="figure" target="#fig_5">5</ref>. The tokens input into the model should be "Sim-pleName_extractFor" and "SimpleName_id" respectively. However, since the token "SimpleName_extractFor" is out of the vocabulary, we use its type "SimpleName" representing it instead. In this way, the out-of-vocabulary tokens are represented by their related type information instead of the meaningless word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>Then we use the Eclipse's JDT compiler <ref type="foot" target="#foot_2">3</ref> to parse the Java methods into ASTs and extract corresponding Javadoc comments which are standard comments for Java methods. The methods without Javadoc are omitted in this paper. For each method with a comment, we use the first sentence appeared in its Javadoc description as the comment since it typically describes the functionalities of Java methods according to Javadoc guidance <ref type="foot" target="#foot_3">4</ref> . Empty or just one-word descriptions are filtered out in this work because these comments have no ability to express the Java methods functionalities. We also exclude the setter, getter, constructor and test methods, since they are easy for a model to generate the comments. Finally, we get 588,108 Java method, comment pairs <ref type="foot" target="#foot_4">5</ref> . Similar to Jiang et al. <ref type="bibr" target="#b19">[20]</ref>'s work, we randomly select 80% of the pairs for training, 10% of the pairs for validation, and rest 10% for testing.</p><p>Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_2">2</ref> illustrate statistics of the corpus. We also give the details of methods lengths and comments lengths. The average lengths of Java methods and comments are 99.94 and 8.86 tokens in this corpus. We find that more than 95% code comments have no more than 50 words and about 90% Java methods no longer than 200 tokens.</p><p>During the training, the numerals and strings are replaced with generic tokens NUM and STR respectively. The maximum length of AST sequences is set to 400. We use a special symbol PAD to pad the shorter sequences and the longer sequences will be cut into sequences with 400 tokens. We add special tokens START and EOS to the decoder sequences during training. START is the start of the decoding sequence and the EOS means the end of it. The maximum comment length is set to 30. The vocabulary sizes for AST sequences and comments are both 30,000 in this paper. While there is no UNK in ASTs sequences, there are a few out-of-vocabulary tokens in comments that are replaced by UNK .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>The model is validated every 2,000 minibatches on the validation set by BLEU <ref type="bibr" target="#b30">[31]</ref> which is a commonly used automatic metric for NMT.</p><p>Training runs for about 50 epochs and we select the best model that has best results on the validation set as the final model. The model is then evaluated on the test set by computing average BLEU scores and the results will be discussed in Section 5. All models are implemented using the Tensorflow framework <ref type="foot" target="#foot_5">6</ref> and extended based on the Seq2Seq model in Tensorflow tutorials <ref type="foot" target="#foot_6">7</ref> . The parameters are shown as follows:</p><p>• The SGD (with minibatch size 100 randomly chosen from training instances) is used to train the parameters. • DeepCom uses two-layered LSTMs with 512 dimensions of the hidden states and 512-dimensional word embeddings. • The learning rate is set to 0.5 and we clip the gradients norm by 5. The learning rate is decayed using the rate 0.99. • To prevent over-fitting, we use dropout with 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Measure: BLEU-4</head><p>DeepCom uses machine translation evaluation metrics BLEU-4 score <ref type="bibr" target="#b30">[31]</ref> to measure the quality of generated comments. BLEU score is a widely-used accuracy measure for NMT <ref type="bibr" target="#b21">[22]</ref> and has been used in software tasks evaluation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. It calculates the similarity between the generated sequence and reference sequence (usually a human-written sequence). The BLEU score ranges from 1 to 100 as a percentage value. The higher the BLEU, the closer the candidate is to the reference. If the candidate is completely equal to the reference, the BLEU becomes 100%. Jiang et al. <ref type="bibr" target="#b19">[20]</ref> exploit it to evaluate the generated summaries for commit messages. Gu et al. <ref type="bibr" target="#b11">[12]</ref> use BLEU to evaluate the accuracy of generated API sequences from natural language queries. Their experiments show that BLEU score is reasonable to measure the accuracy of generated sequences.</p><p>It computes the n-gram precision of a candidate sequence to the reference. The score is computed as:</p><formula xml:id="formula_11">BLEU = BP • exp( N n=1 w n loдp n ) (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where p n is the ratio of length n subsequences in the candidate that are also in the reference. In this paper, we set N to 4, which is the maximum number of grams. BP is brevity penalty,</p><formula xml:id="formula_13">BP = 1 i f c &gt; r e (1-r /c ) i f c ≤ r (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where c is the length of the candidate translation and r is the effective reference sequence length.</p><p>In this paper, we regard a generated comment as a candidate and a programmer-written comment (extracted from Javadoc) as a reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In this section, we evaluate different approaches by measuring their accuracy on generating Java methods' comments. Specifically, we mainly focus on the following research questions:</p><p>• RQ1: How effective is DeepCom compared with the state-ofthe-art baseline? • RQ2: How effective is DeepCom to source code and comments of varying lengths?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">RQ1: DeepCom vs. Baseline</head><p>5.1.1 Baseline. We compare DeepCom with CODE-NN <ref type="bibr" target="#b18">[19]</ref> which is a state-of-the-art code summarization approach and also a deep learning based method. CODE-NN is an end-to-end generation We also compare DeepCom with its variants, that are, the basic Seq2Seq model, the attention based Seq2Seq model, and DeepCom with a classical traversal method (i.e., pre-order traversal). The Seq2Seq model and the attention based Seq2Seq model take the source code as inputs. They aim to evaluate the effectiveness of NMT approaches for comments generation. To evaluate the effectiveness of SBT, we compare SBT with one of the most ordinary traversal methods -pre-order traversal. In addition, we also compare DeepCom with CODE-NN on the dataset that CODE-NN uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results</head><p>. We measure the gap between automatically generated comments and human-written comments. The difference is evaluated by a machine translation metric, i.e., BLEU-4 score. Table <ref type="table" target="#tab_3">3</ref> illustrates the average BLEU-4 scores of different approaches to generating comments for Java methods. The accuracy of machine translation model Seq2Seq substantially outperforms CODE-NN. CODE-NN fails to learn the semantic of the source code when it generates comments from token embeddings of source code directly. Seq2Seq model exploits RNN to build a language model for the source code and effectively learns the semantics of Java methods. The BLEU-4 score increases further while integrating the structural information. Compared to DeepCom with the pre-order traversal, the SBT based model is much more capable of learning semantic and syntactic information within Java methods. In a word, the improvement of our proposed DeepCom (SBT) over CODE-NN is large. The average BLEU-4 score of DeepCom improves about 13% compared to CODE-NN. The results of DeepCom are comparable to the BLEU scores of state-of-the-art NMT models on natural language translation which are about 40% <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>We further conduct experiments on the same datasets CODE-NN used, which includes C# and SQL snippets collected from Stack Overflow. The results are shown in Table <ref type="table" target="#tab_4">4</ref>. Since many of the code snippets in their provided dataset are incomplete and hard to parse them into ASTs, we compare the Seq2Seq model with CODE-NN. It highlights that the Seq2Seq outperforms the state-of-the-art method CODE-NN in different languages. The average BLEU scores of Seq2Seq improve more than 10% on various program languages compared to CODE-NN.</p><p>Through the evaluation, we have verified that comments generation task is very similar to machine translation except that the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RQ2: BLEU-4 scores for source code and comments of different lengths</head><p>We further analyze the prediction accuracy for Java methods and comments of different lengths. Figure <ref type="figure" target="#fig_7">6</ref> presents the average BLEU-4 scores of DeepCom and CODE-NN for source code and ground truth comments of varying lengths. As Figure <ref type="figure" target="#fig_7">6</ref>(a) illustrates, the average BLEU-4 scores tend to be lower when we increase source code length. For most code lengths, the average BLEU-4 scores of DeepCom improve those of CODE-NN by about 10%. For DeepCom, AST lengths grow rapidly as the source code lengths increase and as a result, some features are lost when cutting the long AST sequences into a fixed length sequence during training. For comments of different lengths, DeepCom maintains similar accuracy as shown in Figure <ref type="figure" target="#fig_7">6</ref>(b). However, the accuracy of CODE-NN decreases sharply while code comment length increases. When the code comment lengths are greater than 25 tokens, the accuracy of CODE-NN decreases to less than 10%. DeepCom still performs better when we need to generate comments consisting of 25-28 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative analysis</head><p>Here, we perform qualitative analysis on the human-written comments and comments which are automatically generated by our approach. Table <ref type="table" target="#tab_5">5</ref> shows some examples of Java methods, the comments generated by DeepCom and human-written comments. By analyzing cases of generated results, we find the cases can be divided into the following situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Exactly correct comments.</head><p>DeepCom can generate exactly correct comments from the source code of different lengths (Case 1 and Case 2), which validate the capability of our approach to encode Java methods and decode comments. Generally, DeepCom  performs well when the business logic of these Java methods is clear and code conventions are universal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Algorithm implementations.</head><p>For the Java methods which are more concerned about algorithm than business logic, Deep-Com can generate accurate comments. The algorithm concerned Java methods usually use similar structures to implement the same algorithm function. As Case 5 shows, the method "sort" aims to sort an array using Binary Sort, DeepCom captures the correct functionality and generates the correct comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Cases when generated comments are better than humanwritten ones.</head><p>By analyzing the generated comments and the source code, we find that DeepCom performs better than human written comments when the Java methods aim to determine something true or not. Developers write interrogative sentences as comments sometimes (shown in Case 6 and Case 7). These comments are nonstandard even though they can express the functionalities of Java methods. DeepCom can not only generate accurate comments but also more standard comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">API invocations intensive Java methods.</head><p>Developers usually invoke APIs to implement a specific function. These APIs include platform standard APIs and customized APIs defined by third parties or developers themselves. We find that DeepCom can generate accurate comments when most API invocations are platform standard APIs (shown in Case 1). However, when the majority API invocations in a Java method are customized APIs, DeepCom does not perform as good as human-written comments (shown in Case 4 and Case 9). The influence of API invocations explains that Deep-Com can learn the platform standard APIs usage patterns from a large-scale dataset. However, it can not learn customized APIs well because the customized APIs with the same name have different usage patterns in different programs.</p><p>6.1.5 Low BLEU score cases. The results with lower BLEU scores are mainly divided into two types, meaningless sentences, and sentences with clear semantics. The former mainly contains empty sentences and results with too many repetitive words. We conjecture the problems come from out-of-vocabulary words in original comments or mismatch between the Java methods and comments in the original dataset.</p><p>In the latter ones, most of them are irrelevant to original comments in their semantics. There are also some interesting results that hold relevant semantics but gain low BLEU scores (shown in Case 4). The automatically generated and manual comments may describe similar functionalities but with different words or order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6">Unknown words in generated comments.</head><p>There are unknown words in the generated comments sometimes. As Case 3 shows, DeepCom fails to predict the token "FactoryConfigu-rationError" which is the method name defined by developers. DeepCom is not good at learning the method or identifiers names occurred in comments. Developers define various names while programming and most of these tokens appearing at most once in the comments. During the training process, we replaced all unknown identifier tokens in AST sequences with their types, but we do not replace the unknown identifiers occur in comments. It is hard for DeepCom to learn these user-defined tokens in comments that have been replaced by the unknown token UNK .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Strengths of DeepCom</head><p>A major challenge for generating comments from code is the semantic gap between code and natural language descriptions. Existing approaches are based on manually crafted templates or information retrieval and lack a model to capture the semantic relationship between source code and natural language. DeepCom, a machine translation model, has the ability to bridge the gap between two languages, i.e., programming language and natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Probabilistic model connecting semantics of code and comments.</head><p>One advantage of DeepCom is generating comments directly by learning source code instead of synthesizing comments from keywords or searching similar code snippets' comments.</p><p>Synthesizing comments from keywords usually uses some manually crafted templates. The procedure of templates definition is time-consuming and the quality of keywords depends on the quality of a given Java method. They fail to extract accurate keywords when the identifiers and methods are poorly named. The IR based approaches usually search the similar code snippets and take their comments as the final results. These IR based approaches rely on whether similar code snippets can be retrieved and how similar the snippets are.</p><p>DeepCom builds language models for code and natural language descriptions. The language models are able to handle the uncertainty in the correspondence between code and text. DeepCom learns common patterns from a large-scale source code and the encoder itself is a language model which remembers the likelihood of different Java methods. The decoder of DeepCom learns the context of source code which bridges the gap between natural language and code. Furthermore, the attention mechanism helps align code tokens and natural language words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Generation assisted by structural information.</head><p>Programming languages are formal languages which are more structure dense than text and have formal syntax and semantics. It is difficult for models to learn semantic and syntax information at the same time just given code sequences. Existing approaches usually analyze source code directly and omit its syntax representation.</p><p>In contrast to traditional NMT models, DeepCom takes advantage of rich and unambiguous code structures. In this way, Deep-Com bridges the gap between code and natural language with the assistance of structure information within the source code. From the evaluation results, we find that the structural information improves the quality of comments. The improvements for methods implementing standard algorithms are much more obvious. Java methods realizing the same algorithm may define different variables while their ASTs are much more similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Threats to Validity</head><p>We have identified the following threats to validity: Automatic evaluation metrics: We evaluate the gap between generated comments and human-written comments by machine translation metric BLEU which is gradually used in generativebased software issues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. The reason for this setting is that we want to reduce the impact of the subjectivity of manual evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of collected comments:</head><p>We collected the comments for Java methods from the first sentence of Javadoc as other work does <ref type="bibr" target="#b11">[12]</ref>. Although we define heuristic rules to decrease the noise in comments, there are some mismatched comments in the In the future, we will investigate a better technique to build a better parallel corpus.</p><p>Comparisons on Java dataset: Another threat to validity is that our approach is experimented on Java dataset. Although we fail to evaluate DeepCom directly on CODE-NN' dataset which is difficult to parse into ASTs, the results on Java have proved the effectiveness of DeepCom. In the future, we will extend our approach to other programming languages (e.g., Python).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK 7.1 Code Summarization</head><p>As a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. Automatic code summarization approaches vary from manuallycrafted template <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, IR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43]</ref> to learning-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Creating manually-crafted templates to generate code comments is one of the most common code summarization approaches. Sridhara et al. <ref type="bibr" target="#b34">[35]</ref> use the Software Word Usage Model (SWUM) to create a rule-based model that generates natural language descriptions for Java methods. Moreno et al. <ref type="bibr" target="#b24">[25]</ref> predefine heuristic rules to select information and generate comments for Java classes by combining the information. These rule-based approaches have been expanded to cover special types of code artifacts such as test cases <ref type="bibr" target="#b47">[48]</ref> and code changes <ref type="bibr" target="#b7">[8]</ref>. Human templates usually synthesize comments by extracting keywords from the given source code.</p><p>IR approaches are widely used in summary generation and usually search comments from similar code snippets. Haiduc et al. <ref type="bibr" target="#b14">[15]</ref> apply the Vector Space Model (VSM) and Latent Semantic Indexing (LSI) to generate term-based comments for classes and methods. Their works are replicated and expanded by Eddy et al. <ref type="bibr" target="#b10">[11]</ref> which exploit a hierarchical topic model. Wong et al. <ref type="bibr" target="#b41">[42]</ref> apply code clone detection techniques to find similar code snippets and use the comments from similar code snippets. The work is similar to their previous work AutoComment <ref type="bibr" target="#b42">[43]</ref> which mines human-written descriptions for automatic comment generation from Stack Overflow.</p><p>Recently, some studies try giving natural language summaries by deep learning approaches. Iyer et al. <ref type="bibr" target="#b18">[19]</ref> present RNN networks with attention to produce summaries that describe C# code snippets and SQL queries. It takes source code as plain text and models the conditional distribution of the summary. Allamanis et al. <ref type="bibr" target="#b3">[4]</ref> apply a neural convolutional attentional model to the problem that extremely summarizes the source code snippets into short, namelike summaries. These learning-based approaches mainly learn the latent features from source code, such as semantics, formatting, and etc. The comments are generated according to these learned features. The experimental results of them have proved the effectiveness of deep learning methods on code summarization. In this paper, DeepCom integrates the structure information which is verified important for comments generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Language models for source code</head><p>Recently, thanks to the insight of Hindle et al. <ref type="bibr" target="#b16">[17]</ref>, there is an emerging interest in building language models of source code. These language models vary from n-gram model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>, bimodal model <ref type="bibr" target="#b4">[5]</ref>, and RNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. Hindle et al. <ref type="bibr" target="#b16">[17]</ref> first propose to explore N-gram to model the source code and demonstrate that most software is also natural and find regularities in natural code. Some studies build the models to bridge the gap between the programming language and natural language descriptions. Allamanis et al. <ref type="bibr" target="#b0">[1]</ref> develop a framework to learn the code conventions of a codebase and the framework exploits N-gram model to name Java identifiers. Allamanis et al. <ref type="bibr" target="#b1">[2]</ref> and Raychev et al. <ref type="bibr" target="#b32">[33]</ref> suggest names for variables, methods, and classes. Mou et al. <ref type="bibr" target="#b25">[26]</ref> present a tree-based convolutional neural networks to model the source code and classify programs. Gu et al. <ref type="bibr" target="#b11">[12]</ref> present a classic encoder-decoder model to bridge the gap between the Java API sequences and natural language. Yin and Neubig <ref type="bibr" target="#b46">[47]</ref> build a data-driven syntax-based neural network model for generating code from natural language.</p><p>Learning from source code is applied to various software engineering tasks, e.g., fault detection <ref type="bibr" target="#b31">[32]</ref>, code completion <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>, code clone <ref type="bibr" target="#b37">[38]</ref> and code summarization <ref type="bibr" target="#b18">[19]</ref>. In this paper, we explore the combination of deep learning methods and source code features to generate code comments. Compared to the previous works, DeepCom explains the code summarization procedure from a machine translation perspective. The experimental results also prove the ability of DeepCom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper formulates code summarization task as a machine translation problem which translates source code written in a programming language to comments in natural language. We propose Deep-Com, an attention-based Seq2Seq model, to generate comments for Java methods. DeepCom takes ASTs sequences as input. These ASTs are converted to specially formatted sequences using a new structure-based traversal (SBT) method. SBT can express the structural information and keep the representation lossless at the same time. DeepCom outperforms the state-of-the-art approaches and achieves better results on the machine translation metric. In future work, we plan to improve the effectiveness of our proposed approach by introducing more domain-specific customizations. We also plan to apply our proposed approach to other software engineering tasks that can be mapped to a machine translation problem (e.g., code migration, etc.).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of basic RNN and LSTMSBT, a subtree under a given node is included into a pair of brackets. The brackets represent the structure of the AST and we can restore a tree unambiguously from a sequence generated using SBT.Moreover, to address the vocabulary challenge, we propose a new method to represent unknown tokens. The tokens in AST sequences include terminal nodes, non-terminal nodes, and brackets in our work. The unknown tokens come from the terminal tokens of ASTs. We replace the unknown tokens with their types instead of a universal special UNK token.DeepCom generates comments word-by-word from AST sequences. We train and evaluate DeepCom on the Java dataset that consists of 9,714 Java projects from GitHub. The experimental results show that DeepCom can generate informative comments. Additionally, the results show that DeepCom achieves the best performance when compared with a number of baselines including the state-of-the-art approach by Iyer et al.<ref type="bibr" target="#b18">[19]</ref>.The main contributions of this paper are as follows:• We formulate code comments generation task as a machine translation task. • We customize a sequence-based model to process structural information extracted from source code to generate comments for Java methods. In particular, we propose a new AST traversal method (namely structure-based traversal) and a domain-specific method to deal with out-of-vocabulary tokens better.</figDesc><graphic coords="2,451.70,108.35,75.50,51.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall framework of DeepCom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sequence-to-Sequence model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of sequencing an AST to a sequence by SBT. (For a number, the bold font number after bracket indicates node itself and the number in brackets denotes the tree structure by taking it as the root node.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4. • Next, we traverse the subtrees of the root node and put all root nodes of subtrees into the brackets, i.e., (1(2)2(3)3)1. • Recursively, we traverse each subtree until all nodes are traversed and the final sequence (1(2(4)4(5)5(6)6)2(3)3)1 is obtained. Algorithm 1 Structure-based Traversal 1: procedure SBT(r ) Traverse a tree from root r 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: AST of the Java method named extractFor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) BLEU-4 scores for different code lengths (b) BLEU-4 scores for different comment lengths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The average BLEU-4 scores of different lengths of code and comment in Java language. (We compare two methods DeepCom with SBT and CODE-NN)</figDesc><graphic coords="8,97.01,235.12,165.03,110.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Statistics for code snippets in our dataset</head><label>1</label><figDesc></figDesc><table><row><cell>#Methods</cell><cell>#All Tokens</cell><cell># All Identifiers</cell><cell># Unique Tokens</cell><cell>#Unique Identifiers</cell></row><row><cell>588,108</cell><cell cols="3">44,378,497 13,779,297 794,711</cell><cell>794,621</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Statistics for code lengths and comments lengths</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Code Lengths</cell></row><row><cell cols="4">Avg Mode Median &lt;100</cell><cell>&lt;150</cell><cell>&lt;200</cell></row><row><cell>99.94</cell><cell>16</cell><cell>65</cell><cell cols="2">68.63% 82.06% 89.00%</cell></row><row><cell></cell><cell></cell><cell cols="2">Comments Lengths</cell></row><row><cell cols="3">Avg Mode Median</cell><cell>&lt;20</cell><cell>&lt;30</cell><cell>&lt;50</cell></row><row><cell>8.86</cell><cell>8</cell><cell>13</cell><cell cols="2">75.50% 86.79% 95.45%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Evaluation results on Java methods</head><label>3</label><figDesc></figDesc><table><row><cell>Approaches</cell><cell>BLEU-4 score (%)</cell></row><row><cell>CODE-NN</cell><cell>25.30</cell></row><row><cell>Seq2Seq</cell><cell>34.87</cell></row><row><cell>Attention-based Seq2Seq</cell><cell>35.50</cell></row><row><cell>DeepCom (Pre-order)</cell><cell>36.01</cell></row><row><cell>DeepCom (SBT)</cell><cell>38.17</cell></row><row><cell cols="2">system to generate summaries for code snippets. It exploits an RNN</cell></row><row><cell cols="2">with attention to generate summaries by integrating the token</cell></row><row><cell cols="2">embeddings of source code instead of building language models for</cell></row><row><cell cols="2">source code. We do not use IR approaches as baselines, because the</cell></row><row><cell cols="2">results in CODE-NN has shown that CODE-NN outperforms the IR</cell></row><row><cell>based approaches.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 : Evaluation results on CODE-NN datasets including C# and SQL programming languages.</head><label>4</label><figDesc>DeepCom can generate more informative comments than the state-of-the-art method. Compared to the model without AST, the BLEU score of DeepCom increases to 38.17% and the BLEU-4 scores of about 38% of the instances are greater than 50%. We evaluate two traversal methods SBT and pre-order traversal. DeepCom with SBT performs better than traditional pre-order traversal. This is the case because SBT better preserves the structure of ASTs. Experimental results indicate that the structural information is important for translating text in structured languages to unstructured ones.</figDesc><table><row><cell cols="3">Language Approaches BLEU-4 score(%)</cell></row><row><cell>C#</cell><cell>CODE-NN Seq2Seq</cell><cell>20.4 30.00</cell></row><row><cell>SQL</cell><cell>CODE-NN Seq2Seq</cell><cell>17.0 30.94</cell></row><row><cell cols="3">structural information in source code needs to be taken into ac-</cell></row><row><cell>count.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 : Examples of generated comments by DeepCom. These samples are necessarily limited to short methods because of space limitations. AST structure is not shown in the table, because AST is much longer than source code.</head><label>5</label><figDesc>Automatically generated: Sorts the array in ascending order,using the natural order. Human-written: Rearranges the array in ascending order,using the natural order.</figDesc><table><row><cell>Case ID</cell><cell></cell><cell>Java method</cell><cell>Comments</cell></row><row><cell></cell><cell cols="2">public static byte[] bitmapToByte(Bitmap b){</cell><cell></cell></row><row><cell>1</cell><cell cols="2">ByteArrayOutputStream o = new ByteArrayOutputStream(); b.compress(Bitmap.CompressFormat.PNG,100,o); return o.toByteArray();</cell><cell>Automatically generated: convert Bitmap to byte array Human-written: convert Bitmap to byte array</cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell cols="3">private static void addDefaultProfile(SpringApplication app,</cell><cell></cell></row><row><cell></cell><cell cols="2">SimpleCommandLinePropertySource source){</cell><cell></cell></row><row><cell></cell><cell cols="2">if(!source.containsProperty("spring.profiles.active")</cell><cell>Automatically generated: If no profile has been configured , set by default the "dev"</cell></row><row><cell>2</cell><cell cols="2">&amp;&amp;!System.getenv().containsKey("SPRING_PROFILES_ACTIVE")){</cell><cell>profile.</cell></row><row><cell></cell><cell cols="2">app.setAdditionalProfiles(Constants.SPRING_PROFILE_DEVELOPMENT);</cell><cell>Human-written: If no profile has been configured , set by default the "dev" profile.</cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">public FactoryConfigurationError(Exception e){</cell><cell>Automatically generated: Create a new UNK with a given Exception cause</cell></row><row><cell>3</cell><cell>}</cell><cell>super(e.toString()); this.exception=e;</cell><cell>of the error. Human-written: Create a new FactoryConfigurationError with a given Exception base cause of the error.</cell></row><row><cell></cell><cell cols="2">protected void createItemsLayout(){</cell><cell></cell></row><row><cell></cell><cell cols="2">if (mItemsLayout == null){</cell><cell></cell></row><row><cell>4</cell><cell></cell><cell>mItemsLayout=new LinearLayout(getContext()); mItemsLayout.setOrientation(LinearLayout.VERTICAL);</cell><cell>Automatically generated: Creates item layouts if any parameters Human-written: Creates item layout if necessary</cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">public static void sort(Comparable[] a){</cell><cell></cell></row><row><cell></cell><cell></cell><cell>int n=a.length;</cell><cell></cell></row><row><cell></cell><cell></cell><cell>for (int i=1; i &lt; n; i++){</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Comparable v=a[i];</cell><cell></cell></row><row><cell>5</cell><cell></cell><cell>int lo=0, hi=i; while (lo &lt; hi) { ... }</cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell></row><row><cell></cell><cell></cell><cell>}</cell><cell></cell></row><row><cell></cell><cell></cell><cell>assert isSorted(a);</cell><cell></cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell></cell><cell>public boolean isEmpty(){ return root == null; }</cell><cell>Automatically generated: Returns true if the symbol is empty. Human-written: Is this symbol table empty?</cell></row><row><cell></cell><cell></cell><cell>public boolean contains(int key){</cell><cell>Automatically generated: Checks whether the given object is contained within the</cell></row><row><cell>7</cell><cell></cell><cell>return rank(key) != -1;</cell><cell>given set.</cell></row><row><cell></cell><cell></cell><cell>}</cell><cell>Human-written: Is the key in this set of integers?</cell></row><row><cell cols="3">public void tag(String inputFileName,String outputFileName,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>OutputFormat outputFormat){</cell><cell>Automatically generated: Replaces the message with a given tag</cell></row><row><cell>8</cell><cell cols="2">List&lt;String&gt; sentences=jsc.textFile(inputFileName).collect();</cell><cell>Human-written: Tags a text file , each sentence in a line and writes the result to</cell></row><row><cell></cell><cell cols="2">tag(sentences,outputFileName,outputFormat);</cell><cell>an output file with a desired output format.</cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">public void unlisten(String pattern){</cell><cell></cell></row><row><cell></cell><cell cols="2">UtilListener listener=listeners.get(pattern);</cell><cell></cell></row><row><cell>9</cell><cell cols="2">if(listener!=null){ listener.destroy(); listeners.remove(pattern); }else{</cell><cell>Automatically generated: It can be called when the product only or refresh has ended. Human-written: Removes a listener that was previously registered with listenFor-Subscriptions.</cell></row><row><cell></cell><cell cols="2">client.onError(Topic.RECORD,Event.NOT_LISTENING,pattern);</cell><cell></cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row><row><cell></cell><cell>}</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.tiobe.com/tiobe-index/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="200" xml:id="foot_1"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.eclipse.org/jdt/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.oracle.com/technetwork/articles/java/index-137868.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Data is available at https://github.com/huxingfree/DeepCom</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://www.tensorflow.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/tensorflow/nmt</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201, and the National Natural Science Foundation of China under Grant Nos.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning natural coding conventions</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="281" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Suggesting accurate method and class names</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06182</idno>
		<title level="m">A Survey of Machine Learning for Big Code and Naturalness</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bimodal modelling of source code and natural language</title>
		<author>
			<persName><forename type="first">Miltos</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2123" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A holistic approach to software quality at work</title>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Broy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Deißenböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pizka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd World Congress for Software Quality (3WCSQ)</title>
		<meeting>3rd World Congress for Software Quality (3WCSQ)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatically documenting program changes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Westley</forename><forename type="middle">R</forename><surname>Buse</surname></persName>
		</author>
		<author>
			<persName><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM international conference on Automated software engineering</title>
		<meeting>the IEEE/ACM international conference on Automated software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large scale language modeling in automatic speech recognition</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Shugrina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.8440</idno>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating source code summarization techniques: Replication and expansion</title>
		<author>
			<persName><forename type="first">Brian P</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Carver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Program Comprehension (ICPC), 2013 IEEE 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep API learning</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07734</idno>
		<title level="m">DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supporting program comprehension with source code summarization</title>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jairo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume</title>
		<meeting>the 32nd ACM/IEEE International Conference on Software Engineering-Volume</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="223" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the use of automated text summarization techniques for summarizing source code</title>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jairo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reverse Engineering (WCRE), 2010 17th Working Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering (ICSE), 2012 34th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarizing Source Code using a Neural Attention Model</title>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatically generating commit messages from diffs using neural machine translation</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Armaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 32nd IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zeroshot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<title level="m">Opennmt: Open-source toolkit for neural machine translation</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04856</idno>
		<title level="m">A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic documentation generation via source code summarization of method context</title>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Mcburney</surname></persName>
		</author>
		<author>
			<persName><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Program Comprehension</title>
		<meeting>the 22nd International Conference on Program Comprehension</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic generation of natural language summaries for java classes</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jairo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giriprasad</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Program Comprehension (ICPC)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks over Tree Structures for Programming Language Processing</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On end-to-end program generation from user intention by deep neural networks</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07211</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Movshovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Attias</forename></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A statistical semantic language model for source code</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation (t)</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the naturalness of buggy code</title>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saheel</forename><surname>Godhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bacchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering</title>
		<meeting>the 38th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting program properties from big code</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="111" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">A neural attention model for abstractive sentence summarization</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards automatically generating summary comments for java methods</title>
		<author>
			<persName><forename type="first">Giriprasad</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM international conference on Automated software engineering</title>
		<meeting>the IEEE/ACM international conference on Automated software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatically detecting and describing high level actions within methods</title>
		<author>
			<persName><forename type="first">Giriprasad</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Software Engineering</title>
		<meeting>the 33rd International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Machine Learning Based Approach for Evaluating Clone Detection Tools for a Generalized and Accurate Precision</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Svajlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chanchal</surname></persName>
		</author>
		<author>
			<persName><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Software Engineering and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1399" to="1429" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatically learning semantic features for defect prediction</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering</title>
		<meeting>the 38th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning code fragments for code clone detection</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Vendome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 31st IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clocom: Mining existing source code for automatic comment generation</title>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Analysis, Evolution and Reengineering (SANER), 2015 IEEE 22nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="380" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Autocomment: Mining question and answer sites for automatic comment generation</title>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinqiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 28th IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Measuring program comprehension: A large-scale field study with professionals</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01337</idno>
		<title level="m">Neural generative question answering</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<title level="m">A Syntactic Neural Model for General-Purpose Code Generation</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automated documentation inference to explain failed tests</title>
		<author>
			<persName><forename type="first">Sai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 2011 26th IEEE/ACM International Conference on Automated Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
