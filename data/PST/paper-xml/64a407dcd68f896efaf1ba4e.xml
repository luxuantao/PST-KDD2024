<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Characterization of the Effects of Software Instruction Prefetching on an Aggressive Front-end</title>
				<funder>
					<orgName type="full">ARM</orgName>
				</funder>
				<funder>
					<orgName type="full">Texas A&amp;M High Performance Research Computing</orgName>
				</funder>
				<funder ref="#_AFpQF7n #_gymFqEx #_QtMsxRw">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gino</forename><surname>Chacon</surname></persName>
							<email>ginochacon@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Gober</surname></persName>
							<email>ngober@tamu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
							<email>krishnendra.nathella@arm.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
							<email>pgratz@gratz1.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
							<email>djimenez@acm.org</email>
							<affiliation key="aff4">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Characterization of the Effects of Software Instruction Prefetching on an Aggressive Front-end</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ISPASS57527.2023.00015</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Growing application sizes continue to strain the memory system. As more complex applications are developed, and the instruction memory footprint increases, the cache hierarchy cannot contain relevant instructions causing the front end to become idle as it awaits fetched instructions. Hardware instruction prefetchers can alleviate this problem by learning instruction stream behavior and prefetching instructions into the cache before use. Fetch Directed Prefetching (FDP) is a ubiquitous form of hardware instruction prefetching that uses branch predictor to predict future instruction cache references. Modern processors generally implement aggressive, deep FDP to decouple the front-end from the rest of the machine.</p><p>Instruction accesses, however, provide a small amount of information over long periods due to instruction stream variability and lack of information regarding the context of instruction accesses. Capturing the instruction stream's context requires significant storage overhead to correlate instruction accesses and ensure timely accesses. Software prefetching techniques overcome this problem by profiling and statically analyzing an application's behavior. Prior work demonstrates software instruction prefetching's potential performance benefit but does not evaluate performance in the context of aggressive, decoupled front-ends. While software prefetching provides ?20% improvement in conservative front-ends, we find that it does not yield performance benefit when modeling a baseline with an aggressive FDP, in some cases hurting performance.</p><p>Our analysis finds that using software instruction prefetching negatively impacts an aggressive front-end's behavior. We investigate this finding and characterize the different states a front-end can be in and how introducing instructions into an application can change the front-end's behavior resulting in destructive interference with the software prefetcher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The Memory Wall <ref type="bibr" target="#b0">[1]</ref> manifests in the superscalar core frontend as a lack of instructions available for fetch, as the frontend's resources are encumbered by waiting for the memory hierarchy to service instruction requests <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Recent work demonstrates that the front-end has become a significant bottleneck for modern server workloads due to the availability of instructions <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. This bottleneck results from increased instruction memory footprints in server workloads with everdeeper software stacks. Large footprints further exacerbate the disparity between processor and memory performance as modern L1 instruction (L1-I) caches cannot capture an application's behavior.</p><p>A large body of work has emerged to explore different mechanisms to improve instruction memory performance to address this performance bottleneck. Fetch-Directed Prefetching (FDP) <ref type="bibr" target="#b6">[7]</ref> is an essential form of instruction prefetching, heavily used in current, aggressive, superscalar processor cores <ref type="bibr" target="#b7">[8]</ref>. FDP speculates using the branch prediction structures to aggressively fill the Fetch Target Queue (FTQ) ahead of the instruction stream with the predicted execution path. Entries in the FTQ are sent to the L1-I as soon as their addresses are known to fill the cache ahead of the demand request. The larger the FTQ, the earlier requests can be issued to the L1-I, increasing instruction throughput. An essential aspect of FDP is that it allows decoupling the front-end from the rest of the processor, allowing the front-end to run as far ahead as the branch predictor allows by holding multiple outstanding instruction requests to the L1-I in the FTQ <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Recent works alleviating the front-end bottleneck propose using software instruction prefetching techniques to profile a workload's behavior and identify accesses with a high impact on performance <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, prefetching these high-impact memory accesses ahead of time. Profiling ahead of execution allows hardware to have prior knowledge of which accesses are compulsory or have long-term reuse without obvious short-term locality. These techniques do not require hardware overhead other than ISA support for a prefetching instruction. Software instruction prefetching is an attractive solution as it can improve production binaries that can be modified and updated. Recent software instruction prefetching works demonstrate high potential, such as the state-of-theart AsmDB <ref type="bibr" target="#b12">[13]</ref> prefetcher, seeing benefits as high as ?15% performance improvement.</p><p>Despite software instruction prefetching's potential benefit, prior work in this area evaluates their proposals in systems without FDP or a very conservative FDP implementation. Recent work emphasizes the importance of modeling a realistic FDP implementation to evaluate a proposal <ref type="bibr" target="#b17">[18]</ref>   A conservative FDP implementation limits its ability to run ahead of the instruction stream, inflating an L1-I prefetcher's reported benefit <ref type="bibr" target="#b17">[18]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows AsmDB's performance improvement in Instructions-per-Cycle (IPC) in a conservative fetch environment compared to the IPC of a conservative front-end with no prefetching. We implement AsmDB to profile and insert prefetches into a trace-based simulator's trace and evaluate its performance over a conservative front-end implementing FDP with a 2-entry FTQ. We do not include the additional instructions AsmDB inserts when calculating its IPC. In the figure, we evaluate 48 workloads from the 1st Value Prediction Championship (CVP1) using ChampSim to explore the AsmDB's benefits alongside an industry-standard frontend <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In a conservative front-end, similar to that used in AsmDB's original evaluation, we find that AsmDB improves performance by roughly 20% geomean. The figure also shows the impact of increasing the FTQ's depth to 24 entries (192, 32-bit instructions) to represent modern front-end designs accurately. FDP improves ?41% over the conservative 2-entry FTQ implementation and outperforms AsmDB on a conservative front-end by ?20%. Implementing AsmDB with a larger FTQ does not yield significant performance benefits on average and sometimes degrades performance. This result is counterintuitive, as by its design, AsmDB should capture performance benefits that FDP cannot cover ahead of its demand fetch. The figure also shows the impact of removing the additional instructions AsmDB inserts into the simulator's trace, allowing it to prefetch instructions at no cost. In this case, AsmDB and an industry-standard FDP improve performance to ?49% over the conservative baseline (?9% over an aggressive FDP). The conservative FDP combined with AsmDB benefits from removing software prefetch instructions' overhead, but the industry-standard FDP sees no benefit unless the overhead is removed. This raises the question: why does FDP substantially impact the benefit of software instruction prefetching?</p><p>Here, we deeply examine the front-end's behavior in conservative and industry-standard FDP scenarios to understand how to implement better software prefetching mechanisms in future machines. Specifically, we identify three scenarios the front-end may be experiencing and how introducing software prefetches can negatively impact front-end performance by increasing the occurrence of specific scenarios. We then discuss potential optimizations to software prefetching that can reduce the overhead of inserting software prefetches and direct future design efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contributions</head><p>In this work, we characterize the side-effects of decoupled front-ends in the context of software instruction prefetching. We evaluate state-of-the-art software prefetching techniques in a simulation environment reflecting a realistic decoupled front-end and explore potential solutions.</p><p>? We characterize the front-end stalling behavior in a conservative vs. an industry-standard front-end. ? Evaluation of a state-of-the-art software instruction prefetcher in an industry-standard frontend. ? Discussion of future areas of research to address the growing front-end bottleneck.</p><p>II. BACKGROUND AND MOTIVATION The concepts of a decoupled front-end and recent software instruction prefetching proposals are closely related yet distinct. Here, we discuss each of them and the specific front-end implementation we use to evaluate software instruction prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decoupled Front-Ends</head><p>Reinmann et al. <ref type="bibr" target="#b6">[7]</ref> originally proposed FDP to decouple large modern branch target buffers (BTB) and highly accurate branch predictors from a core's instruction fetch logic. The decoupling discussed in this design refers to isolating the front-end from downstream execution elements, permitting the fetch unit to run independently of execution. However, execution will correct the fetch unit's mispredictions. By decoupling these structures, the fetch elements can aggressively run ahead and populate the Fetch Target Queue (FTQ) with speculative instructions, as shown in Figure <ref type="figure">2</ref> Fig. <ref type="figure">2</ref>: Overview of FDP implementation and optimizations from Ishii et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>a dedicated buffer containing information about speculative fetches directed by the branch prediction structures. Each entry in the FTQ represents varying basic block sizes, allowing one entry to represent eight instructions. FDP fetches the cache line addresses in the FTQ representing the start of a basic block and fills the L1-I before the demand request, regardless of their position in the FTQ. This allows fetches to the L1-I to occur out-of-order, but instructions must move to the decoder in-order to preserve the instructions' ordering. Entries in the FTQ pointing to the same basic block only require a single request to the L1-I, with larger FTQs allowing more aliasing and reducing requests to the cache. As entries become available in the FTQ, the BTB and branch predictors generate new fetch addresses to populate the FTQ. Figure <ref type="figure">2</ref> illustrates the branch structures FDP relies on: the Return Address Stack (RAS), indirect branch predictor, BTB, and branch direction predictor collectively speculate on future instructions. The branch predictors must continuously feed the FTQ new instruction addresses to fully leverage the decoupled front-end's benefits. These predictors rely on previously seen branch behavior to determine the future behavior of a particular branch. In particular, they maintain a Global History Register (GHR) to track the predicted outcomes of each branch. An imprecise GHR can heavily affect the predictors' ability to speculate on future instructions.</p><p>We focus our study on a recent FDP-based design that introduces two improvements to address the challenges of an industry-standard FDP <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The first minimizes the noise from running ahead by preventing the GHR from tracking not-taken branches that miss in the BTB since they do not appear as branches but rather as sequential instruction accesses. By limiting the taken branch history, the GHR may be updated with the new branch information. The GHR can then be flushed and updated to represent the branch history accurately.</p><p>The second improvement extends previously proposed Post-Fetch Correction (PFC) <ref type="bibr" target="#b19">[20]</ref> to allow the information leading to incorrect fetches to be corrected once the branch has resolved. The branch results are compared to the information stored in the GHR, checking if a branch's outcome was correctly predicted. If not, the FTQ is flushed, the GHR is corrected, and prefetching continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AsmDB: Modern Software Instruction Prefetching</head><p>Software prefetching has recently resurfaced as a potential technique to alleviate the front-end bottleneck <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In general, software instruction prefetching techniques follow the general steps of (1) execute and gather information, (2) generate a profile, (3) modify the target binary, (4) rerun binary with software instruction prefetching.</p><p>Collecting statistics about different basic blocks' behaviors allows profiling techniques to generate a control flow graph (CFG) of an application's execution. The software profile recreates the CFG to identify instruction behavior that impacts performance. This work focuses on a contemporary state-ofthe-art software prefetching technique, Assembly-Database (AsmDB) <ref type="bibr" target="#b12">[13]</ref>. AsmDB targets warehouse-scale applications, which prior work has shown to suffer from the front-end bottleneck problem, with front-end stalls accounting for 15-30% of pipeline stalls <ref type="bibr" target="#b5">[6]</ref>. AsmDB profiles the target application to examine the program's control-flow behavior and the program's high-impact misses, traverses the CFG and selects insertion sites for the software instruction prefetches based on the likelihood of a particular path leading to the target miss, and reassembles the program with software instruction prefetches at the selected insertion sites. Below we discuss the details of AsmDB's criteria for targeting particular instructions and selecting the most appropriate insertion site for a software prefetch. 1) Selecting High-Impact Instructions: AsmDB's profiling stage gathers information about an application's instruction stream behavior using the Intel processors' Last-Branch-Record (LBR) hardware. The collected data allows AsmDB to track high L1-I miss rate instructions' location within the CFG, representing basic blocks as nodes and branches as edges.</p><p>Once AsmDB establishes the CFG, it generates an ordered list of potential prefetch targets by ranking the instructions based on their misses. The highest-ranked instruction candidates are selected for prefetch insertion. AsmDB prioritizes instructions with high miss rates, assuming these instructions contribute the most to stalling the front-end.</p><p>2) Inserting Software Instruction Prefetches: Once highimpact misses are selected to target for prefetching, AsmDB traverses the CFG backward from a target, identifying paths leading to it. AsmDB requires the insertion site to be a minimum distance from the miss to ensure prefetches fill before the demand. AsmDB approximates distance by multiplying an application's Instructions Per Cycle (IPC) by the LLC's access latency. The distance is each instruction's worst-case fetch latency, giving AsmDB a notion of the minimum instructions ahead of a miss to insert the prefetch to cover its fetch latency successfully. The window is the maximum number of instructions away AsmDB should insert a prefetch. Figure <ref type="figure" target="#fig_1">3</ref> is an example of the CFG analysis where the node C is not the minimum distance away or a suitable insertion location. Nodes A and B are within the window and the minimum distance, making them potential insertion sites.</p><p>AsmDB considers an insertion site based on what fraction of the succeeding paths include the target instruction within the window, or fanout. Fanout directs the prefetch insertion aggressiveness since higher fanout sites are less likely to lead to the target miss. Increasing AsmDB's fanout threshold decreases its accuracy but results in higher miss coverage.</p><p>3) AsmDB and Industry-Standard Decoupled Front-ends: Applying software instruction prefetching requires the application to execute at least once to generate a profile of its instruction stream behavior, with further executions possibly improving an application's profile accuracy. Regardless of the CFG model's accuracy, software instruction prefetching cannot receive feedback about its predictions during execution. Modern machines, especially in the context of servers, use an out-of-order processor paradigm, resulting in nondeterministic behavior between different executions of the same application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHARACTERIZING FRONT-END BEHAVIOR</head><p>The front-end acts as the forward engine of modern processors attempting to fetch, decode, and issue instructions at a high throughput to drive execution continuously. As described in Sec. II-A, a modern decoupled front-end generally has a form of FDP. We find that interacting with an industry-standard FDP nullifies software instruction prefetching's benefit. This work does not intend to maximize FDP's performance, rather we present a taxonomy of the possible front-end states and investigate how software instruction prefetching affects that state, particularly the interaction between the FDP state and additional prefetch instructions in an application. In this section, we examine the three possible front-end states, the causes of each state, and its potential performance penalty. Specifically, we look at the outcome if particular FTQ entries stall in a conservative FDP with a 2-entry FTQ and an industrystandard FDP with a 24-entry FTQ, and software instruction prefetching's influence on the FTQ's state and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scenario 1: Shoot Through</head><p>Scenario 1 is the ideal FTQ state, where every FTQ entry has completed its fetch, and all instructions are available for decoding. The front-end bottleneck is nonexistent here, and the fetch bandwidth depends only on the decode stage's available bandwidth. In the conservative FDP, shown in Figure <ref type="figure" target="#fig_3">4a</ref>, both FTQ entries are ready for decoding. The low number of FTQ entries limits this scenario's benefit. Figure <ref type="figure" target="#fig_3">4b</ref> illustrates  Scenario 1 for the industry-standard FDP implementation in which each FTQ entry has completed its fetch and is available for decoding, limited only by the decode stage's bandwidth.</p><p>In this scenario, the instruction prefetcher ideally fills the FTQ, prefetching all entries in the FTQ before they can incur stalls. If this scenario is common, it may motivate prefetch designs to have higher coverage without consideration for the cost of redundant prefetches as they attempt to cover as many upcoming instructions as possible.</p><p>Fetch Target Queue (FTQ) Instr. Fetch Decode (a) Scenario 2 in a conservative pipeline. A single FTQ entry stalls the head (orange) of the FTQ while the following instruction (green) is ready to be sent to decode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instr. Fetch</head><p>Fetch Target Queue (FTQ) Decode (b) Scenario 2 in an industrystandrard pipeline, where the head instruction is still waiting for fetch to complete, resulting in potentially 23 FTQ entries to stall as a result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scenario 2: Stalling Head Instruction</head><p>FDP can issue FTQ entries to the L1-I in any order but must send instructions to the decoder in program order. In the next state, the head FTQ entry waits for its cache line to be fetched while the succeeding entries have completed fetch. For a conservative FDP implementation, shown in Figure <ref type="figure" target="#fig_4">5a</ref>, a single fetched FTQ entry must stall until the head has received its cache line. In the industry-standard FDP implementation shown in Figure <ref type="figure" target="#fig_4">5b</ref>, the head instruction causes up to 23 other FTQ entries to stall after completing their fetch, limiting the achievable fetch throughput. This scenario is the most commonly thought-of manifestation of the front-end bottleneck. The performance penalty depends directly on how long it takes for the head instruction to complete its fetch: a low-latency stall has a lower penalty than a request that misses the last-level cache. Despite this bottleneck, FTQ allows entries with a fetch latency lower than the head entry to be fetched before moving to the head of the FTQ.</p><p>As discussed in Sec. II-A, an entry in an FTQ design represents a basic block of up to eight instructions. The decoder's bandwidth may outpace fetch if multiple small basic blocks occupy the queue. Furthermore, FDP may stall to wait for a branch misprediction or BTB miss to resolve, allowing FTQ to issue its contents to the decoder before fetch resumes. As a result, a fetch entry can occupy the head entry for the entirety of its fetch latency, potentially causing a significant loss in potential performance due to halted throughput.</p><p>An ideal prefetcher would identify the head entry as the source of the performance bottleneck and prefetch it as early as possible, converting Scenario 2 into Scenario 1. Since previously proposed software instruction prefetchers operate by targeting instructions with high miss rates, they only target these bottleneck-critical instructions if they have a high miss rate. Software instruction prefetching introduces new instructions into the instruction stream, possibly incurring additional misses, leading to this scenario more frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scenario 3: Shadow Stalls</head><p>A more complex version of Scenario 2 occurs when multiple stalling entries follow a stalling head entry, and the head's fetch latency does not entirely cover the subsequent entries' latency. In the conservative, 2-entry FDP in Figure <ref type="figure" target="#fig_6">6a</ref>, both entries wait for fetch to complete. The head entry has a shorter fetch latency than the following entry. When it completes its fetch, it promotes to decode. The following entry moves to the head, and the FTQ continues stalling. The performance penalty in the 2-entry FDP depends on the difference in instruction latencies, possibly stalling the following instruction cache line inserted into the FTQ.</p><p>In a deeper FTQ, with an example shown in Figure <ref type="figure" target="#fig_6">6b</ref>, the head entry hides only part of the subsequent entries' latency. When it completes, multiple completed FTQ entries can promote to decode up to an uncompleted entry whose latency the head entry does not cover. This scenario's performance penalty varies based on the throughput recovered by entries with fetch latencies the head entry's latency covers.</p><p>A prefetcher can mitigate this scenario by prefetching all instructions in the FTQ ahead of their transition to the FTQ's head. The two stalling entries' impact varies based on their fetch latency and can change during execution, making it difficult for a prefetcher to identify this behavior. A software instruction prefetcher may identify both instructions due to their high miss rates. However, the inserted instructions may stall, which does not remove this scenario. They may also change the spacing between stalling entries, further hiding the latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FTQ State and Software Instruction Prefetches</head><p>Ideally, the FTQ would always be in Scenario 1, where entries are always available to move to the decode stage. Prefetching attempts to reduce the incidence of Scenarios 2 and 3 by prefetching entries that would stall at the FTQ's head.</p><p>In conservative front-ends, the number of entries structurally limits the FTQ's throughput. Mitigating any stall cycles at the head of the FTQ results in increased performance. Introducing new instructions into the instruction stream can, at most, stall both FTQ entries and has a low performance penalty as it covers stalls that will occur later in the program.</p><p>In contrast, a deep, industry-standard FDP is much more affected by inserting additional instructions, increasing the chances for Scenarios 2 and 3. Due to the larger number of entries, it is unlikely that a software instruction prefetching scheme can amortize the overhead of the inserted instructions by covering other stalls. Furthermore, introducing new instructions can drastically change the application's miss profile and the impact of particular misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>We evaluate prior hardware and instruction prefetchers using ChampSim <ref type="bibr" target="#b21">[22]</ref>. ChampSim is a trace-based simulator commonly used to evaluate prefetching techniques. We use a modified version of ChampSim implementing the decoupled front-end described in <ref type="bibr" target="#b17">[18]</ref>, modeling FDP. Our system configuration, shown in Table <ref type="table" target="#tab_3">I</ref>, is analogous to a modern Sunny Cove core. We select ChampSim for evaluation for two reasons: First, we can easily compare our results to the performance of FDP shown in recent works <ref type="bibr" target="#b17">[18]</ref>, and second, we find that other well-known simulation platforms <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> do not model an industry-standard front-end. We evaluate using a 48-trace subset of the traces used in the First Value Prediction Championship (CVP1). The selected traces have large instruction working sets, resulting in MPKI's ranging from ?2 to ?28 MPKI. For readability, we present our results with the workloads numbered 1-48 corresponding to the names shown in Figure <ref type="figure" target="#fig_0">1</ref>. The selected traces see an average of 25.5 L1-I misses per thousand instructions (MPKI) for a 24-entry FTQ, suggesting there is potential performance gain from additional instruction prefetching. We simulate 100 million instructions for each workload for profiling, analysis, and evaluation.</p><p>We model AsmDB based on its description in the original work and tune it to ensure the results reported represent AsmDB's performance. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our model's performance is similar to the original results. While a realworld system would insert software instruction prefetches into a target binary, we generate instruction traces from ChampSim containing the basic blocks' behavioral information, including how long they stall the front-end and when a miss occurs at the L1-I. We use these results to recreate the application's CFG. AsmDB traverses the CFG and generates a new ChampSim trace with inserted prefetches at the end of basic blocks that lead to the high-impact instructions, shifting instruction address to simulate the induced front-end, and L1-I pressure.</p><p>Additionally, we modify ChampSim to recognize software instruction prefetches by treating them as any other instruction request inserted into the decoupled frontend's FTQ, and then fetched from the L1-I cache. Once fetched, we assume a pre-decoder identifies the prefetch instruction and triggers a prefetche for the target instruction.</p><p>We also evaluate AsmDB's idealized performance benefit by ignoring overhead from the software instruction prefetches. Each prefetch issues on a triggering PC, but the prefetch instruction is not inserted into the front-end. AsmDB's performance with no insertion overhead provides the benefits of prefetching without the cost of interacting with FDP in the front-end.     <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref> evaluating the efficacy of hardware prefetchers in a decoupled front-end environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. FRONT-END ANALYSIS</head><p>This section evaluates AsmDB's performance, identifying its percentage increase in instructions, or code bloat and how these additional instructions affect the front-end's performance and behavior. We compare AsmDB's software prefetch instructions effect on a conservative (2-Entry FTQ) and industry-standard FDP (24-Entry FTQ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Code Bloat</head><p>To be performant, AsmDB must target many misses and allow for high fanout insertion points <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Each miss targeted for prefetching requires inserting a software prefetching instruction into the binary, increasing its static size. Adding additional instructions shifts the instruction addresses within the binary, shifting the cache lines' contents. AsmDB  accounts for this shift during prefetch generation, but changing cache lines' contents may change which cache lines stall the FTQ. Figure <ref type="figure" target="#fig_7">7a</ref> shows the increase in the program's size, called the static code bloat. Figure <ref type="figure" target="#fig_7">7b</ref> shows the increased number of instructions executed due to the inserted prefetches referred to as dynamic code bloat. Each software instruction prefetch can be executed multiple times throughout execution, resulting in higher dynamic code bloat than static code bloat. Generally, minimizing static and dynamic code bloat reduces prefetch overhead. However, applications with large instruction footprints and many high-impact misses, that FDP does not cover, require more prefetches to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Changes in Stalling Head FTQ Entries</head><p>Comparing the average number of cycles to fetch the head FTQ entry to the number of cycles to cover an entry not at the head of the FTQ in Figure <ref type="figure">8</ref>, we find that stalling head entries tend to have higher latencies. Comparing the average fetch   Fig. <ref type="figure">8</ref>: The number of cycles to cover a head instruction tends to be larger versus an FTQ entry not at the head, indicating that the head of the FTQ tends to be a miss in the L1-I. times between the 24-entry and 2-entry FDPs in Figures <ref type="figure">8a</ref> and<ref type="figure">8b</ref>, we observe that a deeper FTQ has longer fetch times.</p><p>Since the FTQ merges requests to the same cache line (e.g. due to loops), a deeper FTQ has more opportunities for positive aliasing between entries. We find that the 24-entry FDP experiences ?14% less L1-I accesses than the 2-entry FDP on average. The remaining FTQ entries are more likely to be sent to the cache and have longer fetch latencies. The following FTQ entries that hit in the L1-I are filled before moving to the head of the queue, and the remaining entries that can potentially stall the head must take longer than the current head instruction to fill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. AsmDB's Impact on the Occurrence of Scenario 2</head><p>We measure the number of stalls incurred by the FTQ's head instruction in Figure <ref type="figure">9</ref>. The 24-entry FDP generally experiences fewer stalls at the head instruction. A deeper FTQ allows instructions more opportunities to alias, so remaining instructions either have long fetch latencies or complete fetch before reaching the head of the FTQ. This effect is supported further by Figure <ref type="figure" target="#fig_11">10</ref>, which shows that the number of entries that have completed their fetch and are waiting for the head instruction is lower. The figure illustrates that, on average, the 24-entry FDP decreases the number of waiting instructions compared to the 2-entry FDP due to its deeper FTQ and experiencing head entries with large fetch latencies.</p><p>We measure the number of instructions forced to stall due to waiting on the head instruction to complete its fetch in Figure <ref type="figure" target="#fig_11">10</ref>. Comparing the results of Figures 9 and 10 relative to AsmDB, we find that it increases the number of stalling instructions for conservative and industry-standard FDP implementations compared to their respective baselines, indicating an increase in the occurrence of Scenario 2 (Sec. III-B). The average number of cycles to fill a head instruction is much higher in the industry-standard. The number of overall waiting entries is lower in the industry-standard FDP than the conservative FDP, meaning any delay in the industry-standard FDP will have a higher impact on the throughput of the FTQ. Although AsmDB tends to remove the number of stalls caused by the head entry, any stall at the head of the deeper FTQ that delays sending instructions to decode will result in a loss of potential performance gain.</p><p>Increasing the number of stalls caused by an entry at the head of the FTQ has less impact in a conservative front-end since it will delay a single FTQ entry at most, or roughly 16 instructions assuming each instruction is 32 bits. In contrast, additional stalling entries in the FTQ heavily affect an industry-standard FDP since, in the worse case, it can delay up to 23 FTQ entries or 184 instructions. Although any stall causes delayed execution overall, the high throughput of the industry-standard FDP and longer fetch times make the performance impact of stalling instructions much higher than the conservative FDP. The significant number of stalling entries introduced by AsmDB, regardless of their fetch latency, results in more waiting entries that consume its potential performance benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Software Instruction Prefetching Impact on Scenario 3</head><p>We measure the number of stalling entries in the FTQ that move into the head entry position before completing their fetch or are partially covered by the previous stalling head entry in Figure <ref type="figure" target="#fig_12">11</ref>. We find that overall the 24-entry FTQ experiences fewer partial stalls than the 2-entry FTQ, as the stalling head entries have more significant latencies capable of covering the following outstanding requests.</p><p>AsmDB with instruction overhead demonstrates a decrease in partially covered instructions, reducing the occurrence of Scenario 3 (III-C). A reduction in Scenario 3 indicates that when Scenario 2 occurs, the head entry covers the fetch latency of the following entries. This conversion contributes to the increased number of waiting entries in Figure <ref type="figure" target="#fig_11">10b</ref> as previous partially covered entries are complete and waiting for decoding. VI. POSSIBLE SOLUTIONS Our analysis finds that conservative front-ends with a small FTQ benefit significantly from software instruction prefetching but that the inserted instruction prefetches adversely affect industry-standard front-ends. Still, there are opportunities for software instruction prefetching to benefit aggressive frontends. We propose two directions for future research: metadata preloading and feedback-directed software prefetching.</p><p>Metadata preloading schemes can offset the overhead of inserting the prefetch instructions directly into the application's instruction stream by allocating a portion of the binary to direct a hardware prefetcher. The metadata can then be preloaded into dedicated hardware structures in the LLC when the application begins execution. The prefetching structures can be checked on an access to the L1-I to see if the access triggers a prefetch. If there is no corresponding entry in the L1-I preloader, a metadata request can be sent to the LLC's preloader to check for a corresponding prefetch. This scheme could be extended with metadata replacement and prefetching policies to act as a cache hierarchy for prefetches. Preloading metadata removes the  overhead from the front-end and requires modifying AsmDB's prefetch timing to allow the L1-I preloader to request a prefetch entry L1-I from the LLC.</p><p>Feedback-directed software prefetching may also alleviate the pressure software prefetches placed on the front-end by periodically updating an application's binary to increase or decrease the number of prefetches inserted depending on their performance impact. Prior work in compiler optimization <ref type="bibr" target="#b16">[17]</ref> has proposed similar feedback mechanisms, allowing the software analysis to adapt to different binaries to improve performance. A feedback-directed approach could reduce the effects on the front-end without requiring periodic profiling, analysis, and prefetch insertion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORKS A. Hardware Prefetching</head><p>Hardware prefetching has been a solution to memory bottlenecks for decades, with a plethora of prior art in instruction prefetching. Most instruction accesses are sequential as a program is iterated through, making next-line prefetchers fairly effective <ref type="bibr" target="#b25">[26]</ref>. However, discontinuities exist as control-flow instructions. Hardware instruction prefetcher designs commonly use existing control-flow-related microarchitectural structures to provide context to the application's current behavior and prefetch along a speculative execution path. Prior work has examined using, modifying <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, or replicating <ref type="bibr" target="#b26">[27]</ref> branch prediction structures to direct the L1-I prefetch engine's prefetches by building a context and history leading to the current execution behavior <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Prior designs also  leverage the modern processors' front-ends decoupled nature to prefetch future instructions, with optimizations to handle mispredicted branches and branch targets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p><p>An alternative to predicting an application's control-flow is leveraging the repetitive nature of instruction streams to record recurring instruction behavior. A triggering point is selected to allow the prefetcher to replay misses based on recently executed instructions <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. These prefetchers often attempt to predict control-flow outcomes or disregard them as noise to provide a more concise view of the current execution context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Software Prefetching</head><p>Callahan <ref type="bibr" target="#b15">[16]</ref> proposed one of the first software prefetching designs for data and inserts non-blocking software prefetches into a binary at compile time. They assume that the accesses to elements within an array in nested loops cause many misses and place prefetches before these accesses.</p><p>Luk and Mowry propose cooperative prefetching <ref type="bibr" target="#b14">[15]</ref>, which implements a software prefetcher in tandem with a prefetch filter. Their software analysis targets discontinuities (branches) in the CFG between basic blocks for prefetching. They include additional analysis to combine, remove, compress, and hoist software prefetches to reduce the overhead of inserting prefetches into a binary. Similarly, Mowry et al. propose targeting instructions that frequently cause misses within a loop, inserting prefetches outside of an unrolled loop and scheduling based on an estimated access latency <ref type="bibr" target="#b13">[14]</ref>. I-SPY <ref type="bibr" target="#b20">[21]</ref> extends AsmDB to build a context of the paths leading to a miss by tracking the branch information leading to the miss. The context is embedded in recurring prefetches and prefetching hardware compares it to the execution context to conditionally issue a particular prefetch. They also coalesces prefetches with addresses that are within a set distance from one another. If I-SPY cannot issue a conditional or coalesced prefetch, it defaults to AsmDB's base software prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Software instruction prefetching is a promising solution to the front-end bottleneck; however, prior work uses a conservative front-end model, which inflates the performance benefit of software instruction prefetching. Implementing AsmDB with a contemporary FDP model does not provide further performance benefits due to the overhead of the additional instructions the software prefetcher inserts. We identify the possible scenarios the front-end encounters when fetching instructions and how changing the occurrence of these scenarios can impact performance. An industry-standard front-end model is sensitive to changes in the instruction stream as they exhibit high throughput heavily impacted by additional fetch latency.</p><p>Future work in software instruction prefetching may alleviate the overhead of inserting instructions by reducing the number of inserted instructions or making the software prefetcher adaptive to an application's front-end behavior. We hope to excite future research in creating software instruction prefetchers aware of the effects of additional instructions and may leverage front-end characteristics to improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Comparison of front-end performance of AsmDB, an industry-standard FDP implementation, and EIP with an industrystandard FDP implementation over a conservative front-end 2-entry FTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: An example of the CFG generated by AsmDB's software analysis to select locations to insert software instruction prefetches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fetch</head><label></label><figDesc>Scenario 1 in an industrystandard pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Scenario 1 for conservative and industry-standard FDP implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Scenario 2 for conservative and industry-standard FDP implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fetch</head><label></label><figDesc>Scenario 3 in an industry-standard pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Scenario 3 for conservative and industry-standard FDP implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Static and dynamic code bloat</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Average cycles to fetch the head of a 24-entry FTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Average cycles to fetch the head of a 2-entry FTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Average cycles to fetch an entry not at the head of a 24-entry FTQ. Average cycles to fetch an entry not at the head of a 2-entry FTQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10: This figure illustrates the number of FTQ entries that are forced to wait on a stalling head instruction before progressing through the FTQ. While the conservative FDP has more waiting instructions overall, the increase in waiting instructions in the 24-entry FDP represents a loss of potential performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: This figure illustrates the number of FTQ entries that move into the head entry position while still waiting for their fetch to complete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>accurately.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="31">Performance over a Conservative Front-end with a 2-Entry FTQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.8 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 0.9 0.8</cell><cell>public_srv_60</cell><cell>secret_crypto52</cell><cell>secret_crypto80</cell><cell>secret_crypto90</cell><cell>secret_int_124</cell><cell>secret_int_155</cell><cell>secret_int_290</cell><cell>secret_int_327</cell><cell>secret_int_44</cell><cell>secret_int_624</cell><cell>secret_int_678</cell><cell>secret_int_706</cell><cell>secret_int_83</cell><cell>secret_int_86</cell><cell>secret_int_948</cell><cell>secret_int_965</cell><cell>secret_srv12</cell><cell>secret_srv128</cell><cell>secret_srv194</cell><cell>secret_srv207</cell><cell>secret_srv21</cell><cell>secret_srv222</cell><cell>secret_srv225</cell><cell>secret_srv255</cell><cell>secret_srv259</cell><cell>secret_srv32</cell><cell>secret_srv408</cell><cell>secret_srv41</cell><cell>secret_srv426</cell><cell>secret_srv442</cell><cell>secret_srv48</cell><cell>secret_srv495</cell><cell>secret_srv504</cell><cell>secret_srv537</cell><cell>secret_srv540</cell><cell>secret_srv582</cell><cell>secret_srv61</cell><cell>secret_srv617</cell><cell>secret_srv641</cell><cell>secret_srv669</cell><cell>secret_srv702</cell><cell>secret_srv727</cell><cell>secret_srv73</cell><cell>secret_srv742</cell><cell>secret_srv757</cell><cell>secret_srv764</cell><cell>secret_srv771</cell><cell>secret_srv85</cell><cell>Average</cell></row><row><cell></cell><cell cols="3">AsmDB</cell><cell></cell><cell cols="13">AsmDB -No Insertion Overhead</cell><cell></cell><cell cols="8">FDP (24-Entry FTQ)</cell><cell></cell><cell cols="6">AsmDB+FDP</cell><cell></cell><cell cols="14">AsmDB+FDP -No Insertion Overhead</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. The FTQ is</figDesc><table><row><cell>Predicted</cell><cell cols="2">Branch Predictor</cell><cell>Instr. Fetch</cell></row><row><cell>Branch Target</cell><cell>Return Address Stack</cell><cell>Indirect Predictor</cell><cell>Instr. TLB</cell></row><row><cell></cell><cell>Branch Target Buffer</cell><cell>Br Direction Predictor</cell><cell>I-Cache</cell></row><row><cell></cell><cell cols="2">Fetch Target Queue (FTQ)</cell><cell>Instr. Decode</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BTB-Miss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Predicted</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Taken Br?</cell></row><row><cell cols="2">Post-Fetch Correction</cell><cell cols="2">Prediction Info</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Branch</cell></row><row><cell cols="2">Resolved Branch Target</cell><cell></cell><cell>Execution</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Simulation parameters based on previous work</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The static code bloat for AsmDB, representing the percent increase in the overall size of the binary due to inserting software prefetches.</figDesc><table><row><cell>8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell cols="19">1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 Workload Designation Average</cell></row><row><cell>5% 10% 15% 20% 25% (a) 0%</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell><cell>15</cell><cell>17</cell><cell>19 Workload Designation 21 23 25 27 29 31</cell><cell>33</cell><cell>35</cell><cell>37</cell><cell>39</cell><cell>41</cell><cell>43</cell><cell>45</cell><cell>47</cell><cell>49 Average</cell></row><row><cell cols="20">(b) The dynamic code bloat for AsmDB representing the percent</cell></row><row><cell cols="20">increase in the number of fetched instructions as a result of inserting</cell></row><row><cell cols="12">software prefetches into the application's binary.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Number of stalls incurred by the head entries for the 24-entry and 2-entry implementations of FDP.</figDesc><table><row><cell>Stall Cycles</cell><cell>(Millions)</cell><cell cols="2">200 300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 Workload Designation FDP (FTQ=2) AsmDB+FDP AsmDB+FDP -No Insertion Overhead Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Stalls Caused by Head Entries in 2-Entry FTQ</cell><cell></cell><cell></cell></row><row><cell>Stall Cycles</cell><cell>(Millions)</cell><cell cols="2">100 200 300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 Workload Designation FDP (FTQ=24) AsmDB+FDP AsmDB+FDP -No Insertion Overhead Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Stalls Caused by Head Entries in 24-Entry FTQ</cell><cell></cell><cell></cell></row><row><cell cols="4">12 14 16 18 20 Fig. 9: 10 Number of FTQ Entries (Millions)</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9 FDP (FTQ=2) 11 13</cell><cell>15</cell><cell>17 AsmDB+FDP 19 21 Workload Designation 23 25 27 29 AsmDB+FDP -No Insertion Overhead 31 33 35 37 39</cell><cell>41</cell><cell>43</cell><cell>45</cell><cell>47</cell><cell>49 Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">(a) Number of FTQ Entries waiting on a Stalling Head Entry in a 2-Entry FTQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of</cell><cell>FTQ Entries</cell><cell>(Millions)</cell><cell>12 14 16 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9 FDP (FTQ=24) 11 13</cell><cell>15</cell><cell cols="2">17 AsmDB+FDP 19 21 Workload Designation 23 25 27 29 AsmDB+FDP -No Insertion Overhead 31 33 35 37 39 41</cell><cell>43</cell><cell>45</cell><cell>47</cell><cell>49 Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">(b) Number of FTQ Entries waiting on a Stalling Head Entry in a 24-Entry FTQ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Number of FTQ Entries Partially Covered by a Stalling Head Instruction in a 24-Entry FTQ</figDesc><table><row><cell></cell><cell></cell><cell cols="2">10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of</cell><cell>FTQ Entries</cell><cell>(Millions)</cell><cell>4 6 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell cols="3">9 FDP (FTQ=2) 11 13</cell><cell>15</cell><cell cols="9">17 AsmDB+FDP 19 21 Workload Designation 23 25 27 29 AsmDB+FDP -No Insertion Overhead 31 33 35 37 39</cell><cell>41</cell><cell>43</cell><cell>45</cell><cell>47</cell><cell>49 Average</cell></row><row><cell cols="23">(a) Number of FTQ Entries Partially Covered by a Stalling Head Instruction in a 2-Entry FTQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number of</cell><cell>FTQ Entries</cell><cell>(Millions)</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell><cell>15</cell><cell>17</cell><cell>19</cell><cell cols="2">21 Workload Designation 23 25 27 29</cell><cell>31</cell><cell>33</cell><cell>35</cell><cell>37</cell><cell>39</cell><cell>41</cell><cell>43</cell><cell>45</cell><cell>47</cell><cell>49 Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">FDP (FTQ=24)</cell><cell cols="3">AsmDB+FDP</cell><cell cols="7">AsmDB+FDP -No Insertion Overhead</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 08:02:51 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>IX. ACKNOWLEDGEMENTS</head><p>This work is supported in part by the <rs type="funder">National Science Foundation</rs> through grants <rs type="grantNumber">CNS-1938064</rs>, <rs type="grantNumber">CCF-1823403</rs> and <rs type="grantNumber">CCF-1912617</rs>, and generous gifts from <rs type="funder">Intel</rs>. The work was also in part supported by <rs type="funder">ARM</rs> Limited. Portions of this research were conducted with the advanced computing resources provided by <rs type="funder">Texas A&amp;M High Performance Research Computing</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AFpQF7n">
					<idno type="grant-number">CNS-1938064</idno>
				</org>
				<org type="funding" xml:id="_gymFqEx">
					<idno type="grant-number">CCF-1823403</idno>
				</org>
				<org type="funding" xml:id="_QtMsxRw">
					<idno type="grant-number">CCF-1912617</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance characterization of a quad pentium pro smp using oltp workloads</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Symposium on Computer Architecture, ISCA &apos;98, (USA)</title>
		<meeting>the 25th Annual International Symposium on Computer Architecture, ISCA &apos;98, (USA)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">1526</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dbmss on a modern processor: Where does time go?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Very Large Data Bases, VLDB &apos;99</title>
		<meeting>the 25th International Conference on Very Large Data Bases, VLDB &apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">266277</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale-out processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">500511</biblScope>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">3748</biblScope>
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture, ISCA &apos;15</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture, ISCA &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">158169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
	<note>in MICRO-32</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rebasing instruction prefetching: An industry perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="147" to="150" />
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable front-end architecture for fast instruction delivery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">234245</biblScope>
			<date type="published" when="1999-05">may 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Confluence: Unified instruction supply for scale-out servers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blasting through the frontend bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">3042</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asmdb: Understanding and mitigating front-end stalls in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture, ISCA &apos;19</title>
		<meeting>the 46th International Symposium on Computer Architecture, ISCA &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">462473</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design and evaluation of a compiler algorithm for prefetching</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">6273</biblScope>
			<date type="published" when="1992-09">Sept. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 31st Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>31st Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998-12">Dec 1998</date>
			<biblScope unit="page" from="182" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software prefetching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porterfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4052</biblScope>
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autofdo: Automatic feedbackdirected optimization for warehouse-scale applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO 2016 Proceedings of the 2016 International Symposium on Code Generation and Optimization</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Re-establishing fetchdirected instruction prefetching: An industry perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Arm Neoverse N1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Abernathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ringe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tummala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werkheiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The ibm z15 high frequency mainframe branch predictor industrial product</title>
		<author>
			<persName><forename type="first">N</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heizmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Prasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saporito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">I-spy: Context-driven conditional instruction prefetching with coalescing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The championship simulator: Architectural simulation for education and competition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zsim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">475486</biblScope>
			<date type="published" when="2013-06">jun 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011-08">aug 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 ACM/IEEE Conference on Supercomputing, Supercomputing &apos;91</title>
		<meeting>the 1991 ACM/IEEE Conference on Supercomputing, Supercomputing &apos;91<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings HPCA Seventh International Symposium on High-Performance Computer Architecture</title>
		<meeting>HPCA Seventh International Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-01">Jan 2001</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rdip: Return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999-11">Nov 1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
	<note>in MICRO-32</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Elastic instruction fetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcilvaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Clancy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
			<biblScope unit="page" from="478" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2011-12">Dec 2011</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shift: Shared history instruction fetch for lean-core server processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd Annual International Symposium on Computer Architecture, ISCA &apos;05</title>
		<meeting>the 32Nd Annual International Symposium on Computer Architecture, ISCA &apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Divide and conquer frontend bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Branch agnostic region searching algorithm</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<ptr target="https://research.ece.ncsu.edu/ipc/wp-content/uploads/2020/05/bar%C3%A7a.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
