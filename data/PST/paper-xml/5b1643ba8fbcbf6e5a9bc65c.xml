<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kartik</forename><surname>Hegde</surname></persName>
							<email>kvhegde2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiyong</forename><surname>Yu</surname></persName>
							<email>jiyongy2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rohit</forename><surname>Agrawal</surname></persName>
							<email>rohita2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengjia</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
							<email>mpellauer@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
							<email>cwfletch@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EEC1E37258E2A76F3409A527BA8D70EE</idno>
					<idno type="DOI">10.1109/ISCA.2018.00062</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have begun to permeate all corners of electronic society (from voice recognition to scene generation) due to their high accuracy and machine efficiency per operation. At their core, CNN computations are made up of multi-dimensional dot products between weight and input vectors. This paper studies how weight repetition-when the same weight occurs multiple times in or across weight vectorscan be exploited to save energy and improve performance during CNN inference. This generalizes a popular line of work to improve efficiency from CNN weight sparsity, as reducing computation due to repeated zero weights is a special case of reducing computation due to repeated weights.</p><p>To exploit weight repetition, this paper proposes a new CNN accelerator called the Unique Weight CNN Accelerator (UCNN). UCNN uses weight repetition to reuse CNN sub-computations (e.g., dot products) and to reduce CNN model size when stored in off-chip DRAM-both of which save energy. UCNN further improves performance by exploiting sparsity in weights. We evaluate UCNN with an accelerator-level cycle and energy model and with an RTL implementation of the UCNN processing element. On three contemporary CNNs, UCNN improves throughputnormalized energy consumption by 1.2× ∼ 4×, relative to a similarly provisioned baseline accelerator that uses Eyeriss-style sparsity optimizations. At the same time, the UCNN processing element adds only 17-24% area overhead relative to the same baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We are witnessing an explosion in the use of Deep Neural Networks (DNNs), with major impacts on the world's economic and social activity. At present, there is abundant evidence of DNN's effectiveness in areas such as classification, vision, and speech <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Of particular interest are Convolutional Neural Networks (CNNs), which achieve state-ofthe-art performance in many of these areas, such as image/temporal action recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and scene generation <ref type="bibr" target="#b7">[8]</ref>. An ongoing challenge is to bring CNN inference-where the CNN is deployed in the field and asked to answer online queries-to edge devices, which has inspired CNN architectures ranging from CPUs to GPUs to custom accelerators <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. A major challenge along this line is that CNNs are notoriously compute intensive <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. It is imperative to find new ways to reduce the work needed to perform inference.</p><p>At their core, CNN computations are parallel dot products. Consider a 1-dimensional convolution (i.e., a simplified CNN kernel), which has filter {a, b, a} and input {x, y, z, k, l,...}. We refer to elements in the input as the activations, elements in the filter as the weights and the number of weights in the filter as the filter's size (3 in this case). The output is computed by sliding the filter across the input and taking a filtersized dot product at each position (i.e., {ax + by + az, ay + bz + ak,...}), as shown in Figure <ref type="figure" target="#fig_5">1a</ref>. When evaluated on hardware, this computation entails reading each input and weight from memory, and performing a multiply-accumulate (MAC) on that input-weight pair. In this case, each output performs 6 memory reads (3 weights and 3 inputs), 3 multiplies and 2 adds.</p><p>In this paper, we explore how CNN hardware accelerators can eliminate superfluous computation by taking advantage of repeated weights. In the above example, we have several such opportunities because the weight a appears twice. First (Figure <ref type="figure" target="#fig_5">1b</ref>), we can factor dot products as sum-of-productsof-sums expressions, saving 33% multiplies and 16% memory reads. Second (Figure <ref type="figure" target="#fig_5">1c</ref>), each partial product a * input computed at the filter's right-most position can be memoized and re-used when the filter slides right by two positions, saving 33% multiples and memory reads. Additional opportunities are explained in Section III. Our architecture is built on these two ideas: factorization and memoization, both of which are only possible given repeated weights (two a's in this case).</p><p>Reducing computation via weight repetition is possible due to CNN filter design/weight quantization techniques, and is inspired by recent work on sparse CNN accelerators. A filter is guaranteed to have repeated weights when the filter size exceeds the number of unique weights, due to the pigeonhole principle. Thus, out-of-the-box (i.e., not re-trained <ref type="bibr" target="#b13">[14]</ref>) networks may see weight repetition already. For example, representing each weight in 8 bits <ref type="bibr" target="#b12">[13]</ref> implies there are ≤ 256 unique weights, whereas filter size can be in the thousands of weights <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Augmenting this further is a rich line of work to quantize weights <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which strives to decrease the number of unique weights without losing significant classification accuracy. For example, INQ <ref type="bibr" target="#b17">[18]</ref> and TTQ <ref type="bibr" target="#b16">[17]</ref> use 17 and 3 unique weights, respectively, without changing filter size. Finally, innovations <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref> that exploit CNN sparsity (zero weights/activations) inspire and complement weight repetition. Weight repetition generalizes this optimization: reducing computation due to repeated zero weights is a special case of reducing computation due to repeated weights.</p><p>Exploiting weight repetition while getting a net efficiency win, however, is challenging for two reasons. First, as with sparse architectures, tracking repetition patterns is difficult because they are irregular. Second, naïve representations of tracking metadata require a large amount of storage. This is a serious problem due to added system energy cost of transporting metadata throughout the system (e.g., reading the model from DRAM, PCI-e, etc).</p><p>This paper addresses these challenges with a novel CNN accelerator architecture called UCNN, for Unique Weight CNN Accelerator. UCNN is based on two main ideas. First, we propose a factorized dot product dataflow which reduces multiplies and weight memory reads via weight repetition, and improves performance via weight sparsity <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Second, we propose activation group reuse, which builds on dot product factorization to reduce input memory reads, weight memory reads, multiplies and adds per dot product, while simultaneously compressing the CNN model size. The compression rate is competitive to that given by aggressive weight quantization schemes <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and gives an added ability to exploit weight repetition. We employ additional architectural techniques to amortize the energy cost of irregular accesses and to reduce hardware area overhead. Contributions. To summarize, this paper makes the following contributions.</p><p>1) We introduce new techniques-including dot product factorization and activation group reuse-to improve CNN efficiency by exploiting weight repetition in and across CNN filters. 2) We design and implement a novel CNN accelerator, called UCNN, that improves performance and efficiency per dot product by using the aforementioned techniques. 3) We evaluate UCNN using an accelerator-level cycle and energy model as well as an RTL prototype of the UCNN processing element. On three contemporary CNNs, UCNN improves throughput-normalized energy consumption by 1.2× ∼ 4×, relative to a similarly provisioned baseline accelerator that uses Eyeriss-style sparsity optimizations. At the same time, the UCNN processing element adds only 17-24% area overhead relative to the same baseline. We note that while our focus is to accelerate CNNs due to their central role in many problems, weight repetition is a general phenomena that can be exploited by any DNN based on dot products, e.g., multilayer perceptions. Further, some of our techniques, e.g., dot product factorization, work out of the box for non-CNN algorithms.</p><p>Paper outline. The rest of the paper is organized as follows. Section II gives background on CNNs and where weight repetition occurs in modern networks. Section III presents strategies for CNN accelerators to reduce work via weight repetition. Section IV proposes a detailed processing element (PE)-level architecture to improve efficiency via weight repetition. Section V gives a dataflow and macro-architecture for the PE. Section VI evaluates our architecture relative to dense baselines. Section VII covers related work. Finally Section VIII concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND A. CNN Background</head><p>CNNs are made up of multiple layers, where the predominant layer type is a multi-dimensional convolution. Each convolutional layer involves a 3-dimensional (W × H ×C) input and K 3-dimensional (R × S ×C) filters. Convolutions between the filters and input form a 3-dimensional (W -R + 1) × (H -S + 1) × K output. These parameters are visualized in Figure <ref type="figure" target="#fig_1">2</ref>. C and K denote the layer's input and output channel count, respectively. We will omit '×' from dimensions for brevity when possible, e.g., W × H ×C → W HC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN inference.</head><p>As with prior work <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref>, this paper focuses on CNN inference, which is the online portion of the CNN computation run on, e.g., edge devices. The inference operation for convolutional layers (not showing bias terms, and for unit stride) is given by</p><formula xml:id="formula_0">O[(k, x, y)] = C-1 ∑ c=0 R-1 ∑ r=0 S-1 ∑ s=0 F[(k, c, r, s)] * I[(c, x + r, y + s)] (1) 0 ≤ k &lt; K, 0 ≤ x &lt; W -R + 1, 0 ≤ y &lt; H -S + 1</formula><p>where O, I and F are outputs (activations), inputs (activations) and filters (weights), respectively. Outputs become inputs to the next layer. Looking up O, I and F with a tuple is notation for a multi-dimensional array lookup. As is the case with other works targeting inference <ref type="bibr" target="#b10">[11]</ref>, we assume a batch size of one.</p><p>We remark that CNNs have several other layer types including non-linear scaling <ref type="bibr" target="#b22">[23]</ref> layers, down-sampling/pooling Fig. <ref type="figure">3</ref>. Weight repetition per filter, averaged across all filters, for select layers in a Lenet-like CNN <ref type="bibr" target="#b19">[20]</ref>, AlexNet <ref type="bibr" target="#b5">[6]</ref> and ResNet-50 <ref type="bibr" target="#b15">[16]</ref>.</p><p>All networks are trained with INQ <ref type="bibr" target="#b17">[18]</ref>. LeNet was trained on CIFAR-10 <ref type="bibr" target="#b20">[21]</ref> and AlexNet/ResNet were trained on ImageNet <ref type="bibr" target="#b21">[22]</ref>. MxLy stands for "module x, layer y." In the case of ResNet, we show one instance of each module, where repetition is averaged across filters in the layer. Note that the error bars represent the standard deviation of weight repetition in each layer.</p><p>layers and fully connected layers. We focus on accelerating convolutional layers as they constitute the majority of the computation <ref type="bibr" target="#b23">[24]</ref>, but explain how to support other layer types in Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weight Repetition in Modern CNNs</head><p>We make a key observation that, while CNN filter dimensions have been relatively constant over time, the number of unique weights in each filter has decreased dramatically. This is largely due to several successful approaches to compress CNN model size <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>. There have been two main trends, both referred to as weight quantization schemes. First, to decrease weight numerical precision, which reduces model size and the cost of arithmetic <ref type="bibr" target="#b12">[13]</ref>. Second, to use a small set of high-precision weights <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which also reduces model size but can enable higher accuracy than simply reducing precision.</p><p>Many commercial CNNs today are trained with reduced, e.g., 8 bit <ref type="bibr" target="#b12">[13]</ref>, precision per weight. We refer to the number of unique weights in the network as U. Thus, with 8 bit weights U ≤ 2 8 = 256. Clearly, weight repetition within and across filters is guaranteed as long as U &lt; R * S * C and U &lt; R * S * C * K, respectively. This condition is common in contemporary CNNs, leading to a guaranteed weight repetition in modern networks. For example, every layer except the first layer in ResNet-50 <ref type="bibr" target="#b15">[16]</ref> has more than 256 weights per filter and between K = 64 to K = 512 filters.</p><p>A complementary line of work shows it is possible to more dramatically reduce the number of unique weights, while maintaining state-of-the-art accuracy, by decoupling the number of unique weights from the numerical precision per weight <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Figure <ref type="figure">3</ref> shows weight repetition for several modern CNNs trained with a scheme called Incremental Network Quantization (INQ) <ref type="bibr" target="#b17">[18]</ref>. INQ constrains the trained model to have only U = 17 unique weights (16 non-zero weights plus zero) and achieves state-of-the-art accuracy on many contemporary CNNs. Case in point, Figure <ref type="figure">3</ref> shows a LeNetlike CNN from Caffe <ref type="bibr" target="#b19">[20]</ref> trained on CIFAR-10 [21], and AlexNet <ref type="bibr" target="#b5">[6]</ref> plus ResNet-50 <ref type="bibr" target="#b15">[16]</ref> trained on ImageNet <ref type="bibr" target="#b21">[22]</ref>, which achieved 80.16%, 57.39% and 74.81% top-1 accuracy, respectively.</p><p>Figure <ref type="figure">3</ref> shows that weight repetition is widespread and abundant across a range of networks of various sizes and depths. We emphasize that repetition counts for the non-zero column in Figure <ref type="figure">3</ref> are the average repetition for each non-zero weight value within each filter. We see that each non-zero weight is seldom repeated less than 10 times. Interestingly, the repetition count per non-zero is similar to that of the zero weight for most layers. This implies that the combined repetitions of nonzero weights (as there are U -1 non-zero weights) can dwarf the repetitions of zero weights. The key takeaway message is that there is a large un-tapped potential opportunity to exploit repetitions in non-zero weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPLOITING WEIGHT REPETITION</head><p>We now discuss opportunities that leverage weight repetition to reduce work (save energy and cycles), based on refactoring and reusing CNN sub-computations. From Section II, there are K CNN filters per layer, each of which spans the three dimensions of RSC. Recall, R and S denote the filter's spatial dimensions and C denotes the filter's channels.</p><p>We first present dot product factorization (Section III-A), which saves multiplies by leveraging repeated weights within a single filter, i.e., throughout the RSC dimensions. We then present a generalization of dot product factorization, called activation group reuse (Section III-B), to exploit repetition within and across filters, i.e., throughout the RSCK dimensions. Lastly, we remark on a third type of reuse that we do not exploit in this paper (Section III-C) but may be of independent interest.</p><p>For clarity, we have consolidated parameters and terminology unique to this paper in Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dot Product Factorization</head><p>Given each dot product in the CNN (an RSC-shape filter MACed to an RSC sub-region of input), our goal is to reduce the number of multiplies needed to evaluate that dot product. This can be accomplished by factorizing out common weights in the dot product, as shown in Figure <ref type="figure" target="#fig_5">1b</ref>. That is, input activations that will be multiplied with the same weight (e.g., x, z and y, k and z, l in Figure <ref type="figure" target="#fig_5">1b</ref>) are grouped and summed locally, and only that sum is multiplied to the repeated weight. We refer to groups of activations summed locally as activation groups -we use this term extensively in the rest of the paper. To summarize:</p><p>1) Each activation group corresponds to one unique weight in the given filter.</p><p>2) The total number of activation groups per filter is equal to the number of unique weights in that filter.</p><p>3) The size of each activation group is equal to the repetition count for that group's weight in that filter. We can now express dot product factorization by rewriting the Equation <ref type="formula">1</ref>as</p><formula xml:id="formula_1">O[(k, x, y)] = U ∑ i=0 F[wiT[(k, i)]] * gsz(k,i)-1 ∑ j=0 I[iiT[(k, i, j)]] (2)</formula><p>O, F and I are outputs, filters and inputs from Equation <ref type="formula">1</ref>, gsz(k, i) indicates the size of the i-th activation group for the k-th filter and U represents the number of unique weights in the network (or network layer). Note that each filter can have a different number of unique weights due to an irregular weight distribution. That is, some activation groups may be "empty" for a given filter. For simplicity, we assume each filter has U activation groups in this section, and handle the empty corner case in Section IV-C.</p><p>Activation groups are spread out irregularly within each RSC sub-region of input. Thus, we need an indirection table to map the locations of each activation that corresponds to the same unique weight. We call this an input indirection table, referred to as iiT. The table iiT reads out activations from the input space in activation group-order. That is, iiT[(k, i, 0)] ...iiT[(k, i, gsz(k, i) -1)] represents the indices in the input space corresponding to activations in the i-th activation group for filter k.</p><p>Correspondingly, we also need to determine which unique weight should be multiplied to each activation group. We store this information in a separate weight indirection table, referred to as wiT. wiT[(k, i)] points to the unique weight that must be multiplied to the i-th activation group for filter k. We emphasize that, since the weights are static for a given model, both of the above tables can be pre-generated offline.</p><p>Savings. The primary benefit from factorizing the dot product is reduced multiplications per dot product. Through the above scheme, the number of multiplies per filter reduces to the number of unique weights in the filter (e.g., 17 for INQ <ref type="bibr" target="#b17">[18]</ref>), regardless of the size of the filter or activation group. Referring back to Figure <ref type="figure">3</ref>, average multiplication savings would be the height of each bar, and this ranges from 5× to 373×. As discussed in Section II, even out-of-the-box networks are guaranteed to see savings.</p><p>An important special case is the zero weight, or when F[wiT[(k, i)]] = 0. Then, the inner loop to sum the activation group and the associated multiplication is skipped.  Costs. The multiply and sparsity savings come at the cost of storage overhead for the input and weight indirection tables, which naïvely are the size of the original dense weights, as well as the energy costs to lookup inputs/weights through these tables. We introduce several techniques to reduce the size of each of these tables (Sections IV-B to IV-C). We also amortize the cost of looking up these tables through a novel vectorization scheme (Section IV-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Activation Group Reuse</head><p>Dot product factorization reduces multiplications by exploiting weight repetition within the filter. We can generalize the idea to simultaneously exploit repetitions across filters, using a scheme called activation group reuse. The key idea is to exploit the overlap between two or more filters' activation groups. In Figure <ref type="figure" target="#fig_3">4</ref>, activations x + h + y form an activation group for filter k 1 . Within this activation group, filter k 2 has a sub-activation group which is x + h. The intersection of these two (x + h) can be reused across the two filters.</p><p>Formally, we can build the sub-activation groups for filter k 2 , within filter k 1 's i-th activation group, as follows. First, we build the activation group for k 1 :</p><formula xml:id="formula_2">A(k 1 , i) = {iiT[(k 1 , i, j)] : j ∈ [0, gsz(k 1 , i))}</formula><p>Then, we build up to U sub-activation groups for k 2 by taking set intersections. That is, for i = 0,...,U -1, the i -th subactivation group for k 2 is given by:</p><formula xml:id="formula_3">A(k 1 , i) A(k 2 , i )</formula><p>We can generalize the scheme to find overlaps across G filters. When G = 1, we have vanilla dot product factorization (Section III-A). The discussion above is for G = 2. When G &gt; 2, we recursively form set intersections between filters k g and k g+1 , for g = 1,...,G -1. That is, once sub-activation groups for a filter k 2 are formed, we look for "sub-sub" activation groups within a filter k 3 which fall within the sub-groups for k 2 , etc. Formally, suppose we have a gth-level activation group T g for filter k g . To find the (g + 1)th-level activation groups for filter k g+1 within T g , we calculate T g A(k g+1 , i ) for i = 0,...,U -1, which is analogous to how intersections were formed for the G = 2 case.</p><p>As mentioned previously, irregular weight distributions may mean that there are less than U unique weights in filter k g+1 that overlap with a given gth-level activation group for filter k g . We discuss how to handle this in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Savings. Activation group reuse can bring significant improvements in two ways: 1) Reduced input buffer reads and arithmetic operations:</head><p>From Figure <ref type="figure" target="#fig_3">4</ref>, we can eliminate the buffer reads and additions for reused sub-expressions like x + h. The scheme simultaneously saves multiplies as done in vanilla dot product factorization. 2) Compressed input indirection table iiT: Since we do not need to re-read the sub-, sub-sub-, etc. activation groups for filters k 2 ,...,k G , we can reduce the size of the input indirection table iiT by an O(G) factor. We discuss this in detail in Section IV-C.</p><p>How prevalent is Activation Group Reuse? Activation group reuse is only possible when there are overlaps between the activation groups of two or more filters. If there are no overlaps, we cannot form compound sub-activation group expressions that can be reused across the filters. These overlaps are likely to occur when the filter size R * S * C is larger than U G , i.e., the number of unique weights to the G-th power. For example, for (R, S,C) = (3, 3, 256) and U = 8, we expect to see overlaps between filter groups up to size G = 3 filters. We experimentally found that networks retrained with INQ <ref type="bibr" target="#b17">[18]</ref> (U = 17) and TTQ <ref type="bibr" target="#b16">[17]</ref> (U = 3) can enable G &gt; 1. In particular, INQ satisfies between G = 2 to 3 and TTQ satisfies G = 6 to 7 for a majority of ResNet-50 layers. Note that these schemes can simultaneously achieve competitive classification accuracy relative to large U schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Partial Product Reuse</head><p>We make the following additional observation. While dot product factorization looks for repetitions in each RSCdimensional filter, it is also possible to exploit repetitions across filters, within the same input channel. That is, across the RSK dimensions for each input channel. This idea is shown for 1D convolution in Figure <ref type="figure" target="#fig_5">1c</ref>. In CNNs, for each input channel</p><formula xml:id="formula_4">C, if w = F[(k 1 , c, r 1 , s 1 )] = F[(k 2 , c, r 2 , s 2 )] and (k 1 , r 1 , s 1 ) = (k 2 , r 2 , s 2 )</formula><p>, partial products formed with weight w can be reused across the filters, for the same spatial position, and as the filters slide. We do not exploit this form of computation reuse further in this paper, as it is not directly compatible with the prior two techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROCESSING ELEMENT ARCHITECTURE</head><p>In this section, we will describe Processing Element (PE) architecture, which is the basic computational unit in the accelerator. We will first describe the PE of an efficient Dense CNN accelerator, called DCNN. We will then make PE-level changes to the DCNN design to exploit the weight repetitionbased optimizations from Section III. This is intended to give a clear overview of how the UCNN design evolves over an efficient dense architecture and also to form a baseline for evaluations in Section VI.</p><p>The overall accelerator is made up of multiple PEs and a global buffer as depicted in Figure <ref type="figure">5</ref>. The global buffer is responsible for scheduling work to the PEs. We note that aside from changes to the PEs, the DCNN and UCNN accelerators (including their dataflow <ref type="bibr" target="#b26">[27]</ref>) are essentially the same. We provide details on the overall (non-PE) architecture and dataflow in Section V. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Design: DCNN PE</head><p>The DCNN and UCNN PE's unit of work is to compute a dot product between an RSC region of inputs and one or more filters. Recall that each dot product corresponds to all three loops in Equation <ref type="formula">1</ref>, for a given (k, x, y) tuple.</p><p>To accomplish this task, the PE is made up of an input buffer, weight buffer, partial sum buffer, control logic and MAC unit (the non-grey components in Figure <ref type="figure">6</ref>). At any point in time, the PE works on a filter region of size RSC t where C t ≤ C, i.e., the filter is tiled in the channel dimension. Once an RSC t region is processed, the PE will be given the next RSC t region until the whole RSC-sized dot product is complete.</p><p>Since this is a dense CNN PE, its operation is fairly straightforward. Every element of the filter is element-wise multiplied to every input element in the corresponding region, and the results are accumulated to provide a single partial sum. The partial sum is stored in the local partial sum buffer and is later accumulated with results of the dot products over the next RSC t -size filter tile.</p><p>Datapath. The datapath is made up of a fixed point multiplier and adder as shown in Figure <ref type="figure">6</ref> . Once the data is available in the input and weight buffers, the control unit feeds the datapath with a weight and input element every cycle. They are MACed into a register that stores a partial sum over the convolution operation before writing back to the partial sum buffer. Together, we refer to this scalar datapath as a DCNN lane.</p><p>Vectorization. There are multiple strategies to vectorize this PE. For example, we can vectorize across output channels (amortizing input buffer reads) by replicating the lane and growing the weight buffer capacity and output bus width. DCNN and UCNN will favor different vectorization strategies, and we specify strategies for each later in the section and in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dot Product Factorization</head><p>We now describe how to modify the DCNN architecture to exploit dot product factorization (Section III-A). The UCNN PE design retains the basic design and components of the DCNN To reduce the size of these indirection tables and to simplify the datapath, we sort entries in the input and weight indirection tables such that reading the input indirections sequentially looks up the input buffer in activation group-order. The weight indirection table is read in the same order. Note that because sorting is a function of weight repetitions, it can be performed offline.</p><p>Importantly, the sorted order implies that each weight in the weight buffer need only be read out once per activation group, and that the weight indirection table can be implemented as a single bit per entry (called the group transition bit), to indicate the completion of an activation group. Specifically, the next entry in the weight buffer is read whenever the group transition bit is set and the weight buffer need only store U entries.</p><p>As mentioned in Section III-A, we don't store indirection table entries that are associated with the zero weight. To skip zeros, we sort the zero weight to the last position and encode a "filter done" message in the existing table bits when we make the group transition to zero. This lets UCNN skip zero weights as proposed by previous works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref> and makes the exploitation of weight sparsity a special case of weight repetition. Datapath. The pipeline follows the two steps from the beginning of the section, and requires another accumulator to store the activation group sum as reflected in Figure <ref type="figure">6</ref> . As described above, the sorted input and weight indirection tables are read sequentially. During each cycle in Step 1, the input buffer is looked up based on the current input indirection table entry, and summed in accumulator until a group transition bit is encountered in the weight indirection table. In Step 2, the next weight from the weight buffer is multiplied to the sum in the MAC unit (Figure <ref type="figure">6</ref> ). After every activation group, the pipeline performs a similar procedure using the next element from the weight buffer. Arithmetic bitwidth. This design performs additions before each multiply, which means the input operand in the multiplier will be wider than the weight operand. The worst case scenario happens when the activation group size is the entire input tile, i.e., the entire tile corresponds to one unique non-zero weight, in which case the input operand is widest. This case is unlikely in practice, and increases multiplier cost in the common case where the activation group size is input tile size. Therefore, we set a maximum limit for the activation group size. In case the activation group size exceeds the limit, we split activation groups into chunks up to the maximum size. A local counter triggers early MACs along with weight buffer 'peeks' at group boundaries. In this work, we assume a maximum activation group size of 16. This means we can reduce multiplies by 16× in the best case, and the multiplier is 4 bits wider on one input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Activation Group Reuse</head><p>We now augment the above architecture to exploit activation group reuse (Section III-B). The key idea is that by carefully ordering the entries in the input indirection table (iiT), a single input indirection table can be shared across multiple filters. This has two benefits. First, we reduce the model size since the total storage needed for the input indirection tables shrinks. Second, with careful engineering, we can share subcomputations between the filters, which saves input buffer reads and improves PE throughput. Recall that the number of filters sharing the same indirection table is a parameter G, as noted in Table <ref type="table">I</ref>. If G = 1, we have vanilla dot product factorization from the previous section. Indirection table hierarchical sorting. To support G &gt; 1, we hierarchically sort a single input indirection table to support G filters. Due to the hierarchical sort, we will still be able to implement the weight indirection tables as a single bit per entry per filter, as done in Section IV-B. We give an example for the G = 2 case in Figure <ref type="figure">7</ref>, and walk through how to hierarchically sort the indirection tables below. These steps are performed offline.</p><p>1) Select a canonical order of weights. The order is a, b in the example.  order a, b. Filter k 1 has activation groups (e.g., z + m + l + y + h) and filter k 2 has sub-activation groups within filter k 1 's groups (e.g., z + m and l + y + h). Now, a single traversal of the input indirection table can efficiently produce results for both filters k 1 and k 2 . Crucially, sorts performed in Step 2 and 3 are keyed to the same canonical order of weights we chose in Step 1 (a, b in the example). By keeping the same order across filters, the weight indirection tables (denoted wiT 1 and wiT 2 for k 1 and k 2 , respectively, in Figure <ref type="figure">7</ref>) can be implemented as a single bit per entry.</p><p>As mentioned above, the scheme generalizes to G &gt; 2 in the natural fashion. For example, for G = 3 we additionally sort sub-sub-activation groups within the already established sub-activation groups using the same canonical a, b weight order. Thus, the effective indirection table size per weight is (|iiT.entry| + G * |wiT.entry|)/G = log 2 RSC t /G + 1 which is an O(G) factor compression. We will see the upper bound for G later in this section.</p><p>Datapath. To support activation group reuse, we add a third accumulator to the PE to enable accumulations across different level activation groups (Figure <ref type="figure">6</ref> ). G-th activation groups are first summed in accumulator . At G-th level activation group boundaries, the G-th level sum is merged into running sums for levels g = 1,...,G -1 using accumulator . At any level activation group boundary, sums requiring a multiply are dispatched to the MAC unit .</p><p>For clarity, we now give a step-by-step (in time) description of this scheme using the example in Figure <ref type="figure">7</ref> and the architecture from Figure <ref type="figure">6</ref>. Recall, we will form activation groups for filter k 1 and sub-activation groups for filter k 2 .</p><p>1) The input indirection table iiT reads the indirection to be 2, which corresponds to activation z. This is sent to accumulator which starts building the sub-activation group containing z. We assume accumulator 's state is reset at the start of each sub-activation group, so the accumulator implicitly calculates 0 + z here. Both wiT 1 and wiT 2 read 0s, thus we proceed without further accumulations. 2) iiT reads 6 and wiT 1 and wiT 2 read 0 and 1, respectively. This means we are at the end of the sub-activation group (for filter k 2 ), but not the activation group (for filter k 1 ). Sum z + m is formed in accumulator , which is sent (1) to accumulator -as this represents the sum of only a part of the activation group for filter k 1 -and (2) to the MAC unit to multiply with a for filter k 2 . 3) Both wiT 1 and wiT 2 read 0s, accumulator starts accumulating the sub-activation group containing l. 4) Both wiT 1 and wiT 2 read 0s, accumulator builds l + y. 5) Both wiT 1 and wiT 2 read 1s, signifying the end of both the sub-activation and activation groups. Accumulator calculates l + y + h, while accumulator contains z + m for filter k 1 . The result from accumulator is sent (1) to the MAC Unit -to multiply with b for filter k 2 -and (2) to accumulator to generate z + m + l + y + h. The result from accumulator finally reaches the MAC Unit to be multiplied with a. 6) Repeat steps similar to those shown above for subsequent activation groups on filter k 1 , until the end of the input indirection table traversal. Together, we refer to all of the above arithmetic and control as a UCNN lane. Note that a transition between activation groups in k 1 implies a transition for k 2 as well. Area implications. To vectorize by a factor of G, a dense design requires G multipliers. However, as shown in Figure <ref type="figure">6</ref>, we manage to achieve similar throughput with a single multiplier. The multiplier reduction is possible because the multiplier is only used on (sub-)activation group transitions. We do note that under-provisioning multipliers can lead to stalls, e.g., if (sub-)activation group transitions are very common. Thus, how many hardware multipliers and accumulators to provision is a design parameter. We evaluate a single-multiplier design in Section VI-C. Handling empty sub-activation groups. In Figure <ref type="figure">7</ref>, if weight a or b in filters k 1 or k 2 had a (sub-)activation group size of zero, the scheme breaks because each filter cycles through weights in the same canonical order. To properly handle these cases, we have two options. First, we can allocate more bits per entry in the weight indirection table. That is, interpret weight indirection table entries as n-bit counters that can skip 0 to 2 n -1 weights per entry. Second, we can add special "skip" entries to the weight and input indirection tables to skip the weight without any computations. A simple skip-entry design would create a cycle bubble in the UCNN lane per skip.</p><p>We apply a hybrid of the above schemes in our implementation. We provision an extra bit to each entry in the G-th filter's weight indirection table, for each group of G filters. An extra bit enables us to skip up to 3 weights. We find we only need to add a bit to the G-th filter, as this filter will have the smallest activation groups and hence has the largest chance of seeing an empty group. For any skip distance longer than what can be handled in allocated bits, we add skip entries as necessary and incur pipeline bubbles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional table compression.</head><p>We can further reduce the bits per entry in the input indirection table by treating each entry as a jump, relative to the last activation sharing the same weight, instead of as a direct pointer. This is similar to run-length encodings (RLEs) in sparse architectures <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Represented as jumps, bits per table entry are proportional to the average distance between activations sharing the same weight (i.e., O(log 2 U)), which can be smaller than the original pointer width log 2 RSC t . The trade-off with this scheme is that if the required jump is larger than the bits provisioned, we must add skip entries to close the distance in multiple hops. <ref type="foot" target="#foot_0">1</ref>Activation group reuse implications for weight sparsity. Fundamentally, to service G filters we need to read activations according to the union of non-zero weights in the group of G filters. That is, we can only remove entries from indirection tables if the corresponding weight in filters k 1 and k 2 is 0. Thus, while we get an O(G) factor of compression in indirection tables, less entries will be skip-able due to weight sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spatial Vectorization</head><p>One overhead unique to the UCNN PE is the cost to indirect into the input buffer. The indirection requires an extra buffer access, and the irregular access pattern means the input SRAM cannot read out vectors (which increases pJ/bit). Based on the observation that indirection tables are reused for every filter slide, we propose a novel method to vectorize the UCNN PE across the spatial W H dimensions. Such reuse allows UCNN to amortize the indirection table lookups across vector lanes. We refer to this scheme as spatial vectorization and introduce a new parameter V W to indicate the spatial vector size.</p><p>To implement spatial vectorization, we split the input buffer into V W banks and carefully architect the buffer so that exactly V W activations can be read every cycle. We note the total input buffer capacity required is only</p><formula xml:id="formula_5">O(C t * S * (V W + R)), not O(C t * S * V W * R),</formula><p>owing to the overlap of successive filter slides. The datapath for activation group reuse (Section IV-C) is replicated across vector lanes, thus improving the PE throughput to O(G * V W ) relative to the baseline non-vectorized PE. Given that UCNN significantly reduces multiplier utilization, an aggressive implementation could choose to temporally multiplex &lt; V W multipliers instead of spatially replicating multipliers across lanes.</p><p>Avoiding bank conflicts. Since the input buffer access pattern is irregular in UCNN, there may be bank conflicts in the banked input buffer. To avoid bank conflicts, we divide the input buffer into V W banks and apply the following fill/access strategy. To evaluate V W dot products, we iterate through the input buffer according to the input indirection table. We denote each indirection as a tuple (r, s, c) ∈ RSC t , where (r, s, c) corresponds to the spatial vector base address. Then, the bank id/bank address to populate vector slot v ∈ [0,...,V W -1] for that indirection is: bank</p><formula xml:id="formula_6">(r, s, c, v) = (r + v) % V W (3) addr(r, s, c, v) = s * C t + c + (r + v)/V W * S * C t (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>This strategy is bank conflict free because bank(r, s, c, v) always yields a different output for fixed (r, s, c), varying v.</p><p>Unfortunately, this scheme has a small storage overhead: a ((R +V W -1) % V W )/(R +V W -1) fraction of addresses in the input buffer are un-addressable. Note, this space overhead is always &lt; 2× and specific settings of R and V W can completely eliminate overhead (e.g., V W = 2 for R = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. UCNN Design Flexibility</head><p>Supporting a range of U. Based on the training procedure, CNNs may have a different number of unique weights (e.g., 3 <ref type="bibr" target="#b16">[17]</ref> or 17 <ref type="bibr" target="#b17">[18]</ref> or 256 <ref type="bibr" target="#b13">[14]</ref> or more). Our accelerator can flexibly handle a large range of U, but still gain the efficiency in Section IV-A, by reserving a larger weight buffer in the PE. This enables UCNN to be used on networks that are not re-trained for quantization as well. We note that even if U is large, we still save energy by removing redundant weight buffer accesses.</p><p>Support for other layer types. CNNs are made up of multiple layer types including convolutional, non-linear activation, pooling and fully connected. We perform non-linear activations (e.g., ReLu <ref type="bibr" target="#b27">[28]</ref>) at the PE (see Figure <ref type="figure">8</ref> (F)). Pooling can be handled with minimal additional logic (e.g., max circuits) at the PE, with arithmetic disabled. We implement fully connected layers as convolutions where input buffer slide reuse is disabled (see next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ARCHITECTURE AND DATAFLOW</head><p>This section presents the overall architecture for DCNN and UCNN, i.e., components beyond the PEs, as well as the architecture's dataflow. CNN dataflow <ref type="bibr" target="#b26">[27]</ref> describes how and when data moves through the chip. We present a dataflow that both suits the requirements of UCNN and provides the best power efficiency and performance out of candidates that we tried.</p><p>As described in the previous section and in Figure <ref type="figure">5</ref>, the DCNN and UCNN architectures consist of multiple Processing Elements (PEs) connected to a shared global buffer (L2), similar to previous proposals <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Similar to the PEs, the L2 buffer is divided into input and weight buffers. When it is not clear from context, we will refer to the PE-level input and weight buffers (Section IV) as the L1 input and weight buffers. Each PE is fed by two multicast buses, for input and weight data. Final output activations, generated by PEs, are written back to the L2 alongside the input activations in a doublebuffered fashion. That is, each output and will be treated as an input to the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataflow</head><p>Our dataflow is summarized as follows. We adopt weightand output-stationary terminology from <ref type="bibr" target="#b26">[27]</ref>.</p><p>1) The design is weight-stationary at the L2, and stores all input activations on chip when possible. 2) Each PE produces one column of output activations and PEs work on adjacent overlapped regions of input. The overlapping columns create input halos <ref type="bibr" target="#b11">[12]</ref>. 3) Each PE is output-stationary, i.e., the partial sum resides in the PE until the final output is generated across all C input channels. At the top level, our dataflow strives to minimize reads/writes to DRAM as DRAM often is the energy bottleneck in CNN accelerators <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Whenever possible, we store all input activations in the L2. We do not write/read input activations from DRAM unless their size is prohibitive. We note that inputs fit on chip in most cases, given several hundred KB of L2 storage. <ref type="foot" target="#foot_1">2</ref> In cases where inputs fit, we only need to read inputs from DRAM once, during the first layer of inference. In cases where inputs do not fit, we tile the input spatially. In all cases, we read all weights from DRAM for every layer. This is fundamental given the large (sometimes 10s of MB) aggregate model size counting all layers. To minimize DRAM energy from weights, the dataflow ensures that each weight value is fetched a minimal number of times, e.g., once if inputs fit and once per input tile otherwise.</p><p>At the PE, our dataflow was influenced by the requirements of UCNN. Dot product factorization (Section III) builds activation groups through RSC regions, hence the dataflow is designed to give PEs visibility to RSC regions of weights and inputs in the inner-most (PE-level) loops. We remark that dataflows working over RSC regions in the PEs have other benefits, such as reduced partial sum movement <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Detailed pseudo-code for the complete dataflow is given in Figure <ref type="figure">8</ref>. For simplicity, we assume the PE is not vectorized. Inputs reside on-chip, but weights are progressively fetched from DRAM in chunks of K c filters at a time (A). K c may change from layer to layer and is chosen such that the L2 is filled. Work is assigned to the PEs across columns of input and filters within the K c -size group (B). Columns of input and filters are streamed to PE-local L1 buffers (C). Both inputs and weights may be multicast to PEs (as shown by #multicast), depending on DNN layer parameters. As discussed in Section IV-A, C t input channels-worth of inputs and weights are loaded into the PE at a time. As soon as the required inputs/weights are available, RSC t sub-regions of input are transferred to smaller L0 buffers for spatial/slide data reuse and the dot product is calculated for the RSC t -size tile (E). Note that each PE works on a column of input of size RHC and produces a column of output of size H (D). The partial sum produced is stored in the L1 partial sum buffer and the final output is written back to the L2 (F). Note that the partial sum resides in the PE until the final output is generated, making the PE dataflow output-stationary. decreases energy per access, but still yields SRAMs that meet timing at 1 GHz. DRAM energy is counted at 20 pJ/bit <ref type="bibr" target="#b28">[29]</ref>. Network on chip (NoC) energy is extrapolated based on the number and estimated length of wires in the design (using our PE area and L2 SRAM area estimates from CACTI). We assume the NoC uses low-swing wires <ref type="bibr" target="#b30">[31]</ref>, which are low power, however consume energy each cycle (regardless of whether data is transferred) via differential signaling.</p><p>Activation/weight data types. Current literature employs a variety of activation/weight precision settings. For example, 8 to 16 bit fixed point <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, 32 bit floating point/4 bit fixed point activations with power of two weights <ref type="bibr" target="#b17">[18]</ref> to an un-specified (presumably 16 bit fixed point) precision <ref type="bibr" target="#b16">[17]</ref>. Exploiting weight repetition is orthogonal to which precision/data type is used for weights and activations. However, for completeness, we evaluate both 8 bit and 16 bit fixed point configurations.</p><p>Points of comparison. We evaluate the following design variants: DCNN: Baseline DCNN (Section IV-A) that does not exploit weight or activation sparsity, or weight repetition. We assume that DCNN is vectorized across output channels and denote the vector width as V k . Such a design amortizes the L1 input buffer cost and improves DCNN's throughput by a factor of V k . DCNN sp: DCNN with Eyeriss-style <ref type="bibr" target="#b26">[27]</ref> sparsity optimizations. DCNN sp skips multiplies at the PEs when an operand (weight or activation) is zero, and compresses data stored in DRAM with a 5 bit run-length encoding. UCNN Uxx: UCNN, with all optimizations enabled (Section IV-C) except for the jump-style indirection table (Section IV-C) which we evaluate separately in Section VI-D. UCNN reduces DRAM accesses based on weight sparsity and activation group reuse, and reduces input memory reads, weight memory reads, multiplies and adds per dot product at the PEs. UCNN also vectorizes spatially (Section IV-D). The Uxx refers to the number of unique weights; e.g., UCNN U17 is UCNN with U = 17 unique weights, which corresponds to an INQ-like quantization.</p><p>CNNs evaluated. To prove the effectiveness of UCNN across a range of contemporary CNNs, we evaluate the above schemes on three popular CNNs: a LeNet-like CNN <ref type="bibr" target="#b19">[20]</ref> trained on CIFAR-10 [20], and AlexNet <ref type="bibr" target="#b5">[6]</ref> plus ResNet-50 <ref type="bibr" target="#b15">[16]</ref> trained on ImageNet <ref type="bibr" target="#b21">[22]</ref>. We refer to these three networks as LeNet, AlexNet and ResNet for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy Analysis</head><p>We now perform a detailed energy analysis and design space exploration comparing DCNN and UCNN.</p><p>Design space. We present results for several weight density points (the fraction of weights that are non-zero), specifically 90%, 65% and 50%. For each density, we set (100-density)% of weights to 0 and set the remaining weights to non-zero values via a uniform distribution. Evaluation on a real weight distribution from INQ training is given in Section VI-C. 90% density closely approximates our INQ data. 65% and 50% density approximates prior work, which reports negligible accuracy loss for this degree of sparsification <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We note that UCNN does not alter weight values, hence UCNN run on prior training schemes <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref> results in the same accuracy as reported in those works. Input activation density is 35% (a rough average from <ref type="bibr" target="#b11">[12]</ref>) for all experiments. We note that lower input activation density favors DCNN sp due to its multiplication skipping logic.</p><p>To illustrate a range of deployment scenarios, we evaluate UCNN for different values of unique weights: U = 3, 17, 64, 256. We evaluate UCNN U3 ("TTQ-like" <ref type="bibr" target="#b16">[17]</ref>) and UCNN U17 ("INQ-like" <ref type="bibr" target="#b17">[18]</ref>) as these represent two example state-of-the-art quantization techniques. We show larger U configurations to simulate a range of other quantization options. For example, UCNN U256 can be used on out-of-the-box (not re-trained) networks quantized for 8 bit weights <ref type="bibr" target="#b12">[13]</ref> or on networks output by Deep Compression with 16 bit weights <ref type="bibr" target="#b13">[14]</ref>.</p><p>Hardware parameters. Table II lists all the hardware parameters used by different schemes in this evaluation. To get an apples-to-apples performance comparison, we equalize "effective throughput" across the designs in two steps. First, we give each design the same number of PEs. Second, we vectorize each design to perform the work of 8 dense multiplies per cycle per PE. Specifically, DCNN uses V K = 8 and UCNN uses V W and G such that G * V W = 8, where V W and V K represent vectorization in the spatial and output channel dimensions, respectively. Note that to achieve this throughput, the UCNN PE may only require V W or fewer multipliers (Section IV-C). Subject to these constraints, we allow each design variant to choose a different L1 input buffer size, V W and G to maximize its own average energy efficiency.</p><p>Results. Figure <ref type="figure">9</ref> shows energy consumption for three contemporary CNNs at both 8 and 16 bit precision. Energy is broken into DRAM, L2/NoC and PE components. Each configuration (for a particular network, weight precision and weight density) is normalized to DCNN for that configuration.</p><p>At 16 bit precision, all UCNN variants reduce energy compared to DCNN sp. The improvement comes from three sources. First, activation group reuse (G &gt; 1 designs in We observed similar improvements for the other networks (AlexNet and LeNet) given 16 bit precision, and improvement across all networks ranges between 1.2× ∼ 4× and 1.7× ∼ 3.7× for 90% and 50% weight densities, respectively. At 8 bit precision, multiplies are relatively cheap and DRAM compression is less effective due to the relative size of compression metadata. Thus, improvement for UCNN U3, UCNN U17 and UCNN U256 drops to 2.6×, 2× and 1. 10. Energy breakdown for the 50% weight density and 16 bit precision point, for specific layers in ResNet. Each group of results is for one layer, using the notation C : K : R : S. All results are relative to DCNN for that layer. and thus incur large energy overheads from reading indirection tables from memory. We evaluate additional compression techniques to improve these configurations in Section VI-D.</p><p>To give additional insight, we further break down energy by network layer. Figure <ref type="figure" target="#fig_5">10</ref> shows select layers in ResNet-50 given 50% weight density and 16 bit precision. Generally, early layers for the three networks (only ResNet shown) have smaller C and K; later layers have larger C and K. DRAM access count is proportional to total filter size R * S * C * K, making early and later layers compute and memory bound, respectively. Thus, UCNN reduces energy in early layers by improving arithmetic efficiency and reduces energy in later layers by saving DRAM accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Analysis</head><p>We now compare the performance of UCNN to DCNN with the help of two studies. First, we compare performance assuming no load balance issues (e.g., skip entries in indirection tables; Section IV-C) and assuming a uniform distribution of weights across filters, to demonstrate the benefit of sparse weights. Second, we compare performance given real INQ <ref type="bibr" target="#b17">[18]</ref> data, taking into account all data-dependent effects. This helps us visualize how a real implementation of UCNN can differ from the ideal implementation. For all experiments, we assume the hardware parameters in Table <ref type="table" target="#tab_5">II</ref>. Optimistic performance analysis. While all designs in Table II are throughput-normalized, UCNN can still save cycles due to weight sparsity as shown in Figure <ref type="figure" target="#fig_6">11</ref>. Potential improvement is a function of G: as described in Section IV-C, the indirection tables with activation group reuse (G &gt; 1) must store entries corresponding to the union of non-zero weights across the G filters. This means that choosing G presents a performance energy trade-off: larger G (when this is possible) reduces energy per CNN inference, yet smaller G (e.g., G = 1) can improve runtime.</p><p>Performance on real INQ data. We now compare UCNN to DCNN on real INQ <ref type="bibr" target="#b17">[18]</ref> training data (U = 17) and take into account sources of implementation-dependent UCNN performance overhead (e.g., a single multiplier in the PE datapath, and table skip entries; Section IV-C). The result is presented in Figure <ref type="figure" target="#fig_7">12</ref>. Given that our model trained with INQ has 90% weight density (matching <ref type="bibr" target="#b17">[18]</ref>), UCNN could improve performance by 10% in the best case (Section VI-B). However, we see 0.7% improvement for UCNN (G = 1). We further observe the following: increasing V K = 2 for DCNN sp, DCNN's performance improves by 2×. However, UCNN G = 2 (which is throughput-normalized to DCNN V K = 2) only improves performance by 1.80×, deviating from the ideal improvement of 2×. This performance gap is largely due to skip entries in the indirection table (Section IV-C). Overall, the performance deficit is dominated by the energy savings with UCNN as presented in Section VI-B. Therefore, UCNN still provides a significant performance/watt advantage over DCNN configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Size (DRAM storage footprint)</head><p>Figure <ref type="figure" target="#fig_8">13</ref> compares weight compression rates between UCNN variants, DCNN sp and to the stated model sizes in the TTQ <ref type="bibr" target="#b16">[17]</ref> and INQ <ref type="bibr" target="#b17">[18]</ref> papers. UCNN uses activation group reuse and weight to compress model size (Section IV-C), however uses the simple pointer scheme from Section IV-B to minimize skip entries. DCNN sp uses a runlength encoding as discussed in Section VI-A. TTQ <ref type="bibr" target="#b16">[17]</ref> and INQ <ref type="bibr" target="#b17">[18]</ref> represent weights as 2-bit and 5-bit indirections, respectively. UCNN, TTQ and INQ model sizes are invariant to the bit-precision per weight. This is not true for DCNN sp, so we only show DCNN sp with 8 bits per weight to make it more competitive. TTQ and INQ cannot reduce model size further due to weight sparsity: e.g., a run-length encoding would outweigh the benefit because their representation is smaller than the run-length code metadata.</p><p>UCNN models with G &gt; 1 are significantly smaller than DCNN sp for all weight densities. However, UCNN G = 1 (no activation group reuse) results in a larger model size than DCNN sp for models with higher weight density.</p><p>We now compare UCNN's model size with that of TTQ and INQ. At the 50% weight density point, UCNN G = 4 (used for TTQ) requires ∼ 3.3 bits per weight. If density drops to 30%, model size drops to &lt; 3 bits per weight, which <ref type="bibr" target="#b16">[17]</ref> shows results in ∼ 1% accuracy loss. At the 90% weight density point, UCNN G = 2 (used for INQ) requires 5-6 bits per weight. Overall, we see that UCNN model sizes are competitive with the best known quantization schemes, and simultaneously give the ability to reduce energy on-chip.</p><p>Effect of jump-based indirection tables. Section IV-C discussed how to reduce model size for UCNN further by replacing the pointers in the input indirection table with jumps. The downside of this scheme is possible performance overhead: if the jump width isn't large enough, jumps will be needed to reach the next weight which results in bubbles. We show these effects on INQ-trained ResNet in Figure <ref type="figure" target="#fig_9">14</ref>. There are two takeaways. First, in the G = 1 case, we can shrink the bits/weight by 3 bits (from 11 to 8) without incurring serious performance overhead (∼ 2%). In that case, the G = 1 point never exceeds the model size for DCNN sp with 8 bit weights. Second, for the G = 2 case we can shrink the bits/weight by 1 bit (from 6 to 5), matching INQ's model size with negligible performance penalty. We note that the same effect can be achieved if the INQ model weight density drops below 60%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hardware RTL Results</head><p>Finally, Table VI-E shows the area overhead of UCNN mechanisms at the PE. We implement both DCNN and UCNN PEs in Verilog, using 16 bit precision weights/activations. Synthesis uses a 32 nm process, and both designs meet timing at 1 GHz. Area numbers for SRAM were obtained from CACTI <ref type="bibr" target="#b29">[30]</ref> and the area for logic comes from synthesis. For a throughput-normalized comparison, and to match the performance study in Section VI-C, we report area numbers for the DCNN PE with V K = 2 and the UCNN PE with G = 2,U = 17.    Provisioned with a weight buffer F of 17 entries, the UCNN PE adds 17% area overhead compared to a DCNN PE. If we provision for 256 weights to improve design flexibility (Section IV-E), this overhead increases to 24%. Our UCNN design multiplexes a single MAC unit between G = 2 filters and gates the PE datapath when the indirection table outputs a skip entry (Section VI-C). The RTL evaluation reproduces the performance results from our performance model (Section VI-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Weight quantization. There is a rich line of work that studies DNN machine efficiency-result accuracy trade-offs by skipping zeros in DNNs and reducing DNN numerical precision (e.g., <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b16">[17]</ref>). Deep Compression <ref type="bibr" target="#b13">[14]</ref>, INQ <ref type="bibr" target="#b17">[18]</ref> and TTQ <ref type="bibr" target="#b16">[17]</ref> achieve competitive accuracy on different networks trained on Imagenet <ref type="bibr" target="#b21">[22]</ref>, although we note that TTQ loses several percent accuracy on ResNet <ref type="bibr" target="#b15">[16]</ref>. Our work strives to support (and improve efficiency for) all of these schemes in a precision and weight-quantization agnostic fashion.</p><p>Sparsity and sparse accelerators. DNN sparsity was first recognized by Optimal Brain Damage <ref type="bibr" target="#b32">[33]</ref> and more recently was adopted for modern networks in Han et al. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Since then, DNN accelerators have sought to save cycles and energy by exploiting sparse weights <ref type="bibr" target="#b18">[19]</ref>, activations <ref type="bibr" target="#b9">[10]</ref> or both <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Relative to our work, these works exploit savings though repeated zero weights, whereas we exploit repetition in zero or non-zero weights. As mentioned, we gain additional efficiency through weight sparsity.</p><p>Algorithms to exploit computation re-use in convolutions. Reducing computation via repeated weights draws inspiration from the Winograd style of convolution <ref type="bibr" target="#b33">[34]</ref>. Winograd factors out multiplies in convolution (similar to how we factorized dot products) by taking advantage of the predictable filter slide. Unlike weight repetition, Winograd is weight/input "repetition un-aware", can't exploit cross-filter weight repetition, loses effectiveness for non-unit strides and only works for convolutions. Depending on quantization, weight repetition architectures can exploit more opportunity. On the other hand, Winograd maintains a more regular computation and is thus more suitable for general purpose devices such as GPUs. Thus, we consider it important future work to study how to combine these techniques to get the best of both worlds.</p><p>TTQ <ref type="bibr" target="#b16">[17]</ref> mentions that multiplies can be replaced with a table lookup (code book) indexed by activation. This is similar to partial produce reuse (Section III-C), however faces challenges in achieving net efficiency improvements. For example: an 8 bit and 16 bit fixed point multiply in 32 nm is .1 and .4 pJ, respectively. The corresponding table lookups (512entry 8 bit and 32K-entry 16 bit SRAMs) cost .17 and 2.5 pJ, respectively <ref type="bibr" target="#b29">[30]</ref>. Thus, replacing the multiplication with a lookup actually increases energy consumption. Our proposal gets a net-improvement by reusing compound expressions.</p><p>Architectures that exploit repeated weights. Deep compression <ref type="bibr" target="#b13">[14]</ref> and EIE <ref type="bibr" target="#b10">[11]</ref> propose weight sharing (same phenomena as repeated weights) to reduce weight storage, however do not explore ways to reduce/re-use sub computations (Section III) through shared weights. Further, their compression is less aggressive, and doesn't take advantage of overlapped repetitions across filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>This paper proposed UCNN, a novel CNN accelerator that exploits weight repetition to reduce on-chip multiplies/memory reads and to compress network model size. Compared to an Eyeriss-style CNN accelerator baseline, UCNN improves energy efficiency up to 3.7× on three contemporary CNNs. Our advantage grows to 4× when given dense weights. Indeed, we view our work as a first step towards generalizing sparse architectures: we should be exploiting repetition in all weights, not just zero weights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Standard (a) and different optimized (b, c) 1D convolutions that take advantage of repeated weight a. Arrows out of the grey bars indicate input/filter memory reads. Our goal is to reduce memory reads, multiplications and additions while obtaining the same result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. CNN parameters per convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Activation group reuse example (G = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 )</head><label>2</label><figDesc>Sort entries by activation group for the first filter k 1 .3) Within each activation group of k 1 , sort by sub-activation group for the second filter k 2 using the same canonical + m ) + b(l + y + h ) + a( n ) + b( k + x )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 0</head><label>1</label><figDesc>6×, respectively, relative to DCNN sp on ResNet-50 and 50% weight density. At the 90% weight density point, UCNN variants with U = 64 and U = 256 perform worse than DCNN sp on AlexNet and LeNet. These schemes use G =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11.Normalized runtime in cycles (lower is better) between DCNN sp and UCNN variants. Runtimes are normalized to DCNN sp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Performance study, comparing DCNN sp (V K = 1) and UCNN variants on the three networks from Section VI-A. The geometric means for all variants are shown in (d).</figDesc><graphic coords="13,88.49,72.48,439.61,142.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Model size (normalized per weight), as a function of weight density. UCNN indirection table entries are pointers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. UCNN model size (normalized per weight), decreasing jump entry width, for the INQ-trained ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Fig.6. DCNN/UCNN PE Architecture. Every component in grey is addition over the DCNN PE to design the UCNN PE. represents a DCNN vector lane andrepresents a UCNN vector lane. is an accumulator added to sum activation groups for dot product factorization. is an additional set of accumulators for storing subactivation group partial sums. There are G and G -1 accumulator registers in components and , respectively.</figDesc><table><row><cell cols="2">Input indirections (iiT)</cell><cell></cell><cell cols="2">Input Buffer</cell></row><row><cell></cell><cell></cell><cell>PE Control</cell><cell></cell></row><row><cell cols="2">Weight indirections (wiT)</cell><cell></cell><cell cols="2">Weight Buffer</cell></row><row><cell></cell><cell cols="2">Data dispatcher</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Inputs</cell><cell>Weights</cell><cell>Outputs</cell></row><row><cell>Partial Sum Buffer + Local accumulator</cell><cell>2</cell><cell>+ +</cell><cell>x</cell></row><row><cell></cell><cell>3</cell><cell>+ +</cell><cell>+</cell><cell>1</cell></row></table><note><p><p><p><p><p><p>PE along with its dataflow (Section IV-A). As described by Equation</p>2</p>, now the dot product operation is broken down into two separate steps in hardware:</p>1) An inner loop which sums all activations within an activation group. 2) An outer loop which multiplies the sum from Step 1 with the associated weight and accumulates the result into the register storing the partial sum.</p>Indirection</p>table sorting. Compared to DCNN, we now additionally need two indirection tables: the input indirection table (iiT) and the weight indirection table (wiT) as discussed in Section III-A. Since we work on an RSC t -size tile at a time, we need to load RSC t entries from both indirection tables into the PE at a time. Following Equation 2 directly, each entry in iiT and wiT is a log 2 RSC t and log 2 U -bit pointer, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>LegendFig.7. Example of activation group reuse for G = 2 with weights a, b. The indirection tables iiT and wiT are walked top to bottom (time moves down). At each step, the sequence of adds and multiplies needed to evaluate that step are shown right to left. Recall a MAC is a multiply followed by an add. We assume that at the start of building each sub-activation group, the state for accumulator is reset to 0. As shown, DCNN with two DCNN lanes processes these filters with 16 multiplies, whereas UCNN completes the same work in 6 multiplies.</figDesc><table><row><cell cols="2">Operation doing work for both filter k 1 and k 2</cell></row><row><cell>Operation doing work for filter k 2</cell><cell>Operation doing work for filter k 1</cell></row><row><cell>wiT 2 iiT 1</cell><cell></cell></row><row><cell>+</cell><cell>+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II UCNN</head><label>II</label><figDesc>, DCNN HARDWARE PARAMETERS WITH MEMORY SIZES SHOWN IN BYTES. FOR UCNN: L1 WT. (WEIGHT) IS GIVEN AS THE SUM OF WEIGHT TABLE STORAGE |iiT| + |wiT| + |F| (SECTION III-A).</figDesc><table><row><cell>Design</cell><cell cols="6">P V K V W G L1 inp. L1 wt.</cell></row><row><cell>DCNN</cell><cell>32</cell><cell>8</cell><cell>1</cell><cell>1</cell><cell>144</cell><cell>1152</cell></row><row><cell>DCNN sp</cell><cell>32</cell><cell>8</cell><cell>1</cell><cell>1</cell><cell>144</cell><cell>1152</cell></row><row><cell>UCNN (U = 3)</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>768</cell><cell>129</cell></row><row><cell cols="2">UCNN (U = 17) 32</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>1152</cell><cell>232</cell></row><row><cell cols="2">UCNN (U &gt; 17) 32</cell><cell>1</cell><cell>8</cell><cell>1</cell><cell>1920</cell><cell>652</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TableII) reduces DRAM energy by sharing input indirection tables across filters. Second, activation group reuse (for any G) reduces energy from arithmetic logic at the PE. Third, decreasing Fig.9. Energy consumption analysis of the three popular CNNs discussed in Section VI-A, running on UCNN and DCNN variants. UCNN variant UCNN Uxx is shown as U = xx. Left and right graphs show results using 8 bit and 16 bit weights, respectively. For each configuration, we look at 90%, 65% and 50% weight densities. In all cases, input density is 35%. Each group of results (for a given network and weight precision/density) is normalized to the DCNN configuration in that group.</figDesc><table><row><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2 1.4</cell><cell>LeNet, 8-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density L2</cell><cell>PE</cell><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2</cell><cell>LeNet, 16-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density</cell><cell>L2</cell><cell>PE</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2</cell><cell>AlexNet, 8-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density L2</cell><cell>PE</cell><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2</cell><cell>AlexNet, 16-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density</cell><cell>L2</cell><cell>PE</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2</cell><cell>ResNet, 8-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density L2</cell><cell>PE</cell><cell>Normalized Energy</cell><cell>0.4 0.6 0.8 1 1.2</cell><cell>ResNet, 16-bit 90% density</cell><cell>65% density</cell><cell>DRAM 50% density</cell><cell>L2</cell><cell>PE</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">weight density results in fewer entries per indirection table on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">average, which saves DRAM accesses and cycles to evaluate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">each filter. Combining these effects, UCNN U3, UCNN U17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and UCNN U256 reduce energy by up to 3.7×, 2.6× and 1.9×,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">respectively, relative to DCNN sp for ResNet-50 at 50% weight</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">density. We note that 50% weight density improves DCNN sp's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">efficiency since it can also exploit sparsity. Since DCNN cannot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">exploit sparsity, UCNN's improvement widens to 4.5×, 3.2×</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and 2.4× compared to DCNN, for the same configurations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Interestingly, when given relatively dense weights (i.e., 90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">density as with INQ training), the UCNN configurations attain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">a 4×, 2.4× and 1.5× improvement over DCNN sp. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">improvement for UCNN U3 increases relative to the 50% dense</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">case because DCNN sp is less effective in the dense-weights</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">regime.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III UCNN</head><label>III</label><figDesc>PE AREA BREAKDOWN (IN mm 2 ).</figDesc><table><row><cell>Component</cell><cell cols="2">DCNN(V K = 2) UCNN (G = 2,U = 17)</cell></row><row><cell>Input buffer</cell><cell>0.00135</cell><cell>0.00453</cell></row><row><cell>Indirection table</cell><cell>-</cell><cell>0.00100</cell></row><row><cell>Weight</cell><cell>0.00384</cell><cell>-</cell></row><row><cell>Partial Sum buffer</cell><cell>0.00577</cell><cell>0.00577</cell></row><row><cell>Arithmetic</cell><cell>0.00120</cell><cell>0.00244</cell></row><row><cell>Control Logic</cell><cell>0.00109</cell><cell>0.00171</cell></row><row><cell>Total</cell><cell>0.01325</cell><cell>0.01545</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Similar issues are faced by RLEs for sparsity<ref type="bibr" target="#b10">[11]</ref>,<ref type="bibr" target="#b26">[27]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For example, all but several ResNet-50<ref type="bibr" target="#b15">[16]</ref> layers can fit inputs on chip with 256 KB of storage and 8 bit activations.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ACKNOWLEDGEMENTS</head><p>We thank Joel Emer and Angshuman Parasher for many helpful discussions. We would also like to thank the anonymous reviewers and our shepherd Hadi Esmaeilzadeh, for their valuable feedback. This work was partially supported by NSF award CCF-1725734.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>def CNNLayer():</p><p>BUFFER</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>in_L2 [C][H][W]; BUFFER out_L2[K][H][W]; BUFFER wt_L2 [Kc][C][S][R]; (A)</head><p>for kc = 0 to K/Kc -1  DCNN/UCNN dataflow, parameterized for DCNN (Section IV-A). For simplicity, the PE is not vectorized and stride is assumed to be 1. [x:y] indicates a range; [:] implies all data in that dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>Measurement setup. We evaluate UCNN using a whole-chip performance and energy model, and design/synthesize the DCNN/UCNN PEs in RTL written in Verilog. All designs are evaluated in a 32 nm process, assuming a 1 GHz clock. For the energy model, energy numbers for arithmetic units are taken from <ref type="bibr" target="#b28">[29]</ref>, scaled to 32 nm. SRAM energies are taken from CACTI <ref type="bibr" target="#b29">[30]</ref>. For all SRAMs, we assume itrs-lop as this</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CVPR&apos;12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural networks and their economic applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and security in computing systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applications of artificial neural networks in medical science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current clinical pharmacology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="226" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Malmgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Borga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
		<title level="m">Artificial Neural Networks in Medicine and Biology: Proceedings of the ANNIMAB-1 Conference</title>
		<meeting><address><addrLine>Göteborg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2000-05">May 2000. 2012</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS&apos;12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<idno>MICRO&apos;14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cnvlutin: Ineffectual-neuron-free deep neural network computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<idno>ISCA&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">EIE: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>ISCA&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>ISCA&apos;17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Indatacenter performance analysis of a tensor processing unit,&quot; ISCA&apos;17</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>ICLR&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CVPR&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Trained ternary quantization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01064</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>ICLR&apos;17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cambricon-x: An accelerator for sparse neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>MICRO&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">10 quick train test</title>
		<ptr target="https://github.com/BVLC/caffe/blob/master/examples/cifar10/cifar" />
		<imprint/>
	</monogr>
	<note>Caffe cifar-10 cnn. prototxt</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR&apos;09</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimizing computation in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICANN</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on cpus</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
		<meeting>Deep Learning and Unsupervised Feature Learning NIPS Workshop</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<idno>ISCA&apos;16</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>ICML&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computing&apos;s energy problem (and what we can do about it)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSCC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cacti 6.0: A tool to understand large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Udipi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Balasubramonian</forename><surname>Hipc</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
