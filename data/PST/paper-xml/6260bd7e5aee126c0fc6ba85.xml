<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does Interference Exist When Training a Once-For-All Network?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-20">20 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jordan</forename><surname>Shipard</surname></persName>
							<email>jordan.shipard@hdr.</email>
							<affiliation key="aff0">
								<orgName type="department">Signal Processing</orgName>
								<orgName type="laboratory">Artificial Intelligence and Vision Technologies (SAIVT)</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnold</forename><surname>Wiliem</surname></persName>
							<email>arnoldw@sentientvision.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sentient Vision Systems</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
							<email>c.fookes@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Signal Processing</orgName>
								<orgName type="laboratory">Artificial Intelligence and Vision Technologies (SAIVT)</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Does Interference Exist When Training a Once-For-All Network?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-20">20 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.09210v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Once-For-All (OFA) method offers an excellent pathway to deploy a trained neural network model into multiple target platforms by utilising the supernet-subnet architecture. Once trained, a subnet can be derived from the supernet (both architecture and trained weights) and deployed directly to the target platform with little to no retraining or fine-tuning. To train the subnet population, OFA uses a novel training method called Progressive Shrinking (PS) which is designed to limit the negative impact of interference during training. It is believed that higher interference during training results in lower subnet population accuracies. In this work we take a second look at this interference effect. Surprisingly, we find that interference mitigation strategies do not have a large impact on the overall subnet population performance. Instead, we find the subnet architecture selection bias during training to be a more important aspect. To show this, we propose a simple-yet-effective method called Random Subnet Sampling (RSS), which does not have mitigation on the interference effect. Despite no mitigation, RSS is able to produce a better performing subnet population than PS in four smallto-medium-sized datasets; suggesting that the interference effect does not play a pivotal role in these datasets. Due to its simplicity, RSS provides a 1.9? reduction in training times compared to PS. A 6.1? reduction can also be achieved with a reasonable drop in performance when the number of RSS training epochs are reduced. Code available at https://github.com/Jordan-HS/RSS-Interference-CVPRW2022</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deploying deep neural network models for real-world applications requires accurate and fast inference <ref type="bibr" target="#b36">[37]</ref>. Generally, these accuracy and latency constrains are competing with one another, with only architecturally optimal networks being able to achieve both. Designing these optimal networks by hand is a challenging task and has led to the explosion of the neural architecture search (NAS) field. Some initial NAS methods <ref type="bibr">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref> were based on reinforcement learning approaches and proved the concept viable. Unfortunately, these approaches required extremely high computational resources as they train hundreds of architectures. Moreover, these only searched the architectures which optimise accuracy.</p><p>The goal of the approach eventually shifted to producing optimal networks with high accuracy and fast inference. To this end, two different approaches were developed: (1) the direct NAS methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>; and (2) the one-shot methods <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. The former constructs a continuous search space and utilises gradient descent for the search. Whilst, the later uses a discrete search space which only requires the neural network architecture search space to be trained once. This means the search becomes much faster and requires significantly fewer computational resources.</p><p>Previous one-shot methods <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref> use the trained search space as a guide for finding optimal networks. Once the optimal networks are found, they are trained from scratch before deployment. This presents a problem if we wish to deploy different network variants for various deployment platforms. To address this, the Once-For-All (OFA) method <ref type="bibr" target="#b3">[4]</ref> trains a supernet wherein a subnetwork/subnet architecture and its weights can be directly sampled from the supernet. Once sampled, a small amount of fine-tuning might be applied before the subnet is deployed; thus, side stepping the need for training the sampled subnets from scratch for each deployment platform.</p><p>To achieve its goal, OFA needs to train a large subnet population (2 ? 10 19 subnets) <ref type="bibr" target="#b3">[4]</ref>. When attempting to train the subnet population using naive approaches they find the subnets interfere with each other, resulting in significant accuracy drops. This is taken to suggest that modifying the weights of one subnet could affect the performance of other subnets in the subnet population. However, this interference is not currently well understood. To address the interference, OFA proposes a novel training method, called Progressive Shrinking (PS). PS trains the largest architecture first (i.e., the supernet) and then progressively samples and trains smaller subnet architectures. Recent work in <ref type="bibr" target="#b26">[27]</ref> showed that it is possible to improve the accuracy of the subnets by reducing the search space. The work suggests the accuracy gained is due to a reduction in interference resulting from the reduced search space. Therefore, the limiting factor on improving subnet population performance appears to be related to the interference effect between subnets during training.</p><p>In this work, we take a second look at this effect. In particular we ask the following questions: (1) Does the interference effect exists? (2) If it exists, then by how much does it affect the subnet population's performance? (3) If any other factors impact the subnet population's performance? To examine these questions, this paper introduces a simple method dubbed Random Subnet Sampling (RSS), which randomly samples a single subnet to train at each epoch. Obviously, RSS does not have any mitigation on the interference effect. We compare RSS with OFA's PS method on four datasets: MNIST <ref type="bibr" target="#b19">[20]</ref>, Fashion-MNIST <ref type="bibr" target="#b34">[35]</ref>, CI-FAR10 <ref type="bibr" target="#b16">[17]</ref>, and CIFAR100 <ref type="bibr" target="#b16">[17]</ref>. To our surprise, the subnet population is able to better generalise and achieve higher accuracies than PS. Fig. <ref type="figure" target="#fig_3">1</ref> shows the main findings from this work.</p><p>Our findings suggest that interference between subnets has a minimal effect on the subnet population's performance. Instead, we argue that bias in the subnet selection scheme during training has a larger impact on performance. When a subnet architecture is sampled and trained more often than the others, the subnet tends to have significantly higher accuracy. On the other hand, the performance of rarely sampled subnet architectures tend to have significantly lower accuracy. This sampling bias is analogous to the bias introduced when training a neural network model with an imbalanced dataset. The proposed RSS method addresses this sampling bias by uniformly sampling the subnet architecture during each epoch.</p><p>We also observe that the interference effect becomes more apparent when combining multiple subnets gradients during a single update step. This corroborates recent findings by Xu et al. <ref type="bibr" target="#b35">[36]</ref> in the Natural Language Processing (NLP) field. Contributions -Our contributions are listed as follows, 1. In contrast to the recent belief, we show that the interference has minimal effect when training the subnet population.</p><p>2. Instead, we argue that bias in the subnet selection scheme during training plays a bigger role.</p><p>3. We also show that the interference effect becomes more pronounced when combining the gradients of multiple subnets in a single update step.</p><p>4. To show the above points, we propose a simpleyet-effective method called Random Subnet Sampling (RSS). The proposed RSS method outperforms Once-For-All's Progressive Shrinking (PS) method which suggests point (1). In addition, the reason why RSS has good performance is because it addresses the bias problem as stated in point (2) and it only trains a single subnet for each epoch, in line with point (3).</p><p>We continue our paper as follows. Related works are discussed in Section 2. The subnet population training problem formulation is presented in Section 3. We then introduce the proposed method in Section 4 and discuss subnet sampling bias during training in Section 5. Section 6 presents the experimental results. Finally, Section 7 discusses conclusions and the future direction of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-Shot NAS -The goal of Neural Architecture Search (NAS) is to search for an optimal architecture in a large architecture search space. This search space can be continuous, in the case of direct NAS methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>, or discrete, in the case of one-shot methods <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b10">11]</ref>. Additionally, the name 'one-shot' refers to the subnet population only requiring to be trained once; whereas previous methods used reinforcement learning and trained hundreds of individual networks during search <ref type="bibr">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>. One-shot methods, such as Once-For-All <ref type="bibr" target="#b3">[4]</ref>, train large discrete search spaces using weight sharing techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref> removing the requirement to train every architecture. The relative accuracy of subnets in the subnet population is then used to find an optimal architecture. Once found, the architecture is trained from scratch as the previous shared weights are not fully optimised. As a further improvement, various methods were able to remove this need for retraining <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>, allowing subnets to be directly extracted from the subnet population. Despite its computational benefits, weight sharing is not a perfect solution as it is believed to introduce the problem of interference between subnets. In this work we look to further study this interference as it is currently under explored. Interference Effect in One-Shot Training -Most of the current understanding around the effects of interference come from observations made during the application of specific methods. Guo et al. <ref type="bibr" target="#b10">[11]</ref> states that the weights in the supernet are deeply coupled, although no further study to explain the underlying mechanism of coupling is provided. Cai et al. <ref type="bibr" target="#b3">[4]</ref> state that randomly selecting subnets for training causes interference and accuracy drops; which Sahni et al. <ref type="bibr" target="#b26">[27]</ref> echo and believe their compound heuristic is able to reduce interference and therefore improve accuracy. Liu et al. <ref type="bibr" target="#b33">[34]</ref> state that the existence of unnecessary neurons and connections in the supernet negatively impacts training and leads to greater interference. Perhaps, the most closely related study of interference is from the field of Natural Language Processing (NLP) by Xu et al. <ref type="bibr" target="#b35">[36]</ref>. They conduct a specific analysis of the interference effect during training a single-path one-shot NAS method. Their work finds that interference between subnets is caused by diverse gradient directions produced by multiple sampled subnets. Combining these diverse gradient directions would hamper the training progress. Furthermore, large architectural differences between sampled subnets contribute to the diverse gradient directions. Our work differs as we train a subnet population constructed from a computer vision-based neural network model, instead of a single-path subnet population for NLP. Despite these differences we find their findings to be consistent with our own. Bias in One-Shot Training -The effect of bias during oneshot training has been raised recently by Chu et al. <ref type="bibr" target="#b6">[7]</ref>. Their work observe an unfair bias in the previous one-shot methods' <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> supernet training as the cause for the poor correlation between the subnet proxy performance and its corresponding standalone trained performance (i.e., trained from scratch). Since the proxy performance is used as a guide when searching for optimal architectures, low correlation with the standalone trained performance will produce suboptimal architectures. To this end, they propose a strict fairness based training method. Despite bias being the core issue, their work only focus on the training fairness issue and does not extensively explore the bias itself. Different to their work, in this work we explain bias in the subnet sampling/selection during training of a OFA subnet population, to be analogous to bias in the data imbalance problem. This analogy allows us to borrow solutions developed for this problem to help us address the subnet sampling bias. Data Imbalance -Real world data is not balanced <ref type="bibr" target="#b31">[32]</ref> like the ones commonly used in scientific works. This data imbalance or bias is a well known problem within image classification and many solutions have been developed to address it. These include; Balanced sampling <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>; Hard mining <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> which directly relate to ideas used by Wang et al. <ref type="bibr" target="#b29">[30]</ref> for worst-up training; and focal loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>. As mentioned, we propose that the subnet sampling bias can be explained from the data imbalance perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Description</head><p>We follow the problem definition presented in the original OFA work <ref type="bibr" target="#b3">[4]</ref>. Let arch i ? A be the i-th subnet architecture. It is assumed that all subnet architectures in A can be derived/sampled from the supernet (the largest model amongst all). Let W o be the weights of the supernet and W arch i be the weights of arch i . We derive W arch i from W o via a selection function C(?). The subnet population training problem can then be formalised via a minimisation problem as,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arg min</head><p>Wo archi?A</p><formula xml:id="formula_0">L val (C(W o , arch i )),<label>(1)</label></formula><p>where L val is the loss on the validation set. Essentially, the above equation aims at minimising the loss of every subnet from A on the validation set. In the case of the OFA, and for this paper, the size of A is 2 ? 10 19 subnets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Subnet Population Training</head><p>Due to the large population size (i.e., |A| = 2 ? 10 19 ), directly minimising Eq. 1 is impractical. The general approach is to sample and train a subset ? ? A. The subset ? is sampled according to some selection scheme and set via C(?). Any subnet that does not belong to ? will still be indirectly trained due to its weights being shared with the subnets in ?. In this section we first briefly discuss PS, proposed in the original OFA paper <ref type="bibr" target="#b3">[4]</ref>, and then propose Random Subnet Sampling (RSS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Progressive Shrinking</head><p>The main idea of the PS <ref type="bibr" target="#b3">[4]</ref> is to train the subnets with respect to their Floating Point Operation (FLOP) size; from the largest one (i.e., supernet) to the smaller ones progressively.</p><p>The supernet is required to be trained prior to using the progressive shrinking method. Once the supernet is trained, PS trains the subnet population in three stages by performing a controlled sampling on three network parameters: (1) kernel size (e.g., 7x7, 3x3); (2) number of layers in each block (Depth); and (3) number of channels (Width).</p><p>In the first phase, named dynamic kernel training, only the kernel size is varied while the depth and width are kept at their maximum values. In this stage a single subnet is sampled for each update step. In the second phase, called dynamic depth training, the method varies both the number of layers and the kernel size, while width still remains at its maximum value. This stage samples two subnets and combines the gradient from both for each update step. Finally, in dynamic width training, the method varies all three parameters and combines the gradients from four sampled subnets during each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Random Subnet Sampling</head><p>We propose Random Subnet Sampling (RSS) which is not designed to mitigate the interference effect. In contrast to the progressive shrinking method, RSS does not perform any controlled sampling. It samples the subnets by varying all three network parameters used in PS (i.e., the kernel size, depth and width). We use uniform randomness to choose the value for each parameter. Unlike PS which samples subnets for each update step, or batch, RSS samples a single subnet for each epoch. We show later that per-epoch sampling is a more effective method than per-batch sampling. Algorithm 1 illustrates the proposed RSS method.</p><p>Algorithm 1 Pseudo code for the proposed Random Subnet Sampling (RSS) method. Kernel settings, width settings and depth settings are sets of values.</p><p>For instance, width settings = {3, 4, 6}. rand(?) is a sampling function that uniformly samples values from the input argument. The kernel, width and depth settings are one dimensional vectors with the length prescribed as follows.</p><p>The length of the kernel settings vector (subnet k) and width settings vector (subnet e) is equal to the maximum depth setting multiplied by the number of blocks in the supernet. As an example, if the supernet has five blocks, with the maximum depth setting being four. Our kernel and width settings vectors would therefore contain 20 values (i.e. 5 blocks ? 4 layers = 20). The length of the depth settings vector (subnet d) is equal to the number of blocks. As we see from Algorithm 1, the values of these three setting vectors are randomised between lines 2-3. The subnet is then derived by passing the sampled settings into the selection function C(?) and trained. More specifically, we follow OFA <ref type="bibr" target="#b3">[4]</ref> to derive the subnet and its weights from the supernet by using the sampled settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Subnet Sampling Bias</head><p>Different sampling strategies between PS and the proposed RSS can be studied from the sampling bias perspective. We argue that Eq. 1, bears similarities to the standard classification problem <ref type="bibr" target="#b18">[19]</ref>. To train a classification model, we can solve the following minimisation problem.</p><formula xml:id="formula_1">min W * M i=1 L train (W * , x i , y i ),<label>(2)</label></formula><p>Where x i and y i are the i-th data point and its corresponding class label. W * is the neural network weights and L train is the training loss. Similar to Eq. 1, the above minimisation problem aims at reducing the loss for each data point in the training set, (x i , y i ) M i=1 . The difference for Eq. 1 is that instead of summing the loss over all the data points, it sums the loss over all the subnet architectures.</p><p>Linking the subnet population training problem presented in Eq. 1 with the classification problem in Eq. 2 is appealing as we can use the tools/experience developed in the classification problem into this field. For instance, bias in the data is one of the big themes in the classification field <ref type="bibr" target="#b31">[32]</ref>. When training a network with an imbalanced training set (i.e., some classes have significant higher number of training data than others), the network will tend to have stronger confidence scores towards these large classes.</p><p>Using this new perspective, we argue that PS is a biased selection scheme, as it initially trains the supernet before training the largest to smallest subnets. As described in Section 4.1, PS has three phases which progressively varies the three network parameters. As an example, during dynamic kernel training the width and depth are set at their maximum values. This results in only larger subnets being trained during this stage. Overall this could lead to the effect obsereved by Chu et al. <ref type="bibr" target="#b6">[7]</ref>, with the larger subnets (especially the supernet) performing better than smaller subnets, simply due to more training.</p><p>Unlike PS, RSS randomly samples subnets for the entire training duration. This means, the subnet sampling is not biased towards the larger subnets. We will show in our results that the subnet sampling bias does indeed play a significant role in the overall performance; and that this role is similar to the data sampling bias in the classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>We contrast between the proposed RSS method and PS in this section. If the interference effect has significant contribution, then RSS will have significant lower performance than PS. First, the experimental set-up and the datasets are discussed. Then, we present our main results. Finally, we show additional ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment Setup</head><p>Datasets and Methods -The datasets used for comparison are MNIST <ref type="bibr" target="#b19">[20]</ref>, Fashion-MNIST (FMNIST) <ref type="bibr" target="#b34">[35]</ref>, CI-FAR10 <ref type="bibr" target="#b16">[17]</ref>, CIFAR100 <ref type="bibr" target="#b16">[17]</ref>. These datasets were used to cover a range of dataset complexities with MNIST being the least complex and CIFAR100 being the most complex.</p><p>We compare three training methods as follows. All training was conducted on a single Nvidia RTX3080 Graphics Processing Unit (GPU) with a initial learning rate of 0.01 using cosine learning rate decay; batch size of 64; momentum of 0.9; weight decay of 3e -5 and used crossentropy loss, unless otherwise specified. Subnet Population Architecture -In this work, the Mo-bileNetV3 <ref type="bibr" target="#b13">[14]</ref> is used as the base architecture. consistent with the settings used by OFA. The subnet population is defined along three mutable dimensions, the kernel size, layer width (number of channels), and layer depth (number of layers for each block). The possible settings for each dimension are shown below.</p><p>? Kernel ? {3 ? 3, 5 ? 5, 7 ? 7}.</p><p>? Width ? {3, 4, 6}.</p><p>? Depth ? {2, 3, 4}.</p><p>The supernet consists of five configurable blocks, shown in Fig. <ref type="figure" target="#fig_4">2</ref>. All subnets are unique combinations of the kernel, width and depth settings. The kernel is set layer by layer and controls the active kernel size. The width, or expansion ratio, is also set layer by layer and is a multiplier for the base channel count of each layer. The depth is set for each block and controls the number of active layers in that block. We follow OFA <ref type="bibr" target="#b3">[4]</ref> for transforming supernet weights to derive the prescribed subnet. Evaluation Protocol -OFA <ref type="bibr" target="#b3">[4]</ref> evaluate their training methods performance according to the top performing subnets at various latency constraints. However, improving the performance of top-performing subnets may not correlate to a better performing subnet population. We instead wish to evaluate the average performance of the overall subnet population. This is done by randomly sampling a group of subnets according to the specific architectural size measurement of Mega FLoating-point Operation Per Seconds (MFLOPs). The performance of all sampled subnets is then recorded on the test set. For all datasets, we use the following architecture sizes of evenly spaced MFLOPs values {4, 6, 8, 10, 12, 14}. Subnets of size 6-12 MFLOPs can be found by simply randomising all settings until the resulting subnet is within the required MFLOP range, ?0.5 in our case. Subnets with size 4 and 14 MFLOPs are rare and challenging to sample in this manner. As such, to find these subnets, the width setting is locked at its minimum value to aid sampling of 4 MFLOP subnets; and locked at its maximum to aid sampling of 14 MFLOP subnets. We use the wall clock to measure the training time for each method from the start of the first epoch to the conclusion of the last epoch.   RSS is able to consistently train a better performing subnet population for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Main Results</head><p>Subnet Population Performance -The main results are presented in Fig. <ref type="figure" target="#fig_6">3</ref>. Across all datasets, RSS achieves the best performing subnet population. The gap between RSS and OFA grows larger as the difficulty of the dataset increases (i.e., from MNIST to CIFAR). Furthermore, the range of best performing to worst performing subnets is significantly smaller for RSS methods. These results suggest that the interference effect between subnets during training is negligible. In Section 6.3 we show RSS's consistent performance across different subnet sizes is due to both its unbiased subnet sampling scheme and reduction of interference. In contrast to RSS, OFA shows a significant bias towards larger subnets in all datasets. We attribute this to its initial supernet training and PS training procedure which starts from the larger subnets. Subnet sampling bias is further studied in Section 6.3.2. Training Time -Fig. <ref type="figure" target="#fig_8">4</ref> shows the number of hours required to train each of the methods. Showing RSS and RSS-short are an average 1.9 and 6.1 times faster than OFA respectively. For RSS-short this is expected as it runs for nearly half the epochs of OFA; however, RSS and OFA run for the same number of epochs. This speed up is due to two factors: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Why does RSS outperform OFA?</head><p>Our main results suggest that both proposed RSS variants outperform OFA. In this section we conduct further investigations by studying the two main differences between the methods: (1) the number and frequency of subnets sampled during training; and (2) the selection scheme for selecting subnets during training. We only present the results from CIFAR100 dataset for the study. We choose CI-FAR100 as it offers more variations and complexity com- pared to the other datasets. Furthermore, we only use RSS with the standard number of training epochs (590). Results (in supplementary material) on RSS-Short are similar; in addition to showing our results are not limited to specific hyperparemeter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Effect of Number of Subnets Sampled</head><p>The proposed RSS method samples a single subnet to train each epoch, whereas OFA samples and trains up to four subnets per update step. In this section, we modify the RSS subnet sampling to be similar to OFA, allowing us to compare subnet sampling methods on RSS. More specifically, we first change RSS's sampling from each epoch into sampling each batch (i.e., per update step). We then increase the number of sampled subnets for each batch from one to two. When sampling two or more subnets, we combine gradients in each update step. Fig. <ref type="figure" target="#fig_9">5</ref> shows that despite training fewer subnets, per-epoch sampling produces a better performing subnet population than per-batch sampling. Indicating interference may be more severe in perbatch strategies. As an alternative explanation, we speculate there could be a benefit to allowing sampled subnets to train on the entire dataset instead of only a single batch. Future study is required to investigate this. The interference again appears to increase between sampling one and two subnets per batch. We conjecture this increased interference is likely due to the combining of diverse gradient directions, as also shown in <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Effect of Different Selection Schemes</head><p>The previous section investigated the number of subnets to sample. In this section, we study the subnet selection scheme. As mentioned in Section 5, the selection scheme can induce bias which skews the subnet population's performance towards the more frequently sampled subnets. In this section, we intentionally use biased subnet selection schemes to study the effects on the subnet population. Single subnet selection -The simplest biased subnet selection scheme to test is training a single subnet. More specifically, we compare three variants of this biased selection scheme: (1) smallest only, with all settings at their minimum values; (2) middle only, where kernel=5, expand=4 and depth=3 for all blocks; and (3) largest only, where all settings are set to their maximum (i.e. the supernet). Fig. <ref type="figure" target="#fig_10">6</ref> shows that the subnet performance is skewed towards the size of the selected subnet. Furthermore, the performance drops off as soon as the subnet size deviates from the selected subnet size. Indicating excessive sampling of a subnet during training induces a strong bias in the subnet population's performance. Two subnet selection -Fig. <ref type="figure" target="#fig_10">6</ref> shows that single subnet selection introduces a bias towards the selected subnet. In this part, we examine the effect of training two subnets. More specifically, we select either the smallest or the largest subnet for training. We compare three variants of selection schemes: (1) select the largest subnet for the first half of the total epochs, then select the smallest subnet the rest of the training (Max then Min); (2) select the smallest subnet for the first half of the total epochs, then select the largest subnet the rest of the training (Min then Max); and (3) alternating between smallest and largest subnets each epoch (Alternating). Unlike OFA which uses cosine learning rate decay, in this part of experiment, we use a constant learning rate throughout the training course. We report the results in Fig. <ref type="figure" target="#fig_11">7</ref>.</p><p>Both Min then Max and Max then Min schemes skew their results towards the subnet most recently trained; however, it is clear that both subnet populations benefited from the training of an additional subnet. This is shown by Max then Min having better performing subnets at larger MFLOPs when compared to training the smallest subnet only in Fig. <ref type="figure" target="#fig_10">6</ref>. The same can be observed for Min then Max at lower MFLOPs when compared to training the largest subnet only in Fig. <ref type="figure" target="#fig_10">6</ref>. Interestingly, we do not see mirrored results between Min then Max and Max then Min schemes. This shows that the training sequence within the selection scheme is an important factor.</p><p>Another interesting observation is that the Max then Min scheme has lower performance than the Min then Max scheme. OFA has a similar strategy to the Max then Min. However, OFA uses a decreasing learning rate and a more controlled selection scheme which might help to mitigate this issue. A future study on this is warranted.</p><p>The Alternating scheme produces the best results with a more evenly trained subnet population and only slight bias towards the largest and smallest subnets. This further suggests that mitigating the subnet sampling bias during training is an important issue to address. Interestingly, despite never being trained, the subnets in ranges 8-10 MFLOPs  perform nearly on par with the larger and smaller subnets. The Alternating scheme uses an alternating selection strategy, producing a less biased subnet population towards both trained subnet sizes. The proposed RSS is a step further by enlarging the subnet selection from two subnets to the whole subnet population. Moreover, RSS uses random sampling which enables each individual subnet to have the same chance to be selected for training. The results show there is benefit to sampling and training more subnets as RSS produces a better performing subnet population than the Alternating scheme and the other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This work revisited the interference effect in an OFA based subnet population during training. Interference was believed to be the constraining factor in achieving good performance. To limit interference, OFA proposed progressive shrinking as a novel training method. However, we showed that interference is not the only constraining factor. Additionally, we must consider the subnet sampling bias of our selection scheme during training. Subnet sampling bias states that subnets which receive the most training will perform the best. We drew this conclusion by first connecting the subnet sampling bias to the data imbalance problem in general classification training. We then demonstrate this by proposing a simple-yet-effective training method called Random Subnet Sampling (RSS), which only mitigates the sampling bias, not interference. By mitigating only the bias, RSS trains a better performing subnet population than progressive shrinking on four small-to-medium datasets while also being 1.9 times faster. Furthering our experiments provided insight into both the interference and bias problems. We found interference to be more significant when combing the gradients of multiple sampled subnets. Additionally, we found the impact of bias depends on the subnet training sequence. In the future, we plan to test our hypothesis in large computer vision datasets such as the ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for the Paper: Does Interference Exist When Training a</head><p>Once-For-All Network?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">RSS-Short Ablation Results</head><p>We conducted additional ablation experiments to study the effect of the number of subnets sampled and the selection scheme on the proposed RSS-Short method. As mentioned earlier, RSS trains for 590 epochs and RSS-Short trains for 180 epochs. Other than this, all RSS-Short experiments shared the same hyperparameters as their RSS counterparts. All experiments are conducted on the CIFAR100 dataset <ref type="bibr">[3]</ref>.   <ref type="figure" target="#fig_6">3</ref> differ slightly from the RSS results presented in the paper. For Max then Min, min has better performance than max; for Min then Max, max has better performance than min; and for Alternating, min and max have similar performance. The accuracy of subnets in the 8-10 MFLOP range trained via the Alternating selection scheme perform significantly worse than smaller and larger subnets. This result is not seen in the main results, suggesting that additional epochs are required to reduce the bias in the subnet selection scheme.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Effect of Number of Subnets Sampled</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Effect of Different Selection Schemes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hyperparameter Ablation Studies</head><p>We alter various hyperparameters to ensure our findings are not limited to specific hyperparameter values. We adjust the batch size, dropout rate and learning rate, comparing the effect on RSS-Short and OFA. Fig. <ref type="figure" target="#fig_8">4</ref> shows that increasing the batch size from 64 to 128 results in a overall accuracy drop for both RSS-Short and OFA. Fig. <ref type="figure" target="#fig_9">5</ref> shows that increasing the dropout rate from 0.1 to 0.3 again results in a decrease in accuracy for both methods, with a more significant decrease for OFA. Lastly, Fig. <ref type="figure" target="#fig_10">6</ref> shows that increasing the learning rate from 0.01 to 0.02 reduces the accuracy of both methods. These results show that hyperparameter changes have the same effect for both methods. Confirming our results are not limited to specific hyperparameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ProxylessNAS Base Architecture</head><p>We change the base architecture used during training, showing that results obtained are not unique to the Mo-bileNetV3 <ref type="bibr">[2]</ref> base architecture. Fig. <ref type="figure" target="#fig_11">7</ref> shows this as we switch to using a ProxylessNAS <ref type="bibr">[1]</ref> base architecture and achieve similar results. The population settings remain the same as before; however, the resulting subnet population only ranges from 4 MFLOPs to 12 MFLOPs. Therefore, we only show subnets from sizes 4, 6, 8, 10 and 12 MFLOPs.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a ) 2 Figure 1 .</head><label>a21</label><figDesc>Figure 1. Training time reduction and accuracy improvements of the proposed Random Subnet Sampling (RSS) compared to the Once-For-All (OFA) method. Results on CIFAR100 dataset [17].</figDesc><graphic url="image-1.png" coords="2,60.00,71.84,209.20,172.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Input: kernel settings, width settings, depth settings, n epochs Output: Trained subnet population 1: while i ? n epochs do 2: subnet k ? rand(kernel settings ) 3: subnet e ? rand(width settings ) 4: subnet d ? rand(depth settings ) 5: subnet ? C(subnet k, subnet e, subnet d) 6: train subnet for one epoch 7: end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>OFA [4]-The progressive shrinking method is used. RSS-The proposed random subnet sampling with the same number of training epochs as OFA. RSS-short-The proposed RSS with the same number of training epochs as only OFA's supernet training.The training epochs for each method are shown in Tab. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 .</head><label>1</label><figDesc>From our empirical observations (provided in supplementary material), our findings are still consistent when a different base architecture is used. The subnet derivation from the base architecture is Training epochs for each method on each dataset where (s) denotes supernet training and (ps) denotes progressive shrinking training. For CIFAR10 and CIFAR100 we use the same training protocol as detailed by Cai et al. [4]. Training on MNIST and F-MNIST are scaled versions of the same CIFAR10 and CI-FAR100 training with less epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Diagram showing the MobileNetV3 [14] base architecture used to sample subnets with kernel ? {3x3,5x5,7x7}, width ? {3,4,6} and depth ? {2,3,4}.</figDesc><graphic url="image-3.png" coords="5,308.86,572.31,242.36,96.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>MNIST Subnet Population Performance. (b) FMNIST Subnet Population Performance. (c) CIFAR10 Subnet Population Performance. (d) CIFAR100 Subnet Population Performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparisons between OFA, RSS (proposed), and RSS-Short (proposed) subnet population performance across four datasets.RSS is able to consistently train a better performing subnet population for all datasets.</figDesc><graphic url="image-6.png" coords="6,56.65,252.65,223.05,167.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 1 )</head><label>1</label><figDesc>RSS is more likely to train smaller subnets each epoch; and (2) RSS only trains a single subnet each epoch where OFA's PS trains two subnets during dynamic depth training and four during dynamic width training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparisons of the training time, in hours, for each method as measured on a single Nvidia RTX3080 GPU.</figDesc><graphic url="image-8.png" coords="7,58.43,71.85,219.49,171.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Effect of sampling different numbers of subnets during training. The 1 per epoch strategy is the default RSS method.When sampling 2 per batch the gradients are combined for the update step. As we can see, performance drops for per-batch strategies. The drop is more significant when more subnets are sampled per-batch (i.e., 2 per batch).</figDesc><graphic url="image-9.png" coords="8,66.78,71.85,202.78,152.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Resulting population performance from training a single subnet only. The smallest only uses only the smallest subnet which is constructed by settings all settings to their minimum. The middle only is when kernel=5, expand=4 and depth=3. The largest only is the supernet, when all settings are at their maximum. The results show a heavy bias towards the selected subnet.</figDesc><graphic url="image-10.png" coords="8,66.78,324.04,202.78,153.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Resulting population performance from training the same two largest and smallest subnets in differing sequences, showing that training sequence does matter.</figDesc><graphic url="image-11.png" coords="8,325.66,71.85,202.78,152.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 shows the results from comparing per epoch to per batch sampling during training. These results follow the results presented in the paper with a decrease in population accuracy between per epoch and per batch methods. A further drop occurs when training two subnets per batch and combining their gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figs. 2</head><label>2</label><figDesc>Figs.2 and 3show the results from altering the subnet selection scheme during training. In Fig.2, the selec-</figDesc><graphic url="image-12.png" coords="11,56.65,486.17,223.05,171.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. RSS-Short (proposed) population results from training a single subnet only. The results are the same as in the main ablation results.</figDesc><graphic url="image-13.png" coords="11,315.52,497.14,223.05,171.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. RSS-Short (proposed) population results from training the same largest and smallest subnets in different sequences. These results show a more significant bias towards the largest and smallest subnets than the main ablation results.</figDesc><graphic url="image-14.png" coords="12,66.78,71.85,202.78,155.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( a )</head><label>a</label><figDesc>Resulting RSS-Short (proposed) subnet populations. (b) Resulting OFA subnet populations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Resulting RSS-Short (proposed) and OFA subnet populations from increasing the batch size during training. These results show that the batch size has the same effect for both methods.</figDesc><graphic url="image-16.png" coords="12,66.78,471.29,202.78,155.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Resulting RSS-Short (proposed) and OFA subnet populations from increasing the drop-out rate. These results show that both methods suffer from an increased dropout rate with OFA having a more significant accuracy drop.</figDesc><graphic url="image-19.png" coords="13,74.45,292.27,207.46,155.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>( a )</head><label>a</label><figDesc>Resulting RSS-Short (proposed) subnet populations. (b) Resulting OFA subnet populations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Resulting RSS-Short (proposed) and OFA subnet populations from increasing learning rate. These results show that the learning rate decreases the accuracy of both methods.</figDesc><graphic url="image-20.png" coords="13,315.65,292.27,202.78,155.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Resulting OFA, RSS-Short and RSS subnet populations from training with a ProxylessNAS[1]  base architecture on CI-FAR100[3]. These results are consistent with the main results despite a lower overall accuracy for each method than with the Mo-bileNetV3[2]  base architecture.</figDesc><graphic url="image-21.png" coords="13,196.22,508.53,202.78,155.59" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was partly supported by <rs type="institution">Sentient Vision Systems. Sentient Vision Systems</rs> is one of the leading Australian developers of computer vision and artificial intelligence software solutions for defence and civilian applications.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1611.02167</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Nikhil Naik, and Ramesh Raskar</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smash: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1708.05344</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005">2020. 1, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/1511.05641</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fbnetv3: Joint architecture-recipe search using predictor pretraining</title>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P?ter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Focal Loss Improves the Model Performance on Multi-Label Image Classifications with Imbalanced Data</title>
		<author>
			<persName><forename type="first">Jianxiang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Industrial Control Network And System Engineering Research</title>
		<meeting>the 2nd International Conference on Industrial Control Network And System Engineering Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multiple resampling method for learning from imbalanced data sets</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Estabrooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<idno>abs/1609.09106</idno>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName><forename type="first">Shou-Yong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ponas: Progressive oneshot neural architecture search for very efficient deployment</title>
		<author>
			<persName><forename type="first">Sian-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ta</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving identification of difficult small classes by balancing class distribution</title>
		<author>
			<persName><forename type="first">Jorma</forename><surname>Laurikkala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AIME</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mnist handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="j">ATT Labs</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2005">2010. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Loddon Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1511.06343</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<idno>arXiv: 1802.03268. 3</idno>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Leon Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CompOFA: Compound once-for-all networks for faster multi-platform deployment</title>
		<author>
			<persName><forename type="first">Manas</forename><surname>Sahni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Varshini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alind</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Conference on Learning Representations, ICLR &apos;21</title>
		<meeting>of the 9th International Conference on Learning Representations, ICLR &apos;21</meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Kumar Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537</idno>
		<idno>arXiv: 1412.6537. 3</idno>
		<title level="m">Fracking Deep Convolutional Image Descriptors</title>
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attentivenas: Improving neural architecture search via attentive sampling</title>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="6414" to="6423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The effect of class distribution on classifier learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foster</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><surname>Provost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P?ter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive automatic design of search space for one-shot neural architecture search</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>CoRR, abs/1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Analyzing and Mitigating Interference in Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12821</idno>
		<idno>arXiv: 2108.12821. 2</idno>
		<imprint>
			<date type="published" when="2021-08">Aug. 2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scaling for edge inference of deep neural networks</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thaddeus</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Niemier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Electronics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaling up neural architecture search with big single-stage models</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Bignas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.01578</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Resulting RSS-Short (proposed) subnet populations. (b) Resulting OFA subnet populations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
