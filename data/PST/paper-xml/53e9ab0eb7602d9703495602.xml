<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximate Caches for Packet Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Francis</forename><surname>Chang</surname></persName>
							<email>francis@cse.ogi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Systems Software Laboratory OGI School of Science and Engineering at OHSU</orgName>
								<address>
									<settlement>Beaverton, Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wu-Chang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems Software Laboratory OGI School of Science and Engineering at OHSU</orgName>
								<address>
									<settlement>Beaverton, Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Li</surname></persName>
							<email>kangli@acm.org</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Georgia Athens</orgName>
								<address>
									<settlement>Georgia</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Approximate Caches for Packet Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D49B4E3B81B246578E3B07E9D9C027AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bloom filter</term>
					<term>packet classification</term>
					<term>caches</term>
					<term>probabilistic algorithms</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many network devices such as routers and firewalls employ caches to take advantage of temporal locality of packet headers in order to speed up packet processing decisions. Traditionally, cache designs trade off time and space with the goal of balancing the overall cost and performance of the device. In this paper, we examine another axis of the design space that has not been previously considered: accuracy. In particular, we quantify the benefits of relaxing the accuracy of the cache on the cost and performance of packet classification caches. Our cache design is based on the popular Bloom filter data structure. This paper provides a model for optimizing Bloom filters for this purpose, as well as extensions to the data structure to support graceful aging, bounded misclassification rates, and multiple binary predicates. Given this, we show that such caches can provide nearly an order of magnitude cost savings at the expense of misclassifying one billionth of packets for IPv6-based caches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Modern network devices such as firewalls, network address translators, and edge routers rely on fast packet classification in order to perform well. These services require that packets be classified based on a set of rules that are applied to not only the destination address, but also flow identifiers such as source address, layer-4 protocol type, and port numbers. Unfortunately, packet classification is a very complex task. Because of this, there has been a large amount of work in developing more efficient classification algorithms <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b11">[12]</ref>[17] <ref type="bibr" target="#b20">[22]</ref>[30] <ref type="bibr" target="#b30">[33]</ref>. Still, in the context of highperformance networks, the hardware requirements of performing full classification on each packet at line rates can be overwhelming <ref type="bibr" target="#b23">[25]</ref>.</p><p>To increase the performance of a packet classification engine, a cache is often employed to take advantage of temporal locality <ref type="bibr" target="#b7">[8]</ref>. For example, caching has been shown to increase performance significantly with route lookups <ref type="bibr" target="#b19">[21]</ref> <ref type="bibr" target="#b31">[34]</ref>. Caches are typically evaluated along two axes: size and performance. As additional storage is added, cache hit rates and performance increase. Unlike route caches that only need to store destination address information, packet classification caches require the storage of full packet headers. Unfortunately, due to the increasing size of packet headers (the eventual deployment of IPv6 <ref type="bibr" target="#b16">[18]</ref>), storing full header information can be prohibitive given the cost of the highspeed memory that would be used implement such a cache. To address this problem, this paper examines a third axis for designing packet classification caches: accuracy.</p><p>In particular, we seek to answer the following question:</p><p>What are the quantifiable benefits that relaxing the accuracy of a cache has on the size and performance of packet classification caches?</p><p>While there are many ways of exploring this axis, this paper examines one approach for doing so through the use of a modified Bloom Filter. In this approach, classified packets satisfying a binary predicate are inserted into the filter that caches the decision. Subsequent packets then query the filter to quickly test membership before being processed further. Packets that hit in the filter are processed immediately, based on the predicate, while packets that miss go through the full packet classification lookup process.</p><p>In this paper, we briefly describe the Bloom filter and analyze its properties. In particular, we examine the exact relationship between the size and dimension of the filter, the number of flows that can be supported, and the misclassification probability incurred. While Bloom filters are good for storing binary set membership information, realistic network devices classify packets into many, possibly disjoint, sets. To address this limitation, we extend the basic approach to support multiple binary predicates and analyze its expected performance. Another issue in using Bloom filters in such a manner is the highly dynamic nature of the "dictionary" of packet headers. In particular, such a cache must be able to evict stale entries and preserve a bounded maximum misclassification rate. To address these issues, we present the design and evaluation of extensions for gracefully aging the cache over time to minimize misclassification. We also explore the design and implementation of such a cache on a modern network processor platform.</p><p>Section II covers related work and Section III introduces Bloom filters. Section IV extends the Bloom filters to support multiple binary predicates while Section V analyzes extensions for gracefully aging the cache. Section VI explores the performance impact of running a Bloom filter on a network processor, while Section VII discusses the potential results of misclassification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Due to the high processing costs of packet classification, network appliance designers have resorted to using caches to speed up packet processing time. Early work in network cache design borrowed concepts from computer architecture (LRU stacks, set-associative multi-level caches) <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b24">[26]</ref>. Some caching strategies rely on CPU L1 and L2 cache <ref type="bibr" target="#b19">[21]</ref> while others attempt to map the IP address space to memory address space to use the hardware TLB <ref type="bibr" target="#b23">[25]</ref>. Another approach is to add an explicit timeout to an LRU set-associative cache to improve performance by reducing thrashing <ref type="bibr" target="#b31">[34]</ref>. More recently, in addition to the leveraging the temporal locality observed on networks, approaches to improving cache performance have applied techniques to compress and cache IP ranges to take advantage of the spatial locality in the address space of flow identifiers <ref type="bibr">[7][16]</ref>. This effectively allows multiple flows to be cached to a single cache entry, so that the entire cache may be placed into small high-speed memory such as a processor's L1/L2 cache.</p><p>Much of this work is not applicable to layer-4 flow identification that is the motivation for our work. Additionally, all of these bodies of work are fundamentally different from the material presented in this paper, because they only consider exact caching strategies. Our approach attempts to balance performance and resource requirements with an allowable error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THEORY</head><p>We use a Bloom-filter to construct our approximate cache. A Bloom filter is a space-efficient data structure designed to store and query set-membership information <ref type="bibr" target="#b0">[1]</ref>.</p><p>Bloom filters were originally invented to store large amounts of static data (for example, hyphenation rules on English words). In recent years, this data structure has been rediscovered by the networking community, and has become a key component in many networking systems <ref type="bibr" target="#b2">[3]</ref>[24] <ref type="bibr" target="#b29">[32]</ref>.</p><p>Applications of Bloom filters in computer networking include web caching <ref type="bibr" target="#b10">[11]</ref>, active queue management <ref type="bibr" target="#b12">[13]</ref>, IP traceback <ref type="bibr" target="#b25">[28]</ref>[29], and resource routing <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Bloom Filter</head><p>In our implementation, a Bloom filter data structure consists of L N M × = bins. (Each bin consists of one bit.) These bins are organized into L levels with N bins in each level, to create L N virtual bins (possible permutations). To interact with the Bloom filter, we maintain independent hash functions, each associated with one bin level. Each hash function maps an element into one of the N bins in that level. For each element of the set, e e e S K = , we compute the L hash functions, and set all of the corresponding bins to 1. To test membership of any element in our Bloom filter, we compute the L hash functions, and test if all of the corresponding buckets are set to 1. See Figure <ref type="figure" target="#fig_0">1</ref> for an example.</p><p>This approach may generate false positives -a Bloom filter may incorrectly report that an element is a member of the set S -but a Bloom filter will never generate false negatives.</p><p>For optimal performance, each of the L hash functions,</p><formula xml:id="formula_0">L H H H , , ,<label>2 1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>, should be a member of the class of universal hash functions <ref type="bibr" target="#b4">[5]</ref>. That is, each hash function should distribute elements evenly over the hash's address space, and for each hash function ]</p><formula xml:id="formula_1">1 [ : N e H K , the probability of 2 distinct elements colliding is N 1 . That is to say, ( ) N b a b H a H P 1 ), ( ) ( = = .</formula><p>In practice, we apply only one hash function, ] 1 [ :</p><formula xml:id="formula_2">L N e H K</formula><p>, for each insertion or query operation, and simply use different portions of the resulting hash to implement the L hash functions.</p><p>Our definition of a Bloom filter differs slightly from the original definition <ref type="bibr" target="#b0">[1]</ref>, where each of the L hash functions can address all of the M bit buckets. This definition of the Bloom filter is often used in current designs due to potential parallelization gains to be had by artificially partitioning memory <ref type="bibr" target="#b12">[13]</ref>. It should be noted that this approach yields a slightly worse probability of false positives under the same conditions, but an equal asymptotic false-positive rate <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Properties of the Bloom Filter</head><p>In order to better design our cache and understand its limitations, it is important to understand the behavioural properties of a Bloom filter. In particular, we are interested in how the misclassification probability, and the size of the Bloom filter, will affect the number of elements that it can store. Let us take the example of a firewall to motivate our analysis. The rationale of a firewall is to restrict and censor traffic between the internal and external networks. A firewall acts as both an entry point into and exit point from the network. As such, it must be able to process all traffic travelling to and from a network at line speed. This makes it a simple example in which to apply our approximate cache. Allowed flows are inserted into the cache, while new and censored flows are not.  </p><formula xml:id="formula_3">1 0 1 0 1 0 0 0 0 0 1 1 L k N p = 1 1 1</formula><p>For our purposes, we need to know how many elements (flows), k , we can store in our bloom filter, without exceeding some misclassification probability, p . Solving for k , we get . So, we construct k ,</p><p>From this equation, it is clear that the number of elements, , a Bloom filter can support scales linearly with the amount of memory M . The relative error of this approximation, k , grows linearly with the number of hash functions L , and decreases with increasing M . For the purposes of our application of this approximation, the relative error is negligible 1 .</p><p>Note that solving for p in this equation yields the more popular expression <ref type="bibr">[3][11]</ref>[29],</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(</head><p>) L M L e p = 1 1 For 1024 M bytes, and 50 L , the relative error is less than 0.35%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dimensioning a Bloom Filter</head><p>Bloom-filter design was originally motivated by the need to store dictionaries in memory. The underlying design assumption is that the data is static. However, this assumption no longer holds when dealing with network traffic. Previous work has often attempted to dimension a Bloom filter such that the misclassification rate is minimized for a fixed number of elements <ref type="bibr" target="#b2">[3]</ref>.</p><p>To apply Bloom filters to the problem of storing a cache, we prefer to maximize the number of elements, k , that a Bloom filter can store, without exceeding a fixed maximum tolerable misclassification rate, p . To maximize as a function of L , we take the derivative, dL d</p><p>, set it to 0, and solve for L to find the local maximum.   </p><formula xml:id="formula_4">1 , 0 p , then [ ] 1 , 0 u</formula><p>, so u only has one solution, = u ½, which means is maximized for</p><formula xml:id="formula_5">p p L 2 log 2 ln / ln = =</formula><p>This is an interesting result, because it implies that L is invariant with respect to the size of the Bloom filter, M .</p><p>Another interesting implication of this equation is that the Bloom filter is "optimally full" when half of all the buckets are set (</p><formula xml:id="formula_6">L p 2 1 = ).</formula><p>It should be noted that the accuracy of this approximation ( k ) increases with M . In our testing, for cache sizes greater than 1KB, this approximation yields no error. In all the simulations presented in this paper, this approximation and the optimal value of L are equal. Even if we choose a slightly sub-optimal value of L , the difference in the maximum number of flows the Bloom filter can store is negligible (Figure <ref type="figure">2</ref>). For comparison, we will reintroduce the concept of an exact cache -the traditional cache that does not yield false positives or negatives.</p><p>A less obvious implication of this approximation is the relationship between the amount of memory, M, the number of elements (flows), k, and the probability of a false positive, p .</p><p>Figure <ref type="figure">3</ref> graphs the relationship between p and k. We can see that the relationship is roughly logarithmic. This approximation serves as a good guide for ranges of two orders of magnitude or less.</p><p>Since the optimal choice of L is asymptotically invariant with respect to M , and is proportional to M , we can assert that k is linearly related to M . A visual representation of this relationship is depicted in Figure <ref type="figure" target="#fig_4">4</ref>. Note that a Bloom filter cache with a misclassification rate of one in a billion can store more than twice as many flows as an exact IPv4 cache, and almost 8 times as much as an exact IPv6 cache. (Each entry in an exact IPv6 (37 bytes) cache consumes almost 3 times as much memory as an IPv4 entry (13 bytes) <ref type="bibr" target="#b16">[18]</ref>.)</p><p>It is also important to note that, with our scheme, it is possible to store mixed IPv4/IPv6 traffic without making any major changes to our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To summarize:</head><p>• The optimal value of L , the number of levels, is invariant with respect to the size of the Bloom filter, M . • The number of elements, k , and the misclassification probability, p , are roughly logarithmically related. • k is linearly related to M .</p><p>• An optimally full Bloom filter has ½ of its bits set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTIPLE PREDICATES</head><p>Our first extension to the Bloom filter is to extend its storage capability to support multiple binary predicates (as opposed to the single binary predicate yes/no data storage of a traditional Bloom filter). This extension is needed for more sophisticated applications, such as routers, which need to record forwarding interfaces.</p><p>We propose a modification to our existing algorithm that allows us to store multiple binary predicates, while preserving the desired original operating characteristics of the Bloom filter cache.</p><p>Consider a router with I interfaces. The cache would be required to store a routing interface number. To support this, a data structure that can record I binary predicates is required. To store this information, we will construct a cache composed of I Bloom filters.</p><p>Suppose we are caching a flow, e , that should be routed to the th i interface. We would simply insert e into the th i Bloom filter in our cache. This encoding scheme is similar to "1-hot" encoding.</p><p>To query the cache for the forwarding interface number of flow e , we will simply need to query all I Bloom filters. If e is a member of the th i Bloom filter, this implies that flow e should be forwarded through the th i interface.</p><p>If e is not a member of any Bloom filter, e has not been cached. In the unlikely event that more than one Bloom filter claims e as a member, we have a confounding result. One solution to this problem is to treat the cache lookup as a miss by reclassifying e . This approach preserves correctness while adding only minimal operating overhead.</p><p>The probability of misclassification, p , with this algorithm is</p><formula xml:id="formula_7">( ) [ ] ( ) I L k N p = 1 1 1 1 1</formula><p>Solving for k , the maximum number of flows this approach can store, we find   ( 
)</p><formula xml:id="formula_8">( ) [ ] ( ) ( ) N p k L I = 1 1 ln 1 1 1 ln 1 /</formula><formula xml:id="formula_9">) 2 ln( 1 1 ln / 1 I p L =</formula><p>The proposed extension to the Bloom filter cache requires increasing the number of memory accesses by a factor of I . Additional memory accesses can incur a serious performance penalty. Taking advantage of the memory bus width can easily mitigate this disadvantage by the following technique: Consider a Bloom filter in which each bucket can store a pattern of I bits, where bit i represents interface i . When adding a packet to the bloom filter, we would only update bit i of each bucket.</p><p>When querying the modified Bloom filter for a flow, e , we will take the results from each level of the bloom filter, and AND the results. An example is depicted in Figure <ref type="figure">5</ref>.</p><p>Thus, a router, with I Bloom filters, each Bloom filter having L hash levels, need only make L memory accesses to insert or query the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Non-Uniform Distributions</head><p>The equations presented earlier in Section 5 assume that elements are evenly distributed over the multiple binary predicates. If the elements are not evenly distributed, our modified Bloom filter can become polluted in a short amount of time.</p><p>For example, consider a router with 16 interfaces (binary predicates), using 1KB of memory and a misclassification probability, p , of 1e-9. If flows are distributed evenly over the interfaces, this configuration can support 167 elements. Conversely, if 90% of flows set the first predicate, it would require only 13 elements to "fill" this Bloom filter.  <ref type="figure">7</ref>.) This approach ensures that set bits are uniformly distributed throughout the cache, even when the elements are not evenly distributed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Predicate Comparison</head><p>It is important to examine how the multiple binarypredicates Bloom cache compares to the single-predicate case to understand how our extension affects the behaviour of our Bloom filter cache.</p><p>As discussed previously, the single-bit Bloom filter cache can store a maximum of ) 1 ln( </p><formula xml:id="formula_10">( ) [ ] ( ) M L p k L I = 1 ln 1 1 1 ln 1 / 1</formula><p>Applying the approximation This is an important property, because it means that our proposed algorithm preserves the behaviour of the single binary predicate cache.</p><formula xml:id="formula_11">N e N 1 1 1 we find ( ) [ ] ( ) L I p L M = 1 / 1 1 1 1 ln When L is optimized, becomes ( ) ( ) ( ) [ ] ( ) v I I p p M / 1 / 1 max 1 1 1 ln 1 1 ln ) 2 ln( = where ( ) ( ) I p v / 1</formula><p>Figure <ref type="figure">6</ref> compares the difference in the maximum number of flows that can be stored by a multi-bit Bloom filter cache.</p><p>To better determine the relative performance of the multiple binary predicate and the single-binary-predicate cache approaches, we take the difference in the maximum number of flows that each design will accommodate.</p><p>The difference of the two approaches is,</p><formula xml:id="formula_12">( ) ( ) ( ) ( ) = p p M I ln 1 1 1 ln 1 2 ln / 1 2 max max</formula><p>For p &lt;&lt; 1, ( )</p><formula xml:id="formula_13">I p p I 1 1 / 1</formula><p>, giving If I is not very big, as is the case when considering the number of interfaces of a router (for reference, a Juniper T640 routing node has 160 interfaces) then p ln &gt;&gt; I ln , we can approximate by This is an overestimate of the difference. So, we can say that, at worst, this approach scales logarithmically with I (for M and p constant).</p><p>It is surprising how effective this approach is (Figure <ref type="figure">6</ref>). The algorithm does not pollute the Bloom Filter (setting more bits) than the single binary-predicate approach. However, it is slightly more susceptible to pollution (each membership query examines I L × bits, as opposed to the L bits of the single binary predicate Bloom filter). It should be noted that the multi-predicate solution is a superset of the single-predicate solution -setting I to 1 yields the equations presented in Section 4.1.</p><p>V. BLOOM FILTER AGING Our second extension to the Bloom filter is adding the ability to evict stale entries from the cache. Bloom filters were originally designed to store set membership information of unchanging, or expanding sets. We must adapt this algorithm to allow graceful eviction of elements to use this data structure effectively in a dynamic environment such as the Internet.</p><p>The first step towards developing an algorithm to age a Bloom filter is to decide how much information has already been stored in the cache. A simple method of deciding when the cache is full is to choose a maximum tolerable misclassification probability, p </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K</head><p>be the fractions of buckets in each level of the Bloom filter that are set. The probability of misclassification is simply the product of i 's. This method will accurately estimate the misclassification probability. The drawback to this approach is that it will require counting the exact number of bits we set, complicating later parallel access implementations of this algorithm, as well as adding several per-packet floating-point operations.</p><p>We can devise a simpler estimate of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cold Cache</head><p>This naïve approach to the problem of Bloom filter aging involves simply emptying the cache whenever the Bloom filter becomes "full."</p><p>The main advantage to this solution is that it makes full use of all of the memory devoted to the cache, as well as The disadvantages, however, are quite drastic when considering the context of a high-performance cache:</p><p>• While the cache is being emptied, it cannot be used.</p><p>• Immediately after the cache is emptied, all previously cached flows must be re-classified, causing a load spike in the classification engine. • Zeroing out the cache may cause a high amount of memory access.</p><p>This approach mainly serves as a reference point to benchmark further algorithm refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Double-Buffering</head><p>If we partition the memory devoted to the cache into two Bloom filters, an active cache and a warm-up cache, we can more gracefully age our cache. This approach is similar to the one applied in Stochastic Fair Blue <ref type="bibr" target="#b12">[13]</ref>. The basic algorithm is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>when a new packet arrives if the flow id is in the active cache if the active cache is more than ½ full insert the flow id into the warm-up cache allow packet to proceed otherwise perform a full classification if the classifier allows the packet insert the flow id into the active cache if the active cache is more than ½ full insert the flow id into the warm-up cache allow packet to proceed if the active cache is full switch the active cache and warm-up cache zero out the old active cache</head><p>The goal of this approach is to avoid the high number of cache misses immediately following cache cleaning. By switching to a background cache, we can start from a "warmed-up" state. This approach can be thought of as an extremely rough approximation of LRU.</p><p>However, this approach also has its drawbacks:</p><p>• Double the memory requirement to store the same number of concurrent flows, as compared to the coldcache case. • Zeroing out the expired cache still causes a load spike in the use of the memory bus (although it is a smaller spike). This can be partially mitigated by slowly zeroing out memory.</p><p>• In the simplest implementation, this algorithm can potentially double the number of memory accesses required to store a new flow. This performance loss can be recovered by memory aligning the two bloom filters, so that fetching a word of memory will return the bit states of both Bloom filters. Now, let us turn to the problem of applying this information to age the Bloom filter cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>For evaluation purposes, we used two datasets, each of one hour in length. The first of the datasets was collected by Bell Labs research, Murray Hill, NJ, at the end of May 2002. This dataset was made available through a joint project between NLANR PMA and Internet Traffic Research Group [27]. The trace was of a 9 Mb/s Internet link, serving a staff of 400 people.</p><p>The second trace was a non-anonymized trace collected at Table <ref type="table" target="#tab_8">1</ref> presents a summary of the statistics of these two datasets. A graph of the number of concurrent flows is shown in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>For the purposes of our analysis, a bi-directional flow is considered as 2 independent flows. A flow begins when the first packet bearing a unique 5-tuple (source IP address, destination IP address, protocol, source port, destination port) arrives at our node. A flow ends when the last packet is observed, or after a 60 second timeout. The timeout is chosen in accordance with other measurement studies <ref type="bibr" target="#b13">[15]</ref>, and observations in the field <ref type="bibr" target="#b19">[21]</ref> <ref type="bibr" target="#b21">[23]</ref>.</p><p>As a reference, we introduce the idea of a perfect cache -a fully associative cache, with an infinite amount of memory. This cache only takes compulsory cache misses (the theoretical minimum). The fundamental performance statistics are reported in Table <ref type="table" target="#tab_9">2</ref>.</p><p>For a comparison with exact caching schemes, we simulate a fully associative cache using an LRU replacement policy. The performance of this scheme is presented in Figure <ref type="figure" target="#fig_8">9</ref>. LRU was chosen because of its near-optimal caching performance in networking contexts <ref type="bibr">[21][34]</ref>.</p><p>This simulation is intended to represent best-case exact caching performance, even though it is infeasible to implement a fully associative cache on this scale.</p><p>For our simulation, we use the SHA1 hash function <ref type="bibr">[14]</ref>. It should be noted that the cryptographic strength of the SHA1 hash does not increase the effectiveness of our implementation. It is important to recognize that other, faster hashing algorithms exist. Using a hardware-based hashing implementation is also possible. In the IXP1200 <ref type="bibr" target="#b18">[20]</ref>, the hardware hash unit can complete a hashing operation every nine clock cycles.</p><p>For the purposes of this study, we use a misclassification probability of one in a billion. Typically, TCP checksums will fail for approximately 1 in 1100 to 1 in 32000 packets, even when link-level CRCs should only admit error rates of 1 in 4 billion errors. On average, between 1 in 16 million to 1 in 10 billion TCP packets will contain an undetectable error <ref type="bibr" target="#b28">[31]</ref>. We contend that an imprecision of this magnitude will not meaningfully degrade network reliability.</p><p>To support an exact IPv4 cache, a 4-way set associative IPv4 cache requires a 52-byte memory read on each cache lookup. A 4-way associative IPv6 cache would require a 148-   byte memory access. To support an approximate cache with an error rate of 1 in a billion would require 30 1-bit memory fetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cold Cache Performance</head><p>With the Bell dataset, the cold cache performs reasonably, using 4 KB of cache memory, and a misclassification probability of 1e-9. The optimal dimensions for this Bloom filter this size should have 30 hash functions, storing a maximum of 611 flows.</p><p>Throughout the 1-hour trace, there were no misclassifications and an overall cache hit-rate of 95.15%. Aggregated over 100ms intervals, there were a maximum of 8 cache misses/100ms, with an average of 1.32 and a variance of 10.33.</p><p>Figure <ref type="figure" target="#fig_9">10</ref> illustrates the cache misses during a portion of the trace. We can see that emptying the cache corresponds to a spike in the amount of cache misses that is not present when using a perfect cache. This spike is proportional to the number of concurrent flows. This type of behaviour will apply undue pressure to the classification engine, resulting in overall performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Double-Buffering Performance</head><p>Using a double-buffered approach can smooth the spikes in cache misses associated with suddenly emptying the cache. Double-buffering effectively halves the amount of immediately addressable memory, in exchange for a smoother aging function. As a result, this bloom filter was only able to store 305 flows for a 4096 byte cache, in comparison with the 611 flows of the cold-cache implementation. This implementation had a slightly lower hit rate of 95.04% with the Bell dataset. However, we succeeded in reducing the variance to 5.43 while maintaining an average cache miss rate of 1.34/100ms. Viewing Figure <ref type="figure" target="#fig_9">10</ref>, we can see that the correspondence between cache aging states and miss rates does not correspond to performance spikes as prevalently as in the cold cache implementation.  This implies that the double-buffered approach is an effective approach to smoothing out the performance spikes present in the cold cache algorithm. To better quantify the "smoothness" of the cache miss rate, we graph the variance, and average miss rates (Figure <ref type="figure" target="#fig_10">11</ref> and Figure <ref type="figure" target="#fig_11">12</ref>).</p><p>From Figure <ref type="figure" target="#fig_8">9</ref> and Figure <ref type="figure" target="#fig_10">11</ref>, we observe that for a memory-starved system, the cold-cache approach is more effective with respect to cache hit-rates. It is surprising how effective this naïve caching strategy is, with respect to overall cache performance. Moreover, we note that it performs better than both an IPv6, and IPv4 exact cache, with both datasets for a memory poor cache, and keeps pace as memory improves. As the amount of memory increases, we can see that the double-buffered approach is slightly more effective in reducing the number of cache misses.</p><p>Looking to Figure <ref type="figure" target="#fig_11">12</ref>, we observe that the variance in miss rates decreases much faster in the double-buffered case than in the cold-cache approach. It is interesting to note that in the OGI trace, the variance actually increases, before it decreases. Interpreting Figure <ref type="figure" target="#fig_10">11</ref> and Figure <ref type="figure" target="#fig_11">12</ref>, we can see that for a very memory-starved system, the variance is low because the cache miss rate is uniformly terrible.</p><p>Comparing the double-buffered approximate cache implementation to exact caching gives comparable performance when considering an IPv4 exact cache, even though the approximate approach can cache many more flows. This is due to the imprecision of the aging algorithm -an LRU replacement policy can evict individual flows for replacement, whereas a double-buffered approach must evict ½ of cached flows at a time. However, when considering IPv6 data structures, this disadvantage is overshadowed by the pure amount of storage capacity a Bloom filter can draw upon.</p><p>It is important to note that in all of these graphs, the behaviour of each of the systems approaches optimum as memory increases. This implies that our algorithm is correct and does not suffer fundamental design issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. HARDWARE OVERHEAD A preliminary implementation on Intel's IXP1200</head><p>Network Processor <ref type="bibr" target="#b18">[20]</ref> was constructed, to estimate the amount of processing overhead a Bloom filter would add.</p><p>The hardware tested was an IXP1200 board, with a 200 MHz StrongARM, 6 packet-processing microengines and 16 ethernet ports.</p><p>A simple micro-engine level layer-3 forwarder was implemented as a baseline measurement. A Bloom filter cache implementation was then grafted onto the layer-3 forwarder code base. A null-classifier was used, so that we could isolate the overhead associated with the Bloom filter lookup function. No cache aging strategy was used. The cache was placed into SRAM, because scratchpad memory does not have a pipelined memory access queue, and the SDRAM interface does not support atomic bit-set operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. IXP Overhead</head><p>The performance of our implementation was evaluated on a simulated IXP1200 system, with 16 virtual ports. The implementation's input buffers were kept constantly filled, and we monitored the average throughput of the system. The Bloom filter cache implementation was constructed in a way to ensure that no flow identifier was successfully matched, and each packet required an insertion of its flow ID into the Bloom filter. This was done so that the worst possible performance of a Bloom filter cache could be ascertained. The code was structured in a way to disallow any shortcutting or early negative membership confirmation. The performance results of the IXP implementation are presented in Table <ref type="table" target="#tab_10">3</ref>.</p><p>The IXP is far from an ideal architecture to implement a Bloom filter, in large part due to its lack of small, high-speed bit-addressable on-chip memory. Since there is no memory cache, data must be retained in on-chip registers during processing. The small number of available registers limits the performance of more complex tasks, which can be seen by the sharp drop-off in performance of a 5-level Bloom filter. Ideally, a Bloom filter would be implemented in hardware that supports parallel access on bit-addressable memory <ref type="bibr" target="#b25">[28]</ref>. A simple cheap custom ASIC can be constructed to implement a Bloom filter, effectively. This implementation uses the hardware hash unit. In this case, one hash is as difficult to calculate as four, because we simply use different portions of the generated hash to implement multiple hash functions.</p><p>Next generation IXP2000 hardware will feature 2560 bytes of on-chip memory per micro-engine, with up to 16 microengines per board. The memory access time will be 3 cycles, a vast improvement over the 16-20 cycles latencies of IXP1200 SRAM. A 2560 byte Bloom filter can store 467 elements. The next-generation micro-engines have a concept of "neighbouring" so that micro-engines can easily and quickly pass packet-processing execution to the next micro-engine "in line". This could allow for a high-speed implementation of a Bloom filter where each micro-engine performs one or two memory look-ups so that the costs of a Bloom filter could be distributed across all the micro-engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DEALING WITH MISCLASSIFICATION</head><p>The immediate question that arises when we introduce the possibility of a misclassification is to account for the result of the misclassifications. Let us first consider the case for a firewall. </p><formula xml:id="formula_14">F F F , , , 2 1 K unique flows ) ( L q</formula><p>were to set bits in the Bloom filter that matched the signature to a new flow, F , we will accept F as a previously validated flow.</p><p>In the case that F is a valid flow, no harm is done, even though F would never have been analyzed by the packet classifier. If F is a flow that would have been rejected by the classification engine then there may be more serious repercussions -the cache would have instructed the firewall to admit a bad flow into the network. This case can be rectified for TCP based flows by forcing all TCP SYN packets through the classification engine.</p><p>Another solution would be to periodically reclassify packets that have previously been marked as cached. If a misclassification is detected, all bits corresponding to the signature of the flow id could be zeroed. This approach has the drawback of initially admitting bad packets into the network, as well as causing flows which share similar flow signatures to be reclassified.</p><p>If an attacker wanted to craft an attack on the firewall to allow a malicious flow, F , into the network, they could theoretically construct flows,</p><formula xml:id="formula_15">L F F F , , ,<label>2 1</label></formula><p>K , that would match the flow signature of F . If the firewall's internal hash functions were well known, this could effectively open a hole in the firewall.</p><p>To prevent this possibility, constants internal hash functions should not only be openly advertised, just as it is inadvisable to publish private keys. An additional measure would be to randomly choose the hash functions that the firewall uses. New hash functions can easily be changed as the Bloom filter ages.</p><p>In the case of a router, a misclassified flow could mean that a flow is potentially misrouted, resulting in an artificially terminated connection. In a practical sense, the problem can be corrected by an application or user controlled retry. In the case of UDP and TCP, a new ephemeral port would be chosen, and network connectivity can continue.</p><p>If we randomly force cached flows to be re-classified, we can reduce this "fatal" error to a transient one. TCP retransmit, and application-level UDP error handlers may make this failure transparent to the user.</p><p>The severity of these errors must be taken in into the context of the current Internet and TCP. To prevent IP spoofing attacks, TCP uses a 16-bit randomized initial sequence number. An attacker can already guess an initial sequence number of a TCP stream with a success rate of 1 in 2 16 . UDP packets are not even required to maintain a data checksum. In the Linux implementation of the network stack, even corrupt UDP packets are passed to the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. FUTURE WORK</head><p>The aging functions discussed in this paper are inefficient, in the sense that it under-utilizes the Bloom filter's memory address. In the case of the cold cache algorithm, the Bloom filter is emptied. The Bloom filter is in a constant state of being under-utilized. Using the double-buffered algorithm introduces redundancy through the duplication of data. As in the cold cache algorithm, the Bloom filters are also underutilized. It is possible that an algorithm based on randomly zeroing bits may prove to be an effective aging function -in this manner, we may be able to take advantage of the knowledge that a Bloom filter at optimum performance has ½ of its bits set.</p><p>Many of the implementation details of our architecture share common characteristics with IP traceback. Designing a system to support traceback in addition to caching could prove successful.</p><p>Using an approximate caching system presents us with a unique opportunity to dynamically balance the trade-off between accuracy and performance.</p><p>In more traditional caches, during times of high load, cache performance decreases due to increasing cache misses. Intuitively, this behaviour may be sub-optimal. Ideally, performance should increase with workload. Our usage of a Bloom filter presents us with the opportunity to increase the effective size of the cache without consuming more memory by simply increasing the misclassification probability. This allows us the opportunity to increase cache performance in response to high amounts of traffic. Although more packets are misclassified, even more packets would be correctly forwarded. This may be better than the alternative -dropping more packets in response to increasing work.</p><p>The goal of the feedback system should be to balance the misclassification probability, p , with an acceptable cache performance/hit rate, h . To quantify this balance, we construct a "desirability" . The shape of function f must be chosen by the network administrator, to reflect the operator's preference in balancing hit rate and misclassification rate. The function f should be a monotonically increasing function for a constant p , and monotonically decreasing for a constant h . Thus, we can view the choice of p as the result of a feedback system. A feedback controller would monitor the performance of the cache, and tune p , with the explicit goal of maximizing f . IX. CONCLUSION Typical packet classification caches trade-off size and performance. In this paper, we have explored the benefits that introducing inaccuracy has on packet classification caches. Using a modified Bloom filter, we have shown that allowing a small amount of misclassification can decrease the size of packet classification cache by almost an order of magnitude over exact caches without reducing hit rates. With the deployment of IPv6 and the storage required to support caching of its headers, such a trade-off will become increasingly important.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example: A Bloom filter with 5 = N bins and 3 = L hash levels. Suppose we wish to insert an element, e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Flows, k (in thousands) Number of Hash Levels, L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The maximum number of flows that can be stored by a 512KB cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The relationship between the amount of memory, M , and the maximum number of elements, k (Bloom M Filters use an optimum value of L )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Number of concurrent flows in test data sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cache hit rates as a function of memory, M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparing cold cache and double-buffered bloom caches using 4 KB of memory (Bell dataset) ("Aging intervals" represents transition points in time only, and does not represent any "vertical magnitude")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average cache misses as a function of memory, M (aggregate over 100ms timescales)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Variance of cache misses as a function of memory, M (aggregate over 100ms timescales)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Immediately, we can see that the two approaches are still linearly related in M . (Note that I and p are constants.)</figDesc><table><row><cell></cell><cell cols="2">( ) e Figure 5: An example: A modified Bloom filter with 5 H 1 ( ) e H 2 buckets and 2 hash levels, supporting a router with 8 00000010 00100000 00000000 00000000 00100000 00000000 01000000 01000000 00000000 00000010</cell><cell cols="3">H 1</cell><cell cols="4">( ) e</cell><cell></cell><cell></cell><cell></cell><cell cols="5">00000010 00000000 00000100 00000000 01000000</cell><cell></cell><cell cols="3">H 2</cell><cell>( ) e</cell><cell>00000100 00000000 01000000 00000000 00000010</cell></row><row><cell></cell><cell cols="2">interfaces. Suppose we wish to cache a flow e that gets routed to interface number 2.</cell><cell cols="19">Figure 7: As before, suppose flow e is to be forwarded to interface 2. Now, let us suppose that 3 ) ( = e H . So,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>j</cell><cell>=</cell><cell cols="2">(</cell><cell>i</cell><cell>+</cell><cell>H</cell><cell>(</cell><cell>e</cell><cell>))</cell><cell>mod</cell><cell>I</cell><cell>=</cell><cell>(</cell><cell>2</cell><cell>+</cell><cell>) 3</cell><cell cols="2">mod</cell><cell>8</cell><cell>=</cell><cell>5</cell><cell>.</cell></row><row><cell>Maximum Number of Flows, k (in thousands)</cell><cell>500 1000 1500 2000 2500 3000 3500</cell><cell>Bloom Filter Cache, 1 Interface Bloom Filter Cache, 4 Interfaces Bloom Filter Cache, 16 Interfaces Bloom Filter Cache, 64 Interfaces Exact Cache, IPv4 Exact Cache, IPv6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0 0 Figure 6: Effect of storing routing information on effective 2 4 6 8 10 12 14 16 Amount of Memory, M (in MegaBytes) cache size, 9 1 = e p , using optimal Bloom filter dimensions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell cols="2">ln</cell><cell>1</cell><cell></cell><cell>1 ln(</cell><cell>2</cell><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>Sample Trace Statistics</figDesc><table><row><cell></cell><cell>Bell Trace</cell><cell>OGI Trace</cell></row><row><cell>Trace Length (seconds)</cell><cell>3600</cell><cell>3600</cell></row><row><cell>Number of Packets</cell><cell>974613</cell><cell>15607297</cell></row><row><cell>UDP Packets</cell><cell>671471</cell><cell>10572965</cell></row><row><cell>TCP Packets</cell><cell>303142</cell><cell>5034332</cell></row><row><cell>Number of Flows</cell><cell>32507</cell><cell>160087</cell></row><row><cell>Number of TCP Flows</cell><cell>30337</cell><cell>82673</cell></row><row><cell>Number of UDP Flows</cell><cell>2170</cell><cell>77414</cell></row><row><cell>Avg. Flow Length (seconds)</cell><cell>3.2654</cell><cell>10.2072</cell></row><row><cell>Avg. TCP Flow Length (seconds)</cell><cell>13.8395</cell><cell>11.2555</cell></row><row><cell>Avg. UDP Flow Length (seconds)</cell><cell>155.0410</cell><cell>9.0877</cell></row><row><cell>Longest Flow (seconds)</cell><cell>3599.95</cell><cell>3600</cell></row><row><cell>Avg. Packets/Flow</cell><cell>29.9816</cell><cell>97.4926</cell></row><row><cell>Avg. Packets/TCP Flow</cell><cell>9.9925</cell><cell>60.8945</cell></row><row><cell>Avg. Packets/UDP Flow</cell><cell>309.434</cell><cell>136.577</cell></row><row><cell>Max # of Concurrent Flows</cell><cell>268</cell><cell>567</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 :</head><label>2</label><figDesc>The results of a perfect cache Sample trace statistics</figDesc><table><row><cell>Bell Trace</cell><cell>OGI Trace</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Performance of Bloom Filter cache in worst case possible configuration</figDesc><table><row><cell>Number of</cell><cell>All-Miss Cache</cell></row><row><cell>Hash Levels</cell><cell>Throughput</cell></row><row><cell>0</cell><cell>990 Mb/s</cell></row><row><cell>1</cell><cell>868 Mb/s</cell></row><row><cell>2</cell><cell>729 Mb/s</cell></row><row><cell>3</cell><cell>679 Mb/s</cell></row><row><cell>4</cell><cell>652 Mb/s</cell></row><row><cell>5</cell><cell>498 Mb/s</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Brian Huffman and Ashvin Goel for helping us out with the analysis, and Damien Berger for helpful suggestions in programming the IXP. We would also like to thank Buck Krasic for his guidance, Jim Snow for his insight and Mike Shea, Jie Huang, Brian Code, Andrew Black, Dave Meier, Sun Murthy, and William Howe for their comments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work supported by the National Science Foundation under Grant EIA-0130344 and the generous donations of Intel Corporation. Any opinions, findings, or recommendations expressed are those of the author(s) and do not necessarily reflect the views of NSF or Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Space/time tradeoffs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970-07">July 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable Packet Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baboescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2001-08">2001. August 2001</date>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<title level="m">Network applications of Bloom filters: a survey. 40th Annual Allerton Conference on Communication, Control, and Computing</title>
		<meeting><address><addrLine>Allerton, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-10">October, 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Informed Content Delivery Across Adaptive Overly Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Considine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM 2002</title>
		<meeting>ACM SIGCOMM 2002</meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal classes of hash functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High Performance IP Routing Table Lookup using CPU Caching In</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE INFOCOMM&apos;99</title>
		<meeting>of IEEE INFOCOMM&apos;99<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-03">March 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cache Memory Design for Network Processors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Internet Traffic Characterization</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>San Diego</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An architecture for a secure service discovery service</title>
		<author>
			<persName><forename type="first">S</forename><surname>Czerwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobiCom-99</title>
		<meeting>MobiCom-99<address><addrLine>N.Y.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">New directions in traffic measurement and accounting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2002</title>
		<meeting>the ACM SIGCOMM 2002</meeting>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<biblScope unit="page" from="323" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Summary cache: a scalable wide-area web cache sharing protocol</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Networking</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tradeoffs for Packet Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Blue: A new class of active queue management algorithms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kandlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<idno>U. Michigan CSE-TR-387-99</idno>
		<imprint>
			<date type="published" when="1999-04">April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Fraleigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tobagi</surname></persName>
		</author>
		<idno>TR01-ATL-110101</idno>
	</analytic>
	<monogr>
		<title level="j">Packet-Level Traffic Measurements from a Tier-1 IP Backbone</title>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
			<pubPlace>Burlingame, CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Sprint ATL Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving Route Lookup Performance Using Network Processor Cache</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM SC2002 Conference</title>
		<meeting>the IEEE/ACM SC2002 Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms for packet classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network Special Issue</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="32" />
			<date type="published" when="2001-04">March/April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">IPv6: The New Internet Protocol (2nd Edition)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huitima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monitoring Very High Speed Links</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM Internet Measurement Workshop</title>
		<meeting>ACM SIGCOMM Internet Measurement Workshop<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">November 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://www.intel.com/design/network/products/npfamily/ixp1200.htm" />
		<title level="m">Intel IXP1200 Network Processor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characteristics of destination address locality in computer networks: a comparison of caching schemes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="243" to="254" />
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-speed policy-based packet forwarding using efficient multi-dimensional range matching</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stiliadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 1998</title>
		<meeting>the ACM SIGCOMM 1998</meeting>
		<imprint>
			<date type="published" when="1998-08">August, 1998</date>
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trends in wide area IP traffic patterns a view from Ames Internet exchange</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mccreary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITC Specialist Seminar</title>
		<meeting><address><addrLine>Monterey, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compressed bloom filters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual ACM Symposium on Principles of Distributed Computing</title>
		<meeting>the 20th Annual ACM Symposium on Principles of Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A 50 GB/s IP Router</title>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locality and route caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<ptr target="http://www.caida.org/outreach/isma/9602/positions/partridge.html" />
	</analytic>
	<monogr>
		<title level="j">NSF Workshop on Internet Statistics Measurement and Analysis</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hardware support for a hash-based IP traceback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Milliken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tchakountio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Strayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd DARPA Information Survivability Conference and Exposition</title>
		<meeting>the 2nd DARPA Information Survivability Conference and Exposition</meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hash-based IP traceback</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tchakountio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Strayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2001 Conference</title>
		<meeting>the ACM SIGCOMM 2001 Conference</meeting>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and Scalable Layer Four Switching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waldvogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM 1998</title>
		<meeting>ACM SIGCOMM 1998</meeting>
		<imprint>
			<date type="published" when="1998-09">September, 1998</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When the CRC and TCP checksum disagree</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2000 Conference (SIGCOMM-00)</title>
		<meeting>the ACM SIGCOMM 2000 Conference (SIGCOMM-00)</meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://www.squid-cache.org" />
		<title level="m">Squid Web Proxy Cache</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast firewall implementations for software and hardware-based routers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMETRICS</title>
		<meeting>ACM SIGMETRICS<address><addrLine>Cambridge, Mass, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">2001. June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel cache architecture to support layer-four packet classification at memory access speeds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Degroat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of INFOCOM 2000</title>
		<meeting>eeding of INFOCOM 2000</meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
