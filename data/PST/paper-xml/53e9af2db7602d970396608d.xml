<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring Bandwidth between PlanetLab Nodes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sung-Ju</forename><surname>Lee</surname></persName>
							<email>sjlee@hpl.hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Puneet</forename><surname>Sharma</surname></persName>
							<email>puneet@hpl.hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sujata</forename><surname>Banerjee</surname></persName>
							<email>sujata@hpl.hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sujoy</forename><surname>Basu</surname></persName>
							<email>basus@hpl.hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
							<email>rfonseca@cs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Measuring Bandwidth between PlanetLab Nodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81C4A0EAB45E084F185B288DE7EE2892</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the lack of end-to-end QoS guarantees on existing networks, applications that require certain performance levels resort to periodic measurements of network paths. Typical metrics of interest are latency, bandwidth and loss rates. While the latency metric has been the focus of many research studies, the bandwidth metric has received comparatively little attention. In this paper, we report our bandwidth measurements between PlanetLab nodes and analyze various trends and insights from the data. For this work, we assessed the capabilities of several existing bandwidth measurement tools and describe the difficulties in choosing suitable tools as well as using them on PlanetLab.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The lack of end-to-end QoS and multicast support in the underlying best effort networking infrastructure has spurred a trend towards providing application level intermediaries such as web-caches and service replicas to mitigate the performance issues. It is not only important to provide intermediary services but also to connect the end-clients to the intermediary that can meet the client QoS requirements and provide the best performance. For instance, web applications might want to select the nearest content cache while the online multiplayer game players might want to choose the least loaded game server. There also have been attempts to build overlays by connecting application level intermediaries for composable and personalized web and media services. Normally, such services have QoS requirements such as bandwidth and delay. Hence, building and maintaining such overlays requires periodic or on-demand measurement of end-to-end paths. Motivated by these trends, there have been significant research studies on active and passive network measurement techniques, and measurement studies from many large scale networks <ref type="bibr" target="#b0">[1]</ref>. Clearly, periodic or on-demand measurement of all possible network paths will incur a high overhead and is inefficient. Thus a key concern is the development of scalable measurement and inference techniques which require minimum probing and yet provide the required measurement accuracy.</p><p>The primary network metrics of interest are end-to-end latency, bandwidth and loss rates while application level metrics are HTTP response times, media streaming rates, and so on. Many studies have focused on scalable network distance estimation, mainly using the triangular inequality heuristic <ref type="bibr" target="#b16">[17]</ref>. However, similar triangular inequality does not apply to bandwidth, and hence it is much more difficult to identify the nodes that provide the maximal bandwidth without probing to each node.</p><p>Although there is a plethora of bandwidth measurement techniques, there have been only a few large-scale bandwidth or bottleneck capacity measurement studies. In fact, most of these studies are in conjunction with the validation of a new bandwidth measurement tool. New bandwidth measurement tools continue to be developed, aiming for better accuracy with faster measurements. In this paper, we present results from a large scale bandwidth measurement study on the PlanetLab infrastructure. Our goals are (i) to understand the bandwidth characteristics of network paths connecting PlanetLab nodes and (ii) to ultimately obtain insights into potential trends that will enable scalable bandwidth estimation. We primarily focus on the first goal in this paper. We do not develop a new bandwidth estimation tool nor evaluate and compare the accuracy of various tools. Rather, we assess the capabilities of a number of available tools from a PlanetLab deployment standpoint and report our findings in the hope that it will help other researchers to make an informed choice of a tool.</p><p>In the next section, we describe our methodology and the tools we assessed for this study, followed by an analysis of the data we collected. Section 3 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Measurement Study</head><p>PlanetLab is an attractive platform for bandwidth measurement as it is an open, globally distributed network service platform with hundreds of nodes spanning over 25 countries. PlanetLab has gained the status of the de facto standard for conducting large scale Internet experiments. Although the interdomain connectivity of the PlanetLab hosts may not represent the global Internet <ref type="bibr" target="#b1">[2]</ref>, the characterization of PlanetLab topology is still of utmost importance for designing experiments on PlanetLab and drawing informed conclusions from the results. Several measurement studies have been conducted on PlanetLab topology, mostly focusing on the connectivity and the inter-node latency. In this paper, we study the bottleneck capacity between the PlanetLab nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Methodology</head><p>Our methodology consisted of deploying the bandwidth<ref type="foot" target="#foot_0">3</ref> measurement tool on a selected set of responsive nodes on PlanetLab using standard PlanetLab tools and then executing a script to run the measurements. The collected data is then shipped back to a central node on our site for analysis.</p><p>We performed two sets of measurements at two different time periods. The first set (referred to as Set 1 in rest of the paper) was measured and collected starting in August of 2004, and the second (referred to as Set 2) in January of 2005. Between the two measurements periods, PlanetLab went through a major version change. The second set of experiments were performed after the version 3 rollout on PlanetLab.</p><p>Although there are over 500 deployed nodes on PlanetLab, only a little over half the nodes consistently responded when we started the measurement process. A crucial first step was to select a tool; we describe the selection process below. Conducting pair-wise latency measurements for a few hundred nodes is a relatively quick process for which measurements can be run in parallel and finishes in the order of minutes. However, pair-wise capacity measurements for a few hundred nodes needs to be well coordinated because the capacity measurement tools often do not give accurate results when cross traffic is detected. Thus the measurement process for all pairs can take much longer and is of the order of days to even weeks.</p><p>There are a large number of bandwidth measurement/estimation tools available, with several new tools recently introduced. This in itself is an indication that accurate bandwidth measurement/estimation remains a hard problem even after many years of research and there is room for further improvements <ref type="bibr" target="#b8">[9]</ref>. For the details of the various tools and their measurement accuracy, please use our bibliography or available survey articles <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. For the purposes of our study, our goal was to find a reasonably accurate but low overhead tool that is easily deployable on the PlanetLab platform. Note that the purpose of this study is not to do an accuracy comparison of these tools. After some narrowing of the choices, we evaluated the following tools as described below. We merely present our experiences with different tools in the evaluation process.</p><p>We were hesitant to use per-hop capacity estimation tools as they generate excessive probing traffic overhead. Moreover, we could not build pathchar <ref type="bibr" target="#b9">[10]</ref> or pchar <ref type="bibr" target="#b13">[14]</ref> as they can not be built on newer Linux systems. Currently, PlanetLab runs kernel version 2.4.22, but pathchar supports up to 2.0.30 and pchar up to 2.3. When we tested Clink <ref type="bibr" target="#b4">[5]</ref> on PlanetLab, the experiment were simply "hung" without making any progress. We suspect this is also because of a Linux version compatibility issue.</p><p>As for end-to-end capacity estimation tools, bprobe <ref type="bibr" target="#b2">[3]</ref> works only on SGI Irix. SProbe <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> is an attractive tool as it only requires to be run on the source machine, and hence can measure capacities to hosts where the user does not have account access. In addition, SProbe is included in the Scriptroute <ref type="bibr" target="#b22">[23]</ref> tool that runs as a service on PlanetLab hosts. One key feature of SProbe is that when it detects cross traffic, instead of making a poor estimate of the capacity, it does not report any value. When we ran SProbe between PlanetLab hosts, less than 30% of the measurements returned the capacity estimate. The authors of the tool had a similar experience on their trials with Napster and Gnutella peers. As we have access to all the PlanetLab hosts, we can deploy and run pathrate <ref type="bibr" target="#b3">[4]</ref>. Unless the network hosts are down or we could not login to the hosts for various reasons, we were able to measure capacity between PlanetLab nodes using pathrate. It was the only capacity estimation tool we could successfully run and obtain estimates on PlanetLab.</p><p>We also tested several available bandwidth estimation tools. Similar to bprobe, cprobe <ref type="bibr" target="#b2">[3]</ref> does not run on Linux. One of the most popular tools is pathload <ref type="bibr" target="#b7">[8]</ref>. When we tested pathload on PlanetLab nodes however, we ran into an invalid argument error on connect. This very issue was also recently brought up in PlanetLab user mailing list. We were able to run IGI (Initial Gap Increasing) <ref type="bibr" target="#b6">[7]</ref> without any run-time errors. However, the tool showed poor accuracy with high variance in the estimation of the same pair on sequential attempts, and also reported unusually high estimates (ten times larger than the estimated capacity by pathrate). Spruce <ref type="bibr" target="#b23">[24]</ref> has shown to be more accurate than pathload and IGI. However, Spruce requires the knowledge of the capacity of the path to estimate available bandwidth. We also tested pathChirp <ref type="bibr" target="#b18">[19]</ref> and it ran successfully with reasonable accuracy in our first set of measurements performed in August 2004. However, after the version 3 rollout of PlanetLab, pathChirp, along with STAB <ref type="bibr" target="#b19">[20]</ref>, developed by the same authors of pathChirp, failed to work on PlanetLab. After a few chirps, the tool stops running and hangs. We are communicating with the authors of pathChirp to resolve the issue.</p><p>In our future work, we are planning to test tools such as ABwE <ref type="bibr" target="#b15">[16]</ref>, CapProbe <ref type="bibr" target="#b10">[11]</ref>, pathneck <ref type="bibr" target="#b5">[6]</ref>, and MultiQ <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Measurement Analysis</head><p>For the first set of measurements, we show the capacity measurements from pathrate (version 2.4.0) as it was the only capacity estimation tool we were able to successfully run in a consistent manner. Each pathrate run on average took approximately 30 minutes. Pathrate returns two estimates, a high estimate and a low estimate, for bottleneck capacity between a pair of source and destination nodes. In the first experiment pathrate returned negative values for low capacity estimate in certain measurements. When we reported this to the authors of the pathrate tool, they kindly debugged the calculation error and the modified version (v2.4.1b) was used in the second set of measurements. To avoid this calculation error, we only report the high capacity estimate of the pathrate in this paper.</p><p>The first set of measurements was initiated on August 11th, 2004 and completed on September 6th, 2004. The second set was measured between January 5th, 2005 and January 18th, 2005. On our first attempt in August 2004, we tried measuring capacities between all PlanetLab nodes of the then nearly 400 nodes. However, many of the nodes did not respond consistently, and many of the pathrate capacity estimates were not returned. Ultimately, in the first set, we collected bottleneck capacity data on 12,006 network paths from 279 nodes. In the second set of experiments performed in January of 2005, we prefiltered 178 nodes (and no more than two nodes per site) that consistently responded. It could be one of the reasons why the experiments were finished in a shorter time compared with the first set of measurement experiments. In the second set of measurement we managed to collect data on 21,861 paths.</p><p>We first look at the statistics of the end-to-end capacity over all paths (sourcedestination node pairs) measured. It is important to note that given two nodes A and B, capacity measurements in both directions, i.e., source destination node pairs (A,B) and (B,A) may not both be available. Table <ref type="table" target="#tab_0">1</ref> shows that the average bandwidth between    PlanetLab hosts is nearly 64 Mbps. Table <ref type="table" target="#tab_1">2</ref> shows the distribution, Figure <ref type="figure" target="#fig_2">1</ref> visualizes this distribution (notice the different scaling of y-axis between two subfigures) and Figure <ref type="figure" target="#fig_4">2</ref> shows the cumulative distribution function (notice the different scaling of x-axis between two subfigures). On further analysis, we observed that when certain nodes were the source or the destination, the bandwidth measured was very low. In the first set of measurements for instance, for paths with freedom.ri.uni-tuebingen.de as the source, the average bandwidth was 4.61 Mbps. We noticed a similar behavior in the second set as when 200-102-209-152.paemt7001.t.brasiltelecom.net.br was the source, the average capacity was 0.42 Mbps and when it was the destination, the average capacity was 0.41 Mbps. On the other hand, when planetlab1.eurecom.fr was the destination, the average bandwidth was 3.85 Mbps, with the path from planetlab1.inria.fr having 199.2 Mbps of bandwidth. Without this measurement of 199.2 Mbps, the average bandwidth with planetlab1.eurecom.fr as the destination is 2.13 Mbps. We also noticed nodes with high   average bandwidth. For instance, measurements from planet1.ottawa.canet4.nodes.planetlab.org showed an average of 508.46 Mbps. Some PlanetLab nodes have imposed outgoing bandwidth limit, ranging from 500 Kbps to 10 Mbps. We observed interesting interplay between the traffic shaper for bandwidth limiting and the pathrate probing scheme. In some cases we measured end-to-end capacity of 100 Mbps even though the source was bandwidth limited to 500 Kbps. We are further exploring this interaction.</p><p>The standard deviation for the second set is much smaller than the first set. We believe the prefiltering of the nodes for the second set is the main reason as the nodes that showed extremely low or high capacities in the first set were relatively unstable, and could have been removed from our second experiments. In the second set, we have limited the number of nodes per site to at most two nodes, and hence we have less number of high capacity local paths than the first set. We can also observe that in the second set, more than 99% of the paths show the capacity of less than 120 Mbps.   Table <ref type="table" target="#tab_2">3</ref> shows the capacity measured region by region. We categorize each node into five regions: North America, South America, Asia, Europe, and Oceania. In our first measurement set we did not have any node from the Oceania region part of PlanetLab. On other entries of the table with N/A, no estimates were returned. There were only two nodes from Brasil in South American region in the second set, and as mentioned earlier, capacities of the path to and from these nodes were very low. One might think that pairs between the same region will have higher capacity than those for inter-regions. The table however does not show any strong confirmation of that belief. It is also interesting to see that although the paths from Asia to Oceania show high capacity, the same cannot be said for the reverse direction.</p><p>We now investigate whether the paths between PlanetLab hosts are symmetric in terms of capacity. For the first set, among 12,006 measurements, only 2,172 pairs (4,344 measurements) reported capacity estimates in both directions, and for the second set, 8,758 pairs (17,516 measurements) out of 21,861 measurements returned estimates for both directions. To understand the path asymmetry, we define asymmetry factor between two nodes i and j, α i,j , as follows:</p><formula xml:id="formula_0">α i,j = |BW i,j -BW j,i | max(BW i,j , BW j,i )</formula><p>where BW i,j is the bottleneck bandwidth from node i to node j.</p><p>When the capacity of the forward path equals the capacity of the reverse path (i.e., complete symmetry), the asymmetry factor is zero. A high asymmetry factor implies stronger asymmetry.</p><p>The distribution of the asymmetry factor for both sets are reported in Table <ref type="table" target="#tab_3">4</ref> while the CDF is plotted in Figure <ref type="figure" target="#fig_6">3</ref>. In the first set, only 132 pairs (6%) showed α of less than 0.01 and 692 pairs (32%) are less than 0.1. Although about 60% of the pairs have asymmetry factor below 0.5, there are a significant number of pairs with high asymmetry factor. We further investigated the reason for high asymmetry in 328 pairs that have α larger than 0.5. The PlanetLab imposed artificial bandwidth limit was reason for asymmetry in 189 of these pairs.</p><p>In the second set however, surprisingly large portion of the paths showed high symmetry. Nearly 60% has the asymmetry factor of less than 0.05. We believe that the main reason is, as shown in Table <ref type="table" target="#tab_1">2</ref>, more than half of the capacity estimates were between 80 and 120 Mbps.</p><p>Temporal Analysis Since the measurements from Set 1 and Set 2 were done almost 5 months apart, the obvious question to ask is whether the data suggests significant changes in the PlanetLab infrastructure during this period. Note that we already know of two significant changes -the PlanetLab software version was upgraded to version 3 and the pathrate tool was upgraded to a new version. To answer the above question, we computed the common source-destination node pairs between the two sets and analyzed the bandwidth measurements. We found 128 common nodes in the two sets and 3,409 common source-destination node pair measurements.</p><p>The summary statistics of the measured capacity for these node pairs common to both measurement sets are given in Table <ref type="table" target="#tab_4">5</ref>. There are some interesting differences between the two sets, which could be caused by infrastructure changes, measurement errors or both. The average capacity between the measured node-pairs increased to 71 Mbps from 55 Mbps, as did the minimum measured bandwidth, implying an upgrade of the infrastructure on average. An interesting point to note is that the maximum capacity between any node pair decreased significantly from 1 Gbps to 152 Mbps. This could have been due to stricter bandwidth limits imposed on PlanetLab nodes. In the first set, the capacity between the nodes planetlab1.cse.nd.edu and planetlab2.cs.umd.edu were measured to be 1 Gbps, which in the second set is now close to 100 Mbps. We were unable to determine whether this is due to an infrastructure change, imposed bandwidth limit or measurement error. Diagnosing the causes for these measurement changes is future work.</p><p>While the stated goal of this work was not to verify accuracy of the pathrate tool (this has been done by other researchers in earlier work), we mentioned earlier that in some of the measurements in Set 1, the low estimate of bandwidth reported by pathrate were found to be negative and the authors of pathrate rectified this in the subsequent release. Although the negative values do not affect any of our presented results as we use the high estimate of the bandwidth, it is interesting to note that with the new version of pathrate, of the 3,409 measurements, no negative low estimates were observed in Set 2, while there were 93 negative measurements in Set 1.</p><p>The capacity distribution of the 3,409 common node pairs is given in Table <ref type="table" target="#tab_5">6</ref>. The biggest changes are in the paths with capacity between 20 Mbps and 50 Mbps and those with capacity between 80 Mbps and 120 Mbps. From the data presented it seems that significant number of paths were upgraded from the first band (20∼50 Mbps) to the second band (80∼120 Mbps) in the time between our measurements.</p><p>As mentioned earlier, given two nodes A and B in this common set, capacity measurements in both directions, i.e., source destination node pairs (A,B) and (B,A) may not both be available. Of the 3,409 source-destination node pairs common to Sets 1 and 2, 661 node pairs (i.e., 1,322 measurements) had bandwidth measurements in both directions and hence the asymmetry metric could be computed for these. The asymmetry factor distribution is tabulated in Table <ref type="table" target="#tab_6">7</ref>. Again, the second set of experiments show a significantly reduced asymmetry than the first set.    Correlation Study We now study the correlation between bandwidth and latency. Before we report the result of this study, we explain the motivation of attempting to relate the delay with bandwidth. As mentioned in Section 1, our ultimate goal is to gain insights into potential correlation that will enable scalable bandwidth estimation. For example, to find a node whose path from a given node has the largest capacity, instead of performing bandwidth estimates to all the nodes, can we do the probing to just a small number of nodes (five for instance)? Since measuring latency can be done with less probing overhead with quick turnaround time than measuring bandwidth, there already exist tools that perform scalable network distance estimation <ref type="bibr" target="#b25">[26]</ref>. With these tools available and latency values easily in hand, if there is any relationship or trend between latency and bandwidth, we can scalably estimate network bandwidth without excessive bandwidth probing. That is the main motivation of this trend analysis. Note also that using capacity, instead of available bandwidth, is more appropriate as the values of available bandwidth vary with time, and unless the measurement of bandwidth and latency are done at the same period, the analysis could be meaningless.   For the latency measurement, we initially used the all pair ping data. <ref type="foot" target="#foot_1">4</ref> However, due to some missing ping data, there was little overlap between the available ping data and collected bandwidth measurement. Hence we also used the RTT measurements from pathrate. The resulting trends from ping and RTT measures from pathrate are quite similar, and hence we only present the results based upon the pathrate RTT latency.</p><p>We used two metrics for studying the bandwidth and latency correlation. The first metric, called relative bandwidth correlation metric, captures the ratio of maximum bandwidth path and bandwidth to the closest node. For a given host (node i ), using the latency data, we find the host that has the minimum latency (node minLat ). Similarly, using the capacity measurements, we find the host that provides the maximum bandwidth (node maxBW ). The relative bandwidth correlation penalty metric for node i is then defined as the ratio of the maximum bandwidth (BW i,maxBW ) and the bandwidth from node i to node minLat (BW i,minLat ). This metric takes values greater than or equal to 1.0. The closer this metric is to 1.0, we consider the correlation between latency and bandwidth to be stronger. We plot this metric and its cumulative distribution in Figures <ref type="figure" target="#fig_9">4</ref> and<ref type="figure" target="#fig_12">5</ref>.</p><p>We see from the figure that latency and bandwidth are surprisingly quite correlated, especially in the second set. On further analysis with the CDF for the first set, we notice that for about 40% of the nodes, the bandwidth to the closest node is roughly 40% smaller than the actual maximum bandwidth. Our preliminary investigations reveal one of the primary reasons for this behavior is the imposed bandwidth limit. In some cases, the capacities to even the nearby nodes are quite less than the maximum bandwidth as they have bandwidth limits. For instance, in the Set 1, in lots of cases the node with highest capacity is planetlab1.ls.fi.upm.es which did not have any imposed bandwidth limit.  For the second set on the other hand, we see that about 90% of the closest nodes has the capacity that is nearly equal to the maximum bandwidth. We must remember however, that the roundtrip time measurement was performed by pathrate itself. Tools such as Netvigator <ref type="bibr" target="#b25">[26]</ref> that perform network distance estimation typically uses traceroute or ping, and those tools may return different values. We can further test the correlation between bandwidth and latency using these tools.</p><p>We also used the Spearman rank correlation coefficient <ref type="bibr" target="#b12">[13]</ref>, which is commonly used instead of the Pearson's correlation coefficient, when the two sets of data come from different distributions. In Figure <ref type="figure" target="#fig_14">6</ref>, we rank all the node pairs based on both bandwidth and latency. We plot the bandwidth rank versus the latency rank. If there is good correlation, we expect the points to be clustered along the y = x diagonal line. However, using the Spearman coefficient we did not find any such correlation. The value of Spearman coefficient for Sets 1 and 2 are 0.027 and 0.138 respectively based on the data presented in these figures. We believe that the degree of rank-order correlation is higher between closeby nodes and it decreases as distance between nodes increases. Hence, the low value of Spearman coefficient might be due to its computation over all the possible nodes. We plan to evaluate other rank-order correlation coefficients in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summary and Conclusions</head><p>In this paper, we presented a large scale measurement study of end-to-end capacity of paths between PlanetLab nodes. Our contributions are two-fold in that in addition to presenting the analysis of the data we collected from two sets of experiments, we also described the issues with the deployment of a number of bandwidth measurement tools on the PlanetLab platform. The measurement work is ongoing, but even with the data we have collected so far, there are a number of interesting conclusions one can draw.</p><p>Foremost, we verified our intuition that network paths connecting PlanetLab nodes are highly heterogeneous in the capacity values and in planning PlanetLab experiments, one needs to take this into account. The capacity of paths may have an order of magnitude difference even when they are sourced from the same node and similarly for the same receiver. Paths between two nodes do not necessarily show capacity symmetry. According to PlanetLab policy, bandwidth limits on outgoing traffic have been implemented. But we observed violations of the policy, which could have been due to the inaccuracy of the tool itself and we are investigating this further. In attempting to draw insights for scalable bandwidth estimation, we studied the correlation between latency and bandwidth of a path. Our preliminary results are promising and we plan to investigate this further.</p><p>One of our future work includes modifying the SProbe tool to keep attempting measurements until a valid estimate is made. We can then measure the bandwidth to nodes outside PlanetLab as SProbe does not require user access to destination hosts. We also plan to periodically measure the all pair bandwidth between PlanetLab hosts and make it available to public. Although there exists a running service that measures bandwidth between PlanetLab nodes,<ref type="foot" target="#foot_2">5</ref> it uses Iperf <ref type="bibr" target="#b24">[25]</ref> that measures achievable TCP throughput, which is not necessarily raw capacity or available bandwidth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Bandwidth capacity for all pairs measured.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Cumulative distribution function of bandwidth capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Cumulative distribution function of asymmetry factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Bandwidth/delay correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Cumulative distribution function of bandwidth/delay correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Bandwidth/delay correlation using rank correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>End-to-end capacity statistics.</figDesc><table><row><cell></cell><cell>Set 1</cell><cell>Set 2</cell></row><row><cell>Number of nodes</cell><cell>279</cell><cell>178</cell></row><row><cell cols="3">Measurement period 8/11/04∼9/6/04 1/5/05∼1/18/05</cell></row><row><cell>PlanetLab version</cell><cell>version 2</cell><cell>version 3</cell></row><row><cell>Number of pairs</cell><cell>12,006</cell><cell>21,861</cell></row><row><cell>Minimum capacity</cell><cell>0.1 Mbps</cell><cell>0.3 Mbps</cell></row><row><cell>Maximum capacity</cell><cell>1210.1 Mbps</cell><cell>682.9 Mbps</cell></row><row><cell>Average capacity</cell><cell>63.44 Mbps</cell><cell>64.03 Mbps</cell></row><row><cell>Median capacity</cell><cell>24.5 Mbps</cell><cell>91.4 Mbps</cell></row><row><cell>Standard deviation</cell><cell>119.22 Mbps</cell><cell>43.78 Mbps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>End-to-end capacity distribution.</figDesc><table><row><cell>Capacity (C)</cell><cell cols="4">Set 1 Number of paths Percentage (%) Number of paths Percentage (%) Set 2</cell></row><row><cell>C &lt; 20 Mbps</cell><cell>4013</cell><cell>33.42</cell><cell>6733</cell><cell>30.8</cell></row><row><cell>20 Mbps ≤ C &lt; 50 Mbps</cell><cell>4246</cell><cell>35.37</cell><cell>1910</cell><cell>8.74</cell></row><row><cell>50 Mbps ≤ C &lt; 80 Mbps</cell><cell>674</cell><cell>5.61</cell><cell>1303</cell><cell>5.96</cell></row><row><cell>80 Mbps ≤ C &lt; 120 Mbps</cell><cell>2193</cell><cell>18.27</cell><cell>11744</cell><cell>53.72</cell></row><row><cell>120 Mbps ≤ C &lt; 200 Mbps</cell><cell>207</cell><cell>1.72</cell><cell>139</cell><cell>0.64</cell></row><row><cell>200 Mbps ≤ C &lt; 500 Mbps</cell><cell>392</cell><cell>3.27</cell><cell>21</cell><cell>0.096</cell></row><row><cell>500 Mbps ≤ C</cell><cell>281</cell><cell>2.34</cell><cell>11</cell><cell>0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Capacity by regions (Mbps).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Destination</cell></row><row><cell>Source</cell><cell cols="3">North America South America</cell><cell>Asia</cell><cell>Europe</cell><cell>Oceania</cell></row><row><cell></cell><cell cols="5">Set 1 Set 2 Set 1 Set 2 Set 1 Set 2 Set 1 Set 2 Set 1 Set 2</cell></row><row><cell cols="6">North America 60.67 66.28 8.34 0.41 55.74 60.8 71.78 68.11 N/A 79.64</cell></row><row><cell cols="2">South America 7.94 0.42</cell><cell>106</cell><cell>N/A</cell><cell cols="2">N/A 0.43 6.1 0.41 N/A 0.4</cell></row><row><cell>Asia</cell><cell cols="3">69.13 55.15 N/A 0.41</cell><cell cols="2">73.5 62.36 72.28 42.94 N/A 99.52</cell></row><row><cell>Europe</cell><cell cols="3">69.94 66.43 30.67 0.4</cell><cell cols="2">61.38 47.54 74.82 66.69 N/A 13.15</cell></row><row><cell>Oceania</cell><cell cols="2">N/A 37.25 N/A</cell><cell>0.4</cell><cell cols="2">N/A 22.54 N/A 7.03 N/A 50.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Asymmetry factor distribution.</figDesc><table><row><cell></cell><cell cols="12">Set 1 Asymmetry factor (α) Number of pairs Percentage (%) Number of pairs Percentage (%) Set 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">α &lt; 0.01</cell><cell></cell><cell></cell><cell></cell><cell>132</cell><cell></cell><cell></cell><cell>6.08</cell><cell>1843</cell><cell>21.49</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">0.01 ≤ α &lt; 0.05</cell><cell></cell><cell></cell><cell>395</cell><cell></cell><cell></cell><cell>18.19</cell><cell>3237</cell><cell>37.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">0.05 ≤ α &lt; 0.1</cell><cell></cell><cell></cell><cell>165</cell><cell></cell><cell></cell><cell>7.6</cell><cell>817</cell><cell>9.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">0.1 ≤ α &lt; 0.2</cell><cell></cell><cell></cell><cell>243</cell><cell></cell><cell></cell><cell>11.19</cell><cell>880</cell><cell>10.26</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">0.2 ≤ α &lt; 0.5</cell><cell></cell><cell></cell><cell>328</cell><cell></cell><cell></cell><cell>15.1</cell><cell>1111</cell><cell>12.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.5 ≤ α</cell><cell></cell><cell></cell><cell></cell><cell>909</cell><cell></cell><cell></cell><cell>41.85</cell><cell>870</cell><cell>10.14</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDF</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5 α</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Set 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of end-to-end capacity statistics of common node pairs in Sets 1 and 2.</figDesc><table><row><cell></cell><cell>Set 1</cell><cell>Set 2</cell></row><row><cell>Number of common pairs</cell><cell cols="2">3,409</cell></row><row><cell>Measurement period</cell><cell cols="2">8/11/04∼9/6/04 1/5/05∼1/18/05</cell></row><row><cell>PlanetLab version</cell><cell>version 2</cell><cell>version 3</cell></row><row><cell>Minimum capacity</cell><cell>0.1 Mbps</cell><cell>0.5 Mbps</cell></row><row><cell>Maximum capacity</cell><cell>1014.1 Mbps</cell><cell>152.6 Mbps</cell></row><row><cell>Average capacity</cell><cell>55.79 Mbps</cell><cell>71.09 Mbps</cell></row><row><cell>Median capacity</cell><cell>24.3 Mbps</cell><cell>97.3 Mbps</cell></row><row><cell>Standard deviation</cell><cell>109.94 Mbps</cell><cell>39.32 Mbps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>End-to-end capacity distribution of common node pairs in Sets 1 and 2.</figDesc><table><row><cell>Capacity (C)</cell><cell cols="4">Set 1 Number of paths Percentage (%) Number of paths Percentage (%) Set 2</cell></row><row><cell>C &lt; 20 Mbps</cell><cell>1041</cell><cell>30.54</cell><cell>909</cell><cell>26.66</cell></row><row><cell>20 Mbps ≤ C &lt; 50 Mbps</cell><cell>1491</cell><cell>43.74</cell><cell>103</cell><cell>3.02</cell></row><row><cell>50 Mbps ≤ C &lt; 80 Mbps</cell><cell>105</cell><cell>3.08</cell><cell>180</cell><cell>5.28</cell></row><row><cell>80 Mbps ≤ C &lt; 120 Mbps</cell><cell>587</cell><cell>17.22</cell><cell>2205</cell><cell>64.68</cell></row><row><cell>120 Mbps ≤ C &lt; 200 Mbps</cell><cell>37</cell><cell>1.09</cell><cell>12</cell><cell>0.35</cell></row><row><cell>200 Mbps ≤ C &lt; 500 Mbps</cell><cell>86</cell><cell>2.52</cell><cell>0</cell><cell>0.00</cell></row><row><cell>500 Mbps ≤ C</cell><cell>62</cell><cell>1.82</cell><cell>0</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Asymmetry factor distribution of common node pairs in Sets 1 and 2.</figDesc><table><row><cell cols="9">Set 1 Asymmetry factor (α) Number of pairs Percentage (%) Number of pairs Percentage (%) Set 2</cell></row><row><cell></cell><cell></cell><cell cols="2">α &lt; 0.01</cell><cell></cell><cell>65</cell><cell></cell><cell>9.83</cell><cell>145</cell><cell>21.94</cell></row><row><cell></cell><cell></cell><cell cols="2">0.01 ≤ α &lt; 0.05</cell><cell></cell><cell>167</cell><cell></cell><cell>25.26</cell><cell>299</cell><cell>45.23</cell></row><row><cell></cell><cell></cell><cell cols="2">0.05 ≤ α &lt; 0.1</cell><cell></cell><cell>57</cell><cell></cell><cell>8.62</cell><cell>86</cell><cell>13.01</cell></row><row><cell></cell><cell></cell><cell cols="2">0.1 ≤ α &lt; 0.2</cell><cell></cell><cell>64</cell><cell></cell><cell>9.68</cell><cell>70</cell><cell>10.59</cell></row><row><cell></cell><cell></cell><cell cols="2">0.2 ≤ α &lt; 0.5</cell><cell></cell><cell>83</cell><cell></cell><cell>12.56</cell><cell>48</cell><cell>7.26</cell></row><row><cell></cell><cell></cell><cell cols="2">0.5 ≤ α</cell><cell></cell><cell>225</cell><cell></cell><cell>34.04</cell><cell>13</cell><cell>1.97</cell></row><row><cell>0</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We use the terms bandwidth and capacity inter-changeably throughout the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We obtain this from http://www.pdos.lcs.mit.edu/˜strib/pl_app/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://www.planet-lab.org/logs/iperf/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical evaluation of wide-area Internet bottlenecks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IMC</title>
		<meeting>the ACM IMC<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">2003. October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The interdomain connectivity of planetlab nodes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAM 2004</title>
		<meeting>the PAM 2004<address><addrLine>Sophia-Antipolis, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Server selection using dynamic path characterization in wide-area networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Crovella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE INFOCOM&apos;97</title>
		<meeting>the IEEE INFOCOM&apos;97<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What do packet dispersion techniques measure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dovrolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE INFOCOM</title>
		<meeting>the IEEE INFOCOM<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-04">2001. April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using pathchar to estimate Internet link characteristics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM&apos;99</title>
		<meeting>the ACM SIGCOMM&apos;99<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locating internet bottlenecks: Algorithms, measurements, and implications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM</title>
		<meeting>the ACM SIGCOMM<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation and characterization of available bandwidth probing techniques</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steenkiste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pathload: A measurement tool for end-to-end available bandwidth</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dovrolis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAM 2002</title>
		<meeting>the PAM 2002<address><addrLine>Fort Collins, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-03">March 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ten fallacies and pitfalls on end-to-end available bandwidth estimation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IMC</title>
		<meeting>the ACM IMC<address><addrLine>Taormina, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">2004. October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Jocobson</surname></persName>
		</author>
		<ptr target="ftp://ftp.ee.lbl.gov/pathchar" />
		<title level="m">A tool to infer characteristics of internet paths</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CapProbe: A simple and accurate capacity estimation technique</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Sanadidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM</title>
		<meeting>the ACM SIGCOMM<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MultiQ: Automated detection of multiple bottleneck capacities along a path</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kitabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IMC</title>
		<meeting>the ACM IMC<address><addrLine>Taormina, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-10">2004. October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J M</forename><surname>D'abrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonparametrics: Statistical Methods Based on Ranks</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">pchar: A tool for measuring internet path characteristics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Mah</surname></persName>
		</author>
		<ptr target="http://www.kitchenlab.org/www/bmah/Software/pchar" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparative analysis of active bandwidth estimation tools</title>
		<author>
			<persName><forename type="first">F</forename><surname>Montesino-Pouzols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAM 2004</title>
		<meeting>the PAM 2004<address><addrLine>Sophia-Antipolis, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ABwE: A practical approach to available bandwidth estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAM 2003</title>
		<meeting>the PAM 2003<address><addrLine>La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting Internet network distance with coordinates-based approaches</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE INFOCOM</title>
		<meeting>the IEEE INFOCOM<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">2002. June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bandwidth estimation: Metrics, measurement techniques, and tools</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dovrolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2003-12">November/December 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">pathChirp: Efficient available bandwidth estimation for network paths</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAM 2003</title>
		<meeting>the PAM 2003<address><addrLine>La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Locating available bandwidth bottlenecks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Riedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2004-10">September/October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SProbe: A fast tool for measuring bottleneck bandwidth in uncooperative environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saroiu</surname></persName>
		</author>
		<ptr target="http://sprobe.cs.washington.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SProbe: Another tool for measuring bottleneck bandwidth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saroiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-03">March 2001</date>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
	<note>in Work-in-Progress Report at the USITS 2001</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scriptroute: A facility for distributed internet debugging and measurement</title>
		<author>
			<persName><forename type="first">N</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<ptr target="http://www.scriptroute.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A measurement study of available bandwidth estimation tools</title>
		<author>
			<persName><forename type="first">J</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaashoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IMC</title>
		<meeting>the ACM IMC<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">2003. October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Iperf: The TCP/UDP bandwidth measurement tool</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gibbs</surname></persName>
		</author>
		<ptr target="http://dast.nlanr.net/Projects/Iperf/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Netvigator: Scalable network proximity estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP Laboratories, Tech. Rep</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
