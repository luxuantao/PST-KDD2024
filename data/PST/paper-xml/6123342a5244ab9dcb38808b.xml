<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence-T5 (ST5): Scalable Sentence Encoders from Pre-trained Text-to-Text Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><forename type="middle">Hernández</forename><surname>Ábrego</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence-T5 (ST5): Scalable Sentence Encoders from Pre-trained Text-to-Text Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoderdecoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark (Wang et al., 2018). Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoderdecoder method achieves a new state-of-theart on STS when using sentence embeddings. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence embeddings providing compact meaning representations that are broadly useful for a variety of language processing tasks include classification, question-answering, semantic retrieval, bitext mining, and semantic similarity tasks. We explore sentence embeddings from a new family of pre-trained models: Text-to-Text Transfer Transformer (T5) <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref>. Unlike encoder-only models, which use a transformer encoder to predict random masked tokens, T5 uses an encoder-decoder architecture and a generative span corruption pre-training task. T5 models can be scaled up to hundreds of billions of parameters 1 Our models are released at https://tfhub.dev/ google/collections/sentence-t5/1. 91.63 84.96 SimCSE-RoBERTa (large) (Gao et al., 2021) 2  90.23 83.76 SBERT (large) <ref type="bibr" target="#b19">(Reimers and Gurevych, 2019)</ref> 87.69 76.55 USE <ref type="bibr" target="#b7">(Cer et al., 2018)</ref> 85.10 71.22 InferSent <ref type="bibr" target="#b9">(Conneau et al., 2017)</ref> 85.59 65.01</p><p>Table <ref type="table">1</ref>: ST5 versus notable sentence embedding models on SentEval tasks. The reported numbers are the average of transfer tasks (classification accuracy) and STS tasks (spearman correlation). <ref type="bibr" target="#b11">(Fedus et al., 2021)</ref> and have achieved state-of-theart performance on a broad range of NLP tasks including Generalized Language Understanding Evaluation (GLUE) <ref type="bibr">(Wang et al., 2018)</ref> and Super-GLUE <ref type="bibr" target="#b22">(Wang et al., 2019)</ref>. However, it is difficult to efficiently apply T5 to some tasks such as retrieval or clustering. To score retrieval candidates, T5 would need to perform full inference with crossattention on each query-candidate pair. In contrast, sentence embeddings allow for efficient retrieval and clustering <ref type="bibr" target="#b14">(Gillick et al., 2018;</ref><ref type="bibr" target="#b19">Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b12">Yang et al., 2020)</ref>. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we explore three ways of turning a pre-trained T5 encoder-decoder model into a sentence embedding model: (i) using the first token representation of the encoder; (ii) averaging all token representations from the encoder; (iii) using the first token representation from the decoder. We evaluate the quality of the resulting sentence embeddings on sentence transfer tasks using the SentEval <ref type="bibr">(Conneau and Kiela, 2018)</ref> toolkit and on our extension of SentEval to GLUE benchmark tasks (SentGLUE) in addition to semantic textual similarity (STS) <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016;;</ref><ref type="bibr" target="#b6">Cer et al., 2017)</ref>. We contrast raw representations from pre-trained T5 models with those learned through fine-tuning T5 on natural language inference (NLI) using dual encoders and contrastive learning <ref type="bibr" target="#b9">(Conneau et al., 2017;</ref><ref type="bibr" target="#b7">Cer et al., 2018;</ref><ref type="bibr" target="#b13">Gao et al., 2021)</ref>. We introduce a multi-stage contrastive learning recipe involving fine-tuning first on semi-structured web-mined corpora and then on NLI. Finally, we investigate scaling our T5 sentence embedding model up to 11B parameters. Illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, performance on transfer tasks and semantic textual similarity (STS) both improve with increased model capacity.</p><p>To our knowledge, we are the first to study using large-scale pre-trained text-to-text models for sentence representation learning and to scale sentence embedding models up to 11 billion parameters. We summarize our contributions as follows: (i) even without fine-tuning, encoder-only ST5 models perform well on sentence transfer tasks, outperforming state-of-the-art fine-tuned models such as Sim-CSE BERT and SimCSE RoBERTa <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>; (ii) encoder-decoder sentence embedding models achieve strong performance on STS, establishing a new state-of-the-art on sentence embedding based STS; (iii) contrastive learning is effective for fine-tuning sentence encoders from T5-style pre-trained models, particularly using our proposed two-stage contrastive learning approach;</p><p>(iv) training ST5 longer and with more data using a contrastive loss leads to consistent improvement on both sentence transfer and STS tasks; (v) creating a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval sentence evaluation toolkit <ref type="bibr">(Conneau and Kiela, 2018)</ref> to nine tasks from the GLUE benchmark <ref type="bibr">(Wang et al., 2018)</ref>. We contribute baselines on SentGLUE using influential sentence embedding models from prior work and contrast the performance with our proposed ST5 embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Sentence embedding models have been trained using a variety of methods including: supervised natural language inference pairs (NLI) <ref type="bibr" target="#b9">(Conneau et al., 2017;</ref><ref type="bibr">Reimers and</ref><ref type="bibr">Gurevych, 2019, 2020;</ref><ref type="bibr" target="#b13">Gao et al., 2021)</ref>; conversational input-response and question-answer pairs <ref type="bibr" target="#b7">(Cer et al., 2018;</ref><ref type="bibr" target="#b12">Yang et al., 2020)</ref>; translation pairs <ref type="bibr" target="#b12">(Yang et al., 2020;</ref><ref type="bibr" target="#b12">Feng et al., 2020)</ref>; paraphrasing pairs <ref type="bibr">(Wieting et al., 2016)</ref> and adjacent sentence pairs <ref type="bibr" target="#b16">(Kiros et al., 2015;</ref><ref type="bibr" target="#b17">Logeswaran and Lee, 2018)</ref>. <ref type="bibr" target="#b13">Gao et al. (2021)</ref> achieved the previous state-of-the-art on STS with BERT and RoBERTa models by combining contrastive learning that constructs positive and negative sentence pairs using NLI data.</p><p>In parallel, Text-to-Text transfer transformers (T5) <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref>, as shown in Figure <ref type="figure" target="#fig_1">2a</ref>, are gaining popularity due to their competitive performance, effective scaling to larger model sizes, and ease of use in solving tasks as simple text-totext mapping problems. However, extracting high quality text embeddings from T5 has not been previously explored. Moreover, while recent work has shown that scaling up models improves sentence embedding performance <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>, the largest model sizes investigated only include 355 million parameters rather than the 11 billion parameters available in the largest T5 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence-T5 (ST5)</head><p>We explore producing sentence embeddings from T5 models, ranging in size from 220 million to 11 billion parameters, both as raw sentence embeddings extracted from pretrained T5 models and using fine-tuning to refine the representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>As shown in Figures <ref type="figure" target="#fig_1">2b to 2d</ref>, we explore three strategies to extract T5 sentence representations:</p><p>• Encoder-only first (ST5-Enc first): the encoder output of the first token. • Encoder-only mean (ST5-Enc mean): the average encoder output across all input tokens. • Encoder-Decoder first (ST5-EncDec first): the first decoder output when the input text is fed into the encoder and the standard "start" symbol is fed as the only decoder input.</p><p>The first two are pooling strategies widely used in encoder-only pre-trained models such as BERT. Unlike BERT models, T5 models do not have a 'CLS' token at the beginning of each sentence. For T5 encoder-decoder models, we assume the decoder is aware of the semantics of the entire input sentence when generating its first token prediction; and if so, the first decoder output embeddings (i.e. input to the softmax layer) might naturally capture the sentence semantics.</p><p>For sentence encoder training, we adopt a dual encoder architecture <ref type="bibr" target="#b14">(Gillick et al., 2018;</ref><ref type="bibr" target="#b7">Cer et al., 2018;</ref><ref type="bibr" target="#b19">Reimers and Gurevych, 2019)</ref>. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, this architecture consists of two sharedweight transformer modules that encode the inputs. The transformer module can be either an encoderonly or encoder-decoder architecture. In our experiments, we initialize the transformer modules from a pre-trained T5 model. After each module computes a fixed-length representation for its input sentence, a projection layer and L2 normalization are applied to the resulting embeddings. The projection layer transforms the output to a configurable fixed dimension sentence embedding. The embeddings from paired encoding towers can be scored for similarity tasks using a dot-product 3 or provided to additional layers layers for classification tasks (e.g., NLI).</p><p>3 Since L2 normalization is applied to the output of each  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning</head><p>Applying contrastive learning to sentence embeddings improves the uniformity of the embedding space, leading to better performance on downstream tasks such as STS <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>. We apply contrastive learning to fine-tune the T5 sentence representations.<ref type="foot" target="#foot_1">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Contrastive Loss</head><p>Using a contrastive loss to train a sentence encoder requires paired examples D = {(v i , v + i )} as a training set, where v i is an input sentence and v + i is a related sentence (e.g., that is semantically nearby).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>During training, v +</head><p>i is considered as a positive example for v i and all other examples in a batch are considered as negatives. The model should learn to pull the positive pairs closer together while pushing away the in-batch negatives. We operationalize our contrastive loss using an in-batch sampled softmax <ref type="bibr" target="#b15">(Henderson et al., 2017)</ref>:</p><formula xml:id="formula_0">L = e sim(v i ,v + i )/τ j∈B e sim(v i ,v + j )/τ ,<label>(1)</label></formula><p>The similarity scoring function is sim. B is a minibatch of examples and τ is the softmax temperature.</p><p>When additional negatives v − j are provided for the input example v, the loss can be computed as:</p><formula xml:id="formula_1">L = e sim(v i ,v + i )/τ j∈B e sim(v i ,v + j )/τ + e sim(v i ,v − j )/τ .<label>(2)</label></formula><p>tower, the dot-product between the embeddings will produce their cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two-stage Training</head><p>We  <ref type="bibr">et al., 2017)</ref> datasets. For the first stage, we finetune using the CQA pairs under a dot-product retrieval loss with batch negatives <ref type="bibr" target="#b7">(Cer et al., 2018;</ref><ref type="bibr" target="#b7">Yang et al., 2018</ref><ref type="bibr" target="#b12">Yang et al., , 2020))</ref>. For the second stage, we use NLI pairs with a contrastive loss <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>, where the positives are the 'entailment' pairs while the negatives are the 'contradict' pairs.<ref type="foot" target="#foot_2">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We evaluate using SentEval, which includes 7 transfer and 7 STS tasks <ref type="bibr">(Conneau and Kiela, 2018)</ref> and using our extension of SentEval to the GLUEBenchmark tasks (SentGLUE). For the transfer tasks, sentence embeddings are evaluated by how well they perform as features for a linear classification model. For STS, embeddings are evaluated by how well their cosine similarities correlate with human annotated similiarity scores.<ref type="foot" target="#foot_3">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Configurations</head><p>Our models are implemented using JAX<ref type="foot" target="#foot_4">7</ref> and trained on Cloud TPU-v3. We initialize the dual encoder modules from public T5 checkpoints. <ref type="foot" target="#foot_5">8</ref>During training, we use Adafactor <ref type="bibr" target="#b21">(Shazeer and Stern, 2018)</ref> as the optimizer and set the learning rate to 0.001. Linear decay is applied after 10% of the total number of training steps, reducing the learning rate to 0 by the end of training. To finetune on NLI we use a batch size of 512, while for the Community QA (CQA) dataset the batch size is 2048. We use a softmax temperature τ of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Goals</head><p>Our experiments aim to answer the following:</p><p>• Q1: What is the best way to extract sentence representations from T5? • Q2: How well do raw T5 sentence embeddings perform on downstream tasks? • Q3: How much do contrastive sentence embedding tasks (e.g., NLI, QA) improve T5 sentence embeddings. • Q4: Can we benefit from scaling up T5 model capacity for better sentence representations?</p><p>With these goals, we study transfer and STS performance of T5 sentence embeddings using a variety of model and training configurations, comparing ST5 to state-of-the-art methods including SBERT/SRoBERTa <ref type="bibr" target="#b19">(Reimers and Gurevych, 2019)</ref> and SimCSE <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" target="#tab_2">2</ref> and 3 provide performance on transfer and STS tasks, respectively. We compare ST5 models with two types of baselines: (ii) a model that extracts sentence embeddings from a pre-trained BERT model, listed in rows 1-2 of each table <ref type="table">;</ref> (ii) the current state-of-the-art sentence embedding models fine-tuned from BERT or RoBERTa, listed in rows 6-8 of each table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Raw T5 Sentence Embeddings</head><p>We evaluate T5 sentence embeddings without finetuning using the extraction strategies from section 3.1: (i) Encoder-only first token, (ii) Encoder-only mean, and (iii) Encoder-decoder start token.</p><p>Transfer tasks Results for ST5 models using raw embeddings on transfer tasks are shown in rows 3-5 of ♣ results are from <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>. For all tasks, a logistic regression classifier is trained using the sentence embeddings as features and the classification accuracy on test sets are reported. Table 3: Spearman's correlation coefficient (×100) on STS tasks on the SentEval benchmark. All models are using the Base architecture. ♣ results are from <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>.</p><p>and there are no specific pre-training tasks using the first token's embeddings. It is unlikely that without additional fine-tuning the first token's representation would capture the semantics of the whole sentence. Indeed, our experiments show the first token's representation from encoder or decoder are much worse on all SentEval tasks compared to the mean pooling of the encoder-only model. Mean pooled T5 encoder embeddings greatly outperform mean pooled BERT embeddings. Moreover, even without fine-tuning, mean pooled T5 encoder embeddings outperform the prior best model, SimCSE-RoBERTa <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>, on transfer learning even though SimCSE-RoBERTa benefited from contrastive fine-tuning on NLI.</p><p>The strong performance of ST5 may be due to the fact that T5 is trained on more data than BERT or RoBERTa. Additionally, the original T5 models also include downstream tasks (e.g., GLUE, Su-perGLUE) during pre-training, and this multi-task setting may improve transfer performance. However, we note that there are only two SentEval tasks (SST and MRPC) included in GLUE while the other five tasks are not. As shown in Table <ref type="table" target="#tab_2">2</ref>, we observe significant improvements on the five tasks that are not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS tasks</head><p>As shown in rows 3-5 of Table <ref type="table" target="#tab_5">3</ref> and similar to prior work involving BERT and RoBERTA <ref type="bibr" target="#b10">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b13">Gao et al., 2021)</ref>, mean pooling of T5 embeddings performs poorly on STS, achieving an average correlation of 55.97. While slightly better than BERT using mean pooling, this is still worse than sentence embedding models that have been fine-tuned on supervised tasks such as Sentence-BERT and SimCSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Fine-Tuning T5 Sentence Embeddings</head><p>We next evaluate ST5 models that are fine-tuned on CQA and NLI tasks using our contrastive loss.</p><p>Fine-tuning on NLI Given that mean pooling performs much better than the first token output representation from encoder only T5, we opt to discard the first token T5 model for our fine-tuning experiments. The last three rows of Table <ref type="table" target="#tab_2">2</ref> show that the transfer performance of ST5 models is very consistent across different embedding extracting strategies after fine-tuning. The best fine-tuned model is 0.57 better than the best raw T5 sentence embeddings. In Table <ref type="table" target="#tab_5">3</ref>, we see that fine-tuning on NLI data significantly improves the STS task performance of ST5.</p><p>Fine-tuning on CQA + NLI To investigate the impact of additional training data on contrastive learning, we experiment with the ST5 models first trained on CQA and then fine-tuned on NLI. As shown in Tables <ref type="table" target="#tab_5">2 and 3</ref>, fine-tuning on an additional dataset brings a large performance boost for both transfer and STS tasks. This suggests that we may be able to improve sentence embedding quality further through the mining of additional semistructured data for continued contrastive learning.</p><p>To exclude the effect of mixing in downstream tasks, we also trained a ST5 variant based on the T5 1.1 model which was only pre-trained on the C4 dataset <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref>. As shown on the last row of Table <ref type="table" target="#tab_5">2 and Table 3</ref>, it achieves comparable performance to the original T5 model, outperforming on most tasks but under-performing on STS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Encoder-only vs. Encoder-decoder</head><p>In this section, we compare the performance of two architectures: encoder-only and encoder-decoder.</p><p>Better generalizability for T5's encoder In Table 2, we saw that the encoder-only Base model performs on-par with the encoder-decoder model on transfer tasks. When we scale the ST5 model up from Base to Large, 3B and 11B, the encoder-only models' performance on transfer tasks consistently outperforms the encoder-decoder models as shown in Table <ref type="table" target="#tab_8">5</ref>. This shows that building ST5 on top of the T5's encoder gives strong transfer performance.</p><p>Recently, Chung et al. ( <ref type="formula">2021</ref>) have shown that larger output embeddings (i.e. larger embedding size) effectively prevent the encoder from overspecializing to the pre-training task, thus making the encoder's representations more general and more transferable. We hypothesize that the decoder in the encoder-decoder architecture can improve the generalizability of the encoder's representation in a similar fashion, as the decoder focuses on optimizing for specific tasks.  One explanation is that the additional parameters from the decoder are helpful for improving performance on textual similarity tasks. Another possibility is that the decoder architecture itself helps to improve the sentence embedding quality. As shown in Figure <ref type="figure" target="#fig_1">2d</ref>, the decoder can be considered as an additional attention pooling layer on top of the encoder outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of the decoder In the last two rows of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Scaling up T5</head><p>We leverage the existing checkpoints from large T5 models to study the effect of scaling sentence encoders. The parameters of the T5 models are listed in Table <ref type="table" target="#tab_6">4</ref>. Note however that ST5-EncDec doesn't fully leverage the model parameters; the decoder's learned self-attention is effectively ignored as only the start token is fed into the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Effect on Directly Using T5 Embeddings</head><p>As shown in Table <ref type="table" target="#tab_8">5</ref>, the performance on the transfer tasks of directly using T5 embeddings consistently improves as T5 scales up. This corroborates that large pre-trained models can improve transfer performance of sentence embeddings.</p><p>On the other hand, increasing the model capacity alone is not enough to achieve good performance. Even the embeddings from the T5 11B model still do worse on STS tasks than the fine-tuned models. We believe that the pre-training corruption task of T5 does not require models to avoid anisotropy.<ref type="foot" target="#foot_6">9</ref> This highlights the importance of choosing finetuning tasks for sentence embedding models that are aligned to the goal of similarity and/or retrieval performance.   <ref type="bibr" target="#b13">(Gao et al., 2021)</ref>. The first set of results are for transfer tasks, while the second set are for the similarity task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Scaling Up Improves Fine-tuning</head><p>As shown in Table <ref type="table" target="#tab_8">5</ref>, we find that scaling up model capacity leads to consistently better performance on all downstream tasks. For the ST5 11B model, the encoder-only model achieves an average score of 91.08 for transfer tasks which is better than 90.45 from the ST5 Large model; while the encoderdecoder model pushes the STS score to 84.94 and also outperforms the ST5 Large model. For STS tasks, we observe that the gain from increasing model size from 3B to 11B is smaller than that from Large to 3B. This might be due to the fact that the embedding sizes are fixed for all models in our experiments. One potential exploration is to increase the sentence embedding size for larger models to fully leverage the model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Alignment and Uniformity</head><p>We further investigate the quality of the sentence embeddings by measuring aggregate distance metrics in the learned geometric space. In particular, we compute the alignment loss and uniformity loss as defined in <ref type="bibr">Wang and Isola (2020)</ref>:</p><formula xml:id="formula_2">L align = − E v,v + ∼ppos f (v) − f (v + )<label>(3)</label></formula><formula xml:id="formula_3">L uniform = log E v,w i.i.d ∼ p data e −2 f (v)−f (w) ,<label>(4)</label></formula><p>Above, p pos is all positive data and p data is the data distribution. L align denotes the expected distance between embeddings of the positive pairs, while L uniform indicates how uniformly the embeddings are distributed.</p><p>For both losses, lower numbers indicate better performance. <ref type="bibr" target="#b13">Gao et al. (2021)</ref> has shown that models with lower numbers for these two aggregate metrics tend to have better performance on downstream tasks. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, when models scale up, both the encoder and encoderdecoder models decrease the uniformity loss by a large marge meanwhile only slightly increasing the alignment loss. This indicates that scaling up might help the sentence embeddings to spread out more uniformly in the space while keeping semantically similar pairs clustered together. We leave the further exploration of the connection between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SentGLUE Evaluation</head><p>In this section, we introduce a new sentence representation transfer benchmark -SentGLUE -which extends the sentence evaluation toolkit, SentEval, to nine tasks from the GLUE benchmark including: CoLA, SST-2, MRPC, STS-B, QQP, MNLI-m, MNLI-mm, QNLI, RTE 10 . The GLUE benchmark has been widely adopted for assessing language understanding models. GLUE tasks are either single sentence or sentence pair classification (e.g. NLI) or similarity (STS) tasks. The best models on the GLUE leaderboard are fine-tuned cross-attention models like BERT or T5. Such models change all the parameters in the underlying model during finetuning and for pairwise tasks they allow for early fusion of input features from both sentences being 10 We found the WNLI task from the GLUE benchmark is too challenge for existing sentence embedding models, thus we exclude it in the current version. compared. For SentGLUE, we introduce the constraint that each input needs to be independently encoded into a fixed embedding space representation that can then be feed to additional layers in order to make a prediction. We believe this best adapts the spirit of the original SentEval benchmark for sentence embeddings to the GLUE tasks.</p><p>From Table <ref type="table">6</ref>, ST5-Enc Base outperforms both SBERT-RoBERTa Base and SimCSE-RoBERTa Base on all SentGLUE tasks except CoLA and MNLI.<ref type="foot" target="#foot_7">11</ref> With the model's increased capacity, ST5 Enc 11B's sentence embeddings achieve the best overall performance. Notably, as model size is scaled up, aggregate performance using sentence embeddings approaches that of T5 base. This is remarkable given that T5 base makes use of full crossattention between sentence pairs and adjusts all of the parameters in the model during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We obtaining sentence embeddings from T5, investigating three architectures and two-stage contrastive learning for fine-tuning our representations. We compare the difference between encoder-only and encoder-decoder methods and analyze their performance on downstream tasks. Through extensive experiments on STS, SentEval and GLUE tasks, we show that encoder-only models have strong transfer performance while encoder-decoder models perform better on STS tasks. We demonstrate the effectiveness of scaling up T5 models, greatly improving sentence embedding quality. These findings suggest that future improvements in the scale and quality of pre-trained T5 models may provide further sentence embeddings improvements.</p><p>Alex <ref type="bibr">Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018</ref>. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Black-boxNLP@EMNLP.</p><p>Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML.</p><p>John </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Inference</head><p>We run ST5 encoder-only on different platforms to investigate the computational cost of inference.</p><p>Figure <ref type="figure">5</ref> summarizes the inference speed for different model sizes, sequence length, batch size and platforms. ST5 achieves the fastest inference speed on Cloud TPU-v3. As we increase the batch size, the inference speed can be further improved. For the 11B model, we are able to achieve a speed of 274 examples per second for sequence length 128 and batch size 1024. This shows the feasibility of deploying such large models on TPU hardware.</p><p>We also report the speed on Nvidia Tesla V100 GPU and CPU. The ST5 11B model is able to run on 4 V100 GPUs with sequence length 128 and batch size 1024, achieving an inference speed of 27 examples per second. For CPU, with batch size 512, ST5 11B achieves 0.5 examples per second.</p><p>Although the speed on GPU and CPU are considerably slower than on TPU, the sentence embedding models are much faster than cross-attention based models whose computation time increases quadratically with the number of examples (e.g., clustering 1,000 sentences requires inference over 1 million sentence pairs).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scaling up our ST5 model size improves performance on SentEval (left) and STS (right).</figDesc><graphic url="image-1.png" coords="1,306.14,212.59,218.28,125.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture diagrams for T5 and three ST5 variants to extract sentence representations from T5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the dual encoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Alignment and uniformity losses for different model sizes. We consider the test split of the STS-B dataset. L align is calculated considering all pairs with score greater than 4. L uniform is computed using all sentences. The colormap denotes the models' Spearman's correlation score.</figDesc><graphic url="image-2.png" coords="8,79.88,226.92,220.84,147.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>explore two-stage training to refine T5 sentence embeddings: (i) first training on web mined conversational input-response and question-answering pairs; (ii) then, contrastive training on NLI pairs.</figDesc><table><row><cell>4 Experimental Setup</cell></row><row><cell>4.1 Training Corpus</cell></row><row><cell>For our fine-tuned sentence embeddings, we follow</cell></row><row><cell>prior work showing good sentence embeddings can</cell></row><row><cell>be obtained from supervised training on NLI (Con-</cell></row><row><cell>neau et al., 2017; Reimers and Gurevych, 2019,</cell></row><row><cell>2020; Gao et al., 2021) in combination with train-</cell></row><row><cell>ing to match conversational input-response and</cell></row></table><note>question-answer (CQA) pairs<ref type="bibr" target="#b7">(Cer et al., 2018;</ref><ref type="bibr" target="#b12">Yang et al., 2020)</ref>. We make use of two-stage training using two datasets: one is comprised of 2 Billion conversational input-response and QA (CQA) pairs drawn from web forums such as Reddit and StackExchange; the other consists of NLI pairs from the Stanford Natural Language Inference (SNLI)<ref type="bibr" target="#b5">(Bowman et al., 2015)</ref> and Multi-Genre Natural Language Inference (MNLI) (Williams</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Unlike BERT, T5's first token is not reserved as a special placeholder (i.e., CLS)</figDesc><table><row><cell>Model</cell><cell>Fine-tune data</cell><cell>MR</cell><cell cols="3">CR SUBJ MPQA</cell><cell cols="2">SST TREC MRPC</cell><cell>Avg</cell></row><row><cell>BERT (CLS-vector)</cell><cell>N/A</cell><cell cols="2">78.68 84.85</cell><cell>94.21</cell><cell cols="2">88.23 84.13</cell><cell>91.4</cell><cell>71.13 84.66</cell></row><row><cell>BERT (mean) ♣</cell><cell>N/A</cell><cell cols="2">78.66 86.25</cell><cell>94.37</cell><cell cols="2">88.66 84.40</cell><cell>92.80</cell><cell>69.45 84.94</cell></row><row><cell>ST5-Enc first</cell><cell>N/A</cell><cell cols="2">76.90 86.38</cell><cell>90.93</cell><cell cols="2">88.68 80.01</cell><cell>94.40</cell><cell>66.38 83.38</cell></row><row><cell>ST5-Enc mean</cell><cell>N/A</cell><cell cols="2">86.56 91.31</cell><cell>96.01</cell><cell cols="2">90.57 90.77</cell><cell>94.60</cell><cell>72.93 88.96</cell></row><row><cell>ST5-EncDec first</cell><cell>N/A</cell><cell cols="2">79.96 77.93</cell><cell>91.02</cell><cell cols="2">84.66 86.27</cell><cell>84.00</cell><cell>68.00 81.69</cell></row><row><cell>SBERT-NLI ♣</cell><cell>NLI</cell><cell cols="2">83.64 89.43</cell><cell>94.39</cell><cell cols="2">89.86 88.96</cell><cell>89.60</cell><cell>76.00 87.41</cell></row><row><cell>SimCSE-BERT ♣</cell><cell>NLI</cell><cell cols="2">82.69 89.25</cell><cell>94.81</cell><cell cols="2">89.59 87.31</cell><cell>88.40</cell><cell>73.51 86.51</cell></row><row><cell>SimCSE-RoBERTa ♣</cell><cell>NLI</cell><cell cols="2">84.92 92.00</cell><cell>94.11</cell><cell cols="2">89.82 91.27</cell><cell>88.80</cell><cell>75.65 88.08</cell></row><row><cell>ST5-Enc mean</cell><cell>NLI</cell><cell cols="2">86.17 91.71</cell><cell>94.70</cell><cell cols="2">90.90 90.44</cell><cell>90.00</cell><cell>76.70 88.66</cell></row><row><cell>ST5-EncDec first</cell><cell>NLI</cell><cell cols="2">86.22 91.60</cell><cell>94.05</cell><cell cols="2">90.93 90.72</cell><cell>92.60</cell><cell>76.06 88.88</cell></row><row><cell>ST5-Enc mean</cell><cell>CQA+NLI</cell><cell cols="2">85.75 92.08</cell><cell>94.58</cell><cell cols="2">90.95 91.76</cell><cell>96.40</cell><cell>75.19 89.53</cell></row><row><cell>ST5-Enc-1.1 mean</cell><cell>CQA+NLI</cell><cell cols="2">86.12 92.50</cell><cell>94.73</cell><cell cols="2">90.59 92.15</cell><cell>95.80</cell><cell>76.52 89.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance on transfer tasks on the SentEval benchmark. All models are using the Base architecture.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 ,</head><label>3</label><figDesc>we observe that the encoder-</figDesc><table><row><cell>Model</cell><cell># of params</cell><cell>Base Large</cell><cell>3B</cell><cell>11B</cell></row><row><cell cols="2">ST5-Enc</cell><cell cols="3">110M 335M 1.24B 4.8B</cell></row><row><cell cols="2">ST5-EncDec</cell><cell>220M 770M</cell><cell>3B</cell><cell>11B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Number of parameters for different models.</figDesc><table><row><cell>decoder architecture outperforms encoder-only</cell></row><row><cell>models for all STS tasks. As we scale up the ST5</cell></row><row><cell>model, we also observe improvement on STS tasks.</cell></row><row><cell>As shown in Table 5, the ST5 encoder-decoder</cell></row><row><cell>Large model outperforms the state-of-the-art model</cell></row><row><cell>SimCSE-RoBERTa Large, improving the Spear-</cell></row><row><cell>man's correlation score from 83.76 to 84.11.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of model performance on the SentEval benchmark when scaling up model size. ♣ results are from</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Wieting, Mohit Bansal, Kevin Gimpel, and Karen  Livescu. 2016. Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198. Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. Yinfei Yang, Daniel Matthew Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, G. Ábrego, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2020. Multilingual universal sentence encoder for semantic retrieval. In ACL. Yinfei Yang, Steve Yuan, Daniel Matthew Cer, Sheng yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2018. Learning semantic textual similarity from conversations. In Rep4NLP@ACL.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">SimCSE-RoBERTa achieves its best performance on transfer tasks by adding an additional masked language model loss during training, which is not used by ST5 or other models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">In preliminary experiments, we also explored fine-tuning with the classification loss used in InferSent<ref type="bibr" target="#b9">(Conneau et al., 2017)</ref> and Sentence-BERT<ref type="bibr" target="#b19">(Reimers and Gurevych, 2019)</ref>. However, as previously reported in<ref type="bibr" target="#b13">(Gao et al., 2021)</ref>, our results confirmed that fine-tuning for classification on an NLI dataset is inferior to contrastive learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Using only the entailment and contradict pairs results in 275K contrastive NLI pairs being available for training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">Following SimCSE<ref type="bibr" target="#b13">(Gao et al., 2021)</ref>, we report Spearman's correlation for the 'all' setting for all STS tasks which aggregates the data across different subsets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">7 https://github.com/google/jax</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">https://github.com/google-research/ text-to-text-transfer-transformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">Having sentence embeddings smoothly and uniformly distributed within the learned embedding space, which however can be be achieved by using a contrastive loss or regularization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7">SimCSE and ST5 only use the 'entailment' and 'contradict' pairs from MNLI datasets; while for SBERT, it also uses the 'neutral' pairs. This might explain why SBERT outperforms the others on MNLI.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their helpful comments. We thank Zora Tung, Daniel Andor, Adam Roberts, Hyung Won Chung, Anselm Levskaya and Livio Baldini Soares for help with the JAX implementation, and Alexis Conneau and Chris Tar for feedback and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Sent. Embed. Fine-tuning Score CoLA SST-2 MRPC STS-B QQP MNLI-m MNLI-mm QNLI RTE InferSent <ref type="bibr">(Wang et al., 2018</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Enc-11b</forename><surname>Encdec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<title level="m">11B Enc-3B Enc-Large Enc-Base EncDec-3B EncDec-Large EncDec-Base Sc al in g up References Eneko Agirre</title>
				<meeting><address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>SemEval</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012/Se-mEval 2012</date>
			<pubPlace>SEM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">SEM 2013 shared task: Semantic textual similarity</title>
				<editor>
			<persName><forename type="first">*</forename><surname>Sem</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Matthew Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eneko Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal sentence encoder</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Matthew Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Yi Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. Alexis Conneau and Douwe Kiela</title>
				<imprint>
			<date type="published" when="2018">2021. 2018</date>
		</imprint>
	</monogr>
	<note>LREC</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Languageagnostic BERT sentence embedding</title>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2007.01852</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.08821</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-end retrieval in continuous space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<idno>ArXiv, abs/1811.08008</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">László</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>ArXiv, abs/1705.00652</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skipthought vectors</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified textto-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>JMLR, 21/140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
