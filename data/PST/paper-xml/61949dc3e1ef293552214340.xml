<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
							<email>f.bianchi@unibocconi.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Bocconi University</orgName>
								<address>
									<addrLine>Via Sarfatti 25</addrLine>
									<postCode>20136</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
							<email>s.terragni4@campus.unimib.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Milan-Bicocca Viale Sarca 336</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<email>dirk.hovy@unibocconi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Bocconi University</orgName>
								<address>
									<addrLine>Via Sarfatti 25</addrLine>
									<postCode>20136</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the crucial issues with topic models is the quality of the topics they discover. Coherent topics are easier to interpret and are considered more meaningful. E.g., a topic represented by the words "apple, pear, lemon, banana, kiwi" would be considered a meaningful topic on FRUIT and is more coherent than one defined by "apple, knife, lemon, banana, spoon." Coherence can be measured in numerous ways, from human evaluation via intrusion tests <ref type="bibr" target="#b5">(Chang et al., 2009)</ref> to approximated scores <ref type="bibr" target="#b16">(Lau et al., 2014;</ref><ref type="bibr" target="#b27">RÃ¶der et al., 2015)</ref>.</p><p>However, most topic models still use Bag-of-Words (BoW) document representations as input. These representations, though, disregard the syntactic and semantic relationships among the words in a document, the two main linguistic avenues to coherent text. I.e., BoW models represent the input in an inherently incoherent manner.</p><p>Meanwhile, pre-trained language models are becoming ubiquitous in Natural Language Processing (NLP), precisely for their ability to cap-ture and maintain sentential coherence. Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, the most prominent architecture in this category, allows us to extract pre-trained word and sentence representations. Their use as input has advanced state-of-the-art performance across many tasks. Consequently, BERT representations are used in a diverse set of NLP applications <ref type="bibr" target="#b28">(Rogers et al., 2020;</ref><ref type="bibr" target="#b23">Nozza et al., 2020)</ref>.</p><p>Various extensions of topic models incorporate several types of information <ref type="bibr" target="#b37">(Xun et al., 2017;</ref><ref type="bibr" target="#b39">Zhao et al., 2017;</ref><ref type="bibr" target="#b32">Terragni et al., 2020a)</ref>, use word relationships derived from external knowledge bases <ref type="bibr" target="#b6">(Chen et al., 2013;</ref><ref type="bibr" target="#b38">Yang et al., 2015;</ref><ref type="bibr" target="#b34">Terragni et al., 2020b)</ref>, or pre-trained word embeddings <ref type="bibr" target="#b7">(Das et al., 2015;</ref><ref type="bibr" target="#b9">Dieng et al., 2020;</ref><ref type="bibr" target="#b22">Nguyen et al., 2015;</ref><ref type="bibr" target="#b39">Zhao et al., 2017)</ref>. Even for neural topic models, there exists work on incorporating external knowledge, e.g., via word embeddings <ref type="bibr" target="#b11">(Gupta et al., 2019</ref><ref type="bibr" target="#b12">(Gupta et al., , 2020;;</ref><ref type="bibr" target="#b9">Dieng et al., 2020)</ref>.</p><p>In this paper, we show that adding contextual information to neural topic models provides a significant increase in topic coherence. This effect is even more remarkable given that we cannot embed long documents due to the sentence length limit in recent pre-trained language models architectures.</p><p>Concretely, we extend Neural ProdLDA (Product-of-Experts LDA) <ref type="bibr" target="#b30">(Srivastava and Sutton, 2017)</ref>, a state-of-the-art topic model that implements black-box variational inference <ref type="bibr" target="#b25">(Ranganath et al., 2014)</ref>, to include contextualized representations. Our approach leads to consistent and significant improvements in two standard metrics on topic coherence and produces competitive topic diversity results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>We propose a straightforward and easily implementable method that allows neural topic models to create coherent topics. We show that the use of contextualized document embeddings in neural topic models produces significantly more coherent topics. Our results suggest that topic models benefit from latent contextual information, which is missing in BoW representations. The resulting model addresses one of the most central issues in topic modeling. We release our implementation as a Python library, available at the following link: https://github.com/MilaNLProc/ contextualized-topic-models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Topic Models with Language Model Pre-training</head><p>We introduce a Combined Topic Model (Com-binedTM) to investigate the incorporation of contextualized representations in topic models. Our model is built around two main components: (i) the neural topic model ProdLDA <ref type="bibr" target="#b30">(Srivastava and Sutton, 2017)</ref> and (ii) the SBERT embedded representations <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref>. Let us notice that our method is indeed agnostic about the choice of the topic model and the pre-trained representations, as long as the topic model extends an autoencoder and the pre-trained representations embed the documents.</p><p>ProdLDA is a neural topic modeling approach based on the Variational AutoEncoder (VAE). The neural variational framework trains a neural inference network to directly map the BoW document representation into a continuous latent representation. Then, a decoder network reconstructs the BoW by generating its words from the latent document representation<ref type="foot" target="#foot_0">1</ref> . The framework explicitly approximates the Dirichlet prior using Gaussian distributions, instead of using a Gaussian prior like Neural Variational Document Models <ref type="bibr" target="#b18">(Miao et al., 2016)</ref>. Moreover, ProdLDA replaces the multinomial distribution over individual words in LDA with a product of experts <ref type="bibr" target="#b14">(Hinton, 2002)</ref>. We extend this model with contextualized document embeddings from SBERT <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref>, 2 a recent extension of BERT that allows the quick generation of sentence embeddings. This approach has one limitation. If a document is longer than SBERT's sentence-length limit, the rest of the document will be lost. The document representations are projected through a hidden layer with the same dimensionality as the vocabulary size, concatenated with the BoW representence-transformers sentation. Figure <ref type="figure" target="#fig_0">1</ref> briefly sketches the architecture of our model. The hidden layer size could be tuned, but an extensive evaluation of different architectures is out of the scope of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setting</head><p>We provide detailed explanations of the experiments (e.g., runtimes) in the supplementary materials. To reach full replicability, we use open-source implementations of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate the models on five datasets: 20News-Groups<ref type="foot" target="#foot_2">3</ref> , Wiki20K (a collection of 20,000 English Wikipedia abstracts from Bianchi et al. ( <ref type="formula">2021</ref>)), Tweets2011<ref type="foot" target="#foot_3">4</ref> , Google News <ref type="bibr" target="#b24">(Qiang et al., 2019)</ref>, and the StackOverflow dataset <ref type="bibr" target="#b24">(Qiang et al., 2019)</ref>. The latter three are already pre-processed. We use a similar pipeline for 20NewsGroups and Wiki20K: removing digits, punctuation, stopwords, and infrequent words. We derive SBERT document representations from unpreprocessed text for Wiki20k External word embeddings topic coherence (Î±) provides an additional measure of how similar the words in a topic are. We follow <ref type="bibr" target="#b10">Ding et al. (2018)</ref> and first compute the average pairwise cosine similarity of the word embeddings of the top-10 words in a topic, using <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> embeddings. Then, we compute the overall average of those values for all the topics. We can consider this measure as an external topic coherence, but it is more efficient to compute than Normalized Pointwise Mutual Information on an external corpus.</p><p>Inversed Rank-Biased Overlap (Ï) evaluates how diverse the topics generated by a single model are. We define Ï as the reciprocal of the standard RBO <ref type="bibr" target="#b35">(Webber et al., 2010;</ref><ref type="bibr" target="#b33">Terragni et al., 2021b)</ref>. RBO compares the 10-top words of two topics. It allows disjointedness between the lists of topics (i.e., two topics can have different words in them) and uses weighted ranking. I.e., two lists that share some of the same words, albeit at different rankings, are penalized less than two lists that share the same words at the highest ranks. Ï is 0 for identical topics and 1 for completely different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Models</head><p>Our main objective is to show that contextual information increases coherence. To show this, we compare our approach to ProdLDA (Srivastava and Sutton, 2017, the model we extend) 7 , and the following models: (ii) Neural Variational Document Model (NVDM) <ref type="bibr" target="#b18">(Miao et al., 2016)</ref>; (iii) the very recent ETM (Dieng et al., 2020), MetaLDA (MLDA) <ref type="bibr" target="#b39">(Zhao et al., 2017)</ref> and (iv) LDA <ref type="bibr" target="#b1">(Blei et al., 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Configurations</head><p>To maximize comparability, we train all models with similar hyper-parameter configurations.</p><p>The inference network for both our method and ProdLDA consists of one hidden layer and 100dimension of softplus units, which converts the input into embeddings. This final representation is again passed through a hidden layer before the variational inference process. We follow (Srivastava and Sutton, 2017) for the choice of the parameters. The priors over the topic and document distributions are learnable parameters. For LDA, the Dirichlet priors are estimated via Expectation-Maximization. See the Supplementary Materials for additional details on the configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We divide our results into two parts: we first describe the results for our quantitative evaluation, and we then explore the effect on the performance when we use two different contextualized representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Evaluation</head><p>We compute all the metrics for 25, 50, 75, 100, and 150 topics. We average results for each metric over 30 runs of each model (see Table <ref type="table">2</ref>). As a general remark, our model provides the most coherent topics across all corpora and topic settings, even maintaining a competitive diversity of the topics. This result suggests that the incorporation of contextualized representations can improve a topic model's performance.</p><p>LDA and NVDM obtain low coherence. This result has also also been confirmed by <ref type="bibr" target="#b30">Srivastava and Sutton (2017)</ref>. ETM shows good external coherence (Î±), especially in 20NewsGroups and Stack-Overflow. However, it fails at obtaining a good Ï coherence for short texts. Moreover, Ï shows that the topics are very similar to one another. A manual inspection of the topics confirmed this problem. MetaLDA is the most competitive of the models we used for comparison. This may be due to the incorporation of pre-trained word embeddings into MetaLDA. Our model provides very competitive results, and the second strongest model appears to be MetaLDA. For this reason, we provide a detailed comparison of Ï in Table <ref type="table" target="#tab_2">3</ref>, where we show the average coherence for each number of topics; we show that on 4 datasets over 5 our model provides the best results, but still keeping a very competitive score on 20NG, where MetaLDA is best.</p><p>Readers can see examples of the top words for each model in the Supplementary Materials. These descriptors illustrate the increased coherence of topics obtained with SBERT embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Using Different Contextualized Representations</head><p>Contextualized representations can be generated from different models and some representations might be better than others. Indeed, one question left to answer is the impact of the specific contextualized model on the topic modeling task. To answer to this question we rerun all the experiments with CombinedTM but we used different contextualized sentence embedding methods as input to the model. We compare the performance of CombinedTM using two different models for embedding the contextualized representations found in the SBERT repository:</p><p><ref type="foot" target="#foot_7">8</ref> stsb-roberta-large (Ours-R), as employed in the previous experimental setting, and using bert-base-nli-means (Ours-B). The latter is derived from a BERT model fine-tuned on NLI data. Table <ref type="table" target="#tab_3">4</ref> shows the coherence of the two approaches on all the datasets (we averaged all results). In these experiments, RoBERTa fine-tuned on the STSb dataset has a strong impact on the increase of the coherence. This result suggests that including novel and better contextualized embeddings can further improve a topic model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In recent years, neural topic models have gained increasing success and interest <ref type="bibr" target="#b40">(Zhao et al., 2021;</ref><ref type="bibr" target="#b31">Terragni et al., 2021a)</ref>, due to their flexibility and scalability. Several topic models use neural networks <ref type="bibr">(Larochelle and Lauly, 2012;</ref><ref type="bibr" target="#b29">Salakhutdinov and Hinton, 2009;</ref><ref type="bibr" target="#b12">Gupta et al., 2020)</ref> or neural variational inference <ref type="bibr" target="#b18">(Miao et al., 2016;</ref><ref type="bibr" target="#b20">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b30">Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b17">Miao et al., 2017;</ref><ref type="bibr" target="#b10">Ding et al., 2018)</ref>. <ref type="bibr" target="#b18">Miao et al. (2016)</ref> propose NVDM, an unsupervised generative model based on VAEs, assuming a Gaussian distribution over topics. Building upon NVDM, Dieng et al.</p><p>(2020) represent words and topics in the same embedding space. <ref type="bibr" target="#b30">Srivastava and Sutton (2017)</ref> propose a neural variational framework that explicitly approximates the Dirichlet prior using a Gaussian distribution. Our approach builds on this work but includes a crucial component, i.e., the representations from a pre-trained transformer that can benefit from both general language knowledge and corpusdependent information. Similarly, <ref type="bibr" target="#b0">Bianchi et al. (2021)</ref> replace the BOW document representation with pre-trained contextualized representations to tackle a problem of cross-lingual zero-shot topic modeling. This approach was extended by <ref type="bibr" target="#b21">Mueller and Dredze (2021)</ref> that also considered fine-tuning the representations. A very recent approach (Hoyle et al., 2020) which follows a similar direction uses knowledge distillation <ref type="bibr" target="#b13">(Hinton et al., 2015)</ref> to combine neural topic models and pre-trained transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a straightforward and simple method to incorporate contextualized embeddings into topic models. The proposed model significantly improves the quality of the discovered topics. Our results show that context information is a significant element to consider also for topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statement</head><p>In this research work, we used datasets from the recent literature, and we do not use or infer any sensible information. The risk of possible abuse of our models is low.</p><p>the topic and document distributions are learnable parameters. Momentum is set to 0.99, the learning rate is set to 0.002, and we apply 20% of drop-out to the hidden document representation. The batch size is equal to 200. More details related to the architecture can be found in the original work <ref type="bibr" target="#b30">(Srivastava and Sutton, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Combined TM</head><p>The model and the hyper-parameters are the same used for ProdLDA with the difference that we also use SBERT features in combination with the BoW: we take the SBERT English embeddings, apply a (learnable) function/dense layer R 1024 â R |V | and concatenate the representation to the BoW. We run 100 epochs of the model. We use ADAM optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 LDA</head><p>We use Gensim's 9 implementation of this model.</p><p>The hyper-parameters Î± and Î², controlling the document-topic and word-topic distribution respectively, are estimated from the data during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ETM</head><p>We use the implementation available at https: //github.com/adjidieng/ETM with default hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Meta-LDA</head><p>We use the authors' implementation available at https://github.com/ethanhezhao/MetaLDA.</p><p>As suggested, we use the Glove embeddings to initialize the models. We used the 50-dimensional embeddings from https://nlp.stanford.edu/ projects/glove/. The parameters Î± and Î² have been set to 0.1 and 0.01 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Neural Variational Document Model (NVDM)</head><p>We use the implementation available at https: //github.com/ysmiao/nvdm with default hyperparameters, but using two alternating epochs for encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Computing Infrastructure</head><p>We ran the experiments on two common laptops, equipped with a GeForce GTX 1050: models can be easily run with basic infrastructure (having a GPU is better than just using CPU, but the experiment can also be replicated with CPU). Both lap-9 https://radimrehurek.com/gensim/ models/ldamodel.html tops have 16GB of RAM. CUDA version for the experiments was 10.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Runtime</head><p>What influences the computational time the most is the number of words in the vocabulary. Table <ref type="table" target="#tab_5">5</ref> shows the runtime for one epoch of both our Combined TM (CTM) and ProdLDA (PDLDA) for 25 and 50 topics on Google News and 20Newsgroups datasets with the GeForce GTX 1050. ProdLDA is faster than our Combined TM. This is due to the added representation. However, we believe that these numbers are quite similar and make our model easy to use, even with common hardware.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level sketch of CombinedTM. Refer to (Srivastava and Sutton, 2017) for more details on the architecture we extend.</figDesc><graphic url="image-1.png" coords="2,318.19,127.57,196.44,177.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Docs Vocabulary</cell></row><row><cell cols="2">20Newsgroups 18,173</cell><cell>2,000</cell></row><row><cell>Wiki20K</cell><cell>20,000</cell><cell>2,000</cell></row><row><cell cols="2">StackOverflow 16,408</cell><cell>2,303</cell></row><row><cell>Tweets2011</cell><cell>2,471</cell><cell>5,098</cell></row><row><cell>Google News</cell><cell>11,108</cell><cell>8,110</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Ï between CombinedTM (ours) and MetaLDA over various choices of topics. Each result averaged over 30 runs.â£ indicates statistical significance of the results (t-test, p-value &lt; 0.05).</figDesc><table><row><cell>Wiki20K</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell cols="2">100</cell><cell>150</cell></row><row><cell>Ours</cell><cell>0.17 â£</cell><cell>0.19 â£</cell><cell>0.18 â£</cell><cell>0.19</cell><cell>â£</cell><cell>0.17 â£</cell></row><row><cell>MLDA</cell><cell>0.15</cell><cell>0.15</cell><cell>0.14</cell><cell cols="2">0.14</cell><cell>0.13</cell></row><row><cell>SO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>0.05</cell><cell>0.03 â£</cell><cell>0.02 â£</cell><cell>0.02</cell><cell>â£</cell><cell>0.02 â£</cell></row><row><cell>MLDA</cell><cell>0.05 â£</cell><cell>0.02</cell><cell>0.00</cell><cell cols="2">-0.02</cell><cell>0.00</cell></row><row><cell>GNEWS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>-0.03 â£</cell><cell>0.10 â£</cell><cell>0.15 â£</cell><cell>0.18</cell><cell>â£</cell><cell>0.19 â£</cell></row><row><cell>MLDA</cell><cell>-0.06</cell><cell>0.07</cell><cell>0.13</cell><cell cols="2">0.16</cell><cell>0.14</cell></row><row><cell>Tweets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>0.05 â£</cell><cell>0.10 â£</cell><cell>0.11 â£</cell><cell>0.12</cell><cell>â£</cell><cell>0.12 â£</cell></row><row><cell>MLDA</cell><cell>0.00</cell><cell>0.05</cell><cell>0.06</cell><cell cols="2">0.04</cell><cell>-0.07</cell></row><row><cell>20NG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>0.12</cell><cell>0.11</cell><cell>0.10</cell><cell cols="2">0.09</cell><cell>0.09</cell></row><row><cell>MLDA</cell><cell>0.13 â£</cell><cell>0.13 â£</cell><cell>0.13 â£</cell><cell>0.13</cell><cell>â£</cell><cell>0.12</cell></row></table><note>â£</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ï performance of CombinedTM using different contextualized encoders.</figDesc><table><row><cell></cell><cell>Wiki20K</cell><cell>SO</cell><cell cols="3">GN Tweets 20NG</cell></row><row><cell>Ours-R</cell><cell>0.18</cell><cell cols="2">0.03 0.12</cell><cell>0.10</cell><cell>0.10</cell></row><row><cell>Ours-B</cell><cell>0.18</cell><cell cols="2">0.02 0.08</cell><cell>0.06</cell><cell>0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Time to complete one epoch on Google News and 20Newsgroups datasets with 25 and 50 topics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For more details see(Srivastava and Sutton,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017" xml:id="foot_1">). 2 https://github.com/UKPLab/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://qwone.com/ Ëjason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://trec.nist.gov/data/tweets/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">This can be sub-optimal, but many datasets in the literature are already pre-processed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">stsb-roberta-large</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We use the implementation of<ref type="bibr" target="#b3">Carrow (2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/UKPLab/ sentence-transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our colleague Debora Nozza and Wray Buntine for providing insightful comments on a previous version of this paper. Federico Bianchi and Dirk Hovy are members of the Bocconi Institute for Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We pre-processed 20NewsGroup and Wiki20K. We removed punctuation, digits, and nltk's English stop-words. Following other researchers, we selected 2,000 as the maximum number of words for the BoW, and thus we kept only the 2,000 most frequent words in the documents. The other datasets come already pre-processed (reference links are in the paper) and thus we take them as is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Models and Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 ProdLDA</head><p>We use the implementation made available by <ref type="bibr" target="#b3">Carrow (2018)</ref> since it is the most recent and with the most updated packages (e.g., one of the latest versions of PyTorch). We run 100 epochs of the model. We use ADAM optimizer. The inference network is composed of a single hidden layer and 100-dimension of softplus units. The priors over</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual contextualized topic models with zero-shot learning</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1676" to="1683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PyTorchAVITM: Open Source AVITM Implementation in PyTorch</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Carrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Github</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">IÃ±igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
				<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering coherent topics using general knowledge</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MalÃº</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505519</idno>
	</analytic>
	<monogr>
		<title level="m">22nd ACM International Conference on Information and Knowledge Management, CIKM&apos;13</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10-27">2013. October 27 -November 1, 2013</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00325</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="439" to="453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coherence-aware neural topic modeling</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4. 2018</date>
			<biblScope unit="page" from="830" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Document informed neural autoregressive topic models with distributional prior</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016505</idno>
		<idno>AAAI2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="6505" to="6512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural topic modeling with continual lifelong learning</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schuetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976602760128018</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hugo Larochelle and Stanislas Lauly. 2012. A neural autoregressive topic model</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miserlis Hoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.137</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2717" to="2725" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Jey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
				<meeting>the 14th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
				<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1791" to="1799" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-tuning encoders for improved monolingual and zero-shot polylingual neural topic modeling</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3054" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00140</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02912</idno>
		<title level="m">Making Sense of Language-Specific BERT Models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jipeng</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhenyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07695</idno>
		<title level="m">Short text topic modeling techniques, applications, and performance: A survey</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>RÃ¶der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
		<idno type="DOI">10.1145/2684822.2685324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015</title>
				<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00349</idno>
		<title level="m">A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">OCTIS: Comparing and optimizing topic models is simple!</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Galuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Tropeano</surname></persName>
		</author>
		<author>
			<persName><surname>Candelieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enza</forename><surname>Messina</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2019.09.039</idno>
		<title level="m">Constrained relational topic models. Information Sciences</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page" from="581" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word embedding-based topic similarity measures</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enza</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems -26th International Conference on Applications of Natural Language to Information Systems, NLDB 2021</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Which matters most? comparing the impact of concept and document relationships in topic models</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Messina</forename><surname>Enza</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.insights-1.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Insights from Negative Results in NLP</title>
				<meeting>the First Workshop on Insights from Negative Results in NLP</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A similarity measure for indefinite rankings</title>
		<author>
			<persName><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A correlated topic model using word embeddings</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4207" to="4213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient methods for incorporating knowledge into topic models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Metalda: A topic model that efficiently incorporates meta information</title>
		<author>
			<persName><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00498</idno>
		<title level="m">Topic modelling meets deep neural networks: A survey</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
