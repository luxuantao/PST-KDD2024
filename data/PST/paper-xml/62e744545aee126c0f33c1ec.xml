<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Hypergraph Transformer for Recommender Systems</title>
				<funder>
					<orgName type="full">Department of Computer Science &amp; Musketeers Foundation Institute of Data Science at the University of Hong Kong</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-28">28 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chaohuang75@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<email>chuxuzhang@brandeis.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Brandeis University Waltham</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KDD&apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Washington DC</orgName>
								<address>
									<addrLine>10 pages</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Hypergraph Transformer for Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-28">28 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539473</idno>
					<idno type="arXiv">arXiv:2207.14338v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-Supervised Learning</term>
					<term>Graph Neural Networks</term>
					<term>Hypergraph Representation</term>
					<term>Recommender System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have been shown as promising solutions for collaborative filtering (CF) with the modeling of useritem interaction graphs. The key idea of existing GNN-based recommender systems is to recursively perform the message passing along the user-item interaction edge for refining the encoded embeddings. Despite their effectiveness, however, most of the current recommendation models rely on sufficient and high-quality training data, such that the learned representations can well capture accurate user preference. User behavior data in many practical recommendation scenarios is often noisy and exhibits skewed distribution, which may result in suboptimal representation performance in GNN-based models. In this paper, we propose SHT, a novel Self-Supervised Hypergraph Transformer framework (SHT) which augments user representations by exploring the global collaborative relationships in an explicit way. Specifically, we first empower the graph neural CF paradigm to maintain global collaborative effects among users and items with a hypergraph transformer network. With the distilled global context, a cross-view generative self-supervised learning component is proposed for data augmentation over the user-item interaction graph, so as to enhance the robustness of recommender systems. Extensive experiments demonstrate that SHT can significantly improve the performance over various stateof-the-art baselines. Further ablation studies show the superior representation ability of our SHT recommendation framework in alleviating the data sparsity and noise issues. The source code and evaluation datasets are available at: https://github.com/akaxlh/SHT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems have become increasingly important to alleviate the information overload for users in a variety of web applications, such as e-commerce systems <ref type="bibr" target="#b4">[5]</ref>, streaming video sites <ref type="bibr" target="#b16">[17]</ref> and location-based lifestyle apps <ref type="bibr" target="#b3">[4]</ref>. To accurately infer the user preference, encoding user and item informative representations is the core part of effective collaborative filtering (CF) paradigms based on the observed user-item interactions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Earlier CF models project interaction data into latent user and item embeddings using matrix factorization (MF) <ref type="bibr" target="#b12">[13]</ref>. Due to the strong representation ability of deep learning, various neural network CF models have been developed to project users and items into latent low-dimensional representations, such as autoencoder <ref type="bibr" target="#b14">[15]</ref> and attention mechanism <ref type="bibr" target="#b1">[2]</ref>. Recent years have witnessed the development of graph neural networks (GNNs) for modeling graphstructural data <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. One promising direction is to perform the information propagation along the user-item interactions to refine user embeddings based on the recursive aggregation schema. For example, upon the graph convolutional network, PinSage <ref type="bibr" target="#b37">[38]</ref> and NGCF <ref type="bibr" target="#b25">[26]</ref> attempt to aggregate neighboring information by capturing the graph-based CF signals for recommendation. To simplify the graph-based message passing, LightGCN <ref type="bibr" target="#b5">[6]</ref> omits the burdensome non-linear transformer during the embedding propagation and improve the recommendation performance. To further enhance the graph-based user-item interaction modeling, some follow-up studies propose to learn intent-aware representations with disentangled graph neural frameworks (e.g., DisenHAN <ref type="bibr" target="#b28">[29]</ref>), differentiate behavior-aware embeddings of users with multi-relational graph neural models (e.g., MB-GMN <ref type="bibr" target="#b33">[34]</ref>).</p><p>Despite the effectiveness of the above graph-based CF models by providing state-of-the-art recommendation performance, several key challenges have not been well addressed in existing methods. First, data noise is ubiquitous in many recommendation scenarios due to a variety of factors. For example, users may click their uninterested products due to the over-recommendation of popular items <ref type="bibr" target="#b41">[42]</ref>. In such cases, the user-item interaction graph may contain " interest-irrelevant" connections. Directly aggregating information from all interaction edges will impair the accurate user representation. Worse still, the embedding propagation among multi-hop adjacent vertices (user or item) will amplify the noise effects, which misleads the encoding of underlying user interest in GNN-based recommender systems. Second, data sparsity and skewed distribution issue still stand in the way of effective useritem interaction modeling, leading to most existing graph-based CF models being biased towards popular items <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref>. Hence, the recommendation performance of current approaches severely drops with the user data scarcity problem, as the high-quality training signals could be small. While there exist a handful of recently developed recommendation methods (SGL <ref type="bibr" target="#b30">[31]</ref> and SLRec <ref type="bibr" target="#b36">[37]</ref>) leveraging self-supervised learning to improve user representations, these methods mainly generate the additional supervision information with probability-based randomly mask operations, which might keep some noisy interaction and dropout some important training signals during the data augmentation process.</p><p>Contribution. In light of the aforementioned challenges, this work proposes a Self-Supervised Hypergraph Transformer (SHT) to enhance the robustness and generalization performance of graphbased CF paradigms for recommendation. Specifically, we integrate the hypergraph neural network with the topology-aware Transformer, to empower our SHT to maintain the global cross-user collaborative relations. Upon the local graph convolutional network, we first encode the topology-aware user embeddings and inject them into Transformer architecture for hypergraph-guided message passing within the entire user/item representation space.</p><p>In addition, we unify the modeling of local collaborative relation encoder with the global hypergraph dependency learning under a generative self-supervised learning framework. Our proposed new self-supervised recommender system distills the auxiliary supervision signals for data augmentation through a graph topological denoising scheme. A graph-based meta transformation layer is introduced to project hyergraph-based global-level representations into the graph-based local-level interaction modeling for user and item dimensions. Our new proposed SHT is a model-agnostic method and serve as a plug-in learning component in existing graph-based recommender systems. Specifically, SHT enables the cooperation of the local-level and global-level collaborative relations, to facilitate the graph-based CF models to learn high-quality user embeddings from noisy and sparse user interaction data.</p><p>The key contributions of this work are summarized as follows:</p><p>? In this work, we propose a new self-supervised recommendation model-SHT to enhance the robustness of graph collaborative filtering paradigms, by integrating the hypergraph neural network with the topology-aware Transformer.</p><p>? In the proposed SHT method, the designed hypergraph learning component encodes the global collaborative effects within the entire user representation space, via a learnable multi-channel hyperedge-guided message passing schema. Furthermore, the local and global learning views for collaborative relations are integrated with the cooperative supervision for interaction graph topological denoising and auxiliary knowledge distillation.</p><p>? Extensive experiments demonstrate that our proposed SHT framework achieves significant performance improvement over 15 different types of recommendation baselines. Additionally, we conduct empirical analysis to show the rationality of our model design with the ablation studies. Recommendation with Graph Neural Networks. Recent studies have attempted to design various graph neural architectures to model the user-item interaction graphs through embedding propagation. For example, PinSage <ref type="bibr" target="#b37">[38]</ref> and NGCF <ref type="bibr" target="#b25">[26]</ref> are built upon the graph convolutional network over the spectral domain. Later on, LightGCN <ref type="bibr" target="#b5">[6]</ref> proposes to simplify the heavy non-linear transformation and utilizes the sum-based pooling over neighboring representations. Upon the GCN-based message passing schema, each user and item is encoded into the transformed embeddings with the preservation of multi-hop connections. To further improve the user representation, some recent studies attempt to design disentangled graph neural architecture for user-item interaction modeling, such as DGCF <ref type="bibr" target="#b27">[28]</ref> and DisenHAN <ref type="bibr" target="#b28">[29]</ref>. Several multirelational GNNs are proposed to enhance recommender systems with multi-behavior modeling, including KHGT <ref type="bibr" target="#b31">[32]</ref> and HMG-CR <ref type="bibr" target="#b34">[35]</ref>. However, most of existing graph neural CF models are intrinsic designed to merely rely on the observation interaction lables for model training, which makes them incapable of effectively modeling interaction graph with sparse and noisy supervision signals. To overcome these challenges, this work proposes a self-supervised hypergraph transformer architecture to generate informative knowledge through the effective interaction between local and global collaborative views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph-based Recommender</head><p>Systems. There exist some recently developed models constructing hypergraph connections to improve the relation learning for recommendation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref>. For example, HyRec <ref type="bibr" target="#b24">[25]</ref> regards users as hyperedges to aggregate information from the interacted items. MHCN <ref type="bibr" target="#b38">[39]</ref> constructs multichannel hypergraphs to model high-order relationships among users. Furthermore, DHCF <ref type="bibr" target="#b10">[11]</ref> is a hypergraph collaborative filtering model to learn the hybrid high-order correlations. Different from these work for generating hypergraph structures with manually design, this work automates the hypergraph structure learning process with the modeling of global collaborative relation.</p><p>Self-Supervised Graph Learning. To improve the embedding quality of supervised learning, self-supervised learning (SSL) has become a promising solution with auxiliary training signals <ref type="bibr" target="#b15">[16]</ref>, such as augmented image data <ref type="bibr" target="#b11">[12]</ref>, pretext sequence tasks for language data <ref type="bibr" target="#b23">[24]</ref>, knowledge graph augmentation <ref type="bibr" target="#b35">[36]</ref>. Recently, self-supervised learning has also attracted much attention on graph representation <ref type="bibr" target="#b9">[10]</ref>. For example, DGI <ref type="bibr" target="#b22">[23]</ref> and GMI <ref type="bibr" target="#b17">[18]</ref> perform the generative self-supervised learning over the GNN framework with auxiliary tasks. Inspired by the graph self-supervised learning, SGL <ref type="bibr" target="#b30">[31]</ref> produces state-of-the-art performance by generating contrastive views with randomly node and edge dropout operations. Following this research line, HCCF <ref type="bibr" target="#b32">[33]</ref> leverages the hypergraph to generate contrastive signals to improve the graph-based recommender system. Different from them, this work enhances the graph-based collaborative filtering paradigm with a generative selfsupervised learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we present the proposed SHT framework and show the overall model architecture in Figure <ref type="figure" target="#fig_0">1</ref>. SHT embeds local structure information into latent node representations, and conduct global relation learning with the local-aware hypergraph transformer. To train the proposed model, we augment the regular parameter learning with the local-global cross-view self-augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Graph Structure Learning</head><p>To begin with, we embed users and items into a ?-dimensional latent space to encode their interaction patterns. For user ? ? and item ? ? , embedding vectors e ? , e ? ? R ? are generated, respectively. Also, we aggregate all the user and item embeddings to compose embedding matrices E (?) ? R ? ?? , E (?) ? R ? ?? , respectively. We may omit the superscript (?) and (?) for notation simplification when it is not important to differentiate the user and item index.</p><p>Inspired by recent success of graph convolutional networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> in capturing local graph structures, we propose to encode the neighboring sub-graph structure of each node into a graph topologyaware embedding, to inject the topology positional information into our graph transformer. Specifically, SHT employs a two-layer light-weight graph convolutional network as follows:</p><formula xml:id="formula_0">?(?) = GCN 2 (E (?) , G) = ? ? ?? E (?) + ? ? E (?)<label>(1)</label></formula><p>where ?(?) ? R ? ?? denotes the topology-aware embeddings for users. GCN 2 (?) denotes two layers of message passing. ? ? R ? ?? refers to the normalized adjacent matrix of graph G, which is calculated by ??,? = A ?,? /(D</p><formula xml:id="formula_1">(?)1/2 ? D (?)1/2 ?</formula><p>), where A is the original binary adjacent matrix. D</p><formula xml:id="formula_2">(?) ? , D (?)</formula><p>? refer to the degree of ? ? and ? ? in graph G, respectively. Note that SHT considers neighboring nodes in different distance through residual connections. The topologyaware embeddings for items can be calculated analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hypergraph Transformer for Global Relation Learning</head><p>Though existing graph-based neural networks have shown their strength in learning interaction data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>, the inherent noise and skewed data distribution in recommendation scenario limit the performance of graph representation for user embeddings. To address this limitation, SHT adopts a hypergraph transformer framework, which i) alleviates the noise issue by enhancing the user collaborative relation modeling with the adaptive hypergraph relation learning; ii) transfer knowledge from dense user/item nodes to sparse ones. Concretely, SHT is configured with a Transformer-like attention mechanism for structure learning. The encoded graph topology-aware embeddings are injected into the node representations to preserve the graph locality and topological positions.</p><p>Meanwhile, the multi-channel attention <ref type="bibr" target="#b21">[22]</ref> further benefits our structure learning in SHT.</p><p>In particular, SHT generates input embedding vectors for ? ? and ? ? by combining the id-corresponding embeddings (e ? , e ? ) together with the topology-aware embeddings ( vectors ?? , ?? from embedding tables ?(?) and ?(?) ) as follows:</p><formula xml:id="formula_3">?? = e ? + ?? ; ?? = e ? + ??<label>(2)</label></formula><p>Then, SHT conducts hypergraph-based information propagation as well as hypergraph structure learning using ?? , ?? as input. We utilize ? hyperedges to distill the collaborative relations from the global perspective. Node embeddings are propagated to each other using hyperedges as intermediate hubs, where the connections between nodes and hyperedges are optimized to reflect the implicit dependencies among nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Node-to-Hyperedge Propagation. Without loss of generality, we mainly discuss the information propagation between user nodes and user-side hyperedges for simplicity. The same process is applied for item nodes analogously. The propagation from user nodes to user-side hyperedges can be formally presented as follows:</p><formula xml:id="formula_4">z? = ? ?=1 z?,? ; z?,? = ? ?? ?=1 v ?,? k ? ?,? q ?,?<label>(3)</label></formula><p>where z? ? R ? denotes the embedding for the ?-th hyperedge. It is calculated by concatenating the ? head-specific hyperedge embeddings z?,? ? R ?/? . q ?,? , k ?,? , v ?,? ? R ?/? are the query, key and value vectors in the attention mechanism which will be elaborated later. Here, we calculate the edge weight between hyperedge ? and user ? ? through a linear dot-product k ? ?,? q ?,? , which reduces the complexity from ? (? ? ? ??/? ) to ? ((? + ?) ?? 2 /? 2 ) by avoiding directly calculating the node-hyperedge connections (i.e. k ? ?,? q ?,? ), but the key-value dot-product first (i.e. ? ?=1 v ?,? k ? ?,? ). In details, the multi-head query, key and value vectors are calculated through linear transformations and slicing. The ?-headspecific embeddings are calculated by:</p><formula xml:id="formula_5">q ?,? = Z ?,? ?-1 :? ? ; k ?,? = K ? ?-1 :? ? ,: ?? ; v ?,? = V ? ?-1 :? ? ,: ?? (4)</formula><p>where q ?,? ? R ?/? denotes the ?-head-specific query embedding for the ?-th hyperedge, k ?,? , v ?,? ? R ?/? denotes the ?-headspecific key and value embedding for user ? ? . Z ? R ??? represents the embedding matrix for the ? hyperedges. K, V ? R ??? represents the key and the value transformation of all the ? heads, respectively. ? ?-1 = (?-1)? ?</p><p>and ? ? = ?? ? denote the start and the end indices of the ?-th slice.</p><p>To further excavate the complex non-linear feature interactions among the hyperedges, SHT is augmented with two-layer hierarchical hypergraph neural networks for both user side and item side. Specifically, the final hyperedge embeddings are calculated by:</p><formula xml:id="formula_6">? = HHGN 2 ( Z); HHGN(X) = ? (H ? X + X)<label>(5)</label></formula><p>where ?, Z ? R ??? represent the embedding tables for the final and the original hyperedge embeddings, consisting of hyperedgespecific embedding vectors ?, z ? R ? , respectively. HHGN 2 (?) denotes applying the hierarchical hypergraph network (HHGN) twice. HHGN is configured with a learnable parametric matrix H ? R ??? , which characterizes the hyperedge-wise relations. An activation function ? (?) is introduced for non-linear relation modeling. Additionally, we utilize a residual connection to facilitate gradient propagation in our hypergraph neural structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User-Item Interaction Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Hyperedge-to-Node Propagation. With the final hyperedge embeddings ?, we propagate the information from hyperedges to user/item nodes through a similar but reverse process:</p><formula xml:id="formula_7">?? ? = ? ?=1 ?? ?,? ; ?? ?,? = ? ?? ?=1 v ? ?,? k ?? ?,? q ? ?,?<label>(6)</label></formula><p>where ?? ? ? R ? denotes the new embedding for user ? ? refined by the hypergraph neural network. ?? ?,? ? R ?/? denotes the node embedding calculated by the ?-th attention head for</p><formula xml:id="formula_8">? ? . q ? ?,? , k ? ?,? , v ? ?,? ? R ?/?</formula><p>represent the query, key and value vectors for user ? ? and hyperedge ?. The attention calculation in this hyperedge-to-node propagation process shares most parameters with the aforementioned node-to-hyperedge propagation. The former query serves as key, and the former key serves as query here. The value calculation applies the same value transformation for the hyperedge embedding. The calculation process can be formally stated as:</p><formula xml:id="formula_9">q ? ?,? = k ?,? ; k ? ?,? = q ?,? ; v ? ?,? = V ? ?-1 :? ? ,: ??<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Iterative Hypergraph Propagation.</head><p>Based on the prominent node-wise relations captured by the learned hypergraph structures, we propose to further propagate the encoded global collaborative relations via stacking multiple hypergraph transformer layers. In this way, the long-range user/item dependencies can be characterized by our SHT framework through the iterative hypergraph propagation. In form, taking the embedding tables ??-1 in the (? -1)-th iteration as input, SHT recursively applies the hypergraph encoding (denoted by HyperTrans(?)) and obtains the final node embeddings ? ? R ? ?? or R ? ?? as follows:</p><formula xml:id="formula_10">?? = HyperTrans( ??-1 ); ? = ? ?? ?=1 ?? (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where the layer-specific embeddings are combined through elementwise summation. The iterative hypergraph propagation is identical for the user nodes and item nodes. Finally, SHT makes predictions through dot product as ? ?,? = ?(?)? ? ?(?) ? , where ? ?,? is the forecasting score denoting the probability of ? ? interacting with ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local-Global Self-Augmented Learning</head><p>The foregoing hypergraph transformer addresses the data sparsity problem through adaptive hypergraph message passing. However, the graph topology-aware embedding for local collaborative relation modeling may still be affected by the interaction data noise. To tackle this challenge, we propose to enhance the model training with self-augmented learning between the local topology-aware embedding and the global hypergraph learning. To be specific, the topology-aware embedding for local information extraction is augmented with an additional task to differentiate the solidity of sampled edges in the observed user-item interaction graph. Here, solidity refers to the probability of an edge not being noisy, and its label in the augmented task is calculated based on the learned hypergraph dependencies and representations. In this way, SHT transfers knowledge from the high-level and denoised features in the hypergraph transformer, to the low-level and noisy topologyaware embeddings, which is expected to recalibrate the local graph structure and improve the model robustness. The workflow of our self-augmented module is illustrated in Fig 2 . 

3.3.1 Solidity Labeling with Meta Networks. In our SHT model, the learned hypergraph dependency representations can serve as useful knowledge to denoise the observed user-item interactions by associating each edge with a learned solidity score. Specifically, we reuse the key embeddings k ?,? , k ?,? in Eq 4 to represent user ? ? and item ? ? when estimating the solidity score for the edge (? ? , ? ? ). This is because that the key vectors are generated for relation modeling and can be considered as helpful information source for interaction solidity estimation. Furthermore, we propose to also take the hyperedge embeddings Z ? R ??? in Eq 4 into consideration, to introduce global characteristics into the solidity labeling.</p><p>Concretely, we first concatenate the multi-head key vectors and apply a simple perceptron to eliminate the gap between user/itemhyperedge relation learning and user-item relation learning. Formally, the updated user/item embeddings are calculated by:</p><formula xml:id="formula_12">? ? = ? (?) ? ?=1 k ?,? ; ? ? = ? (?) ? ?=1 k ?,?<label>(9)</label></formula><p>where ? (?) (?), ? (?) (?) are the user-and item-specific perceptrons for feature vector transformation, respectively. This projection is conducted with a meta network, using the user-side and the itemside hyperedge embeddings as input individually: </p><formula xml:id="formula_13">? (x; Z) = ? (Wx + b); W = V 1 z + W 0 ; b = V 2 z + b 0 (<label>10</label></formula><formula xml:id="formula_14">z ? /?). V 1 ? R ????? , W 0 ? R ??? , V 2 ? R ??? , b 0 ? R ? are the parameters of the meta network.</formula><p>With the updated user/item embeddings ? ? , ? ? , SHT then calculates the solidity labels for edge (? ? , ? ? ) through a two-layer neural network as follows:</p><formula xml:id="formula_15">? ?,? = sigm(d ? ? ? (T ? [? ? ; ? ? ] + ? ? + ? ? + c))<label>(11)</label></formula><p>where ? ?,? ? R denotes the solidity score given by the hypergraph transformer. sigm(?) denotes the sigmoid function which limits the value range of ? ?,? . d ? R ? , T ? R ??2? , c ? R ? are the parametric matrices or vectors. [?; ?] denotes the vector concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Pair-wise Solidity Ranking. To enhance the optimization of topological embeddings, SHT employs an additional objective function to better estimate the edge solidity using the above ? ?,? as training labels. In particular, ? pairs of edges {(? 1,1 , ? 1,2 ),...,(? ?,1 , ? ?,2 )} from the observed edges in G are sampled, and SHT gives predictions on the solidity using the topology-aware embeddings. The predictions are then updated by optimizing the loss below: </p><formula xml:id="formula_16">L ?? = ? ?? ? =1 max(0, 1 -( ?? ?,1 ,?</formula><p>where L ?? denotes the loss function for our self-augmented learning. ?? ?,1 ,? ?,1 , ?? ?,2 ,? ?,2 denote the solidity scores predicted by the topology-aware embedding, while ? ? ?,1 ,? ?,1 , ? ? ?,2 ,? ?,2 denote the edge solidity labels given by the hypergraph transformer. Here ? ?,1 and ? ?,1 represent the user and the item node of edge ? ?,1 , respectively.</p><p>In the above loss function, the label term (? ? ?,1 ,? ?,1 -? ? ?,2 ,? ?,2 ) not only indicates the sign of the difference (i.e. which one of ? ?,1 and ? ?,2 is bigger), but also indicates how bigger the difference is. In this way, if the solidity labels for a pair of edges given by the hypergraph transformer are close to each other, then the gradients on the predicted solidity scores given by the topology-aware embedding will become smaller. In this way, SHT is self-augmented with an adaptive ranking task, to further refine the low-level topologyaware embeddings using the high-level embeddings encoded from the hypergraph transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Learning</head><p>We train our SHT by optimizing the main task on implicit feedback together with the self-augmented ranking task. Specifically, ? ? positive edges (observed in G) and ? ? negative edges (not observed The following pair-wise marginal objective function is applied:</p><formula xml:id="formula_18">L = ? ? ?? ? =1 max(0, 1 -(? ? ?,1 ,? ?,1 -? ? ?,2 ,? ?,2 )) + ? 1 L sa + ? 2 ??? 2 F (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where ? ? ?,1 ,? ?,1 and ? ? ?,2 ,? ?,2 are prediction scores for edge ? ?,1 and ? ?,2 , respectively. ? 1 and ? 2 are weights for different loss terms. ??? 2 F denotes the ? 2 regularization term for weight decay. 3.4.1 Complexity Analysis. We compare our SHT framework with several state-of-the-art approaches on collaborative filtering, including graph neural architectures (e.g. NGCF <ref type="bibr" target="#b25">[26]</ref>, LightGCN <ref type="bibr" target="#b5">[6]</ref>) and hypergraph neural networks(e.g. DHCF <ref type="bibr" target="#b10">[11]</ref>). As discussed before, our hypergraph transformer enables the complexity reduction from ? (? ? (? + ? ) ? ?) to ? ((? + ? + ?) ? ? 2 ). As the typical value of the number of hyperedge ? is much smaller than the number of nodes ? and ? , but larger than the embedding size ?, the latter term is smaller and close to ? ((? + ? ) ?? 2 ). In comparison, the complexity for a typical graph neural architecture is ? (? ? ? + (? + ? ) ? ? 2 ). So our hypergraph transformer network can achieve comparable efficiency as GNNs, such as graph convolutional networks in model inference. The existing hypergraph-based methods commonly preprocess high-order node relations to construct hypergraphs, which makes them usually more complex than the graph neural networks. In our SHT, the self-augmented task with the loss L sa has the same complexity with the original main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To evaluate the effectiveness of our SHT, our experiments are designed to answer the following research questions: ? RQ1: How does our SHT perform by comparing to strong baseline methods of different categories under different settings? ? RQ2: How do the key components of SHT (e.g., the hypergraph modeling, the transformer-like information propagation) contribute to the overall performance of SHT on different datasets? ? RQ3: How well can our SHT handle noisy and sparse data, as compared to baseline methods? ? RQ4: In real cases, can the designed the self-supervised learning mechanism in SHT provide useful interpretations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Experimental Datasets. The experiments are conducted on three datasets collected from real-world applications, i.e., Yelp, Gowalla and Tmall. The statistics of them are shown in Table <ref type="table" target="#tab_3">1</ref>.</p><p>? Yelp: This commonly-used dataset contains user ratings on business venues collected from Yelp. Following other papers on implicit feedback <ref type="bibr" target="#b8">[9]</ref>, we treat users' rated venues as interacted items and treat unrated venues as non-interacted items.</p><p>? Gowalla: It contains users' check-in records on geographical locations obtained from Gowalla. This evaluation dataset is generated from the period between 2016 and 2019. ? Tmall: This E-commerce dataset is released by Tmall, containing users' behaviors for online shopping. We collect the page-view interactions during December in 2017.</p><p>4.1.2 Evaluation Protocols. Following the recent collaborative filtering models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>, we split the datasets by 7:2:1 into training, validation and testing sets. We adopt all-rank evaluation protocol. When testing a user, the positive items in the test set and all the non-interacted items are tested and ranked together. We employ the commonly-used Recall@N and Normalized Discounted Culmulative Gain (NDCG)@N as evaluation metrics for recommendation performance evaluation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>. N is set as 20 by default.</p><p>4.1.3 Compared Baseline Methods. We evaluate our SHT by comparing it with 15 baselines from different research lines for comprehensive evaluation.</p><p>Traditional Factorization-based Technique.</p><p>? BiasMF <ref type="bibr" target="#b12">[13]</ref>: This method augments matrix factorization with user and item bias vectors to enhance user-specific preferences.</p><p>Neural Factorization Method.</p><p>? NCF <ref type="bibr" target="#b6">[7]</ref>: This method replaces the dot-product in conventional matrix factorization with multi-layer neural networks. Here, we adopt the NeuMF variant for comparison.</p><p>Autoencoder-based Collaborative Filtering Approach.</p><p>? AutoR <ref type="bibr" target="#b20">[21]</ref>: It improves user/item representations with a threelayer autoencoder trained under a behavior reconstruction task.</p><p>Graph Neural Networks for Recommendation.</p><p>? GCMC <ref type="bibr" target="#b0">[1]</ref>: This is one of the pioneering work to apply graph convolutional networks (GCNs) to the matrix completion task. ? PinSage <ref type="bibr" target="#b37">[38]</ref>: It applies random sampling in graph convolutional framework to study the collaborative filtering task .</p><p>? NGCF <ref type="bibr" target="#b25">[26]</ref>: This graph convolution-based approach additionally takes source-target representation interaction learning into consideration when designing its graph encoder. ? STGCN <ref type="bibr" target="#b39">[40]</ref>: The model combines conventional graph convolutional encoders with graph autoencoders to improve the model robustness against sparse and cold-start samples. ? LightGCN <ref type="bibr" target="#b5">[6]</ref>: This work conducts in-depth analysis to study the effectiveness of modules in standard GCN for collaborative data, and proposes a simplified GCN model for recommendation. ? GCCF [3]: This is another method which simplifies the GCNs by removing the non-linear transformation. In GCCF, the effectiveness of residual connections across graph iterations is validated.</p><p>Disentangled Graph Model for Recommendation.</p><p>? DGCF <ref type="bibr" target="#b27">[28]</ref>: It disentangles user interactions into multiple latent intentions to model user preference in a fine-grained way.</p><p>Hypergraph-based Neural Collaborative Filtering.</p><p>? HyRec <ref type="bibr" target="#b24">[25]</ref>: This is a sequential collaborative model that learns item-wise high-order relations with hypergraphs. ? DHCF <ref type="bibr" target="#b10">[11]</ref>: This model adopts dual-channel hypergraph neural networks for both users and items in collaborative filtering.</p><p>Recommenders enhanced by Self-Supervised Learning .</p><p>? MHCN <ref type="bibr" target="#b38">[39]</ref>: This model maximizes the mutual information between node embeddings and global readout representations, to regularize the representation learning for interaction graph. ? SLRec <ref type="bibr" target="#b36">[37]</ref>: This approach employs the contrastive learning between the node features as regularization terms to enhance the existing recommender systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison (RQ1)</head><p>In this section, we validate the effectiveness of our SHT framework by conducting the overall performance evaluation on the three datasets and comparing SHT with various baselines. We also retrain SHT and the best-performed baseline (i.e. SGL) for 10 times to compute p-values. The results are presented in Table <ref type="table" target="#tab_5">2</ref>.</p><p>? Performance Superiority of SHT. As shown in the results, SHT achieves best performance compared to the baselines under both top-20 and top-40 settings. The t-tests also validate the significance of performance improvements. We attribute the superiority to: i) Based on the hypergraph transformer, SHT not only realizes global message passing among semantically-relevent users/items, but also refines the hypergraph structure using the multi-head attention. ii) The global-to-local self-augmented learning distills knowledge from the high-level hypergraph transformers to regularize the topology-aware embedding learning, and thus alleviate the data noise issue. ? Effectiveness of Hypergraph Architecture. Among the stateof-the-art baselines, approaches that based on hypergraph neural networks (HGNN) (i.e., HyRec and DHCF) outperforms most of the GNN-based baselines (e.g., GCMC, PinSage, NGCF, STGCN). This sheds lights on the insufficiency of conventional GNNs in capturing high-order and global graph connectivity. Meanwhile, our SHT is configured with transformer-like hypergraph structure learning which further excavates the potential of HGNN in global relation learning. In addition, most existing hypergraphbased models utilize user or item nodes as hyperedges, while our SHT adopts latent hyperedges which not only enables automatic graph dependency modeling, but also avoids pre-calculating the large-scale high-order relation matrix. ? Effectiveness of Self-Augmented Learning. From the evaluation results, we can observe that self-supervised learning obviously improves existing CF frameworks (e.g., MHCN, SLRec, SGL). The improvements can be attributed to incorporating the augmented learning task, which provides the beneficial regularization on the parameter learning based on the input data itself. Specifically, MHCN regularizes the node embeddings according to a read-out global information of the holistic graph. This approach may be too strict for large graphs containing many local sub-graphs with their own characteristics. Meanwhile, SLRec and SGL adopt stochastic data augmentation to construct multiple data views, and conduct contrastive learning to capture the invariant feature from the corrupted views. In comparison to the above methods, the self-augmentation in our SHT has mainly two merits: i) SHT adopts meta networks to generate global-structure-aware mapping functions for domain adaption, which adaptively alleviates the gap between local and global feature spaces. ii) Our self-supervised approach does not depend on random masking, which may drop important information to hinder representation learning. Instead, SHT self-augment the model training by transferring knowledge from the high-level hypergraph embeddings to the low-level topology-aware embedding. The superior performance of SHT compared to the baseline self-supervised approaches validates the effectiveness of our new design of self-supervised learning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Ablation Test (RQ2)</head><p>To validate the effectiveness of the proposed modules, we individually remove the applied techniques in the three major parts of SHT (i.e., the local graph structure capturing, the global relation learning, and the local-global self-augmented learning). The variants are re-trained for test on the three datasets. Both prominent components (e.g., the entire hypergraph transformer) and small modules (e.g., the deep hyperedge feature extraction) of SHT are ablated. The results can be seen in Table <ref type="table" target="#tab_6">3</ref>. We have the following major conclusions:</p><p>? Removing either the graph topology-aware embedding module or the hypergraph transformer (i.e., -Pos and -Hyper) severely damage the performance of SHT in all cases. This result suggests the necessity of local and global relation learning, and validates the effectiveness of our GCN-based topology-aware embedding and hypergraph transformer networks. ? The variant without self-augmented learning (i.e. -SAL) yields obvious performance degradation in all cases, which validates the positive effect of our augmented global-to-local knowledge transferring. The effect of our meta-network-based domain adaption can also be observed in the variant -Meta. ? We also ablate the components in our hypergraph neural network.</p><p>Specifically, we substitute the hypergraph transformer with independent node-hypergraph matrices (-Trans), and we remove the deep hyperedge feature extraction to keep only one layer of hyperedges (-DeepH ). Additionally, we remove the high-order hypergraph iterations (-HighH ). From the results we can conclude that: i) Though using much less parameters, the transformer-like hypergraph attention works much better than learning hypergraphbased user/item dependencies. ii) The deep hyperedge layers indeed make contribution to the global relation learning through non-linear feature transformation. iii) Though our hypergraph transformer could connect any users/items using learnable hyperedges, high-order iterations still improve the model performance through the iterative hypergraph propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Robustness Test (RQ3)</head><p>4.4.1 Performance w.r.t. Data Noise Degree. In this section, we first investigate the robustness of SHT against the data noise.</p><p>To evaluate the influence of noise degrees on model performance, we randomly substitute different percentage of real edges with randomly-generated fake edges, and re-train the model using the corrupted graphs as input. Concretely 5%, 10%, 15%, 20%, 25% of the edges are replaced with noisy signals in our experiments. We compare SHT with MHCN and LightGCN, which are recent recommenders based on HGNN and GNN, respectively. To better study the effect of noise on performance degradation, we evaluate the relative performance compared to the performance on original data. The results are shown in Fig 3 . We can observe that our method presents smaller performance degradation in most cases compared to the baselines. We ascribe this observation to two reasons: i) The global relation learning and information propagation by our hypergraph transformer alleviate the noise effect caused by the raw observed user-item interactions. ii) The self-augmented learning task distills knowledge from the refined hypergraph embeddings, so as to refine the graph-based embeddings. In addition, we can observe that the relative performance degradation on the Gowalla data is more obvious compared with other two datasets. This is because the noisy data has larger influence for the performance on the sparsest Gowalla dataset.   In <ref type="bibr">Fig 4,</ref><ref type="bibr"></ref> we present both the recommendation accuracy and performance difference between our SHT and compared methods. From the results, we have the following observations: i) The superior performance of SHT is consistent on datasets with different sparsity degrees, which validates the robustness of SHT in handling sparse data for both users and items. ii) The sparsity of item interaction vectors has obviously larger influence on model performance for all the methods. This indicates that the collaborative pattern of items are more difficult to model compared to users, such that more neighbors usually result in better representations. iii) In the item-side experiments, the performance gap on the middle subdatasets is larger compared to the gap on the densest sub-dataset. This suggests the better anti-sparsity capability of SHT for effectively transferring knowledge among dense samples and sparse samples with our proposed hypergraph transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study (RQ4)</head><p>In this section, we analyze the concrete data instances to investigate the effect of our hypergraph transformer with self-augmentation from two aspects: i) Is the hypergraph-based dependency modeling in SHT capable of learning useful node-wise relations, especially the implicit relations unknown to the training process? ii) Is the selfaugmented learning with meta networks in SHT able to differentiate noisy edges in the training data? To this end, we select three users with fair number of interactions from Tmall dataset. The interacted items are visualized as colored circles representing their trained embeddings (refer to the supplementary material for details about the visualization algorithm). The results are shown in Fig 5 . For the above questions, we have the following observations:</p><p>? Implicit relation learning. Even if the items are interacted by same users, their learned embeddings are usually divided into multiple groups with different colors. This may relate to users' multiple interests. To study the differences between the item groups, we present additional item-wise relations that are not utilized in the training process. Specifically, we connect items belonging to same categories, and items co-interacted by same users. Note that only view data is used in model training, so interactions in other behaviors are unknown to the trained model. It is clear that there exist dense implicit correlations among samecolored items (e.g., the green items of user (a), the purple items of user (b), and the orange items of user (c)). Meanwhile, there are much less implicit relations between items of different colors. This results shows the capability of SHT in identifying useful implicit relations, which we ascribe to the global structure learning of our hypergraph transformer. ? Noise discrimination. Furthermore, we show the solidity scores ? estimated from our self-augmented learning, for the user-item relations in Fig 5 . We also show the normalized values of some notable edges in the corresponding circles (e.g., edges of item 10202 and 6508 are labeled with 2.3 and 1.9). The red values are anomalously low, which may indicates noise. The black values are the lowest and highest solidity scores for edges except the anomalous ones. By analyzing user (a), we can regard the yellow and green items as two interests of user (a) as they are correlated in terms of their learned embeddings. In contrast, item 6508 and 10202 have few relations to other interacted items of user (a), which may not reflect the real interactive patterns of this user. Thus, the model may consider this two edges as noisy interactions. Similar cases can be found for user (b), where item 2042 has few connections to the other items and show difference with the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we explore the self-supervised recommender systems with an effective hypergraph transformer network. We propose a new recommendation framework SHT, which seeks better useritem interaction modeling with self-augmented supervision signals.</p><p>Our SHT model improves the robustness of graph-based recommender systems against noise perturbation. In our experiments, we achieved better recommendation results on real-world datasets.</p><p>Our future work would like to extend our SHT to explore the disentangled user intents with diverse user-item relations for encoding multi-dimensional user preferences.</p><p>In the supplementary material, we first show the learning process of SHT with pseudocode summarized in Algorithm 1. Then, we investigate the influences of different hyperparameter settings, and discuss the impact of three key hyperparameters. Finally, we describe the details of our vector visualization algorithm used in the case study experiments.</p><p>6.1 Learning process of SHT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hyperparameter Investigation</head><p>We study the effect of three important hyperparameters, the hidden dimensionality ?, the number of latent hyperedges ?, and the number of graph iterations ?. To present more results, we calculate the relative performance decrease in terms of evaluation metrics, compared to the best performance under default settings. The results are shown in Fig <ref type="figure" target="#fig_6">6</ref>, our observations are shown below:</p><p>? The latent embedding dimension size largely determines the representation ability of the proposed SHT model. Small ? greatly limits the efficacy of SHT, by 15%-35% performance decrease. However, greater ? does not always yield obvious improvements. As shown by results when ? = 68 on Yelp data, the performance increases marginally due to the over-fitting effect. ? The curve of performance w.r.t. hyperedge number ? typically follows the under-to over-fitting pattern. However, it is interesting that ? has significantly less influence compared to ? (at most -6% and -35%, respectively). This is because the hyperedgenode connections in SHT are calculated in a ?-dimensional space, which reduces the amount of independent parameters related to ? to ? (? ? ?). So ? has much smaller impact on model capacity compared to ?, which relates to ? ((? + ? ) ? ?) parameters. ? For the number of graph iterations ?, smaller ? hinders nodes from aggregating high-order neighboring information. When ? = 0, graph neural networks degrades significantly. Meanwhile, by stacking more graph layers may cause over-smoothing issue, which yields indistinguishable node embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Vector Visualization Algorithm</head><p>In our case study experiments, each item embeddings of 32 dimensions is visualized with a color. This visualization process should preserve the learned item information in the embedding vectors. Meanwhile, to make the visualization results easy to understand, it would be better to pre-select several colors to use. Considering the above two requirements, we design a neural-network-based dimension reduction algorithm. Specifically, we train a multi-layer perceptron to map 32-dimensional item embeddings to 3-dimensional RGB values. The network is trained using two objective functions, corresponding to the forgoing two requirements. Firstly, the compressed 3-d vectors (colors) are fed into a classifier, to predict the original item ids. Through this self-discrimination task, the network is trained to preserve the original embedding information in the RGB vectors. Secondly, the network is trained with a regularizer that calculates the distance between each color vectors and the preferred colors. Using the two objectives, we can map embeddings into preferred colors while preserving the embedding information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall framework of the proposed SHT model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relative performance degradation w.r.t noise ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 4 . 2</head><label>42</label><figDesc>Performance w.r.t. Data Sparsity. We further study the influence of data sparsity from both user and item side on model performance. We compare our SHT with two representative baselines LightGCN and SGL. Multiple user and item groups are constructed in terms of their number of interactions in the training set. For example, the first group in the user-side experiments contains users interacting with 15-20 items, and the first group in the item-side experiment contains items interacting with 0-8 users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance w.r.t. different data sparsity degrees on Gowalla data. Lines present Recall@40 and NDCG@40 values, and bars shows performance differences between baselines and our SHT with corresponding colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Case study on inferring implicit item-wise relations and discriminating potential noise edges. Circles denote items interacted by the centric users, and their learned embeddings are visualized with colors. Implicit item-wise relations not utilized during model training are presented by green and blue lines. The type of co-interactions are also labeled (e.g., view-cart denotes viewed and added-to-cart by same users). Also, the inferred solidity scores ? are shown on the circles, where red values are anomalously low scores indicating noisy edges. embedding color. It is labeled with low ? scores and considered as noise by SHT. The results show the effective noise discrimination ability of the self-augmented learning in SHT, which recalibrate the topology-aware embedding using global information encoded from hypergraph transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hyperparameter study of the SHT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Recap Graph Collaborative Filtering Paradigm. To enhance the Collaborative Filtering with the multi-order connectivity information, one prominent line of recommender systems generates graph structures for user-item interactions. Suppose our recommendation scenario involves ? users and ? items with the user set U = {? 1 , ...? ? } and item set V = {? 1 , ...? ? }. Edges in the user-item interaction graph G are constructed if user ? ? has adopted item ? ? . Upon the constructed interaction graph structures, the core component of graph-based CF paradigm lies in the information aggregation function-gathering the feature embeddings of neighboring users/items via different aggregators, e.g., mean or sum.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 2: Workflow of the self-augmented learning. the mean pooling of hyperedge embeddings (i.e. z = ? ?=1</figDesc><table><row><cell cols="2">Solidity Labeling</cell><cell></cell><cell>Solidity Predicting</cell></row><row><cell cols="3">Meta Network</cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>? (?) ? (?)</cell><cell>? 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell>? ? ? j</cell><cell>? ??</cell><cell>? ??</cell></row><row><cell>? 1,1 ? 1,2</cell><cell>?</cell><cell>? ?,1 ? ?,2</cell><cell>? ?? ? ? s ? s ? ?</cell></row></table><note><p><p>)</p>where x ? R ? denotes the input user/item key embedding (e.g. ? ? , ? ? ). ? (?) being user-specific or item-specific depends on Z being userside or item-side hyperedge embedding table. W ? R ??? and b ? R ? are the parameters generated by the meta network according to the input Z. In this way, the parameters are generated based on the learned hyperedge embeddings, which encodes global features of user-or item-specific hypergraphs. z ? R ? denotes Sample ? edge pairs</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistical information of the experimental datasets.</figDesc><table><row><cell>Stat.</cell><cell>Yelp</cell><cell>Gowalla</cell><cell>Tmall</cell></row><row><cell># Users</cell><cell>29601</cell><cell>50821</cell><cell>47939</cell></row><row><cell># Items</cell><cell>24734</cell><cell>24734</cell><cell>41390</cell></row><row><cell># Interactions</cell><cell>1517326</cell><cell>1069128</cell><cell>2357450</cell></row><row><cell>Density</cell><cell>2.1 ? 10</cell><cell></cell><cell></cell></row></table><note><p>-3 4.0 ? 10 -4 1.2 ? 10 -3 in G) are sampled {(? 1,1 , ? 1,2 ), (? 2,1 , ? 2,2 )..., (? ? ? ,1 , ? ? ? ,2 )}, where ? ?,1 and ? ?,2 are individual positive and negative sample, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Implementation Details. We implement our SHT using TensorFlow and use Adam as the optimizer for model training with the learning rate of 1? -3 and 0.96 epoch decay ratio. The models are configured with 32 embedding dimension size, and the number of graph neural layers is searched from {1,2,3}. The weights ? 1 , ? 2 for regularization terms are selected from {? ? 10 -? : ? ? {1, 3}, ? ? {2, 3, 4, 5}}. The batch size is selected from {32, 64, 128, 256, 512}. The rate for dropout is tuned from {0.25, 0.5, 0.75}. For our model, the number of hyperedges is set as 128 by default. Detailed hyperparameter settings can be found in our released source code.</figDesc><table><row><cell>? SGL [31]: This model conducts data augmentation through ran-</cell></row><row><cell>dom walk and feature dropout to generate multiple views. It</cell></row><row><cell>enhances LightGCN with self-supervised contrastive learning.</cell></row><row><cell>4.1.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on Yelp, MovieLens, Amazon datasets in terms of Recall and NDCG.</figDesc><table><row><cell>Data</cell><cell>Metric</cell><cell cols="3">BiasMF NCF AutoR GCMC PinSage NGCF STGCN LightGCN GCCF DGCF HyRec DHCF MHCN SLRec SGL</cell><cell>SHT</cell><cell>p-val.</cell></row><row><cell></cell><cell cols="2">Recall@20 0.0190 0.0252 0.0259 0.0266 0.0345 0.0294 0.0309</cell><cell>0.0482</cell><cell cols="2">0.0462 0.0466 0.0472 0.0449 0.0503 0.0476 0.0526 0.0651 9.3? -7</cell></row><row><cell>Yelp</cell><cell cols="2">NDCG@20 0.0161 0.0202 0.0210 0.0251 0.0288 0.0243 0.0262 Recall@40 0.0371 0.0487 0.0504 0.0585 0.0599 0.0522 0.0504</cell><cell>0.0409 0.0803</cell><cell cols="2">0.0398 0.0395 0.0395 0.0381 0.0424 0.0398 0.0444 0.0546 9.1? -8 0.0760 0.0774 0.0791 0.0751 0.0826 0.0821 0.0869 0.1091 4.1? -7</cell></row><row><cell></cell><cell cols="2">NDCG@40 0.0227 0.0289 0.0301 0.0373 0.0385 0.0330 0.0332</cell><cell>0.0527</cell><cell cols="2">0.0508 0.0511 0.0522 0.0493 0.0544 0.0541 0.0571 0.0709 2.2? -7</cell></row><row><cell></cell><cell cols="2">Recall@20 0.0196 0.0171 0.0239 0.0301 0.0576 0.0552 0.0369</cell><cell>0.0985</cell><cell cols="2">0.0951 0.0944 0.0901 0.0931 0.0955 0.0925 0.1030 0.1232 5.3? -7</cell></row><row><cell>Gowalla</cell><cell cols="2">NDCG@20 0.0105 0.0106 0.0132 0.0181 0.0373 0.0298 0.0217 Recall@40 0.0346 0.0216 0.0343 0.0427 0.0892 0.0810 0.0542</cell><cell>0.0593 0.1431</cell><cell cols="2">0.0535 0.0522 0.0498 0.0505 0.0574 0.0581 0.0623 0.0731 6.3? -7 0.1392 0.1401 0.1306 0.1356 0.1393 0.1305 0.1500 0.1804 1.5? -7</cell></row><row><cell></cell><cell cols="2">NDCG@40 0.0145 0.0118 0.0160 0.0212 0.0417 0.0367 0.0262</cell><cell>0.0710</cell><cell cols="2">0.0684 0.0671 0.0669 0.0660 0.0689 0.0680 0.0746 0.0881 3.2? -7</cell></row><row><cell></cell><cell cols="2">Recall@20 0.0103 0.0082 0.0103 0.0103 0.0202 0.0180 0.0146</cell><cell>0.0225</cell><cell cols="2">0.0209 0.0235 0.0233 0.0156 0.0203 0.0191 0.0268 0.0387 4.3? -9</cell></row><row><cell>Tmall</cell><cell cols="2">NDCG@20 0.0072 0.0059 0.0072 0.0072 0.0136 0.0123 0.0105 Recall@40 0.0170 0.0140 0.0174 0.0159 0.0345 0.0310 0.0245</cell><cell>0.0154 0.0378</cell><cell cols="2">0.0141 0.0163 0.0160 0.0108 0.0139 0.0133 0.0183 0.0262 4.9? -9 0.0356 0.0394 0.0350 0.0261 0.0340 0.0301 0.0446 0.0645 4.0? -9</cell></row><row><cell></cell><cell cols="2">NDCG@40 0.0095 0.0079 0.0097 0.0086 0.0186 0.0168 0.0140</cell><cell>0.0208</cell><cell cols="2">0.0196 0.0218 0.0199 0.0145 0.0188 0.0171 0.0246 0.0352 3.5? -9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on key components of SHT.</figDesc><table><row><cell>Category</cell><cell cols="2">Data Variants Recall NDCG Recall NDCG Recall NDCG Yelp Gowalla Tmall</cell></row><row><cell>Local</cell><cell>-Pos</cell><cell>0.0423 0.0352 0.0816 0.0487 0.0218 0.0247</cell></row><row><cell></cell><cell cols="2">-Trans 0.0603 0.0504 0.0999 0.0608 0.0321 0.0206</cell></row><row><cell>Global</cell><cell cols="2">-DeepH 0.0645 0.0540 0.1089 0.0634 0.0347 0.0234 -HighH 0.0598 0.0497 0.1091 0.0646 0.0336 0.0227</cell></row><row><cell></cell><cell cols="2">-Hyper 0.0401 0.0346 0.0879 0.0531 0.0209 0.0144</cell></row><row><cell>SAL</cell><cell cols="2">-Meta 0.0615 0.0526 0.1108 0.0717 0.0375 0.0255 -SAL 0.0602 0.0519 0.1099 0.0699 0.0363 0.0251</cell></row><row><cell cols="2">SHT</cell><cell>0.0651 0.0546 0.1232 0.0731 0.0387 0.0262</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Algorithm 1 :</head><label>1</label><figDesc>Learning Process of SHT Framework Input: user-item interaction graph G, number of graph layers ?, number of edges to sample ?, ? ? , maximum epoch number ?, learning rate ? Output: trained parameters in ? 1 Initialize all parameters in ? 2 for ? = 1 to ? do Draw a mini-batch U from all users {1, 2, ..., ? }</figDesc><table><row><cell>4</cell><cell cols="4">Calculate the graph topology-aware embeddings</cell><cell>?</cell></row><row><cell>5</cell><cell cols="4">Generate input embeddings ?0 for hypergraph</cell></row><row><cell></cell><cell cols="3">transformer</cell></row><row><cell>6</cell><cell cols="3">for ? = 1 to ? do</cell></row><row><cell>7</cell><cell cols="4">Conduct node-to-hyperedge propagation to obtain</cell></row><row><cell></cell><cell>Z(?)</cell><cell>,</cell><cell cols="2">Z(?) for both users and items</cell></row><row><cell>8</cell><cell cols="4">Conduct hierarchical hyperedge feature</cell></row><row><cell></cell><cell cols="3">transformation for</cell><cell>?(?)</cell><cell>,</cell><cell>?(?)</cell></row><row><cell>9</cell><cell cols="4">Propagate information from hyperedges back to</cell></row><row><cell></cell><cell cols="4">user/item nodes to obtain</cell><cell>?(?) ? ,</cell><cell>?(?) ?</cell></row><row><cell>10</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell>11</cell><cell cols="4">Aggregate the iteratively propagated embeddings to get</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell>22</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell cols="2">23 end</cell><cell></cell><cell></cell></row><row><cell cols="4">24 return all parameters ?</cell></row></table><note><p>3 12 Sample ? edge pairs for self-augmented learning 13 Acquire the user/item transformation function ? (?) and ? (?) with the meta network 14 Conduct user/item embedding transformations using ? (?) to get ? (?) , ? (?) 15 Calculate the solidity score ? for the ? edge pairs 16 Calculate the solidity predictions ? for the ? edge pairs 17 Compute loss L sa for self-augmented learning according to Eq 12 18 Sample ? ? edge pairs for the main task 19 Calculate the pair-wise marginal loss L according to Eq 13 20 for each parameter ? ? ? do 21 ? = ? -? ? ?L/??</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research work is supported by the research grants from the <rs type="funder">Department of Computer Science &amp; Musketeers Foundation Institute of Data Science at the University of Hong Kong</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Curriculum meta-learning for next POI recommendation</title>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2692" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Debiasing grid-based product search in e-commerce</title>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2852" to="2860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Recent advances in heterogeneous relation learning for recommendation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03455</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xiaoping Lai, and Yanfang Ye. 2021. Knowledge-aware coupled graph neural network for social recommendation</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huance</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised auxiliary learning with meta-paths for heterogeneous graphs</title>
		<author>
			<persName><forename type="first">Dasol</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunyoung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10294" to="10305" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dual channel hypergraph collaborative filtering</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In KDD. 2020-2029</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contragan: Contrastive learning for conditional image generation</title>
		<author>
			<persName><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21357" to="21369" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An adversarial approach to improve long-tail performance in neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Adit</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1491" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concept-Aware Denoising Graph Neural Network for Micro-Video Recommendation</title>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequential recommendation with self-attentive multi-adversarial network</title>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering vs. matrix factorization revisited</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax.. In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LexFit: Lexical fine-tuning of pretrained language models</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><surname>Glava?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5269" to="5283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Next-item recommendation with sequential hypergraphs</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1101" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>WWW. 2022-2032</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disenhan: Disentangled heterogeneous graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyao</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1605" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4486" to="4493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12200</idno>
		<title level="m">Hypergraph Contrastive Collaborative Filtering</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph meta network for multi-behavior recommendation</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Knowledge Graph Contrastive Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.00976</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised Learning for Large-scale Item Recommendations</title>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Star-gcn: Stacked and reconstructed graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A model of two tales: Dual transfer learning framework for improved long-tail item recommendation</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In WWW. 2220-2231</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal intervention for leveraging popularity bias in recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
