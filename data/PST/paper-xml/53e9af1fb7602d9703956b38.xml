<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised sparse representation method with a heuristic strategy and face recognition experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-11-15">15 November 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Network Oriented Intelligent Computation</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zizhu</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Bio-Computing Research Center</orgName>
								<orgName type="department" key="dep2">Shenzhen Graduate School</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Shenzhen Graduate School</orgName>
								<orgName type="department" key="dep2">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised sparse representation method with a heuristic strategy and face recognition experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-11-15">15 November 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">B8C1A6D05B3E60EA45FF2159BE69E6CC</idno>
					<idno type="DOI">10.1016/j.neucom.2011.10.013</idno>
					<note type="submission">Received 24 April 2010 Received in revised form 20 October 2011 Accepted 21 October 2011 Communicated by D. Tao</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Face recognition Pattern recognition Image representation Classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a supervised sparse representation method for face recognition. We assume that the test sample could be approximately represented by a sparse linear combination of all the training samples, where the term ''sparse'' means that in the linear combination most training samples have zero coefficients. We exploit a heuristic strategy to achieve this goal. First, we determine a linear combination of all the training samples that best represents the test sample and delete the training sample whose coefficient has the minimum absolute value. Then a similar procedure is carried out for the remaining training samples and this procedure is repeatedly carried out till the predefined termination condition is satisfied. The finally remaining training samples are used to produce a best representation of the test sample and to classify it. The face recognition experiments show that the proposed method can achieve promising classification accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past two decades much attention has been paid to face recognition and various face recognition methods have been proposed <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. For example, the well-known dimensionality reduction methods first generate a subspace from the original samples and then classify the test sample in this subspace. Stateof-the-art dimensionality reduction methods such as principal component analysis (PCA) <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> and the discriminant transform method (DTM) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> have shown competitive performance when used for face recognition. We also note that recently proposed max-min distance based <ref type="bibr" target="#b14">[15]</ref>, class-mean based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, patch alignment based <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> dimensionality reduction methods and the manifold learning methods <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> can achieve very promising performance. These dimensionality reduction methods have different motivations. For example, when transforming samples into a lower-dimensional space, PCA and DTM aim to preserve the most representative information of the original samples and the discriminative information of different classes, respectively. The max-min distance based dimensionality reduction method <ref type="bibr" target="#b14">[15]</ref> aims to make all class-pairs separate as much as possible by maximizing the minimum pairwise distance of all the classes in the obtained low dimensional subspace. The class-mean-based dimensionality reduction method attempts to maximize the geometric mean of the Kullback-Leibler divergences between different classes <ref type="bibr" target="#b16">[17]</ref>.</p><p>Recently, Wright et al. proposed a novel face recognition method, sparse representation method (SRM) <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. SRM exploits a sparse linear combination of all the training samples to represent and classify the face image. Hereafter the term ''sparse'' means that a number of coefficients are zero or very close to zero, and we refer to non-zero coefficients as ''sparse'' coefficients. This method evaluates the contribution, to representing the test sample, of each class and exploits the evaluation result to classify the test sample <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. As it is not known how many zero coefficients there are and which coefficients are zero, we refer to the method proposed in <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> as unsupervised sparse representation method. These methods usually use an iterative solution scheme to achieve the sparse linear combination. This solution scheme not only requires that the deviation between the test sample and the sparse linear combination be as small as possible but also requires that the l 1 -norm of the solution vector should be small enough.</p><p>The idea of sparsity has also been introduced in dimensionality reduction for face recognition. For example, non-negativeness <ref type="bibr" target="#b18">[19]</ref> and elastic net <ref type="bibr" target="#b19">[20]</ref> have been adopted to enforce the sparsity of either the projection matrix or the representation coefficients. In this paper, we focus on the SRM framework for face recognition, and propose a heuristic strategy to achieve a sparse representation of the test sample for accurate face recognition. We note that latest studies have new findings on sparserepresentation-based face recognition. For example, as shown in Section 4, some literatures claimed that the ''sparseness'' did not have the dominant effect in achieving accurate face recognition. Also, some researchers pointed out that the l 1 -norm was not necessary and the l 2 -norm could obtain a similar performance.</p><p>In this paper we propose a supervised sparse representation method. This method is based on a heuristic strategy and represents the test sample as a sparse linear combination of all the training samples. Hereafter 'supervised' means that the proposed method produces the sparseness in a supervised way where the number of the zero coefficients is known or predefined and it is known that to which training samples the zero coefficients are assigned. In our method the sparse coefficients are true zero, whereas in the SRM method <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> the coefficients are obtained using an iterative solution scheme and most of the 'sparse' coefficients are probably close to zero.</p><p>The idea of supervised sparse representation is inspired by the original SRM method <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> and the local learning methods <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. Previous studies have shown that the local learning method is able to obtain a higher accuracy than the global learning method <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Since the proposed method uses only a subset of all the training samples to represent the test sample, it can also be viewed as a method that uses ''local'' training samples to represent and classify the test sample. The proposed method first uses an iterative and heuristic strategy to determine ''local'' training samples for the test sample and then exploits only these ''local'' training samples to classify the test sample. Actually, the method also converts the original classification problem into a simpler classification problem that contains fewer classes. The explanation is as follows: it has been empirically proven and widely admitted that if the test sample is very far from the training samples of a class, it will be very reasonable not to classify the test sample into this class. Also, a distance-based classifier always classifies the test sample into a class that is near to it. As shown later, our method inclines to remove the training sample that is far from the test sample from the linear combination to represent the test sample. As a result, the remaining training samples are from only a few classes rather than all the classes and our method can make a decision in a smaller decision space. When our method uses the finally remaining training samples to classify the test sample, a higher accuracy might be obtained. As shown in Section 5, the experimental results also verify the feasibility and the effectiveness of the proposed method. Another contribution of our work is that it provides a way to integrate an idea similar to ''sparse representation'' with the l 2 norm for face recognition. Our paper also illustrates the following two points: first, the sparse representation is useful for accurate face recognition. Second, the l 1 norm is not the sole means to obtain sparse representation. The l 2 norm can be integrated with a special scheme such as the one presented in the paper to obtain sparse representation.</p><p>The remainder of the paper is organized as follows: in Section 2, we describe the global method that exploits all the training samples to represent and classify the test sample. In Section 3, we present the supervised sparse representation method. Section 4 shows the characteristics, rationale and advantages of the proposed method. Section 5 presents the experimental results. Finally, in Section 6, we offer our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Global method</head><p>In this section we present the global method that exploits all the training samples to represent and classify the test sample.</p><p>Suppose that there are n training samples, respectively, denoted by n column vectors A 1 yyA n . The global method assumes that test sample Y can be approximately represented by a linear combination of all the training samples. In other words, it considers that the following equation is approximately satisfied:</p><formula xml:id="formula_0">Y ¼ X n i ¼ 1 b i A i<label>ð1Þ</label></formula><p>We can rewrite Eq. ( <ref type="formula" target="#formula_0">1</ref>) as</p><formula xml:id="formula_1">Y ¼ Ab<label>ð2Þ</label></formula><formula xml:id="formula_2">where b ¼ ðb 1 :::b n Þ T , A ¼ ðA 1 , Á Á Á ,A n Þ.</formula><p>If A T A is non-singular, the least squares solution of Eq. ( <ref type="formula" target="#formula_1">2</ref>) is</p><formula xml:id="formula_3">b ¼ ðA T AÞ À1 A T Y<label>ð3Þ</label></formula><formula xml:id="formula_4">If A T A is (or is nearly) singular, we solve b using b ¼ ðA T A þ gIÞ À1 A T Y<label>ð4Þ</label></formula><p>where g is a small positive constant and I is the identity matrix.</p><p>Once we obtain b using Eq. ( <ref type="formula" target="#formula_3">3</ref>) or Eq. ( <ref type="formula" target="#formula_4">4</ref>), we refer to :YÀAb: as the deviation of the linear combination Ab from test sample Y.</p><p>From Eq. ( <ref type="formula" target="#formula_0">1</ref>), we know that every training sample has its own contribution to representing the test sample, and the contribution of the ith training sample is b i A i . Since we know which class each training sample A i ði ¼ 1,2,:::,nÞ is from, we can calculate the sum of the contribution of the training samples from a class. For example, if all the training samples from the kth class are A s . . .. . .A t , then the contribution, in representing the test sample, of the kth class will be g</p><formula xml:id="formula_5">k ¼ b s A s þ::: þ b t A t .</formula><p>The global method regards that the smaller the e k ¼ :YÀg k :</p><p>2 , the greater the contribution of the kth class, and classifies Y into the class that makes the greatest contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Description of our method</head><p>The main steps of our method are as follows:</p><p>Step 1. First exploit a linear combination of all the samples from the set of training samples to represent the test sample and use Eq. (4) to solve the corresponding linear system for obtaining the coefficients. Repeatedly run the following procedure: remove the training sample with the smallest absolute value from the set of training samples and then use a linear combination of the remaining training samples to represent the test sample.</p><p>Step 2. Suppose that a linear combination of the elements of the set of the finally remaining training samples, B 1 ,:::,B m , can approximate the test sample. Solve the coefficients using</p><formula xml:id="formula_6">b 0 ¼ ðB T B þ g 0 IÞ À1 B T y, B ¼ ½B 1 :::B m ð<label>5Þ</label></formula><p>where b 0 ¼ ½b 0 1 :::b 0 m T , g 0 is a small positive constant and I still denotes the identity matrix.</p><p>Step 3. Let B p ,:::,B q be all the training samples from the kth class.</p><p>:yÀ P q j ¼ p b 0 j B j : is referred to as the error of the kth class in representing the test sample. The test sample is classified into the class that has the minimum error.</p><p>In our method, :U: always denotes the l 2 norm. It should be pointed out that if the finally remaining training samples do not include any training sample of a certain class, our method will assigns the maximum error to this class. This ensures that our method works in the expected way. In our method, the twodimensional matrix corresponding to Bb 0 is referred to as reconstruction image of the test sample.</p><p>After the execution of Steps 1-3, we can obtain the classification result of the test sample. In Section 5 we will transform P q j ¼ p b 0 j B j into a matrix that has the same size as the original face image and refer to it as two-dimensional image corresponding to the contribution of the kth class, where B p ,:::,B q denote the training samples from the kth class and are all the elements of the set of the finally remaining training samples. If the contribution of the kth class is the closest to the test sample, we say that this class contributes the most to representing the test sample. It is clear that if the kth class is also the genuine class of the test sample, our method will correctly classify the test sample. Moreover, if the test sample is correctly classified into the kth class, the two-dimensional image corresponding to the contribution of the kth class will also look like the image of the test sample. Thus, as shown in Figs. <ref type="figure" target="#fig_4">3</ref><ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref>, we can intuitively determine whether the test sample will be correctly classified by observing the two-dimensional image corresponding to the contribution of the class that contributes the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis of our method</head><p>In this section we describe the characteristics, rationale, and potential advantages of the proposed method. The basic characteristic of this method is to use a linear combination of a subset of training samples to best represent the test sample and to exploit the representation result to perform classification. Here 'best' means that the error between the obtained linear combination and test sample is almost the smallest.</p><p>As shown in Section 3, the proposed method removes the training sample with the smallest absolute value from the set of training samples. The proposed method indeed can be also viewed as a method that exploits a linear combination of all available training samples to represent test sample and sets the coefficient that has the smallest absolute value to zero. The underlying rationale is as follows: for a coefficient having a very small absolute value, if we do not take into account this coefficient and the corresponding training sample, the error between the test sample and the linear combination will change only little. Our experimental results also show that the training sample corresponding to the coefficient with the smallest absolute value is usually very dissimilar to the test sample in terms of the distance. As a result, by setting a coefficient with the smallest absolute value to zero, our method is able to greatly reduce the adverse influence on classification of the training samples that are very dissimilar to the test sample. By identifying and removing these training samples, the proposed method is able to perform very well.</p><p>Another characteristic of the proposed method is that it is quite different from the popular transform-based face recognition methods such as principal component analysis (PCA) and linear discriminant analysis (LDA). Transform-based face recognition methods first exploit all the training samples to generate transform axes and then transform all the training and test samples into a new space by projecting them onto the transform axes. A linear transform-based face recognition method will transform the samples into a lower-dimensional space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, whereas a nonlinear transform-based face recognition method can transform the samples into a lower-or higher-dimensional space <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. However, our method has no such a conventional transform process. Our method is just a method that bases the classification procedure of the test sample on its representation result and the representation result has the same dimensionality as the original test sample. If we view our method as a ''generalized'' transform method, Bb The first advantage of the proposed method is that it produces the sparseness in a supervised way where the number of the zero coefficients is known or predefined and it is known that to which training samples the zero coefficients are assigned.</p><p>The second advantage of our method is as follows: when it uses the finally remaining training samples to represent and classify the test sample, it will classify the test sample into one of the classes of the finally remaining training samples. Since the number of the classes of the finally remaining training samples is smaller than that of the classes of all the training samples, our method indeed converts the original classification problem into a simpler one that contains fewer classes. However, the global method presented in Section 2 makes a classification decision in the global decision space. In other words, the global method should choose the most proper class label for the test sample from all L candidates (L is the number of the classes of all the training samples). Therefore, the global method is not able to eliminate the adverse influence on classification of some training samples that have long distances to the test sample.</p><p>Another advantage of our work is that it provides a way to integrate an idea similar to ''sparse representation'' with the l 2 norm for face recognition. Our work also somewhat confirms the previous conclusion that the merit of the sparse representation method should be not mainly attributed to the use of the l 1 norm <ref type="bibr" target="#b41">[42]</ref>. As we know, Wright et al. showed that the sparse representation induced by the l 1 norm can lead to a high accuracy for face recognition. However, some recent studies argued that in sparse-representation-based face recognition, it is not true that the ''sparseness'' plays a dominant role and some literatures have shown that the l 1 norm is not necessary <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. For example, Zhang et al. <ref type="bibr" target="#b41">[42]</ref> simply expressed the test sample as a linear combination of all the training samples and exploited only the l 2 norm. The experimental results show that the accuracy of their method is comparable to that of the original sparse representation method proposed by Wright et al. Zhang et al. <ref type="bibr" target="#b41">[42]</ref> considered that the collaborative representation based classification procedure rather than the sparseness indeed plays the dominant role in face recognition. Shi et al. <ref type="bibr" target="#b42">[43]</ref> also pointed out that ''a simple l 2 approach to the face recognition problem is not only significantly more accurate than the state-of-the art approach, it is also more robust, and much faster''. Moreover, Rigamonti has also presented ''We have performed an in-depth analysis of sparse representations in image classification <ref type="bibr" target="#b43">[44]</ref>. Our experimental results suggest that solely enforcing sparsity is not helpful in terms of recognition rate''.</p><p>Our experimental results clearly show that our method outperforms the global method. This means that the sparseness representation is helpful for achieving a higher accuracy. Moreover, this paper shows that the l 1 norm is not the sole means that can obtain sparse representation and the l 2 norm can be integrated with a special scheme such as the one presented in the paper to obtain sparse representation. Our above conclusions are very different from those of both Refs. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>We used the ORL [45], FERET <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b45">47]</ref> and AR [48] face databases to test our method. The face images of these three databases were obtained under the condition of varying pose, facial expression, or lighting. Occluded face images are also included in the AR face database. For the FERET face database, we used only a subset of 1400 images from 200 individuals <ref type="bibr" target="#b36">[37]</ref>. Fig. <ref type="figure" target="#fig_2">1</ref> shows the 14 face images of two subjects in this subset of the FERET database. From the AR face database, we used 3120 face images from 120 subjects. These images were taken in two sessions.</p><p>We resized each face image of the AR database to a 40 by 50 image by using the down-sampling algorithm in <ref type="bibr" target="#b46">[49]</ref>. The face images of the ORL and the FERET databases were also resized using the same algorithm. In the experiments on the AR database, the first four samples per class were used as training samples and the others were used as test samples. The proposed method exploited 200 finally remaining training samples to represent and classify the test sample. In the experiments on the ORL database, five samples per class were used as training samples and the others were used as test samples. 40 finally remaining training samples are used to represent and classify the test sample. For the ORL database, four sets of training samples and test samples were generated. The first set of training samples consists of the 1st, 2nd, 3rd, 4th, 5th samples of each subject. The second, third and fourth training sets are composed of the 1st, 2nd, 3rd, 4th, 6th samples of each subject; the 1st, 2nd, 3rd, 4th, 7th samples of each subject; the 1st, 2nd, 3rd, 4th, 8th samples of each subject, respectively. For each set of training samples, the set of test samples consists of the samples that were not used as training samples. In the experiments on the FERET database, four samples per subject were used as training samples and the others were used as test samples. The proposed method exploited 200 finally remaining training samples to represent and classify the test sample. Four sets of training samples and test samples were also generated from the FERET database. The first set of training samples consists of the 1st, 2nd, 3rd and 4th samples of each subject. The second, third and fourth sets of training samples are composed of the 1st, 2nd, 3rd, 5th samples of each subject; the 1st, 2nd, 3rd, 6th samples of each subject; the 1st, 2nd, 3rd, 7th samples of each subject, respectively. For each training set, the set of test samples also consists of the samples that were not used as training samples.</p><p>Fig. <ref type="figure" target="#fig_3">2</ref> shows the original face images and reconstruction images of the test samples of four subjects from the ORL database. It seems that the reconstruction images can somewhat reduce the difference between the samples of the same subject. Figs. <ref type="figure" target="#fig_4">3</ref><ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref>show a test sample from the ORL face database and the two-dimensional images corresponding to the contributions of the first four classes that contribute the most in representing the test sample. The results of Figs. <ref type="figure" target="#fig_4">3</ref><ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref>were obtained using the global method, the proposed method and the sparse representation   method presented in <ref type="bibr" target="#b22">[23]</ref>. We implemented PCA and LDA as follows. PCA used the eigenvectors corresponding to the first 100 largest eigenvalues of the covariance matrix as transform axes. LDA used the eigenvectors corresponding to the first LÀ1 (L is the number of the classes) largest eigenvalues of the eigen-equation as transform axes. In all of these three figures, the first five samples of each subject were used as training samples and the others were used as test samples. In each of these three figures, (a) always shows the same test sample. (b), (c), (d), and (e) always, respectively, show the two-dimensional images of the contributions of the first four classes that contribute the most in representing the test sample. In other words, (b), (c), (d), and (e) show the twodimensional images corresponding to the four classes whose contributions have the first four smallest errors, respectively. As shown in Section 3, the test sample will be classified into the class whose contribution has the minimum error. Moreover, if this class is the genuine class of the test sample, then the two-dimensional image corresponding to the contribution of this class will look like the image of the test sample. However, if this class is not the genuine class of the test sample, then the two-dimensional image will not look like the image of the test sample. Fig. <ref type="figure" target="#fig_4">3</ref> clearly shows that the global method will erroneously classify the test sample. From Fig. <ref type="figure" target="#fig_5">4</ref>, we know that the proposed method could correctly classify the test sample. Fig. <ref type="figure" target="#fig_6">5</ref> also shows that the sparse representation method proposed in <ref type="bibr" target="#b22">[23]</ref> will also correctly classify the test sample.</p><p>Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref> show the experimental results of the proposed method, the global method and the sparse representation method proposed in <ref type="bibr" target="#b22">[23]</ref>, PCA, and LDA on the ORL and FERET databases, respectively. In these two figures, ''SRM'' still denotes the sparse representation method <ref type="bibr" target="#b22">[23]</ref>. The vertical coordinate shows the classification accuracy. Table <ref type="table">1</ref> shows the classification accuracies of these methods on the AR database. These two figures and Table <ref type="table">1</ref> show that our method classified more accurately than the global method and the sparse representation method <ref type="bibr" target="#b22">[23]</ref>. The experimental results all show that the proposed method is slightly superior to SRM in terms of classification accuracy. From Figs. <ref type="figure" target="#fig_6">5</ref> and<ref type="figure">6</ref> and Table <ref type="table">1</ref>, we can see that:</p><p>(1) The proposed method usually can achieve a better classification accuracy than PCA and LDA, which indicates the effectiveness of the proposed method.  Fig. <ref type="figure">6</ref>. Experimental results of our method, the global method and the sparse representation method proposed in <ref type="bibr" target="#b22">[23]</ref>, PCA and LDA on the ORL database. Fig. <ref type="figure">7</ref>. Experimental results of our method, the global method and the sparse representation method proposed in <ref type="bibr" target="#b22">[23]</ref>, PCA and LDA on the FERET database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Classification accuracies of PCA, LDA, the proposed method, the global method as well as the sparse representation method proposed in <ref type="bibr" target="#b22">[23]</ref> on the AR database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The proposed method</head><p>The global method</p><p>The sparse representation method <ref type="bibr" target="#b22">[23]</ref> PCA LDA 0.699 0.675 0.648 0.500 0.564</p><p>(2) In all the experiments on the three databases, the proposed method achieves a higher recognition accuracy than the SRM and the global methods. This observation indicates that, to use the heuristic strategy in Section 2 rather than the l 1sparse regularizer to obtain sparse representation is the main reason why the proposed method can perform better. Thus we claim that, sparse representation can be used to improve recognition accuracy of face recognition and the proposed heuristic strategy is a feasible way to obtain sparse representation for accurate face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The proposed method uses a heuristic iterative algorithm to obtain supervised sparse representation of the test sample and then exploits the representation result to perform classification. As the proposed method is a supervised sparse method, we not only know how many zero coefficients there are but also know which coefficients are zero. Our paper illustrates the following two points: first, the sparse representation is useful for accurate face recognition. Second, the l 1 norm is not the sole means to obtain sparse representation. The l 2 norm can be integrated with a special scheme such as the one presented in the paper to obtain sparse representation.</p><p>The proposed method has the following rationale: when our method ultimately uses the determined subset of the training samples to represent and classify the test sample, it indeed converts the original classification problem into a simpler classification problem that contains fewer classes and fewer training samples. The experimental results show the feasibility and effectiveness of the proposed method. The results also show that our proposed method outperforms the previous sparse representation method and the global method that exploits all the training samples to represent and classify the test sample.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/neucom Neurocomputing 0925-2312/$ -see front matter &amp; 2011 Elsevier B.V. All rights reserved. doi:10.1016/j.neucom.2011.10.013</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>0 in :yÀBb 0 : can be regarded as the transform result of test sample y (B consists of all the finally remaining training samples and b 0 is the vector consisting of the corresponding coefficients). It should be pointed out that when our method deals with different test samples, both B and b 0 vary with the test sample. However, in conventional transform-based methods, the same transform axes are used to transform all the training and test samples into the new space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Fourteen face images of two subjects from the FERET database.</figDesc><graphic coords="4,124.76,58.64,336.24,104.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The reconstruction images of the test samples of four subjects of the ORL database, obtained using the proposed method. The first and third rows show the original 20 test samples and the second and fourth rows show the reconstruction images of the test samples shown in the first and third rows, respectively.</figDesc><graphic coords="4,124.76,205.09,336.24,180.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A test sample from the ORL face database and the two-dimensional image corresponding to the contributions of the first four classes (subjects) that contribute the most in representing the test sample. The global method is used. (b) Shows the two-dimensional image generated from the class that makes the most contribution. (c), (d) and (e) show the two-dimensional images generated from the classes that make the second, third and fourth most contribution, respectively.</figDesc><graphic coords="4,303.65,434.42,247.32,66.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The test sample from the ORL face database and the two-dimensional image corresponding to the contributions of the first four classes (subjects) that contribute the most in representing the test sample. The proposed method is used. (b) shows the two-dimensional image generated from the class that makes the most contribution. (c), (d) and (e) Show the two-dimensional images generated from the classes that make the second, third and fourth most contribution, respectively.</figDesc><graphic coords="5,44.83,58.64,246.60,67.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The test sample from the ORL face database and the two-dimensional image corresponding to the contributions of the first four classes (subjects) that contribute the most in representing the test sample. The sparse representation method proposed in [23] is used. (b) Shows the two-dimensional image generated from the class that makes the most contribution. (c), (d) and (e) Show the twodimensional images generated from the classes that make the second, third and fourth most contribution, respectively.</figDesc><graphic coords="5,44.83,213.20,246.60,66.72" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This article was partly supported by Key Laboratory of Network Oriented Intelligent Computation, Program for New Century Excellent Talents in University (No. NCET-08-0156), The Fundamental Research Funds for the Central Universities (Grant no.HIT.NSRIF. 2009130) and the National Nature Science Foundations of China under Grant nos. 61071179, 60902099, 61073125, 61020106004, 61173086, and 60873140.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human and machine recognition of faces: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sirohey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="705" to="740" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-dimensional PCA: a new approach to appearance-based face representation and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An approach for directly extracting features from matrix data and its application in face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A feature extraction method for use with bimodal biometrics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-dimensional discriminant transform for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1125" to="1129" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modified linear discriminant analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="443" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face recognition using discriminant locality preserving projections based on maximum margin criterion</title>
		<author>
			<persName><forename type="first">G.-F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3572" to="2579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature reduction via generalized uncorrelated linear discriminant analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janardan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1312" to="1322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advanced pattern recognition technologies with applications to biometrics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Inf. Sci. Ref</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face recognition using a multi-manifold discriminant analysis method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="page" from="527" to="530" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Color space normalization: enhancing the discriminating power of color spaces for face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1454" to="1466" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Max-min distance analysis by using sequential SDP relaxation for dimension reduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.189</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminant feature extraction based on center distance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1249" to="1252" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geometric mean for subspace selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="274" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Patch alignment for dimensionality reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1299" to="1313" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-negative patch alignment framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1218" to="1230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Manifold elastic net: a unified framework for sparse dimension reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl. Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="371" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pietik äinen: Manifold learning for video-to-video face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COST 2101/2102 Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
		<title level="m">Putting local features on a manifold, CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1743" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bimodal biometrics based on a reperesentation and recognition approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.3554740</idno>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="37202" to="037202" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local face sketch synthesis learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1921" to="1930" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Invariant image watermarking based on local feature regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Cyberworlds 2008</title>
		<meeting>the International Conference on Cyberworlds 2008</meeting>
		<imprint>
			<publisher>CW</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative locality alignment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision (ECCV (1))</title>
		<meeting>the 10th European Conference on Computer Vision (ECCV (1))</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="725" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Biased discriminant Euclidean embedding for content-based Image retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="545" to="554" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local coordinates alignment (LCA): a novel manifold learning approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Anal. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="667" to="690" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using local dependencies within batches to improve large margin classifiers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="183" to="206" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal local basis: a reinforcement learning approach for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-008-0161-5</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="191" to="204" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved system for object detection and star/ galaxy classification via local subspace analysis</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="437" to="451" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1027" to="1061" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One improvement to two-dimensional locality preserving projection method for use with face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="245" to="249" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local linear discriminant analysis framework using sample neighbors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1119" to="1132" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A two-phase test sample sparse representation method for use with face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kernel full-space biased discriminant analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<biblScope unit="page" from="1287" to="1290" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A method for speeding up feature extraction based on KPCA</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1056" to="1061" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sch ölkopf, An introduction to kernel-based learning algorithms</title>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Network</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: which helps face recognition</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is face recognition really a compressive sensing problem</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are sparse representations really relevant for image classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/iad/humanid/feret/feret_master.htmlS" />
		<title level="m">The Facial Recognition Technology (FERET) Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Down-sampling face images and low-resolution face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Innovative Computing, Information and Control</title>
		<meeting>the Third International Conference on Innovative Computing, Information and Control</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">He received his Ph.D. degree in Pattern recognition and Intelligence System at NUST(China) in 2005. Now he works at Shenzhen graduate school, Harbin Institute of Technology. His current interests include feature extraction, biometrics, face recognition, machine learning, image processing and video analysis</title>
	</analytic>
	<monogr>
		<title level="m">Zizhu Fan received the M.S. degree in computer science from Hefei University of Technology</title>
		<editor>
			<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Asia</forename></persName>
		</editor>
		<meeting><address><addrLine>China; Hefei, China</addrLine></address></meeting>
		<imprint>
			<publisher>Harbin Institute of Technology (HIT</publisher>
			<date type="published" when="1972">1972. 1994. 1997. July to December 2004. November 2005 to August 2006. July 2007 to February 2008. August 2009 to February 2010. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Harbin Institute of Technology, Harbin, China ; Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>His current interests include pattern recognition and machine learning. He has published more than 10 journal papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
