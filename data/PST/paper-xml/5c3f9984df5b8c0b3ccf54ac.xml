<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Micro-expression recognition based on 3D flow convolutional neural network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science and Technology</orgName>
								<orgName type="institution">Wenzhou University</orgName>
								<address>
									<postCode>325035</postCode>
									<settlement>Wenzhou, Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yandan</forename><surname>Wang</surname></persName>
							<email>yandan.wang@wzu.edu.cn</email>
							<idno type="ORCID">0000-0002-0144-010X</idno>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science and Technology</orgName>
								<orgName type="institution">Wenzhou University</orgName>
								<address>
									<postCode>325035</postCode>
									<settlement>Wenzhou, Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>See</surname></persName>
							<email>johnsee@mmu.edu.my</email>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Computing and Informatics</orgName>
								<orgName type="institution">Multimedia University</orgName>
								<address>
									<addrLine>Persiaran Multimedia, 63100 Cyberjaya</addrLine>
									<settlement>Selangor</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computer Science and Technology</orgName>
								<orgName type="institution">Wenzhou University</orgName>
								<address>
									<postCode>325035</postCode>
									<settlement>Wenzhou, Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Yandan Wang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Micro-expression recognition based on 3D flow convolutional neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7630E54B8CA8A33664CA6A93E77DC124</idno>
					<idno type="DOI">10.1007/s10044-018-0757-5</idno>
					<note type="submission">Received: 5 June 2017 / Accepted: 29 October 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial micro-expressions</term>
					<term>Micro-expression recognition</term>
					<term>3D CNN</term>
					<term>Optical flow</term>
					<term>CASME</term>
					<term>SMIC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Micro-expression recognition (MER) is a growing field of research which is currently in its early stage of development. Unlike conventional macro-expressions, micro-expressions occur at a very short duration and are elicited in a spontaneous manner from emotional stimuli. While existing methods for solving MER are largely non-deep-learning-based methods, deep convolutional neural network (CNN) has shown to work very well on such as face recognition, facial expression recognition, and action recognition. In this article, we propose applying the 3D flow-based CNNs model for video-based micro-expression recognition, which extracts deeply learned features that are able to characterize fine motion flow arising from minute facial movements. Results from comprehensive experiments on three benchmark datasets-SMIC, CASME/CASME II, showed a marked improvement over state-of-the-art methods, hence proving the effectiveness of our fairly easy CNN model as the deep learning benchmark for facial MER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In pattern recognition research, deep learning, from the early efforts of Hinton <ref type="bibr" target="#b8">[9]</ref>, has penetrated various domains such as machine vision, speech recognition and natural language processing due to its superior self-learning ability. Very recently, these techniques have found their ways to the task of recognizing facial expressions, achieving a good measure of success <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. In our work, we introduce deep learning to facial micro-expression recognition (MER) with the efficient CNN as the benchmark due to its simplicity and effectiveness in most applications.</p><p>Micro-expressions are different from normal facial expressions (or macro-expressions in this case), in terms of its brief duration and spontaneous occurrence, a form of involuntary response toward an emotional stimuli. It was first discovered by Haggard and Isaacs <ref type="bibr" target="#b6">[7]</ref> as a kind of 'micro-momentary' expression, or a nonverbal communication during psychotherapy, reported again by <ref type="bibr" target="#b2">[3]</ref> 3 years later. Based on Ekman's account, they examined a taped video, a talk between a patient and a psychologist. Despite the patient portraying a happiness expression throughout the talk, they found that the patient showed a quick subtle and abnormal expression in her face (this was done by examining the video frame by frame with naked eyes). The patient also confessed her tendency for suicide under the psychologist's cross-examination.</p><p>Micro-expressions also occur in high-stakes situations, where people have something to lose or gain. The importance of micro-expression study is useful in many areas, e.g., clinical psychology, criminal interrogation, public security. In clinical psychology, it can help the psychologist diagnose whether the patient is concealing the truth; in criminal interrogation, it can help the policemen decide whether the suspects are lying; for public security, it can help identify potential dangerous persons at public areas such as airport and railway station. Therefore, to develop an automatic micro-expression detection and recognition system is much desired.</p><p>CNN is one of the most popular deep learning methods and widely used especially on image classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Furthermore in <ref type="bibr" target="#b1">[2]</ref>, the authors presented CNN for facial expression recognition with a fairly small dataset without overfit issues, and in <ref type="bibr" target="#b13">[14]</ref>, the authors presented CNN for human action recognition with gray images, optical flow field and gradient as input data. Based on those superior works of the state of the art, we will give a maiden attempt of introducing a 3D flow-based spatiotemporal CNN into our work and some popular CNNs improvement techniques will also be introduced to boost the results. We believe that more and more deep learning work will be done in microexpression recognition. In non-deep learning-based work, LBP-TOP was applied as the benchmark, while we wish our simple CNN model will be the benchmark for the deep learning work in micro-expression recognition.</p><p>The rest of the article is organized as follows: First, we will give a brief review of micro-expression recognition and convolutional neural network in related work. Next, we will detail our methods. Experiments will be reported in next section. Finally, we will conclude and discuss our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since micro-expression recognition is still in its early stage, there are not many published works every year. Therefore, we present a concise review of related works on microexpression recognition, and convolutional neural networks related to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Facial micro-expression recognition</head><p>Facial micro-expression recognition has been a challenging research topic since 2009. Although it has been a decade, there are still not many related works. In the early days, the lack of spontaneous facial micro-expression data is a stumbling block. However, with the joint effort of psychologists and computer scientists, three datasets have been created recently, all elicited in a spontaneous way: SMIC <ref type="bibr" target="#b18">[19]</ref> by University of Oulu, and CASME <ref type="bibr" target="#b28">[29]</ref>/CASME II <ref type="bibr" target="#b27">[28]</ref> by Chinese Academy of Sciences.</p><p>The most comprehensive and largest dataset, CASME II, established the baseline method using Local Binary Pattern-Three Orthogonal Planes (LBP-TOP) for spatiotemporal feature extraction and support vector machine (SVM) for classification. Later works followed the use of LBP-TOP but using different base features based on optical strain <ref type="bibr" target="#b19">[20]</ref> and monogenic signals <ref type="bibr" target="#b20">[21]</ref>. These works enhance the ability of extracting LBP-TOP features by processing more meaningful data. In the work of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, the authors reconstructed two spatiotemporal methods, LBP-SIP and LBP-MOP, both derived from the concept of LBP-TOP. However, LBP-SIP is not robust enough when the data are processed by other methods such as noise filtering or frame interpolation. LBP-MOP is frame interpolation independent and thus only works well when a suitable smoothing filter is applied to reduce the influence of noise. Wang et al. <ref type="bibr" target="#b23">[24]</ref> introduced video motion signal magnification to preprocess video signals and used LBP-TOP for feature extraction that get the results significantly enhanced. However, the method is not robust as the noise could also be amplified when the subtle facial expression get magnified. Moreover, the Leave-One-Video-Out (LOVO) cross-validation method lead by the baseline has been discarded and replaced by the Leave-One-Subject-Out (LOSO) cross-validation method, due to the subject dependency policy of the LOVO caused the unfair and high recognition results. Goswami et al. <ref type="bibr" target="#b5">[6]</ref> proposed another variant called Local Ordinal Contrast Pattern-Three Orthogonal Plane (LOCP-TOP) which was found to be slightly better than the rest <ref type="bibr" target="#b10">[11]</ref>.</p><p>Optical flow can play a significant role in computing how expression changes in face across time. Liong et al. <ref type="bibr" target="#b19">[20]</ref> improved the recognition performance by assigning weights computed from the optical strain (a derivative of the optical flow field) to each local frame block. A more recent work by <ref type="bibr" target="#b26">[27]</ref> also used principal optical flow directions to construct facial dynamic maps. In short, these previous works manually construct and design the descriptors and representations based on researchers' analysis, which may not learn the minute spatiotemporal changes in micro-expressions in the most natural manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Brief convolutional neural network frameworks</head><p>After a decade since Hinton's introductory idea of learning deeper neural networks <ref type="bibr" target="#b8">[9]</ref>, the deep convolutional neural network (CNN), one of the more popular architectures, has gained significant interest in fields such as image classification, object detection and speech recognition, after its first successful application to handwritten digits recognition <ref type="bibr" target="#b17">[18]</ref>. Since there are tones of CNN works, we only mention works that is related to our model only. Our model starts with AlexNet 7 layers, then improved by techniques in other works, and derived our model with 12 layers based on our intensive experiments.</p><p>Krizhevky et al. <ref type="bibr" target="#b15">[16]</ref> proposed AlexNet by using CNN and produced the best result in the ImageNet competition (ILSVRC) in 2012. The AlexNet has 7 layers, which is deeper than the LeNet-5 <ref type="bibr" target="#b17">[18]</ref>. In their paper, the authors introduced ReLU and dropout techniques. ReLU is used to accelerate convergence and solve the problem of vanishing gradients, while dropout is used to reduce the overfitting problem. Since then, the advancement of GPUs has rapidly propelled the wide usage of deep CNN. In 2014, the top two entries to the ILSVRC went to GoogLeNet <ref type="bibr" target="#b22">[23]</ref> and VggNet <ref type="bibr" target="#b21">[22]</ref>, respectively. Both models exploited the effectiveness of increasing depth in a CNN architecture with 'deeper' models. The 19-layer VggNet used smaller convolution kernels than its predecessors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> to learn more detailed features. GoogLeNet presented a wide cascaded topology called inception that uses 1 × 1 convolution kernels to resolve the concentration of neurons in a single region. Recently, the far-reaching advantages of CNN have seen its penetration into facial expression recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. Byeon et al. <ref type="bibr" target="#b1">[2]</ref> proposed a five-layer CNN, extending convolution operations to 3D convolutions for video sequences. Subsequent works by <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b12">[13]</ref> proposed models that jointly learn from two types of data, i.e., shape/geometry and appearance; the former was used for expression recognition, while the latter was used to identify the occurrence of facial action units (AUs).</p><p>In this article, we proposed an application of 3D flowbased CNN (3D-FCNN) model, which exploits the advantages of state-of-the art work as the attempt at learning deeper models for micro-expression recognition. Motivated by the VggNet <ref type="bibr" target="#b21">[22]</ref>, we used a small 3D convolution kernel of 3 × 3 × 3 to better represent the minute spatial changes at each local region. In contrast to two recent 3D CNN models proposed by <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b13">[14]</ref>, we introduce optical flow (dynamic information) together with standard grayscale frames (appearance information) as input data in a 12-layer deep network (5 pairs of convolution and pooling layers, a fully connected layer and an output softmax layer). Our final model is derived based on our extensive experiments.</p><p>To avoid overfitting our model in a deep network, we utilize batch normalization (BN) and dropout techniques in our model. Furthermore, we applied rectified linear unit (ReLU) activation functions <ref type="bibr" target="#b4">[5]</ref> to tackle the problem of vanishing gradients and for faster training, while zero-padding (ZP) is introduced to avoid information loss caused by the convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we will present our proposed 3D flow-based CNN architecture in greater detail followed by network improvement techniques we applied that we utilize in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed 3D flow-based CNN architecture</head><p>The architecture of our 3D flow-based CNN is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> with the number of feature cubes and feature maps. We design a 12-layer architecture, which consists of inputs from three 'data streams,' i.e., grayscale frame sequence or In the 3D convolutional layers, a 3D convolution kernel of size m x x m y x m t is introduced to extract spatiotemporal feature information. For instance, a 3 × 3 × 3 convolutional kernel sets the temporal dimension as 3, which means it handles adjoining three frames with spatial window sized 3 × 3 . Figure <ref type="figure">2</ref> shows an example how the 3D convolution works in a data cube with the video of 6 frames. The labeled 3D convolution kernel with 3 × 3 × 3 is performing the convolu- tion operations on every three adjoining images (either blue or red, sliding from first three images to last three images) with a spatial window of size 3 × 3 sliding from left to right and top to bottom. Let W be the convolution kernel matrix, I i as the ith image and f i the ith convolution output. Since con- volving every three I i yields a single output f, the 3D output size is</p><formula xml:id="formula_0">[h -m x + 1, w -m y + 1, l -m t + 1]</formula><p>, where h, w and l are the height, width and length of the I sequence, respectively. Based on the example in Fig. <ref type="figure">2</ref>, six I video frames yield four f output maps after the 3D convolution operation:</p><p>In order to extract features with a finer coverage, we utilize more than one 3D convolution kernels. Formally, the pixel at (x, y, z) on the jth feature map of the ith layer is given as: where w is the weight of the 3D convolution kernel, v on the RHS of equation is the value of previous layer and v on the LHS of the equation is the value after convolution and ReLU. All the computations in Eq. 2 are in three dimensions, while the ReLU function adds the nonlinearity with better acceleration toward convergence.</p><p>3D max-pooling layer is applied to 3D spatiotemporal data in the same manner as 3D convolution by considering all three axes, X, Y and T. The pooling process summarizes the outputs of neighboring groups of neurons in the same feature map to reduce its dimensionality for the next layer. However, because of the short length of T dimension (in relative to the network depth), we only apply 3D spatiotemporal max-pooling in the last subsampling layer, while standard 2D spatial max-pooling is applied to all other subsampling layers. By experiments, we found that max-pooling (1)</p><formula xml:id="formula_1">f i = I i * W 1 + I i+1 * W 2 + I i+2 * W 3 (2) v xyz ij = relu b ij + ∑ m P i -1 ∑ p=0 Q i -1 ∑ q=0 R i -1 ∑ r=0 w pqr ijm v (x+p)(y+q)(z+r) (i-1)m</formula><p>Fig. <ref type="figure">2</ref> The illustration of 3D convolutional operation performs better than average-pooling on the task of microexpression recognition, as similarly concluded in other problem domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. Hence, we opt for max-pooling in our architecture.</p><p>The ReLU and BN techniques are applied to each convolutional layer before going to the following pooling layer, while zero-padding is applied to each pooling layer (except S10) before going to its subsequent convolutional layer. The size of all convolutional kernels is set to 3 × 3 × 3 . The pooling size is set to 2 × 2 × 1 except the pooling in layer S10 ( 2 × 2 × 2 ). The output from S10 for all three data cube streams is flattened and concatenated, reducing the final dimensionality to 128 × 3.</p><p>The output layer takes the size corresponding to the number of classes, which is derived by the softmax classifier, where j are the weights and k = {5, 3} represents the num- ber of classes on the CASME/CASME II and SMIC datasets, respectively. The predicted class for input x (i) is the one with the largest probability h (x (i) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network improvements</head><p>We apply zero-padding to the inputs of all pooling layers (except S10 which is connected to a FC). Zeros are padded at the edge of input volumes to control the desired spatial size of the output volumes. This prevents the convolutional operation from reducing the dimensionality of feature maps as what VggNet does, since we use pooling to reduce the dimensionality. The size of 1 × 1 × 1 zero-padding is applied, except layer S8 uses size of 1 × 1 × 2 . This way, we can increase the depth of our architecture to learn more specific subtle spatial changes that occur in micro-expressions.</p><p>Batch normalization (BN) <ref type="bibr" target="#b11">[12]</ref> is popularly used for CNN training as it helps accelerate the rate of convergence even with small learning rates. We introduce BN to the convolutional layers of our 3D CNN, after the ReLU activations. This is a characteristic not found yet in other existing 3D CNN models <ref type="bibr" target="#b13">[14]</ref>. In BN, to normalize input (in a layer) = {x (1) , … , x (d) } , its kth dimension can be normalized as Eq. 4:</p><p>where E and Var represent the mean and variance of the batch. If we normalize inputs to a nonlinear activation</p><formula xml:id="formula_2">(3) h (x (i) ) = 1 ∑ k-1 j=0 e T j x (i) ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ e T 0 x (i) e T 1 x (i) ⋮ e T k-1 x (i) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ (4) x(k) = x (k) -E[x (k) ] √ Var[x (k) ]</formula><p>function (e.g., sigmoid), then the output would only be bounded to the linear part. Thus, we can scale and shift the normalized input, where (k) and (k) are the scaling and shifting parameters, respectively, that need to be learnt. Following <ref type="bibr" target="#b22">[23]</ref>, we use mean and variance of the batch for normalization during training, while the mean and variance of the dataset are used for testing. Dropout fine-tuning is a regularization technique that helps reduce the overfitting problem in neural networks by preventing complex coadaptations on training data <ref type="bibr" target="#b9">[10]</ref> caused by fully connected layers. Given a dropout rate p, which is set to 0.5 in our NNs, 50% units will be retained, while another 50% will be omitted and re-insert into the networks with initialized weights. Simply, 'dropout' just randomly ignores some neurons. However in testing stage, the output of each neuron will be weighted by the factor 1 -p (the retain rate) to keep it the same effect as dropout made in training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we briefly describe the datasets used and present the experimental results with analysis and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>SMIC <ref type="bibr" target="#b18">[19]</ref> consists of micro-and macro-expressions, and we select only the micro-expression samples. There are a total of 164 videos from 16 subjects, capturing spontaneous micro-expressions from 3 classes (positive, negative and surprise). The samples were recorded with a 100 fps highspeed camera.</p><p>CASME <ref type="bibr" target="#b28">[29]</ref> was the first dataset built by Chinese Academy of Sciences (CAS). It consists of 180 videos from 19 subjects, and it was recorded at 60 fps. A total of 35 participants (13 females, 22 males) were recruited with an average age of 22.03 years ( = 1.60 ) in their study. It consists of 8 identified micro-expressions, i.e., amusement, sadness, disgust, surprise, contempt, fear, repression and tense. The top five classes (tense, disgust, happiness, surprise and repression) that had the most samples were selected for experiments (the rest have too few samples for proper evaluation)</p><p>CASME II <ref type="bibr" target="#b27">[28]</ref> was also built by CAS and is publicly available for research. It is composed of 247 videos from 26 subjects with 9 classes (happiness, surprise, disgust, fear, sadness, anger, repression, tense and negative). The top five classes (tense, disgust, happiness, surprise and repression) that had the most samples were selected for experiments, while the other four were discarded due to lack of samples.</p><p>(5) y (k) = 𝛾 (k) x(k) + 𝛽 (k)  The dataset was recorded using a 200-fps high-speed camera. Figure <ref type="figure" target="#fig_1">3</ref> presents frames of a happiness micro-expression video clip. Unlike the macro-expression that the facial movement is obvious and observable, the muscle movements of micro-expression are subtle. During the entire video, we cannot see obvious changes by our naked eyes, which brings big challenges analyzing the discriminant features manually, hence enhancing even a minor recognition accuracy is a difficult task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter settings</head><p>In order to keep evaluation comparisons fair, our implementations follow the same data preprocess and evaluation protocol used in previous works: <ref type="bibr" target="#b0">(1)</ref> Frames are resized into 226 × 226 . (2) The length of sequences (number of frames) are interpolated to 10 and 15 using temporal interpolation model (TIM) <ref type="bibr" target="#b31">[32]</ref>. (3) Leave-One-Subject-Out (LOSO) cross-validation <ref type="bibr" target="#b16">[17]</ref> is used in our evaluation to prevent subject bias during learning.</p><p>In our proposed 3D-FCNN approach, input using only grayscale sequences is denoted as 'G,' while the same input with 5 × 5 block partitions is denoted as 'G-5x5.' In a similar fashion to block partitioning used in LBP-TOP <ref type="bibr" target="#b30">[31]</ref>, each frame is partitioned into 5 × 5 blocks. In our architecture, all blocks are concatenated in the input layer before passing through the network. With additional optical flow information, we also tested with all three data streams-grayscale frame sequences, and the optical flow sequences in the xand y-directions (input sequence is less by one), denoted by 'G,XF,YF.' Similarly, the 'G-5x5,XF,YF' has the same inputs as 'G,XF,YF' except that each grayscale frame is partitioned into 5 × 5 blocks. All these different input configura- tions used the same 3D-FCNN architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>, except for the input layer.</p><p>The model is composed of five 3D convolutional layers with kernel size 3 × 3 × 3 . There are four max-pooling lay- ers of size 2 × 2 × 1 while the final max-pooling layer is of size 2 × 2 × 2 . Henceforth, the zero-padding size is set to 1 × 1 × 1 , except the last padding of 1 × 1 × 2.</p><p>The final extracted features are flattened and concatenated for classification by the last softmax layer based on LOSO cross-validation. Learning rate is set to 0.01, decay is set to 5 × 10 -4 , and weights are randomly initialized as proposed in <ref type="bibr" target="#b3">[4]</ref>. As comparison with traditional descriptors, we implement the LBP-TOP, LOCP-TOP and LBP-SIP methods on the three datasets under same data preprocess and evaluation protocols, which frames are also interpolated to length of 10 and 15, respectively, and partitioned into 5 × 5 blocks for feature extraction before by using SVM with LOSO cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results of the proposed 3D-FCNN method on the SMIC, CASME and CASME II datasets, after interpolating to 10 and 15 frames using temporal interpolation model (TIM) <ref type="bibr" target="#b31">[32]</ref>. TIM was used in the baseline of SMIC <ref type="bibr" target="#b18">[19]</ref> and adopted as well by many later works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>.  Overall, we find that interpolating to 10 frames (TIM10) is superior to that of 15 frames (TIM15), while flow-based approach performs much better than using grayscale frames only, which demonstrates the importance of learning dynamic information in a CNN. Meanwhile, by further block partitioning the grayscale frames with the flow information, we observe a generally better performance except on the CASME II. We hypothesize that this might be caused by too little changes at local block level in the case of high-speed capture (200 fps). Nevertheless, the results show the effectiveness of our proposed 3D-FCNN model for MER compared to that using standard image appearance information.</p><p>We take one video from sub01 of CASME II dataset and visualize the feature maps of each convolutional layer in Fig. <ref type="figure" target="#fig_2">4</ref>, presenting 3 sample feature maps from each data cube channel. As the depth increases (C1 to C5), we can clearly see from the 'gray' channel that the feature map starts losing the irrelative visual information. Gradually, the network begins to learn specific subtle spatial changes, focusing on vital regions of interests, i.e., eyes and mouth regions (see C5). Likewise, the 'x-flow' and 'y-flow' channels also depict more details in the deeper layers, preserving finer facial details.</p><p>The best flow-based results on the SMIC, CASME and CASME II datasets (from Table <ref type="table" target="#tab_0">1</ref>) are chosen for further comparisons with other state-of-the-art methods in Table <ref type="table" target="#tab_1">2</ref>. For fair benchmarking, we re-implemented these methods applying the same preprocess: Length of the video sequence is interpolated to 10 frames and spatially resized to 226x226 pixels. From the table, our proposed 3D-FCNN model outperforms the other state-of-the-art methods: LBP-TOP, LOCP-TOP and LBP-SIP on all evaluated datasets (SMIC, CASME, CASME II), which are 6.1%, 5.55% and 2.43% better compared to LBP-TOP on three datasets, respectively, 9.76%, 4.44%, 2.43% better compared to LOCP-TOP, respectively, 12.81%, 8.33%, 5.26% better than that of LBP-SIP, respectively. Overall, we can see that the improvements on CASME II are not obvious but still promising, while the improvements on other two datasets are significant and pleasant. LBP-SIP performed slightly worse than  LOCP-TOP and LBP-TOP possibly due to the effects of frame interpolation, while the performance of LBP-TOP and LOCP-TOP is somewhat at a similar level. Micro-expression recognition is a fairly difficult task that the subtle changes in face cannot be observed and analyzed by our naked eyes, thus enhancing the recognition accuracy is an extremely difficult challenge. However, from Table <ref type="table" target="#tab_0">1</ref> our proposed method performs best, and the results are still promising.</p><p>In comparison with other non-TIM-based methods (i2D, OSW and FDM), our method still performs exceedingly well, particularly in the CASME and CASME II datasets that are around 12% and 17% better, respectively. Despite the need for the interpolation operation (warranted in our case to reduce the complexity of the input layer and keep the data size consistency), our 3D-FCNN method still shows its ability to self-learn its own features without needing to specific features design or heuristic rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and discussions</head><p>In this article, we demonstrated the effectiveness of our proposed 3D flow-based CNN (3D-FCNN) compared to the state-of-the-art traditional approaches for facial microexpression recognition (MER). Our experiments cover an extensive evaluation on three existing benchmark MER datasets: SMIC, CASME, and CASME II. Although CNN naturally requires a longer training time, we show that it can learn minute spatiotemporal changes in a deep neural network, with the aid of essential dynamic information from optical flow. With the increased leveraging of GPUs, the complexity of training deep CNN is no longer a key issue. Our proposed 12-layer model yields promising results, but there are further problems to explore in future. Among many, we intend to investigate how deeply learned action units (AUs) can be jointly propagated in our architecture as a separate new data stream. Also, a more robust keyframe selection method may be able to choose better frames than the current temporal interpolation scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Architecture of the proposed 3D flow-based CNN. It consists of 3 data stream sub-networks that each sub-network has input data with one tube channel only: grayscale frame sequences, 'gray' (red cube), the vertical and horizontal optical flow, 'flow_x' and 'flow_y' (blue and green cubes). The number of feature cubes is computed and is shown on the top notes with k@m × n, where k is the number of feature cubes, m × n is the size of feature map. C is the convolution layer, and S is the pooling layer. The number of convolutional</figDesc><graphic coords="3,84.64,441.33,426.00,180.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig.3The illustration of a video frames in happy micro-expression</figDesc><graphic coords="6,53.80,57.76,232.56,264.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref> The feature map examples produced at each convolutional layer of sub-network stream with corresponding input cube channel respectively</figDesc><graphic coords="7,85.06,418.53,425.16,263.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,183.41,397.10,360.84,318.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The experiment results of the proposed 3D-FCNN method with various input data</figDesc><table><row><cell>Methods</cell><cell>Accuracy (%)</cell><cell></cell></row><row><cell>SMIC</cell><cell>TIM10</cell><cell>TIM15</cell></row><row><cell>3D-FCNN (G)</cell><cell>36.59</cell><cell>42.07</cell></row><row><cell>3D-FCNN (G-5 × 5)</cell><cell>38.41</cell><cell>39.02</cell></row><row><cell>3D-FCNN (G, XF, YF)</cell><cell>47.56</cell><cell>26.83</cell></row><row><cell>3D-FCNN (G-5 × 5,XF,YF)</cell><cell>55.49</cell><cell>55.49</cell></row><row><cell>CASME</cell><cell></cell><cell></cell></row><row><cell>3D-FCNN (G)</cell><cell>31.67</cell><cell>33.89</cell></row><row><cell>3D-FCNN (G-5 × 5)</cell><cell>36.11</cell><cell>27.22</cell></row><row><cell>3D-FCNN (G, XF, YF)</cell><cell>48.89</cell><cell>42.22</cell></row><row><cell>3D-FCNN (G-5x5, XF, YF)</cell><cell>54.44</cell><cell>43.89</cell></row><row><cell>CASME II</cell><cell></cell><cell></cell></row><row><cell>3D-FCNN (G)</cell><cell>38.06</cell><cell>37.65</cell></row><row><cell>3D-FCNN (G-5 × 5)</cell><cell>40.10</cell><cell>41.30</cell></row><row><cell>3D-FCNN (G, XF, YF)</cell><cell>59.11</cell><cell>52.23</cell></row><row><cell>3D-FCNN (G-5 × 5 , XF, YF)</cell><cell>55.06</cell><cell>50.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The comparison between the proposed method and recent state-of-the-art methods</figDesc><table><row><cell>Method/accuracy (%)</cell><cell>SMIC</cell><cell>CASME</cell><cell>CASME II</cell></row><row><cell>LBP-TOP* [19, 28]</cell><cell>49.39</cell><cell>48.89</cell><cell>56.68</cell></row><row><cell>LOCP-TOP* [6]</cell><cell>45.73</cell><cell>50.00</cell><cell>56.68</cell></row><row><cell>LBP-SIP* [26]</cell><cell>42.68</cell><cell>46.11</cell><cell>53.85</cell></row><row><cell>i2D [21]</cell><cell>43.29</cell><cell>N/A</cell><cell>45.53</cell></row><row><cell>OSW [20]</cell><cell>53.05</cell><cell>N/A</cell><cell>41.70</cell></row><row><cell>FDM [27]</cell><cell>54.88</cell><cell>42.02</cell><cell>41.96</cell></row><row><cell>Proposed method*</cell><cell>55.49</cell><cell>54.44</cell><cell>59.11</cell></row><row><cell cols="4">*These methods are based on our implementation with TIM10</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA Corporation for the donation of a Quadro K5200 GPU used in this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is funded in part by the Zhejiang Provincial Natural Science Foundation of China (Grants Nos. LQ17F020002, R1110261) and the National Science Foundation of China (Grants Nos. 61572367, 61272018)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facial expression recognition using 3d convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Adv Comput Sci Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="107" to="112" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonverbal leakage and clues to deception</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatry</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="106" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local ordinal contrast pattern histograms for spatiotemporal, lip-based speaker authentication</title>
		<author>
			<persName><forename type="first">B</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><forename type="middle">C H</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 fourth IEEE international conference on biometrics: theory applications and systems (BTAS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Haggard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Isaacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods of research in psychotherapy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1966">1966</date>
			<biblScope unit="page" from="154" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial microexpression recognition using spatiotemporal local binary pattern with integral projection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piteikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spontaneous subtle expression recognition: imbalanced databases and solutions</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">See</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="33" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">spontaneous micro-expression database: Inducement, collection and baseline</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Subtle expression recognition using optical strain weighted features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See</forename><forename type="middle">J</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="644" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intrinsic two-dimensional local structures for micro-expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Phari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rcw</forename></persName>
		</author>
		<author>
			<persName><forename type="first">See</forename><forename type="middle">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech, and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1851" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective recognition of facial micro-expressions with video motion magnification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rcw</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rahulamathavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-016-4079-6</idno>
		<ptr target="https://doi.org/10.1007/s11042-016-4079-6" />
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal local binary patterns for spontaneous facial micro-expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rcw</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="124" to="674" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LBP with six intersection points: reducing redundant information in lbp-top for microexpression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rcw</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="525" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microexpression identification and categorization using a facial dynamics map</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Affect Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="267" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Casme II: an improved spontaneous micro-expression database and the baseline evaluation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86041</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Casme database: a dataset of spontaneous micro-expressions collected from neutralized faces</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An image-based visual speech animation system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circ Syst Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1420" to="1432" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
