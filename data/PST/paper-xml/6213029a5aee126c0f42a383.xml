<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probing Pretrained Models of Source Code</title>
				<funder ref="#_RPDd2Gj">
					<orgName type="full">Russian Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Troshin</surname></persName>
							<email>stroshin@hse.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">HSE University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadezhda</forename><surname>Chirkova</surname></persName>
							<email>nchirkova@hse.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">HSE University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probing Pretrained Models of Source Code</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models are widely used for solving challenging code processing tasks, such as code generation or code summarization. Traditionally, a specific model architecture was carefully built to solve a particular code processing task. However, recently general pretrained models such as CodeBERT or CodeT5 have been shown to outperform task-specific models in many applications. While pretrained models are known to learn complex patterns from data, they may fail to understand some properties of source code. To test diverse aspects of code understanding, we introduce a set of diagnosting probing tasks. We show that pretrained models of code indeed contain information about code syntactic structure and correctness, the notions of identifiers, data flow and namespaces, and natural language naming. We also investigate how probing results are affected by using code-specific pretraining objectives, varying the model size, or finetuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning and especially Natural Language Processing (NLP) methods have been widely and successfully adopted to process source code. Example tasks include code generation <ref type="bibr" target="#b4">(Allamanis et al., 2015;</ref><ref type="bibr" target="#b8">Chen et al., 2021)</ref> where the task is usually formulated as to produce a code of a function given the natural description; code translation <ref type="bibr" target="#b25">(Nguyen et al., 2013;</ref><ref type="bibr" target="#b28">Roziere et al., 2020)</ref> where the model needs to translate from one programming language to another; and code summarization <ref type="bibr" target="#b15">(Haiduc et al., 2010;</ref><ref type="bibr">Alex et al., 2020)</ref> where the task is to produce natural language description (NL) for a given code snippet. Deep learning is also widely used in discriminative tasks, such as automated bug search and repair <ref type="bibr" target="#b16">(Hellendoorn et al., 2020)</ref>.</p><p>In recent years, the focus has shifted from developing task-specific models incorporating prior knowledge about the task, to relying on general pre-trained models of code such as CodeBERT <ref type="bibr" target="#b12">(Feng et al., 2020)</ref> or CodeT5 <ref type="bibr" target="#b29">(Wang et al., 2021)</ref>. These models, once pretrained, can be finetuned on the downstream tasks with a little additional cost, surpassing task-specific models. While the performance of the models is high on a wide range of downstream tasks <ref type="bibr" target="#b24">(Lu et al., 2021)</ref>, the boundary between what the models know and where they fail remains hidden behind the complexity of the downstream tasks. The lack of interpretability of pretrained models limits their practical use. At the same time, a deeper examination of model's understanding of source code may increase developers' trust and broaden the applicability of pretrained models.</p><p>In NLP, there is an established probing approach for a more fine-grained examination of the knowledge of various aspects of the language, e.g. morphology, syntax, or discource understanding <ref type="bibr" target="#b7">(Belinkov et al., 2020;</ref><ref type="bibr" target="#b28">Tenney et al., 2019;</ref><ref type="bibr" target="#b21">Koto et al., 2021)</ref>. Probing usually means training a linear model on top of hidden representations of a model for various simple tasks, e.g. to predict a part-ofspeech tag, to detect whether a sentence was corrupted, or to estimate the number of objects in the main clause <ref type="bibr">(Conneau et al., 2018a)</ref>. Probing experiments may suggest ways to improve the quality of the pretrained model or provide recommendations on how to tune the model better in applied tasks <ref type="bibr" target="#b5">(Belinkov, 2021)</ref>.</p><p>Inspired by the insights probing provided in NLP, we develop probing tasks to understand the extent to which the current state-of-the-art pretrained models capture structural and semantic properties of source code. Our contributions are as follows:</p><p>? we introduce a set of syntactic and semantic probing tasks, suitable for testing diverse aspects of code understanding;</p><p>? we study an effect of the model probing results;</p><p>? we use probings to highlight which information about code is preserved by finetuned models in different downstream tasks.</p><p>The rest or the work is organized as follows: Section 2 describes our probing tasks; Section 3 describes the pretrained models used for comparison; Section 4 describes the experimental setup of the work; experiments are presented in Section 5; and Section 6 is devoted to related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probing tasks</head><p>We probe pretrained models of code using linear regression or classification trained on top of code representations extracted from each model layer (layers weights are not finetuned) <ref type="bibr" target="#b2">(Alain and Bengio, 2016)</ref>. We develop auxiliary tasks with synthetic data that test models' understanding of various properties of source code: strict syntactic structure and correctness, the notion of data flow, the notion of namespaces, and naming. We consider both global tasks (predicting a property of the whole code snippet) and local tasks (predicting a property of a particular token or a group of tokens).</p><p>Notation. Pretrained models of code usually follow the standard NLP methodology: representing a code snippet as a sequence of subtokens, e. g. byte-pair encoding subtokens, and pretraining the model on a large corpora of source code using masked language modeling. We denote the sequence of subtokens as s 1 , . . . , s m . Let us denote t(s i ) a mapping from a subtoken s i into a corresponding code token t(s i ), e.g. for a subtoken sequence <ref type="bibr">[ (, for, public, get, Status]</ref>, t( get) = getStatus. For each subtoken s i , we extract the model's embedding w i ? R d for a particular layer , where d is the size of hidden representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Token Path Length</head><p>The first three probing tasks test whether pretrained models contain information about the syntactic structure of code. The first task consists of predicting the distance from the root to a token in the Abstract Syntax Tree (AST). Given a subtoken s i and corresponding embedding w i , the task is to predict the length of the path from the root to the t(s i ) token. This task is formulated as a regression problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Token Path Type</head><p>The second task consists of predicting the position of a token in the AST. Given a subtoken s i and corresponding embedding w i , the task is to predict the path type from the root to the t(s i ) token, e.g. [1, 2, 1], meaning go to the first child, then to the second one, then to the first one. This task is formulated as a classification problem over 15 most common path types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">AST depth</head><p>The third syntactic task is defined on a code snippet level and consists of predicting the depth of the AST built from the snippet (regression problem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Is Bracket Misused</head><p>The fourth task evaluates the model's understanding of syntactic correctness. In this task, we replace 30% of the brackets in code with wrong brackets and ask a probing model to identify if the particular bracket token is misused (binary classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Token Is Identifier</head><p>The next task tests the ability of the model to distinguish code elements. Particularly, the task is to detect if a subtoken is a part of an identifier (the label is provided from the AST node type). To solve this task, the model must distinguish the variable name from the content of the constants, e.g. strings, and from the keywords of the language. The task is formulated as binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Edge Prediction in Data Flow Graph edge classification</head><p>The task measures to what extent a model encodes the information about the data flow. Given two subtokens s i and s j the task is to predict a data flow edge type between t(s i ) and t(s j ), given w i , w j .</p><p>There can be no edge (negative example), "comes-From" edge, and "compFrom" edge. The task is formulated as classification of a pair of subtokens (their embeddings are concatenated). An example bellow illustrates the task:</p><formula xml:id="formula_0">public static void foo () { int x1 = 1 ; // comesFrom 1 int y1 = 0 ; // comesFrom 0 if (x1 == 0) { // comesFrom x1 y2 = x1 + y1; // compFrom x1, y1 } else { x3 = y1 ; // compFrom y1 } }</formula><p>To obtain a balanced training set, in addition to existent data flow edges we select a roughly equal number of negative examples by selecting random pairs of nodes from AST with suitable node types (e.g. pairs of identifiers, constants, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Is Variable Declared</head><p>This task tests the model's understanding of the notion of namespaces. The model is asked, whether there is an 'undeclared variable name' error for a particular expression with an identifier. For example, in the first code snippet the identifier y is correctly used after a declaration:</p><formula xml:id="formula_1">int x = 0; if (x == 0) { int y = x;</formula><p>System.out.println(y); } However, in the second snippet there is an error, since y is outside of the scope of it's declaration:</p><formula xml:id="formula_2">int x = 0; if (x == 0) { int y = x; } System.out.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>println(y);</head><p>We generate positive and negative examples using the following procedure. For a code snippet, we find a variable name declaration, e.g. float x = 0. Next, we find a random place in code after the variable declaration where we can insert a printing expression e.g. System.out.println(x);, and define a label for binary classification analyzing variable scopes. The task is formulated for the subtokens of the token representing the inserted variable name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Variable Name</head><p>Finally, we introduce a semantic task targeting the ability to link code elements and their natural language descriptions. A model should predict a variable name, given a code snippet with all occurrences of the original name replaced with a placeholder var.</p><p>We formulate this task as classification, targeting only 15 most popular identifier names. The feature vector is a mean hidden representation for all occurrences of the identifier in code. In such way, the model should be able to predict the identifier name based on the context in which the variable was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>In this section, we briefly describe the models to be compared. We have selected several widely used pretrained models, which vary in the model architecture, pretraining objective, model size, and training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CodeBERT</head><p>CodeBERT <ref type="bibr" target="#b12">(Feng et al., 2020)</ref> is one of the first attempts to pretrain a Transformer-based encoder model for source code representation learning and comprehension. It is a 12 layer encoder model based on RoBERTa-base (125M) <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> and trained with masked language modeling and replaced token detection objectives. The model is trained on 6M CodeSeachNet dataset <ref type="bibr" target="#b18">(Husain et al., 2020)</ref>, composed of functions from 6 programming languages (Java, Python, JavaScript, PHP, Ruby, Go) and NL comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GraphCodeBERT</head><p>GraphCodeBERT <ref type="bibr">(Guo et al., 2021)</ref> extends the work of <ref type="bibr" target="#b12">(Feng et al., 2020)</ref>, by introducing codespecific objectives, aimed at explicitly forcing the model to pay attention to data flow graph of a program. <ref type="bibr">Ahmad et al. (2021a)</ref> introduced a 140M parameter PLBART model with 6 encoder and 6 decoder layers. The model is based on the BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref> architecture. The authors released a PLBART checkpoint pretrained on the 470M Java, 210M Python functions/methods and on 47M NL descriptions, as well as a PLBART_large checkpoint (400M, 12 layer encoder, 12 layer decoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PLBART</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CodeT5</head><p>CodeT5 <ref type="bibr" target="#b29">(Wang et al., 2021)</ref> is an encoder-decoder model based on the T5 <ref type="bibr" target="#b27">(Raffel et al., 2020)</ref> architecture and pretrained on 8.35M functions in 8 programming languages (Python, Java, JavaScript, PHP, Ruby, Go, C, and C#). The model combines the masked language modeling objective with codespecific objectives, including identifier tagging and predicting variable names. We experiment with two released model checkpoints: CodeT5-base (220M) and CodeT5-small (60M).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Probing details</head><p>For each probing task, we average results over three 70% train / 30% test data splits. For each model, we use a single checkpoint as usually only one checkpoint is released. Each pretrained model returns representations for a sequence of subtokens s 1 , . . . , s m , e. g. bytepair encoding subtokens. When the task is formulated on a code snippet level, the layer-wise embeddings of the snippet are obtained by averaging subtoken embeddings, following <ref type="bibr" target="#b21">(Koto et al., 2021)</ref>.</p><p>For the probing models, we use linear models from scikit-learn (0.24.2) <ref type="bibr" target="#b26">(Pedregosa et al., 2011)</ref>, including SGDClassifier with logistic regression loss for classification tasks (we select optimal alpha parameter via grid search over [0.0001, 0.001, 0.01, 0.1] range, and set tolerance to 0.0001); and default RidgeCV for regression tasks.</p><p>We also provide a baseline for each probing task, dubbed "Simple Bound", which is a median predictor for regression, or most-common predictor for classification. For token-level tasks, we use a stronger "Simple Bound" baseline: we choose the most-common predictor for each subtoken in the vocabulary individually (or a pair of subtokens in edge classification). For test subtokens that are not present in the train set, we resort to the "global" most-common predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our research questions are as follows:</p><p>? To what extent do the models pretrained on code capture syntactic and and semantic information?</p><p>? Does multitask pretraing with code-specific objectives provide richer representations?</p><p>? How does the model size affect probing results? Which representations are better: provided by the encoder or by the decoder?</p><p>? Does finetuning preserve syntactic and semantic information in different downstream tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison of different models</head><p>In this subsection, we study the performance of different pretrained models in all probing tasks. In this experiment, we report the results for the best performing layer representation for each model. Figure <ref type="figure">1</ref> presents the results. Overall, we observe that all models achieve quite low errors in all tasks and are much more accurate than constant predictions (Simple Bound). Thus we conclude that pretrained models do contain information about code properties such as syntactic structure and correctness, the notion of namespaces and data flow, and naming possibilities. The lower bound in all tasks except the last one is zero error, which could be achieved by performing static code analysis.</p><p>Next, we find that there is no single model that excels for all the tasks. Particularly, models pretrained with code-specific objectives, CodeT5 and GraphCodeBERT, do not show consistent gain over other models, pretrained with single objectives. However, they do perform well in tasks, related to variable processing ("Is Variable Declared", "Edge Prediction in Data Flow Graph", "Variable Name"). We link this observation to the specialized variable names recognition objectives of the CodeT5 model and the data flow objective of GraphCodeBERT. With the help of the data flow loss component, GraphCodeBERT performs better than CodeBERT or on par with it in all tasks except "Bracket Is Misused", where CodeBERT excels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Encoder vs Decoder</head><p>This subsection compares the representations of the encoder and the decoder. We consider representations of two encoder-decoder models, PLBARTbase and CodeT5-base. Table <ref type="table" target="#tab_2">1</ref> compares best performing encoder representations and best performing decoder representations for all probing tasks. We observe that in almost all probing tasks, the decoder representations perform worse or on par with the encoder representations. In some tasks, e. g."Bracket Is Misused", the decoder shows much worse results than the encoder. A possible explanation is that the main aim of the encoder is to provide rich representations for the decoder, hence the encoder is more suitable for information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The effect of the model size</head><p>In this subsection, we are interested whether larger models capture more information about the source code properties than smaller models. Table 1 reports the performance of CodeT5-base and CodeT5-small models, and of PLBART-large and PLBART-base models (two other models are not available in variable sizes). For simpler syntactic tasks, such as "Token Path Length", "Token Path Type", "AST depth", or "Token Is Identifier", the performance of two models is very close. However, for harder tasks, i.e. "Bracket Is Misused", "Is Variable Declared", or "Variable Name", the larger models often expectedly perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Per-layer probing performance</head><p>We now analyse probing results for different Transformer layers. Figure <ref type="figure">2</ref> exposes the per-layer performance of all considered pretrained models. In syntactic tasks ("Token Path Length", "Token Is Identifier", "AST depth"), the middle (5-8) encoder layers provide the most informative representations, except for the more complex "Token Path Type" task (8-10 encoder layers). In the tasks testing the model's understanding of the concepts of identifiers, namespace or data flow, 8-10 encoder layers often perform best. Finally, in the NL-connected task, variable naming, the last encoder layers perform best. An exception is PLBART, which is a small model (6 layers in encoder/decoder) and in which the last encoder layer is the most effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The effect of finetuning</head><p>In this section, we study the effect of finetuning on probing results. Specifically, we are interested 1) whether finetuned models preserve information contained in pretrained models; 2) does pretraining enrich the representations of finetuned models, compared with the representations of models trained from scratch.</p><p>In this section, we focus on the PLBART model and finetune it for 5 downstream tasks: 3 generative tasks (Code Translation from Python to Java, Java Code Generation based on natural language descriptions, Java Code Summarization into textual description) and 2 discriminative tasks (Clone Detection, Defect Prediction). We use the AVATAR dataset <ref type="bibr">(Ahmad et al., 2021b)</ref> in the Code Translation task and CodeXGLEU benchmark <ref type="bibr" target="#b24">(Lu et al., 2021)</ref> in other tasks. We use scripts for PLBART finetuning on these tasks provided in PLBART 1 1 https://github.com/wasiahmad/PLBART and AVATAR<ref type="foot" target="#foot_0">2</ref> repositories. Figure <ref type="figure" target="#fig_1">3</ref> compares 3 scenarios: the PLBART checkpoint after pretraining (leftmost bar), checkpoints after PLBART finetuning on each of 5 downstream tasks (dark bars), and checkpoints after training from scratch on each of 5 downstream tasks (semi-transporent bars). We also add a Simple Bound (SB) for reference.</p><p>Models finetuned for discriminative tasks exhibit a highest information loss between the initial pretrained stage and the finetuned stage, which may indicate that models trained on these tasks rely on some spurious features, rather than on code syntax or semantics.</p><p>Among generative tasks, the Code Translation model exhibits almost no gap between pretrained and finetuned stages. This could be attributed to having code as both input and output of the task. Code Generation and Code Summarization models have code only as either the input or the output of the task, and exhibit a slightly larger gap. The Code Summarization model is a winner on the Variable Name task, since summarization models produce textual information based on code and rely heavily on variable naming.</p><p>As for models, trained from scratch for downstream tasks (semi-transparent bars), the overall trend is similar across the downstream tasks, but the absolute results are much worse, compared to finetuned models, and are close to the context-free Simple Bound model. The downstream tasks alone do not provide high-quality code representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Probing became a universal tool in NLP for testing pretrained models' understanding or knowledge of various language aspects. Pioneer works of <ref type="bibr" target="#b20">K?hn (2015)</ref> and <ref type="bibr" target="#b14">Gupta et al. (2015)</ref> used probing to study the morphological, syntactic, and semantic properties of static word embeddings, while the later works of <ref type="bibr" target="#b11">Ettinger et al. (2016)</ref> and <ref type="bibr">Conneau et al. (2018b)</ref> used probing to study sentence embeddings. Recent works propose more complex probing frameworks, e. g. <ref type="bibr" target="#b17">Hewitt and Manning (2019)</ref> utilize metric learning approach to restore parse trees from embeddings, or consider new language aspects, e. g. discourse understanding <ref type="bibr" target="#b21">(Koto et al., 2021)</ref>. We refer to <ref type="bibr" target="#b5">Belinkov (2021)</ref> for a broad review of existing probing works in NLP. In the context of source code, Karmakar and Robbes (2021) make first steps towards probing pretrained models. However, they only consider four simple tasks and two code models, CodeBERT and GraphCodeBERT. In contrast to their work, we propose a wider set of more complex tasks, including several token-wise tasks, consider a wider range of pretrained models, and investigate various dimensions, including different pretraining objectives, model sizes, and the effect of finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a diagnosis tool, based on probing tasks, that can be used to estimate to which extent deep learning models capture the information about various properties of source code in their hidden representations. Our results show that pretrained models of code do contain information about code syntactic structure and correctness, the notions of identifiers, namespaces and data flow, and natural language-based naming. Using code-specific pretraining objectives enriches the understanding of the code aspects addressed in the corresponding objective, and finetuning may deteriorate the model's understanding of code properties, especially in classification downstream tasks. Based on these results, future works may consider developing more diverse code-specific pretraining objectives or including code-specific objectives in finetuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on the effect of finetuning. Pre-train (leftmost bar): pretrained-only checkpoint. The following bars: dark -finetuned models, semi-transparent -models trained from scratch. Results for 5 downstream tasks: Code Translation, Code Generation, Code Summarization, Defect Prediction, Clone Detection. Simple Sound (SB) is provided for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Results for the best performing layer representations, for all probing tasks. Models: CodeBERT (CB), GraphCodeBERT (GCB), PLBART (PLB), CodeT5. Metrics are the lower the better. Simple Bound baselines: Token Path Length -2.0, Token Path Type -0.15, for AST depth -2.5, Bracket Is Misused -0.2, for Token Is Identifier -0.09 , for Edge Prediction in Data Flow Graph -0.15, for Is Variable Declared -0.12, for Variable Name -0.8.For our experiments, we use the test dataset provided byAhmad et al. (2021a)  consisting of 10k examples of Java functions and methods. Preprocessing removes comments and new line symbols, and standartizes code snippets. For each pretrained model, we apply it's subtokenization procedure over the code tokens. All models have a limit of 512 input subtokens. We crop subtoken sequences that are longer than 512 subtokens.</figDesc><table><row><cell>Token Is Identifier</cell><cell cols="4">Edge Prediction in Data Flow Graph</cell><cell></cell><cell></cell><cell cols="2">Is Variable Declared</cell><cell></cell><cell></cell><cell>Variable Name</cell></row><row><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell></row><row><cell>1-ACC</cell><cell>0.005</cell><cell></cell><cell></cell><cell>1-ACC</cell><cell>0.02</cell><cell></cell><cell></cell><cell>1-ACC</cell><cell>0.05 0.10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.000</cell><cell>CB</cell><cell>GCB Pretrained Model PLB</cell><cell>CodeT5</cell><cell>0.00</cell><cell>CB</cell><cell>GCB Pretrained Model PLB</cell><cell>CodeT5</cell><cell>0.00</cell><cell>CB</cell><cell>GCB Pretrained Model PLB</cell><cell>CodeT5</cell></row><row><cell>Figure 1: 4 Experimental setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Data and preprocessing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Encoder vs decoder performance for PLBART-base and CodeT5-base; and comparison of small vs large models: PLBART-base vs PLBART-large, and CodeT5-small vs CodeT5-base. Metrics: the lower the better.</figDesc><table><row><cell></cell><cell cols="2">PLBART</cell><cell cols="2">CodeT5</cell><cell cols="2">PLBART</cell><cell>CodeT5</cell></row><row><cell>Task</cell><cell cols="5">encoder decoder encoder decoder base</cell><cell>large small base</cell></row><row><cell>Token Path Length</cell><cell>1.411</cell><cell>1.273</cell><cell>1.296</cell><cell>1.307</cell><cell cols="2">1.273 1.384 1.290 1.295</cell></row><row><cell>Token Path Type</cell><cell>0.022</cell><cell>0.025</cell><cell>0.024</cell><cell>0.057</cell><cell cols="2">0.022 0.022 0.024 0.024</cell></row><row><cell>AST depth</cell><cell>0.880</cell><cell>0.860</cell><cell>0.870</cell><cell>0.866</cell><cell cols="2">0.860 0.865 0.875 0.866</cell></row><row><cell>Bracket Is Misused</cell><cell>0.029</cell><cell>0.065</cell><cell>0.034</cell><cell>0.096</cell><cell cols="2">0.029 0.021 0.043 0.034</cell></row><row><cell>Token Is Identifier</cell><cell>0.019</cell><cell>0.022</cell><cell>0.022</cell><cell>0.035</cell><cell cols="2">0.019 0.024 0.022 0.022</cell></row><row><cell cols="2">Edge Prediction in DFG 0.009</cell><cell>0.021</cell><cell>0.008</cell><cell>0.008</cell><cell cols="2">0.009 0.010 0.007 0.008</cell></row><row><cell>Is Variable Declared</cell><cell>0.040</cell><cell>0.048</cell><cell>0.014</cell><cell>0.020</cell><cell cols="2">0.040 0.014 0.023 0.014</cell></row><row><cell>Variable Name</cell><cell>0.172</cell><cell>0.170</cell><cell>0.143</cell><cell>0.197</cell><cell cols="2">0.163 0.156 0.179 0.143</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/wasiahmad/AVATAR</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The results were supported by the <rs type="funder">Russian Science Foundation</rs> grant <rs type="grantNumber">?19-71-30020</rs>. The research was supported in part through the computational resources of HPC facilities at <rs type="institution">NRU HSE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RPDd2Gj">
					<idno type="grant-number">?19-71-30020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified pre-training for program understanding and generation</title>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2655" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Wasi Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Golam Rahman Tushar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11590</idno>
		<title level="m">Avatar: A parallel corpus for java-python program translation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1610.01644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved code summarization via a graph neural network</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haque</forename><surname>Sakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Lingfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mcmillan</forename><surname>Collin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3387904.3389268</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bimodal modelling of source code and natural language</title>
		<author>
			<persName><forename type="first">Miltos</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2123" to="2132" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probing Classifiers</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00422</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Shortcomings</forename><surname>Promises</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Advances</forename></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00422</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the linguistic representational power of neural machine translation models</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00367</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Hassan Sajjad, and James Glass</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probing for semantic evidence of composition by means of simple classification tasks</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2524</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP</title>
		<meeting>the 1st Workshop on Evaluating Vector-Space Representations for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="134" to="139" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Code-BERT: A pre-trained model for programming and natural languages</title>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.139</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graphcode{bert}: Pre-training code representations with data flow</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Shujie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributional vectors encode referential attributes</title>
		<author>
			<persName><forename type="first">Abhijeet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pad?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the use of automated text summarization techniques for summarizing source code</title>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jairo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1109/WCRE.2010.13</idno>
	</analytic>
	<monogr>
		<title level="m">2010 17th Working Conference on Reverse Engineering</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global relational models of source code</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName><surname>Bieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What do pre-trained code models know about code?</title>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Karmakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Robbes</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASE51524.2021.9678927</idno>
	</analytic>
	<monogr>
		<title level="m">2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1332" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What&apos;s in an embedding? analyzing word embeddings through multilingual evaluation</title>
		<author>
			<persName><forename type="first">Arne</forename><surname>K?hn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discourse probing of pretrained language models</title>
		<author>
			<persName><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jey</forename><surname>Han Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3849" to="3864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Codexglue: A machine learning benchmark dataset for code understanding and generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2102.04664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lexical statistical machine translation for language migration</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tuan Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2491411.2494584</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2013</title>
		<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2013<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Baptiste Roziere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lowik</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><surname>Lample ; Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berlin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weishi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8696" to="8708" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
