<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperReconNet: Joint Coded Aperture Optimization and Image Reconstruction for Compressive Hyperspectral Imaging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lizhi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ying</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hua</forename><surname>Huang</surname></persName>
							<email>huahuang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Technology</orgName>
								<orgName type="institution" key="instit2">China (e-mail: lzwang</orgName>
								<address>
									<postCode>2120171112</postCode>
									<region>fuying</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HyperReconNet: Joint Coded Aperture Optimization and Image Reconstruction for Compressive Hyperspectral Imaging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22DCA7408D72F28C3F32AAEE539D4BCE</idno>
					<idno type="DOI">10.1109/TIP.2018.2884076</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2884076, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2884076, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 2 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2884076, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 4</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Coded aperture snapshot spectral imaging</term>
					<term>convolution neural network</term>
					<term>coded aperture optimization</term>
					<term>hyperspectral image reconstruction Compressive Image Scene Measurement Phase HSI Reconstruction Phase Coded Aperture Design Intrinsic Correlation Exploitation Joint Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Coded aperture snapshot spectral imaging (CASSI) system encodes the 3D hyperspectral image (HSI) within a single 2D compressive image and then reconstructs the underlying HSI by employing an inverse optimization algorithm, which equips with the distinct advantage of snapshot but usually results in low reconstruction accuracy. To improve the accuracy, existing methods attempt to design either alternative coded apertures or advanced reconstruction methods, but cannot connect these two aspects via a unified framework, which limits the accuracy improvement. In this paper, we propose a convolution neural network (CNN) based endto-end method to boost the accuracy by jointly optimizing the coded aperture and the reconstruction method. On the one hand, based on the nature of CASSI forward model, we design a repeated pattern for the coded aperture, whose entities are learned by acting as the network weights. On the other hand, we conduct the reconstruction through simultaneously exploiting intrinsic properties within HSI -the extensive correlations across the spatial and the spectral dimensions. By leveraging the power of deep learning, the coded aperture design and the image reconstruction are connected and optimized via a unified framework. Experimental results show that our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H YPERSPECTRAL imaging systems densely sample the spectral signature of each point in the scene. The captured hyperspectral image (HSI) provides massive lighting and material information about the scene, which is beneficial to various fields, including remote sensing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, computer vision <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, medical diagnosis <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and more. Existing 2D imaging sensors are incapable of capturing the 3D HSI with a single exposure. Instead, conventional hyperspectral imaging systems <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref> sacrifice the temporal dimension and scan the scene along either spatial or spectral dimension to capture a full HSI. Thus, these scanning-based systems cannot be used to capture dynamic scenes.</p><p>Recently, owing to the rapid advancement of computational imaging, snapshot spectral imagers <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr">[16]</ref> have been developed with diverse optical designs and elaborate reconstruction algorithms. Based on the compressive sensing (CS) theory, coded aperture snapshot spectral imaging (CASSI) <ref type="bibr">[15]</ref>, <ref type="bibr">[16]</ref> stands out Figure <ref type="figure">1</ref>. The compressive hyperspectral imaging includes the measurement phase and the reconstruction phase. Existing methods ignore the connection between these two phases and just consider only one phase each time, which limits the improvement of the accuracy. Our method jointly optimizes these two phases, in which the coded aperture is designed for the measurement phase and the intrinsic spatial and spectral correlation is exploited for the reconstruction phase.</p><p>as a promising solution. By introducing a binary coded aperture, CASSI encodes the 3D HSI into a single 2D compressive image. An inverse optimization algorithm is then used to reconstruct the underlying HSI from the 2D image.</p><p>Since the reconstruction problem is severely under-determined, the accuracy of the reconstructed data is usually limited. To improve the accuracy for CASSI system, existing methods make efforts mainly from two phases, i.e., the measurement phase and the reconstruction phase. From the measurement phase, alternative coded aperture patterns are designed with the purpose of encoding the HSI information more efficiently <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. From the reconstruction phase, elaborate reconstruction algorithms are proposed along with the advancement of CS theory <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. These two phases generally determine the reconstruction accuracy together. However, existing methods ignore the connection between these two phases and usually just consider only one phase each time, which limits the improvement of the accuracy.</p><p>In this paper, we present a convolution neural network (CNN) based end-to-end method which jointly optimizes the coded aperture and image reconstruction (Figure <ref type="figure">1</ref>). Specifically, for the measurement phase, based on the nature of CASSI forward model, we decompose the CASSI forward model from the imagebased modeling to patch-based one, which keeps consistent with the complexity requirement of CNN. This motivation comes from the key observation that each small patch of the 2D compressive image corresponds to an oblique parallelepiped within the underlying 3D HSI <ref type="bibr" target="#b20">[21]</ref>. By making full use of the merit of the patchbased modeling, we further design a repeated coded aperture pattern and optimize the entities of the coded aperture by treating them as the network weights. Then for the reconstruction phase, given the correlation across both spatial and spectral dimensions, we design a spatial network and a spectral network to exploit the intrinsic properties of HSIs, and concatenate these two networks to finish the reconstruction. By leveraging the power of deep learning, the coded aperture optimization and HSI reconstruction are unified into one framework, dubbed as HyperReconNet. To the best of our knowledge, it is the first time to integrate the CNN with the compressive hyperspectral imaging system to directly reconstruct 3D HSI from the 2D compressive image. Also, it is the first attempt to bridge the connection between the measurement phase and reconstruction phase, which boosts the reconstruction fidelity and makes a solid step forward for making the compressive hyperspectral imaging system practical.</p><p>The rest of this paper is organized as follows. Section II reviews the related works. Section III presents the proposed method. The experimental results are provided in Section IV. Conclusions are drawn and future directions are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Compressive HSI Acquisition</head><p>Conventional spectral cameras usually make trade-off between temporal/spatial and spectral resolution. Whiskbroom and Pushbroom based <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> acquisition systems captured the full scene by scanning pointwisely or linewisely in the spatial dimension. Rotating and tunable filters based systems <ref type="bibr" target="#b8">[9]</ref> employed multiple color bandpass filters to capture one band for each exposure. Spatial variant color filters <ref type="bibr" target="#b9">[10]</ref> captured different spectra at different points, and multiple exposures were required to obtain the full spectral information. All these methods require multiple exposures to obtain the full spectral information and thus sacrifice the temporal resolution.</p><p>To improve the temporal resolution of HSI acquisition, several snapshot spectral imagers have been proposed with elegant implementations to map the 3D HSI onto the 2D spatial sensor, but trade the spatial resolution for the spectral resolution <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Recently, relying on the CS theory, CASSI has made a significant breakthrough towards hyperspectral video acquisition and can simultaneously capture high spatial and temporal resolution HSIs <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. CASSI employed two dispersers <ref type="bibr">[15]</ref> or one disperser <ref type="bibr">[16]</ref> with a coded aperture to uniformly encode spectral information along space, and the HSI was then recovered through computational reconstruction. To improve the performance of the CASSI, the dual-camera compressive hyperspectral imager <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> incorporated a co-located panchromatic camera to collect more information simultaneously with the CASSI measurement. As an advancement of CASSI, a dual-coded compressive spectral imager was proposed recently <ref type="bibr" target="#b27">[28]</ref>, which separately encoded spatial and spectral dimensions using a digital micromirror device (DMD) and a liquid crystal on silicon (LCOS), respectively. By jointly considering the spatial-spectral coding mechanism, the spatial-spectral compressive hyperspectral image <ref type="bibr" target="#b28">[29]</ref> achieved a spatially varying spectral coding by using only one coded aperture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computational HSI Reconstrcution</head><p>Computational reconstruction algorithms play an important role in compressive hyperspectral imaging systems. The subsequent reconstruction problem is how to derive the underlying 3D HSI from the 2D compressive measurement. This derivation is ill-posed by nature, and thus image priors play a crucial role for a faithful reconstruction. In the early development of CASSI, the gradient projection for sparse reconstruction (GPSR) algorithm</p><note type="other">Objective Lens Relay Lens Dispersive Prism Detector Coded Aperture Compressive Image Scene CASSI Figure 2</note><p>. CASSI architecture. The scene information will go through three kinds of optical processing, i.e.spatial modulation, spectral dispersion and spectral summation was employed <ref type="bibr" target="#b29">[30]</ref>, which imposed the sparsity constraint of the whole 3D HSI on the orthogonal basis <ref type="bibr" target="#b30">[31]</ref>. Later, the two-step iterative shrinkage/thresholding (TwIST) algorithm <ref type="bibr" target="#b31">[32]</ref>, together with the total variation (TV) prior, was employed to provide higher reconstruction fidelity <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>. TV prior has been demonstrated to be effective in preserving boundaries and recovering smooth regions, but it tends to smear out image details due to the local smoothness assumption. Recently, by integrating the approximate message passing (AMP) framework, Tan proposed to employ an adaptive Wiener filter as the image denoiser in each iteration <ref type="bibr" target="#b33">[34]</ref>. AMP can produce better results compared with TwIST and GPSR, meanwhile with the advantage of free of parameter tuning. Blind compressed sensing (BCS) and on-line dictionary learning have also been proposed to solve the CASSI reconstruction problem. BCS makes effort to jointly infer the underlying image and learn the model of the image (e.g., dictionaries or basis of sparsity) from the compressive measurements <ref type="bibr" target="#b34">[35]</ref>. The first attempt that applied BCS to multiframe CASSI was reported in <ref type="bibr" target="#b35">[36]</ref> with a Bayesian implementation, which enforced each small 3D hyperspectral cube to be a sparse combination of the dictionary atoms. Then, to impose the constraint of compressibility rather than sparsity on the recovered dictionary coefficients, a new BCS model was proposed with global-local shrinkage prior <ref type="bibr" target="#b36">[37]</ref>. By further making use of nonlocal similarity, the 3D non-local sparse representation model was proposed with improved performance <ref type="bibr" target="#b19">[20]</ref>.</p><p>Besides the advancement of the reconstruction algorithm, researchers are making efforts to exploit the parallelism to lower the complexity and speed up the reconstruction. The outstanding strategy is to decompose the image-based reconstruction to patchbased one, by making use of the nature of imaging mechanism of the systems. The representative works including <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> which adopt the similar reconstruction strategy, by imposing the sparsity on small 3D hyperspectral cubes. Motivated by the previous works, we leverage the divide-and-conquer strategy to solve the contradiction between high training complexity of the network and limited computing resource available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimal Coded Aperture Design</head><p>According to the CS theory, the sensing matrix that maps the HSI to the compressive image in CASSI plays a crucial role in the reconstruction quality. With fixed detector and dispersive prism, the sensing matrix is only determined by the entities of the coded aperture. Initial design for the coded aperture employed random binary entities <ref type="bibr">[16]</ref>. However, the random coded aperture does not fully exploit the structure of the sensing mechanism of CASSI, which leads to sub-optimal reconstruction. To make the CASSI measurement contain more useful information, the optimization of the coded aperture pattern has attracted increasing attention in recent years <ref type="bibr" target="#b16">[17]</ref>. Arguello started to optimize the coded aperture from the analysis of the Restricted Isometry Property (RIP) <ref type="bibr" target="#b37">[38]</ref> of the projection matrix which is the product of the sensing matrix with a specific basis matrix <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. It proposed to convert the coded aperture optimization problem to a rank minimization problem, which was solved with generic algorithm. This method focused on spectral selectivity, which required more than one shots, since the multiple measurements from different shots were manipulated to obtain a new measurement containing and only containing the information from the desired spectral bands. Potentially, it can be used for single frame system, since the degrees of freedom are smaller than that of multiple frame systems. Recently, new advances in micro-lithography and coating technology allowed the design of colored coded apertures, which was also introduced into CASSI system <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Parada-Mayorga turned to analyze the coherence of the projection matrix instead of the RIP <ref type="bibr" target="#b17">[18]</ref>. It was proposed that the optimization of the colored coded aperture pattern was equivalent to a coherence minimum problem. Ramirez and Arguello proposed to model the entity distribution of the Gram matrix of the projection matrix. The colored coded aperture was designed such that the variance of the Gram matrix was minimized <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>. For these methods, it should determine a specific sparsity matrix before the optimization starts. Recent researches have shown that fixed sparsity matrix will produce sup-optimal reconstruction results. Instead, blind compressive sensing and on-line dictionary learning methods 1 have shown the power with higher quality <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b46">[47]</ref>. These methods can learn the sparsity basis according to the scene characteristic adaptively. In this sense, the sparsity basis is not available before imaging, and thus cannot be used to design the coded aperture.</p><p>Motivated by the trends of coded aperture optimization methods as described above, in this paper, we turn to solve the problem via a data-driven method, which is complementary to the previous methods that are based on optimization. We believe the optimization based and the data-driven based coded aperture design methods are two complementary directions for the success of coded aperture imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CNN-based HSI Processing</head><p>CNN-based methods can effectively learn the complex features and have been widely used in HSI processing. Liebel <ref type="bibr" target="#b47">[48]</ref> directly extended SRCNN <ref type="bibr" target="#b48">[49]</ref>, which presented a super-resolution method for gray image based on CNN, to the HSI super-resolution by replacing the input and output data with the HSI. Yuan <ref type="bibr" target="#b49">[50]</ref> utilized the learned network by SRCNN from the gray natural image dataset to perform the HSI super-resolution, and then employed the collaborative nonnegative matrix factorization to enhance the super-resolution results. Li <ref type="bibr" target="#b50">[51]</ref> combined spatial constraint strategy with a deep spectral difference CNN model to upsample HSI. Li <ref type="bibr" target="#b51">[52]</ref> reconstructed HSI based on CNN to enhance the spatial features, which was further used for the classification. All these CNN-based methods are presented for the HSI super-resolution or enhancement, and are not used for the CASSI system.</p><p>Recently, Xiong <ref type="bibr" target="#b52">[53]</ref> initially reconstructed the HSI from RGB or CASSI through simple interpolation or CS reconstruction, and 1 Interested reader please check <ref type="bibr" target="#b45">[46]</ref> for the details description about dictionary learning The coded aperture is first reshaped into vertorized form, then repeated in the horizontal direction, each time with a uniform shift in the vertical direction with zero padding, as many times as the number of spectral band. then employed CNN-based method to enhance the initialized results and obtain the high quality HSI. Choi <ref type="bibr" target="#b53">[54]</ref> learned nonlinear spectral representations by building a convolutional autoencoder instead of the dictionary learning, which was jointly regularized with the sparsity of gradients in the spatial domain to reconstruct HSI from compressive sensing (CS) measurement. Both methods can be used for the HSI reconstruction from CASSI system, but they resort to the traditional CS reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. JOINT CODED APERTURE OPTIMIZATION AND HSI RECONSTRUCTION</head><p>In this section, we briefly formulate the forward model for the CASSI system, and then introduce the networks for HSI reconstruction and coded aperture optimization, which are further connected into a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CASSI Formulation</head><p>In CASSI system, as shown in Figure <ref type="figure">2</ref>, the scene information is first projected onto the coded aperture, which plays a spatial modulation. The spatially modulated information is then spectrally dispersed by the prism and finally captured by the detector. Let s(m, n, k) denotes the scene information of a 3D hyperspectral image in its discrete form, where 1 ≤ m ≤ M and 1 ≤ n ≤ N index the spatial coordinates, and 1 ≤ k ≤ K indexes the spectral coordinate. The 2D CASSI measurement can be formulated as</p><formula xml:id="formula_0">y(m, n) = K k=1 s(m -k, n, k)T (m -k, n),<label>(1)</label></formula><p>where T (m, n) represents the binary coded aperture pattern. Note in Eq. ( <ref type="formula" target="#formula_0">1</ref>) we set the spectral dispersion along the vertical direction, and in practice the principle and deduction hereafter is identical to horizontal dispersion. Please refer to [16] for a detailed description about the CASSI forward model. Equation ( <ref type="formula" target="#formula_0">1</ref>) can be rewritten in a linear matrix form as</p><formula xml:id="formula_1">Y = ΦS,<label>(2)</label></formula><p>where Y ∈ R (M +K-1)N and S ∈ R M N K are the vectorized representation of the compressive image y(m, n) and the underlying HSI s(m, n, k), respectively, and Φ is the sensing matrix of the CASSI system which is determined by the coded aperture pattern T (m, n). Figure <ref type="figure" target="#fig_0">3</ref> shows the diagram of relationship between coded aperture and the sensing matrix. The sensing matrix in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HSI Reconstruction</head><p>Inspired by the impressive development in learning-based image recovery <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>, we propose to perform the HSI reconstruction by means of CNN. Given the 3D nature of HSI, training specific networks demands high computing resource. Thus, it is hard to support full image throughput within the network. To this end, we propose to employ the divide-and-conquer strategy and turn to reconstruct small 3D HSI patches each time <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Now the bottleneck lies in how to divide the image-based reconstruction into patch-based one. Intuitively, we can trace a 3D HSI patch of size P × P × K (P &lt; M , P &lt; N ) through the system forward model, which is illustrated in Figure <ref type="figure" target="#fig_1">4a</ref>. After the HSI patch is modulated by the coded aperture and dispersed by the prism, the corresponding measurement on the detector is a 2D patch of size (P + K -1) × P . However, the 2D patch would contain scene information from the neighboring HSI patches and cannot be mapped to one individual HSI patch. Thus, the decomposition method in Figure <ref type="figure" target="#fig_1">4a</ref> cannot satisfy the requirement.</p><p>Based on the nature of the CASSI forward model, we turn to trace a P × P patch on the detector back through the optical system as shown in Figure 4b <ref type="bibr" target="#b20">[21]</ref>. We can see that the corresponding HSI patch is no longer a cube but an oblique parallelepiped with K shifted spectral bands. Each band has a one-pixel shift relative to its neighboring bands in the dispersive direction. The decomposition method in Figure <ref type="figure" target="#fig_1">4b</ref> provides a kind of one-to-one mapping between the oblique parallelepiped HSI patch and the 2D compressive patch on the detector, and thus effectively fits the divide-and-conquer strategy.</p><p>According to the employed decomposition method, we need to reconstruct each oblique parallelepiped s i from the measurement patch y i , where the superscript indicates the selected patch. To this end, we first design a spatial CNN and a spectral CNN to exploit the spatial correlation between the neighboring pixels and spectral correlation between neighboring bands, respectively. Then, the two sub-networks are concatenated to finish the reconstruction together. Figure <ref type="figure" target="#fig_3">5</ref> shows the network architecture.</p><p>1) Spatial CNN: We establish a CNN to learn a nonlinear function h(•) that maps a measured 2D patch y i to an oblique parallelepiped HSI patch. To simultaneously consider the spatial correlation along vertical and horizontal directions, we employ the same filter for all pixels in the same row and each filter convolves with all pixels in the same column for each layer, which is shown in the left part of Figure <ref type="figure" target="#fig_3">5</ref>. The input of the first layer is the compressive patch y i . Let y i p denote all pixels in the p-th row of input compressive patch y i . Thus, the output for the p-th row is</p><formula xml:id="formula_2">h 1 (y i p ) = max(W 1,p * y i + c 1,p , 0),<label>(3)</label></formula><p>where W 1,p and c 1,p represent the filters and biases for the p-th row, respectively. Here, W 1,p corresponds to K filters with the size 1 × p × 3, where a filter operates on p × 3 spatial region. Thus, the output consists of K feature maps. Note the number of the feature map is consistent with that of spectral band. We also apply ReLU to performs the nonlinear operation. Since we design the same filters for the same row and different filters for the different row, the output of first layer is</p><formula xml:id="formula_3">h 1 (y i ) = [h 1 (y i 1 ); • • • ; h 1 (y i p ); • • • ; h 1 (y i P )].<label>(4)</label></formula><p>Here, we employ three CNN layers and the output of other two layers can be described as</p><formula xml:id="formula_4">h 2 (y i p ) = max(W 2,p * h 1 (y i ) + c 2,p , 0), h 2 (y i ) = [h 2 (y i 1 ); • • • ; h 2 (y i p ); • • • ; h 2 (y i P )], h 3 (y i p ) = max(W 3,p * h 2 (y i ) + c 3,p , 0), h 3 (y i ) = [h 3 (y i 1 ); • • • ; h 3 (y i p ); • • • ; h 3 (y i P )].<label>(5)</label></formula><p>For the last two layers, the kernel size is K × p × 3. All convolutional layers have the same stride of 1 without pooling operation, so as to keep the final output size unchanged.</p><p>In this spatial CNN, we mainly consider the spatial correlation and reconstruct the initial oblique parallelepiped HSI patch from the measured 2D patch, In the following, we design the spectral CNN to further model the shifted structure and exploit the spectral correlation between neighboring bands.</p><p>2) Spectral CNN: Previous works [16] <ref type="bibr" target="#b57">[58]</ref> have assumed that the neighboring spatial spectral bands have strong similarity. Derived from this assumption, we can employ the neighboring bands to recover the current band. Therefor, we establish a CNN to learn a nonlinear function that mainly exploits the spectral correlation to assist the HSI reconstruction.</p><p>The input of the first layer in the spectral CNN is the output of spatial CNN, which is denoted as   the k-th band in the a i . Thus, the output for k-th band through the first layer is</p><formula xml:id="formula_5">a i = h 3 (y i ) . Let a i k denote</formula><formula xml:id="formula_6">f 1 (a i 1 ) = max(V 1,1 * cat(a i 1 , a i 2 ) + d 1,1 , 0), f 1 (a i k ) = max(V 1,k * cat(a i k-1 , a i k , a i k+1 ) + d 1,k , 0), f 1 (a i K ) = max(V 1,K * cat(a i K-1 , a i K ) + d 1,K , 0),<label>(6)</label></formula><p>where cat(•, •) denotes the concatenation between neighboring bands. Two neighboring bands are used for the first and last bands, and three neighboring bands are used for all other bands. V 1,k and d 1,k represent the filters and biases for the k-th band, respectively.</p><p>Here, we use three CNN layers and the output of other two layers can be described as</p><formula xml:id="formula_7">f 2 (a i k ) = max(V 2,k * f 1 (a i k ) + d 2,k , 0), f 3 (a i k ) = V 3,k * f 2 (a i k ) + d 3,k ,<label>(7)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_3">5</ref>, the first layer of spectral CNN uses kernel size 9 × 9 and generates 64 feature maps. The second layer uses 1 × 1 kernel size and generates 32 feature maps. The final layer uses 5 × 5 kernel size and generates only one feature map. All the convolutional layers also have the same stride of 1 without pooling operation.</p><p>Since we reconstruct each band separately with its neighboring bands, we can obtain the output from the last layer by</p><formula xml:id="formula_8">f 3 (a i ) = cat(f 3 (a i 1 ), • • • , f 3 (a i k ), • • • , f 3 (a i K ))<label>(8)</label></formula><p>In contrast to traditional CNN, He et al. <ref type="bibr" target="#b58">[59]</ref> showed that residual CNN can effectively preserve some information from previous layers. In our task, we attempt to employ this property to recover more image details (e.g., edges). Besides, residual CNN can improve the convergence rate and accelerate the training process. Therefore, we design our network in terms of residual CNN, and the final output can be expressed as</p><formula xml:id="formula_9">s i = f 3 (a i ) + a i<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Coded Aperture Optimization</head><p>From the analysis of Sec. III-B, we can obtain a one-to-one mapping between the 3D oblique parallelepiped HSI patch and the 2D compressive patch. The one-to-one mapping allows us to decompose the image reconstruction into small-scale problem. However, for the forward model in Figure <ref type="figure" target="#fig_1">4b</ref>, a small oblique parallelepiped HSI patch with P × P spatial points and K spectral bands is spatially modulated by a coded aperture patch of size (P + K -1) × P . It is obvious that the spatial resolution of the coded aperture patch is larger than that of the oblique parallelepiped. As a result, the coded aperture patch modulates not only the current oblique parallelepiped patch but also its neighboring patches . That is to say the coded aperture patches corresponding to neighboring oblique parallelepiped patches would have overlapped area. In this sense, if we optimize the coded aperture patch for current oblique parallelepiped HSI patch, we must consider its effect on the neighboring patches. This cross-talk would spread all over the full image, and thus the one-to-one mapping is invalid.</p><p>To eliminate the cross-talk effect of the coded aperture, we propose to design a local random and global repeated coded aperture. In this design, P × P entities are treated as the basic unit. Once optimized, the basic unit is tiled to full resolution as the final coded aperture. In this manner, for each coded aperture patch with size of (P + K -1) × P , the upper (K -1) × P entities (as shown in the red box of the coded aperture in Figure <ref type="figure" target="#fig_1">4b</ref>) are the same with the lower (K -1) × P entities. Specifically, the basic unit of the coded aperture is expressed as</p><formula xml:id="formula_10">B 1 =      b 1 • • • b P . . . . . . . . . b P (P -1)+1 • • • b P 2 ,     <label>(10)</label></formula><p>where the entities in B 1 , i.e., b p = 0 or 1 (p = 1, • • • , P 2 ), need to be learned. Since the information is spectrally dispersed by the prism, the coded aperture corresponding to each spectral band has a one-pixel shift relative to its neighboring bands. Thus, the binary coded aperture for the k-th spectral band can be described as the circular shift of B 1 , i.e.</p><formula xml:id="formula_11">B k =                b k(P -1)+1 • • • b kP . . . . . . . . . b P (P -1)+1 . . . b P 2 b 1 . . . b P . . . . . . . . . b k(P -2)+1 • • • b k(P -1) .               <label>(11)</label></formula><p>As an example, Figure <ref type="figure" target="#fig_5">6</ref> illustrates the coded aperture patterns of three neighboring bands.</p><p>In order to transform such a structure into a neural network and learn the optimal coded aperture, we first move the lower part of the oblique parallelepiped patch s i to the upper location as indicated by the orange line in Figure <ref type="figure" target="#fig_6">7</ref>, and obtain a cubic HSI patch ŝi . We extract all spectral elements at the spatial location</p><formula xml:id="formula_12">p (p = 1, • • • , P 2 ) in ŝi , which is denoted as ŝi p ∈ R K . Let ŝi p,k (k = 1, • • • , K) denotes the k-th spectral element in ŝi p .</formula><p>We set b p as the 1 × 1 kernel weight and obtain the encoded 3D patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ê(s</head><formula xml:id="formula_13">i p ) = K k=1 b p ŝi p,k .<label>(12)</label></formula><p>Then, by returning back the moved part as indicated by the orange line, we obtain the encoded oblique parallelepiped. So far, the spatial modulation model is built for CNN-based coded aperture optimization. Then, by the following spectral dispersion and summation, we obtain the 2D compressive measurement.</p><p>The BinaryConnect method <ref type="bibr" target="#b59">[60]</ref> is used to learn the entities in B 1 to be 0 or 1 by the sign function, i.e.</p><formula xml:id="formula_14">b b = 1, b r &gt; 0, 0, b r ≤ 0 (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where b r is the real-valued entities of B 1 for the update of the network. The learned b b from this coding CNN is the value in B 1 and builds the optimal coded aperture. The full coded aperture is obtained by repeating this kind of learned entities to fit the spatial resolution of the coded aperture. An implementation of such a coded aperture on existing systems employing DMD or LCoS can be easily performed. Since the learned coded aperture is uniformly used for each oblique parallelepiped HSI patch, the corresponding reconstructed patches is non-overlapping. Thus, it is easy to introduce the blocky artifacts. To mitigate the blocky artifacts and improve the reconstruction quality, we present the overlapping processing. In this manner, each oblique parallelepiped patch is divided into four patches of size P 2 × P 2 × K, which share the same coded aperture pattern. Thus, P 2 4 values have to be learned instead of P 2 values for the coded aperture in the set B 1 . This kind of processing can make that the reconstructed oblique parallelepiped patch of size P × P × K has the spatial overlapping of size P 2 × P 2 with its neighboring patches. Therefore, the reconstructed full HSI can be obtained by merging all these patches and averaging them at the overlapping area. Taking P = 8 as an example, which is illustrated in Figure <ref type="figure" target="#fig_6">7</ref>, it can be divided into four sub-patches of the size 4 × 4 × K. Thus, only 16 binary values need to be estimated and each binary value corresponds to four different inputs. For instance, the ê(s i 1 ), ê(s i 5 )), ê(s i 33 ), ê(s i 37 ) share the same parameter b 1 in terms of Equation <ref type="bibr" target="#b11">(12)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Joint Training</head><p>Computational imaging systems that optically code the recorded data and recover it via computation is obvious a trend towards high performance hyperspectral imaging. How to bridge the connection between the measurement phase and the reconstruction phase plays a crucial role in the system performance. Thus, we propose to jointly train the coded aperture optimization network and the HSI reconstruction network by learning all the weights and bias in one model. The set of all parameters can be denoted as  Learning the mapping function requires to estimate the network parameters Θ firstly. This can be achieved by minimizing the loss between the reconstructed image s and the corresponding ground truth image s. The Mean Squared Error (MSE) is employed as the loss function,</p><formula xml:id="formula_16">Θ = {b 1 , • • • , b Mp/4 ; W 1 , W 2 , W 3 ; c 1 , c 2 , c 3 ; V 1 , V 2 , V 3 ; d 1 , d 2 , d 3 }</formula><formula xml:id="formula_17">= 1 L L l=1 s l (Θ) -s l 2 , (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where s l is the l th output of the network and s l is the corresponding ground truth. L is the number of training samples. The loss is minimized with the stochastic gradient descent (SGD) method <ref type="bibr" target="#b60">[61]</ref>. As shown in Figure <ref type="figure" target="#fig_7">8</ref>, the black arrows denote the training process to jointly learn the optimal coded aperture and parameters for HSI reconstruction. In the testing process denoted by red arrows, the learned optimal coded aperture is first used to code the HSIs and the underlying HSIs are reconstructed by using the HSI reconstruction network.</p><p>In our network, we set the mini-batch size of SGD to 128 and momentum parameter to 0.9. The weights for each convolution layer's are initialized by the method in <ref type="bibr" target="#b61">[62]</ref>. The initial learning rate is set to 10 -4 and divided 10 every 20 epochs. The network has been trained with the deep learning tool Caffe <ref type="bibr" target="#b62">[63]</ref> on a NVIDIA Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Generation</head><p>We evaluate our method on two public hyperspectral datasets, including the ICVL dataset <ref type="bibr" target="#b63">[64]</ref> and the Harvard dataset <ref type="bibr" target="#b64">[65]</ref>. The ICVL dataset consists of 200 HSIs. To eliminate the overfitting effect, we exclude 30 HSIs with similar background and 20 HSIs with similar contents. We randomly select 100 HSIs in the subsets for training and 50 HSIs of the rest for testing. Harvard dataset consists of 50 outdoor HSIs with distinct natural scenes captured under daylight illumination. We remove 6 HSIs which have highlight parts, and randomly use 35 HSIs for training and 9 HSIs for testing.</p><p>To formulate the training and validation datasets, oblique parallelepiped HSI patches with the size of 64×64 are uniformly extracted with the stride of 33. We randomly select 80% patches for training and 20% patches for validation. Note we conduct the experiment for multiple times, each time with shuffled data, and the results keep consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with State-of-the-art Methods</head><p>To verify the performance of the proposed method, we compare it with several competitive methods based on diverse priors and algorithms. Specifically, GPSR and AMP algorithms together with sparsity prior are usually employed to solve CASSI reconstruction problem <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b33">[34]</ref>. TwIST algorithm with TV prior is another choice <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Besides, the alternative direction multiplier method (ADMM) <ref type="bibr" target="#b65">[66]</ref> and constrained split augmented Lagrangian shrinkage algorithm (C-SALSA) <ref type="bibr" target="#b66">[67]</ref> are two of leading optimization algorithms. Although they are not officially proposed for CASSI, we introduce to use them to solve the TVbased problem as two of the benchmarks. The 3DNSR method the non-local similarity and sparsity prior into the CASSI reconstruction <ref type="bibr" target="#b19">[20]</ref>. HSCNN enhance the TwIST result with a CNN <ref type="bibr" target="#b52">[53]</ref>. All the compared method employ global random coded aperture as they reported previously. All the codes for the competitive methods are provided by the authors or implemented by ourselves with great efforts making on the parameter tuning to reach the best performance. For the proposed method, we define HyperReconNet-baseline as the basic method  which uses random coded aperture and only performs the HSI reconstruction with the network, and HyperReconNet as the method which jointly designs the coded aperture and reconstructs the HSI.</p><p>Three quantitative image quality metrics are utilized to evaluate the performance of these methods, including peak signal-to-noise ratio (PSNR), structural similarity (SSIM) <ref type="bibr" target="#b67">[68]</ref> and spectral angle mapping (SAM) <ref type="bibr" target="#b68">[69]</ref>. PSNR and SSIM are calculated on each 2D spatial image, and show the spatial fidelity between the reconstructed high resolution HSI and the ground truth. The larger values of PSNR and SSIM imply better performance. SAM is calculated on the 1D spectral vector, and shows the spectral fidelity. A smaller value of SAM suggests a better reconstruction.</p><p>In Table <ref type="table">I</ref>, we show the evaluation results for all HSIs in the testing sets from ICVL and Harvard datasets. On the one hand, it can be seen that our methods outperform all competitive methods. It implies the efficacy of CNN-based end-to-end HSI reconstruction. On the other hand, compared with baseline method, our method with optimal coded aperture achieves better reconstruction results (HyperReconNet). The PSNR further increases more than 1 dB on both ICVL and Harvard datasets. It demonstrates that joint coded aperture optimization can effectively improve the HSI reconstruction quality.</p><p>To visualize the experimental results for all methods, several 1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Computational Complexity</head><p>Besides the reconstruction method, patch-based reconstruction framework for CASSI has been proposed with the purpose of lower computational complexity <ref type="bibr" target="#b16">[17]</ref>. Actually, the proposed method also belongs to the framework of patch-based recon-   The time is recorded when reconstructing a patch with P = 64 and K = 31. All the codes are implemented on an Intel Core i7-6800K CPU and no specific parallel operation and code optimization are conducted. The proposed HyperReconNet method is 6-10 times faster than the TwIST and GPSR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects from Different Settings</head><p>1) Spatial and Spectral Correlation: In the spatial CNN, we employ the same convolution kernels for all the pixels in the same row to consider the spatial correlation. Besides, this architecture can also decrease the learned parameters. To evaluate the effect of spatial correlation, we turn to design the convolution kernels individually for each pixel in the spatial CNN. The second row in Table <ref type="table" target="#tab_4">III</ref> provides the evaluation results without considering spatial correlation. We can see that our method with considering spatial correlation achieves better reconstruction performance, where the PSNR increases about 2 dB on both ICVL and Harvard datasets (the last column in Table <ref type="table">I</ref>).</p><p>In the spectral CNN, we mainly exploit the spectral correlation among the neighboring bands. To show the efficacy of spectral correlation, we conduct comparative experiment to ignore the spectral correlation where each band is separately reconstructed. The results without spectral correlation are shown in Table <ref type="table" target="#tab_4">III</ref>. We can see that exploiting spectral correlation can effectively improve the reconstruction results and achieve better reconstruction performance in terms of all the metrics.</p><p>2) Overlapping Processing: To mitigate the block artifacts, we have employed the overlapping processing to reconstruct each patch and merge them to obtain the full HSI. As explained in Section III-C, the size of the overlapping range is related to the coded aperture design, so overlapping size is set as P 2 × P 2 × K here. To show the efficacy of the overlapping processing, we turn to learn an identical coded aperture for each patch, and then reconstruct each oblique parallelepiped patch without overlapping. Table <ref type="table" target="#tab_4">III</ref> provides the evaluation results for the reconstructed results without overlapping processing, which performs much worse than our results with overlapping processing.</p><p>To simultaneously show the results for all spectral bands, we integrate reconstructed HSI into a RGB image against the CIE 1964 color matching function. Figure <ref type="figure" target="#fig_10">11</ref> provides the reconstructed results under different settings for three typical scenes. We can see that our method can reconstruct high quality HSIs, which further demonstrates the efficacy of the spatial correlation, spectral correlation and overlapping processing.</p><p>3) Patch size: In the proposed method, we employ the divideand-conquer strategy that optimizes one patch of the coded aperture. Here, we test the influence of the patch size on Harvard dataset. The result is shown in Table <ref type="table" target="#tab_6">IV</ref>. It can be seen that larger patch size often leads to higher PSNR value and longer training time consequently. So it would be beneficial to increase the patch size.</p><p>In this paper, considering the 3D nature of the hyperspectral images, we select the biggest patch size (64 × 64) as the current computing resource (one NVIDIA Titan X GPU) can support. Theoretically in the extreme case with unlimited computing resources, the proposed method does support the optimization of full-resolution coded aperture, which would lead to better performance potentially. We would like to leave this as the further work in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extension to multi-frame CASSI</head><p>Despite of the snapshot advantage from the hardware design, the proposed method is also suitable for multi-frame CASSI, since the imaging principles for multi-frame CASSI and singleframe CASSI are identical as explained in Sec. III To test the efficacy of the proposed method, we conduct the experiment on multi-frame CASSI. Here, we show the results on 2-frame and 4-frame CASSI together with 1-frame CASSI in Table <ref type="table" target="#tab_6">V</ref>. We can see that our method works well under multi-frame CASSI, which demonstrates the expandability of our method. In this paper, we attempt to introduce the CNN with CASSI, which can simultaneously optimize the coded aperture and reconstruct the HSI. From the measurement phase, based on the nature of CASSI forward model, the entities of the coded aperture are optimized via being treated as the network weights. From the reconstruction phase, two sub-networks are designed to fully exploit the correlations across spatial and spectral dimensions. By leveraging the power of deep learning, the coded aperture optimization and the HSI reconstruction are connected into a unified framework. Experimental results show that our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and visual quality.</p><p>There are several issues that need further efforts. First direction comes from the motivation of this paper that we try to bridge the connection between the coded aperture optimization method and HSI reconstruction algorithm. In this paper, under the framework of deep learning, an implicit connection between the coded aperture and the reconstruction algorithm is built. However, compared with the rich theory behind CS, the connection seems somewhat brute-force and lacks complete mathematical explanation. Currently the optimized coded aperture obtained from HyperReconNet is not compatible with previous methods that are based on convex/non-convex optimization. For example, table VI shows the PSNR comparison between global random and optimal coded aperture on TwIST and 3DNSR on Harvard dataset. The reconstruction quality under optimized coded aperture is worse than that under global random coded aperture, which demonstrates the coded aperture should be optimized together with the reconstruction algorithm. Further effort is needed to explicitly model the connection with theoretical support by going deeper with the clue from this paper. And we would like to say, the method we proposed in this paper is complementary to, but not substitute for, the previous methods that are based on convex/nonconvex optimization. We believe it is meaningful to seek the relation between these two kinds of methods.</p><p>Second direction points to the network itself. This paper demonstrates the possibility of introducing CNN with CASSI. For the disadvantage of high training complexity, it must adopt the divide-and-conquer strategy, which may lead to sub-optimal result. Further effort is need to simplify the network architecture to account for full-resolution image training. For the advantage of low testing complexity, extensive efforts should be made on the platform of the real system. It is not trivial to deploy the method with portable computing devices, such as FPGA. Then by further integrating the portable computing device with the optical hardware system, it would speed up the update of the compressive hyperspectral imager towards practical application.</p><p>The main limitation of our method comes from the data mismatch dilemma between the training and the testing, as this problem exists in all the training-based methods for the tasks in the fields of image processing and computer vision. For reference, we perform a cross verification experiment where the training and the testing are conducted on different datasets. The results are shown in Table <ref type="table" target="#tab_6">VII</ref>. It can be seen that the performance decreases dramatically during cross verification. This is mainly because that the employed hyperspectral cameras for these two datasets are totally different, including the the manufacturers, camera models, spectral range and imaging manner, so the property of the images from these two datasets are heterogeneous <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Still we would like to pursue more robust method which could handle the diversity of camera configurations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The diagram of relation between coded aperture and the sensing matrix.The coded aperture is first reshaped into vertorized form, then repeated in the horizontal direction, each time with a uniform shift in the vertical direction with zero padding, as many times as the number of spectral band.</figDesc><graphic coords="3,446.93,71.34,121.80,60.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The patch-based forward model. (a) Each 3D HSI patch of size P × P × K corresponds to a 2D patch of size (P + K -1) × P which contains some information from the neighboring HSI patches and cannot map back to one individual HSI patch. (b) Each compressive patch corresponds to an oblique parallelepiped HSI patch, which is a one-to-one mapping between compressive patch and oblique parallelepiped HSI patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 corresponds to a HSI with 4 × 4 spatial points and 3 spectral bands (i.e., M = N = 4, K = 3). The sensing matrix contains a set of diagonal patterns (red circles) in horizontal direction. The number of the diagonal patterns is equivalent to the number of spectral band (here K = 3). Each diagonal pattern is the coded aperture after reshaped in vectorized form. Adjacent diagonal patterns are with a uniform shift in vertical direction with zero padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The CNN architecture for HSI reconstruction, which consists of the spatial CNN and spectral CNN to effectively exploit the correlation across the spatial and spectral dimensions.</figDesc><graphic coords="5,163.07,147.48,81.68,89.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An example of coded aperture for the first three bands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The learning principle of the coded aperture with overlapping setting. For an oblique parallelepiped HSI patch of size P × P × K, its lower part is first moved to the upper part, since these parts are modulated by the same coded aperture entity. Then, each 3D patch is divided into four sub-patches of size P 2 × P 2 × K. The same coded apertures are learned for these sub-patches. Finally, the encoded 3D HSI patch is rearranged to the oblique parallelepiped HSI patch which corresponds to one compressive patch in terms of the forward model of CASSI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Overview of the proposed method, which effectively combines the coded aperture optimization and HSI reconstruction into a unified framework. The black arrow shows the training process and the red arrow denotes the testing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visual quality comparison on ICVL dataset. The PSNR and SSIM results for each reconstructed band are shown in the brackets.</figDesc><graphic coords="8,50.76,432.57,91.51,91.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Visual quality comparison on Harvard dataset. The PSNR and SSIM results for each reconstructed band are shown in the brackets.</figDesc><graphic coords="9,50.76,432.57,91.51,91.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visual quality comparison under different network settings. The ground truth, reconstructed results without spatial correlation/spectral correlation/overlapping, and our results are shown from left to right columns. The corresponding PSNR and SAM values for each reconstructed HSI are provided.</figDesc><graphic coords="10,53.32,312.69,91.51,91.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Citation information: DOI 10.1109/TIP.2018.2884076, IEEE Transactions on Image Processing</figDesc><table><row><cell>GPSR</cell><cell>AMP</cell><cell>TwIST</cell><cell>C-SALSA</cell><cell>ADMM</cell></row><row><cell>(26.07 / 0.94)</cell><cell>(27.91 / 0.954)</cell><cell>(28.74 / 0.962)</cell><cell>(27.63 / 0.952)</cell><cell>(28.92 / 0.964)</cell></row><row><cell>3DNSR</cell><cell>HSCNN</cell><cell>HyperReconNet-Baseline</cell><cell>HyperReconNet</cell><cell>Ground Truth</cell></row><row><cell>(30.58 / 0.975)</cell><cell>(30.32 / 0.974)</cell><cell>(30.97 / 0.982)</cell><cell>(31.98 / 0.983)</cell><cell>(PSNR / SSIM)</cell></row><row><cell>GPSR</cell><cell>AMP</cell><cell>TwIST</cell><cell>C-SALSA</cell><cell>ADMM</cell></row><row><cell>(24.45 / 0.891)</cell><cell>(29.36 / 0.972)</cell><cell>(28.45 / 0.962)</cell><cell>(27.63 / 0.952)</cell><cell>(28.92 / 0.964)</cell></row><row><cell>3DNSR</cell><cell>HSCNN</cell><cell>HyperReconNet-Baseline</cell><cell>HyperReconNet</cell><cell>Ground Truth</cell></row><row><cell>(30.15 / 0.981)</cell><cell>(31.03 / 0.982)</cell><cell>(31.16 / 0.981)</cell><cell>(32.72 / 0.988)</cell><cell>(PSNR / SSIM)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III HYPERRECONNET</head><label>III</label><figDesc>PERFORMANCE UNDER DIFFERENT SETTINGS. Both GPSR and ADMM are iterative optimization algorithms, for which we analyze the complexity for each iteration and record time needed for all the iterations. TableIIshows the complexity for reconstructing each HSI patch with</figDesc><table><row><cell>Methods</cell><cell cols="5">ICVL PSNR SSIM SAM PSNR SSIM SAM Harvard</cell></row><row><cell>w/o spatial correlation</cell><cell>30.86</cell><cell>0.981 0.048</cell><cell>29.63</cell><cell>0.96</cell><cell>0.118</cell></row><row><cell>w/o spectral correlation</cell><cell>31.48</cell><cell>0.985 0.049</cell><cell>29.78</cell><cell>0.96</cell><cell>0.122</cell></row><row><cell>w/o Overlapping</cell><cell>31.78</cell><cell>0.984 0.040</cell><cell>30.06</cell><cell>0.88</cell><cell>0.123</cell></row><row><cell cols="6">complexity with the patch-based reconstruction framework. Orig-</cell></row><row><cell cols="6">inally, the patch-based reconstruction is demonstrated with GPSR</cell></row><row><cell cols="6">algorithm and sparsity prior 2 [21]. Further we select the ADMM</cell></row><row><cell cols="6">algorithm with TV prior as another representative method of the</cell></row><row><cell cols="3">patch-based reconstruction.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>size of P × P × K.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V PERFORMANCE</head><label>V</label><figDesc>OF THE PROPOSED METHOD ON MULTI-FRAME CASSI</figDesc><table><row><cell>Datasets</cell><cell>Evaluation Indexes</cell><cell cols="3">1 frame 2 frames 4 frames</cell></row><row><cell></cell><cell>PSNR</cell><cell>33.63</cell><cell>34.48</cell><cell>35.23</cell></row><row><cell>ICVL</cell><cell>SSIM</cell><cell>0.989</cell><cell>0.991</cell><cell>0.993</cell></row><row><cell></cell><cell>SAM</cell><cell>0.032</cell><cell>0.03</cell><cell>0.029</cell></row><row><cell></cell><cell>PSNR</cell><cell>31.35</cell><cell>31.81</cell><cell>32.46</cell></row><row><cell>Harvard</cell><cell>SSIM</cell><cell>0.972</cell><cell>0.974</cell><cell>0.978</cell></row><row><cell></cell><cell>SAM</cell><cell>0.104</cell><cell>0.103</cell><cell>0.101</cell></row><row><cell></cell><cell cols="3">V. CONCLUSION AND DISCUSSION</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2884076, IEEE Transactions on Image Processing</figDesc><table><row><cell></cell><cell cols="2">Table VII</cell><cell></cell></row><row><cell cols="4">CROSS VERIFICATION WITH DIFFERENT DATASETS</cell></row><row><cell cols="2">PSNR</cell><cell cols="2">Training dataset</cell></row><row><cell></cell><cell></cell><cell>ICVL</cell><cell>Harvard</cell></row><row><cell>Testing</cell><cell>ICVL</cell><cell>33.63</cell><cell>28.66</cell></row><row><cell>dataset</cell><cell>Harvard</cell><cell>29.37</cell><cell>31.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Table VI</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">PERFORMANCE COMPARISON BETWEEN THE OPTIMIZED AND RANDOM</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CODED APERTURE ON PREVIOUS ALGORITHMS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell>TwIST 3DNSR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Global random Coded aperture</cell><cell>26.15</cell><cell>27.95</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Optimized Coded aperture</cell><cell>20.53</cell><cell>21.07</cell></row></table><note><p>1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note for GPSR algorithm, the patch-based reconstruction would gain around 2dB improvement in PSNR compared with the full image reconstruction<ref type="bibr" target="#b20">[21]</ref>. Given the improvement, from TableIwe can deduce the proposed method still outperforms all the other methods according to the evaluative metrics</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61425013, Grant 61701025 and Grant 61672096, and in part by Beijing Municipal Science and Technology Project under Grant Z181100003018003.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Borengasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Hungate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyperspectral Remote Sensing: Principles and Applications</title>
		<title level="s">ser. Remote Sensing Applications Series</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral evidence for hydrated salts in recurring slope lineae on Mars</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Murchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mcewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Massé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chojnacki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Geoscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">829</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomaly detection in hyperspectral images based on low-rank and sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1990" to="2000" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Medical hyperspectral imaging: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Optics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10901</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards real-time medical diagnostics using hyperspectral imaging technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bjorgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Randeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical and Biomedical Spectroscopy and Imaging IV</title>
		<imprint>
			<biblScope unit="page">953712</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A system overview of the airborne visible/infrared imaging spectrometer (aviris)</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Enmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Technical Symposium</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hydice system: Implementation and performance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Basedow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Carmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE&apos;s Symposium on OE/Aerospace Sensing and Dual Use Photonics</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-fidelity video and stillimage communication based on spectral information: Natural vision system and its applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haneishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iwama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ohyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="60" to="620G" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized mosaicing: wide field of view multispectral imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1334" to="1348" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-image-format computed tomography imaging spectrometer for fluorescence microscopy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Descour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="444" to="453" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development of four-dimensional imaging spectrometers (4d-IS)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE Optics + Photonics</title>
		<meeting>of SPIE Optics + Photonics</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6302</biblScope>
			<biblScope unit="page" from="63" to="83M" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Snapshot Image Mapping Spectrometer (IMS) with high sampling density for hyperspectral microscopy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Kester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Tkaczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="14" to="330" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A prism-based system for multispectral bideo acquisition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2423" to="2435" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Singleshot compressive spectral imaging with a dual-disperser architecture</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single disperser design for coded aperture snapshot spectral imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagadarikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSA Applied Optics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressive coded aperture spectral imaging: An introduction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Colored coded aperture design in compressive spectral imaging via minimum coherence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parada-Mayorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="216" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting spectral-spatial correlation for coded hyperspectral image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3727" to="3736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive nonlocal sparse representation for dual-camera compressive hyperspectral imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2104" to="2111" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast lapped block reconstructions in compressive spectral imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSA Applied Optics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal spectral sensitivity functions for a single-camera one-shot multispectral imaging system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Monno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Image Processing</title>
		<meeting>of IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2137" to="2140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single-shot fourier transform multispectroscopy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Image Processing</title>
		<meeting>of IEEE International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4205" to="4209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computational snapshot multispectral cameras: Toward dynamic capture of the spectral world</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of snapshot multidimensional optical imaging: measuring photon tags in parallel</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual-camera design for coded aperture snapshot spectral imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSA Applied Optics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="848" to="858" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highspeed hyperspectral video acquisition by combining nyquist and compressive sampling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual-coded compressive hyperspectral imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Letters</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2044" to="2047" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial-spectral encoded compressive hyperspectral imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">233</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single disperser design for coded aperture snapshot spectral imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagadarikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSA Applied Optics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="44" to="B51" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new twist: Two-step iterative shrinkage/thresholding algorithms for image restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2992" to="3004" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiframe image estimation for coded aperture snapshot spectral imagers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kittle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagadarikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSA Applied Optics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="6824" to="6833" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compressive hyperspectral imaging via approximate message passing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="401" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind compressed sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gleichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6958" to="6975" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coded hyperspectral imaging and blind compressive sensing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajwade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kittle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Science (SIIMS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="812" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compressive hyperspectral imaging with side information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Llull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="964" to="976" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Compressed sensing: theory and applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rank minimization code aperture design for spectrally selective compressive imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="941" to="954" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Code aperture optimization for spectrally agile compressive imaging</title>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Soceity of America A (JOSA A)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2400" to="2413" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compressive spectral imaging based on colored coded apertures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7799" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spectral super-resolution in colored coded aperture spectral imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parada-Mayorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="440" to="455" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectral image classification from optimal coded-aperture compressive measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3299" to="3309" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spectral image unmixing from optimal coded-aperture compressive measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="405" to="415" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Colored coded aperture design by concentration of measure in compressive spectral imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Arce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1896" to="1908" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4311</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compressive sensing by learning a gaussian mixture model from measurements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Llull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single-image super resolution for multispectral remote sensing data using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Archives of the Photogrammetry Remote Sensing &amp; S</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="883" to="890" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision (ICCV)</title>
		<meeting>of International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hyperspectral image superresolution by transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of selected topics in applied earth observations and remote sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1963" to="1974" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hyperspectral image superresolution using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hyperspectral image reconstruction by deep convolutional neural network for classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hscnn: Cnn-based hyperspectral image recovery from spectrally undersampled projections</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Computer Vision Workshops</title>
		<meeting>of International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-quality hyperspectral reconstruction using a spectral prior</title>
		<author>
			<persName><forename type="first">I</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reconnet: Non-iterative reconstruction of images from compressively sensed measurements</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kerviche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Using deep neural networks for inverse problems in imaging: beyond analytical methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hyperspectral image denoising employing a spectral-spatial adaptive total variation model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3660" to="3677" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>of International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of ACM international conference on Multimedia</title>
		<meeting>.of ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sparse recovery of hyperspectral signal from natural rgb images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Arad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ben-Shahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Computer Vision (ECCV)</title>
		<meeting>of European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Statistics of real-world hyperspectral images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="681" to="695" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Lizhi Wang received his B.S. degree and Ph.D. degree from Xidian University in 2011 and 2016, respectively. He is currently a postdoctoral researcher with the School of Computer Science and Technology, Beijing Institute of Technology. His research interests include computational photography and image processing</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lefkoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Boardman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Heidebrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Barloon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F H</forename><surname>Goetz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="145" to="163" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Beijing Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Tao Zhang received the B.S. degree from Beijing Institute of Technology in 2017. His research interests include deep learning, image processing and computational photography</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">D. degree in information science and technology from the University of Tokyo in 2015. She is currently a professor with the School of Computer Science and Technology, Beijing Institute of Technology. Her research interests include physics-based vision, image processing</title>
	</analytic>
	<monogr>
		<title level="m">Ying Fu received the B.S. degree in Electronic Engineering from Xidian University in 2009, the M.S. degree in Automation from Tsinghua University in 2012, and the Ph</title>
		<imprint/>
	</monogr>
	<note>and computational photography</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
