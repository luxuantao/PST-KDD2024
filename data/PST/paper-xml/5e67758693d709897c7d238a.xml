<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised learning in spiking neural networks: A review of algorithms and evaluations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangwen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xianghong</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaochao</forename><surname>Dang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neural</forename><surname>Networks</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwest Normal University</orgName>
								<address>
									<postCode>730070</postCode>
									<settlement>Lanzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwest Normal University</orgName>
								<address>
									<postCode>730070</postCode>
									<settlement>Lanzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science and Engineering</orgName>
								<orgName type="institution">Northwest Normal University</orgName>
								<address>
									<postCode>730070</postCode>
									<settlement>Lanzhou</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised learning in spiking neural networks: A review of algorithms and evaluations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E16F079C5F0861996AED2E6B6D48C32</idno>
					<idno type="DOI">10.1016/j.neunet.2020.02.011</idno>
					<note type="submission">Manuscript submitted to Neural Networks</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spiking neural network</term>
					<term>Spike train</term>
					<term>Spiking neuron</term>
					<term>Supervised learning</term>
					<term>Performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Artificial neural networks (ANNs) are abstractions and simulations of the structure and function of the biological nervous system <ref type="bibr" target="#b0">[1]</ref>. Traditional ANNs encode neural information by the spike firing rate of the biological neurons <ref type="bibr" target="#b1">[2]</ref>, in which the inputs and outputs of neurons are generally expressed as analog variables. However, experimental evidence from neuroscience suggests that biological nervous systems encode information through the precise timing of spikes, not only through the neuronal firing rate <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Using a biologically plausible spiking neuron model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> as the basic unit for constructing spiking neural networks (SNNs), they encode and process neural information through precisely timed spike trains. SNNs are often referred to as the third generation of neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. They have a greater computing capacity to simulate a variety of neuronal signals and approximate any continuous function <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and have been shown to be suitable tools for processing spatiotemporal information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Supervised learning in ANNs involves a mechanism of providing the desired outputs with the corresponding inputs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Experimental studies have shown that supervised learning exists in the biological nervous system <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, but there is no clear conclusion to explain how biological neurons realize this process. For SNNs, the internal state variable of a neuron and the error function between the actual and desired output spike trains no longer satisfy the property of continuous differentiability. Consequently, the supervised learning algorithms for the traditional ANNs, such as the error back-propagation algorithm <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, cannot be used directly for SNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof Supervised learning for SNNs is a significant research field. Researchers have conducted many studies on supervised learning for SNNs and achieved some results <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The supervised learning algorithms for SNNs proposed in recent years can be divided into several categories from different perspectives, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. From the perspective of applicability to network architecture, they can be divided into supervised learning algorithms for single-layer SNNs, multilayer feed-forward SNNs, and recurrent SNNs. From the perspective of the running mode, they can be divided into online learning algorithms and offline learning algorithms (or batch learning algorithms) <ref type="bibr" target="#b24">[25]</ref>. From the perspective of information encoding, they can be divided into producing a single spike and producing a spike train as outputs in response to input temporal or spatiotemporal data. From the perspective of structural dynamics, they can be divided into learning in fixed SNN structures and learning in evolving SNN structures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. From the perspective of knowledge representation, they can be divided into non-knowledge-based learning and learning for knowledge representation <ref type="bibr" target="#b25">[26]</ref>. In Section 2, a comparison between SNNs and traditional ANNs is provided. In Section 3, some basic theories of supervised learning for SNNs are introduced, including the general framework and some related theories. In Sections 4-6, the state-of-the-art supervised learning algorithms for different SNN architectures are reviewed. A performance comparison of spike train learning of some representative algorithms is done in the corresponding sections. Section 7 presents the qualitative performance evaluation and taxonomy of supervised learning algorithms for SNNs. Section 8 outlines some future research directions in this research field. The conclusions for this article are presented in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Comparison between SNNs and traditional ANNs</head><p>ANNs are abstractions and simulations of the structure and function of the biological nervous system, and play important roles in information processing and pattern recognition. An ANN is a computational model consisting of neurons as basic computational units. Information exchange between neurons is accomplished through synapses. According to their computational units, ANN models can be divided into three different generations <ref type="bibr" target="#b9">[10]</ref>, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>First-generation ANNs use McCulloch-Pitts neurons <ref type="bibr" target="#b28">[29]</ref> as computational units, whose characteristic feature is that the output value is a binary variable.</p><p>A McCulloch-Pitts neuron is the simplest neuron model, and can be described as</p><formula xml:id="formula_0">y = f ( NI ∑ i=1 w i x i -θ ) =    1 if ∑ NI i=1 w i x i -θ ≥ 0, 0 if ∑ NI i=1 w i x i -θ &lt; 0,<label>(1)</label></formula><p>where y ∈ {0, 1} is the output of the McCulloch-Pitts neuron, f (•) is the activation function or state transition function, N I is the number of input neurons,</p><p>x i ∈ R is the input of neuron i, w i ∈ R is the synaptic weight between the input neuron i and the output neuron, and θ ∈ R is the activation threshold. The representative computational model of first-generation ANNs is the perceptron <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Second-generation ANNs use artificial neurons as computational units that apply a continuous function as the activation function of neurons to realize the processing of real numerical input and output. The artificial neuron can be described as</p><formula xml:id="formula_1">y = f ( NI ∑ i=1 w i x i + b ) ,<label>(2)</label></formula><p>where b ∈ R is the bias. This mode of information processing is a rate encoding scheme. The representative computational model is the feed-forward back-propagation neural network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Remarkable achievements in many fields have been made with second-generation ANNs. Sometimes traditional ANNs are referred to as second-generation ANNs. However, the computational power of the artificial neuron still does not reach its full potential because the temporal information on individual spikes is not represented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof</p><p>Third-generation ANNs are SNNs that use biologically plausible spiking neurons as the basic computational units. The spiking neuron can be described as a hybrid system formalism <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_2">   dX dt = f (X ), X ← g i (X ),<label>(3)</label></formula><p>where X is a vector consisting of the state variables of the neuron, f (•) represents the differential equations for the evolution of the state variables, and g i (•)</p><p>represents the change of the state variables caused by spike events from synapse i. Neural information in the spiking neuron is transmitted and processed by the precisely timed spike trains. This is a temporal encoding scheme. Compared with the first-and second-generation ANN models, SNNs can describe the real biological nervous system more accurately, so as to achieve efficient information processing.</p><p>Table <ref type="table" target="#tab_0">1</ref> provides a brief comparison between SNNs and traditional ANNs from the following four aspects <ref type="bibr" target="#b25">[26]</ref>:</p><p>1. Information encoding and representation. SNNs use the temporal encoding scheme to encode the information into spike trains, while ANNs use the rate encoding scheme to encode the information into scalars. This is one of the fundamental differences between SNNs and ANNs. Therefore, SNNs have more powerful information representation ability than ANNs, especially for complex temporal or spatiotemporal data.</p><p>2. Computational unit and network simulation. The basic computational unit for an SNN is the spiking neuron, which is expressed by differential equations, whereas the basic computational unit for an ANN is the artificial neuron, in which the inputs are processed by an activation function.</p><p>This is another fundamental difference between SNNs and ANNs. The corresponding simulation strategies for SNNs are mainly clock-driven and event-driven, while the simulation strategy for ANNs is a step-by-step simulation process. SNNs use the discrete spike train instead of analog signals to transmit information, which is more suitable for hardware implementation with lower energy consumption <ref type="bibr" target="#b32">[33]</ref>.</p><p>J o u r n a l P r e -p r o o f  </p><formula xml:id="formula_3">= {t f ∈ Γ : f = 1, • • • , F }</formula><p>is the ordered sequence of spike times at which a spiking neuron fires in the time interval Γ = [0, T ], and can be expressed formally as</p><formula xml:id="formula_4">s(t) = F ∑ f =1 δ ( t -t f ) ,<label>(4)</label></formula><p>where t f is the f th spike time in s(t), F is the number of spikes, and δ(•)</p><p>represents the Dirac delta function: δ(x) = 1 if x = 0 and δ(x) = 0 otherwise.</p><p>Although the various supervised learning algorithms for SNNs are different, the overall goal of them is consistent: for a given set of input spike trains S i and desired output spike trains S d , finding the appropriate synaptic weight matrix W for SNNs to make the actual output spike trains S a as similar as possible to the corresponding desired output spike trains S d ; that is, the value of the error evaluation function E (S a , S d ) between them is a minimum. In the supervised learning process, the output spike trains S a are determined by the input spike trains S i and the synaptic weight matrix W connected to them, which can be expressed formally as</p><formula xml:id="formula_5">S a = F (S i , W ), (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where F is a functional relationship between S i and W . Therefore, supervised learning for SNNs can be defined as</p><formula xml:id="formula_7">min E (S a , S d ) = min NO ∑ m=1 |s m d (t) -s m a (t)| ,<label>(6)</label></formula><p>where N O is the number of output neurons, and |s m d (t)s m a (t)| is the similarity distance between s m d (t) and s m a (t). That means supervised learning is a process of minimizing the total error of the network, where the similarity measurement between spike trains can be used to calculate the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal Pre-proof</head><p>Assuming that an SNN contains N I input neurons and N O output neurons, the general framework of supervised learning for the SNN is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>Starting with the initial synaptic weight matrix W generated randomly, each supervised learning process for the SNN can be divided into the following four stages:</p><p>1. Encoding the input data into spike trains s n i (t) ∈ S i through some specific coding methods.</p><p>2. Inputting the spike trains s n i (t) into the SNN, and then using a certain simulation strategy to run the network. Finally, the actual output spike trains s m a (t) ∈ S a can be obtained. 3. According to the desired output spike trains s m d (t) ∈ S d , computing the value of the error function E (S a , S d ) and adjusting the synaptic weights:</p><formula xml:id="formula_8">W ← W + ∆W .</formula><p>4. Determining whether the error of the SNN obtained by the learning process has reached a predetermined minimum error or the upper limit of learning epochs has been exceeded. If the termination condition is not satisfied, the learning process is repeated. After supervised learning, the output spike trains S a of the network are decoded by a specific decoding method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Related theories of supervised learning</head><p>The key to a supervised learning algorithm for SNNs is to construct the proper learning rule of synaptic weights. Meanwhile, it is also influenced by other related theories <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. These methods and techniques are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Neural information encoding and decoding</head><p>Since the input and output of SNNs are spike trains, the analog quantities cannot be calculated directly. The first consideration is the encoding and decoding mechanism for neural information <ref type="bibr" target="#b35">[36]</ref>. The encoding of input and output J o u r n a l P r e -p r o o f  spike trains has a great influence on the performance of supervised learning algorithms for SNNs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Encoding refers to converting sample data or a stimulus signal into spike trains, while decoding is the reverse process of encoding <ref type="bibr" target="#b38">[39]</ref>, mapping the spike trains to output or a particular response. By referencing the encoding mechanism of biological neurons to specific stimulus signals, researchers have provided many temporal encoding strategies <ref type="bibr" target="#b39">[40]</ref>, such as the time-to-first-spike method <ref type="bibr" target="#b40">[41]</ref>, the latency-phase method [42], the population encoding method <ref type="bibr" target="#b41">[43]</ref>, and Ben's spiker algorithm <ref type="bibr" target="#b42">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Computational models of spiking neurons</head><p>Spiking neurons are the basic computational units of SNNs. To simulate the function of the nervous system and solve practical problems using SNNs, it is of great importance to model suitable spiking neuron models. Depending on the complexity, the computational model of the spiking neuron can be divided into three categories <ref type="bibr" target="#b43">[45]</ref>: physiological models with biological plausibility, nonlinear models with a spiking mechanism, and linear models with a fixed threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof Some spiking neuron models, such as the Hodgkin-Huxley model <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b45">47]</ref>, are too complex, so it is difficult to realize large-scale SNN simulation with them, whereas some simple spiking neuron models, such as the integrate-and-fire neuron model <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b47">49]</ref> and the spike response model (SRM) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">50]</ref> with analytic expressions are commonly used in SNN simulation, especially in supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Simulation strategy for SNNs</head><p>Since the spiking neuron is represented as a hybrid system through the continuous system and discrete spike events, simulation of SNNs is different from that of the traditional ANNs. Generally, two simulation strategies are used for SNNs: a clock-driven simulation strategy <ref type="bibr" target="#b49">[51]</ref> and an event-driven simulation strategy <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b51">53]</ref>. Some hybrid simulation strategies have also been proposed in recent years <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b55">57]</ref>. Research results show that different simulation strategies will affect the dynamic characteristics and learning performance of SNNs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Similarity measure for spike trains</head><p>Similarity measure for spike trains is a quantitative method to calculate the degree of similarity between spike trains. Researchers have already provided some different measurement methods <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b60">62]</ref>. In the supervised learning process of SNNs, similarity measure between the actual and desired output spike trains is needed to calculate their errors. Through the calculation of spike train errors, on the one hand, the accuracy of supervised learning is calculated, and when the error is less than a given value, the process of learning iteration is ended. On the other hand, in some supervised learning algorithms (e.g., gradient-descent-based supervised learning algorithms), the defined specific error function is applied to the derivation of learning rules. There is only one layer of synaptic weight in single-layer SNNs. Therefore, supervised learning for single-layer SNNs is actually for spiking neurons. Because of its simple architecture, a single-layer SNN is widely used in SNN simulation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning algorithms for single-layer SNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Perceptron-based algorithms</head><p>Supervised learning for a spiking neuron is equivalent to correctly distinguishing the times of desired output spikes and the other times during the running process of the neuron by adjusting the synaptic weights. This is actually a classification problem, so the perceptron <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> can be used to solve this problem <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b62">64]</ref>.</p><p>Gütig and Sompolinsky <ref type="bibr" target="#b63">[65]</ref> proposed a biologically plausible supervised synaptic learning rule named tempotron, in which the spiking neurons realize the optimization of the synaptic weights by minimizing the error between the J o u r n a l P r e -p r o o f Journal Pre-proof actual output potential and the desired output potential. The synaptic weight adjustment with the tempotron learning rule is expressed as</p><formula xml:id="formula_9">- dE ± dw i = ± ∑ ti&lt;tmax K(∆t i ) ± ∂V (t max ) ∂t max dt max dw i ,<label>(7)</label></formula><p>where t i is the spike time of the ith afferent neuron, K(t) is the normalized postsynaptic potential, t max is the time at which the postsynaptic potential reaches its maximum value, and ∆t i = t maxt i is the time difference between the activation of a synapse and the voltage maximum. By training synaptic weights of integrate-and-fire neurons in the input layer, the tempotron successfully implements the classification of single-spike spatiotemporal patterns.</p><p>Urbanczik and Sen <ref type="bibr" target="#b64">[66]</ref> derived another supervised learning rule for the tempotron task using the gradient descent rule. Experimental results showed that the new algorithm converges more quickly and reliably than the original tempotron rule, and does not rely on a specific reset mechanism. Yu et al.</p><p>[67] used the tempotron learning rule to recognize images of digits in the M-NIST database. Zhao et al. <ref type="bibr" target="#b66">[68]</ref> simulated the tempotron with the event-driven method. Inspired by the tempotron, Roy et al. <ref type="bibr" target="#b67">[69]</ref> proposed a morphological learning algorithm that can be used to find the optimal morphology of neurons with nonlinear dendrites and binary synapses.</p><p>Xu et al. <ref type="bibr" target="#b61">[63]</ref> proposed a perceptron-based spiking neuron learning rule (PBSNLR) with temporal encoding. It transforms supervised learning into a classification problem and then solves the problem by using the perceptron learning rule. The PBSNLR can be expressed as</p><formula xml:id="formula_10">w new i =          w old i + ηP td i if d td = 1 and a td = 0, w old i -ηP td i if d td = 0 and a td = 1,</formula><formula xml:id="formula_11">w old i if d td = a td ,<label>(8)</label></formula><p>where w i is the weight of the ith synapse, η is the learning rate, P td i is the sum of postsynaptic potentials induced by all the spikes that have arrived through the ith synapse at t d , and d td and a td are signals to represent whether t d is the desired or actual output spike time, respectively. Qu et al. <ref type="bibr" target="#b68">[70]</ref> further improved J o u r n a l P r e -p r o o f the PBSNLR by use of dynamic learning parameters with distance items and a better method for choosing negative samples. Then they used the improved PBSNLR for real-time user authentication. Zhang et al. <ref type="bibr" target="#b69">[71]</ref> analyzed the strategies used to make spiking neurons robust to noise and proposed a training strategy that trains spiking neurons with a dynamic firing threshold named noise threshold to improve the noise tolerance of the PBSNLR. The noise threshold can be applied by other supervised learning methods for spiking neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Synaptic plasticity algorithms</head><p>Synaptic plasticity plays an important role in learning and memory <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b71">73]</ref>.</p><p>Hebb <ref type="bibr" target="#b72">[74]</ref> first proposed a synaptic plasticity hypothesis, which emphasizes the importance of the synergistic activity between presynaptic and postsynaptic neurons on increasing synaptic potentiation <ref type="bibr" target="#b73">[75]</ref>. The mathematical model of the Hebb learning rule can be expressed as</p><formula xml:id="formula_12">∆w ij ∝ v i v j ,<label>(9)</label></formula><p>where v i and v j are neuronal activities of presynaptic and postsynaptic neurons, respectively. The spike train can not only cause persistent changes in synaptic strength but also satisfies the STDP learning rule <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b75">77]</ref>. The STDP learning window for synaptic changes can be expressed mathematically as</p><formula xml:id="formula_13">W (s) ST DP =    +A + exp(-s τ+ ) if s ≥ 0, -A -exp( s τ-) if s &lt; 0,<label>(10)</label></formula><p>where s = t postt pre is the time delay between the presynaptic spike arrival time t pre and the postsynaptic firing time t post , A + and A -are the amplitudes, and τ + and τ -are the time constants of the learning window.</p><p>On the basis of the STDP learning mechanism, researchers have proposed many biologically plausible supervised learning algorithms for single-layer SNNs.</p><p>Ruf and Schmitt <ref type="bibr" target="#b76">[78]</ref> first provided a supervised Hebbian learning algorithm based on the spike firing time. However, this algorithm can learn only a single spike. Legenstein et al. <ref type="bibr" target="#b77">[79]</ref> used the STDP mechanism to propose a supervised Hebbian learning algorithm for spiking neurons. In this algorithm, the external J o u r n a l P r e -p r o o f injected synaptic current is used as a teacher signal. They further provided tools for the analytic treatment of reward-modulated STDP <ref type="bibr" target="#b78">[80]</ref>, which allows us to predict under what conditions reward-modulated STDP will achieve the desired learning effect <ref type="bibr" target="#b79">[81,</ref><ref type="bibr" target="#b80">82]</ref>. Similar work was presented in <ref type="bibr" target="#b81">[83]</ref>. Gardner and</p><p>Grüning <ref type="bibr" target="#b82">[84]</ref> implemented a stochastic neuron model and investigated its ability to learn a target spike train in response to a spatiotemporal spiking pattern through a reward-modulated STDP rule. Franosch et al. <ref type="bibr" target="#b83">[85]</ref> proved that under very general conditions, supervised STDP converges to a stable configuration of synaptic weight leading to a reconstruction of primary sensory input. Jeyasothy Ponulak and Kasiński <ref type="bibr" target="#b85">[87]</ref> represented the adjustment of synaptic weight as a combination of STDP and anti-STDP and proposed a remote supervised method (ReSuMe) for spiking neurons. The adjustment of synaptic weight can be expressed as</p><formula xml:id="formula_14">∆w i (t) = [s d (t) -s a (t)] [ a + ∫ ∞ 0 W (s)s i (t -s)ds ] ,<label>(11)</label></formula><p>where s i (t), s a (t), and s d (t) are the input spike train, the actual output spike train, and the desired output spike train, respectively, and a is the non-Hebbian item to accelerate the convergence of the training process. The integral kernel W (s) defines the synaptic plasticity determined by the spike time correlation.</p><p>The ReSuMe algorithm can be applied to various neuron models <ref type="bibr" target="#b86">[88]</ref>. However, the ReSuMe algorithm can learn the synaptic weight only of the output layer of the liquid state machine, in which the hidden layer with recurrent architecture is a static network. The ReSuMe algorithm has been used in various spatiotemporal pattern classification and recognition problems <ref type="bibr" target="#b87">[89,</ref><ref type="bibr" target="#b88">90]</ref>.</p><p>In recent years, Taherkhani et al. <ref type="bibr" target="#b89">[91]</ref> proposed a delay learning remote supervised method (DL-ReSuMe) for spiking neurons to merge the delay shift approach and the ReSuMe-based weight adjustment to improve the learning J o u r n a l P r e -p r o o f performance. In DL-ReSuMe, synaptic weights are updated by the delayed version of the ReSuMe rule, which is expressed as</p><formula xml:id="formula_15">∆w i (t) = [s d (t) -s a (t)] [ a + ∫ ∞ 0 W (s)s i (t -dt i -s)ds ] ,<label>(12)</label></formula><p>where dt i is the synaptic delay of the ith synapse. They further extended DL- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Spike train convolution algorithms</head><p>Since the spike train is a set consisting of discrete spike events, to facilitate analysis and calculation, a symmetric and positive definite kernel function κ(t)</p><p>[96] can be chosen to convert the spike train to a unique continuous function by use of the convolution</p><formula xml:id="formula_16">f s (t) = s(t) * κ(t) = F ∑ f =1 κ ( t -t f ) .<label>(13)</label></formula><p>For any two given spike trains s i (t) and s j (t), the inner product of the corresponding continuous functions f si (t) and f sj (t) can be defined in L 2 (Γ) space as [97]</p><formula xml:id="formula_17">F (s i (t), s j (t)) = ⟨ f si (t), f sj (t) ⟩ L2(Γ) = ∫ Γ f si (t)f sj (t)dt. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>Using the inner product representation of spike times, one can further rewrite Eq. ( <ref type="formula" target="#formula_17">14</ref>) as the accumulation form of spike time pairs with order O(F i F j ):</p><formula xml:id="formula_19">F (s i (t), s j (t)) = Fi ∑ m=1 Fj ∑ n=1 ∫ Γ κ (t -t m i ) κ ( t -t n j ) dt = Fi ∑ m=1 Fj ∑ n=1 κ ( t m i , t n j ) ,<label>(15)</label></formula><p>J o u r n a l P r e -p r o o f where t m i and t n j are spike times in the spike trains s i (t) and s j (t), respectively, and F i and F j are the numbers of spikes in s i (t) and s j (t), respectively. Because F (s i (t), s j (t)) has symmetry and positivity properties, according to the Moore-Aronszajn theorem, there exists a reproducing kernel Hilbert space (RKHS) H F induced by the spike train inner product F <ref type="bibr" target="#b95">[97]</ref>. With use of the inner products of spike trains, a formal definition of the similarity measure of spike trains can be obtained <ref type="bibr" target="#b96">[98]</ref>, which is the basis of an error function for supervised learning in SNNs. <ref type="bibr" target="#b97">[99]</ref> used the linear algebra method to learn the spatiotemporal patterns of spike trains, in which the adjustment of synaptic weight is calculated by the projection defined through the Gram-Schmidt process <ref type="bibr" target="#b98">[100]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Carnell and Richardson</head><p>Later, Mohemmed et al. <ref type="bibr" target="#b99">[101,</ref><ref type="bibr" target="#b100">102]</ref> proposed a spike pattern association neuron (SPAN) supervised learning algorithm for learning spatiotemporal spike patterns. The idea of the SPAN algorithm is to transform the spike trains during the learning phase into analog signals by an α-kernel function so that common mathematical operations can be performed on them. The adjustment of synaptic weight with the SPAN algorithm is expressed as</p><formula xml:id="formula_20">∆w i (t) = ηf si (t) [f sd (t) -f sa (t)] ,<label>(16)</label></formula><p>where f si (t), f sa (t), and f sd (t) are the convolved continuous functions corresponding to the input spike train, the actual output spike train, and the desired output spike train, respectively. The SPAN algorithm can also realize an offline or incremental learning process <ref type="bibr" target="#b101">[103]</ref>. Inspired by the SPAN algorithm, Yu et al. <ref type="bibr" target="#b102">[104]</ref> proposed a precise-spike-driven (PSD) supervised learning rule that can be used to train neurons to associate an input spatiotemporal spike pattern with a desired spike train. The PSD learning rule convolves the input spike trains only by the double-exponential kernel function. In the experiment, the PSD algorithm was applied to optical character recognition and a more complex recognition problem involving handwritten digits <ref type="bibr" target="#b103">[105,</ref><ref type="bibr" target="#b104">106]</ref>.</p><p>m. The adjustment of synaptic weights in the STKLR algorithm is expressed as</p><formula xml:id="formula_21">∆w i = η   Fd ∑ g=1 Fi ∑ f =1 κ ( t g d , t f i ) - Fa ∑ h=1 Fi ∑ f =1 κ ( t h a , t f i )   , (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>where η is the learning rate. t f i , t h a , and t g d are the spikes in the input spike train, the actual output spike train, and the desired output spike train, respectively, and κ is the kernel function. They then used the STKLR algorithm to solve an image recognition and classification problem <ref type="bibr" target="#b106">[108]</ref>. Further, they proposed some other related supervised learning algorithms <ref type="bibr" target="#b107">[109,</ref><ref type="bibr" target="#b108">110,</ref><ref type="bibr" target="#b109">111]</ref> based on the STKLR using the online learning mechanism, nonlinear spike train kernels, and the delay learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Other supervised learning algorithms</head><p>Pfister et al. <ref type="bibr" target="#b110">[112]</ref> derived a supervised spike-based learning algorithm starting with statistical learning criteria and neural spike train analysis <ref type="bibr" target="#b111">[113]</ref>. They used a supervised learning paradigm to derive the synaptic weight updating rule that optimizes the likelihood of postsynaptic firing at one or several desired spike times by gradient ascent. On the basis of the statistical method of Pfister et al. <ref type="bibr" target="#b110">[112]</ref>, Gardner and Grüning <ref type="bibr" target="#b112">[114]</ref> proposed two spike-based learning methods: one that relies on an instantaneous error signal to update synaptic weights in a network (INST rule) and one that relies on a filtered error signal for smoother synaptic weight modifications (FILT rule).</p><p>Inspired by learning rules for locally recurrent analog neural networks, Schrauwen and Van Campenhout <ref type="bibr" target="#b113">[115]</ref> presented a new learning rule for spiking neurons that uses the general population-temporal encoding model. The proposed method can learn very fast and can operate on a wide class of decoding schemes. Dora et al. <ref type="bibr" target="#b114">[116,</ref><ref type="bibr" target="#b115">117]</ref> presented a biologically inspired structure learning algorithm for single-layer SNNs to solve pattern classification problems using population encoding. The learning algorithm can choose either to add a new output neuron or to update the synaptic weight of existing output neurons. On the basis of a rank-order-based learning rule <ref type="bibr" target="#b116">[118,</ref><ref type="bibr" target="#b117">119]</ref>, Wang et al. <ref type="bibr" target="#b118">[120]</ref> presented J o u r n a l P r e -p r o o f an enhanced rank-order-based learning algorithm, called SpikeTemp, for singlelayer SNNs with a dynamically adaptive structure.</p><p>Tapson et al. <ref type="bibr" target="#b119">[121]</ref> proposed a synaptic kernel inverse method (SKIM) for spiking neurons. The pseudo-inverse method or any similar convex optimization can be used to train the SKIM algorithm, so it may be an online, adaptable, and biologically plausible supervised learning algorithm. Lee et al. <ref type="bibr" target="#b120">[122]</ref> proposed two novel convex-optimized synaptic efficacy (CONE) algorithms to estimate the weight for spiking neurons in a convex optimization framework, which is similar to the SKIM. Their main contribution is to convert the supervised learning problem for spiking neurons into a standard optimization problem.</p><p>Zhang et al. <ref type="bibr" target="#b121">[123,</ref><ref type="bibr" target="#b122">124]</ref> presented an efficient membrane-potential-driven supervised learning algorithm named MemPo-Learn for spiking neurons. At the desired output time, the gradient descent method is implemented to minimize the error function defined as the difference between the membrane potential and the firing threshold:</p><formula xml:id="formula_23">E = 1 2 [u i (t) -V thresh ] 2 , (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>where u i (t) is the membrane potential of the neuron and V thresh is the firing threshold. At an undesired output time, synaptic weights are adjusted to make the membrane potential below the threshold, and the error function is defined as</p><formula xml:id="formula_25">E = 1 2 [u i (t) -(V thresh -p)] 2 ,<label>(19)</label></formula><p>where p determines the magnitude of modification of the synaptic weight. Similar studies are presented in <ref type="bibr" target="#b123">[125,</ref><ref type="bibr" target="#b124">126,</ref><ref type="bibr" target="#b125">127]</ref>. Yu et al. <ref type="bibr" target="#b126">[128]</ref> proposed several threshold-driven plasticity rules to train neurons to fire the desired number of output spikes in response to input patterns, which can be used to process spike patterns that are encoded with both a rate coding and a temporal coding. Property proofs of robustness and convergence have also been provided. Motivated by the selective attention mechanism of the primate visual system, Xie et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance comparison of spike train learning</head><p>In this subsection, the spike train learning performances of some representative supervised learning algorithms for single-layer SNNs are compared. The ReSuMe <ref type="bibr" target="#b85">[87]</ref>, SPAN <ref type="bibr" target="#b99">[101]</ref>, PSD <ref type="bibr" target="#b102">[104]</ref>, and STKLR <ref type="bibr" target="#b105">[107]</ref> algorithms are selected.</p><p>In addition, the supervised learning algorithm for multilayer feed-forward SNNs proposed by Xu et al. <ref type="bibr" target="#b128">[130]</ref>, which is a typical gradient-descent-based super-      </p><formula xml:id="formula_26">vised</formula><formula xml:id="formula_27">= -η∇E = -η ∂E ∂w , (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>where η is the learning rate and ∇E is the gradient of the error function E to the synaptic weight w.</p><p>Drawing lessons from the back-propagation algorithm <ref type="bibr" target="#b20">[21]</ref> in traditional ANNs, Bohte et al. <ref type="bibr" target="#b131">[133]</ref> first proposed a back-propagation training algorithm for multilayer feed-forward SNNs, called SpikeProp, in which the SRM is used.</p><p>To overcome the discontinuity of the internal state variable caused by spike firing, they approximated the thresholding function. All neurons in the SNNs are limited to fire only a single spike. Yang et al. <ref type="bibr" target="#b132">[134]</ref> further proved that the assumption of Bohte et al. is mathematically correct. The error function of SNNs in the form of the least mean square is defined as</p><formula xml:id="formula_29">E = 1 2 NO ∑ o=1 ( t a o -t d o ) 2 ,<label>(21)</label></formula><p>where t a o and t d o are output and desired spikes of the oth output neuron in the output layer, respectively. For a multiple-synapse model, the adjustment of the kth synaptic weight between the presynaptic neuron i and the postsynaptic neuron j is expressed as</p><formula xml:id="formula_30">∆w k ij = -η ∂E ∂w k ik = -ηy k i (t a j )δ j ,<label>(22)</label></formula><p>where y k i (t a j ) is the postsynaptic potential without synaptic weight and δ j represents the different gradient descent learning rule depending on in which layer the synapse is located. Experimental results showed that the SpikeProp algorithm can solve the nonlinear pattern classification problem.</p><p>In recent years, researchers have analyzed and improved the SpikeProp algorithm from different aspects. Xin and Embrechts <ref type="bibr" target="#b133">[135]</ref> presented a method with a simple momentum term that increases the convergence speed of the SpikeProp algorithm. The learning performance of the SpikeProp algorithm is further enhanced by adding learning rules based on gradient descent for parameters such as synaptic delays, the threshold of spike firing, and the time constant J o u r n a l P r e -p r o o f <ref type="bibr" target="#b134">[136,</ref><ref type="bibr" target="#b135">137,</ref><ref type="bibr" target="#b136">138,</ref><ref type="bibr" target="#b137">139,</ref><ref type="bibr" target="#b138">140,</ref><ref type="bibr" target="#b139">141]</ref>. McKennoch et al. <ref type="bibr" target="#b140">[142]</ref> proposed the RProp and QuickProp algorithms, with faster convergence, and further extended SpikeProp to a class of nonlinear neuron models and constructed a back-propagation algorithm in theta neuron networks <ref type="bibr" target="#b141">[143]</ref>. Yang et al. <ref type="bibr" target="#b142">[144]</ref> proposed a modified spiking neuron model that involves the derivative of the state function at spike firing time. The supervised learning algorithm for the modified spiking neuron was also given. Shrestha and Song <ref type="bibr" target="#b143">[145,</ref><ref type="bibr" target="#b144">146,</ref><ref type="bibr" target="#b145">147,</ref><ref type="bibr" target="#b146">148,</ref><ref type="bibr" target="#b147">149]</ref> improved the Spike-Prop algorithm by use of an adaptive learning rate, adaptive delays, and an event-driven simulation strategy. Zhao et al. <ref type="bibr" target="#b148">[150]</ref> further improved the Spike-Prop algorithm by introducing the smoothing L 1/2 regularization term into the error function and proved convergence of this algorithm under some reasonable conditions. Mostafa <ref type="bibr" target="#b149">[151]</ref> defined the network input-output relation as locally linear after a transformation of the time variable and then trained the SNN through gradient descent directly. However, all of the above-mentioned algorithms encode information using a single spike, a limitation that means they cannot be effective for solving complex spatiotemporal problems.</p><p>A more important extension of the SpikeProp algorithm was presented by Booij and tat Nguyen <ref type="bibr" target="#b150">[152]</ref>. Their algorithm allows the neurons in the input and hidden layers to fire multiple spikes, but only the first spike in the output layer is considered. The error function of the SNNs is the same as that of the SpikeProp algorithm, using Eq. ( <ref type="formula" target="#formula_29">21</ref>). On the basis of <ref type="bibr" target="#b150">[152]</ref>, Fang and Wang <ref type="bibr" target="#b151">[153]</ref> derived an additional error back-propagation learning rule for the coefficient of the refractoriness function, in which the dependence of the postsynaptic potential on the firing times of the postsynaptic neuron is not ignored. Similarly, Ghosh-Dastidar and Adeli <ref type="bibr" target="#b152">[154]</ref> proposed a back-propagation learning algorithm named Multi-SpikeProp, with derivations of the learning rule based on the chain rule for a multiple spiking network model. Multi-SpikeProp was applied to the standard XOR problem and the Fisher Iris and EEG classification problems. Experimental results showed that the Multi-SpikeProp algorithm has higher classification accuracy than the SpikeProp algorithm. Luo et al. <ref type="bibr" target="#b153">[155,</ref><ref type="bibr" target="#b154">156]</ref> used the dynamic momentum and learning rate adaption and antinoise learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof rule to improve the learning performance of the extended SpikeProp algorithm presented by Booij and tat Nguyen <ref type="bibr" target="#b150">[152]</ref>.</p><p>Supervised learning for multiple spikes is more complex than that for a single spike, in which the error function involves multiple spikes in computing. For any given input spike trains, the least mean square error function of SNNs for multiple spikes is no longer expressed by Eq. ( <ref type="formula" target="#formula_29">21</ref>) but is defined as</p><formula xml:id="formula_31">E = 1 2 NO ∑ o=1 Fo ∑ f =1 ( t f o -tf o ) 2 , (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>where t f o and tf o are output and desired spikes, respectively. Recently, Xu et al. <ref type="bibr" target="#b128">[130]</ref> extended the Multi-SpikeProp algorithm to allow neurons to fire multiple spikes in all layers; that is, the algorithm can implement the complex spatiotemporal pattern learning of spike trains. Gong <ref type="bibr" target="#b155">[157]</ref> further investigated the supervised learning algorithm based on multiple-spike back-propagation in combination with the long-term memory characteristics of the SRM. Gardner et al. <ref type="bibr" target="#b156">[158]</ref> introduced a new supervised learning rule named MultilayerSpiker that can train multilayer feed-forward SNNs to perform transformations between spatiotemporal input and output spike patterns. The stochastic spiking neuron model is used in this method. The learning rule is robust against input noise and could generalize well on an example dataset.</p><p>Banerjee <ref type="bibr" target="#b157">[159]</ref> derived a synaptic weight update rule for learning temporally precise spike-train-to-spike-train transformations in multilayer feed-forward networks of spiking neurons. The framework, aiming at seamlessly generalizing error back-propagation to the deterministic spiking neuron setting, is based strictly on spike timing and avoids invoking concepts about spike rates or probabilistic models of spiking. Wu et al. <ref type="bibr" target="#b158">[160]</ref> proposed a spatiotemporal back-propagation training framework for SNNs that combines both the spatial domain and the temporal domain in the training phase. Experimental results showed that the proposed method achieved high accuracy on either static or dynamic datasets.</p><p>Zenke and Ganguli <ref type="bibr" target="#b159">[161]</ref> derived a nonlinear voltage-based three-factor learning rule named SuperSpike by using a surrogate gradient approach to train multi-J o u r n a l P r e -p r o o f layer feed-forward SNNs. They also assessed the impact of different types of feedback credit assignment strategies for the hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Synaptic plasticity algorithms</head><p>Researchers have proposed various supervised learning algorithms for multilayer feed-forward SNNs combining synaptic plasticity with the gradient descent rule. Strain et al. <ref type="bibr" target="#b160">[162]</ref> presented a cross-correlated training algorithm for SNNs that uses an extended form of the STDP rule. The input and output neurons are conductance-based LIF neurons, while the hidden neurons are dynamic threshold neurons in their model. By combination of the Bienenstock-Cooper-Munro learning rule <ref type="bibr" target="#b161">[163,</ref><ref type="bibr" target="#b162">164]</ref> with the STDP mechanism, a synaptic weight association training algorithm <ref type="bibr" target="#b163">[165]</ref> for SNNs was proposed that yields a unimodal synaptic weight distribution where weight stabilization is achieved with the sliding threshold associated with the Bienenstock-Cooper-Munro model after a period of training. Zheng and Mazumder <ref type="bibr" target="#b164">[166,</ref><ref type="bibr" target="#b165">167]</ref> formulated an online learning algorithm for hardware-based multilayer feed-forward SNNs through the modulation of weight-dependent STDP. The proposed learning method can estimate the gradient components with the help of the spike timings in an SNN. <ref type="bibr" target="#b166">[168]</ref> extended the ReSuMe algorithm <ref type="bibr" target="#b85">[87]</ref> to multilayer feed-forward SNNs using back-propagation of the network error and proposed the Multi-ReSuMe algorithm. The Multi-ReSuMe algorithm can be applied to neurons firing multiple spikes in SNNs. Synaptic weight modification at time t for the output neurons is expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sporea and Grüning</head><formula xml:id="formula_33">∆w oh (t) = 1 N H s h (t) {∫ ∞ 0 W pre (s) [s o d (t -s) -s o a (t -s)] ds } + 1 N H [s o d (t) -s o a (t)] [ a + ∫ ∞ 0 W post (s)s h (t -s)ds ] ,<label>(24)</label></formula><p>where N H is the number of hidden neurons, s h (t) is the spike trains fired by hidden neurons, and a is a non-Hebbian term. Synaptic weight modification at</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof time t for the hidden neurons is expressed as</p><formula xml:id="formula_34">∆w hi (t) = 1 N I N H s i (t) NO ∑ o=1 {∫ ∞ 0 W pre (s) [s o d (t -s) -s o a (t -s)] ds } w oh + 1 N I N H NO ∑ o=1 [s o d (t) -s o a (t)] [ a + ∫ ∞ 0 W post (s)s i (t -s)ds ] w oh ,<label>(25)</label></formula><p>where N I and N O are the numbers of input neurons and output neurons, respectively. In addition, Taherkhani et al. <ref type="bibr" target="#b167">[169]</ref> extended their DL-ReSuMe <ref type="bibr" target="#b89">[91]</ref> to train both synaptic weights and synaptic delays of a multilayer feed-forward SNN to fire multiple desired spikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Spike train convolution algorithms</head><p>Lin et al. <ref type="bibr" target="#b168">[170]</ref> extended their STKLR algorithm <ref type="bibr" target="#b105">[107]</ref> to multilayer feedforward SNNs and presented a new supervised learning algorithm for multilayer SNNs (Multi-STIP). They first constructed a novel error function of spike trains based on spike train kernels and then derived a synaptic weight learning rule combining the mechanism of error back-propagation. In Multi-STIP, the adjustment rule for synaptic weight between a neuron in the output layer and a neuron in the hidden layer is</p><formula xml:id="formula_35">∆w oh = -η [F (s o a (t), s h (t)) -F (s o d (t), s h (t))] = η   F o d ∑ g=1 Fh ∑ k=1 κ ( t g d , t k h ) - F o a ∑ j=1 Fh ∑ k=1 κ ( t j a , t k h )   ,<label>(26)</label></formula><p>and the adjustment rule for synaptic weight between a neuron in the hidden layer and a neuron in the input layer is</p><formula xml:id="formula_36">∆w hi = -η NO ∑ o=1 [F (s o a (t), s i (t)) -F (s o d (t), s i (t))] w oh = η NO ∑ o=1   F o d ∑ g=1 Fi ∑ f =1 κ ( t g d , t f i ) - F o a ∑ j=1 Fi ∑ f =1 κ ( t j a , t f i )   w oh ,<label>(27)</label></formula><p>where N O is the number of output neurons, t f i and t k h are spike firing times in the input spike train s i (t) and hidden neuron spike train s h (t), respectively, t j a J o u r n a l P r e -p r o o f and t g d are actual and desired spikes corresponding to an output neuron, and Drawing on the SPAN algorithm <ref type="bibr" target="#b99">[101]</ref>, Zhang and Lin <ref type="bibr" target="#b169">[171]</ref> proposed another supervised learning algorithm based on the convolution computing of spike trains for multilayer SNNs. This algorithm is the extension of the SPAN algorithm for multilayer feed-forward SNNs. In <ref type="bibr" target="#b170">[172]</ref>, both the PSD rule <ref type="bibr" target="#b102">[104]</ref> and the tempotron rule <ref type="bibr" target="#b63">[65]</ref> were extended to multiple layers, leading to new rules of multilayer PSD (MutPSD) and multilayer tempotron (MutTmptr).</p><formula xml:id="formula_37">F i , F h , F o a ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Other supervised learning algorithms</head><p>Belatreche et al. <ref type="bibr" target="#b171">[173,</ref><ref type="bibr" target="#b172">174]</ref> investigated the viability of evolutionary strategies for supervised learning in SNNs. These algorithms can learn only a single spike, and the computation is very time-consuming. Similar work is described in <ref type="bibr" target="#b173">[175]</ref>. There are other algorithms that use optimization algorithms to train SNNs <ref type="bibr" target="#b174">[176,</ref><ref type="bibr" target="#b175">177,</ref><ref type="bibr" target="#b176">178,</ref><ref type="bibr" target="#b177">179]</ref>.</p><p>Wang et al. <ref type="bibr" target="#b178">[180]</ref> proposed an online hybrid learning method for feed-forward SNNs with an adaptive structure that combines unsupervised and supervised learning rules. A growing and pruning strategy is used to adjust the structure of the hidden layer, and classification at the output layer is achieved through supervised learning. Similar studies are described in <ref type="bibr" target="#b179">[181,</ref><ref type="bibr" target="#b180">182,</ref><ref type="bibr" target="#b181">183]</ref>.</p><p>Xie et al. <ref type="bibr" target="#b182">[184]</ref> extended the PBSNLR algorithm <ref type="bibr" target="#b61">[63]</ref> to multilayer feedforward SNNs and proposed a normalized perceptron-based learning rule. Differently from traditional methods, the normalized perceptron-based learning rule trains only the selected misclassified time points and the target ones, using the perceptron-based neuron. They also extended their ASA algorithm <ref type="bibr" target="#b127">[129]</ref> to multilayer feed-forward SNNs and proposed a normalized spiking error backpropagation algorithm <ref type="bibr" target="#b183">[185]</ref>.</p><p>J o u r n a l P r e -p r o o f</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance comparison of spike train learning</head><p>In this subsection, the learning performances of some representative supervised learning algorithms for multilayer feed-forward SNNs are compared. The ST-SpikeProp <ref type="bibr" target="#b128">[130]</ref>, Multi-ReSuMe <ref type="bibr" target="#b166">[168]</ref>, and Multi-STIP <ref type="bibr" target="#b168">[170]</ref> algorithms are selected. These three algorithms can achieve spike train learning. There are 50 input neurons, 30 hidden neurons, and one output neuron in multilayer feedforward SNNs. The reference length of the spike trains is 200 ms and the reference firing rate of the spike trains is 40 Hz.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supervised learning for recurrent SNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The architecture of recurrent SNNs</head><p>A recurrent SNN is an SNN that has feedback loops in the network. It is difficult to construct the learning algorithms and analyze the dynamics of recurrent SNNs <ref type="bibr" target="#b184">[186,</ref><ref type="bibr" target="#b185">187,</ref><ref type="bibr" target="#b186">188,</ref><ref type="bibr" target="#b187">189]</ref>. Maass and Natschläger <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b188">190]</ref> proved that recurrent SNNs could simulate Turing machines and arbitrary Hopfield neural networks <ref type="bibr" target="#b189">[191]</ref>. There are various architectures of recurrent SNNs, but they can be divided roughly into two kinds: fully recurrent SNNs and locally recurrent SNNs. Because of the difference in architecture between these two types of recurrent SNNs, the construction of their learning algorithms and their dynamic performance are different.</p><p>Fully recurrent SNNs are usually single-layer SNNs with feedback connections between each neuron. In most cases, each neuron is connected to other neurons, and each neuron has its own connection. Fig. <ref type="figure" target="#fig_1">12</ref> shows an architecture of a fully recurrent SNN.</p><p>Although fully recurrent SNNs have rich dynamic behavior, their architecture is so complex that they are difficult to analyze and train. Therefore, it is often necessary to simplify the architecture of fully recurrent SNNs in prac- Journal Pre-proof hidden layer is not only inputted forward to the output layer but is also inputted back to the hidden layer itself. Fig. <ref type="figure" target="#fig_0">14</ref> shows an architecture of a recurrent SNN with internal feedbacks.</p><p>input layer output layer hidden layer Fig. <ref type="figure" target="#fig_0">14</ref>. Architecture of a recurrent spiking neural network with internal feedbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Learning algorithms for recurrent SNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Gradient descent algorithms</head><p>Back-propagation through time (BPTT) <ref type="bibr" target="#b190">[192]</ref> is one of classical supervised learning algorithms for recurrent ANNs. In the spirit of BPTT, Tiňo and Mills <ref type="bibr" target="#b191">[193]</ref> extended SpikeProp <ref type="bibr" target="#b131">[133]</ref> to recurrent network topologies and proposed a SpikeProp through time (SPTT) supervised learning algorithm for recurrent SNNs. They turned the feed-forward SNN into a recurrent SNN by extending the feed-forward architecture with feedback connections. Similarly, Diehl et al. <ref type="bibr" target="#b192">[194]</ref> presented a train-and-constrain method that trains recurrent ANNs using BPTT, then discretizes the weights, and finally converts them to recurrent SNNs by matching the responses of artificial neurons with those of the spiking neurons. Huh and Sejnowski <ref type="bibr" target="#b193">[195]</ref> presented another gradient descent method for optimizing recurrent SNN dynamics on the timescale of individual spikes by introducing a differentiable formulation of SNNs and deriving the exact gradient calculation. Then they applied their method to predictive coding tasks and a delayed-memory XOR task. Experimental results showed that this algorith-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof m can optimize spiking networks to perform nonlinear computations over an extended time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Kullback-Leibler divergence algorithms</head><p>On the basis of <ref type="bibr" target="#b110">[112]</ref>, Brea et al. <ref type="bibr" target="#b194">[196,</ref><ref type="bibr" target="#b195">197]</ref> proposed a generic supervised learning rule for recurrent SNNs. The learning rule is matched to the neural dynamics, and it can be adapted to a wide range of neuron models. They first defined the Kullback-Leibler divergence from the distribution P * (v) of desired output spike trains to the distribution P w (v) of spike trains produced by the network:</p><formula xml:id="formula_38">D(P * (v) ∥ P w (v)) = ⟨ log P * (v) P w (v) ⟩ P * (v) . (<label>28</label></formula><formula xml:id="formula_39">)</formula><p>Then they minimized the upper bound on the Kullback-Leibler divergence and derived the learning rule for synaptic weight using the gradient descent method:</p><formula xml:id="formula_40">∆w ij ∝ ⟨ ∂ ∂w ij log P w (v, h) ⟩ Pw(h|v)P * (v) .<label>(29)</label></formula><p>Both offline and online learning rules were given. The derived learning rule is consistent with STDP and can be matched to the voltage-triplet rule <ref type="bibr" target="#b196">[198]</ref>.</p><p>Rezende et al. <ref type="bibr" target="#b197">[199,</ref><ref type="bibr" target="#b198">200]</ref> derived a similar learning algorithm for recurrent SNNs combining principles from variational learning and reinforcement learning. The resulting online supervised learning algorithm is a Hebbian-type trace modulated by a global novelty signal. Experimental results showed that this algorithm is able to learn both stationary and nonstationary patterns of spike trains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3.">Other supervised learning algorithms</head><p>Memmesheimer et al. <ref type="bibr" target="#b199">[201]</ref> presented a simple, efficient, and biologically plausible supervised learning rule named finite precision that allows feedforward and recurrent SNNs to learn multiple mappings between input and desired output spike trains. They proved that their algorithm converges after a finite number of updates. Gilra and Gerstner <ref type="bibr" target="#b200">[202]</ref> presented a supervised learning algorithm named feedback-based online local learning of weights (FOL-LOW) for feed-forward and recurrent SNNs. In this learning rule, the error J o u r n a l P r e -p r o o f of the network is fed back through fixed random connections with a negative gain, causing the network to follow the desired dynamics. It is an online and local learning rule. They also proved that this algorithm converges to a stable solution. Lin and Shi <ref type="bibr" target="#b201">[203]</ref> extended their STKLR algorithm <ref type="bibr" target="#b105">[107]</ref> to recurrent SNNs and proposed a new supervised multispike learning algorithm named R-STKLR that can implement complex spatiotemporal pattern learning of spike trains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Performance comparison of spike train learning</head><p>In this subsection, the spike train learning performances of FOLLOW <ref type="bibr" target="#b200">[202]</ref> and R-STKLR <ref type="bibr" target="#b201">[203]</ref> are compared. These two algorithms can achieve spike train learning. There are 50 input neurons, 30 hidden neurons, and one output neuron in the recurrent SNNs with internal feedbacks. The reference length of the spike trains is 200 ms and the reference firing rate of the spike trains is 40 Hz.      The supervised learning algorithms for SNNs proposed in recent years exhibit different characteristics. To understand and compare different algorithms better, the performance of supervised learning algorithms for SNNs is evaluated qualitatively mainly from the following five aspects <ref type="bibr" target="#b202">[204]</ref>:</p><p>1. Learning ability of spike trains. Although the application of supervised learning algorithms is related to the spike encoding method chosen, from the performance of the supervised learning algorithm itself, some algorithms can achieve multiple-spike learning tasks, while some algorithms can achieve only single-spike learning tasks. Generally, the supervised learning algorithms based on spike train learning have greater learning ability and wider applicability but are more complex than the single-spike learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r -p o o f</head><p>Journal Pre-proof 2. Offline and online processing performance. Supervised learning for SNNs generally has two running modes: offline learning and online learning <ref type="bibr" target="#b203">[205]</ref>.</p><p>Offline learning methods the adjustment of synaptic weights according to the entire actual and desired output spike after running of the network, while online learning methods adjust the synaptic weight of a neuron when the neuron fires a spike during the running process <ref type="bibr" target="#b204">[206]</ref>.</p><p>Offline learning algorithms are suitable for static data processing. Spatiotemporal data are generally expressed as continuous spike train streams <ref type="bibr" target="#b26">[27]</ref>, which requires the learning algorithm to train the network in real time. Online learning algorithms are suitable and effective for dealing with such real-time tasks <ref type="bibr" target="#b205">[207]</ref>. Online learning is a biologically plausible learning mode, so research on online learning algorithms has important significance.</p><p>The synaptic weight is updated several times in a learning epoch for the online learning algorithm. The time-varying adjustment of the synaptic weight is usually expressed as ∆w(t). In contrast, the synaptic weight is updated only once in a learning epoch for the offline learning algorithm, in which the adjustment of the synaptic weight is usually expressed as ∆w. Some algorithms can run not only in offline mode but also in online mode, while others can run only in either offline mode or online mode. For the algorithm that can run in two modes, the update of the synaptic weight for online learning and offline learning has the following relationship:</p><formula xml:id="formula_41">∆w = ∫ Γ ∆w(t)dt. (<label>30</label></formula><formula xml:id="formula_42">)</formula><p>3. Locality property of learning rules. Locality property means that the learning rules are determined only by the presynaptic and postsynaptic activities of neurons, and the synaptic weight itself <ref type="bibr" target="#b206">[208]</ref>; namely, the quantity that modifies the weight of a synapse must be available locally at the synapse <ref type="bibr" target="#b200">[202]</ref>. It is a biologically plausible property and can be formally defined as</p><formula xml:id="formula_43">∆w ij = F (v i , v j , w ij ),<label>(31)</label></formula><p>J o u r n a l P e -r o f</p><p>where v i and v j are activities of presynaptic and postsynaptic neurons, respectively, and F (v i , v j , w ij ) is a functional relationship between v i , v j , and w ij . Supervised learning algorithms with locality property have a wider range of applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">A taxonomy for supervised learning algorithms</head><p>In this subsection, we have tried to develop a principled taxonomy for supervised learning algorithms to make classification and comparison of different algorithms possible according to the following five performance evaluation criteria described in Section 7.1:</p><p>J o u r n a l P r e p r o o f 1. Spike train learning. Fig. <ref type="figure" target="#fig_24">18</ref> shows a taxonomy for supervised learning algorithms from the spike train learning dimension. It can be seen that some learning algorithms can achieve only single-spike learning; for example, tempotron <ref type="bibr" target="#b63">[65]</ref>, SpikeProp <ref type="bibr" target="#b131">[133]</ref>, and SPTT <ref type="bibr" target="#b191">[193]</ref>. Meanwhile, some algorithms can achieve spike train learning; for example, PBSNLR <ref type="bibr" target="#b61">[63]</ref>, ReSuMe <ref type="bibr" target="#b85">[87]</ref>, STKLR <ref type="bibr" target="#b105">[107,</ref><ref type="bibr" target="#b107">109]</ref>, ST-SpikeProp <ref type="bibr" target="#b128">[130]</ref>, Multi-ReSuMe <ref type="bibr" target="#b166">[168]</ref>, Multi-STIP <ref type="bibr" target="#b168">[170]</ref>, and the algorithm presented by Brea et al. <ref type="bibr" target="#b195">[197]</ref>.</p><p>In addition, for some algorithms, the neurons in the input and hidden layers can fire multiple spikes, while neurons in the output layer can fire only a single spike; for example, the algorithm presented by Booij and tat</p><p>Nguyen <ref type="bibr" target="#b150">[152]</ref> and Multi-SpikeProp <ref type="bibr" target="#b152">[154]</ref>. 2. Running mode. Fig. <ref type="figure" target="#fig_25">19</ref> shows a taxonomy for supervised learning algorithms from the running mode dimension. It can be seen that some algorithms can run only in offline mode; for example, PBSNLR <ref type="bibr" target="#b61">[63]</ref>, ST-SpikeProp <ref type="bibr" target="#b128">[130]</ref>, and Multi-STIP <ref type="bibr" target="#b168">[170]</ref>. Meanwhile, some algorithms can J o u r n a l P e -p r o o f run only in online mode; for example, DL-ReSuMe <ref type="bibr" target="#b89">[91]</ref> and CCDS <ref type="bibr" target="#b91">[93]</ref>. Some algorithms that can achieve only single-spike learning <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b131">133]</ref> or output neurons that fire a single spike <ref type="bibr" target="#b152">[154]</ref> can be seen as an online running mode. In addition, some learning algorithms can run in both offline mode and online mode; for example, ReSuMe <ref type="bibr" target="#b85">[87]</ref>, SPAN <ref type="bibr" target="#b99">[101]</ref>, PSD <ref type="bibr" target="#b102">[104]</ref>, STKLR <ref type="bibr" target="#b105">[107,</ref><ref type="bibr" target="#b107">109]</ref>, Multi-ReSuMe <ref type="bibr" target="#b166">[168]</ref>, and the algorithm presented by Brea et al. <ref type="bibr" target="#b195">[197]</ref>. 3. Locality property. A supervised learning algorithm that is local means that the adjustment of the synaptic weight is available locally at the synapse. Therefore, all supervised learning algorithms for single-layer SNNs possess the locality property, while all supervised learning algorithms for multilayer feed-forward SNNs do not possess this property. Some supervised learning algorithms for recurrent SNNs possess this property, such as the algorithm presented by Brea et al. <ref type="bibr" target="#b195">[197]</ref> and FOLLOW <ref type="bibr" target="#b200">[202]</ref>. However, there are still some supervised learning algorithms for J o u r n a l P e -p r o o f recurrent SNNs that do not possess this property, such as SPTT <ref type="bibr" target="#b191">[193]</ref>. A taxonomy for supervised learning algorithms from the locality property dimension is shown in Fig. <ref type="figure" target="#fig_26">20</ref>. 4. Stability of the optimal solution. Fig. <ref type="figure" target="#fig_27">21</ref> shows a taxonomy for supervised learning algorithms from the stability dimension. Some supervised learning algorithms have been proved to possess stability of the optimal solution; for example, ReSuMe <ref type="bibr" target="#b85">[87]</ref>, SpikeProp <ref type="bibr" target="#b131">[133]</ref>, and FOLLOW <ref type="bibr" target="#b200">[202]</ref>.</p><p>Meanwhile, some supervised learning algorithms have been proved to not possess stability of the optimal solution; for example, the algorithm presented by Legenstein et al. <ref type="bibr" target="#b77">[79]</ref>. However, there are still many supervised learning algorithms for which it cannot be determined whether they possess stability of the optimal solution; for example, PBSNLR <ref type="bibr" target="#b61">[63]</ref>, STKLR <ref type="bibr" target="#b105">[107,</ref><ref type="bibr" target="#b107">109]</ref>, ST-SpikeProp <ref type="bibr" target="#b128">[130]</ref>, Multi-ReSuMe <ref type="bibr" target="#b166">[168]</ref>, Multi-STIP <ref type="bibr" target="#b168">[170]</ref>, and the algorithm presented by Brea et al. <ref type="bibr" target="#b195">[197]</ref>.</p><p>5. Spiking neuron models. Fig. <ref type="figure" target="#fig_1">22</ref> shows a taxonomy for supervised learning algorithms from the spiking neuron model dimension. It can be seen that some supervised learning algorithms, especially some gradientdescent-based algorithms <ref type="bibr" target="#b150">[152,</ref><ref type="bibr" target="#b152">154,</ref><ref type="bibr" target="#b131">133,</ref><ref type="bibr" target="#b128">130,</ref><ref type="bibr" target="#b191">193]</ref>, can use only the neurons in which the internal state can be described by an analytic ex- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Future directions</head><p>In general, supervised learning for SNNs is a significant research field. Researchers have conducted much research and achieved a series of fruitful results. However, because of the inherent complexity of SNNs, it is difficult to construct supervised learning algorithms with extensive applicability. There are many difficult problems that need to be solved, and new learning mechanisms and algorithms need to be explored. By analyzing and summarizing the current supervised learning algorithms for SNNs, we can predict the major problems that will need to be solved in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Online supervised learning for real-time problems</head><p>Supervised learning algorithms for multilayer feed-forward SNNs, especially some gradient-descent-based algorithms, have achieved more research results with intensive research. However, most of these algorithms use offline processing. The spatiotemporal data acquired in the real world exhibit spatial and temporal characteristics <ref type="bibr" target="#b207">[209]</ref>. Therefore, it is necessary to develop online supervised learning algorithms in SNNs for real-time problems. At present, there are few online supervised learning algorithms for SNNs, especially gradientdescent-based supervised learning algorithms <ref type="bibr" target="#b131">[133,</ref><ref type="bibr" target="#b204">206]</ref>. By defining the realtime error function of spike trains and setting the reasonable adjustment of synaptic weights, one can realize the real-time adjustment of synaptic weight in SNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Supervised learning with various synaptic plasticity</head><p>Research suggests that the timing of presynaptic and postsynaptic spike trains can cause long-term potentiation or long-term depression of synapses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r -p o o f</head><p>Journal Pre-proof Induction of synaptic potentiation and depression is also accompanied by the bidirectional modification <ref type="bibr" target="#b208">[210,</ref><ref type="bibr" target="#b209">211]</ref> of the overall excitability of presynaptic neurons. Synaptic potentiation or depression can be reversed and rapidly transmitted to synapses on dendrites of presynaptic neurons. This specific and fast synaptic plasticity back-propagation of presynaptic neurons is similar to the mechanism in the back-propagation algorithm. It can exist in biological neural networks and play a certain role <ref type="bibr" target="#b129">[131,</ref><ref type="bibr" target="#b130">132]</ref>. Therefore, it is of great importance to construct supervised learning algorithms using various biological synaptic plasticity mechanisms for complex SNN architectures <ref type="bibr" target="#b210">[212,</ref><ref type="bibr" target="#b211">213,</ref><ref type="bibr" target="#b212">214,</ref><ref type="bibr" target="#b213">215,</ref><ref type="bibr" target="#b214">216]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Supervised learning for hybrid SNNs</head><p>Feed-forward SNNs have been extensively studied for the processing of nontemporal problems. However, feed-forward SNNs cannot solve real-world temporal tasks very well. Recurrent SNNs can represent more complex time-varying systems. It is important to construct efficient hybrid SNNs in combination with the advantages of feed-forward SNNs and recurrent SNNs. In most cases, recurrent architecture in hybrid SNNs is static. Supervised learning algorithms for hybrid SNNs with dynamic recurrent architecture are not a lot <ref type="bibr" target="#b215">[217]</ref>. It is necessary to study the dynamic characteristics of recurrent architecture in hybrid SNNs and construct efficient supervised learning algorithms for large-scale hybrid SNNs in combination with the bidirectional synaptic plasticity mechanism <ref type="bibr" target="#b208">[210,</ref><ref type="bibr" target="#b209">211]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Supervised learning for deep SNNs</head><p>In recent years, deep learning <ref type="bibr" target="#b216">[218,</ref><ref type="bibr" target="#b217">219]</ref> has shown remarkable performance in many areas of pattern recognition. Most deep neural network models use the second generation of ANNs. Some research recently attempted to take advantages of both deep learning and SNNs to develop spiking deep neural networks to achieve high performance <ref type="bibr" target="#b218">[220,</ref><ref type="bibr" target="#b219">221,</ref><ref type="bibr" target="#b220">222,</ref><ref type="bibr" target="#b221">223]</ref>. In the future, it will be necessary to construct efficient computational models combining deep learning in J o u r n a l P r e p r o o f traditional ANNs and SNNs, such as spiking deep belief networks and spiking convolutional neural networks. Furthermore, efficient supervised learning algorithms for large-scale spiking deep neural networks should be constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Hardware implementation of supervised learning</head><p>The hardware implementation of supervised learning is an important and challenging field in SNNs <ref type="bibr" target="#b222">[224]</ref>. There are two popular hardware architectures for neuromorphic computing <ref type="bibr" target="#b164">[166,</ref><ref type="bibr" target="#b165">167]</ref>. One architecture is called a centralized memory architecture, which is closely related to the conventional von Neumann architecture. Synaptic weights are stored in a memory array, and they can be accessed through buses. Another architecture, the distributed memory architecture, is more related to biological neural networks. This architecture has been very popular in recent years because of many emerging memory technologies, such as memristors and phase-change memory. It is of great importance to study the hardware implementation of supervised learning for SNNs using neuromorphic devices, especially for large-scale SNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Large-scale SNNs for pattern recognition</head><p>An SNN has the outstanding characteristics of high self-adaptability and fault tolerance because of its precise spike timing encoding and spike-driven nonlinear dynamics. Supervised learning in SNNs has been extensively studied for algorithm design and pattern recognition. The implementation of most of the supervised algorithms is not limited by the network scale. However, because of the complexity of temporal or spatiotemporal data in the real world, the application of large-scale SNNs for pattern recognition is not wide enough and the solution of complex problems is not accurate enough. Kasabov et al. have done much work on pattern recognition based on large-scale SNNs <ref type="bibr" target="#b25">[26]</ref>. In particular, they have developed a unifying computational framework, NeuCube <ref type="bibr" target="#b26">[27]</ref>, for dealing with spatiotemporal and spectrotemporal brain data. According to the spike-driven information processing mechanism in SNNs, it is necessary to convert the formal datasets to event-driven datasets. In the future, much effort J o u r a l r e -p r o o f Journal Pre-proof needs to put into the development of large-scale SNNs for pattern recognition to achieve better performance in terms of both accuracy and computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>Aiming at the characteristics of precise spike timing encoding and neural information processing, we have provided a comprehensive review of supervised learning algorithms for SNNs. We first provided a comparison between SNNs and traditional ANNs. The basic theories of supervised learning for SNNs were also introduced, including the general framework and some related theories of supervised learning. We then surveyed the state-of-the-art supervised learning algorithms from the perspectives of the applicability to SNN architecture and the inherent mechanisms of supervised learning algorithms. A performance comparison of spike train learning of some representative algorithms was also made. Furthermore, a qualitative performance evaluation and a taxonomy of supervised learning algorithms were provided. Finally, some future research directions in this research field were outlined.</p><p>Similarly to traditional ANNs, there are three kinds of common topological architecture for SNNs: feed-forward SNNs, recurrent SNNs, and hybrid SNNs.</p><p>A single-layer SNN is a special feed-forward SNN. Because of the simplicity of its structure, a single-layer SNN is widely used in the pattern recognition field.</p><p>In most cases, recurrent architecture in hybrid SNNs is static. Therefore, from the perspective of network topological architecture, we discussed the supervised learning algorithms for single-layer SNNs, multilayer feed-forward SNNs, and recurrent SNNs, respectively.</p><p>Although there are various supervised learning algorithms for SNNs, some It is difficult to evaluate a supervised learning algorithm. In this review, we provided five qualitative performance evaluation criteria for supervised learning algorithms for SNNs, including the learning ability of spike trains, the offline and online processing performance, the locality property of learning rules, the stability of the optimal solution, and the applicability of spiking neuron models. Furthermore, a principled taxonomy for supervised learning algorithms was presented according to these five performance evaluation criteria. It shows that some algorithms can learn only a single spike, while some algorithms can learn spike trains; some algorithms can run with both offline and online, while some algorithms can run either only offline or only online; all algorithms for single-layer SNNs and some algorithms for recurrent SNNs possess the locality property, while all algorithms for multilayer feed-forward SNNs and some algorithms for recurrent SNNs do not possess this property; some algorithms possess stability of the optimal solution, while some algorithms do not possess this property; some algorithms (e.g., some gradient-descent-based algorithms) are restricted to working only with the SRM or the LIF model, while some algorithms are independent of the spiking neuron model.</p><p>An SNN plays a significant role in brain-inspired artificial intelligence, while supervised learning is a core research topic in SNNs. Developing high-performance supervised learning algorithms with wide applicability will help accelerate the utility process of large-scale SNNs. Many research results have been achieved, but there are still many difficult problems that need to be solved. This requires continuous and highly effective studies by researchers. of this paper greatly. This work was supported by the National Natural Science Foundation of China under grant no. 61762080, the Lanzhou Municipal Science and Technology Project under grant no. 2019-1-34, and the Program for Innovative Team in Northwest Normal University under grant no.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6008-01602.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Spike response model</head><p>The SRM <ref type="bibr" target="#b6">[7]</ref> is used in the spike train learning. Assuming that a neuron has N I input synapses, the ith synapse transmits a total of F i spikes, and the f th spike (f ∈ [1, F i ]) is fired at time t f i . The internal state u(t) of the neuron at time t is given by</p><formula xml:id="formula_44">u(t) = NI ∑ i=1 Fi ∑ f =1 w i ε(t -t f i ) + η(t -t l o ),<label>(32)</label></formula><p>where w i is the synaptic weight for the ith synapse. The spike response function ε(tt f i ) describes the effect of the presynaptic spike on the internal state of the postsynaptic neuron, which is expressed as</p><formula xml:id="formula_45">ε(t -t f i ) =    t-t f i τ exp(1 - t-t f i τ ), t -t f i &gt; 0, 0, t -t f i ≤ 0, (<label>33</label></formula><formula xml:id="formula_46">)</formula><p>where τ is the time decay constant of postsynaptic potentials. η(tt l o ) is the refractoriness function, which is reflected mainly in the effect that only the last output spike t l o contributes to the refractoriness:</p><formula xml:id="formula_47">η(t -t l o ) =    -θ exp(- t-t l o τR ), t -t l o &gt; 0, 0, t -t l o ≤ 0, (<label>34</label></formula><formula xml:id="formula_48">)</formula><p>where θ is the spike firing threshold and τ R is the time constant. To quantitatively evaluate the learning performance, the spike train kernel is used to define a correlation-based metric <ref type="bibr" target="#b57">[59]</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Categories of supervised learning algorithms for spiking neural networks (SNNs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three generations of artificial neural networks (ANNs). MLP, multilayer perceptron; MP, McCulloch-Pitts; SNNs, spiking neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 . 4 .</head><label>34</label><figDesc>Synaptic plasticity and learning. The synaptic plasticity mechanism of J o u r n a l P r e -p r o o f SNNs emphasizes the spike-timing-dependent plasticity (STDP) between presynaptic and postsynaptic neurons, while the mechanism of ANNs generally satisfies the Hebb rule. When supervised learning algorithms are designed, SNNs generally have various mentalities (see Sections 4-6), while ANNs are mainly based on finding the derivative of the loss function. Parallel and hardware implementation. SNNs can realize fast and massively parallel information processing, while ANNs are relatively weak.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Journal Pre-proof 3 .</head><label>3</label><figDesc>-timing-dependent plasticity. J o u r n a l P r e -p r o o f Basic theories of supervised learning for SNNs 3.1. The general framework of supervised learning Supervised learning for SNNs is implemented through learning the spatiotemporal patterns of spike trains. A spike train s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The general framework of supervised learning for spiking neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>J</head><label></label><figDesc>o u r n a l P r e -p r o o f 4. Supervised learning for single-layer SNNs 4.1. The architecture of single-layer SNNs A single-layer SNN is a class of special feed-forward SNNs. It contains only one input layer and one output layer and does not contain any hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 Fig. 4 .</head><label>44</label><figDesc>Fig.4shows a single-layer SNN architecture with a single output neuron, where w i is the connection weight of the ith synapse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>et al.<ref type="bibr" target="#b84">[86]</ref> presented a time-varying long-term synaptic efficacy function-based leaky integrate-and-fire (LIF) neuron model (SEFRON). Corresponding supervised learning rule based on STDP was also presented for pattern classification problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>ReSuMe to train multiple neurons to classify spatiotemporal spiking patterns and proposed the Multi-DL-ReSuMe algorithm<ref type="bibr" target="#b90">[92]</ref>. Guo et al.<ref type="bibr" target="#b91">[93]</ref> proposed a cross-correlated delay shift (CCDS) supervised learning method for spiking neurons, in which synapse delays and axonal delays are variants and are modulated together with weights during learning. The CCDS learning method is another improved version of ReSuMe achieved by integrating synaptic delays and axonal delays in the synaptic weight learning process. In addition, combining the ReSuMe algorithm and triplet-based STDP<ref type="bibr" target="#b92">[94]</ref>, Lin et al.<ref type="bibr" target="#b93">[95]</ref> proposed a learning algorithm (T-ReSuMe) for spiking neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>[ 129 ]</head><label>129</label><figDesc>proposed an accurate synaptic-efficiency adjustment (ASA) method to increase the efficiency of training SNNs. The adjustment of synaptic weights in J o u r n a l P r e -p r o o f the ASA algorithm combines the membrane-potential-driven mechanism with the STDP learning window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>learning algorithm, is considered. The weight updating rule of the output layer is used to train the single-layer SNNs. For convenience, it is called the spike train SpikeProp (ST-SpikeProp) rule. All of these algorithms can achieve spike train learning. There are 500 input neurons and one output neuron in the single-layer SNNs. The reference length of the spike trains is 400 ms and the reference firing rate of the spike trains is 60 Hz. First, the learning performance of these algorithms with different lengths of spike trains was tested. The length of the spike trains was increased from 200 ms to 1000 ms in steps of 200 ms while the other settings remained the same. The learning rates of these algorithms corresponding to the different lengths of the spike trains are shown in Table 2. The learning results are shown in Fig. 5. Fig. 5(a) shows the average learning accuracy C after 1000 learning epochs. It can be seen that the learning accuracy C of all these algorithms decreases gradually with increasing length of the spike trains. The SPAN algorithm always has the highest learning accuracy. Fig. 5(b) shows the average number of learning epochs when the learning accuracy C reaches the maximum value. The learning epochs of these algorithms have no significant differences under the current conditions. Furthermore, the learning performance of these algorithms with different firing rates of spike trains was tested. The firing rate of spike trains was increased from 20 Hz to 180 Hz in steps of 40 Hz while the other settings remained the same. The learning rates of these algorithms corresponding to the different J o u r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Learning results with different lengths of spike trains for single-layer spiking neural networks after 1000 learning epochs. (a) The learning accuracy C. (b) The learning epochs when the learning accuracy C reaches the maximum value. PSD, precise spike driven; SPAN, spike pattern association neuron; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp. J o u r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 Fig. 6 .Fig. 7 .</head><label>767</label><figDesc>Fig.7shows the average running time of one trial of these algorithms under the reference conditions. The ST-SpikeProp algorithm has the longest running time because of its intrinsic complex mechanism. The running time of the other algorithms is relatively short.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Architecture of a multilayer feed-forward spiking neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 shows the learning results for these three algorithms with different lengths of spike trains. The length of the spike trains was increased from 100 ms to 500 ms in steps of 100 ms while the other settings remained the same. The learning rates of these algorithms corresponding to the different lengths of the spike trains are shown in Table 4. The average learning accuracy C after 1000 learning epochs is shown in Fig. 9(a). It can be seen that the Multi-STIP algorithm always has the highest learning accuracy. There is little difference in learning accuracy between ST-SpikeProp and Multi-ReSuMe. Fig. 9(b) shows the average number of learning epochs when the learning accuracy C reaches the maximum value. When the length of the spike trains is longer, the learning epoch of ST-SpikeProp is lower.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Learning results with different lengths of spike trains for multilayer feed-forward spiking neural networks after 1000 learning epochs. (a) The learning accuracy C. (b) The learning epochs when the learning accuracy C reaches the maximum value. ST-SpikeProp, spike train SpikeProp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Learning results with different firing rates of spike trains for multilayer feed-forward spiking neural networks after 1000 learning epochs. (a) The learning accuracy C. (b) The learning epochs when the learning accuracy C reaches the maximum value. ST-SpikeProp, spike train SpikeProp. J o u r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>JFig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Architecture of a fully recurrent spiking neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 15 showsFig. 16</head><label>1516</label><figDesc>Fig.15shows the learning results for FOLLOW and R-STKLR with different lengths of spike trains. The length of the spike trains was increased from 100 ms to 500 ms in steps of 100 ms. The learning rates of FOLLOW and R-STKLR corresponding to the different lengths of spike trains are shown in Table6. Fig.15(a) shows the average learning accuracy C after 1000 learning epochs. It can be seen that the length of the spike trains has little effect on the learning performance of FOLLOW. When the length of the spike trains is short, the learning accuracy of R-STKLR is higher than that of FOLLOW. Fig.15(b)shows the average number of learning epochs when the learning accuracy C reaches the maximum value. It can be seen that when the length of the spike trains is longer, the learning epoch of FOLLOW is greater than that of R-STKLR. Fig. 16 shows the learning results for FOLLOW and R-STKLR with different firing rates of spike trains. The firing rate of spike trains was increased from 20 Hz to 100 Hz in steps of 20 Hz. The learning rates of FOLLOW and R-STKLR corresponding to the different firing rates of spike trains are shown in J o u r n a l P r e p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 .FOLLOWFig. 16 .</head><label>1516</label><figDesc>Fig. 15. Learning results with different lengths of spike trains for recurrent spiking neural networks after 1000 learning epochs. (a) The learning accuracy C. (b) The learning epochs when the learning accuracy C reaches the maximum value. FOLLOW, feedback-based online local learning of weights; R-STKLR, spike train kernel learning rule for recurrent spiking neural networks. J o u r n a l r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 17 showsFig. 17</head><label>1717</label><figDesc>Fig. 17 shows the average running time of one trial of FOLLOW and R-J o u r n a l r e p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>.</head><label></label><figDesc>Fig. 17. Running time of one trial of feedback-based online local learning of weights (FOLLOW) and spike train kernel learning rule for recurrent spiking neural networks (R-STKLR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>7 .</head><label>7</label><figDesc>Qualitative performance evaluation and 7.1. Qualitative performance evaluation of supervised learning algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>4 . 5 .</head><label>45</label><figDesc>Stability of optimal Supervised learning SNNs is a process of optimization of synaptic weights. It is expected that the global optimal solution can be obtained in every learning epoch, but not the local optimal solution. This requires the learning algorithm to possess stability of the optimal solution. Stability of the optimal solution is usually characterized by the convergence of synaptic weights. When a supervised learning algorithm for SNNs is convergent, the synaptic modifications are essentially driven by the difference E (S a , S d ) between the desired and the actual output spike trains. That means ∆W =0 if and only if E (S a , S d ) = 0.In other words, the synaptic weight matrix W reaches a fixed point if the actual output spike trains S a equal the desired output spike trains S d . It can be shown that under certain conditions, this fixed point is a global, positive attractor in weight space. Applicability of spiking neuron models. A spiking neuron is the basic computational unit of SNNs. There are many supervised learning algorithms for SNNs, especially the gradient-descent-based algorithms, which are restricted to work only with analytically tractable spiking neuron models, such as with the SRM. However, some supervised learning algorithms rely explicitly only on the spike times and do not refer to the particular properties of the spiking neuron models. It is expected that the algorithm should work correctly independently of the spiking neuron model used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. A taxonomy for supervised learning algorithms from the spike train learning dimension. PBSNLR, perceptron-based spiking neuron learning rule; SPTT, SpikeProp through time; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. A taxonomy for supervised learning algorithms from the running mode dimension. CCDS, cross-correlated delay shift; PBSNLR, perceptronbased spiking neuron learning rule; PSD, precise spike driven; SPAN, spike pattern association neuron; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. A taxonomy for supervised learning algorithms from the locality property dimension. FOLLOW, feedback-based online local learning of weights; SNNs, spiking neural networks; SPTT, SpikeProp through time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>JFig. 21 .</head><label>21</label><figDesc>Fig. 21. A taxonomy for supervised learning algorithms from the stability dimension. FOLLOW, feedback-based online local learning of weights; PBSNL-R, perceptron-based spiking neuron learning rule; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>J</head><label></label><figDesc>Fig. 22. A taxonomy for supervised learning algorithms from the spiking neuron model dimension. LIF, leaky integrate and fire; PSD, precise spike driven; SPAN, spike pattern association neuron; SPTT, SpikeProp through time; SRM, spike response mode; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>of them have common characteristics. Some biological mechanisms and mathematical methods are commonly used to design supervised learning algorithms for SNNs. In this review, we discussed mainly supervised learning algorithms for SNNs based on the mechanisms of the perceptron, gradient descent, synaptic plasticity, spike train convolution, and Kullback-Leibler divergence. There are J o u r n a l r e -p r o o f Journal Pre-proof also some supervised learning algorithms based on other mechanisms, just like some optimization methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>The parameter values of the SRM used in the spike train learning are the time constant of the postsynaptic potential τ = 2 ms, the time constant of the refractory period τ R = 50 ms, the spike firing threshold θ = 1, and the length of the absolute refractory period t ref = 1 ms. J o u r n a l P r e -p o o Appendix B. Spike train learning task The clock-driven simulation strategy with time step dt = 0.1 ms is used to implement the spike train learning tasks. All simulations run on NetBeans IDE 8.2 with Java 1.8 on a six-core system with 16-GB RAM in a Windows 10 environment. Initially, the synaptic weights are generated randomly as the uniform distribution in the interval [0, 0.2]. Every input spike train and desired output spike train is generated randomly by a homogeneous Poisson process within the time interval of Γ with the same firing rate. There is only one weighted connection between presynaptic and postsynaptic neurons. All neurons are assumed to be excitatory. The parameters of the learning algorithms themselves are consistent with those in the corresponding references. All simulation results are averaged over 20 trials, and on each testing trial, the learning algorithm is applied for a maximum of 1000 learning epochs or until the network error E = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between spiking neural networks (SNNs) and traditional artificial neural networks (ANNs).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Learning rates of selected algorithms with different lengths of spike trains.</figDesc><table><row><cell>Algorithm</cell><cell>200 ms</cell><cell>400 ms</cell><cell>600 ms</cell><cell>800 ms</cell><cell>1000 ms</cell></row><row><cell>ReSuMe</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell>SPAN</cell><cell>0.0001</cell><cell>0.00005</cell><cell>0.00001</cell><cell>0.00001</cell><cell>0.00001</cell></row><row><cell>PSD</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell>STKLR</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0001</cell></row><row><cell cols="6">ST-SpikeProp 0.000001 0.000001 0.0000001 0.00000005 0.00000001</cell></row></table><note><p>PSD, precise spike driven; SPAN, spike pattern association neuron; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Learning rates of selected algorithms with different firing rates of spike trains.</figDesc><table><row><cell>Algorithm</cell><cell>20 Hz</cell><cell>60 Hz</cell><cell>100 Hz</cell><cell>140 Hz</cell><cell>180 Hz</cell></row><row><cell>ReSuMe</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0001</cell><cell>0.00005</cell></row><row><cell>SPAN</cell><cell>0.0001</cell><cell>0.00005</cell><cell>0.00001</cell><cell>0.00001</cell><cell>0.00001</cell></row><row><cell>PSD</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0001</cell></row><row><cell>STKLR</cell><cell>0.01</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell cols="6">ST-SpikeProp 0.000001 0.000001 0.0000005 0.0000005 0.0000001</cell></row></table><note><p>PSD, precise spike driven; SPAN, spike pattern association neuron; STKLR, spike train kernel learning rule; ST-SpikeProp, spike train SpikeProp.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Learning rates of selected algorithms with different lengths of spike trains.</figDesc><table><row><cell>Algorithm</cell><cell>100 ms</cell><cell>200 ms</cell><cell>300 ms</cell><cell>400 ms</cell><cell>500 ms</cell></row><row><cell cols="6">ST-SpikeProp 0.0000005 0.0000001 0.0000001 0.00000005 0.00000001</cell></row><row><cell>Multi-ReSuMe</cell><cell>0.01</cell><cell>0.005</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0001</cell></row><row><cell>Multi-STIP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.005</cell><cell>0.005</cell><cell>0.001</cell></row><row><cell cols="3">ST-SpikeProp, spike train SpikeProp.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Learning rates of selected algorithms with different firing rates of spike trains.</figDesc><table><row><cell>Algorithm</cell><cell>20 Hz</cell><cell>40 Hz</cell><cell>60 Hz</cell><cell>80 Hz</cell><cell>100 Hz</cell></row><row><cell cols="6">ST-SpikeProp 0.000001 0.0000001 0.0000001 0.0000001 0.00000005</cell></row><row><cell>Multi-ReSuMe</cell><cell>0.005</cell><cell>0.005</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell>Multi-STIP</cell><cell>0.01</cell><cell>0.01</cell><cell>0.005</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell cols="3">ST-SpikeProp, spike train SpikeProp.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Learning rates of feedback-based online local learning of weights (FOL-LOW) and spike train kernel learning rule for recurrent spiking neural networks (R-STKLR) with different lengths of spike trains.</figDesc><table><row><cell cols="6">Algorithm 100 ms 200 ms 300 ms 400 ms 500 ms</cell></row><row><cell>FOLLOW</cell><cell>0.005</cell><cell>0.005</cell><cell cols="3">0.0005 0.0001 0.00005</cell></row><row><cell>R-STKLR</cell><cell>0.01</cell><cell>0.005</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>C to express the distance between the desired output spike train s o d (t) and the actual output spike train s o a (t). The metric C is calculated after each learning epoch according to (t), f s o a (t)⟩ are the Euclidean norms of f s o d (t) and f s o a (t), respectively. C = 1 for identical spike trains and decreases toward 0 for loosely correlated spike trains.</figDesc><table><row><cell>C =</cell><cell>⟨f s o d (t), f s o a (t)⟩ ∥f s o d (t)∥∥f s o a (t)∥</cell><cell>,</cell><cell>(35)</cell></row><row><cell cols="4">where f s o d (t) and f s o a (t) are continuous functions convolved by a Gaussian filter</cell></row><row><cell cols="4">function corresponding to spike trains s o d (t) and s o a (t), respectively. ⟨f s o d (t), f s o a (t)⟩ √</cell></row><row><cell cols="3">is the inner product of f s o d (t) and f s o a (t). ∥f s o d (t)∥ = ∥f s o a (t)∥ = √ ⟨f s o a</cell><cell>⟨f s o d (t), f s o d (t)⟩ and</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint submitted to Neural NetworksDecember 15, 2019</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the Co-Editor in Chief, the Action Editor and the anonymous reviewers very much for their careful work and valuable constructive suggestions and comments that have helped improve the quality J o u r n a l P e -p r o o f</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The authors declared that they have no conflicts of interest to this work.</p><p>We declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural networks: an overview of early research, current frameworks and J o u r n a l P r e -r o o f new challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ortigosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="242" to="268" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural networks and learning machines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<publisher>Pearson Education Upper Saddle River</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Theunissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information theory and neural coding</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="947" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The evidence for neural information processing with precise spike-times: a survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Bohte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural coding: timing is key in the olfactory system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Whalley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="458" to="458" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computation by time</title>
		<author>
			<persName><forename type="first">F</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Röhrbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Kistler</surname></persName>
		</author>
		<title level="m">Spiking neuron models: single neurons, populations, plasticity</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Which model to use for cortical spiking neurons</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1063" to="1070" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamical properties of piecewise linear spiking neuron model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1270" to="1276" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: the third generation of neural network models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1671" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh-Dastidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
		<title level="m">Spiking neural networks, International Journal of Neural Systems</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lower bounds for the computational power of networks of spiking neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast sigmoidal networks via spiking neurons</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="304" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Categorization and decisionmaking in a neurobiologically plausible spiking network using a STDP-like learning rule</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beyeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Krichmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">To spike, or when to spike?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gütig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="134" to="139" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spiking neural networks for handwritten digit recognition -supervised learning and network optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rajendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="118" to="127" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Review of advances in neural networks: neural design technology stack</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Almási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woźniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leblebici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Engbersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The brain as an efficient and robust adaptive learner</title>
		<author>
			<persName><forename type="first">S</forename><surname>Denéve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bourdoukan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="969" to="977" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised learning in the brain</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Knudsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3985" to="3997" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The roles of supervised machine learning in systems neuroscience</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Glasera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Benjamina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farhoodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Kordinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="126" to="137" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rummelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<title level="m">Backpropagation: theory, architectures, and applications</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised learning algorithms for spiking neural networks: a review</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="586" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Comparison of supervised learning methods for spike time coding in spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kasiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ponulak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Mathematics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spiking neural networks and online learning: an overview and perspectives</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="88" to="100" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Time-space, spiking neural networks and brain-inspired artificial intelligence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NeuCube: a spiking neural network architecture for mapping, learning and understanding of spatio-temporal brain data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="62" to="76" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic evolving spiking neural networks for on-line spatio-and spectro-temporal pattern recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dhoble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nuntalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943. 1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="386" to="408" />
			<date type="published" when="1958">1958. 1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papert</surname></persName>
		</author>
		<author>
			<persName><surname>Perceptrons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page">1969</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simulation of networks of spiking neurons: a review of tools and strategies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carnevale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="398" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A lowcost high-speed neuromorphic hardware based on spiking neural network</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Farsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1586" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>J o u r a l P</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Advances in design and application of spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing spiking neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dorogyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolisnichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Modern Problems of Radio Engineering, Telecommunications and Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="124" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selection and optimization of temporal spike encoding methods for spiking neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Petro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An optimal time interval of input spikes involved in synaptic adjustment of spike sequence learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimization of output spike train encoding for a spiking neuron based on its spatiotemporal input pattern</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural decoding with kernel-based metric learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Brockmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Kriminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1080" to="1107" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Principles of neural coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panzeri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Information encoding and reconstruction from the phase of action potentials</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Tuckwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y M</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005">2005. 2005. 2009. 2009</date>
		</imprint>
	</monogr>
	<note>Frontiers in Systems Neuroscience</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neuronal population coding of movement direction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kettner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="issue">4771</biblScope>
			<biblScope unit="page" from="1416" to="1419" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BSA, a fast and accurate spike train encoding scheme</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Survey on modeling methods for single compartmental spiking neuron</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Engineering and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="41" to="44" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Currents carried by sodium and potassium ions through the membrane of the giant axon of Loligo</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hodgkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Huxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="472" />
			<date type="published" when="1952">1952. 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A quantitative description of membrane current and its application to conduction and excitation in nerve</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hodgkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Huxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="544" />
			<date type="published" when="1952">1952. 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review of the integrate-and-fire neuron model: I. Homogeneous synaptic input</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Burkitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of the integrate-and-fire neuron model: II. Inhomogeneous synaptic input and network properties</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Burkitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A framework for spiking neuron models: the spike response model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Biological Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="469" to="516" />
			<date type="published" when="2001">2001. 2001</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accuracy evaluation of numerical methods used in state-of-the-art simulators for spiking neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Partzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schüffny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="326" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient event-driven simulation of large networks of spiking neurons and dynamical synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Del Giudice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2305" to="2329" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Event-driven simulation scheme for spiking neural networks using lookup tables to characterize neuronal dynamics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ortigosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2959" to="2993" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Voltage-stepping schemes for the simulation of spiking neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tonnelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="423" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the performance of voltage stepping for the simulation of adaptive, nonlinear integrate-and-fire neuronal networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kaabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tonnelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1187" to="1204" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Toward unified hybrid simulation techniques for spiking neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>D'haene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1055" to="1079" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A novel time-event-driven algorithm for simulating spiking neural networks based on circular array</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simulation of spiking neural networks -architectures and implementations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rückert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="647" to="679" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A new correlation-based measure of spike timing reliability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiesinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="925" to="931" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Measuring spike train synchrony</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D I</forename><surname>Abarbanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Politi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On similarity measures for spike trains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vialatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improved similarity measures for small sets of spike trains</title>
		<author>
			<persName><forename type="first">R</forename><surname>Naud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mensi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3016" to="3069" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A new supervised learning algorithm for spiking neurons</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1472" to="1511" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Neuromorphic accelerators: a comparison between neuroscience and machine-learning approaches</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>B.-D. Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="494" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The tempotron: a neuron that learns spike timingcbased decisions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gütig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="428" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A gradient learning rule for the tempotron</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="340" to="352" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rapid feedforward computation by temporal encoding and learning with spiking neurons</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1539" to="1552" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Event-driven simulation of the tempotron spiking neuron</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Circuits and Systems Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="667" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning spike time codes through morphological learning with binary synapses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1572" to="1577" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improved perception-based spiking neuron learning rule for real-time user authentication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="310" to="318" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Supervised learning in spiking neural networks with noise-threshold</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="333" to="349" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spikedriven synaptic plasticity: theory, simulation, VLSI implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annunziato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Badoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2227" to="2258" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A synaptic plasticity rule providing a unified approach to supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kiselev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3806" to="3813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">The organization of behavior: a neuropsychological theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Biological context of Hebb learning in artificial neural networks, a review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kuriscak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marsalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stroffek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Spike timing-dependent plasticity: a Hebbian learning rule</title>
		<author>
			<persName><forename type="first">N</forename><surname>Caporale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="25" to="46" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The spike-timing dependence of plasticity</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="556" to="571" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning temporally encoded patterns in networks of spiking neurons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">What can a neuron learn with spiketiming-dependent plasticity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Naeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2337" to="2382" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Functional requirements for reward-modulated spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frémaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sprekeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="13326" to="13337" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A learning theory for rewardmodulated spike-timing-dependent plasticity with application to biofeedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pecevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000180</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Theoretical analysis of learning with reward-modulated spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pecevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Supervised learning in SNN via reward-modulated spike-timing-dependent plasticity for a target reaching vehicle</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning temporally precise spiking patterns through reward modulated spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grüning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Supervised spike-timingdependent plasticity: a spatiotemporal neuronal learning rule for function approximation and decisions</title>
		<author>
			<persName><forename type="first">J.-M</forename><forename type="middle">P</forename><surname>Franosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Van Hemmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3113" to="3130" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">SEFRON: a new spiking neuron model with time-varying synaptic efficacy function for pattern classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jeyasothy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1231" to="1240" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Supervised learning in spiking neural networks J o u r a l P r e -p r o o f Journal Pre-proof with ReSuMe: sequence learning, classification, and spike shifting</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ponulak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasiński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="510" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Analysis of the ReSuMe learning process for spiking neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ponulak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasiński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Mathematics and Computer Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Receptive field optimisation and supervision of a fuzzy spiking neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glackin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="256" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A spike-timing-based integrated model for pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="472" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">DL-ReSuMe: a delay learning-based remote supervised method for spiking neurons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3137" to="3149" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Multi-DL-ReSuMe: multiple neurons delay learning remote supervised method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A cross-correlated delay shift supervised learning method for spiking neurons with application to interictal spike detection in epilepsy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cabrerizo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adjouadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1750002</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Triplets of spikes in a model of spike timingdependent plasticity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="9673" to="9682" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">An improved supervised learning algorithm using triplet-based spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inter-J o u r a l P r e -p r o o f Journal Pre-proof national Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="44" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Strictly positive-definite spike train kernels for point-process divergences</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Príncipe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2223" to="2250" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A reproducing kernel Hilbert space framework for spike train signal processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R C</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Príncipe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="424" to="449" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Kernel methods on spike train space for neuroscience: a tutorial</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R C</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Linear algebra for times series of spikes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="363" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Gram-Schmidt orthogonalization: 100 years and more</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å</forename><surname>Björck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Linear Algebra with Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="532" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">SPAN: spike pattern association neuron for learning spatio-temporal spike patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohemmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1250012</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Training spiking neural networks to associate spatio-temporal inputcoutput spike patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohemmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Incremental learning algorithm for spatiotemporal spike pattern classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohemmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Precise-spike-driven synaptic plasticity: learning hetero-association of spatiotemporal spike patterns</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">78318</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A brain-inspired spiking neural network model with temporal encoding and learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">A spiking neural network system for robust sequence recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="635" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A new supervised learning algorithm for spiking neurons based on spike train kernels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2877" to="2886" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Supervised learning of single-layer spiking neural networks for image classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence Applications and Technologies</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page">12049</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">An online supervised learning algorithm based on nonlinear spike train kernels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Supervised learning algorithm for spiking neurons based on nonlinear inner products of spike trains</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A delay learning algorithm based on spike train kernels for spiking neurons</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">252</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Optimal spike-timingdependent plasticity for precise action potential firing in supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toyoizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1318" to="1348" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">An overview of bayesian methods for neural spike train analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">251905</biblScope>
			<date type="published" when="2013">2013. 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Supervised learning in spiking neural networks for precise temporal encoding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grüning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">161335</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Backpropagation for populationtemporal coded spiking neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="1797" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A sequential learning algorithm for a minimal spiking neural network (MSNN) classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="2415" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">A sequential learning algorithm for a spiking neural classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">On-line learning with structural adaptation in a network of spiking neurons for visual pattern recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Wysoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benuskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Online spatio-temporal pattern recognition with evolving spiking neural networks utilising address event representation, rank order, and temporal spike learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dhoble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nuntalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">SpikeTemp: an enhanced rank-order-based learning approach for spiking neural network-J o u r n a l P r e -p r o o f Journal Pre-proof s with adaptive structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="43" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Synthesis of neural networks for spatio-temporal spike pattern recognition and processing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Stiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Buskila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">CONE: convex-optimizedsynaptic efficacies for temporally precise spike mapping</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Kukreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Thakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="861" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">EMPD: an efficient membrane potential driven supervised learning algorithm for spiking neurons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="162" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A highly effective and robust membrane potential-driven supervised learning method for spiking neurons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="137" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Efficient and robust supervised learning algorithm for spiking neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensing and Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Relative ordering learning in spiking neural network for pattern recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">First error-based supervised learning algorithm for spiking neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">559</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Spike timing or rate? Neurons learn to make decisions for both through threshold-driven plasticity</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2178" to="2189" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Efficient training of supervised spiking neural network via accurate synaptic-efficiency adjustment method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1424" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Theories of error back-propagation in the brain</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Backpropagation through time and the brain</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Error-backpropagation in temporally encoded networks of spiking neurons</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poutré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A remark on the error-backpropagation learning algorithm for spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1118" to="1120" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Supervised learning with spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Embrechts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1772" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Extending SpikeProp</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="471" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Improving SpikeProp: enhancements to an error-backpropagation rule for spiking neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ProRISC workshop</title>
		<meeting>the 15th ProRISC workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="301" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Learning under weight in networks of temporal encoding spiking neurons</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glackin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1912" to="1922" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Improving the efficiency of spiking network learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Thiruvarudchelvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antolovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">BPSpike: a backpropagation learning for all parameters in spiking neural networks with multiple layers and multiple spikes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">BPSpike II: a new backpropagation learning algorithm for spiking neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Fast modifications of the SpikeProp algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mckennoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Bushnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="3970" to="3977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Spike-timing error backpropagation in theta neuron networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mckennoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Voegtlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bushnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="45" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Modified spiking neuron that involves derivative of the state function at firing time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="144" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Adaptive learning rate of SpikeProp based on weight convergence analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Adaptive delay learning in SpikeProp based on delay convergence analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Robust spike-train learning in spike-event based weight update</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="54" to="68" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Robust learning in SpikeProp</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="33" to="46" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Robustness to training disturbances in SpikeProp learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3126" to="3139" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The convergence analysis of spikeprop algorithm with smoothing L 1/2 regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Supervised learning based on temporal coding in spiking neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3227" to="3235" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A gradient descent rule for spiking neurons emitting multiple spikes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="552" to="558" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Spiking neural networks with neurons firing multiple spikes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="638" to="644" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh-Dastidara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1419" to="1431" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">An extended algorithm using adaptation of momentum and learning rate for spiking neurons</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Work-Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Improving the stability for spiking neural networks using anti-noise learning rule</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim International Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Multi-spike timing error backpropagation algorithm in spiking neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2013</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northwest Normal University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Learning spatiotemporally encoded pattern transformations in structured spiking neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sporea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grüning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2548" to="2586" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Learning precise spike train-to-spike train transformations in multilayer feedforward neuronal networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="826" to="848" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Spatio-temporal backpropagation for training high-performance spiking neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">331</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">SuperSpike: supervised learning in multi-layer spiking neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1514" to="1541" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">An STDP training algorithm for a spiking neural network with dynamic threshold neurons</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Strain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Sayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="463" to="480" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Cooperk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="48" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Relating STDP to BCM</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Izhikevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Desai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1511" to="1523" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">SWAT: a spiking neural network training algorithm for classification problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Mcdaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Sayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1817" to="1830" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Online supervised learning for hardware-based multilayer spiking neural networks through the modulation of weightdependent spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4287" to="4302" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Learning in memristor crossbar-based spiking neural networks through modulation of weight-dependent spike-timingdependent plasticity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="520" to="532" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Supervised learning in multilayer spiking neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sporea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grüning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="473" to="509" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">A supervised learning algorithm for learning precise timing of multiple spikes in multilayer spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5394" to="5407" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Supervised learning in multilayer spiking neural networks with inner products of spike trains</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Supervised learning based on convolution calculation for multilayer spiking neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Engineering and Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="348" to="353" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Temporal learning in multilayer spiking neural networks through construction of causal connections, in: Neuromorphic Cognitive Systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="115" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">An evolutionary strategy for supervised training of biologically plausible neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence and Natural Computing</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="1524" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Evolutionary design of spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Mathematics and Natural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="253" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Spiking neural network training using evolutionary algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Tasoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Plagianakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nikiforidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Jiont Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2190" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Optimization of spiking neural networks with dynamic synapses for spike sequence generation using PSO</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohemmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schliebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dhoble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="2969" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Training spiking neurons by means of particle swarm optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Garro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Swarm Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Training spiking neural models using artificial bee colony</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Garro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2015">2015. 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Supervised training of spiking neural network by adapting the E-MWO algorithm for pattern classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abusnaina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="661" to="682" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">An online supervised learning method for spiking neural networks with adaptive structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="526" to="536" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">A two stage learning algorithm for a growing-pruning spiking neural network for pattern classification problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">A training algorithm for spike sequence in spiking neural networks -a discussion on growing network for stable training performance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Takuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Haruhiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiroharu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shinji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1773" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">SpikeCD: a parameterinsensitive spiking neural network with clustering degeneracy strategy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Efficient training of supervised spiking neural networks via the normalized perceptron based learning rule</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="152" to="163" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">An efficient supervised training algorithm for multilayer spiking neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">150329</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Dynamics of a recurrent network of spiking neurons before and following learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brunel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="373" to="404" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Spontaneous dynamics of asymmetric random recurrent spiking neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beslon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mazet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1002211</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Spatiotemporal dynamics and reliable computations in recurrent spiking neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18103</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons can emulate arbitrary hopfield nets in temporal coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Natschläger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="371" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Learning beyond finite memory in recurrent networks of spiking neurons</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tiňo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J S</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="591" to="613" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">U</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zarrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Pedroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Neftci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Rebooting Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Gradient descent for spiking neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1438" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Sequence learning with hidden units in spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Matching recall and storage in sequence learning with spiking neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="9565" to="9575" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Connectivity reflects coding: a model of voltage-based STDP with homeostasis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Büsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vasilaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="344" to="352" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Variational learning for recurrent spiking networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, Neural Information Processing Systems Foundation</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Stochastic variational learning in recurrent spiking networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Learning precisely timed spikes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Ölveczky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28295</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">A supervised multi-spike learning algorithm for recurrent spiking neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="222" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Supervised learning in spiking neural networks with ReSuMe method</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ponulak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Poznan University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Online versus offline learning for spiking neural networks: a review and new strategies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cybernetic Intelligent Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">An online supervised learning method based on gradient descent for spiking neurons</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="7" to="20" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Variable-structure-systems based approach for online learning of spiking neural networks and its experimental evaluation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3269" to="3285" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">A theory of local learning, the learning channel, and the optimality of backpropagation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="51" to="74" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Integrating space, time, and orientation in spiking neural networks: a case study on multimodal brain data modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Mcnabb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5249" to="5263" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Bear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bidirectional synaptic plasticity: from theory to reality, Philosophical Transactions of the Royal Society of London B</title>
		<imprint>
			<date type="published" when="1432">1432. 2003. 2003</date>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="649" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Bidirectional modification of presynaptic neuronal excitability accompanying spike timing-dependent synaptic plasticity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Learning by the dendritic prediction of somatic spiking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="528" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Somato-dendritic synaptic plasticity and error-backpropagation in active dendrites</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schiess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1004638</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Learning with three factors: modulating hebbian plasticity with errors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kuśmierz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Isomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toyoizumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="170" to="177" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">BP-STDP: approximating backpropagation using spike timing dependent plasticity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Supervised learning algorithm for multi-spike liquid state machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="243" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: an overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Deep learning in spiking neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavanaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Deep learning with spiking neurons: opportunities and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">774</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">STDPbased spiking deep convolutional neural networks for object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Biologically plausible deep learningcbut how far can we go with shallow networks?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Illing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="90" to="101" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Spiking neural networks hardware implementations and challenges: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valentian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesquida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rummens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyboz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vianello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal on Emerging Technologies in Computing Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
