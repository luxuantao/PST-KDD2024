<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Integrated Systems Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Hager</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Integrated Systems Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Integrated Systems Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A88D0436DA1699F3BBC702B01BDF19BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks.</p><p>We present a novel 12-layer deep convolutional network for image compression artifact suppression with hierarchical skip connections and a multi-scale loss function. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an improvement of up to 0.36 dB over the best previous ConvNet result. We show that a network trained for a specific quality factor (QF) is resilient to the QF used to compress the input image-a single network trained for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Compression methods can be split into two categories: lossless (e.g. PNG) and lossy (e.g. JPEG) <ref type="bibr" target="#b0">[1]</ref>. While lossless methods provide the best visual experience to the user, lossy methods have an non-invertible compression function but can achieve a much higher compression ratio. They often come with a parameter to span the trade-off between file size and quality of the decompressed image. In practical uses, lossy compression schemes are often preferred on consumer devices for their much higher compression rate <ref type="bibr" target="#b0">[1]</ref>.</p><p>Particularly at high compression rates, the differences between the decompressed and the original image become visible with artifacts that are specific of the applied compression scheme. These are not only unpleasant to see, but also have a negative impact on many low-level vision algorithms <ref type="bibr" target="#b1">[2]</ref>. Many compression algorithms rely on tiling the images into blocks, applying a sparsifying transform and re-quantization, followed by a generic loss-less data compression <ref type="bibr" target="#b2">[3]</ref>.</p><p>JPEG has become the most widely accepted standard in lossy image compression <ref type="bibr" target="#b3">[4]</ref>, with many efficient software transcoders publicly available and specialized hardware accelerators deployed in many cameras. Due to its popularity, JPEG-compressed images are also widely found on storage devices containing memories of moments experienced with family and friends, capturing the content of historic documents, and holding on to evidence in legal investigations.</p><p>Image compression is also used in wireless sensors systems to transfer visual information from sensor nodes to central storage and processing sites. In such systems, the transmitting node is often battery-powered and thus heavily power-constrained <ref type="bibr" target="#b4">[5]</ref>. Transmitting data is often the most expensive part in terms of energy, and strong compression can mitigate this by reducing the required transmit energy at the expense of introducing compression artifacts <ref type="bibr" target="#b2">[3]</ref>. Similar challenges are also seen in mobile devices storing data: size and cost constraints limit the amount of memory for data storage, and the energy available on such devices is depleted rapidly when writing to flash memory-so much that it pays off to apply compression before writing to flash memory <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. On the processing site, these space and energy constraints are absent and much more computational power is available to decompress and possibly post-process the transmitted or stored images <ref type="bibr" target="#b2">[3]</ref>.</p><p>Deep convolutional neural networks (ConvNets) have become an essential tool for computer vision, even exceeding human performance in tasks such as image classification <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref>, and semantic segmentation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In addition, they have also started to gain relevance for regression tasks in low-level image and video processing, computing saliency maps <ref type="bibr" target="#b11">[12]</ref>, optical flow fields <ref type="bibr" target="#b12">[13]</ref> and single-image super-resolution images <ref type="bibr" target="#b13">[14]</ref> with state-of-the-art performance.</p><p>In this work, we present 1) the construction of a new deep convolutional neural network architecture to remove compression artifacts in JPEG compressed image data, 2) a strategy to train this deep network, adaptable to other low-level vision tasks, and 3) extensive evaluations on the LIVE1 dataset, highlighting the properties of our network and showing that this is the current state-of-the-art performance ConvNet for compression artifact suppression (CAS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Traditional approaches to suppress compression artifacts can be split into three categories. Various types of intelligent edgeaware denoising such as SA-DCT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, BM3D <ref type="bibr" target="#b16">[17]</ref> have been proposed to address this task during the late 2000s. In recent years, dictionary-based sparse recovery algorithms such as DicTV <ref type="bibr" target="#b17">[18]</ref>, RTF <ref type="bibr" target="#b18">[19]</ref>, S-D2 <ref type="bibr" target="#b19">[20]</ref>, D 3 <ref type="bibr" target="#b20">[21]</ref>, DDCN <ref type="bibr" target="#b21">[22]</ref> have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG. These algorithms explicitly attempt to optimally reverse the effect of DCT-domain quantization using learned dictionaries very specific to the applied compressor and quantization tables.</p><p>This work was inspired by single-image super-resolution ConvNets, which are a special case of compression artifact removal, where the compression is a simple sub-sampling operation. Several networks have shown to be very successful at this task, such as SRCNN <ref type="bibr" target="#b13">[14]</ref> or DRCN <ref type="bibr" target="#b22">[23]</ref>. They use different training procedures and approaches for network construction, but both ConvNets are a simple sequence of convolution and point-wise non-linearity layers.</p><p>Recently, two important works have been published, which apply ConvNets for compression artifact suppression: AR-CNN <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref> and the approach presented in <ref type="bibr" target="#b24">[25]</ref>. The former starts from the architecture presented in SRCNN. In order to overcome convergence problems, they use transfer-learning from the 4-layer network retrained for artifact reduction to a deeper 5-layer network, as well as between networks trained for different JPEG quality factors (QFs) and datasets. In <ref type="bibr" target="#b24">[25]</ref> a residual structure extends the simple stacking of convolutional, non-linearity and pooling layers, such that the network is only trained to produce an increment compensating for the distortions. Furthermore, skip elements where some feature maps are bypassing one or multiple layers and are then concatenated to the feature maps at a later stage were introduced. Additionally, they do not use a plain MSE loss function but also include an additional term to emphasize edges.</p><p>The networks of both works were trained on the 400 images contained in the BSDS500 train and test sets and evaluated on the remaining 100 images in the validation set. Testing of these networks was then performed on the LIVE1 dataset (29 images) <ref type="bibr" target="#b25">[26]</ref> and, in case of AR-CNN, on the 5 test images of <ref type="bibr" target="#b14">[15]</ref> and a self-collected dataset of 40 photographs from twitter as well. We will adopt their test datasets, procedures and quality measures. Our choice of the training dataset is discussed in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We start from the basic concept of training a deep ConvNet for a regression problem, as has been done for the related task of superresolution <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref> or other low-level computer vision operations such as optical flow estimation <ref type="bibr" target="#b12">[13]</ref>. The authors of <ref type="bibr" target="#b24">[25]</ref> propose several new elements for artifact reduction ConvNets: A residual architecture, an edgeemphasized loss function, symmetric weight initialization, and skip connections. All these elements were introduced to alleviate the obstacles preventing the training of deep networks for regression tasks. Taking inspiration from deep neural networks such as FlowNet <ref type="bibr" target="#b12">[13]</ref> and FCN <ref type="bibr" target="#b9">[10]</ref> developed for optical flow estimation and semantic segmentation respectively, we propose a neural network with hierarchical skip connections (cf. Section III-A) and a multi-scale loss function (cf. Section III-C) for compression artifact suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>An overview of our proposed network is shown in Figure <ref type="figure">1</ref>. The blocks A, . . . , D each consist of two convolutional layers, increasing the number of channels from 1 to 128 and later to 256, the deeper they are in the network. At the same time the resolution is reduced by down-sampling (DS), which is implemented with 2 × 2 pixel average-pooling layers with 2 × 2 stride. The main path through the ConvNet (marked A (1)  conv 128 1 3 × 3 1k A (2)  conv 128 128 3 × 3 147k B (1)  conv 128 128 3 × 3 147k B (2)  conv 128 128 3 × 3 147k C (1)  conv 128 256 3 × 3 295k C (2)  conv 256 256 3 × 3 590k D (1)  conv 256 256 3 × 3 590k D (2)  conv 256 256</p><formula xml:id="formula_0">3 × 3 590k D fullconv 256 256 4 × 4 /2 1049k D conv 1 256 3 × 3 2k C fullconv 128 513 4 × 4 /2 1051k Ĉ conv 1 513 3 × 3 5k B fullconv 128 257 4 × 4 /2 526k B conv 1 257 3 × 3 2k Â conv 1 257 3 × 3 2k Total 5144k</formula><p>blue in Figure <ref type="figure">1</ref>) then proceeds through the full-convolution<ref type="foot" target="#foot_1">1</ref> layers D, . . . , B and the normal convolution layer Â. This way we obtain a 12-layer ConvNet, which however cannot be trained to achieve state-of-the-art accuracy using standard training methods. In the following, we list modifications to the network reducing the average path length, allowing it to converge to beyond state-of-the-art accuracy.</p><p>To reduce the path length, the higher-resolution intermediate results after each full-convolution layer are enhanced in the next layer by concatenating the lower-level features extracted earlier in the network natively at this resolution (marked red in Figure <ref type="figure">1</ref>). We expect this to benefit the network two-fold: once through the additional information to help infer high-resolution outputs, and second to aid in training the early layers of the network by means of bypassing the middle layers.</p><p>Training deep networks for regression tasks is problematic and while we have reduced the path length for some paths (e.g. input → A → Â → output) using the aforementioned method, some very long paths remain. The gradients for adjusting the weights of D are propagated from the output through Â, B, C, D, D. To improve on this, we introduce a multi-scale optimization criterion: instead of optimizing inputto-output, we reconstruct low-resolution images already from deep within the network using a single convolutional layer (marked green in Figure <ref type="figure">1</ref>), i.e. D, Ĉ, B for 1/64-th, 1/16-th, and 1/4-th of the resolution, respectively. We do not discard the output, but up-sample (US) it by a factor of 2× in each spatial dimension using nearest-neighbor interpolation and concatenate it to the feature maps generated by the full-convolution layer parallel to this path (marked yellow in Figure <ref type="figure">1</ref>). Using this configuration, we have further shortened the deepest stack of layers significantly by reducing the distance from the middle layers to the output.</p><p>The parameters of the convolution and full-convolution layers are listed in Table <ref type="table" target="#tab_0">I</ref>. All these layers are followed by a Parametric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b27">[28]</ref> activation layer, where the slope for negative inputs is learned from data rather than pre-defined. These units have shown superior performance for ImageNet classification <ref type="bibr" target="#b27">[28]</ref>, reducing the issues of dead features <ref type="bibr" target="#b28">[29]</ref>.</p><p>We have found that learning a residual to the input image instead of the reconstructed image as suggested in previous work <ref type="bibr" target="#b24">[25]</ref> did not improve the performance of the proposed ConvNet and thus do not include it in our network. The initial weight and bias values are drawn uniformly from the interval -n</p><formula xml:id="formula_1">-1/2 in , n -1/2 in</formula><p>, where n in is the number of input channels into that layer.</p><p>Batch normalization has shown to reduce the achievable accuracy. The batch-wise normalization of means and variances introduces batch-to-batch jitter thereof into the system, preventing full convergence of the network to the maximum accuracy obtained otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Metrics</head><p>The most wide-spread performance metrics to evaluate differences between images and many other signals are the mean-squared error (MSE) and the closely related peak signalto-noise ratio (PSNR). The MSE is the pixel-wise average over the squared difference in intensity between the distorted and the reference image. The PSNR is the MSE normalized to the maximum possible signal values typically expressed in decibel (dB). Following <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> with pixel values normalized to the range [0, 1], we use PSNR(X, X) = 10 log 10 1/MSE(X, X) ,</p><p>(1)</p><formula xml:id="formula_2">MSE(X, X) = ⎛ ⎝ p∈P e(x p , xp ) 2 ⎞ ⎠ / |P| , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where P is the set of pixel indexes, X is the reference image, X is the image to evaluate, and e is the per-pixel error function (e.g. |x p -xp | for grayscale images).</p><p>Both metrics are fully referenced, comparing individual pixels to the original image and converging to zero for a perfect reconstruction. They are known to differ from perceived visual quality <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> but find wide-spread use due to their simplicity. A variation of the PSNR measure is the IPSNR (increase in PSNR), which is the PSNR difference to the baseline distorted image and thus measures quality improvement. It is also more stable across different datasets.</p><p>A popular alternative is to use the structural similarity index (SSIM) <ref type="bibr" target="#b29">[30]</ref>, which is the mean of the product of three terms assessing similarity in luminance, contrast and structure over multiple localized windows. We use the Matlab implementation provided with <ref type="bibr" target="#b29">[30]</ref> for evaluation and use the same parameters as related work <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>: K 1 = 0.01, K 2 = 0.03, and a 8×8 local statistics window w of ones. A third measure used in related work is the PSNR-B <ref type="bibr" target="#b32">[33]</ref>, which adds a (non-referenced) blocking effect factor (BEF) term to the MSE measure. The BEF measures luminance discontinuities at the horizontally and vertically oriented block boundaries. We define the IPSNR-B analogous to the IPSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function</head><p>During the training of the ConvNets we minimize the MSE criterion, penalizing deviations from the reference image by the squared distance. However, as mentioned in Section III-A, in order to improve the training procedure we include not only the full-resolution output, but also the low-resolution outputs from within the network. The reference for these is computed by down-sampling the input image, averaging across 4, 16 and 64 pixels, respectively. Each of these outputs' MSE contributes equally to the overall multi-scale (MS) loss function.</p><p>We run the training until convergence with this objective, before removing the lower resolution images from the loss function and continue the training for several epochs to minimize the MSE of only the full-resolution output image (output loss), fine-tuning (FT) the network with this optimization objective. For QF 20, training is performed with MS loss until saturation of the testing MSE (Epoch 200), and then fine-tuned with the output loss/MSE criterion before selecting the weights with the lowest testing MSE (cf. Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>In previous work, including an edge-emphasized term into the loss function has been proposed <ref type="bibr" target="#b24">[25]</ref>. We decided not to introduce such a loss term because it leads to an additional hyperparameter to adjust the weight and because we consider it inconsistent to train the network with a loss function different from the quality measure used to benchmark the results. Tuning the hyperparameters for the best PSNR would result in choosing the weight value of the edge-emphasized loss term to be zero.</p><p>As such, it prevents further improvement in terms of PSNR and SSIM beyond some limit, and the factor with which it is weighted can be used to trade-off overall reconstruction quality and deblocking. We do not include such a term in our setup because our main objective is to maximize the overall reconstruction, which already implies a high-quality deblocking. By training on a large dataset we do not require such a regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataset</head><p>Previous networks for compression artifact reduction were trained on the 400 train and test images of the BSDS500 dataset and tested on the 100 remaining validation images <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The authors of <ref type="bibr" target="#b24">[25]</ref> show that this is the limiting factor for further improvement of their larger L8 network with 220k learned parameters. We do not want to constrain the size of our network by the amount of available training data, particularly since we do not need hard-to-obtain labels for it. We thus use the large, widely-known and publicly available ImageNet 2013 detection dataset <ref type="bibr" target="#b34">[35]</ref>, which consists of 396k training and 20k validation color images of various sizes. From each image we take cut-outs of 120 × 120 pixels to generate our dataset.</p><p>The color images are transformed to YCbCr space and only the luminance channel is used further. The input to the network is then generated by compressing the resulting image using the Matlab JPEG compressor 2 with a bit depth of 8.</p><p>For training our network we take 50k images of the 120 × 120 pixel cut-outs from the training set and 10k cut-outs for the validation set. We increase the size of the training set to 150k for fine-tuning with the output loss function. Testing is performed on the 29 images of the LIVE1 dataset.</p><p>We use the Torch framework <ref type="bibr" target="#b35">[36]</ref> with cuDNN v5.1.3 <ref type="bibr" target="#b36">[37]</ref> for our evaluations. We use the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> starting with a learning rate of 10 -4 . A minibatch size of 20 images was chosen and training was parallelized over two Nvidia Titan X Maxwell GPUs. We have not applied any preprocessing to the images before feeding them into the network. Our main training was conducted for QF 20 compressed input data and we have trained the networks for other quality factors starting from this one to reduce training time. For the forward pass, a throughput of 1.01 Mpixel/s has been measured with a Nvidia GTX1080 using single-precision floating-point operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS &amp; DISCUSSION</head><p>We have evaluated the mean PSNR, PSNR-B and SSIM across the LIVE1 dataset for the JPEG quality factors 10, 20, 40, 60 and 80, and compare them to related work in Table <ref type="table" target="#tab_2">II</ref>. We use the same JPEG compressor as in AR-CNN <ref type="bibr" target="#b23">[24]</ref> and Svoboda et al. <ref type="bibr" target="#b24">[25]</ref> (i.e. Matlab), with which we obtain the identical baseline PSNR of 30.07 dB for QF 20 and 27.77 dB for QF 10 for the JPEG compressed image with respect to the uncompressed reference. 2 We have used this compressor to remain comparable with related work. Other implementations such as libjpeg or libjpeg-turbo use different quantization tables and, in case of these two libraries, result in a significantly larger file size and as a consequence also a better PSNR for the same quality factor.  For our network, we list results directly after training with the multi-scale loss function as well as after fine-tuning with the output loss. The already state-of-the-art results are further improved by this two-step learning procedure. Overall, we can see a significant improvement in PSNR of 0.19 dB over the L8 network <ref type="bibr" target="#b24">[25]</ref>, 0.30 dB over AR-CNN and 1.63 dB over ordinary JPEG for QF 20. The SSIM is also improved to 0.895. For QF 10 we see a gain of 1.67 dB over ordinary JPEG, 0.36 dB over the L4 network and 0.31 dB over AR-CNN, the state-of-the-art ConvNet for this configuration.</p><p>For QF 10, we improve the PSNR-B by 0.45 dB over previous work. However, for a lower compression rate, we do not exceed the PSNR-B value achieved by the L8 network. As described in the next paragraph, there are no visible blocking artifacts after applying our ConvNet. PSNR-B has been introduced for benchmarking deblocking algorithms, and by its definition the blocking artifact-penalizing term measuring the differences between pixels along the block boundary does not vanish even for a perfect reconstruction. An image with higher reconstruction quality might thus suffer from a lower PSNR-B value because of clearer edges all over the image including at the block boundaries.</p><p>In Figure <ref type="figure">3</ref> we show the distribution of the individual images of the LIVE1 dataset in terms of PSNR and SSIM with respect to the used number of bits per pixel for several QFs. The average PSNR and SSIM for each QF is also shown, visualizing that this method works for strong as well as for weak compression. Looking at the individual images, it becomes visible that our method improves not only the mean PSNR and SSIM, but enhances each individual image.</p><p>As discussed in Section III-B, the visual perception can differ from quantitative evaluations using classical quality measures. To give a visual impression as well, we provide a qualitative visual comparison in Figure <ref type="figure" target="#fig_2">5</ref>. The lighthouse3 image serves as a basis for this comparison and is the same one used in <ref type="bibr" target="#b24">[25]</ref>. It is shown with black markers in Figure <ref type="figure">3</ref>, indicating that this image is not a particularly wellworking outlier. A clear improvement is visible, there are no perceptible blocking artifacts anymore and the ringing artifacts are strongly suppressed without blurring the railing depicted in the image. For completeness, we also provide the results for the 5 classical test images used throughout many compression papers (cf. Figure <ref type="figure">6</ref>). The trained models and scripts required to reproduce these images are available online 3 .</p><p>In Figure <ref type="figure">4</ref>, we show that the networks trained for a specific quality factor do not need to be retrained for the specific quality factor with which the image was compressed to achieve a high improvement in PSNR or PSNR-B. The network trained for QF 60 already boosts the PSNR by more than 1.5 dB for quality factors ranging from 25 to almost 60. This resilience to variations in quantization has not been shown for approaches focusing on DCT-domain recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a 12-layer deep convolutional neural network for compression artifact suppression in JPEG images with hierarchical skip connections and trained with a multi-scale loss function. The result is a new state-of-the-art ConvNet achieving a boost of up to 1.79 dB in PSNR over ordinary JPEG and showing an improvement of up to 0.36 dB over the best previous ConvNet result. We have shown that a network trained for a specific quality factor is resilient to the QF used to compress the input image-a single network trained for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76. The obtained results are also qualitatively superior to those of existing ConvNets. The network is not tailored to the JPEG-specific compression procedure, and can thus potentially be applied to a wide range of image compression algorithms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Loss improvement by number of training epochs for compression with quality factor 20. The multi-scale MSE loss is minimized ( , scaled up by 3× for readability) until the testing MSE ( ) is saturated at Epoch 200, after which the network is fine-tuned using the MSE loss on the output image (). Then the parameters for which the testing MSE () is minimal are selected. Note: An epoch during the fine-tuning phase contains 150k instead of 50k images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig.3: PSNR (left) and SSIM (right) evaluated on the LIVE1 dataset with respect to the number of bits per pixel required to store the compressed image. The ordinary JPEG performance is shown as () for QF 10 to 90 in steps of 10, averaged over all images in the dataset. Individual images are shown with markers: ordinary JPEG ( ), after CAS-CNN ( ). The image depicted in Figure5is marked with ( and ). The different quality factors are color coded: QF 20 ( ), QF 40 ( ), QF 60 ( ), QF 80 ( ). The CAS-CNN output quality averaged over the dataset is shown as ().</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Qualitative comparison of reconstruction quality on the lighthouse3 image of the LIVE1 dataset for JPEG quality factor 20. Images (a),(b),(d),(e) reprinted with permission from [25].</figDesc><graphic coords="7,53.75,303.17,161.90,221.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Hyperparameters of the Layers name type #outp. ch. #inp. ch. filter size #param.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Structure of the proposed ConvNet. The paths are color coded: main path (bold), concatenation of lower-level features, multi-scale output paths, re-use of multi-scale outputs.</figDesc><table><row><cell></cell><cell>Stage 1</cell><cell></cell><cell></cell><cell>Stage 2</cell><cell></cell><cell></cell><cell>Stage 3</cell><cell>Stage 4</cell></row><row><cell>input</cell><cell>1×h×w</cell><cell cols="2">128×h×w</cell><cell>DS</cell><cell cols="2">128×(h/2)×(w/2)</cell><cell>DS</cell><cell>256×(h/4)×(w/4)</cell><cell>DS</cell></row><row><cell></cell><cell cols="2">257×h×w</cell><cell>concat</cell><cell>128×h×w</cell><cell>concat</cell><cell cols="2">128×(h/2)×(w/2)</cell><cell>concat</cell><cell>256×(h/4)×(w/4)</cell><cell>256×(h/8)×(w/8)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>US</cell><cell></cell><cell></cell><cell>US</cell><cell>US</cell></row><row><cell>output</cell><cell cols="2">1×h×w</cell><cell></cell><cell></cell><cell>1×(h/2)×(w/2)</cell><cell></cell><cell cols="2">1×(h/4)×(w/4)</cell><cell>1×(h/8)×(w/8)</cell></row><row><cell></cell><cell>MSE loss</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Equi-weighted multi-scale MSE loss</cell></row><row><cell>Fig. 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Restoration Quality Comparison on LIVE1</figDesc><table><row><cell cols="2">QF Algorithm</cell><cell></cell><cell cols="3">PSNR [dB] PSNR-B [dB] SSIM</cell></row><row><cell></cell><cell>JPEG</cell><cell>[34]</cell><cell>27.77</cell><cell>25.33</cell><cell>0.791</cell></row><row><cell></cell><cell>SA-DCT</cell><cell>[15]</cell><cell>28.65</cell><cell>28.01</cell><cell>0.809</cell></row><row><cell>10</cell><cell>AR-CNN L4</cell><cell>[2] [25]</cell><cell>29.13 29.08</cell><cell>28.74 28.71</cell><cell>0.823 0.824</cell></row><row><cell></cell><cell>ours, MS loss</cell><cell></cell><cell>29.36</cell><cell>28.92</cell><cell>0.830</cell></row><row><cell></cell><cell>ours, w/ loss FT</cell><cell></cell><cell>29.44</cell><cell>29.19</cell><cell>0.833</cell></row><row><cell></cell><cell>JPEG</cell><cell>[34]</cell><cell>30.07</cell><cell>27.57</cell><cell>0.868</cell></row><row><cell></cell><cell>SA-DCT</cell><cell>[15]</cell><cell>30.81</cell><cell>29.82</cell><cell>0.878</cell></row><row><cell></cell><cell>AR-CNN</cell><cell>[2]</cell><cell>31.40</cell><cell>30.69</cell><cell>0.890</cell></row><row><cell>20</cell><cell>L4</cell><cell>[25]</cell><cell>31.42</cell><cell>30.83</cell><cell>0.890</cell></row><row><cell></cell><cell>L8</cell><cell>[25]</cell><cell>31.51</cell><cell>30.92</cell><cell>0.891</cell></row><row><cell></cell><cell>ours, MS loss</cell><cell></cell><cell>31.67</cell><cell>30.84</cell><cell>0.894</cell></row><row><cell></cell><cell>ours, w/ loss FT</cell><cell></cell><cell>31.70</cell><cell>30.88</cell><cell>0.895</cell></row><row><cell></cell><cell>JPEG</cell><cell>[34]</cell><cell>32.35</cell><cell>29.96</cell><cell>0.917</cell></row><row><cell></cell><cell>SA-DCT</cell><cell>[15]</cell><cell>32.99</cell><cell>31.79</cell><cell>0.924</cell></row><row><cell>40</cell><cell>AR-CNN L4</cell><cell>[2] [25]</cell><cell>33.63 33.77</cell><cell>33.12 -</cell><cell>0.931 -</cell></row><row><cell></cell><cell>ours, MS loss</cell><cell></cell><cell>33.98</cell><cell>32.83</cell><cell>0.935</cell></row><row><cell></cell><cell>ours, w/ loss FT</cell><cell></cell><cell>34.10</cell><cell>33.68</cell><cell>0.937</cell></row><row><cell>60</cell><cell>JPEG ours, w/ loss FT</cell><cell>[34]</cell><cell>33.99 35.78</cell><cell>31.89 35.10</cell><cell>0.940 0.954</cell></row><row><cell>80</cell><cell>JPEG ours, w/ loss FT</cell><cell>[34]</cell><cell>36.88 38.55</cell><cell>35.47 37.73</cell><cell>0.964 0.973</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-5090-6182-2/17/$31.00 ©2017 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We use the definition of full-convolution (also known as up-convolution, deconvolution, backwards convolution, or fractional-strided convolution) as described in<ref type="bibr" target="#b9">[10]</ref>,<ref type="bibr" target="#b26">[27]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Thilo Weber and Jonas Wiesendanger for their preliminary explorations on this topic. This work has received funding from armasuisse Science &amp; Technology and the European Union's Horizon 2020 research and innovation programme under grant agreement no. 732631.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Why is image quality assessment so difficult?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Convolution Networks for Compression Artifacts Reduction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02778</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survey of image compression algorithms in wireless sensor networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Symp. Inf. Technol</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">HTTP Archive -Interesting Stats</title>
		<author>
			<persName><forename type="first">S</forename><surname>Souders</surname></persName>
		</author>
		<ptr target="http://httparchive.org/interesting.php" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A lowpower wireless video sensor node for distributed object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kerhet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Real-Time Image Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="342" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Energy Aware Lossless Data Compression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MobiSys</title>
		<meeting>of MobiSys</meeting>
		<imprint>
			<date type="published" when="2003-05">May, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy-aware data compression for Multi-Level Cell (MLC) flash memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE Des</title>
		<meeting>ACM/IEEE Des</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="716" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating Real-Time Embedded Scene Labeling with Convolutional Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE Des</title>
		<meeting>ACM/IEEE Des</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection by multicontext deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="page" from="1265" to="1274" />
			<date type="published" when="2015-06">June. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Glokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>arXiv:15047.06852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality deblocking of compressed color images</title>
	</analytic>
	<monogr>
		<title level="s">Eur. Signal Process. Conf.</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3D transformation-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing artifacts in JPEG decompression via a learned dictionary</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Loss-Specific Training of Non-Parametric Image Restoration Models: A New State of the Art</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data-driven sparsity-based restoration of JPEG-compressed images in dual transform-pixel domain</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5171" to="5178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building Dual-Domain Representations for Compression Artifacts Reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeply-Recursive Convolutional Network for Image Super-Resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04491</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Int. Conf. Comput. Vis. IEEE</title>
		<imprint>
			<date type="published" when="2015-12">dec 2015</date>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compression Artifacts Removal Using Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zemcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. WSCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">LIVE image quality assessment database release 2</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<date type="published" when="2014-11">nov 2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Digital Images and Human Vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ch. What&apos;s wro</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="207" to="220" />
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quality Assessment of Deblocked Images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2011-01">jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">MATLAB version 8.5 (R2015a)</title>
		<meeting><address><addrLine>Natick, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The Mathworks, Inc</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Torch7: A Matlab-like Environment for Machine Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst. Work</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">cuDNN: Efficient Primitives for Deep Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<imprint>
			<date type="published" when="2014-10">oct 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
