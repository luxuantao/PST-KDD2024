<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2AB3E895E91E124AF61B36B47AD4E662</idno>
					<idno type="DOI">10.1109/TIP.2014.2336542</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-In this paper, a learning-based shape descriptor for shape matching is demonstrated. Formulated in a bag-ofwords like framework, the proposed method summarizes the local features extracted from certain shape to generate a integrated representation. It contributes to the speed-up of shape matching, since the distance metric in the vector space analysis can be directly applied to compare the constructed global descriptors, eliminating the time consuming stage of local feature matching. Similar to the philosophy in spatial pyramid matching, a strategy for feature division is applied in the phase of encoded feature pooling and vocabulary learning, which helps to construct a more discriminative descriptor incorporating both global and local information. Also, a local contour-based feature extraction method is designed for 2D shapes, while significant properties of the local contours are inspected for the design of feature division rules. The designed local feature extraction method and the feature division rules manage to reduce the variances of shape representation due to the changes in rotation. In addition to 2D shape, we also present a simple and natural method to extend the proposed method to the scenario of 3D shape representation. The proposed shape descriptor is validated on several benchmark data sets for evaluating 2D and 3D shape matching algorithms, and it is observed that the investigated shape descriptor maintains superior discriminative power as well as high time efficiency. Index Terms-Shape matching, BoW, feature division, vocabulary learning, feature pooling, 2D shape, 3D shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S HAPE matching is a fundamental problem in computer vision with many applications like shape retrieval, object recognition, robot navigation, gesture recognition and animation synthesis. The mechanism for shape matching of our human beings is still unclear, but it is obvious that the existing methods have a large gap with the level of human beings in both recognition accuracy and computational efficiency. One of the most important problems in the current shape analysis community is to obtain a meaningful shape similarity/dissimilarity measure when comparing two shape instances, which can be then applied to large-scale shape matching scenarios. However, traditional pairwise matching based approaches may not always provide a faithful similarity measure for shape comparison, which is capable of dealing with impacts such as large intra-class variations, nonrigid deformation, and part occlusion. These approaches often encounter limitations as they somehow ignore the structure of the underlying data manifold. Thus, there are growing interests in unsupervised learning a context sensitive similarity recently. These methods demonstrate that exploring the context between all the instances of the database can lead to a refined similarity measure, which significantly improves the retrieval/recognition performance.</p><p>To summarize, the previous works on shape similarity measures can be coarsely divided into two categories: the pairwise similarity measure, and the context-based similarity measure. The pairwise similarity measure computes the matching cost between a given pair of shapes by designing some smart shape descriptors with enough discriminative power or making an effort to establish the robust correspondence <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>. On the other hand, the context-based similarity measure enhance the given similarities by exploring the similarity context between all the database instances <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Apparently, the approaches in the second category usually work well based on the output shape similarity measure of that in the first category.</p><p>Meanwhile, although most methods today pay much attention to improving the retrieval accuracy or recognition rate, they often neglect the truth that the time cost of performing pairwise shape matching in a large database is often intolerable and impractical. Thus, some other methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> focusing on efficient shape matching have been investigated recently. But it turns out that many of the approaches cannot handle complex deformations like articulations of part structures, as it is really challenging to get a confident similarity measure without establishing a robust alignment between the plenty of shape descriptors. Therefore, the trade-off between the efficiency and robustness of shape matching is badly required at present. Different from the typical methods for exemplar-based pairwise shape matching, we propose a compact and robust shape representation in this paper. Specifically, each shape instance is represented by a single feature vector, which is robust against the disturbances caused by large intra-class variation and deformation. Consequently, shape matching can be simply accomplished via vector comparison, which is particularly efficient for shape retrieval and ranking in a large database.</p><p>Our work is partly inspired by the recent huge progress in image representation and retrieval under Bag-of-Words (BoW) framework <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>, which does not explicitly provide correspondence to local features in images. Also, it has seldom been applied to shape analysis, since the popular image descriptors like SIFT <ref type="bibr" target="#b17">[18]</ref> are mainly designed for describing the local appearance variations. Thus, our goal is to construct an informative shape representation following the concept of BoW in the proposed method. Given a segmented shape (or a binary image), we decompose its outer contour into several contour fragments, which are our basic shape descriptors for learning the shape codes (or a shape vocabulary). The practice of using contour fragments as basic descriptors is inspired by the phenomenon that contour fragments under different scales contain both global and local shape information and are stable to articulation changes or drastic boundary deformation. The popular coding strategy for local descriptors can be utilized for encoding contour fragments. Then, for each shape, a statistical histogram of shape codes is used to represent the shape; the similarity measures between shapes can be directly computed by comparing these histograms. We do not use the typical matching algorithms such as the Hungarian method, Thin Plate Spline (TPS), Dynamic Programming (DP), RANSAC, etc. Instead, the involved BoW framework in fact provides a natural and fast solution for finding the correspondences of two set of local contour fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work on 2D Shape Matching</head><p>The key issue about 2D shape matching is to find an effective representation of corresponding shape, with limited information (typically a binary mask) in hand. Traditional 2D shape representation methods mainly follow two trends:</p><p>(1) local feature based representation and (2) global feature based representation. Local feature based methods aim at using a set of local features to describe certain shape and most of them are contour based descriptors <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Evolving from object silhouette, skeleton based methods also demonstrate its power in shape representation <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b27">[28]</ref>. On the other hand, global feature based methods tend to summary the underlying information about a shape as a whole, and there are several well known global shape descriptors <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref> playing important roles in the development of 2D shape matching history.</p><p>Meanwhile, problems remain in both local and global feature based methods. When a shape is represented by a collection of local features, evaluating the similarity between two given shapes is typically formalized as the problem of building correspondence between the local features (via the Hungarian method, dynamic programming, etc.). Although better accuracy might be obtained, this could often be time consuming, limiting the promotion of these methods to real time applications. Moreover, advanced learning based classification methods (e.g., SVM <ref type="bibr" target="#b31">[32]</ref>) usually require vector-like input, thus local features based shape representation may not benefit from the performance boost provided by the modern machine learning techniques. With respect to the global feature based method, it often occurs that only the rough structure information related to certain shape is captured by the corresponding descriptor, and the sole use of global information will probably lead to the deficiency of discriminative power.</p><p>Instead of using the manually designed features for describing a shape, some learning based shape descriptors are investigated later. These methods are designed so as to automatically retain the most discriminative information about certain shape and then build a compact representation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b32">[33]</ref>. To design a learning based shape descriptor, both supervised and unsupervised methods can be applied. Nguyen et al. <ref type="bibr" target="#b33">[34]</ref> utilize Support Vector Machine (SVM) for selecting the salient feature points on a shape and describe each point with well designed local descriptors. Dynamic programming (DP) is then applied to match the local features in the given pair of shape for the evaluation of similarity. On the other hand, Li et al. <ref type="bibr" target="#b12">[13]</ref> design a new local descriptor named ROMS, and then construct a global representation with the extracted features by performing unsupervised learning under the Bag-of-Words (BoW) framework. However, it is observed in their experiment that the discriminative power of the BoW representation degenerates in comparison to the direct use of dynamic programming. As we can see, the introduced learning based shape descriptors have demonstrated their potential in shape representation, but there is still room for enhancement in terms of time efficiency and descriptive capability.</p><p>In this paper, we proposed an unsupervised learning based method for integrating local features into a integrated one. The demonstrated method is also inspired by the commonly used Bag-of-Words (BoW) paradigm in natural image matching. Similar to the strategy taken in Spatial Pyramid Matching (SPM) <ref type="bibr" target="#b34">[35]</ref>, a feature division based strategy is applied to encoded feature pooling as well as shape vocabulary (or code book) learning. It intends to help better distinguish the visual primitives in the corresponding feature space and meanwhile incorporate local and global characteristics of certain shape. Also, a contour based local feature extraction method is designed for representing the 2D shape, and important properties of the decomposed contour fragments are inspected, allowing for the construction of rotation invariant feature division criterions. The proposed shape descriptor demonstrates encouraging performance for matching 2D shape in the experiments conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Work on 3D Shape Matching</head><p>Early research of content-based 3D model matching mainly focuses on exploring various shape descriptors, hoping to find the best one to represent 3D models. Shilane et al. <ref type="bibr" target="#b35">[36]</ref> compared 12 different descriptors on the criterion of 3D model database Princeton Shape Benchmark (PSB) given by the Shape Retrieval and Analysis Group at the University of Princeton, and Light Field Descriptor (LFD) <ref type="bibr" target="#b36">[37]</ref> is declared to have the best performance. More recently, more sophisticated descriptors are developed to exploit the significant properties of 3D Shapes. Vranic <ref type="bibr" target="#b37">[38]</ref> composes a shape feature using depth images, silhouettes, and ray-extents of the polygonal mesh. Papadakis et al. <ref type="bibr" target="#b38">[39]</ref> combine previous Fig. <ref type="figure">1</ref>. Our learning based shape representation pipeline. Following a BoW-like framework, we build a compact while robust descriptor from certain shape. Instead of using one universal vocabulary, a feature division based strategy is applied to generate a vocabulary set. Also, when incorporating local features into the final representation, the same feature division criterion is engaged. This helps to construct a more discriminative descriptor for the given shape and makes the shape matching procedure accomplished efficiently via vector comparison techniques.</p><p>2D and 3D descriptors (such as <ref type="bibr" target="#b36">[37]</ref>) to produce a compact representation and achieve superior performance.</p><p>So as to keep track of the development of 3D shape descriptors, ways to categorize these methods are proposed. Akgul et al. <ref type="bibr" target="#b39">[40]</ref> offer us a possibility to review these methods and they classify these methods into four groups: (1) histogram based method, (2) transform based method, (3) 2D view based method and (4) graph based method (see <ref type="bibr" target="#b39">[40]</ref> for detailed illustration). Since our method is originally designed for 2D shapes, next we mainly focus on introducing some state-ofart 2D view-based shape descriptors and how we are inspired from them. The well-known Light Field Descriptor <ref type="bibr" target="#b36">[37]</ref> was also one in this category. A light field is defined as the silhouette projection when the 3D object is captured at certain view angle, and a collection of projections is rendered from sampled positions on a view sphere. Then each projection is represented by some 2D shape descriptor (Zernike moments <ref type="bibr" target="#b28">[29]</ref> and Fourier descriptor <ref type="bibr" target="#b29">[30]</ref>). Eventually, the dissimilarity of two 3D objects is defined as the minimum l 1 difference between some pair of descriptors extracted from the projected silhouettes, after rotation alignment of the corresponding 3D objects is performed. The method works pretty well on many 3D Model benchmarks, despite the intuitive fact that valuable 3D information is discarded under this framework. The research on 2D view-based methods also raises great interest in the community in recent years, and more ingenious methods <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref> are discovered later.</p><p>In this paper, we also manage to find a method under the 2D view-based framework to finish the task of 3D modeling. The method is a simple and natural extension of our 2D shape descriptor, and it proves effective and efficient on the well known 3D shape matching benchmark <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEARNING BASED SHAPE REPRESENTATION</head><p>Unlike most natural images, the underlying information about a shape is often difficult to be obtained via traditional ways aimed at modeling the target appearance or texture (Color Histogram <ref type="bibr" target="#b44">[45]</ref>, SIFT <ref type="bibr" target="#b17">[18]</ref>, HOG <ref type="bibr" target="#b45">[46]</ref> or LBP <ref type="bibr" target="#b46">[47]</ref>, etc.). And this is mostly due to the natural lack of information since a shape is often stored in the digital form of a binary mask, rather than a color or gray-scale image. Therefore, many literatures related to shape representation usually manage to describe a shape by extracting a set of features (Shape Context <ref type="bibr" target="#b2">[3]</ref>, Inner Distance <ref type="bibr" target="#b4">[5]</ref>, etc.) from the object contour and successfully accomplished the job of shape modeling. However, the evaluation of (dis)similarity between a pair of shapes is often formulated as a problem of building correspondences between those local features and obtaining a total score of matching, which is often of great computational cost and time consuming. The overhead for evaluating the similarity score is often blamed for the limitation of these shape modeling methods to many problems, such as shape classification and retrieval.</p><p>In this section, we will present a learning based method for shape modeling (the pipeline of our modeling method is shown in Fig. <ref type="figure">1</ref>). The proposed method follows a BoWlike framework, and seeks to exploit the local information by feature division in the phase of codebook learning and encoded feature pooling. Therefore, the local and global information about certain shape are incorporated, and a integrated representation of the shape is constructed eventually. Therefore, the (dis)similarity between shapes could then be measured via traditional ways used in vector space analysis, thus the computational cost is greatly reduced in this procedure while maintaining the descriptive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Introduction to BoW and Beyond</head><p>In the history of image processing and computer vision, histograms have been widely used in the representation of image. The usage of histogram to describe certain image may originate from the introduction of gray level histogram, where each bin of the histogram counts the number of occurrences of pixels with corresponding gray level. By altering the semantic definition of each bin in the histogram, a number of variants of histogram representations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref> are then proposed.</p><p>Inspired by the commonly used techniques in the text retrieval community, another variant of histogram representation is developed later, which is often mentioned as the Bag of Words (BoW) representation. To help define the visual word in the image, certain unsupervised learning technique is applied to the low level features (often called as the visual primitives) extracted, and a visual word list (also mentioned as a codebook, lexicon or vocabulary) is generated. Thus, the final representation of a given image is presented as the histogram of the visual words, after each low level feature extracted from the image is assigned to the corresponding visual word. The BoW paradigm have demonstrated its power in many image related applications, such as image retrieval <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> and classification <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref>.</p><p>Although the BoW proves its potential in image understanding, the other end of the powerful sword has to be realized. Due to the nature of histogram representation, it is often blamed for the loss of the spatial information thus reducing the discriminative power. To alleviate this problem, the Spatial Pyramid Matching <ref type="bibr" target="#b34">[35]</ref> (SPM) is then investigated. It is intended to compute the spatially constrained local histograms after dividing the image into cells of different pyramid resolutions, and concatenate them with the global one as the final representation. This helps retain the local spatial information in contrast to the BoW paradigm, making it more informative and descriptive. The term 'matching' in the phase Spatial Pyramid Matching indicates that the local visual primitives are equivalently undergoing some approximate matching procedure <ref type="bibr" target="#b50">[51]</ref> in the SPM paradigm, when evaluating the similarity between histograms of two images.</p><p>Still, many questions may arise in the philosophy of the SPM paradigm. In the next section, we will continue to focus on the discussion of problems may be encountered in the SPM, and meanwhile present a vocabulary learning and visual word assigning strategy for shape matching under the BoW paradigm. The proposed method utilizes a feature division based philosophy similar to SPM, so as to enhance the discriminative power while maintaining time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Division Under the BoW Paradigm</head><p>Given a set of local features F (S) extracted from certain shape S as visual primitives, we could already evaluate the similarity between a pair of shapes S i and S j . This can be simply implemented by building correspondences between the two sets of features F (S i ) and F (S j ), resulting a score computed in some way. However, the direct matching of two large feature sets under certain distance metric (e.g., l 2 distance, χ 2 distance) is often computationally prohibited. To alleviate this problem, a typical way is to build a visual vocabulary of the visual primitives in advance, and quantize each visual primitive to one integer (or several decimals). Thus a compact representation of the visual primitives is obtained and the comparison between them will be simply checking whether the given pair of quantized features are the same <ref type="bibr" target="#b16">[17]</ref>. And the matching between two sets of features can be simplified as measuring the similarity between two histograms of the corresponding quantized features. This is also an illustration of the BoW paradigm, in which an approximate matching of the low level features is conducted at the back end.</p><p>Inspired by the fast matching potential of the BoW paradigm, we present a feature division based method under this framework. The insight behind the proposed feature division strategy is quite similar to that of the SPM, which also seeks to incorporate the local and global features to enhance the discriminative power. However, the proposed method differs with SPM in two aspects: (1) the SPM paradigm divides the feature according to the location (spatial information) of each local features, while our method inspects other significant properties; (2) the division of features is not only applied in the stage of computing histogram of the encoded features, but also involved in the codebook learning procedure. And here we do not consider the spatial information of corresponding local features, since shapes often contain large rotation variation in the tested dataset. This is quite different from the scenario when SPM is applied, where the evaluated dataset is well aligned in rotation.</p><p>1) Feature Division in Vocabulary Construction: To construct a visual vocabulary, both supervised (e.g., sparse coding <ref type="bibr" target="#b51">[52]</ref>) and unsupervised (clustering techniques such as k-means) methods can be applied. More often, an unsupervised algorithm is involved to help partition the original feature space into K regions automatically, and each region represents a visual word in the constructed vocabulary. Note that in this paradigm, if two visual primitives fall into the same region r i , they will be assigned to the same visual word identified by an integer i and otherwise they will be treated as two different ones. Therefore, the quality of the partitioning of the original feature space is a crucial factor on the discriminative power of the quantized feature. Conventionally, choosing a high value of the vocabulary size K will result in a more finegrained partition of the feature space and help increase the performance of matching. But this will also lead to greater computational cost in the phase of vocabulary building and visual word assignment if traditional unsupervised technique (e.g., k-means) is used individually. To maintain a high discriminative power of the quantized feature while keeping the computational time reasonable, many unsupervised algorithms (Hierarchical k-means <ref type="bibr" target="#b14">[15]</ref>, Approximate k-means <ref type="bibr" target="#b15">[16]</ref>, Hamming Embedding <ref type="bibr" target="#b16">[17]</ref>) have been investigated and proven efficient in the application of image retrieval.</p><p>In our specific shape matching scenario, we propose a feature division based way to learn the shape vocabulary. Given a training set of local components of shapes as visual primitives, our intuition is that if initially we can group the components by some properties associated with them, and then partition visual primitives in each group using conventional unsupervised learning method, the resulting quality of division in the feature space will be presumably much better than automatically partitioning them as a whole. Meanwhile, it can be observed that the number of visual primitives in each group is relatively small and the number of divisions k i in the ith group can be also chosen as a smaller value, thus boosting the speed when constructing the entire visual vocabulary or quantizing the visual primitives.</p><p>More specifically, when using a set of local components C(S) for representing certain shape S, more than vectorizing every component c ∈ C(S) by some descriptor</p><formula xml:id="formula_0">x = f (c) ∈ R d , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>we attribute it with some property</p><formula xml:id="formula_2">y = g(c) ∈ R p . (2)</formula><p>This results a set of local features extracted</p><formula xml:id="formula_3">F (S) = {x|x = f (c), c ∈ C(S)},<label>(3)</label></formula><p>and the corresponding component properties</p><formula xml:id="formula_4">G(S) = { y| y = g(c), c ∈ C(S)}. (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Thus for a given shape database</p><formula xml:id="formula_6">S database = {S 1 , S 2 , . . . , S N d },<label>(5)</label></formula><p>a feature pool of the visual primitives</p><formula xml:id="formula_7">F pool = {x|x ∈ F (S), S ∈ S database }<label>(6)</label></formula><p>can be constructed. And subsequently we can divide each x ∈ F pool into N g groups according to the corresponding property y. Suppose N g division rules</p><formula xml:id="formula_8">I = {I 1 ( y), I 2 ( y), . . . , I N g ( y)} (7)</formula><p>is manually given in advance, where I i ( y) is an indicator function determining whether certain y satisfies the corresponding rule. Then the ith group of the divided feature pool could be formalized as</p><formula xml:id="formula_9">F pool i = {x|x ∈ F pool , I i ( y) = 1}.<label>(8)</label></formula><p>Afterwards, we apply k-means to learn the vocabulary V i ∈ R d×k i of size k i for each feature pool F pool i , constructing an assemble of vocabularies</p><formula xml:id="formula_10">V = {V 1 , V 2 , . . . , V N g }.<label>(9)</label></formula><p>Note that each x is not restricted to lie in only one divided group, depending on the selection of rules I. This resembles the strategy taken in the SPM <ref type="bibr" target="#b34">[35]</ref>, where certain x can fall into different spatial pyramids, and make multiple contributions to the final image representation. And it should be pointed out that x and y could also be merely concatenated to make a new description of the local component and directly apply k-means to all features in the feature pool F pool . But this degenerates to the paradigm taken in the traditional BoW framework, thus it is not concentrated in our context. Fig. <ref type="figure">2</ref> presents an intuitive illustration of the practice using divided vocabularies. And a more detailed discussion on our specific choices of C(S), f (c), g(c) and the division rules I will be given in Section III-C and III-D, and we show that exploiting the associated property of the visual primitives in this way helps further bring the possibility of promoting the performance.</p><p>2) Feature Division in Encoded Feature Pooling: In this subsection, we describe how to use the constructed vocabularies to encode each shape component and summarize them to build the final shape representation. A traditional way to encode some visual primitive x is to perform vector quantization (VQ <ref type="bibr" target="#b52">[53]</ref>), that is to assign it to its nearest neighbor in the pre-constructed vocabulary. And this may lead to large quantization error if the feature space is partitioned improperly. Thus, locality constrained linear coding (LLC) <ref type="bibr" target="#b53">[54]</ref> is investigated in Fig. <ref type="figure">2</ref>. To construct divided vocabularies. Suppose we are to describe the mood of some people using the learned words, if a universal vocabulary is used, then we may just make a thorough description of the mood. On the other hand, if we can classify the words via the associated property in advance, and then construct a vocabulary for each type of words, a more detailed and accurate representation of the mood will be obtained. Likewise, the same thing happens when we are using visual words to describe the appearance of certain shape.</p><p>previous literature, which uses a linear combination of the k nearest neighbors of x to perform soft feature encoding. This helps better retain the discriminative power of the original feature, thus we also take LLC as a powerful tool in the proposed method.</p><p>To illustrate this procedure, still let F (S) be the set of visual primitives extracted from some shape S. Then we follow the same criterion mentioned in Section III-D to divide these features into N g groups, thus the ith group of features can be formulated as</p><formula xml:id="formula_11">F i (S) = {x|x ∈ F (S), I i ( y) = 1}. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Subsequently we are to apply soft vector quantization to each group of features under the framework of LLC. To illustrate, for certain x ∈ F i (S), we find its k nearest neighbors in the ith vocabulary V i ∈ R d×k i , and denote it as</p><formula xml:id="formula_13">V π i ∈ R d×k . Here π = {π 1 , π 2 , . . . , π k }<label>(11)</label></formula><p>contains the indexes of the targeted nearest neighbors in V i , thus the specified k columns of</p><formula xml:id="formula_14">V i constitutes V π i . Suppose w = h(x) ∈ R k i (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>is the final encoded signature of x, and we determine the coefficients w π ∈ R k in the as follows</p><formula xml:id="formula_16">min w π x -V π i w π 2 , (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>subject to</p><formula xml:id="formula_18">1 T w π = 1,<label>(14)</label></formula><p>where w π is the k entries of w indicated by π and the other entries of w are set to zeros. The constraint 1 T w π = 1 suggests that all entries of w π should be summed to one and the computation complexity for solving the problem is O(k i +k 2 ). Each entry of w can be also illustrated as the probability of assigning x to the corresponding visual word. Once all the visual primitives are assigned to the corresponding visual words in a soft manner, we could use the Fig. <ref type="figure">3</ref>. From shape to a set of contour fragments. For certain shape, an approximate polygon of the object contour is obtained via applying DCE. Then a set of contour fragments is generated via the introduced criterion. After pose normalization, an concatenated shape context histogram is generated to describe each fragment. In the third image, the red dots are the chosen vertices and the contour in pink indicates the selected contour fragment. The contour fragments of different lengths are also depicted. In the last images, the green squares are the N s sampled points on the selected contour fragment, and the red square indicates the median point of the sampled points. The purple rays represents the N r shape context histograms computed on the rotated contour fragment.</p><p>encoded signatures for building a shape representation that is robust while compact. Traditionally, the sum pooling method is used in the BoW paradigm, and each bin in the final BoW histogram is the sum of the number of occurrences of corresponding visual word (before histogram normalization is performed). And in <ref type="bibr" target="#b53">[54]</ref>, another way of pooling the encoded features is investigated, where each bin in the final BoW histogram is the maximum probability of occurrence of the corresponding visual word. The above feature pooling strategy is often mentioned as the max pooling method, and it is justified in many previous soft feature encoding literatures.</p><p>The max pooling strategy is also involved in our practice of shape description. Suppose the ith group of encoded signatures of certain S is denoted as</p><formula xml:id="formula_19">W i (S) = {w|w = h(x), x ∈ F i (S)},<label>(15)</label></formula><p>then max pooling is involved to get a summary of W i (S), resulting a histogram</p><formula xml:id="formula_20">z i = (z i1 , z i2 , . . . , z ik i ) T ∈ R k i , (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where</p><formula xml:id="formula_22">z i j = max w∈W i (S) w j . (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>Eventually, the final vector representation of S is a concatenation of all z i , and it can be determined as</p><formula xml:id="formula_24">R(S) = [z T 1 , z T 2 , . . . , z T N g ] T . (<label>18</label></formula><formula xml:id="formula_25">)</formula><p>The dimension of R(S) can be therefore calculated as</p><formula xml:id="formula_26">Dim(R(S)) = N g i=1 k i . (<label>19</label></formula><formula xml:id="formula_27">)</formula><p>To this end, the task of shape representation is accomplished, incorporating information (local or global) preserved in each individual feature group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. From Shape to a Set of Local Features</head><p>There are many ways of decomposing certain shape to a set of components and form a local feature based shape representation. Conventionally, the contour of an object shape turn out to preserve many important properties of the original object. Therefore, we also adopt contour fragments as the local components for shape representation at the first stage. To obtain a set of contour fragments from certain shape S, we first apply Discrete Contour Evolution (DCE) <ref type="bibr" target="#b54">[55]</ref> to acquire a simplified polygon, which acts as an approximation of the object outer contour. Suppose the set of vertices of the generated polygon is denoted as</p><formula xml:id="formula_28">E(S) = {e 1 , e 2 , . . . , e N e },<label>(20)</label></formula><p>where N e is the number of vertices and it is automatically determined by an empirically chosen threshold τ (the maximum curvature) when performing DCE, then a contour fragment c i j could be obtained by tracing the path from vertex e i to e j on the object contour. Enumerating all possible combinations of the vertex pairs, we could obtain a set of contour fragments</p><formula xml:id="formula_29">C(S) = C c f (S) = {c i j = (e i , e j</formula><p>), e i = e j , e i , e j ∈ E(S)}.</p><p>(</p><p>Note that c i j and c j i are regarded as two different contour segments since the direction of the related path is opposite. An intuitive illustration of the procedure can be found in Fig. <ref type="figure">3</ref>.</p><p>In practice, 20-50 vertices are automatically detected during the process and the number may vary according to the geometry of certain shape. Thus, up to a few thousands (typically several hundreds) contour fragments are used, making it computationally applicable. As all significant properties about certain shape are automatically learned without human intervention, more local features than needed are provided here. This redundancy has guaranteed the adequate information for learning the shape descriptor. Similar strategies using dense or exhaustive local features could also be found in <ref type="bibr" target="#b53">[54]</ref> and <ref type="bibr" target="#b55">[56]</ref>, and they presents superior performance in constructing learning based image representations.</p><p>With those informative contour fragments in hand, it is also of great significance to develop a proper way to describe them. And this can be formulated as finding a well designed feature extraction method x = f (c), which embeds the given contour fragment c into a vector space x ∈ R d . The choices of the mapping f can be various and here we utilize a concatenated shape context histograms <ref type="bibr" target="#b2">[3]</ref> as a power tool to accomplish the task. More specifically, we sample N s points evenly spaced on c, and then choose N r representatives (also equidistantly distributed) to compute the shape context histogram around them; eventually we concatenate the N r histograms into one and normalize it to unit length, constructing the final descriptor</p><formula xml:id="formula_31">x = f (c) = f csc (c) ∈ R N r ×d sc , (<label>22</label></formula><formula xml:id="formula_32">)</formula><p>Fig. <ref type="figure">4</ref>. The properties associate with each contour fragment. The above figure illustrates how the three properties are computed for certain contour fragment. Note that each of the designed properties is invariant to shape rotation, thus they provide a fair principle for feature division in different shape rotation configurations.</p><p>where d sc is the dimension of the designed shape context histogram computed at single point.</p><p>To make the descriptor invariant to rotation, an affine transform is applied to the sampled points before computing the shape context histogram: we select the middle point on the sampled contour fragment and compute the angle θ between its tangent line to the horizon, then we rotate the whole fragment c by θ in clockwise (see Fig. <ref type="figure">3</ref> for more details). To this end, we have collected a set of contour fragment features from the original shape</p><formula xml:id="formula_33">F (S) = F c f (S) = {x|x = f csc (c), c ∈ C cf (S)}. (<label>23</label></formula><formula xml:id="formula_34">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Selection of the Feature Division Rules</head><p>In Section III-B1 and III-B2, we propose to represent every local component c with some kind of feature x = f (c) and associate it with some property y = g(c), and then we proceed to divide the features into N g group, according to N g criterions I = {I 1 ( y), I 2 ( y), . . . , I N g ( y)}. Subsequently, we will give details about the choice of g(c) and I in our specific shape matching scenario.</p><p>As described in III-C, each local component c is specified as a single contour fragment, or equivalently an ordered sequence of 2D points. Then the following properties are inspected:</p><p>(1) the length ratio r l ∈ [0, 1] of c to the whole shape contour, (2) the eigenvalues ratio r e ∈ [0, 1] of the two principle components after PCA <ref type="bibr" target="#b56">[57]</ref> is applied and (3) the angle a eme ∈ [0, π] between the two rays determined by the middle point and the two end points of c. Therefore, the associated property can be further specified as</p><formula xml:id="formula_35">y = g(c) = (r l , r e , a eme ) ∈ R 3 . (<label>24</label></formula><formula xml:id="formula_36">)</formula><p>A more intuitive illustration of the property extraction procedure is depicted in Fig. <ref type="figure">4</ref>. It can be observed that the designed properties are invariant to shape rotation, thus they provide a fair principle for feature division in different shape rotation configurations.</p><p>To design the division rules, the most straightforward way is to evenly divide the range of each property, and define features situated in the same range as one group. For example, the range of r l , r e and a eme are evenly divided into 8, 4, and 4 grids respectively in our practice. Thus, the local information will be preserved in the descriptors generated by the above property division rules. Besides, for some rule I ( y) ≡ 1 (i.e., all properties satisfy the rule), the vocabulary constructed under this rule is indeed to perform vocabulary learning and encoded feature pooling on all features, which forms a global representation of certain shape equivalent to traditional BoW framework. Eventually, the locally constrained division rules together with the one global rule constitute the final set of division criterions I. Therefore, it is possible to divide the feature space in the proposed framework according to the constructed division rules, resulting a informative descriptor combining local and global shape codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXTENSION TO 3D SHAPES</head><p>More recently, the field of 3D shape search and retrieval has attracted more and more attention in several research communities, including computer vision, computer graphics, augmented reality, etc. Indeed, a quick and accurate method for comparing 3D objects is not only of great significance in daily life practice, but also interesting in theoretical research. In this section, we show that an extension of the proposed shape descriptor can also be applied in the 3D scenario and achieve promising performance.</p><p>Unlike objects in 2D images, a 3D object is often described by another form (e.g., polygon meshes) in the vision of computer. Thus the method introduced in Section III can not be directly applied, and a preprocessing stage is required to make that possible. Similar to the 2D view based method introduced in Section II-B, for a 3D shape S we first place it at different view configurations, resulting a collection of 2D projections of the 3D Shape</p><formula xml:id="formula_37">P(S) = {S 1 , S 2 , . . . , S N v }, (<label>25</label></formula><formula xml:id="formula_38">)</formula><p>where N v is the number of projections. Then we redefine the contour fragment features in 3D shape S as</p><formula xml:id="formula_39">F (S) = F (S 1 ) ∪ F (S 2 ) ∪ . . . ∪ F (S N v ), (<label>26</label></formula><formula xml:id="formula_40">)</formula><p>where F (S j ) refers to the local features extracted from some 2D projection S j . Namely, we merge the local contour features extracted from different views of the shape (an illustration of the pre-process stage can be found in Fig. <ref type="figure" target="#fig_0">5</ref>), and subsequently follow the same paradigm to construct a compact representation of the 3D shape, except that we are using F(S) to represent the local features extracted from the corresponding shape S. Not that if sophisticated rotation alignment <ref type="bibr" target="#b36">[37]</ref> procedure or 3D pose normalization methods <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref> are involved in our way of extension, more promising shape matching performance is expected to be observed. And in terms of the consideration for efficiency and simplicity, we follow the initially introduced method to validate the potential of proposed descriptor against rotation changes when applied to the 3D scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>To validate the descriptive and discriminative power of the proposed shape descriptor, we evaluate it on both 2D and 3D shape databases. Since the proposed descriptor is initially designed to achieve efficient matching, the selected datasets for evaluation tend to contain a relative larger number of shapes (MPEG-7 <ref type="bibr" target="#b57">[58]</ref>, Animal <ref type="bibr" target="#b25">[26]</ref> and Princeton 3D <ref type="bibr" target="#b35">[36]</ref>). A standard 3D dataset for competition (SHREC Watertight <ref type="bibr" target="#b58">[59]</ref>) is also involved for the verification of our method. We show that our integrated representation of shape is able to achieve impressive results as well as high time efficiency in this largescale shape matching scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details and Machine Configuration 1) Shape Vocabulary Learning:</head><p>As described in Section III-D, a total number of N g = 17 feature groups are obtained in total. The number of the clustering centers in each locally divided group is set as 2000. As for the global group of features, 10000 partitions are made in the original feature space. The standard k-means algorithm is applied for learning the vocabulary.</p><p>2) Encoded Feature Pooling: The identical feature division criterion is followed at this stage. The k = 5 nearest neighbors is involved in the approximate LLC procedure for soft feature encoding.</p><p>3) Local Feature Extraction: When utilizing DCE for computing the approximate polygon for certain shape, the maximum curvature τ is set as 0.5. After contour fragments are decomposed from the shape, N s = 100 points are sampled 4) Extension to 3D Shape: To capture the projected views of the given 3D shape, we evenly divide the longitude and the latitude of a sphere, to generate a grid of view points. The axis (the direction from the south pole to the north pole) of the view sphere is set as the default z axis of the 3D model, and the sphere shares the same geometric center with the inspected 3D shape. An illustration of the sampled points on a view sphere is depicted in Fig. <ref type="figure" target="#fig_1">6</ref>. To trade off between efficiency and discriminative power, we uniformly partition the longitude and the latitude into 8 and 4 regions respectively (as shown in the middle of Fig. <ref type="figure" target="#fig_1">6</ref>), and generate the projections observed at the sampled positions.</p><p>The parameters for shape matching are kept the same in all the experiments conducted, so as to validate the robustness of the proposed method. The algorithm is implemented in Matlab with some utilities written in MEX files, and experiments are carried out on a desktop machine with an Intel(R) Core(TM) i7-2600 K CPU (3.40 GHz) and 8 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on 2D Shapes 1) Shape Retrieval on the MPEG-7 Dataset:</head><p>The MPEG-7 2D benchmark dataset is also known as MPEG-7 Core Experiment CE-Shape-1 <ref type="bibr" target="#b57">[58]</ref>. It offers a possibility to compare various shape descriptors of non-rigid shapes with a single closed contour. The dataset consists of 70 different classes of shapes, with each class containing 20 binary masks of the object (see Fig. <ref type="figure">7</ref> for more details). Note that some Fig. <ref type="figure">7</ref>. Exemplar images from the MPEG-7 dataset (two images per category). It can be observed that some classes are fairly similar in the dataset and there is often complicated appearance changes within the same class.</p><p>classes are fairly similar in the dataset and there is often complicated appearance changes within the same class, thus it is often hard to perfectly accomplish the task of shape retrieval on the dataset. The retrieval performance on the dataset is evaluated by a method called the bulls-eye test. That is, for each query image, we calculate the percentage of images that belong to the same class in the top 40 query results and the final retrieval rate is obtained by average the statistics among all queries. In our experiment, we calculate the euclidian distance between proposed shape descriptors to indicate the dissimilarity between two given shapes. We rank the retrieval results in ascending order of the computed dissimilarities and then calculate the retrieval rate using the mentioned evaluation metric.</p><p>The bulls-eye scores achieved by our algorithm and previous methods are listed in Table <ref type="table" target="#tab_1">I</ref>. It demonstrates that our shape descriptor preserves encouraging discriminative power in shape matching. Note that here we only focus on listing the results of methods using pairwise similarity measure, and it should be pointed out that evaluating the similarity using context information (see <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b66">[67]</ref>) can further enhance the performance. Although the retrieval rate of our method is not as good as that of <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, our descriptor outperforms in terms of time efficiency. This is justified by the fact that only metric for evaluating vector similarity (e.g., the l 2 distance) is involved in our method, in comparison to the more sophisticated matching techniques such as the Hungarian method <ref type="bibr" target="#b2">[3]</ref>, dynamic programming <ref type="bibr" target="#b33">[34]</ref> and total Bregman divergences <ref type="bibr" target="#b64">[65]</ref> applied in related methods.</p><p>More specifically, for those methods using sophisticated matching techniques, it often takes considerable time to match a single pair of shape. This is usually not reasonable in real-time image retrieval system. In contrast, the matching stage of our method only takes milliseconds to accomplish a query on the whole MPEG-7 dataset (1400 comparisons in total). And it can be expected that the speed can be further boosted if more advanced indexing technique is incorporated. In practice, we apply the inverted index <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> technique for fast computing the pairwise similarities between all shapes, and improvement in efficiency is observed while maintaining the retrieval rate (the degree of speed-up varies with the sparsity of the shape descriptor). Table I also shows the average pairwise matching time of different methods, where the statistics of the running time are directly taken from corresponding references. For those methods using database indexing (see <ref type="bibr" target="#b11">[12]</ref>) to boost the speed of shape matching, we divide the total retrieval time by the maximum possible number of comparisons (i.e., 1400 × 1400) to compute the average. Although the computational time of the reported algorithms may vary slightly due to different machine configurations, the overall time required by our method is still much smaller compared to those outstanding shape descriptors.</p><p>2) Shape Classification on the Animal Dataset: The animal dataset was introduced in <ref type="bibr" target="#b25">[26]</ref>, with 2000 manually extracted shape carefully partitioned into 20 categories. Compare to the Fig. <ref type="figure">8</ref>. Exemplar images from the Animal dataset (four images per category). Compared to the MPEG-7 dataset, more articulated and non-rigid deformation can be observed in the dataset, making it fairly challenging for shape matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II</head><p>CLASSIFICATION ACCURACY ON THE ANIMAL DATASET MPEG-7 dataset, it contains more images in each category (100 v.s. 20), thus containing larger variation of a single class (see Fig. <ref type="figure">8</ref> for more details). Moreover, as it is dealing with living creatures, more articulated and non-rigid deformation can be observed in the dataset, which makes it fairly challenging. To evaluate the performance on the dataset, a halftraining and half-testing strategy was applied along the history. More specifically, we randomly choose 50 shapes per class for training, and use the rest for nearest neighbor classification. The procedure is repeated for 100 times to eliminate the uncertainty of performance brought by the random division of data, thus we can see whether the proposed shape descriptor is uniformly outperforming. The average accuracy and the corresponding standard deviation of our method are both computed for more intuitive understanding of the performance.</p><p>The experimental results of shape classification on the Animal dataset are shown in Table <ref type="table" target="#tab_1">II</ref>. Also, we observe competitive performance of the proposed descriptor against previous methods in this challenging dataset. Still, the high time efficiency is the characteristic that highlights our method among those state-of-the-art algorithms. To further examine the potential of the proposed descriptor, we make it the input of the classical SVM (facilitated by the implementation in <ref type="bibr" target="#b31">[32]</ref>) algorithm. This results a collection of statistical models for discriminating the characteristic of the given shapes, and better classification performance is demonstrated in our experiment. Note that more advanced supervised learning methods can be incorporated with our descriptor, and performance boost are therefore to be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on 3D Shapes</head><p>1) Evaluation Methods on 3D Shape Retrieval: In this section, we will introduce some statistics for performance evaluation on 3D shape retrieval. And if in any doubt, please refer to <ref type="bibr" target="#b35">[36]</ref> for a more detailed definition of each evaluation method.</p><p>a) Nearest neighbor: The percentage of the closest matches that belong to the same class as the query. This statistic provides an indication of how well a nearest neighbor classifier would perform.</p><p>b) First-tier and second-tier: The percentage of models in the query's class that appear within the top M matches, where M depends on the size of the querys class. Specifically, for a class with C members, M = C -1 for the first tier and M = 2(C -1) for the second tier.</p><p>c) Discounted cumulative gain: A statistic that weights correct results near the front of the list more than correct results later in the ranked list under the assumption that a user is less likely to consider elements near the end of the list.</p><p>The four mentioned evaluation methods emphasize on different aspects of the performance of certain shape matching algorithm. The nearest neighbor (NN) evaluation metric merely focus on the discriminative power of the shape representation as it only consider the most similar one in the retrieved ranked list. In contrast, the First-tier (FT) and Second-tier (ST) metric try to examine the generative power of the shape representation as well as the discriminative one. They evaluate the extent to which certain descriptor is capable to trade off between intra-class variation and inter-class discrepancy. And further more, the Discounted Cumulative Gain (DCG) consider the retrieval results as a whole, offering a more objective view of the inspected shape descriptor.</p><p>2) Shape Retrieval on the Princeton 3D Dataset: The Princeton Shape Benchmark dataset <ref type="bibr" target="#b35">[36]</ref> provides a repository of 3D models for comparing shape matching models. Its creation is to promote the use of standardized data sets and evaluation methods for research in matching, classification, clustering, and recognition of 3D models. Each model of the 3D shape is represented by the polygonal surface geometry of the corresponding object. There are 1814 models in the database and these models are portioned evenly into training and testing sets. More specifically, 907 models of 90 classes are used for training the parameters of shape description or matching algorithms, and the other different 907 models of 92 classes are involved to conduct performance evaluation (see Fig. <ref type="figure" target="#fig_2">9</ref> for more intuitive view of the dataset).</p><p>We perform shape retrieval in the 3D scenario using the method introduced in Section IV. The Princeton 3D dataset provides classifications of different granularity for the analysis  of retrieval results. That is, the coarsest classification of the testing set contains two classes only: man-made objects and other naturally occurring objects; meanwhile, the finest classification partitions similar objects into different classes (e.g., monoplane v.s. biplane). We use the classification setting of finest granularity (also known as the base classification) to evaluate the performance of shape retrieval.</p><p>Using the four evaluation methods, we compare our method to previous 3D matching methods (see Table <ref type="table" target="#tab_2">III</ref> for details). Note that although we use a simple prepossessing strategy to extend our descriptor to the 3D scenario, competitive performance is obtained when compared with recent yet more complicated 3D shape models. Although the NN score is not as good as that of the other 3D matching algorithms, when emphasizing on different prospectives, the proposed method tends to outperform instead. This suggests that our method demonstrates higher capability of tolerating intra-class variation as well as outstanding discriminative power. Besides, the efficiency in shape matching can still be observed, although we are currently using a MATLAB based implementation for 3D shape retrieval, which is not well tuned for 3D object rendering.</p><p>3) Shape Retrieval on the SHREC Database: The general objective of the 3D SHape REtrieval Contest (SHREC) is to evaluate the effectiveness of 3D Shape retrieval algorithms. Each year, the contest would involve multiple tracks of 3D models for evaluation. In this section, the Watertight Track in SHape REtrieval Contest 2007 (WT-SHREC <ref type="bibr" target="#b58">[59]</ref>) is used to further evaluate the performance of our method.</p><p>The WT-SHREC 1 database contains 400 models of 20 classes. Each model will be used in turn as a query against 1 http://watertight.ge.imati.cnr.it/ the remaining part of the database when quantifying the shape retrieval performance. We summarize the performance of our descriptor against some most recent methods in Table <ref type="table" target="#tab_3">IV</ref>. It can be observed that the proposed 3D shape descriptor demonstrates outstanding properties of shape retrieval in the WT-SHREC database. As described in Section V-A, fixed parameters are used during all our experiments. Thus, further improvements can be expected if the mentioned parameters are determined in a heuristic manner for each specific dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Discussions on Our Method 1) Universal Vocabulary v.s. Divided Vocabularies:</head><p>To demonstrate the potential of the proposed learning based descriptor, experiments are conducted to compare it with traditional BoW framework using one universal vocabulary. Fig. <ref type="figure" target="#fig_3">10</ref> shows the retrieval rate on the MPEG-7 dataset, where two different vocabulary construction strategies are applied. For fair comparison, all other parameters are set the same as described in Section V-A, except that we are using the global vocabulary or local vocabularies individually with varying vocabulary size. It is observed that the divided vocabularies has brought  more accurate descriptions of shape instances, and the performance boost varies with different vocabulary sizes. It is also interesting to find that, for some large vocabulary size, the performance of the divided vocabularies somehow degenerates. This could be explained by the fact that the original feature space may be over-partitioned given this setting, resulting in some inappropriate quantization of the local feature. Still, it is clear that the upper bound of performance curve of the divided vocabularies will be much better than that of the universal one. The speed for vocabulary construction is also improved in the proposed method, compared to that in the traditional BoW framework. An acceleration of 3 to 4 times is observed in our experiment when using the same vocabulary size.</p><p>2) The Impact of Vocabulary Size: The vocabulary size may play the most important role in a visual word based method <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> and here we are to have a further discussion on the impact of it. Tables V and VI show the influences of the local and global vocabulary size respectively. It can be observed in Table V that incorporating local vocabularies can bring promising performance boost, and as the vocabulary size gets larger, the retrieval rate may get saturated at certain value. Fig. <ref type="figure">11</ref>. The impact of boundary noise on the shape retrieval rate of MPEG-7 dataset. It is observed that the proposed shape descriptor is also robust against boundary noise under certain degree.</p><p>And as described in Table <ref type="table" target="#tab_4">VI</ref>, the global vocabulary can be also complementary with the local ones. Although it may bring lower retrieval rate when vocabulary size is small, performance boost can be still achieved when a proper vocabulary size is selected.</p><p>3) The Potential Against Boundary Noise: It is observed in our experiment that the proposed shape descriptor is also robust against boundary noise under certain degree. To validate this potential, we add white noise with zero mean and some standard deviation σ on the boundary points and then proceed to perform shape retrieval on the MPEG-7 dataset. The scale of certain shape is also taken into consideration when applying the white noise, and this is to guarantee the consist level of modifications to different shapes. Besides, only the divided vocabularies with size 2000 are used in the experiment and the related experiment results are displayed in Fig. <ref type="figure">11</ref>. It shows that as the noise level increases, our shape descriptor still presents reasonable performance. The overall trend of the retrieval score is decreasing as σ gets larger, and the fluctuation of the score may be the consequence of the randomness brought by the k-means learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a learning based shape descriptor for shape matching is investigated. The proposed method summarizes the local features extracted from certain shape to generate a integrated representation under a BoW-like framework. It contributes to the speed-up of shape matching, as it eliminates the time consuming stage of local feature matching. A strategy for feature division is applied in the phase of encoded feature pooling as well as vocabulary learning, which helps construct a more discriminative descriptor incorporating both global and local information. Also, a local contour based feature extraction method is designed for 2D shapes, while significant properties of the local contours are inspected for the design of feature division rules. The designed local feature extraction method and the feature division rules manage to reduce the variances of shape representation due to the changes in rotation. In addition to 2D shape, we also present a simple and natural method to extend the proposed method to the scenario of 3D shape representation. The proposed shape descriptor is validated on several benchmark datasets for evaluating 2D and 3D shape matching algorithms, and it is observed that the investigated shape descriptor maintains superior discriminative power as well as high time efficiency.</p><p>Moreover, as suggested in this work, there are also many aspects for us to explore in the future. Firstly, more advanced vocabulary learning methods (such as Vocabulary Tree <ref type="bibr" target="#b14">[15]</ref>, Approximate k-means <ref type="bibr" target="#b15">[16]</ref>, Hamming Embedding <ref type="bibr" target="#b16">[17]</ref>) can be incorporated in our paradigm, further enhancing the accuracy and speed of feature encoding. Secondly, various local shape representation methods may be inspected, apart from the exploited contour fragments representation. Besides, more ingenious application dependent feature property extraction and division methods remain to be explored, making the final representation more compact and informative. Also, the strategy for extending our method to the 3D scenario can be under further discussion. For example, pose moralization may be applied before the 3D model is projected; only salient views are to be selected for further process; we may also consider other low level features for 3D object description. Eventually, more talented methods for indexing the shape database can be employed, so as to improve the time efficiency in the scenario of large-scale shape matching application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Extend the shape descriptor to the 3D scenario. For each 3D shape, we project it at different view points on the surface of a visual sphere and merge local contour features extracted from the projected views. In the third and fourth image, colored dots indicates features extracted from four corresponding representative views, and the black dots indicates those extracted from other views (not displayed here).</figDesc><graphic coords="8,49.43,61.61,93.14,93.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. To sample view points on a view sphere. The sampled positions (red dots) of different densities are displayed. We choose the sampled positions as the setting depicted in the middle, so as to trade off between efficiency and discriminative power.</figDesc><graphic coords="8,487.67,219.89,66.98,67.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Exemplar images from the Princeton 3D dataset (one image per category). The finest classification granularity of the testing set is shown in the above figure, which means that different categories are likely to share similar characteristic in their appearance or functionality (e.g., a standing human v.s. a walking human).</figDesc><graphic coords="11,74.51,58.61,462.77,80.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The impact of vocabulary construction strategy on the shape retrieval rate of MPEG-7 dataset. The retrieval scores under different strategies with varying vocabulary size are depicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,74.51,58.13,462.74,162.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Shape Vocabulary: A Robust and Efficient Shape Representation for Shape Matching Xiang Bai, Member, IEEE, Cong Rao, and Xinggang Wang, Student Member, IEEE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I BULLS</head><label>I</label><figDesc>-EYE SCORE AND AVERAGE PAIRWISE MATCHING TIME ON THE MPEG-7 DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>ON THE PRINCETON 3D BENCHMARK</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SHAPE</head><label>IV</label><figDesc>RETRIEVAL ON THE WT-SHREC DATABASE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V THE</head><label>V</label><figDesc>IMPACT OF LOCAL VOCABULARY SIZE ON THE SHAPE RETRIEVAL RATE OF MPEG-7 DATASET TABLE VI THE IMPACT OF GLOBAL VOCABULARY SIZE ON THE SHAPE RETRIEVAL RATE OF MPEG-7 DATASET</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their valuable suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61222308 and Grant 60903096, in part by the Program for New Century Excellent Talents in University in China under Grant NCET-12-0217, and in part by the Fundamental Research Funds for the Central Universities, Huazhong University of Science and Technology, Wuhan, China, under Grant 2013TS115. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Dimitrios Tzovaras. Xiang Bai received the B.S., M.S., and Ph.D. degrees from the Huazhong University of Science and Technology (HUST), Wuhan, China, in 2003, 2005, and 2009, respectively, all in electronics and information engineering. He is currently a Professor with the Department of Electronics and Information Engineering at HUST. He is also the Vice Director of the National Center of Anti-Counterfeiting Technology at HUST. His research interests include object recognition, shape analysis, scene text recognition, and intelligent systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust image corner detection through curvature scale space</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Suomela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1376" to="1381" />
			<date type="published" when="1998-12">Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape similarity measure based on correspondence of visual parts</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakämper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1185" to="1190" />
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification of contour shapes using class segment sets</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Super</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="727" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape classification using the innerdistance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="299" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical procrustes matching for shape retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="885" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning contextsensitive shape similarity by graph transduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Affinity learning with diffusion on tensor product graph</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusion with diffusion for robust visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Inf</title>
		<meeting>Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2987" to="2995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-transduction for shape retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2747" to="2757" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1320" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient and robust algorithm for shape indexing and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="372" to="385" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometry-based 2D shape descriptor for retrieval in large database</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 11th Int. Conf. Signal Process. (ICSP)</title>
		<meeting>IEEE 11th Int. Conf. Signal ess. (ICSP)</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1096" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE Int. Conf. Comput. Vis</title>
		<meeting>9th IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>10th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A boundary-fragment-model for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>9th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometry-based image retrieval in binary image databases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1003" to="1013" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical matching of deformable shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Groups of adjacent contour segments for object detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fevrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shape matching and classification using height functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="143" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disconnected skeleton: Shape at its absolute scale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2188" to="2203" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path similarity skeleton graph matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1282" to="1292" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating contour and skeleton for shape classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis. Workshops (ICCV)</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis. Workshops (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognition of shapes by editing their shock graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="571" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shock graphs and shape matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="32" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Invariant image recognition by Zernike moments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khotanzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="489" to="497" />
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shape-based image retrieval using generic Fourier descriptor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process., Image Commun</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="825" to="848" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape representation and classification using the Poisson equation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1991">1991-2005, Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bag of contour fragments for robust shape classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2116" to="2125" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector shape: A classifier-based shape representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="970" to="982" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The princeton shape benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape Modeling Applications</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On visual similarity based 3D model retrieval</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DESIRE: A composite 3D-shape descriptor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vranic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2005-07">Jul. 2005</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D object retrieval using an efficient and compact hybrid shape descriptor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Eurograph. Workshop 3D Object Retrieval</title>
		<meeting>1st Eurograph. Workshop 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D model retrieval using probability density-based shape descriptors</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Akgul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1117" to="1133" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient 3D shape matching and retrieval using a concrete radialized spherical projection representation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2437" to="2452" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new descriptor for 2D depth image indexing and 3D model retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2007-10">Sep./Oct. 2007</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PANORAMA: A 3D shape descriptor based on panoramic views for unsupervised 3D object retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual similarity based 3D shape retrieval using bag-of-features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Shape Model. Int. Conf. (SMI)</title>
		<meeting>Shape Model. Int. Conf. (SMI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient color histogram indexing for quadratic form distance functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="729" to="736" />
			<date type="published" when="1995-07">Jul. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sampling strategies for bagof-features image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>9th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="490" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluating bag-of-visual-words representations in scene classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Multimedia Inf. Retrieval</title>
		<meeting>Int. Workshop Multimedia Inf. Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Statist. Learn. Comput. Vis. ECCV</title>
		<meeting>Workshop Statist. Learn. Comput. Vis. ECCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>10th IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image coding using vector quantization: A review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="957" to="971" />
			<date type="published" when="1988-08">Aug. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convexity rule for shape decomposition based on discrete contour evolution</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakämper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shape descriptors for nonrigid shapes with a single closed contour</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eckhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2000-06">Jun. 2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="424" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Shape retrieval contest 2007: Watertight models track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biasotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paraboschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SHREC Competition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Efficient planar graph cuts with applications in computer vision</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Toppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="351" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust symbolic representation for shape recognition and retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1782" to="1798" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Total Bregman divergence and its applications to shape retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3463" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2D shape matching by contour flexibility</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="186" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Locally affine invariant descriptors for shape matching and retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="803" to="806" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Shape retrieval using hierarchical total Bregman soft clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2047" to="2419" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Articulation-invariant representation of non-planar shapes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>11th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="286" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improving shape retrieval by learning graph transduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>10th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="788" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A hierarchical shape tree for shape classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Image Vis</title>
		<meeting>25th Int. Conf. Image Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Shape classification using local and global features</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Pacific-Rim Symp. Image Video Technol</title>
		<meeting>4th Pacific-Rim Symp. Image Video Technol</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A new 3D-matching method of nonrigid and partially similar models using curve analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Colot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="852" to="858" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Combination of bag-of-words descriptors for robust partial shape retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lavoué</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="931" to="942" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Covariance descriptors for 3D shape matching and retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Philippe-Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
