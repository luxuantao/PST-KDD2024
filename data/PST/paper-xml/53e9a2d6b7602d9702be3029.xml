<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delay-Slope-Dependent Stability Results of Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Wei</forename><surname>Xing Zheng</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Chong</forename><surname>Lin</surname></persName>
							<email>linchong2004@hotmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Communication</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution">University of Western Sydney</orgName>
								<address>
									<postCode>2751</postCode>
									<settlement>Penrith</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computing and Mathematics</orgName>
								<orgName type="institution">University of Western Sydney</orgName>
								<address>
									<postCode>2751</postCode>
									<settlement>Penrith</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Institute of Complexity Science</orgName>
								<orgName type="department" key="dep2">College of Automation Engineering</orgName>
								<orgName type="institution">Qingdao University</orgName>
								<address>
									<postCode>266071</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Delay-Slope-Dependent Stability Results of Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">56B20B0BDD17F29D02E945E1854FE325</idno>
					<idno type="DOI">10.1109/TNN.2011.2169425</idno>
					<note type="submission">received January 7, 2011; accepted September 12, 2011. Date of publication October 6, 2011; date of current version December 1, 2011.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Asymptotic stability</term>
					<term>delay-slope-dependent</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>By using the fact that the neuron activation functions are sector bounded and nondecreasing, this brief presents a new method, named the delay-slope-dependent method, for stability analysis of a class of recurrent neural networks with time-varying delays. This method includes more information on the slope of neuron activation functions and fewer matrix variables in the constructed Lyapunov-Krasovskii functional. Then some improved delay-dependent stability criteria with less computational burden and conservatism are obtained. Numerical examples are given to illustrate the effectiveness and the benefits of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recurrent neural networks (RNNs) have been an active research area for the last few decades and have been successfully applied to various fields such as image processing, pattern recognition, partial differential equations solving, etc., <ref type="bibr" target="#b6">[1]</ref>, <ref type="bibr" target="#b7">[2]</ref>. Some of these applications require that the equilibrium points of the designed network be stable. So, it is important to study the stability of RNNs. However, in the implementation of artificial RNNs, time delays are frequently encountered as a result of the finite switching speed of amplifiers and the inherent communication time of neurons. It has been shown that the existence of time delays might affect the dynamical properties of the equilibrium points such as oscillation, divergence, and even instability. Therefore, the equilibrium and stability properties of RNNs with time delays have received considerable attention (see <ref type="bibr" target="#b8">[3]</ref>- <ref type="bibr" target="#b28">[23]</ref>).</p><p>So far, the stability criteria of RNNs with time delay are classified into two categories, i.e., delay-independent criteria <ref type="bibr" target="#b8">[3]</ref>- <ref type="bibr" target="#b14">[9]</ref>, and delay-dependent criteria <ref type="bibr" target="#b15">[10]</ref>- <ref type="bibr" target="#b28">[23]</ref>. For the delay-dependent case, some criteria have been obtained by using Lyapunov-Krasovskii functional (LKF) <ref type="bibr" target="#b19">[14]</ref>- <ref type="bibr" target="#b28">[23]</ref>. It is well known that the choice of an appropriate LKF is crucial for deriving less conservative stability criteria. Thus, some new techniques have been developed for reducing conservatism, such as free-weighting matrix LKF <ref type="bibr" target="#b20">[15]</ref>, <ref type="bibr" target="#b21">[16]</ref>, discretized LKF <ref type="bibr" target="#b23">[18]</ref>, augmented LKF <ref type="bibr" target="#b26">[21]</ref>, weighting delay LKF <ref type="bibr" target="#b28">[23]</ref>, and so on. However, these methods suffer some common shortcomings: 1) many matrix variables, some of which are even useless for reducing conservatism (see <ref type="bibr" target="#b25">[20]</ref> for details), are introduced in the obtained results, which causes a large computational burden, and 2) the information of neuron activation functions is not adequately considered, which may lead to some conservatism.</p><p>In practical applications, it has been found that the suitable and more generalized neuron activation functions can improve the performance of RNNs. For example, in <ref type="bibr" target="#b29">[24]</ref> it was shown that the absolute capacity of an associative memory model can be remarkably improved by replacing the usual sigmoid activation function with a nonmonotonic activation function. In <ref type="bibr">[25]</ref> and <ref type="bibr">[26]</ref>, the finite slope of neuron nonlinearities was exploited for obtaining less conservative conditions for global stability of neural networks in the non-delayed case. In <ref type="bibr">[27]</ref>, it was pointed out that the number of degrees freedom when solving the nonlinear optimization task can be reduced by studying the relation between the learning rate and the slope of neuron activation functions. Furthermore, it was also noted in <ref type="bibr" target="#b27">[22]</ref> that the property of the neuron activation functions can affect the allowable time delay upper bound for RNNs.</p><p>Motivated by the preceding discussion, this brief mainly considers the relationship between the time delay upper bound and the slope of neuron activation functions. On this basis, a new method, called the delay-slope-dependent method, is proposed to deal with the stability of RNNs with time-varying delay, so that a larger allowable upper bound for the time delay can be obtained. Different from previous studies, this method has the following features: 1) compared with the methods in <ref type="bibr" target="#b20">[15]</ref>, <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b25">[20]</ref>, and <ref type="bibr" target="#b28">[23]</ref>, more information of the slope of neuron activation functions is utilized in the proposed LKF and then less conservative stability criteria are obtained, and 2) while maintaining the efficiency of the stability conditions, the proposed method introduces far fewer matrix variables than the existing methods. These two distinctive features are the novelty of the work presented in this brief.</p><p>Notation: Throughout this brief, a real symmetric matrix P &gt; 0 (≥0) denotes P being a positive definite (positive semidefinite) matrix, and</p><formula xml:id="formula_0">A &gt; (≥)B means A -B &gt; (≥)0.</formula><p>I is used to denote an identity matrix with proper dimension. Matrices, if not explicitly stated, are assumed to have compatible dimensions. The symmetric terms in a symmetric matrix are denoted by an asterisk * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION</head><p>Consider the following RNNs with time-varying delays:</p><formula xml:id="formula_1">ẋi (t) = -c i x i (t) + n j =1 a i j g j (x j (t)) + n j =1 b i j g j (x j (t -d(t))) + J i , i = 1, 2, . . . , n (1)</formula><p>where n denotes the number of neurons in a neural network, x i (t) corresponds to the state of the i th neuron at time t, g j (x j (t)) denotes the activation function of the j th neuron at time t, a i j denotes the constant connection weight of the j th neuron on the i th neuron at time t, b i j denotes the constant connection weight of the j th neuron on the i th neuron at time td(t), J i is the external input on the i th neuron, c i &gt; 0 represents the rate with which the i th neuron will reset its potential to the resting state in isolation when disconnected from the network and external input, and d(t) corresponds to the time-varying transmission delay at time t. Note that the RNN model was first introduced by Hopfield in his seminal work <ref type="bibr">[28]</ref>, and (1) is a variant of Hopfield's RNN model with time-varying delay, which has also been studied in <ref type="bibr" target="#b8">[3]</ref>- <ref type="bibr" target="#b28">[23]</ref>.</p><p>We can rewrite (1) in the following matrix-vector form:</p><formula xml:id="formula_2">ẋ(t) = -Cx + Ag(x(t)) + Bg(x(t -d(t))) + J (2)</formula><p>where </p><formula xml:id="formula_3">x(t) = [x 1 (t), x 2 (t), . . . , x n (t)] τ ∈ R n , g(x(t)) = [g 1 (x 1 (t)), g 2 (x 2 (t)), . . . , g n (x n (t))] τ ∈ R n , g(x(t -d(t))) = [g 1 (x 1 (t -d(t))), g 2 (x 2 (t -d(t))), . . . , g n (x n (t -d(t)))] τ ∈ R n , J = [J 1 , J 2 , . . . , J n ] τ ∈ R n , C = diag{c 1 ,</formula><formula xml:id="formula_4">&lt; d(t) ≤ h, 0 ≤ ḋ(t) ≤ μ</formula><p>where h and μ are known constants. Assumption 2: The neuron activation functions g i (•), i = 1, 2, . . . , n, are continuous and bounded, which satisfy the following conditions:</p><formula xml:id="formula_5">k - i ≤ g i (x) -g i (y) x -y ≤ k + i ∀ x, y ∈ R, x = y, i = 1, 2, . . . , n<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">k - i , k + i , i = 1, 2, .</formula><p>. . , n are some constants. Remark 1: As pointed out in <ref type="bibr" target="#b27">[22]</ref> and <ref type="bibr" target="#b28">[23]</ref>, the constants k - i and k + i in (3) are allowed to be positive, negative, or zero. Especially, when k - i = 0 and k + i &gt; 0, Assumption 2 describes the class of globally Lipschitz continuous and monotone nondecreasing activation functions, when k + i &gt; k - i &gt; 0, Assumption 2 describes the class of globally Lipschitz continuous and monotone increasing activation functions in <ref type="bibr">[29]</ref>. Obviously, such neuron activation functions as sigmoid type and piecewise linear type are also special cases of the functions satisfying Assumption 2.</p><p>Assume that</p><formula xml:id="formula_7">x * = [x * 1 , x * 2 , . . . , x * n ]</formula><p>τ is an equilibrium point of neural network <ref type="bibr" target="#b7">(2)</ref>. One can derive from (2) that the transformation z(•) = x(•)x * transforms neural network (2) into the following neural network:</p><formula xml:id="formula_8">ż(t) = -Cz(t) + A f (z(t)) + B f (z(t -d(t))) (4)</formula><p>where</p><formula xml:id="formula_9">z(t) = [z 1 (t), z 2 (t), . . . , z n (t)] τ is the state vector of the transformed neural network, f (z(t)) = [ f 1 (z 1 (t)), f 2 (z 2 (t)), . . . , f n (z n (t))] τ , f (x(t -d(t))) = [ f 1 (x 1 (t -d(t))), f 2 (x 2 (t -d(t))), . . . , f n (x n (t -d(t)))] τ ∈ R n and f i (z i (t)) = g i (z i (t) + x * i ) -g i (x * i ), i = 1, 2, . . . , n. Note that the functions f i (•), i = 1, 2, .</formula><p>. . , n satisfy the following conditions:</p><formula xml:id="formula_10">k - i ≤ f i (x) x ≤ k + i , f i (0) = 0 ∀ x = 0, i = 1, 2, . . . , n.</formula><p>(5) Thus, under this assumption, the following inequalities hold for any diagonal positive-definite matrices D 1 &gt; 0 and</p><formula xml:id="formula_11">D 3 &gt; 0 K 1 z(t) -f (z(t))) τ D 1 ( f (z(t)) -K 2 z(t) ≥ 0 z τ (t)K D 3 K z(t) -f (z(t)) τ D 3 ( f (z(t) ≥ 0</formula><p>where</p><formula xml:id="formula_12">K 1 = diag k + 1 , k + 2 , . . . , k + n ; K 2 = diag k - 1 , k - 2 , . . . , k - n ; K = diag max |k - 1 |, |k + 1 | , . . . , max |k - n |, |k + n | .</formula><p>The objective of this brief is to establish less conservative conditions guaranteeing that the delayed neural network described by ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) is globally asymptotically stable via a delay-slope-dependent approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MAIN RESULTS</head><p>In this section, a new LKF is constructed to derive delayslope-dependent stability criteria for RNNs.</p><p>Theorem 1: Given scalars h ≥ 0 and μ, for any delay d(t) satisfying Assumption 1, RNNs (4) is asymptotically stable if there exist matrices P &gt; 0,</p><formula xml:id="formula_13">Q 3 &gt; 0, R &gt; 0, Q 11 Q 12 Q τ 12 Q 22 &gt; 0, diagonal matrices ≥ 0, ≥ 0, D 1 ≥ 0, D 2 ≥ 0, D 3 ≥ 0 such that the following linear matrix inequality (LMI) holds 1 = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 11 1 h R 0 14 15 -hC τ R * 22 1 h R 0 25 0 * * 33 0 0 0 * * * 44 45 h A τ R * * * * 55 h B τ R * * * * * -h R ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">11 = -PC -C τ P + Q 3 -2K 1 D 1 K 2 + K D 3 K -(K 1 -K 2 )C -C τ (K 1 -K 2 ) - 1 h R +Q 11 -K 2 Q τ 12 -Q 12 K 2 + K 2 Q 22 K 2 14 = P A -C τ ( -) + (K 1 -K 2 )A +(K 1 + K 2 )D 1 + Q 12 -K 2 Q 22 15 = P B + (K 1 -K 2 )B 22 = -2K 1 D 2 K 2 -(1 -μ) K D 3 K + Q 11 -K 2 Q τ 12 -(1 -μ)(K 2 Q 22 K 2 -Q 12 K 2 ) - 2 h R 25 = (K 1 + K 2 )D 2 -(1 -μ)(Q 12 -K 2 Q 22 ) 33 = -Q 3 - 1 h R 44 = ( -)A + A τ ( -) + Q 22 -D 3 -2D 1 45 = ( -)B 55 = -(1 -μ)(Q 22 -D 3 ) -2D 2 .</formula><p>Proof: First, we show the uniqueness of the equilibrium point by the method of contradiction. To this end, let z be the equilibrium point of RNNs ( <ref type="formula">4</ref>), then we have</p><formula xml:id="formula_15">-C z + (A + B) f (z) = 0.</formula><p>Suppose z = 0. Then it follows that:</p><formula xml:id="formula_16">2 zτ (P + (K 1 -K 2 )) -f τ (z)( -) {-C z + (A + B) f (z)} = 0.<label>(7)</label></formula><p>Moreover, one can infer from (5) that</p><formula xml:id="formula_17">(D 1 + D 2 )(K 1 z -f (z)) τ ( f (z) -K 2 z) ≥ 0.<label>(8)</label></formula><p>Combining ( <ref type="formula" target="#formula_16">7</ref>) and ( <ref type="formula" target="#formula_17">8</ref>) together gives</p><formula xml:id="formula_18">ξ τ ξ ≥ 0<label>( 9 )</label></formula><p>where</p><formula xml:id="formula_19">ξ = z f (z) , = ¯ 11 -2K 1 D 2 K 2 ¯ 12 * ¯ 22</formula><p>where</p><formula xml:id="formula_20">¯ 11 = -PC -C τ P -(K 1 -K 2 )C -C τ (K 1 - K 2 ) + Q 3 -2K 1 D 1 K 2 , ¯ 12 = P(A + B) -C τ ( -) + (K 1 -K 2 )(A + B)+(K 1 + K 2 )(D 1 + D 2 ), ¯ 22 = ( -) (A + B) + (A + B) τ ( -) -2D 1 -2D 2 . Meanwhile, it is noted that [z f (z) -K 2 z] τ Q 11 Q 12 Q τ 12 Q 22 z f (z) -K 2 z ≥ 0 zτ K D 3 K z -f (z) τ D 3 f (z) ≥ 0</formula><p>which means the following inequality holds:</p><formula xml:id="formula_21">ξ τ ϒ 11 Q 12 -K 2 Q 22 * Q 22 -D 3 ξ ≥ 0 (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>where</p><formula xml:id="formula_23">ϒ 11 = K D 3 K + Q 11 -K 2 Q τ 12 -Q 12 K 2 + K 2 Q 22 K 2 .</formula><p>On the other hand, let = I I I 0 0 0 0 0 0 I I 0 .</p><p>Multiplying 1 by and τ on its left and right sides, respectively, and using ( <ref type="formula" target="#formula_18">9</ref>) and ( <ref type="formula" target="#formula_21">10</ref>), we have</p><formula xml:id="formula_24">+ μ ϒ 11 Q 12 -K 2 Q 22 * Q 22 -D 3</formula><p>&lt; 0 so we obtain &lt; 0. Obviously, this contradicts with (9). This implies that z = 0. That is, the origin of RNNs (4) is the unique equilibrium point.</p><p>Next, we show the stability condition by considering the following LKF candidate for RNNs (4)</p><formula xml:id="formula_25">V (z(t)) = V 1 (z(t)) + V 2 (z(t)) (<label>11</label></formula><formula xml:id="formula_26">)</formula><p>where</p><formula xml:id="formula_27">V 1 (z(t)) = z τ (t)Pz(t) + 0 -h t t +θ żτ (s)R ż(s)dsdθ + t t -h z τ (s)Q 3 z(s)ds + 2 n i=1 λ i z j 0 ( f j (s) -k - i s)ds + 2 n i=1 δ i z j 0 (k + i s -f j (s))ds V 2 (z(t)) = t t -d(t ) z τ (s)K D 3 K z(s)ds - t t -d(t ) ( f (z(s)) τ D 3 ( f (z(s))ds + t t -d(t ) η τ (s) Q 11 Q 12 Q τ 12 Q 22 η(s)ds and η(s) = [z τ (s), ( f (z(s)) -K 2 z(s)) τ ] τ . Let z d = z(t - d(t)), z h = z(t -h).</formula><p>Calculating the time derivative of V 1 (z(t)) along the solution of RNNs (4) yields</p><formula xml:id="formula_28">V1 (z(t)) ≤ 2z τ (t)P ż(t) + h żτ (t)R ż(t) + z τ d z τ h -1 h R 1 h R 1 h R -1 h R z d z h + z τ (t) z τ d -1 h R 1 h R 1 h R -1 h R z(t) z d + z τ (t)Q 3 z(t) -z τ h Q 3 z h + 2 f τ (z(t))( -) × [-Cz(t) + A f (z(t)) + B f (z d )] + 2z τ (t)(K 1 -K 2 ) × [-Cz(t) + A f (z(t)) + B f (z d )] (<label>12</label></formula><formula xml:id="formula_29">)</formula><p>where = diag{λ 1 , λ 2 , . . . , λ n } ≥ 0, = diag{δ 1 , δ 2 , . . . , δ n } ≥ 0. Similarly, we also have</p><formula xml:id="formula_30">V2 (z(t)) ≤ z τ (t)ϒ 11 z(t) -(1 -μ)z τ d ϒ 11 z d + 2z τ (t)(Q 12 -K 2 Q 22 ) f (z(t)) + f τ (z(t))(Q 22 -D 3 ) f (z(t)) -2(1 -μ)z τ d (Q 12 -K 2 Q 22 ) f (z d ) -(1 -μ) f τ (z d )(Q 22 -D 3 ) f (z d ). (<label>13</label></formula><formula xml:id="formula_31">)</formula><p>Similar to the method in <ref type="bibr" target="#b27">[22]</ref>, from (5) one can obtain</p><formula xml:id="formula_32">2(K 1 z(t) -f (z(t))) τ D 1 ( f (z(t)) -K 2 z(t)) ≥ 0 (14) 2(K 1 z d -f (z d )) τ D 2 ( f (z d ) -K 2 z d ) ≥ 0. (<label>15</label></formula><formula xml:id="formula_33">)</formula><p>Then combining ( <ref type="formula" target="#formula_28">12</ref>)-( <ref type="formula" target="#formula_32">15</ref>) together, it can be seen that V (z(t)) &lt; 0 if 1 &lt; 0. Thus, according to the Lyapunov-Krasovskii theorem, the equilibrium point of RNNs ( <ref type="formula">4</ref>) is globally asymptotically stable. Remark 2: In Theorem 1, the constructed LKF uses more information of the slope of neuron activation functions, which is called the delay-slope-dependent approach. Specifically, V 1 (z(t)) includes the two terms</p><formula xml:id="formula_34">z j 0 ( f i (s) -k - i s)ds and z j 0 (k + i s -f i (s))ds, while V 2 (z(t)) includes the term t t -d(t ) [z τ (s)K D 3 K z(s) -( f (z(s)) τ D 3 f (z(s))</formula><p>]ds and the vector f (z(s)) -K 2 z(s). Thus, the obtained criterion could lead to better results than the previous ones.</p><p>In <ref type="bibr" target="#b27">[22]</ref>, the effectiveness of</p><formula xml:id="formula_35">z j 0 ( f i (s) -k - i s)ds and z j 0 (k + i s -f i (s)</formula><p>)ds was demonstrated. In this brief, the effectiveness of</p><formula xml:id="formula_36">t t -d(t ) [z τ (s)K D 3 K z(s) -( f (z(s)) τ D 3 f (z(s)</formula><p>]ds and the vector f (z(s)) -K 2 z(s) in the constructed LKF will be mainly shown. Then we change V 2 (z(t)) of the form (11) into</p><formula xml:id="formula_37">V 2 (z(t)) = t t -d(t ) ( f (z(s)) -K 2 z(s)) τ Q 22 ( f (z(s)) -K 2 z(s))ds + t t -d(t ) z τ (s)Q 11 z(s)ds + t t -d(t ) z τ (s)K D 3 K z(s) -( f (z(s)) τ D 3 f (z(s)ds.</formula><p>The following corollary can be obtained.</p><p>Corollary 1: Given scalars h ≥ 0 and μ, for any delay d(t) satisfying Assumption 1, the origin of RNNs ( <ref type="formula">4</ref>) is asymptotically stable if there exist matrices P &gt; 0,</p><formula xml:id="formula_38">Q 3 &gt; 0, R &gt; 0, Q 11 &gt; 0, Q 22 &gt; 0, diagonal matrices ≥ 0, ≥ 0, D 1 ≥ 0, D 2 ≥ 0, D 3 ≥ 0 such that the following LMI holds: ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ˆ 11 1 h R 0 ˆ 14 15 -hC τ R * ˆ 22 -2 h R 1 h R 0 ˆ 25 0 * * 33 0 0 0 * * * 44 45 h A τ R * * * * 55 h B τ R * * * * * -h R ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0</formula><p>where</p><formula xml:id="formula_39">ˆ 11 = -PC -C τ P -(K 1 -K 2 )C -C τ (K 1 -K 2 ) + Q 3 - 1 h R -2K 1 D 1 K 2 + K D 3 K + Q 11 + K 2 Q 22 K 2 ˆ 14 = P A -C τ ( -) + (K 1 -K 2 )A + (K 1 + K 2 )D 1 + K 2 Q 22 ˆ 22 = -2K 1 D 2 K 2 -(1 -μ)(K D 3 K + Q 11 + K 2 Q 22 K 2 ) ˆ 25 = (K 1 + K 2 )D 2 -(1 -μ)K -Q 22</formula><p>and 15 , 44 , and 55 are the same as defined in <ref type="bibr" target="#b11">(6)</ref>.</p><p>Remark 3: In the previous results given in <ref type="bibr" target="#b21">[16]</ref> and <ref type="bibr" target="#b27">[22]</ref>, the chosen LKFs include only the terms  If the slope K 2 and K of neuron activation functions are not considered, then V 2 (z(t)) in the LKF of the form (11) will reduce to</p><formula xml:id="formula_40">V 2 (z(t)) = t t -d(t ) [z(s) f (z(s))] τ Q 11 Q 12 Q τ 12 Q 22 z(s) f (z(s))</formula><p>ds.</p><p>The following result is straightforward by following a similar line as the proof of Theorem 1.</p><p>Corollary 2: Given scalars h ≥ 0 and μ, for any delay d(t) satisfying Assumption 1, the origin of RNNs ( <ref type="formula">4</ref>) is asymptotically stable if there exist matrices P &gt; 0,</p><formula xml:id="formula_41">Q 3 &gt; 0, R &gt; 0, Q 11 Q 12 Q τ 12 Q 22 &gt; 0, diagonal matrices ≥ 0, ≥ 0, D 1 ≥ 0, D 2 ≥ 0 such that the following LMI holds: ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ˜ 11 1 h R 0 ˜ 14 15 -hC τ R * ˜ 22 -2 h R 1 h R 0 ˜ 25 0 * * ˜ 33 0 0 0 * * * 44 ˜ 45 h A τ R * * * * 55 h B τ R * * * * * -h R ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ &lt; 0</formula><p>where</p><formula xml:id="formula_42">˜ 11 = -PC -C τ P -(K 1 -K 2 )C + Q 3 -C τ (K 1 -K 2 ) -2K 1 D 1 K 2 + Q 11 - 1 h R ˜ 14 = P A -C τ ( -) + (K 1 -K 2 )A + (K 1 + K 2 )D 1 + Q 12 ˜ 22 = -2K 1 D 2 K 2 -(1 -μ)Q 11 ˜ 25 = (K 1 + K 2 )D 2 -(1 -μ)Q 12</formula><p>and 44 and 55 are the same as defined in <ref type="bibr" target="#b11">(6)</ref>.</p><p>Remark 4: Compared with the results in <ref type="bibr" target="#b19">[14]</ref> and <ref type="bibr" target="#b21">[16]</ref>, under the circumstances of maintaining the efficiency of the stability conditions, some redundant matrices are not introduced in Corollary 2, except for the matrix Q 12 . Thus, the computational burden is largely reduced. On the other hand, it is noted that [22, Th. 1] can be covered by letting Q 12 = 0 in Corollary 2. Moreover, the matrix Q 12 is useful for reducing the conservatism of the results, which can be seen from Example 1 to be given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL EXAMPLES</head><p>In this section, we use two examples to show the usefulness of our results.</p><p>Example 1: Consider the delayed RNNs (4) with parameters as given in <ref type="bibr" target="#b27">[22]</ref>  <ref type="table">I</ref> provides some comparisons of the maximum allowable delay bounds and the numbers of the variables involved in the results of this brief and those in <ref type="bibr" target="#b19">[14]</ref>, <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b27">[22]</ref>, and <ref type="bibr" target="#b28">[23]</ref>. From Table <ref type="table">I</ref>  </p><formula xml:id="formula_43">C = 2 0 0 2 , A = 1 1 -1 -1 , B = 0.88 1 1 1 , k + 1 = 0.4, k + 2 = 0.8. Table</formula><formula xml:id="formula_44">⎤ ⎥ ⎥ ⎥ ⎦ , k + 1 = 0.1137, k + 2 = 0.1279, k + 3 = 0.7994, k + 4 = 0.2368. When k - 1 = k - 2 = k - 3 = k - 4</formula><p>= 0 and μ = 0, the maximum allowable delay bound is 1.4224 in <ref type="bibr" target="#b17">[12]</ref>, 1.9321 in <ref type="bibr" target="#b18">[13]</ref>, 3.5841 in <ref type="bibr" target="#b21">[16]</ref>, 3.6156 in <ref type="bibr" target="#b28">[23]</ref>, and 3.7327 in <ref type="bibr" target="#b27">[22]</ref>. Then, applying Theorem 1 in this brief, the maximum allowable delay bound is 3.8363, which shows that our criterion is much less conservative than those in <ref type="bibr" target="#b17">[12]</ref>, <ref type="bibr" target="#b18">[13]</ref>, <ref type="bibr" target="#b21">[16]</ref>, <ref type="bibr" target="#b27">[22]</ref>, and <ref type="bibr" target="#b28">[23]</ref>.</p><p>When  <ref type="table">II</ref>, it is easy to see that the stability condition in this brief is less conservative and involves fewer variables than the theorems in <ref type="bibr" target="#b26">[21]</ref> and <ref type="bibr" target="#b27">[22]</ref>. For example, in the case of μ = 0.1, when compared with [21, Th. 1], Theorem 1 in this brief has improved the maximum allowable delay bound by about 91% and in the meantime has reduced the computed variables by about 130%.</p><formula xml:id="formula_45">k - 1 = -0.4, k - 2 = 0.1, k - 3 = 0, k - 4 = -0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this brief, we have investigated the stability problem of the neural networks with time-varying delays by employing the slope-dependent LKF. Comparative studies have shown that the derived criteria are less conservative and contain fewer slack variables than those in the literature. This brief mainly focuses on the influence of the slope of neuron activation functions. In our future work, we will try to combine our slope-dependent approach with some technique for dealing with the time delay term so as to further improve the results. Another interesting work would be to extend the frequencydomain-based stability criteria given in <ref type="bibr">[30]</ref> to RNNs with time-varying delay. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Vector quantization (VQ) has come a long way from its first definition given by Shannon <ref type="bibr" target="#b6">[1]</ref>: "• • • it is a mapping of real vectors (an ordered set of signal samples) into binary vectors using a minimum distortion rule." It is defined, in a broader sense, as a process that maps a set of points to another set of points (usually, a smaller set with a lower dimension) by satisfying a given criterion. In this brief, we define the vector quantization in the spirit of the work by Grossberg <ref type="bibr" target="#b7">[2]</ref>, <ref type="bibr" target="#b8">[3]</ref>, and Kohonen <ref type="bibr" target="#b9">[4]</ref>, it samples the probability density function of the pattern space and consolidates information from enormous data to model the information world. A vector quantizer alone has a limited use, yet given the quantized set of points known as prototypes, useful clusters can be formed which represent the given data well in a complex pattern space. Besides statistical approaches, artificial neural networks are widely used for solving many classification or clustering problems through the VQ approach. Recently, some new directions in biologically inspired dynamical systems have been developed <ref type="bibr" target="#b10">[5]</ref>, <ref type="bibr" target="#b11">[6]</ref>, however their effectiveness remains to be seen. Other methods based on the parameter approximation and the finite mixture models <ref type="bibr" target="#b12">[7]</ref>, <ref type="bibr" target="#b13">[8]</ref> (also called the radial bases functions models) are approaching near optimum results, however, the computational complexity on a serially-run platform is always challenging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t t -d(t ) z τ (s)Q 11 z(s)ds and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t t -d(t ) f τ (z(s))Q 22 f (z(s))ds, which ignores the slope information K 2 and K of neuron activation functions. So it may lead to considerable conservatism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-Time Vector Quantization and Clustering Based on Ordinary Differential Equations Jie Cheng, Mohammad R. Sayeh, Mehdi R. Zargham, and Qiang Cheng Abstract-This brief presents a dynamical system approach to vector quantization or clustering based on ordinary differential equations with the potential for real-time implementation. Two examples of different pattern clusters demonstrate that the model can successfully quantize different types of input patterns. Furthermore, we analyze and study the stability of our dynamical system. By discovering the equilibrium points for certain input patterns and analyzing their stability, we have shown the quantizing behavior of the system with respect to its vigilance parameter. The proposed system is applied to two real-world problems, providing comparable results to the best reported findings. This validates the effectiveness of our proposed approach. Index Terms-Neural networks, ordinary differential equation-based clustering, real-time clustering, vector quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, it is seen that our results (Theorem 1, Corollaries 1 and 2) contain a smaller number of computed variables and are less conservative than [14, Th. 1], [16, Th. 1], [23, Th. 1], and [22, Th. 2]. For example, when</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>3, the maximum allowable delay bounds h derived by Theorem 1 in this brief, [21, Th. 1] and [22, Th. 2] are listed in Table II. Form the comparisons presented in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Associate Editor and the four anonymous reviewers for their valuable comments and suggestions, which led to significant improvements of the quality and presentation of this brief.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation of China, under Grant 60904025, Grant 60904026, and Grant 61174033, the Key Laboratory of Education Ministry for Image Processing and Intelligent Control, under Grant 200805, and the Research Grant from the Australian Research Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimizing a differentiable function over a differential manifold</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gabay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="219" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The gradient projection method along geodesics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manage. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="620" to="631" />
			<date type="published" when="1972-07">Jul. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified algorithm for principal and minor components extraction</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="369" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unified stabilization approach to principal and minor components extraction algorithms</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1377" to="1387" />
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the achievable throughput of a multiantenna Gaussian broadcast channel</title>
		<author>
			<persName><forename type="first">G</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1691" to="1705" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the adjugate matrix</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="151" to="164" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Unbehauen</surname></persName>
		</author>
		<title level="m">Neural Networks for Optimization and Signal Processing</title>
		<meeting><address><addrLine>Chichester, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cellular neural networks: Applications</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1273" to="1290" />
			<date type="published" when="1988-10">Oct. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An analysis of global asymptotic stability of delayed neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1239" to="1242" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generalized LMI-based approach to the global asymptotic stability of delayed cellular neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="225" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global stability of class of neural networks with time varying delays</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ensari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="126" to="130" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global asymptotic stability and robust stability of recurrent neural networks with time delays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="417" to="426" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global exponential stability and periodicity of recurrent neural networks with time delays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="920" to="931" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global asymptotic stability of recurrent neural networks with multiple time-varying delays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="873" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global asymptotic stability of delayed cellular neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="947" to="950" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On global asymptotic stability of neural networks with discrete and distributed delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stability analysis for stochastic Cohen-Grossberg neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="814" to="820" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel global asymptotic stability criteria for delayed cellular neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="349" to="353" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Novel delay-dependent robust stability criterion of delayed cellular neural networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos, Solitons, Fractals</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1194" to="1200" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">New results on stability analysis of neural networks with time-varying delays</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="page" from="335" to="340" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LMI-based stability criteria for neural networks with multiple time-varying delays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New delay-dependent stability criteria for neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="314" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved delay-dependent asymptotic stability criteria for delayed neural networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2154" to="2161" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delay-dependent exponential stability of neural networks with variable delay: An LMI approach</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Design of exponential state estimators for neural networks with mixed time delays</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. A</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stability criteria with less LMI variables for neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1188" to="1192" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved stability criteria of neural networks with time-varying delays: An augmented LKF approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1038" to="1047" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">New delay-dependent stability results for neural networks with time-varying delay</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1783" to="1791" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Novel weightingdelay-based stability criteria for recurrent neural networks with timevarying delay</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Capacity of associative memory using a nonmonotonic neural model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="176" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On global asymptotic stability of a class of nonlinear systems arising in neural network theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Different. Equat</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="264" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">New conditions for global stability of neural networks with application to linear and quadratic programming problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I: Fundam. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="354" to="366" />
			<date type="published" when="1995-07">Jul. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relating the slope of the activation function and the learning rate within a recurrent neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1069" to="1077" />
			<date type="published" when="1999-07">Jul. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neurons with graded response have collective computational properties like those of two state neurons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat. Acad. Sci. United States Amer</title>
		<meeting>Nat. Acad. Sci. United States Amer</meeting>
		<imprint>
			<date type="published" when="1984-05">May 1984</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="3088" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Absolute exponential stability of recurrent neural networks with generalized activation function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1075" to="1089" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stability of systems with several monotone nonlinearities</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Barabanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="670" />
			<date type="published" when="2003-04">Apr. 2003. 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Manuscript received January. revised September 27</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Date of publication October 31, 2011; date of current version December 1, 2011</title>
		<imprint>
			<date type="published" when="2011-09-28">September 28, 2011</date>
			<pubPlace>Hilo</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cheng is with the Department of Computer Science, University of Hawaii</orgName>
		</respStmt>
	</monogr>
	<note>HI 96720 USA (e-mail: chengjie@hawaii.edu</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sayeh is with the Department of Electrical and Computer Engineering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<pubPlace>Southern Illinois University Carbondale, Carbondale</pubPlace>
		</imprint>
	</monogr>
	<note>IL 62901 USA (e-mail: sayeh@siu.edu</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Color versions of one or more of the figures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Zargham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2011.21726271045-9227/$26.00©2011IEEE</idno>
		<ptr target="http://ieeexplore.ieee.org" />
		<imprint>
			<pubPlace>Carbondale</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Southern Illinois University Carbondale</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Digital Object Identifier</note>
	<note>IL 62901 USA (e-mail: mehdi@cs. in this paper are available</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
