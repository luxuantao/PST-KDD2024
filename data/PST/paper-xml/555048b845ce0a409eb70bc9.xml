<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Dependable Data Repairing with Fixing Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiannan</forename><surname>Wang</surname></persName>
							<email>jnwang@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Tang</surname></persName>
							<email>ntang@qf.org.qa</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Qatar Computing Research Institute (QCRI</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">SIGMOD&apos;14</orgName>
								<address>
									<addrLine>June 22-27</addrLine>
									<postCode>2014</postCode>
									<settlement>Snowbird</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Dependable Data Repairing with Fixing Rules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E67D6865F7AA3C3321C15101E8C52D6B</idno>
					<idno type="DOI">10.1145/2588555.2610494</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>&apos;4: (([capital</term>
					<term>conf]</term>
					<term>[Beijing</term>
					<term>ICDE])</term>
					<term>(city</term>
					<term>{Hongkong})</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the main challenges that data cleaning systems face is to automatically identify and repair data errors in a dependable manner. Though data dependencies (a.k.a. integrity constraints) have been widely studied to capture errors in data, automated and dependable data repairing on these errors has remained a notoriously hard problem. In this work, we introduce an automated approach for dependably repairing data errors, based on a novel class of fixing rules. A fixing rule contains an evidence pattern, a set of negative patterns, and a fact value. The heart of fixing rules is deterministic: given a tuple, the evidence pattern and the negative patterns of a fixing rule are combined to precisely capture which attribute is wrong, and the fact indicates how to correct this error. We study several fundamental problems associated with fixing rules, and establish their complexity. We develop e cient algorithms to check whether a set of fixing rules is consistent, and discuss approaches to resolve inconsistent fixing rules. We also devise e cient algorithms for repairing data errors using fixing rules. We experimentally demonstrate that our techniques outperform other automated algorithms in terms of the accuracy of repairing data errors, using both real-life and synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data quality is essential to all businesses, which demands dependable data cleaning solutions. Traditionally, data dependencies (a.k.a. integrity constraints) have been widely studied to capture errors from semantically related values. However, automated and dependable data repairing on these data errors has remained a notoriously hard problem.</p><p>A number of recent research [4, 7, 10] have investigated the data cleaning problem introduced in [2]: repairing is to find another database that is consistent and minimally differs from the original database. They compute a consistent database by using di↵erent cost functions for value updates ⇤ Work done while interning at QCRI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and various heuristics to guide repairing. However, it is known that such heuristics might introduce data errors <ref type="bibr" target="#b19">[19]</ref>. In order to ensure that a repair is dependable, users are involved in the process of data repairing <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28]</ref>, which is usually time-consuming and cumbersome.</p><p>In response to practical need for automated and dependable data repairing, in this work, we propose new data cleaning algorithms, based on a class of fixing rules. Given a tuple, fixing rules are designed to precisely capture which attribute is wrong, and specify what value it should take. Motivating example. We first illustrate by examples how existing solutions work. We then motivate our approach.</p><p>Example 1: Consider a database D of travel records for a research institute, specified by the following schema:</p><p>Travel (name, country, capital, city, conf), where a Travel tuple specifies a person, identified by name, who has traveled to conference (conf), held at the city of the country with capital. One Travel instance is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. All errors are highlighted and their correct values are given between brackets. For instance, r2[capital] = Shanghai is wrong, whose correct value is Beijing.</p><p>2 We next describe how existing methods work. Data dependencies. A variety of data dependencies have been used to capture errors in data, from traditional constraints (e.g., functional and inclusion dependencies <ref type="bibr" target="#b7">[7,</ref><ref type="bibr">9]</ref>) to their extensions (e.g., conditional functional dependencies <ref type="bibr" target="#b15">[15]</ref>). Suppose that a functional dependency (FD) is specified for the Travel table as:</p><formula xml:id="formula_0">1: Travel ([country] ! [capital])</formula><p>which states that country uniquely determines capital. One can verify that in Fig. <ref type="figure" target="#fig_0">1</ref>, the two tuples (r1, r2) violate 1, since they have the same country but carry di↵erent capital values, so do (r1, r3) and (r2, r3).</p><p>In order to compute a consistent database w.r.t. User guidance. While using data dependencies to detect errors is appropriate, dependencies on their own are not sucient to guide dependable data repairing. To improve the accuracy of data repairing, users have been involved <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b28">28]</ref> and master data (a.k.a. reference data) has been used <ref type="bibr" target="#b19">[19]</ref>. Consider a recent work <ref type="bibr" target="#b19">[19]</ref> that uses editing rules and master data. Figure <ref type="figure">2</ref> shows a master data Dm of schema Cap (country, capital), which is considered to be correct. An editing rule eR1 defined on two relations (Travel, Cap) is: eR1 : ((country, country) ! (capital, capital), tp1[country] = ())</p><p>Rule eR1 states that: for any tuple r in a Travel table, if r[country] is correct and it matches s[country] from a Cap table, we can update r[capital] with the value s[capital] from Cap. For instance, to repair r2 in Fig. <ref type="figure" target="#fig_0">1</ref>, the users need to ensure that r2[country] is correct, and then match r2[country] and s1 <ref type="bibr">[country]</ref> in the master data, so as to update r2[capital] to s1 <ref type="bibr">[capital]</ref>. It proceeds similarly for the other tuples. Key challenge &amp; observation. The above examples tell us that data dependencies can detect errors but fall short of automatically guiding data repairing, while involving users is generally cost-ine↵ective. Hence, one of the main challenges in data cleaning is how to automatically detect and repair errors in a dependable manner.</p><p>Data cleaning is not magic; it cannot guess something from nothing. What it does is to make decisions from evidence. Certain data patterns of semantically related values can provide evidence to precisely capture and rectify data errors. For example, when values (China, Shanghai) for attributes (country, capital) appear in a tuple, it su ces to judge that the tuple is about China, and Shanghai should be Beijing, the capital of China. In contrast, the values (China, Tokyo) are not enough to decide which value is wrong. Fixing rules. Motivated by the observation above, in this work, we address the problem of automatically finding dependable repairs, by using fixing rules. A fixing rule contains an evidence pattern, a set of negative patterns, and a fact value. Given a tuple, the evidence pattern and the negative patterns of a fixing rule are combined to precisely tell which attribute is wrong, and the fact indicates how to correct it.</p><p>Example 2: Figure <ref type="figure" target="#fig_3">3</ref> shows two fixing rules. The brackets mean that the corresponding cell is multivalued.</p><p>For the first fixing rule '1, its evidence pattern, negative patterns and the fact are China, {Shanghai, Hongkong}, and Beijing, respectively. It states that for a tuple t, if its country is China and its capital is either Shanghai or Hongkong, capital should be updated to Beijing. For instance, consider the database in Fig. and r3[country], still remain. We will discuss later how they are repaired, when more fixing rules are available. 2 Remark. Fixing rules are designed to both capture semantic errors for specific domains (e.g., (China, Shanghai) is an error for (country, capital)), and specify how to fix it (e.g., change Shanghai to Beijing), in a deterministic and dependable manner. They are also conservative: they tend to avoid repairing ambiguous errors such as (China, Tokyo), which is also di cult for users to repair since it could be either (China, Beijing) or (Japan, Tokyo).</p><p>Contributions. We propose a framework for automatically and dependably repairing data errors.</p><p>(1) We formally define fixing rules and their repairing semantics (Section 3). Given a tuple t, fixing rules tell us which attribute is wrong and what value it should take.</p><p>(2) We study fundamental problems of fixing rules (Section 4). Specifically, given a set ⌃ of fixing rules, we determine whether these rules have conflicts. We show that this problem is in PTIME. We also study the problem of whether some other fixing rules are implied by ⌃. We show that this problem is coNP-complete, but it is down to PTIME when the relation schema is fixed.</p><p>(3) We develop e cient algorithms to check whether a set of fixing rules is consistent i.e., conflict-free (Section 5). We also discuss solutions to resolve inconsistent fixing rules.</p><p>(4) We propose two repairing algorithms for a given set ⌃ of fixing rules (Section 6). The first algorithm is chase-based. It runs in O(size(⌃)|R|) for one tuple, where |R| is the cardinality of R and size(⌃) is the size of ⌃. The second one is a fast linear algorithm that runs in O(size(⌃)) for one tuple, by interweaving inverted lists and hash counters.</p><p>(5) We experimentally verify the e↵ectiveness and scalability of the proposed algorithms (Section 7). We find that algorithms with fixing rules can repair data with high precision. In addition, they scale well with the number of fixing rules. One natural concern is how to generate fixing rules. Inspired by the work of <ref type="bibr" target="#b27">[27]</ref>, we show how a large number of fixing rules can be obtained from examples. Organization. Section 2 discusses related work. Section 3 introduces fixing rules. Section 4 studies fundamental problems for fixing rules. Section 5 describes algorithms to check consistency of fixing rules and ways to resolve inconsistent rules. Section 6 presents repairing algorithms using fixing rules. Section 7 reports our experimental findings, followed by conclusion in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Despite the need for dependable algorithms to automatically repair data, there has been little discussion about data cleaning solutions that can both capture semantic data errors, and explicitly specify an action to correct these errors, without interacting with users, and without any assumption about confidence values placed on the data.</p><p>In recent years, there has been an increasing amount of literature on using data dependencies in cleaning data (e.g., <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b22">22]</ref>; see <ref type="bibr" target="#b14">[14]</ref> for a survey). They have been revisited to better capture data errors as violations of these dependencies (e.g., CFDs <ref type="bibr" target="#b15">[15]</ref> and CINDs <ref type="bibr" target="#b8">[8]</ref>). As remarked earlier, fixing rules di↵er from those dependencies in that fixing rules can not only detect semantic errors, but also explicitly specify how to fix these errors.</p><p>Editing rules <ref type="bibr" target="#b19">[19]</ref> have been introduced for the process of data monitoring to repair data that is guaranteed correct. However, editing rules require users to examine every tuple, which is expensive. Fixing rules di↵er from them in that they do not depend on users to trigger repairing operations. Instead, fixing rules use both evidence pattern and negative patterns to automatically trigger repairing operations.</p><p>Data repairing algorithms have been proposed <ref type="bibr">[6,7,10-13, 18-20, 23, 28]</ref>. Heuristic methods are developed in <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20]</ref>, based on FDs <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b22">22]</ref>, FDs and inds <ref type="bibr" target="#b7">[7]</ref>, CFDs <ref type="bibr" target="#b15">[15]</ref>, CFDs and MDs <ref type="bibr" target="#b18">[18]</ref>, denial constraints <ref type="bibr" target="#b10">[10]</ref> and editing rules <ref type="bibr" target="#b20">[20]</ref>. Some works employ confidence values placed by users to guide a repairing process <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18]</ref> or use master data <ref type="bibr" target="#b19">[19]</ref>. Statistical inference is studied in <ref type="bibr" target="#b23">[23]</ref> to derive missing values, and in <ref type="bibr" target="#b6">[6]</ref> to find possible repairs. To ensure the accuracy of generated repairs, <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b28">28]</ref> require to consult users. In contrast to these prior art, (1) fixing rules are more conservative to repair data, which target at determinism and dependability, instead of computing a consistent database; (2) we neither consult the users, nor assume the confidence values placed by the users. Indeed, our method can be treated as a complementary technique to heuristic methods i.e., one may compute dependable repairs first and then use heuristic solutions to find a consistent database.</p><p>There has also been a lot of work on more general data cleaning: data transformation, which brings the data under a single common schema <ref type="bibr" target="#b24">[24]</ref>. ETL tools (see <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b21">21]</ref> for a survey) provide sophisticated data transformation methods, which can be employed to merge data sets and repair data based on reference data. Some recent work has been studied for semantic transformations <ref type="bibr" target="#b27">[27]</ref> of strings. However, they are designed for value transformation instead of capturing semantic errors as fixing rules do. Hence, they can be treated as an orthogonal technique which prepares data that is in turn to be repaired by other data cleaning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FIXING RULES</head><p>In this section, We first give the formal definition of fixing rules and their semantics (Section 3.1). We then describe the repairing semantics for a set of fixing rules (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>Consider a schema R defined over a set of attributes, denoted by attr(R). We use A 2 R to denote that A is an attribute in attr(R). For each attribute A 2 R, its domain is specified in R, denoted as dom(A). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Repairing Semantics with Fixing Rules</head><p>We next describe the semantics of applying a set of fixing rules. Note that when applying a fixing rule ' to a tuple</p><formula xml:id="formula_1">t, we update t[B'] with t + p [B'].</formula><p>To ensure that the change makes sense, the corrected values should remain unchanged in the following process. That is, after applying ' to t, the set X' [ {B'} of attributes should be marked as correct.</p><p>In order to keep track of the set of attributes that has been marked correct, we introduce the notion assured attributes to represent them, denoted by At relative to tuple t. We simply write A when t is clear from the context.</p><p>We say that a fixing rule ' is properly applied to a tuple t w.r.t. the assured attributes A, denoted by t ! (A,') t 0 , if (i) t matches ', and (ii) B' 6 2A . That is, it is justified that to apply ' to t, for those t match ', is correct. As A has been assured, we do not allow it to be changed by enforcing B' 6 2A (condition (ii)). r2 matches '1); and moreover, capital 6 2A r 2 . This yields an updated tuple r 0 2 where r 0 2 [capital] = Beijing. 2 Observe that if t ! (A,') t 0 , then X' and B' will also be marked correct. Thus, the assured attributes A should be extended as well, to become A [ X' [ {B'}. Example 6: Consider Example 5. After '1 is applied to r2, the assured attribute Ar 2 will be expanded correspondingly, by including X' 1 (i.e., {country}) and {B' 1 } (i.e., {capital}), which results in an expanded assured attribute set Ar 2 = {country, capital}.</p><formula xml:id="formula_2">2 We write t = ! (A,') t if ' cannot be properly applied to t, i.e., t is unchanged by ' relative to A, if either t does not match ', or B' 2 A.</formula><p>Consider a set ⌃ of fixing rules defined on R. Given a tuple t of R, we want a unique fix of t by using ⌃. That is, no matter in which order the fixing rules of ⌃ are properly applied, ⌃ yields a unique t 0 by updating t.</p><p>To formalize the notion of unique fixes, we first recall the repairing semantics of fixing rules. Notably, if ' is properly applied to t via t ! (A,') t 0 w.r.t. assured attributes A, it yields an updated t 0 where t</p><formula xml:id="formula_3">[B'] 2 T p [B'] and t 0 [B'] = t + p [B'].</formula><p>More specifically, the fixing rule ' first identifies t[B'] as incorrect, and as a logical consequence of the application of ', t[B'] will be updated to t + p [B'], as a validated correct value in t 0 . Once an attribute value t 0 [B] is validated, we do not allow it to be changed, together with the attributes X' that are used as the evidence to assert that t[B'] is incorrect. Fixes. We say that a tuple t 0 is a fix of t w.r.t. a set ⌃ of fixing rules, if there exists a finite sequence t = t0, t1, . . . , t k = <ref type="bibr" target="#b1">(1)</ref> ensures that each step of the process is justified, i.e., a fixing rule is properly applied. Condition (2) ensures that t 0 is a fixpoint and cannot be further updated.</p><formula xml:id="formula_4">t 0 of tuples of R such that for each i 2 [1, k], there ex- ists a 'i 2 ⌃ such that (1) ti 1 ! (A i ,' i ) ti, where A1 = ;, Ai = Ai 1[X' i [{B' i }; and (2) for any ' 2 ⌃, t 0 = ! (A k ,') t 0 . Condition</formula><p>We write t ⇤ ! (A,⌃) t 0 to denote that t 0 is a fix of t. Unique fixes. We say that an R tuple t has a unique fix by a set ⌃ of fixing rules if there exists a unique t 0 such that</p><formula xml:id="formula_5">t ⇤ ! (;,⌃) t 0 .</formula><p>Example 7: Consider Example 5. Indeed, r 0 2 is a fix of r2 w.r.t. rules '1 and '2 in Example 3, since no rule can be properly applied to r 0 2 , given the assured attributes to be {country, capital}.</p><p>Moreover, r 0 2 is also a unique fix, since one cannot get a tuple di↵erent from r 0 2 when trying to apply rules '1 and '2 on tuple r2 in other orders. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FUNDAMENTAL PROBLEMS</head><p>We next identify fundamental problems associated with fixing rules, and establish their complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Termination</head><p>One natural question associated with rule based data repairing processes is the termination problem that determines whether a rule-based process will stop. In fact, it is readily to verify that the fix process, by applying fixing rules (see Section 3.2), always terminates.</p><p>Consider the following. For a sequence of updates t0 ! (A 1 ,' 1 ) t1 . . . ! (A i ,' i ) ti . . ., each time a fixing rule 'i (i 1) is applied as ti 1 ! (A i ,' i ) ti, the number of validated attributes in A is strictly increasing up to |R|, the cardinality of schema R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consistency</head><p>The problem is to decide whether a set ⌃ of fixing rules does not have conflicts. We say that ⌃ is consistent if for any input tuple t of R, t has a unique fix by ⌃.</p><p>Example 8: Consider a fixing rule ' 0 1 by adding a negative pattern to the '1 in Example 3 as the following:</p><formula xml:id="formula_6">' 0 1 : (([country], [China]</formula><p>), (capital, {Shanghai, Hongkong, Tokyo})) ! Beijing The revised rule ' 0 1 states that, for a tuple, if its country is China and its capital value is Shanghai, Hongkong or Tokyo, its capital is wrong and should be updated to Beijing.</p><p>Consider another fixing rule '3 as: for t in relation Travel, if the conf is ICDE, held at city Tokyo and capital Tokyo, but the country is China, its country should be updated to Japan. This fixing rule can be formally expressed below:</p><formula xml:id="formula_7">'3: (([capital, city, conf], [Tokyo, Tokyo, ICDE]),</formula><p>(country, {China})) ! Japan We show that these two fixing rules, ' 0 1 and '3, are inconsistent. Consider the tuple r3 in Fig. <ref type="figure" target="#fig_0">1</ref>. Both ' 0 1 and '3 can be applied to r3. It has the following two fixes:</p><p>(1) r3 ! (;,' 0 1 ) r 0 3 : it will change attribute r3[capital] from Tokyo to Beijing. This will result in an updated tuple as: r 0 3 : (Peter, China, Beijing , Tokyo, ICDE). It also marks attributes {country, capital} as assured, such that '3 cannot be properly applied, i.e., r 0 3 is a fixpoint. (2) r3 ! (;,' 3 ) r 00 3 : it will update r3[country] from China to Japan. This will yield another updated tuple as: r 00 3 : (Peter, Japan, Tokyo , Tokyo, ICDE).</p><p>The attributes {country, capital, conf} will be marked as assured, such that ' 0 1 cannot be properly applied, i.e., r 00 3 is also a fixpoint.</p><p>Observe that the above two fixes (i.e., r 0 3 and r 00 3 ) will lead to di↵erent fixpoints, where the di↵erence is highlighted above. Therefore, ' 0 1 and '3 are inconsistent. Indeed, r 0 3 contains errors while r 00 3 is correct. 2</p><p>Consistency problem. The consistency problem is to determine, given a set ⌃ of fixing rules defined on R, whether ⌃ is consistent. Intuitively, this is to determine whether the rules in⌃ are dirty themselves. The practical need for the consistency analysis is evident: we cannot apply these rules to clean data before ⌃ is ensured consistent itself.</p><p>This problem has been studied for CFDs, MDs, and editing rules. It is known that the consistency problem for MDs <ref type="bibr" target="#b17">[17]</ref> is trivial: any set of MDs is consistent <ref type="bibr" target="#b18">[18]</ref>. They are NPcomplete (resp. coNP-complete) for CFDs <ref type="bibr" target="#b15">[15]</ref> (resp. editing rules <ref type="bibr" target="#b19">[19]</ref>). We shall show that the problem for fixing rules is PTIME, lower than their editing rules counterparts.</p><p>Theorem 1: The consistency problem of fixing rules is PTIME.</p><p>2 We prove Theorem 1 by providing a PTIME algorithm for determining if a set of fixing rules is consistent in Section 5.2.</p><p>The low complexity from the consistency analysis tells us that it is feasible to e ciently find consistent fixing rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implication</head><p>Given a set ⌃ of consistent fixing rules, and another fixing rule ' that is not in ⌃, we say that ' is implied by ⌃, denoted by⌃ |= ', if (i) ⌃ [ {'} is consistent; and (ii ) for any input t where t ⇤ !⌃ t 0 and t ⇤ ! ⌃[{'} t 00 , t 0 and t 00 are the same. Condition (i) says that ⌃ and ' must agree on each other. Condition (ii) ensures that for any tuple t, applying ⌃ or ⌃[{'} will result in the same updated tuple, which indicates that ' is redundant. Implication problem. The implication problem is to decide, given a set ⌃ of consistent fixing rules, and another fixing rule ', whether ⌃ implies '.</p><p>Intuitively, the implication analysis helps us find and remove redundant rules from ⌃, i.e., those that are a logical consequence of other rules in ⌃, to improve performance.</p><p>No matter how desirable it is to remove redundant rules, unfortunately, the implication problem is coNP-complete.</p><p>Theorem 2: The implication problem of fixing rules is coNP-complete. It is down to PTIME when the relation schema R is fixed.</p><p>2 Proof sketch: (A) General case. Lower bound. We show the implication problem is coNP-hard by reduction from the 3SAT problem, which is NP-complete <ref type="bibr" target="#b25">[25]</ref>, to the complement of the implication problem.</p><p>Upper bound. To show it is in coNP, we first establish a small model property: a set ⌃ of fixing rules is consistent if and only if for any tuple t of R consisting of values appeared in ⌃, t has a unique fix by ⌃. We then give an NP algorithm to its complement problem that first guesses a tuple t with values appear in ⌃ and then checks whether t has a unique fix by ⌃ in PTIME. (B) Special case: when R is fixed. We show that for fixed R, only a polynomially number of tuples need to be guessed and checked with a PTIME algorithm. Thus it is down to PTIME in this special case.</p><p>2 Details are omitted due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Determinism</head><p>Determinism problem. The determinism problem asks whether all terminating cleaning processes end up with the same repair.</p><p>From the definition of consistency of fixing rules, it is trivial to get that, if a set ⌃ of fixing rules is consistent, for any t of R, applying ⌃ to t will terminate, and the updated t 0 is deterministic (i.e., a unique result).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ENSURING CONSISTENCY</head><p>Our next goal is to study methods for identifying consistent rules. We first describe the workflow for obtaining consistent fixing rules (Section 5.1). We then present algorithms to check whether a given set of rules is consistent (Section 5.2). We also discuss how to resolve inconsistent fixing rules, and ensure the workflow terminates (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>Given a set ⌃ of fixing rules, our workflow contains the following three steps to obtain a set⌃ 0 of fixing rules that is ensured to be consistent.</p><formula xml:id="formula_8">⌃ / / step1 NO / / YES ( ( step2 i i step3 / / ⌃ 0</formula><p>Step 1: It checks whether the given ⌃ of fixing rules is consistent. If it is inconsistent, it goes to step <ref type="bibr" target="#b2">(2)</ref>. Otherwise, it goes to step (3).</p><p>Step 2: We allow either an automatic algorithm or experts to examine and resolve inconsistent fixing rules. After some rules are revised, it will go back to step (1).</p><p>Step 3: It terminates when the set⌃ 0 of (possibly) modified fixing rules is consistent.</p><p>It is desirable that the users are involved in step (2) when resolving inconsistent rules, in order to obtain high quality fixing rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Checking Consistency</head><p>We first present a proposition, which is important to design e cient algorithms for checking consistency.</p><p>Proposition 3: For a set ⌃ of fixing rules, ⌃ is consistent, i↵ any two fixing rules 'i and 'j in ⌃ are consistent.</p><p>2 Proof sketch: Let n be the number of rules in ⌃. When n = 1, ⌃ is trivially consistent. When n = 2, ⌃ is consistent is the same as 'i and 'j are consistent (i 6 = j). When n 3, we prove by contradiction. ) Suppose that although the fixing rules are pairwise consistent, when putting together, they are inconsistent. In other words, they may lead to (at least) two di↵erent fixes, i.e., the fixes are not unique. More concretely, there exist (at least) two non-empty sequences of fixes as follows:</p><formula xml:id="formula_9">S 1 : t = t 0 ! (;,' 1 ) t 1 . . . ! (A i 1 ,' i ) t i . . . ! (A m 1 ,'m) tm = t 0 S 2 : t = t 0 0 ! (;,' 0 1 ) t 0 1 . . . ! (A 0 j 1 ,' 0 j ) t 0 j . . . ! (A 0 n 1 ,' 0 n ) t 0 n = t 00</formula><p>We consider the following three cases: (i )</p><formula xml:id="formula_10">Am \ A 0 n = ;; (ii) Am \ A 0 n 6 = ; and t 0 [Am \ A 0 n ] = t 00 [Am \ A 0 n ]; and (iii) Am \ A 0 n 6 = ; and t 0 [Am \ A 0 n ] 6 = t 00 [Am \ A 0 n ], where Am = Am 1 [X' m [{B' m } and A 0 n = A 0 n 1 [X ' 0 n [{B ' 0 n }.</formula><p>For cases (i)(ii), we prove that either S1 or S2 does not reach a fixpoint, i.e., it is not a fix. For case (iii), we show that there must exist a 'i (in sequence S1) and a ' 0 j (in sequence S2) that are inconsistent. Putting all contradicting cases (i,ii,iii) together, it su ces to see that we were wrong to assume that ⌃ is inconsistent. ( Assume there exist inconsistent 'i and 'j. We show that for any tuple t that leads to di↵erent fixes by 'i and 'j, we can construct two fixes S 0 1 and S 0 2 on t by using the rules in ⌃. In S 0 1 , 'i is applied first; while in S 0 2 , 'j is applied first. We prove that these two fixes must yield two di↵erent fixpoints. This su ces to show that we were wrong to assume that there exist inconsistent 'i and 'j.</p><p>2</p><p>Details are omitted due to space constraints. Proposition 3 tells us that to determine whether ⌃ is consistent, it su ces to only check them pairwise. This significantly simplifies the problem and complexity of checking consistency. Next, we describe two algorithms to check the consistency of two fixing rules, by using the result from Proposition 3. One algorithm is based on tuple enumeration, while the other is through rule characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Tuple enumeration</head><p>We first consider that whether there exists a finite set of tuples such that it su ces to only inspect these tuples to determine whether rules 'i and 'j are consistent or not. That is, for the other tuples, neither 'i nor 'j can be applied.</p><p>To design an algorithm for tuple enumeration, let's understand what tuples are necessary to be enumerated, and in which cases tuple enumeration can be avoided.</p><p>Lemma 4: Fixing rules 'i and 'j are consistent, if there does not exist any tuple t that matches both 'i and 'j. 2</p><p>Proof. If 6 9t such that t `'i and t `'j, for any t, there are two cases: either no rule can be applied, or there exists a unique sequence of applying both rules. Either case will not cause di↵erent fixes, i.e., 'i and 'j are consistent.</p><p>Note that Lemma 4 is for "if" but not "i↵", which tells us that only tuples that draw values from evidence pattern and negative patterns can (possibly) match both rules at the same time. Next we illustrate the tuples that are needed to be generated by an example.</p><p>Example 9: Consider rules '1 and '2 in Example 3. We have two constants in the evidence pattern as {China, Canada}, and three constants in the negative patterns as {Shanghai, Hongkong, Toronto}. Hence, we only need to enumerate 2 ⇥ 3 = 6 tuples for relation Travel:</p><p>( , China, Shanghai, , ), ( , China, Hongkong, , ) ( , China, Toronto, , ), ( , Canada, Shanghai, , ) ( , Canada, Hongkong, , ), ( , Canada, Toronto, , ) where ' ' is a special character that is not in any active domain, i.e., it does not match any constant. One can verify that no other tuples can both match '1 and '2.</p><p>2</p><p>Let {A1, . . . , Am} be all attributes appearing in 'i and 'j. Let V' ij (A) denote the set of constant values of A that appear either in evidence pattern or negative patterns of 'i and 'j. The total number of tuples to be enumerated is</p><formula xml:id="formula_11">Q l2[1,m] (|V' ij (A l )|), where Q indicates a product and |V' ij (A l )| denotes the cardinality of V' ij (A l ).</formula><p>Given a set ⌃ of fixing rules, we check them pairwise (see <ref type="bibr">Example 8)</ref>. If any pair of rules is inconsistent, we judge that ⌃ is inconsistent; otherwise, ⌃ is consistent. This algorithm is referred to as isConsist t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Rule characterization</head><p>Now let's shift gears and concentrate on a rather di↵erent kind of analysis, by characterizing fixing rules and avoiding enumerating tuples.</p><p>Also based on Lemma 4, let us focus on the cases of 'i and 'j that there exists some t that can match both fixing rules, i.e., it is possible that applying 'i and 'j on t in di↵erent orders may result in di↵erent fixes. Assume that these rules are represented as follows:</p><formula xml:id="formula_12">'i: ((Xi, tp i [Xi]), (Bi, T p i [Bi])) ! t + p i [Bi] 'j: ((Xj, tp j [Xj]), (Bj, T p j [Bj])) ! t + p j [Bj]</formula><p>Note that a tuple t matching 'i and 'j implies that the following conditions hold:</p><formula xml:id="formula_13">t[Xi] = tp i [Xi] and t[Xj] = tp j [Xj]. Hence, we have tp i [Xi \ Xj] = tp j [Xi \ Xj],</formula><p>where a special case is Xi \ Xj = ;. We consider two cases: Bi = Bj and Bi 6 = Bj. Case 1: Bi = Bj. Let B = Bi = Bj. There is a conflict only when (i) there exists a tuple t that matches both 'i and 'j, and (ii) 'i and 'j will update t to di↵erent values. From (i) we have t </p><formula xml:id="formula_14">[B] 2 T p i [B] and t[B] 2 T p j [B], which gives T p i [B] \ T p j [B] 6 = ;, i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm isConsist r</head><p>Input: a set ⌃ of fixing rules. Output: true (consistent) or false (inconsistent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">for any two distinct</head><formula xml:id="formula_15">' i , ' j 2 ⌃ do 2. if X i \ X j = ; or tp i [X i \ X j ] = tp j [X i \ X j ] do 3. if B i = B j do 4. if T p i [B i ] \ T p j [B i ] 6 = ; and t + p i [B i ] 6 = t + p j [B i ] do 5.</formula><p>return false; 6.</p><p>elseif B i 2 X j and B j 6 2 X i and tp</p><formula xml:id="formula_16">j [B i ] 2 T p i [B i ] 7.</formula><p>return false; 8. elseif B j 2 X i and B i 6 2 X j and tp</p><formula xml:id="formula_17">i [B j ] 2 T p j [B j ] 9.</formula><p>return false; 10. elseif B j 2 X i and B i 2 X j and tp</p><formula xml:id="formula_18">i [B j ] 2 T p j [B j ] and tp j [B i ] 2 T p i [B i ] 11.</formula><p>return false; 12. return true;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Consistency check via rule characterization</head><p>Since ' 0 1 (resp. '2) is only applied to a tuple whose country is China (resp. Canada), there does not exist any tuple that can match both rules at the same time. Therefore, based on Lemma 4, we have ' 0 1 and '2 are consistent. Also, it can be verified that ' 0 1 and '3 are inconsistent. Consider the following: (i) B' 3 2 X ' 0 1 (i.e., country 2 {country, capital}),</p><formula xml:id="formula_19">(ii) tp 1 [B' 3 ] 2 T p 3 [B' 3 ] (i.e., China 2 {China}), (iii) B ' 0 1 2 X' 3 (i.e., capital<label>2</label></formula><p>{capital, city, conf}), and (iv )</p><formula xml:id="formula_20">tp 3 [B ' 0 1 ] 2 T p 1 [B ' 0 1 ] (i.e., Tokyo 2 {Shanghai, Hongkong, Tokyo}).</formula><p>Hence, these two rules will lead to di↵erent fixes, which is captured by case 2(c).</p><p>2 Algorithm. The algorithm to check whether a set of fixing rules is consistent via rule characterization, referred to as isConsist r , is given in Fig. <ref type="figure">4</ref>. It takes ⌃ as input, and returns a boolean value, where true indicates that ⌃ is consistent and false otherwise.</p><p>It enumerates all pairs of distinct rules (lines 1-11). If any pair is inconsistent, it returns false (lines 5,7,9,11); otherwise, it reports that ⌃ is consistent (line 12). It covers all the cases that two rules can be inconsistent, i.e., case 1 (lines 2-5), case 2(a) (lines 6-7), case 2(b) (lines 8-9) and case 2(c) (lines <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref>. Note that in case 2(d), two rules are trivially consistent. Hence, there is no need to investigate such case. Correctness &amp; complexity. From the analysis above, isConsist r covers all the cases that two rules can be inconsistent. Thus, it is proved to be correct based on Proposition 3 and Lemma 4. In terms of complexity, we can use a hash table to check that whether a value matches some negative pattern in constant time. Since isConsist r enumerates all pairs of rules, its time complexity is O(size(⌃) 2 ), where size(⌃) is the size of ⌃.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Resolving Inconsistent Rules</head><p>When fixing rules are inconsistent, it may lead to conflicting repairing results. In this section, we discuss how to resolve inconsistent fixing rules.</p><p>Consider two inconsistent rules, ' 0 1 and '3, in Example 10. Fig. <ref type="figure" target="#fig_1">5</ref> highlighted the values that result in a conflict. A conservative algorithm is to remove all the rules that are in conflicts. This process ensures termination since the number of rules is strictly decreasing, until the set of rules is consis- tent or becomes empty. Although the bright side is that the remaining rules are consistent, the problem is that this will also remove some useful rules (e.g., '3). It is di cult for automatic algorithms to solve such semantic problem well. Hence, in order to obtain high quality rules, we ask experts to examine rules that are in conflicts. For example, the experts can naturally remove Tokyo from the negative patterns of ' 0 1 , since one cannot judge, given (China, Tokyo), which attribute is wrong. This will result in a modified rule '1 (see Example 3), which is consistent with '3. Note that in order to ensure termination, we only allow the experts to remove some negative patterns (e.g., from ' 0 1 to '1), or remove some fixing rules, without adding values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REPAIRING WITH FIXING RULES</head><p>After completing our study of finding a set of consistent fixing rules, the next most important item on nearly everybody's wish list is how to use these rules to repair data.</p><p>In the following, we first present a chase-based algorithm to repair one tuple (Section 6.1), with time complexity in O(size(⌃)|R|). We also present a fast algorithm (Section 6.2) running in O(size(⌃)) time for repairing one tuple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Chase-based Algorithm</head><p>When a given set ⌃ of fixing rules is consistent, for any t, applying ⌃ to t will get a unique fix (see Section 3.2), which is also known as the Church-Rosser property <ref type="bibr" target="#b1">[1]</ref>. We next present an algorithm to repair a tuple with consistent fixing rules. It iteratively picks a fixing rule that can be properly applied, until a fix is reached. Algorithm. The algorithm, referred to as cRepair, is shown in Fig. <ref type="figure">6</ref>. It takes as input a tuple t and a set ⌃ of consistent fixing rules. It returns a repaired tuple t 0 w.r.t. ⌃.</p><p>The algorithm first initializes a set of assured attributes, a set of fixing rules that can be possibly applied, a tuple to be repaired, and a flag to indicate whether the tuple has been changed (line 1). It then iteratively examines and applies the rules to the tuple (lines 2-7). If there is a rule that can be properly applied (line 5), it updates the tuple (line 6), maintains the assured attributes and rules that can be used correspondingly, and flags this change (line 7). It terminates when no rule can be further properly applied (line 2), and the repaired tuple will be returned (line 8).</p><p>Correctness &amp; complexity. The correctness of cRepair is inherently ensured by the Church-Rosser property, since⌃ is consistent. For the complexity, observe the following. The outer loop (lines 2-7) iterates at most |R| times. For each loop, it needs to scan each unused rule, and checks whether it can be properly applied to the tuple. From these it follows that Algorithm 6 runs in O(size(⌃)|R|) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A Fast Repairing Algorithm</head><p>Our next goal is to study how to improve the chase-based procedure. One natural way is to consider how to avoid pkeyp plistp  Rule '4 states that: for t in relation Travel, if the conf is ICDE, held at some country whose capital is Beijing, but the city is Hongkong, its city should be Shanghai. This holds since ICDE was held in China only once at 2009, in Shanghai but never in Hongkong.</p><formula xml:id="formula_21">country, China ! '1 X country, Canada ! '2 X conf, ICDE ! '3, '4 X capital, Tokyo ! '3 X city, Tokyo ! '3 X capital, Beijing ! '4 X (a) Inverted listsX r 1 : = {' 1 }, c(' 1 ) = 1 itr1: r 0 1 = r 1 , = ; r 2 : ={' 1 }, c(' 1 , ' 3 , ' 4 )=1; itr1: r 0 2 [capital]= Beijing c(' 3 )=1, c(' 4 )=2, = {' 4 }; itr2: r 0 2 [city]= Shanghai , = ;</formula><p>Given the four fixing rules '1-'4, the corresponding inverted lists are given in Fig. <ref type="figure" target="#fig_4">8(a)</ref>. For instance, the third key (conf, ICDE) links to rules '3 and '4, since conf 2 X' 3 (i.e., {capital, city, conf}) and tp 3 [conf] = ICDE; and moreover, conf 2 X' 4 (i.e., {capital, conf}) and tp 4 [conf] = ICDE. The other inverted lists are built similarly. Now we show how the algorithm works over tuples r1 to r4, which is also depicted in Fig. <ref type="figure" target="#fig_4">8</ref>. Here, we highlight these tuples in two colors, where the green color means that the tuple is clean (i.e., r1), while the red color represents the tuples containing errors (i.e., r2, r3 and r4).</p><p>r1: It initializes (lines 1-7) and finds that '1 may be applied, maintained in . In the first iteration (lines 8-16), it finds that '1 cannot be applied, since r1[capital] is Beijing, which is not in the negative patterns {Shanghai, Hongkong} of '1. Also, no other rules can be applied. It terminates with tuple r1 unchanged. Actually, r1 is a clean tuple.</p><p>r2: It initializes and finds that '1 might be applied. In the first iteration (lines 8-16), rule '1 is applied to r2 and updates r2[capital] to Beijing. Consequently, it uses inverted lists (line 13) to increase the counter of '4 (line 14) and finds that '4 might be used (line 15). In the second iteration, rule '1 is applied and updates r2[city] to Shanghai. It then terminates since no other rules can be applied.</p><p>r3: It initializes and finds that '3 might be applied. In the first iteration, rule '3 is applied and updates r3[coutry] to Japan. It then terminates, since no more applicable rules.</p><p>r4: It initializes and finds that '2 might be applied. In the first iteration, rule '2 is applied and updates r4[capital] to Ottawa. It will then terminate. At this point, we see that all the fours errors shown in Fig. <ref type="figure" target="#fig_0">1</ref> have been corrected, as highlighted in Fig. <ref type="figure" target="#fig_4">8</ref>. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTAL STUDY</head><p>We conducted experiments with both real-life and synthetic data to examine our algorithms and help us discover the deficiency of fixing rules and algorithms to be improved. Specifically, we evaluated (1) the e ciency of consistency checking for fixing rules; (2) the accuracy of our data re-pairing algorithms with fixing rules; and (3) the e ciency of data repairing algorithms using fixing rules.</p><p>It is worth noting that the purpose of these experiments is to test, when given high quality fixing rules, how they can be used to automatically repair data with high dependability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setting</head><p>Experimental data. We used real-life and synthetic data.</p><p>(1) hosp was taken from us Department of Health &amp; Human Services<ref type="foot" target="#foot_0">1</ref> . It has 115K records with the following attributes: Provider Number (PN), Hospital Name (HN), address1, address2, address3, city, state, zip, county, Pho-neNumber (phn), HospitalType (ht), HospitalOwner (ho), EmergencyService (es) Measure Code (MC), Measure Name (MN), condition, and stateAvg.</p><p>(2) uis data was generated by a modified version of the uis Database generator<ref type="foot" target="#foot_1">2</ref> . It produces a mailing list that has the following schema: RecordID, ssn, FirstName (fname), MiddleInit (minit), LastName (lname), stnum, stadd, apt, city, state, zip. We generated 15K records. Dirty data generation. We treated clean datasets as the ground truth. Dirty data was generated by adding noise only to the attributes that are related to some integrity constraints, which is controlled by noise rate (10% by default). We introduced two types of noises: typos and errors from the active domain. Fixing rules generation. We next discuss how fixing rules can be obtained. Seed fixing rule generation. Since each fixing rule is defined on semantically related attributes, we started with known dependencies (e.g., FDs for our testing). We first detected violations of given FDs, and presented them to experts. The experts produced several fixing rules as seeds (or samples), based on their understanding of these violations. Rule enrichment. Given seed fixing rules, we enriched them by only enlarging their negative patterns, via extracting new negative patterns from related tables in the same domain. For instance, consider Example 2. If users provide a fixing rule that takes China as the evidence pattern, and some Chinese cities (e.g., Shanghai, Hongkong) other than Beijing as negative patterns, one can enlarge its negative patterns by extracting cities from a table about Chinese cities. Note that when an appropriate ontology is available, we can extract the above information as evidence patterns, negative patterns and facts. In such case, the generated fixing rules are usually general. Consequently, they can be applied to multiple databases.</p><p>In the experiment, we generated 1000 fixing rules for hosp data 100 fixing rules for uis data. Measuring quality. To assess the accuracy of data cleaning algorithms, we use precision and recall, where precision is the ratio of corrected attribute values to the number of all the attributes that are updated, and recall is the ratio of corrected attribute values to the number of all erroneous attribute values.</p><p>Remark. We mainly compare with the state-of-the-art automated data cleaning techniques. Note that they are designed for a slightly di↵erent target: computing a consistent database. We consider it a relatively fair comparison, since all fixing rules we generated are from FD violations. In other words, the fixing rules and the FDs used are defined on exactly the same set of attributes. We employed the following FDs for hosp and uis data, respectively. Algorithms. We have implemented the following algorithms in C++: (1) isConsist t : the algorithm for checking consistency based on tuple enumeration (Section 5.2); (2) isConsist r : the algorithm for checking consistency based on rule characterization (Fig. <ref type="figure">4</ref> in Section 5.2); (3) cRepair: the basic chase-based algorithm for repairing with fixing rules (see Fig. <ref type="figure">6</ref>); and (4) lRepair: the fast repairing algorithm (see Fig. <ref type="figure">7</ref>). Moreover, for comparison, we have implemented two algorithms for FD repairing, a cost-based heuristic method <ref type="bibr" target="#b7">[7]</ref>, referred to as Heu, and a approach for cardinality set minimal <ref type="bibr" target="#b5">[5]</ref>, referred to as Csm. Both approaches were implemented in Java.</p><p>All experiments were conducted on a Windows machine with a 3.0GHz Intel CPU and 4GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experimental Results</head><p>We next report our findings from our experimental study. Exp-1: E ciency of checking consistency. We evaluated the e ciency of checking consistency by varying the number of rules. The results for hosp (resp. uis) are shown in Fig. <ref type="figure" target="#fig_7">9</ref>(a) (resp. Fig. <ref type="figure" target="#fig_7">9(b)</ref>). The x-axis is the number of rules multiplied by 100 (resp. 10) for hosp (resp. uis), and the y-axis is the running time in millisecond (msec).</p><p>For either isConsist t or isConsist r , we plotted its worst case, i.e., checking all pairs of rules, as well as its 10 real cases where it terminated when some pair was detected to be inconsistent. For example, in Fig. <ref type="figure" target="#fig_7">9</ref>(a), the big circle for x = 2 was for checking 200 rules in the worst case, while the 10 small circles below it were for real cases. In Fig. <ref type="figure" target="#fig_7">9</ref>(b), real cases are the same as the worst case, since the 100 rules are consistent and all pairs of distinct rules have to be checked.</p><p>These figures show that to check consistency of fixing rules, the algorithm with tuple enumeration (isConsist t ) is slower, as expected. The reason is that enumerating tuples for two rules is more costly than characterizing two rules.</p><p>In addition, this set of experiment validated that the consistency of fixing rules can be checked e ciently. For example, it only needs 12 seconds to check the consistency of 1000*1000 pairs of rules, i.e., the top right point in Fig. <ref type="figure" target="#fig_7">9(a)</ref>.</p><p>The result of this study indicates that it is feasible to check consistency for a reasonably large set of fixing rules. Exp-2: Accuracy. In this set of experiments, we will study the following.   Recall that noise was obtained by either introducing typos to an attribute value or changing an attribute value to another one from the active domain of that specific attribute. For example, an error for Ottawa could be Ottawo (i.e., a typo) or Beijing (i.e., a value from active domain). Precision. We fixed the noise rate at 10%, and varied the percentage of typos from 0% to 100% by a step of 10% (xaxis in both charts from Figs. <ref type="figure" target="#fig_0">10(a</ref>) and 10(e) for hosp and uis, respectively). Both figures showed that our method using fixing rules performed dependable fixes (i.e., high precision), and was not sensitive to types of errors. While for the existing algorithms Heu and Csm, they had lower precision when more errors were from the active domain. The reason is that for such errors, heuristic methods would erroneously connect some tuples as related to violations, which might link previously irrelevant tuples and complicate the process when fixing the data. Indeed, however, both Heu and Csm computed a consistent database, as targeted.</p><p>Note that fixing rules also made mistakes, e.g., the precision in Fig. <ref type="figure" target="#fig_0">10</ref>(a) is not 100%, which means some changes were not correct. The reason is that, when more errors are from the active domain (e.g., typo rate is 0 in Fig. <ref type="figure" target="#fig_0">10(a)</ref>), it will mislead fixing rules to make decisions. For example, consider the two rules in Fig. <ref type="figure" target="#fig_3">3</ref>, if the correct (country, capital) values of some tuple are (China, Shanghai) but were changed by using values from the active domain to (Canada, Toronto), using fixing rules will make mistakes. Although this is not very common in practice, it deserves a further study to improve our algorithms in the future. Recall. In order to better understand the behavior of these algorithms, Figs. 10(b) and 10(f) show the recall corresponding to Figs. 10(a) and 10(e), respectively. Not surprisingly, our algorithm did not outperform existing approaches in terms of recall. This is because heuristic approaches would repair some potentially erroneous values, but at the tradeo↵ of decreasing precision. Although our method was relatively low in recall, we did our best to ensure the precision, instead of repairing as more errors as possible. Hence, when recall is a major requirement for some system, existing heuristic methods can be used after fixing rules being applied, to compute a consistent database. Fig. <ref type="figure" target="#fig_0">10</ref>(f) shows that the recall is very low (below 8%) for all methods. The reason is that, the uis dataset generated has few repeated patterns w.r.t. each FD. When noise was introduced, many errors cannot be detected, hence no method can repair them. Note, however, that recall can be improved by learning more rules as shown below. (b) Varying the number of fixing rules. We studied the accuracy of our repairing algorithms by varying the number of fixing rules. We fixed noise rate at 10% and half of them are typos. For hosp, we varied the number of rules from 100 to 1000, and reported the recall and precision in Fig. <ref type="figure" target="#fig_0">10(c</ref>) and Fig. <ref type="figure" target="#fig_0">10(d)</ref>, respectively. For uis, we varied the number of rules from 10 to 100, and reported the results in Fig. <ref type="figure" target="#fig_0">10</ref>(g) and Fig. <ref type="figure" target="#fig_0">10</ref>(h), respectively. For Heu and Csm, as the typo rate was fixed, their precision and recall values were horizontal lines.</p><p>The experimental results indicate that when more fixing rules are available, our approach can achieve better recall, while keeping a good precision, as expected. (c) Evaluation for negative patterns. To further investigate fixing rules, we sorted the fixing rules of hosp based on the number of negative patterns, and plotted every 30 points in Fig. <ref type="figure" target="#fig_8">11(a)</ref>. We see that most of the fixing rules have a small number of negative patterns. For instance, around 80% of fixing rules contain two negative patterns. We added up all negative patterns, and evaluated the accuracy of our repairing algorithms by varying the number of negative patterns for all rules in total. Figure <ref type="figure" target="#fig_8">11(b)</ref> shows the precision and recall of our approach. We can see that adding more negative patterns can lead to a better recall while keeping a high precision. This experimental result further validates the dependable feature of fixing rules. (d) Comparison with editing rules. We also compared our approach with editing rules <ref type="bibr" target="#b19">[19]</ref>. Although editing rules can repair data that is guaranteed to be correct, they are measured by the number of user interactions per tuple. That is, for each tuple and for each editing rule to be applied, the users have to be asked. To this purpose, we evaluated the number of errors that can be corrected by every fixing rule (see Fig. <ref type="figure" target="#fig_9">12(a)</ref>) using hosp data with 100 rules and 10% dirty rate, where the x-axis is for fixing rules and the y-axis is the number of errors they can correct. This experiment shows that a single fixing rule was able to repair errors in more than fifty tuples, but if we employ editing rules to repair these errors, the approach has to interact with users over fifty times.</p><p>Moreover, we encoded data values from master data into editing rules, to make it an automated rule. Note that error information is not in master data, e.g., the negative patterns in fixing rules, which cannot be encoded. Hence, we removed negative patterns in fixing rules, to simulate editing rules. Specifically, each time when seeing an evidence pattern, it simulated users by saying yes, and then updated the right hand side value to the fact. The experimental results are shown in Fig. <ref type="figure" target="#fig_9">12(b)</ref>, where Fix (resp. Edit) indicates fixing rules (resp. editing rules). The reason that fixing rules have better precision and recall is that, if we have errors in the right hand side of such rules, (automated) editing rules can correct them. However, if there are errors in the left hand side, they will introduce new errors by treating these errors as correct values, resulting in lower precision and in consequence, lower recall. Exp-3: E ciency of repairing algorithms. In this last set of experiments, we study the e ciency of our data repairing with fixing rules. As they are linear in data size, we evaluated their e ciency by varying the number of rules.</p><p>The results for hosp and uis are given in Fig. <ref type="figure" target="#fig_3">13</ref>(a) and Fig. <ref type="figure" target="#fig_3">13</ref>(b), respectively. In both figures, the x-axis is for the number of rules and the y-axis is for running time. These two figures show that algorithm lRepair is more e cient. For example, it ran in less than 2 seconds to repair 115K tuples, using 1000 rules (the bottom right node in Fig. <ref type="figure" target="#fig_3">13(a)</ref>). In Fig. <ref type="figure" target="#fig_3">13</ref>(b), cRepair was faster only when the number of rules was very small (i.e., 10), where the reason is the extra overhead of using inverted lists and hash counters. However, in general, lRepair was much faster, since it only examined the rules that can be used instead of checking all rules.</p><p>We also compared our fast repairing algorithm lRepair with Heu and Csm. Using both hosp and uis data, the results are given in the table below. It shows that lRepair runs much faster than the others. The reasons are twofold: (1) lRepair detects errors on each tuple individually, while the others need to consider a combination of two tuples for violation detection; and (2) lRepair repairs each tuple in linear time, while Heu and Csm repair data by holistically considering all violations, which have much higher time complexity. Summary. We found the following from the above experiments. (a) It is e cient to detect whether a set of fixing rules is consistent (Exp-1). (b) Data repairing using fixing rules is dependable, i.e., they repair data errors with high precision (Exp-2). (c) The recall of using fixing rules can be improved when more fixing rules are available (Exp-2). (d) It is e cient to repair data via fixing rules, which reveals its potential to be used for large datasets (Exp-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>We have proposed a novel class of data cleaning rules, namely, fixing rules, that (1) compared with data dependencies used in data cleaning, are able to find dependable fixes for input tuples, without using heuristic solutions; and (2) di↵er from editing rules, that are able to repair data automatically without any user involvement. We have identified fundamental problems for deciding whether a set of fixing rules is consistent or redundant, and established their complexity bounds. We have proposed e cient algorithms for checking consistency, and discussed strategies to resolve inconsistent fixing rules. We have also presented data repairing algorithms by capitalizing on fixing rules. Our experimental results with real-life and synthetic data have verified the e↵ectiveness and e ciency of the proposed rules and the presented algorithms. These yield a promising method for automated and dependable data repairing.</p><p>The study of automated and dependable data repairing is still in its infancy. This research is just a first attempt to tackle this problem, and it has thrown up many questions in need of further investigation. (1) Rule discovery. Our techniques in the paper allow users to define fixing rules manually, or generate rules using examples. We are planning to design algorithm to automatically discover fixing rules.</p><p>(2) Interaction with other data quality rules. A challenging topic is to explore the interaction between fixing rules and other data quality and rules, such as CFDs, MDs, editing rules, and the users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Database D: an instance of schema Travel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 5 :</head><label>5</label><figDesc>Consider the fixing rule '1 in Example 3 and the tuple r2 in Fig. 1. Initially, Ar 2 = ;. The rule '1 can be properly applied to r2 w.r.t. Ar 2 , since r2[country] = China and r2[capital] = Shanghai 2 {Shanghai, Hongkong} (i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>' 3 :Figure 5 :</head><label>35</label><figDesc>Figure 5: Illustrations in resolving conflicts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>r 3 :</head><label>3</label><figDesc>={' 3 }, c(' 3 )=3; itr1: r 0 3 [country]= Japan , = ; r 4 : ={' 2 }, c(' 2 )=1; itr1: r 0 4 [capital]= Ottawa , = ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A running example ! Shanghai</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) The e↵ect of di↵erent data errors (i.e., typos or errors from active domain) for repairing algorithms. (b) The influence of fixing rules w.r.t. their sizes. (c) E↵ect of negative patterns. (d) Comparison with editing rules. We use Fix to represent repairing algorithms with fixing rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>worst case) isConsist r (worst case) (b) uis (varying |⌃|)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: E ciency for checking consistency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Evaluation for negative patterns (hosp)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Comparison with editing rules (hosp)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>uis (varying |⌃|) Figure13: E ciency for data repairing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For instance, they can change r2[capital] from Shanghai to Beijing, and r3[capital] from Tokyo to Beijing, which requires two changes. One may verify that this is a repair with the minimum cost of two updates. Though these changes correct the error in r2[capital], they do not rectify r3[country]. Worse still, they mess up the correct value in r3[capital].</figDesc><table><row><cell>country s 1 : China Beijing capital s 2 : Canada Ottawa Tokyo s 3 : Japan</cell><cell>country ' 1 : China</cell><cell>{capital } capital + Shanghai Beijing Hongkong</cell><cell>country ' 2 : Canada</cell><cell>{capital } capital + Toronto Ottawa</cell></row><row><cell>Figure 2: Data Dm of schema Cap</cell><cell></cell><cell cols="2">Figure 3: Example fixing rules</cell><cell></cell></row><row><cell cols="2">many algorithms have been presented [2, 5, 7, 10, 11, 19, 20].</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>1</p>with the minimum cost (e.g., the number of changes),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. Rule '1 detects that r2[capital] is wrong, since r2[country] is China, but r2[capital] is Shanghai. It will then update r2[capital] to Beijing. Similarly, the second fixing rule '2 states that for a tuple t, if its country is Canada, but its capital is Toronto, then its capital is wrong and should be Ottawa. It detects that r4[capital] is wrong, and then will correct it to Ottawa. After applying '1 and '2, two errors, r2[capital] and r4[capital], can be repaired. The other two errors, r2[city]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. A fixing rule ' defined on a schema R is formalized as ((X, tp[X]), (B, T p [B])) ! t + = tp[X] and (ii) t[B] 2 T p [B]. In other words, tuple t matches rule ' indicates that ' can identify errors in t. X consists of country and B is capital. Here, '1 states that, if the country of a tuple is China and its capital value is in {Shanghai, Hongkong}, its capital value is wrong and should be updated to Beijing. Similarly for '2. Editing rules need users to trigger the action of repairing. That is, when matching some values from dirty data to values in master data, editing rules by themselves cannot tell if the values used for matching are correct, without which the repairing operation cannot be executed. (b) Fixing rules encode evidence pattern and negative patterns to decide the correct and erroneous values, which then automatically triggers the repair operation. Please see Example 2 for more details.(3) We have also investigated on how to get fixing rules. Inspired by the work of<ref type="bibr" target="#b27">[27]</ref> that learns transformation rules from examples, we discuss in Section 7 how to generate fixing rules from examples. Notations. For convenience, we introduce some notations. Given fixing rule ' : ((X, tp[X]), (B, T p [B])) ! t + p [B], we denote by X' the set X of attributes in '. Similarly, we write tp[X'], B', T p [B'] and t +</figDesc><table><row><cell>they di↵er in the way of repairing errors. (a)</cell><cell></cell></row><row><cell></cell><cell>p [B] where</cell></row><row><cell cols="2">1. X is a set of attributes in attr(R), and B is an attribute</cell></row><row><cell cols="2">in attr(R) \ X (i.e., B is not in X); 2. tp[X] is a pattern with attributes in X, referred to as</cell></row><row><cell cols="2">the evidence pattern on X, and for each A 2 X, tp[A] is a constant value in dom(A);</cell></row><row><cell cols="2">3. T p [B] is a finite set of constants in dom(B), referred</cell></row><row><cell cols="2">to as the negative patterns of B; and</cell></row><row><cell cols="2">4. t + p [B] is a constant value in dom(B) \ T p [B], referred to as the fact of B.</cell></row><row><cell cols="2">Intuitively, the evidence pattern tp[X] of X, together with</cell></row><row><cell cols="2">the negative patterns T p [B] impose the condition to deter-mine whether a tuple contains an error on B. The fact t + p [B]</cell></row><row><cell cols="2">in turn indicates how to correct this error.</cell></row><row><cell cols="2">Note that condition (4) enforces that the correct value</cell></row><row><cell cols="2">(i.e., the fact) is di↵erent from known wrong values (i.e.,</cell></row><row><cell cols="2">negative patterns) relative to a specific evidence pattern.</cell></row><row><cell cols="2">We say that a tuple t of R matches a rule ' :</cell></row><row><cell cols="2">((X, tp[X]), (B, T p [B])) ! t + p [B], denoted by t `', if (i)</cell></row><row><cell cols="2">t[X] Example 3: Consider the fixing rules in Fig. 3. They can</cell></row><row><cell cols="2">be formally expressed as follows:</cell></row><row><cell cols="2">'1: (([country], [China]), (capital, {Shanghai, Hongkong})) ! Beijing '2: (([country], [Canada]), (capital, {Toronto})) ! Ottawa</cell></row><row><cell>In both '1 and '2, Consider D in Fig. 1.</cell><cell>Tuple r1 does not match</cell></row><row><cell cols="2">rule '1, since r1[country] = China but r1[capital] 6 2 As another example, tuple {Shanghai, Hongkong}. r2 matches rule '1, since r2[country] = China, and</cell></row><row><cell cols="2">r2[capital] 2{Shanghai, Hongkong}. Similarly, we have r4 matches '2. 2</cell></row><row><cell cols="2">Semantics. We next give the semantics of fixing rules.</cell></row><row><cell cols="2">We say that a fixing rule ' is applied to a tuple t, denoted</cell></row><row><cell cols="2">by t !' t 0 , if (i) t matches ' (i.e., t `'), and (ii) t 0 is obtained by the update t[B] := t + p [B].</cell></row><row><cell cols="2">That is, if t[X] agrees with tp[X], and t[B] appears in the</cell></row><row><cell cols="2">set T p [B], then we assign t + p [B] to t[B]. Intuitively, if t[X]</cell></row><row><cell cols="2">matches tp[X] and t[B] matches some value in T p [B], it is</cell></row><row><cell cols="2">evident to judge that t[B] is wrong and we can use the fact</cell></row><row><cell cols="2">t + p [B] to update t[B]. This yields an updated tuple t 0 with t 0 [B] = t + p [B] and t 0 [R \ {B}] = t[R \ {B}]. Example 4: As shown in Example 2, we can correct r2 by</cell></row><row><cell cols="2">applying rule '1. As a result, r2[capital] is changed from</cell></row><row><cell cols="2">Shanghai to Beijing, i.e., r2 !' 1 r 0 2 where r 0 2 [capital] = Beijing and the other attributes of r 0 2 remain unchanged. Similarly, we have r4!' 2 r 0 4 where the only updated at-tribute value is r 0 4 [capital] = Ottawa. 2</cell></row><row><cell cols="2">Remark. (1) Fixing rules are di↵erent from traditional data</cell></row><row><cell cols="2">dependencies e.g., FDs [1] and CFDs [15]. Data dependencies</cell></row><row><cell cols="2">only detect violations. In contrast, a fixing rule ' specifies</cell></row><row><cell cols="2">an action: applying ' to a tuple t yields an updated t 0 .</cell></row><row><cell cols="2">(2) Editing rules [19] also have dynamic semantics. However,</cell></row></table><note><p>p [B'], relative to '.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>they can be applied at the same time. From (ii ) we have t + p Xj and Bj 6 2 Xi, (b) Bi 6 2 Xj and Bj 2 Xi, (c) Bi 2 Xj and Bj 2 Xi, and (d) Bi 6 2 Xj and Bj 6 2 Xi. Xj and Bj 6 2 Xi. If a tuple t matches 'i and 'j, then (i) t[Bi] 2 T p i [Bi] (to match 'i), and (ii) t[Bi] = tp j [Bi] (to match 'j). Observe the following: if 'j is applied to t first, since Bi 2 Xj, it will keep t[Bi] unchanged, whereas if 'i is applied first, it will update t[Bi] to a di↵erent value (i.e., t + p i [Bi]). This will cause di↵erent fixes. Hence, 'i and 'j are inconsistent only when tp j [Bi] 2 T p i [Bi] (by merging (i) and (ii)). (b) Bi 6 2 Xj and Bj 2 Xi. This is symmetric to case (a). Therefore, 'i and 'j are inconsistent only when tp i [Bj] 2 T p j [Bj]. (c) Bi 2 Xj and Bj 2 Xi. This is the combination of cases (a) and (b). Thus, 'i and 'j are inconsistent only when tp i [Bj] 2 T p j [Bj] and tp j [Bi] 2 T p i [Bi]. (d) Bi 6 2 Xj and Bj 6 2 Xi.</figDesc><table><row><cell>For any tuple t that matches</cell></row><row><cell>both 'i and 'j, rule 'i (resp. 'j) will determin-</cell></row><row><cell>istically update t[Bi] (resp. t[Bj]) to t + p i [Bi] (resp. t + p</cell></row></table><note><p>i [B] 6 = t + p j [B], i.e., they lead to di↵erent fixes. From (i) and (ii), the extra condition that 'i and 'j are inconsistent under such case is (T p i [B]\T p j [B] 6 = ; and t + p i [B] 6 = t + p j [B]). Case 2: Bi 6 = Bj. Again, we consider four cases: (a) Bi 2 (a) Bi 2 j [Bj]). That is, 'i and 'j are always consistent in this case. Example 10: Consider ' 0 1 and '3 in Example 8 and '2 in Example 3.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>FDs for uis ssn ! fname, minit, lname, stnum, stadd, apt, city, state, zip fname,minit,lname ! ssn, stnum, stadd, apt, city, state, zip zip ! state, city</figDesc><table><row><cell>FDs for hosp</cell></row><row><cell>PN ! HN, address1, address2, address3, city, state, zip, county, phn, ht, ho, es</cell></row><row><cell>phn ! zip, city, state, address1, address2, address3 MC ! MN, condition PN, MC ! stateAvg state, MC ! stateAvg</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.hospitalcompare.hhs.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.cs.utexas.edu/users/ml/riddle/data.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm cRepair</head><p>Input: a tuple t, a set ⌃ of consistent fixing rules. Output: a repaired tuple t 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A := ;;</head><p>:=⌃ ;t 0 := t; updated := true; 2. while updated do 3. updated := false; 4.</p><p>for each ' 2 do 5.</p><p>if t 0 matches ' and B' 6 2A then 6.</p><p>t 0 [B'] := t + p [B'] (by applying '); 7.</p><p>A := A[X' [{B'}; := \{'}; updated := true; 8. return t 0 ; Figure <ref type="figure">6</ref>: Chase-based repairing algorithm repeatedly checking whether a rule is applicable, after each update of the tuple being examined.</p><p>Note that a key property of employing fixing rules is that, for each tuple, each rule can be applied only once. After a rule is applied, in consequence, it will mark the attributes associated with this rule as assured, and does not allow these attributes to be changed any more (see Section 3.2).</p><p>Hence, two important steps are, after each value update, to (i) e ciently identify the rules that cannot be applied, and (ii) determine unused rules that can be possibly applied.</p><p>We employ two types of indices in order to perform the above two targets. Inverted lists are used to achieve (i ), and hash counters are employed for (ii ).</p><p>Before describing how to use these indices to design a fast algorithm, we shall pause and define these indices, which is important to understand the algorithm. Inverted lists. Each inverted list is a mapping from a key to a set ⌥ of fixing rules. Each key is a pair (A, a) where A is an attribute and a is a constant value. Each fixing rule ' in the set ⌥ satisfies A 2 X' and tp[A] = a.</p><p>For example, an inverted list w.r.t. '1 in Example 3 is as:</p><p>Intuitively, when the country of some tuple is China, this inverted list will help to identify that '1 might be applicable. Hash counters. It uses a hash map to maintain a counter for each rule. More concretely, for each rule ', the counter c(') is a nonnegative integer, denoting the number of attributes that a tuple agrees with tp[X'].</p><p>For example, consider '1 in Example 3 and r2 in Fig. <ref type="figure">1</ref>. We have c('1) = 1 w.r.t. tuple r2, since both r2[country] and tp 1 [country] are China. As another example, consider r4 in Fig. <ref type="figure">1,</ref><ref type="figure">we have c</ref></p><p>We are now ready to present a fast algorithm by using the two indices introduced above. Note that inverted lists are built only once for a given ⌃, and keep unchanged for all tuples. The hash counters will be initialized to zero for the process of repairing each new tuple. Algorithm. The algorithm lRepair is given in Fig. <ref type="figure">7</ref>. It takes as input a tuple t, a set ⌃ of consistent fixing rules, and inverted lists I. It returns a repaired tuple t 0 w.r.t. ⌃.</p><p>It first initializes a set of assured attributes, a set of fixing rules to be used, and a tuple to be repaired (line 1). It also clears the counters for all rules (line 2). It then uses inverted lists to initialize the counters (lines 3-5). After the counters are initialized, it checks and maintains a list of rules that might be used (lines 6-7), and uses a chase process to  During the process (lines 8-16), it first randomly picks a rule that might be used (line 9). The rule will be applied if it is verified to be applicable (lines <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref>. The set of attributes that is assured correct is increased correspondingly (line 12). The counters will be recalculated (lines <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref>. Moreover, if new rules might be used due to this update, it will be identified (line 15). The rule that has been checked will be removed (line 16), no matter it is applicable or not.</p><p>Observe the following two cases. (i ) If a rule is removed after being applied at line 16 (i.e., line 10 gives a true), it cannot be used again and will not be checked at lines <ref type="bibr">13-15.</ref> (ii) If a rule ' is removed without being applied at line 16 (i.e., line 10 gives a false), it cannot be used either at lines 13-15. The reason is that: for any rule ', if ' cannot be properly applied to t 0 , any update on attribute B' will mark it as assured, such that ' cannot be properly applied afterwards. From the above (i) and (ii), it follows that it is safe to remove a rule from , after it has been checked, once and for all. Correctness. Note that ⌃ is consistent, we only need to prove the repaired tuple t 0 is a fix of t. This can be proved based on (1) at any point, includes all fixing rules that might match the given tuple; and (2) each fixing rule is added into at most once. Hence, the algorithm terminates until it reaches a fixpoint when is empty. Complexity. It is clear that the three loops (line 2, lines 3-5 and lines 6-7) all run in time linear to size(⌃). Next let us consider the while loop (lines 8-16). Observe that each rule ' will be checked in the inner loop (lines 13-15) up to |X'| times, by using the inverted lists and hash counters, independent of the number of outer loop iterated. The other lines of this while loop can be done in constant time. Putting together, the total time complexity of the algorithm is O(size(⌃)). We next show by example how Algorithm lRepair works. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Foundations of Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vianu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Consistent query answers in inconsistent databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chomicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Data Quality: Concepts, Methodologies and Techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Batini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scannapieco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data cleaning and query answering with matching dependencies and matching functions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampling the repairs of functional dependency violations under hard constraints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beskales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling and querying possible repairs in duplicate detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beskales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Soliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A cost-based model and e↵ective heuristic for repairing constraints by value modification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bohannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extending dependencies with conditions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimal-change integrity maintenance using tuple deletions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chomicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcinkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Comput</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Holistic data cleaning: Put violations into context</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ilyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving data quality: Consistency and accuracy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NADEEF: a commodity data cleaning system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dallachiesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ebaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eldawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">NADEEF: A generalized data cleaning system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ebaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Quiané-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>PVLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependencies revisited for improving data quality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional functional dependencies for capturing data inconsistencies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kementsietsidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inferring data currency and consistency for conflict resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning about record matching rules</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interaction between record matching and data repairing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards certain fixes with editing rules and master data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic approach to automatic edit and imputation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fellegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Statistical Association</title>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Scheuren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<title level="m">Data Quality and Record Linkage Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On approximating optimum repairs for functional dependency violations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kolahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ERACER: a database approach for statistical inference and data cleaning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data fusion in three Resolving schema, tuple, and value inconsistencies</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Bilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computational Complexity</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Potter&apos;s Wheel: An interactive data cleaning system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning semantic string transformations from examples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Yakout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guided data repair</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
