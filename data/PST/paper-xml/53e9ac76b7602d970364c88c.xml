<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimum spanning tree based split-and-merge: A hierarchical clustering method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-04-13">13 April 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caiming</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Eastern Finland</orgName>
								<address>
									<postBox>P.O. Box: 111</postBox>
									<postCode>FIN-80101</postCode>
									<settlement>Joensuu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">College of Science and Technology</orgName>
								<orgName type="institution">Ningbo University</orgName>
								<address>
									<postCode>315211</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Duoqian</forename><surname>Miao</surname></persName>
							<email>miaoduoqian@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pasi</forename><surname>Fr√§nti</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Eastern Finland</orgName>
								<address>
									<postBox>P.O. Box: 111</postBox>
									<postCode>FIN-80101</postCode>
									<settlement>Joensuu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Minimum spanning tree based split-and-merge: A hierarchical clustering method</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-04-13">13 April 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">0B106812F4C4CF47E2DC753D85B5F709</idno>
					<idno type="DOI">10.1016/j.ins.2011.04.013</idno>
					<note type="submission">Received 6 December 2009 Received in revised form 4 April 2011 Accepted 6 April 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most clustering algorithms become ineffective when provided with unsuitable parameters or applied to datasets which are composed of clusters with diverse shapes, sizes, and densities. To alleviate these deficiencies, we propose a novel split-and-merge hierarchical clustering method in which a minimum spanning tree (MST) and an MST-based graph are employed to guide the splitting and merging process. In the splitting process, vertices with high degrees in the MST-based graph are selected as initial prototypes, and K-means is used to split the dataset. In the merging process, subgroup pairs are filtered and only neighboring pairs are considered for merge. The proposed method requires no parameter except the number of clusters. Experimental results demonstrate its effectiveness both on synthetic and real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering plays an important role in data mining, pattern recognition, and machine learning. It aims at grouping N data points into K clusters so that data points within the same cluster are similar, while data points in diverse clusters are different from each other. From the machine learning point of view, clustering is unsupervised learning as it classifies a dataset without any a priori knowledge. A large number of clustering algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> have been presented in the literature since K-means <ref type="bibr" target="#b31">[32]</ref>, one of the most popular clustering algorithms, was published. The algorithms can be grouped into hierarchical clustering <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>, partitional clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>, density-based clustering <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, grid-based clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>, model-based clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>, and graph-based clustering <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. Hierarchical and partitional clustering are the two most common groups <ref type="bibr" target="#b29">[30]</ref>.</p><p>Generally, a hierarchical clustering algorithm partitions a dataset into various clusters by an agglomerative or a divisive approach based on a dendrogram. Agglomerative clustering starts by considering each point as a cluster, and iteratively combines two most similar clusters in terms of an objective function. In contrast, divisive clustering starts with only one cluster including all data points. It iteratively selects a cluster and partitions it into two subclusters. The main advantage of hierarchical clustering is that it produces a nested tree of partitions and can therefore be more informative than non-hierarchical clustering. Furthermore, with the dendrogram, the optimal number of clusters can be determined. However, hierarchical clustering has a relatively high computational cost. Single linkage <ref type="bibr" target="#b38">[39]</ref> and complete linkage <ref type="bibr" target="#b25">[26]</ref> are two well-known examples of hierarchical clustering algorithms, and they take O(N 2 logN) time.</p><p>Partitional clustering splits a dataset at once using an objective function. K-means is one of the most popular examples of partitional clustering. It employs mean-squared-error as its objective function. Its main advantage is that it runs efficiently: its computational complexity is O(NKId), where I is the number of iterations for convergence, and d is the dimensionality of the dataset. Since K and d are usually far less than N, the algorithm runs in a linear time on low-dimensional data. However, there does not exist a universal objective function that can be used to discover all different intrinsic structures of datasets. Therefore, partitional clustering produces inaccurate results when the objective function used does not capture the intrinsic structure of the data. This is the reason why partitional clustering algorithms are incapable of dealing with clusters of arbitrary shapes, different sizes and densities.</p><p>Clustering algorithms that combine the advantages of hierarchical and partitional clustering have been proposed in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. This kind of hybrid algorithms analyzes the dataset in two stages. In the first stage, the dataset is split into a number of subsets with a partitioning criterion. In the second stage, the produced subsets are merged in terms of a similarity measure. Different split and merge approaches have been designed in several hybrid algorithms. CSM <ref type="bibr" target="#b29">[30]</ref> first applies K-means to partition the dataset into K 0 subsets, where K 0 is an input parameter. Afterwards, single linkage, which uses a dedicated cohesion function as the similarity measure, is utilized to iteratively merge the K 0 subsets until K subsets are achieved. In the split stage, as K-means may produce different partitions in different runs, the final results may be unstable.</p><p>CHAMELEON <ref type="bibr" target="#b22">[23]</ref> is another example of a hybrid clustering algorithm. It constructs a K-nearest neighbor graph, and employs a graph cut scheme to partition the graph into K 0 subsets. Relative inter-connectivity and relative closeness are defined to merge the subsets. Liu et al. <ref type="bibr" target="#b30">[31]</ref> proposed a multi-prototype clustering algorithm, which can also be considered as a hybrid method. The method uses a convergence mechanism, and repeatedly performs split and merge operations until the prototypes remain unchanged. However, many empirical parameters are involved. Kaukoranta et al. <ref type="bibr" target="#b24">[25]</ref> proposed a split-andmerge algorithm, where the objective function is to minimize the mean squared error.</p><p>A minimum spanning tree (MST) is a useful graph structure, which has been employed to capture perceptual grouping <ref type="bibr" target="#b20">[21]</ref>. Zahn defined several criteria of edge inconsistency for detecting clusters of different shapes <ref type="bibr" target="#b48">[49]</ref>. However, for datasets consisting of differently shaped clusters, the method lacks an adaptive selection of the criteria. Xu et al. <ref type="bibr" target="#b46">[47]</ref> proposed three MST-based algorithms: removing long MST-edges, a center-based iterative algorithm, and a representative-based global optimal algorithm. But for a specific dataset, users do not know which algorithm is suitable.</p><p>In this paper, we propose a minimum spanning tree based split-and-merge method (SAM). It works on numerical data and assumes that the graph can be calculated in a vector space. In the splitting stage, three iterations of MSTs are used to construct a neighborhood graph called 3-MST graph. The vertices with high degrees in the graph are selected as the initial prototypes, and K-means is then applied. In the merge stage, the neighboring subsets with respect to the MST are filtered out and considered for merge.</p><p>The rest of the paper is organized as follows: In Section 2, the proposed algorithm is described. The experimental results are presented in Section 3, and conclusions are drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem formulation</head><p>Given a set of data points X = {x 1 , x 2 , . . . , x i , . . . , x N }, where x i ¬º √∞x i1 ; x i2 ; . . . ; x ij ; . . . ;</p><formula xml:id="formula_0">x id √û T 2 R d is a feature vector, the goal of clustering is to partition the set X into K clusters: C 1 , C 2 , . . . , C K , where C i -;, C i \ C j = ;, X = C 1 [ C 2 √Å √Å √Å [ C K , i = 1: K, j = 1: K, i -j.</formula><p>An undirected graph has been employed to represent a dataset for clustering <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>. Let G(X) = (V, E) denote the undirected complete graph of X, the weights of the edges can be calculated with function w : E ! R, where V = X, E = {(x i , x j )jx i , x j 2 X, i -j}, and w√∞x i ;</p><formula xml:id="formula_1">x j √û ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi √∞x i √Ä x j √û T √∞x i √Ä x j √û q .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Overview of the split-and-merge algorithm</head><p>The algorithm consists of three main stages as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. First, an MST of the given dataset is constructed to guide the clustering process. In the splitting stage, K-means is applied to split the dataset into subsets, which are then adjusted according to the MST. In the merge stage, the final clusters are obtained by performing a carefully designed criterionbased merge that aims at maximizing intra-cluster similarity and minimizing inter-connectivity between the clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The stage of constructing a 3-MST graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Pruning leaves</head><p>A minimum spanning tree (MST) of graph G(X) is a spanning tree T such that W√∞T√û ¬º P √∞x i ;x j √û2T w√∞x i ; x j √û is minimum. The leaves of an MST, called hairs in <ref type="bibr" target="#b48">[49]</ref>, are the vertices of degree 1. The leaves usually locate outside of kernels or skeletons of a dataset. For splitting two neighboring clusters, the data points in the neck <ref type="bibr" target="#b48">[49]</ref> will have a negative effect on the clustering process. To alleviate the effect, we design a pruning step of removing the leaves so that the clusters are analyzed only on the essential data points. In <ref type="bibr" target="#b48">[49]</ref>, the skeleton of an MST was detected by repeatedly pruning until the number of removed hairs in two successive iterations remains the same. However, this may remove an entire cluster, and therefore, we conservatively perform only one pruning. Definition 1. Let X 0 be the pruned version of X as in</p><formula xml:id="formula_2">X 0 ¬º X n v i jv i 2 V; degree√∞v i √û ¬º 1 f g ;<label>√∞1√û</label></formula><p>where degree(v i ) denotes the degree of vertex v i in the MST of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Constructing a 3-MST graph</head><p>An MST describes the intrinsic skeleton of a dataset and accordingly can be used for clustering. In our proposed method, we use it to guide the splitting and merging processes. However, a single MST loses some neighborhood information that is crucial for splitting and merging. To overcome this drawback, we combine several MSTs and form a graph G mst (X, k) as follows:</p><formula xml:id="formula_3">Definition 2. Let T 1 = f mst (V, E) denote the MST of G(X) = (V, E).</formula><p>The following iterations of an MST are defined as:</p><formula xml:id="formula_4">T i ¬º f mst V; E n [ i√Ä1 j¬º1 T j ;<label>√∞2√û</label></formula><p>where f mst : (V, E) ? T is a function to compute an MST from graph G(X) = (V, E), and i P 2.</p><p>In theory, the above definition of T i is not rigorous because E n [ i√Ä1 j¬º1 T j may produce isolated subgraph. For example, if there exists a vertex v in T 1 and the degree of v is jVj √Ä 1, v will be isolated in G(V, EnT 1 ). Hence, the second MST (T 2 ) cannot be completed in terms of Definition 2. In practice, this is not a problem because the first MST T 1 is still connected and we can simply ignore it as a minor artefact, because it has no noticeably effect on the performance of the overall algorithm. However, for the sake of completeness, we solve this minor problem by always connecting such an isolated subgraph with an edge randomly selected from those connecting the isolated subgraph in T 1 .</p><p>Let G mst (X, k) denote the k-MST graph, which is defined as a union of the k MSTs:</p><formula xml:id="formula_5">G mst (X, k) = T 1 [ T 2 [ √Å √Å √Å [T k .</formula><p>In this paper, we use G mst (X 0 , k) to determine the initial prototypes in the split stage and to calculate the merge index of a neighboring partition pair in the merge stage. Here, k is set to 3 in terms of the following observation: 1 round of MST is not sufficient for the criterion-based merge but 3 iterations are. The number itself is a small constant and can be justified from computational point of view. Additional iterations do not add much to the quality, but only increase processing time. A further discussion concerning the selection of k can be found in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Split stage</head><p>In the split stage, initial prototypes are selected as the nodes of highest degree in the graph G mst (X 0 , k). K-means is then applied to the pruned dataset using these prototypes. The produced partitions are adjusted to keep the clusters connected with respect to the MST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Application of K-means</head><p>The pruned dataset is first split by K-means in the original Euclidean space, where the number of partitions K 0 is set to ffiffiffiffiffiffiffi jX 0 j p . This is done under the assumption that the number of clusters in a dataset is smaller than the square root of the number of patterns in the dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. If ffiffiffiffiffiffiffi jX 0 j p 6 K, K 0 can be set to K + k(jX 0 j √Ä K) to grantee that K 0 is greater than K, where 0 &lt; k &lt; 1. Since this is not a normal situation, we do not discuss the parameter k in this paper. Moreover, if jX 0 j 6 K, the splitand-merge scheme will degenerate into a traditional agglomerative clustering.</p><p>However, to determine the K 0 initial prototypes is a tough problem, and a random selection would give an unstable splitting result. For example, the method proposed in <ref type="bibr" target="#b29">[30]</ref> uses K-means with randomly selected prototypes in its split stage, and the final clustering results are not unique. We therefore utilize the MST-based graph G mst (X 0 , 3) to avoid this problem. The overview of split-and-merge. In Stage 1, the dataset X is pruned into X 0 according to the MST of X, and three iterations of MSTs of X 0 are computed and combined into a 3-MST graph. In Stage 2, X 0 is partitioned by K-means, where the initial prototypes are generated from the 3-MST graph. The partitions are then adjusted so that each partition is a subtree of the MST of X 0 . In Stage 3, the partitions are merged into the desired number of clusters and the pruned data points are distributed to the clusters.</p><formula xml:id="formula_6">Definition 3. Let v i be the ith prototype from G mst (X 0 , 3) = (V, E), V i√Ä1 = {v 1 , v 2 , . . . , v i√Ä1 }, E i√Ä1 = {(x i , x j )j(x i , x j ) 2 E ^(x i = v _ x j = v) ^v 2 V i√Ä1 }, v i is generated as: v i ¬º arg max v2VnV i√Ä1 Car x i ; x j √Ä √Å j x i ; x j √Ä √Å 2 E n E i√Ä1 √∞ √û^x i ¬º v _ x j ¬º v √Ä √Å √à √â √Ä √Å ;<label>√∞3√û</label></formula><p>where Car(S) denotes the cardinality of set S.</p><p>The above definition determines the vertex with the maximum degree as a prototype. It is a recursive definition, when we determine the ith prototype v i , the edges connected to the existing i √Ä 1 prototypes will not be considered for counting the degrees of vertices. However, there may exist two or more vertices in G mst (X 0 , 3) simultaneously having the maximum degree.</p><p>In this case, the vertex with the minimum sum of weights of its edges is to be selected.</p><p>After the K 0 initial prototypes have been determined, K-means is applied. The seven subsets (C 1 , . . ., C 7 ) in stage 2 of Fig. <ref type="figure" target="#fig_0">1</ref> are the partitions on T 1 produced by K-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Adjusting partitions</head><p>In traditional MST-based clustering methods such as <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, each partition is a subtree of the MST rather than a forest. This is a reasonable observation, because data points in the same cluster are usually in the same branch of the MST. However, the partitions in stage 2 of Fig. <ref type="figure" target="#fig_0">1</ref> may not coincide with the observation. For example, the four subsets (C 1 , C 4 , C 6 , C 7 ) are forests but not trees.</p><p>Furthermore, the partitions in stage 2 of Fig. <ref type="figure" target="#fig_0">1</ref> reduce the ability of the MST to guide the merging process. This is because only neighboring partitions will be considered to be merged, and the neighboring partitions can be easily determined by the MST. However, if the partitions are forests, the MST would lose the ability.</p><p>Therefore, a process of redistributing the vertices is designed to transform a forest into a tree. The process is described as follows. Suppose that T 1 of X 0 is partitioned into K 0 forests, which are denoted as F 1 ; F 2 ; . . . ; F K 0 . For redistributing the vertices, the main tree in each forest is defined. Definition 4. Let F i = {t 1 , t 2 , . . ., t j , . . . , t n }, and each t j being a tree. The main tree of F i is defined as:</p><formula xml:id="formula_7">Maintree√∞F i √û ¬º arg max t j 2F i Car√∞t j √û;<label>√∞4√û</label></formula><p>where Car(t j ) denotes the edge number of the tree t j . The vertices which are not included in any main tree will be re-allocated so that each subset is a subtree of T 1 of X 0 . Suppose F 1 , . . . , F 7 are the forests of the subsets C 1 , . . . , C 7 in Fig. <ref type="figure" target="#fig_0">1</ref>, respectively. To adjust the seven subsets into seven subtrees, vertices v1 and v2 can be re-allocated to Maintree(F 2 ), v3 to Maintree(F 1 ), v4, v5, v6 to Maintree(F 6 ), and v7 to Maintree(F 7 ). The re-allocation process is described as follows.</p><p>Let SV denote the set of vertices that are not in any main tree, ST the set of the main trees. The redistribution operation is defined as: for an edge e ab from T 1 of X 0 , if a 2 SV, and $T(T 2 ST ^b 2 T), then the vertex a is redistributed to T. For example, v2 is re-allocated to Maintree(F 2 ), since e v2v8 2 T 1 , v2 2 SV, and v8 2 Maintree(F 2 ). We iteratively perform this operation until all of the vertices in SV are re-allocated to the main trees.</p><p>The above operation may produce non-unique redistributions. Take v6 and v7 for example, if e v6v9 and e v7v6 are considered before e v7v10 , then v6 and v7 will be redistributed to Maintree(F 6 ). Similarly, v6 and v7 may be redistributed to Maintree(F 7 ), or v6 to Maintree(F 6 ) and v7 to Maintree(F 7 ). However, the non-unique redistribution does not noticeably affect the final clustering result for the two reasons: one is that the subtrees in a subset are usually smaller than the main tree of the subset, the other one is that the non-uniqueness most often happens inside rather than between the expected clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Merge stage</head><p>After X 0 has been split into K 0 subgroups, the merge stage is performed to obtain the final clusters. In the merging process, the crucial problem is to determine which pairs of subgroups should be merged. By brute force, there are K 0 √Ç (K 0 √Ä 1)/2 candidate pairs for merge. In this paper, a stepwise refinement method is taken to address the problem. At the beginning, T 1 is employed to filter out the pairs. On the basis of MST-based clustering <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, unconnected subtrees cannot be merged. For example, the subgroups C 3 and C 7 in Fig. <ref type="figure" target="#fig_0">1</ref> will not be considered as a pair for merge. Consequently, only the neighboring pairs with respect to T 1 are the candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">Neighboring pairs</head><p>Definition 5. Let Pairs be the set of neighboring pairs from X 0 as in:</p><formula xml:id="formula_8">Pairs ¬º C i ; C j √Ä √Å j9 x p ; x q √Ä √Å 2 T 1 ; x p 2 C i ^xq 2 C j √Ä √Å _ x p 2 C j ^xq 2 C i √Ä √Å √à √â ;<label>√∞5√û</label></formula><p>where ij.</p><p>For any two subgroups, if there exists an edge in T 1 connecting them, then the two subgroups belong to Pairs.</p><p>For selecting the best pair to be merged, we define an inter-connectivity index and an intra-similarity index to rank the pairs.</p><p>2.5.2. Inter-connectivity index Definition 6. Suppose (C i , C j ) 2 Pairs. In G mst (X 0 , 3), let E inter (C i , C j ) be the set of edges across the partitions C i and C j :</p><formula xml:id="formula_9">E inter √∞C i ; C j √û ¬º f√∞x p ; x q √ûj√∞√∞x p 2 C i ^xq 2 C j √û _ √∞x p 2 C j ^xq 2 C i √û√ûg:<label>√∞6√û</label></formula><p>In Fig. <ref type="figure">2</ref>, E inter (C 6 , C 7 ) = {(a, g), (a, h), (b, g), (c, i), (d, i), (d, j), (e, i), (e, j), (e, l), (e, k)}, i.e. the edges crossing the dotted curve.</p><p>Definition 7. Suppose that (C i , C j ) 2 Pairs. In G mst (X 0 , 3), set V i,j = {x p j(x p , x q ) 2 E inter (C i , C j ) ^xp 2 C i }, E i,j is a set of edges within C i where at least one endpoint is in V i,j :</p><formula xml:id="formula_10">E i;j ¬º x p ; x q √Ä √Å jx p ; x q 2 C i ^xp 2 V i;j _ x q 2 V i;j √Ä √Å √à √â :<label>√∞7√û</label></formula><p>In Fig. <ref type="figure">2</ref>, for example, V 6,7 = {g, h, i, j, k, l}, V 7,6 = {a, b, c, d, e}, E 6,7 includes all the internal edges of C 6 (i.e. the edges whose both endpoints are from C 6 ) except {(n, m), (n, o), (n, p), (m, o), (m, p), (m, q), (p, o), (p, q)}, and E 7,6 includes all the internal edges of C 7 . Actually, E i,j is defined as the edges in the region of C i that is close to C j .</p><p>Connection span is then defined with respect to G mst (X 0 , 3) as a factor of measuring the similarity of two clusters based on the width of their connection. The intuition behind this index is that it estimates the size of the common border of the two clusters. The larger the common border is, the higher the priority for merging these clusters will be. It depends only on the distances in vector space and makes no assumption on the dimensionality. Definition 8. Suppose that (C i , C j ) 2 Pairs. In G mst (X 0 , 3), the connection span of C i with respect to C j is:</p><formula xml:id="formula_11">ConnSpan i;j ¬º max xp;xq2V i;j w x p ; x q √Ä √Å :<label>√∞8√û</label></formula><p>In Fig. <ref type="figure">3</ref>, the connection span of C 6 and C 7 is marked by the dotted edges (g, k) and (a, e), respectively.</p><p>Definition 9. Suppose that (C i , C j ) 2 Pairs. In G mst (X 0 , 3), the inter-connectivity (IC) of C i and C j is defined as:</p><formula xml:id="formula_12">IC C i ; C j √Ä √Å ¬º E inter C i ; C j √Ä √Å min jC i j; jC j j √Ä √Å√Ç min Avg E i;j √Ä √Å ; Avg E j;i √Ä √Å √Ä √Å Avg E inter C i ; C j √Ä √Å √Ä √Å √Ç max ConnSpan i;j ; ConnSpan √∞ j; i√û ;<label>√∞9√û</label></formula><p>where Avg(E) denotes the average weight of an edge set E. From Eq. ( <ref type="formula" target="#formula_12">9</ref>), the inter-connectivity index IC is a composite of three factors. The factor jE inter (C i , C j )j/min(jC i j, jC j j) describes that the more edges straddling a pair of clusters, the stronger is the connective strength between the two clusters. In the second factor, min (Avg(E i,j ), Avg(E j,i )) is the minimum of the average weights of the edges that are in the two close regions from C i and C j , respectively. Avg(E inter (C i , C j )) is the average weight of the edges straddling C i and C j . This factor reflects that the ratio of the average weight of the straddling edges to the minimum average weight in the two close regions is inversely proportional to the connectivity of the two clusters. In fact, this is based on the observation that if the density between the two clusters is low compared with those of the close regions, the two clusters have a small probability to be merged. The third factor max (ConnSpan i,j , ConnSpan j,i ) indicates that a pair of clusters with large connection span has strong connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">Intra-similarity index</head><p>For describing the intra-similarity of pairs of clusters, a strategy inspired by Karypis et al. <ref type="bibr" target="#b22">[23]</ref> is employed. Each cluster of a neighboring pair is bisected in terms of T 1 , and the corresponding inter-edges with respect to G mst (X 0 , 3) are used to evaluate the intra-similarity between the two clusters. The process is described as follows.</p><p>Suppose C i is the cluster to be bisected, and T i1 is the subtree of T 1 restricted to nodes and edges of C i . The bisecting edge e bisect 2 T i1 can be determined as: </p><formula xml:id="formula_13">e j 2T i1 Car t j1 √Ä √Å √Ä Car t j2 √Ä √Å ;<label>√∞10√û</label></formula><p>where t j1 and t j2 are two subtrees of T i1 generated by removing e j from T i1 , and Car(t) denotes the number of edges in a tree t.</p><p>An example of the bisection is shown in Fig. <ref type="figure">4</ref>. Correspondingly, C i is bisected into two subsets:</p><formula xml:id="formula_14">C 1 i and C 2 i . The inter-edge set E inter √∞C 1 i ; C 2 i √û with respect to G mst (C i , 3</formula><p>) is obtained by Definition 6. The intra-similarity of C i and C j is then defined as follows:</p><p>Definition 10. Suppose that (C i , C j ) 2 Pairs, C i is bisected into C 1 i and C 2 i , C j is bisected into C 1 j and C 2 j , the intra-similarity (IS) of the pair (C i , C j ) is defined as:</p><formula xml:id="formula_15">IS C i ; C j √Ä √Å ¬º 1 r 1 √Ç r 2 √Ç ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Avg E inter C 1 i ; C 2 i √Ç Avg E inter C 1 j ; C 2 j r Avg E inter C 1 i ; C 2 i √æ Avg E inter C 1 j ; C 2 j ;<label>√∞11√û</label></formula><p>where r 1 and r 2 are the numbers of edges of E inter √∞C 1 i ; C 2 i √û and E inter √∞C 1 j ; C 2 j √û, respectively, and Avg(E) denotes the average weight of an edge set E.</p><p>Eq. ( <ref type="formula" target="#formula_15">11</ref>) implies that the intra-similarity between the pair (C i , C j ) is high when the two averages Avg√∞E inter √∞C 1 i ; C 2 i √û√û and Avg√∞E inter √∞C 1 j ; C 2 j √û√û are close to each other. For the two numbers r 1 and r 2 , unbalanced and small sizes indicate that the two clusters are more likely to be merged.</p><p>Taking into account the inter-connectivity and the intra-similarity as a whole, we define the overall merge index as:</p><formula xml:id="formula_16">R C i ; C j √Ä √Å ¬º IC C i ; C j √Ä √Å √Ç IS C i ; C j √Ä √Å :<label>√∞12√û</label></formula><p>The neighboring pair with the highest R() value is merged first. After one neighboring pair is merged, Pairs and corresponding R() values are updated. When K partitions are achieved, the merge process stops. Because a pruned leaf 2X is connected to exact one vertex x i 2 X 0 in the MST of X, it is assigned the same label as x i . Finally, the proposed algorithm is described as follows:</p><p>Algorithm 1: Split-and-merge (SAM)</p><formula xml:id="formula_17">Input: Dataset X, number of clusters K Output: K clusters C 1 , C 2 , . . . , C K 1. Construct 3-MST graph 1.1 Construct an MST on X 1.</formula><p>2 Produce X 0 by pruning the leaves of the MST 1.3 Create three MSTs on X 0 : T 1 , T 2 and T 3 1.4 Compute 3-MST graph based on X 0 : G mst (X 0 , 3)</p><formula xml:id="formula_18">T 1 [ T 2 [ T 3 2. Split the pruned dataset X 0 into clusters 2.</formula><p>1 Select K 0 highest degree nodes from G mst (X 0 , 3) as initial prototypes 2.2 Apply K-means with the prototypes to produce K 0 partitions 2.3 For each of the partitions, find its main tree in T 1 2.4 For each of the subtrees, repeatedly combine it with another subtree until it belongs to a main tree 3. Merge the partitions into final clusters </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Computational complexity</head><p>To construct an MST, if Fibonacci heaps are employed in Prim's algorithm, the running time is O(jEj + jVjlogjVj) <ref type="bibr" target="#b8">[9]</ref>. In this paper, an MST is computed from a complete graph, jEj = O(jVj 2 ) = O(N 2 ), and therefore, the complexity of constructing an MST is O(N 2 ).</p><p>In Step 3.4 can be processed in constant time. To up, the computational complexity of the proposed method (SAM) is O(N 2 ), which is dominated by the construction of the 3-MST graph. If also the factor of dimensionality d is considered, the exact complexity would be O(N 2 d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>The clustering performance of the proposed method is evaluated on six synthetic and four real datasets. The first four synthetic datasets DS1-DS4 are taken from the literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>, and the next two DS5, DS6 are from <ref type="bibr" target="#b41">[42]</ref>. These datasets are illustrated in Fig. <ref type="figure">5</ref>. The four real world instances are taken from the UCI datasets [50], including IRIS, WINE, Breast Cancer Wisconsin (WBC), and Breast Cancer Wisconsin Diagnostic (WDBC). The descriptions of these datasets are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The proposed method SAM is compared to the following six clustering algorithms:</p><p>1. K-means <ref type="bibr" target="#b31">[32]</ref>. 2. DBScan <ref type="bibr" target="#b9">[10]</ref>.</p><p>3. Single linkage <ref type="bibr" target="#b38">[39]</ref>. 4. Spectral clustering <ref type="bibr" target="#b37">[38]</ref>. 5. CSM <ref type="bibr" target="#b29">[30]</ref>. 6. CHAMELEON <ref type="bibr" target="#b22">[23]</ref>.</p><p>The first four algorithms are traditional clustering methods, whereas the next two are hybrid ones. In addition, three variants of SAM are performed to demonstrate the importance of the various steps and design choices of the algorithm:</p><p>5. Illustration of the six original synthetic datasets.</p><p>1. SAM-No-Pruning: split-and-merge without the pruning step. 2. SAM-2-MST: split-and-merge but using 2-MST instead of 3-MST. 3. SAM-SL: split-and-merge but using single linkage within the merge stage instead of the criterion based on R()-values.</p><p>As K-means and CSM may produce different partitions in different runs, in the following experiments we take the best clustering result out of 10 trial runs performed for each dataset in terms of one of the most used external clustering validity indices, Adjusted Rand (see Section 3.1). For DBScan, since we have no a priori knowledge about the parameters (MinPts and Eps), proper values were selected by trial and error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">External clustering validity indexes</head><p>An external clustering validity index is defined measure the degree of correspondence between a clustering result and the prespecified partitions <ref type="bibr" target="#b40">[41]</ref>. We will employ the four popular external indexes, Rand, FM, and Adjusted Rand ( <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>), to evaluate the quality of the clustering results. Briefly, these four indexes are described as follows.</p><p>Suppose {P 1 , . . . , P L } is the set of prespecified partitions, {C 1 , . . . , C K } is a set of partitions produced by a clustering. For a pair of vectors (x u , x v ), it is referred as: (a) SS pair if $I(x u 2 P I ^xv 2 P I ) and $J(x u 2 C J ^xv 2 C J ), (b) SD pair if $I(x u 2 P I ^xv 2 P I ) and 9 = J(x u 2 C J ^xv 2 C J ), (c) DS pair if 9 = I(x u 2 P I ^xv 2 P I ) and $J(x u 2 C J ^xv 2 C J ), (d) DD pair if 9 = I(x u 2 P I ^xv 2 P I ) and 9 = J(x u 2 C J ^xv 2 C J ). Let a, b, c, d denote the numbers of SS, SD, DS, and DD pairs, respectively, and M the number of total pairs, the four indexes are defined as:</p><formula xml:id="formula_19">1. Rand R ¬º √∞a √æ d√û=M:<label>√∞13√û</label></formula><formula xml:id="formula_20">2. Jaccard J ¬º a=√∞a √æ b √æ c√û:<label>√∞14√û</label></formula><formula xml:id="formula_21">3. FM FM ¬º ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi a a √æ b a a √æ c r :<label>√∞15√û</label></formula><formula xml:id="formula_22">4. Adjusted Rand AR ¬º 2 Ma √Ä √∞a √æ b√û√∞a √æ c√û √∞ √û M√∞2a √æ b √æ c√û √Ä 2√∞a √æ b√û√∞a √æ c√û :<label>√∞16√û</label></formula><p>3.2. Results on synthetic datasets DS1: This instance contains two datasets shaped like a crescent with different densities. The clustering results are illustrated in Fig. <ref type="figure" target="#fig_6">6</ref>. SAM and SAM-No-Pruning, DBScan, CSM and CHAMELEON can partition the dataset properly. Both SAM-2-MST and SAM-SL fail. Since K-means favors spherical clusters, it fails on DS1. Single linkage clustering produces unsatisfactory partitions because it measures the distance between two clusters as the minimum distance between the pairs of data points in the two clusters. In the spectral clustering algorithm, the similarity matrix is created by a Gaussian kernel function with Euclidean distances, but the clustering result of this algorithm is similar to that of K-means. Although DBScan produces the expected partitions, it was difficult to tune the two parameters to achieve the proper result.</p><p>DS2: The set is composed of two Gaussian distributed clusters and one unclosed ring cluster surrounding the first two. Fig. <ref type="figure" target="#fig_5">7</ref> illustrates the clustering results. SAM and SAM-2-MST provide satisfactory clustering results, whereas SAM-No-Pruning    The corresponding Adjusted Rand index values of the clustering results on these six synthetic datasets are shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on real datasets</head><p>The performance on each of the four UCI datasets is evaluated using the four common external clustering validity indices: Rand, Adjusted-Rand, Jaccard coefficient, and Fowlkes and Mallows (FM). The evaluation results on the four datasets are shown in Tables <ref type="table" target="#tab_3">3</ref><ref type="table" target="#tab_2">4</ref><ref type="table">5</ref><ref type="table" target="#tab_4">6</ref>, respectively. The parameters of DBScan for IRIS, WINE, WBC, and WDBC are set to (MinPts = 8, Eps = 0.4), (MinPts = 3, Eps = 0.3), (MinPts = 9, Eps = 2), (MinPts = 4, Eps = 32.3), respectively.</p><p>For the IRIS dataset, Table <ref type="table" target="#tab_3">3</ref> indicates that SAM, SAM-2-MST, and SAM-SL have the same performance and outperform the others. SAM-No-Pruning, CSM, DBScan, and K-means also provide partitions with relative high quality. In the case of the WINE dataset, the corresponding clustering qualities are shown in Table <ref type="table" target="#tab_2">4</ref>. K-means has the best performance, single linkage and spectral clustering provide more proper partitions than the proposed method SAM. It can be seen from Table <ref type="table">5</ref> that SAM outperforms the other algorithms except K-means on the WBC dataset. As for the WDBC dataset in Table <ref type="table" target="#tab_4">6</ref>, DBScan has the     best performance, while the proposed method SAM is better than K-means, single linkage, spectral clustering, CSM as well as its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion about the SAM variants</head><p>Pruning gives a small but consistent improvement (SAM vs. SAM-No-Pruning) based on the numeric results (Tables <ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_3">3</ref><ref type="table" target="#tab_2">4</ref><ref type="table">5</ref>). In the case of DS2, pruning is critical to obtain the correct clustering whereas for the other test sets its effect is more like finetuning.</p><p>For the merge criterion, the index R(C i , C j ) is more complicated to evaluate than some simple alternatives like the single linkage criterion. Its effect on the clustering quality, however, is significant and in most cases the proposed approach (SAM) outperforms the single linkage variant (SAM-SL) using the single linkage algorithm in the merge stage. We consider the criterion-based merge critical for the performance of the algorithm.</p><p>The role of the k-MST is important. The value k = 3 was fixed already in preliminary tests using DS1-DS6, but let us discuss other choices. In SAM, 1-MST cannot be used in the merge stage as the criterion requires more information about the neighborhood than a simple spanning tree can provide, as the results of SAM-SL already showed. The question about the exact value of k, however, is less critical. In most cases, SAM and SAM-2-MST provide the same result, and only in two cases (DS1 and DS2) the 2-MST variant fails. Higher values than k = 3 were also tested, see Fig. <ref type="figure" target="#fig_11">12</ref>. In most cases, the improvements due to higher values for k are insignificant and just increase the processing time. A contradicting example is the WDBC test set where higher values turn out to be harmful. Therefore, it is reasonable that the value of k is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The proposed method employs minimum spanning trees in different stages. Before a dataset is split into different clusters, the hairs (leaves together with the connecting edges) of the first MST computed for the whole instance are pruned.</p><p>In the splitting stage, more than the desired K clusters are created by K-means. Three MSTs on an iteratively refined graph are computed and combined to determine the initial prototypes for K-means, because randomly selected initial prototypes would lead to unstable partitions.</p><p>After splitting, the initial MST is employed to adjust the partitions to make each subgroup corresponding to a subtree. Finally, in the merge step only neighboring pairs with respect to the same MST are considered to be merged.</p><p>Experiments have demonstrated the importance of each step of the algorithm. Except the number of clusters, there are no parameters left to be tuned by the user.</p><p>In summary, various MSTs are utilized during the whole split-and-merge algorithm because they can capture the intrinsic structure of a dataset. However, the computational complexity of constructing an MST is close to O(N 2 ). The expensive computational cost obstructs the application of an MST to large scale data sets. One of our future work is to find a fast algorithm to construct an approximate MST.</p><p>One drawback of the proposed method is that the universality of the definitions of inter-connectivity and intra-similarity is insufficient. Although there does not exist a universal clustering method that can deal with all kinds of clustering problems, we try to improve the definition of inter-connectivity and intra-similarity to make the proposed method suitable for as many clustering problems as possible.</p><p>Although the proposed method does not have any direct restrictions for being applied to datasets with high dimensions, it is assumed to have all the same weaknesses as the other distance-based methods. Because to determine the intrinsic structure of this kind of datasets, different dimensions may have varied importance, whereas the proposed method equally considers all of dimensions of an input dataset. Thus, subspace clusterings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or other methods tailored for high dimensional data are expected to work better for high-dimensional datasets.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The overview of split-and-merge. In Stage 1, the dataset X is pruned into X 0 according to the MST of X, and three iterations of MSTs of X 0 are computed and combined into a 3-MST graph. In Stage 2, X 0 is partitioned by K-means, where the initial prototypes are generated from the 3-MST graph. The partitions are then adjusted so that each partition is a subtree of the MST of X 0 . In Stage 3, the partitions are merged into the desired number of clusters and the pruned data points are distributed to the clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Illustration of the inter edges between subgroups C 6 and C 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 . 1 1 Fig. 4 . 2 i</head><label>31142</label><figDesc>Fig. 4. Illustration of the bisection process of a cluster C i . The data set is bisected into C 1 i and C 2 i by the cut so that the difference of cardinalities of C 1 i and C 2 i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Step 1.1, the MST is constructed in O(N 2 ) time, and in Step 1.2, the complexity of pruning the leaves from the MST is O(N). In Step 1.3, to generate the three MSTs of X 0 takes O(N 2 ) time. G mst (X 0 , 3) is obtained in O(N) time in Step 1.4. In Step 2.1, the initial prototypes are determined in O(N), the time complexity of K-means on X 0 is O(K 0 NdI), where d is the dimensionality of X 0 , and I is the number of iterations. Since K 0 6 ffiffiffiffi N p , the total complexity of Step 2.1 is O(N 3/2 dI). The main trees can be determined in O(N) in Step 2.2, and re-allocation can also be achieved in O(N) in Step 2.3. Step 3.1 takes O(N) to generate Pairs, and Step 3.2 takes O(N) to compute the merge index R() for every pair in Pairs. As the merging of a pair can be achieved in constant time, the maximum complexity of updating the merge index R() is O(N), and the number of iterations of Step 3.3 is O√∞ ffiffiffiffi N p √û, the worst case complexity of this Step is therefore O(N 3/2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>DS3:</head><label></label><figDesc>This dataset contains a spherical cluster and a half ring shaped cluster. The clustering results are shown in Fig. 8. All the algorithms except K-means and single linkage discover the expected clusters. DS4: This dataset consists of 7 Gaussian distributed clusters. Fig. 9 illustrates the clustering results. SAM, its variant SAM-No-Pruning and SAM-2-MST, spectral clustering, CSM, and CHAMELEON find the expected clusters. SAM-SL and K-means provide partitions with low quality. DS5: This consists of 31 Gaussian distributed clusters. The clustering results are illustrated in Fig. 10. Spectral clustering, CSM, and CHAMELEON produce the expected partitions. SAM and SAM-SL are also successful except they fail to detect the cluster on the rightmost part. SAM-No-Pruning and SAM-2-MST perform much worse, whereas K-means, single linkage, and DBScan fail to detect almost all the expected clusters. DS6: The 15 Gaussian distributed clusters in this dataset are arranged in two concentric circles. Fig. 11 describes the clustering results. SAM, SAM-SL, spectral clustering, CSM, and CHAMELEON produce the proper partitions, but SAM-No-Pruning, SAM-2-MST, K-means, single linkage, and DBScan do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of clustering results on DS2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustration of clustering results on DS1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Illustration of clustering results on DS4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of clustering results on DS3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Illustration of clustering results on DS6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Illustration of clustering results on DS5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The quality of clustering results for different values of k to compute k-MSTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Descriptions of used datasets. SL produce improper results. CSM can sometimes identify the proper clusters, but not always. The results of DBScan and spectral clustering are not perfect, but better than those of K-means, single linkage, and CHAMELEON.</figDesc><table><row><cell>Data set</cell><cell>Data size (N)</cell><cell>Dimensionality (d)</cell><cell>Number of clusters (K)</cell></row><row><cell>DS1</cell><cell>373</cell><cell>2</cell><cell>2</cell></row><row><cell>DS2</cell><cell>300</cell><cell>2</cell><cell>3</cell></row><row><cell>DS3</cell><cell>240</cell><cell>2</cell><cell>2</cell></row><row><cell>DS4</cell><cell>788</cell><cell>2</cell><cell>7</cell></row><row><cell>DS5</cell><cell>3100</cell><cell>2</cell><cell>31</cell></row><row><cell>DS6</cell><cell>600</cell><cell>2</cell><cell>15</cell></row><row><cell>Iris</cell><cell>150</cell><cell>4</cell><cell>3</cell></row><row><cell>Wine</cell><cell>178</cell><cell>13</cell><cell>3</cell></row><row><cell>WBC</cell><cell>683</cell><cell>9</cell><cell>2</cell></row><row><cell>WDBC</cell><cell>569</cell><cell>30</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Adjusted Rand index values of clustering performances on the six synthetic datasets.</figDesc><table><row><cell>Method</cell><cell>DS1</cell><cell>DS2</cell><cell>DS3</cell><cell>DS4</cell><cell>DS5</cell><cell>DS6</cell></row><row><cell>SAM</cell><cell>1.0000</cell><cell>0.9597</cell><cell>0.9339</cell><cell>0.9835</cell><cell>0.8896</cell><cell>0.9928</cell></row><row><cell>SAM-No-Pruning</cell><cell>1.0000</cell><cell>0.7272</cell><cell>0.9180</cell><cell>0.9920</cell><cell>0.8051</cell><cell>0.8723</cell></row><row><cell>SAM-2-MST</cell><cell>0.3181</cell><cell>0.9597</cell><cell>0.9178</cell><cell>0.9902</cell><cell>0.8183</cell><cell>0.8375</cell></row><row><cell>SAM-SL</cell><cell>0.2563</cell><cell>0.6130</cell><cell>0.9178</cell><cell>0.8743</cell><cell>0.8755</cell><cell>0.9928</cell></row><row><cell>K-means</cell><cell>0.5146</cell><cell>0.4739</cell><cell>0.4312</cell><cell>0.7186</cell><cell>0.8218</cell><cell>0.8055</cell></row><row><cell>Single linkage</cell><cell>0.2563</cell><cell>0.0004</cell><cell>0.0103</cell><cell>0.7996</cell><cell>0.1739</cell><cell>0.5425</cell></row><row><cell>DBScan</cell><cell>1.0000</cell><cell>0.8213</cell><cell>0.8859</cell><cell>0.8043</cell><cell>0.4321</cell><cell>0.8804</cell></row><row><cell>Spectral clustering</cell><cell>0.3178</cell><cell>0.8757</cell><cell>0.9178</cell><cell>0.9919</cell><cell>0.9522</cell><cell>0.9928</cell></row><row><cell>CSM</cell><cell>1.0000</cell><cell>0.9118</cell><cell>0.9667</cell><cell>0.9978</cell><cell>0.9196</cell><cell>0.9857</cell></row><row><cell>CHAMELEON</cell><cell>1.0000</cell><cell>0.4756</cell><cell>0.9617</cell><cell>1.0000</cell><cell>0.9274</cell><cell>0.9928</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Clustering performances on WINE.</figDesc><table><row><cell>Method</cell><cell>Rand</cell><cell>Adjusted Rand</cell><cell>Jaccard</cell><cell>FM</cell></row><row><cell>SAM</cell><cell>0.7334</cell><cell>0.4007</cell><cell>0.4294</cell><cell>0.6009</cell></row><row><cell>SAM-No-Pruning</cell><cell>0.7012</cell><cell>0.3842</cell><cell>0.4113</cell><cell>0.5819</cell></row><row><cell>SAM-2-MST</cell><cell>0.7334</cell><cell>0.4007</cell><cell>0.4294</cell><cell>0.6009</cell></row><row><cell>SAM-SL</cell><cell>0.6976</cell><cell>0.3956</cell><cell>0.4702</cell><cell>0.6521</cell></row><row><cell>K-means</cell><cell>0.8797</cell><cell>0.7302</cell><cell>0.6959</cell><cell>0.8208</cell></row><row><cell>Single linkage</cell><cell>0.7766</cell><cell>0.5638</cell><cell>0.5891</cell><cell>0.7635</cell></row><row><cell>DBScan</cell><cell>0.6878</cell><cell>0.3171</cell><cell>0.3866</cell><cell>0.5582</cell></row><row><cell>Spectral clustering</cell><cell>0.7655</cell><cell>0.4741</cell><cell>0.4821</cell><cell>0.6506</cell></row><row><cell>CSM</cell><cell>0.6742</cell><cell>0.3757</cell><cell>0.4708</cell><cell>0.6618</cell></row><row><cell>CHAMELEON</cell><cell>0.7364</cell><cell>0.4769</cell><cell>0.5266</cell><cell>0.7049</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clustering performances on WBC.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Rand</cell><cell>Adjusted Rand</cell><cell>Jaccard</cell><cell>FM</cell></row><row><cell>SAM</cell><cell>0.9026</cell><cell>0.8033</cell><cell>0.8372</cell><cell>0.9114</cell></row><row><cell>SAM-No-Pruning</cell><cell>0.8876</cell><cell>0.7682</cell><cell>0.8061</cell><cell>0.8953</cell></row><row><cell>SAM-2-MST</cell><cell>0.8922</cell><cell>0.7820</cell><cell>0.8222</cell><cell>0.9025</cell></row><row><cell>SAM-SL</cell><cell>0.5565</cell><cell>0.0337</cell><cell>0.5453</cell><cell>0.7346</cell></row><row><cell>K-means</cell><cell>0.9240</cell><cell>0.8465</cell><cell>0.8703</cell><cell>0.9307</cell></row><row><cell>Single linkage</cell><cell>0.5453</cell><cell>0.0025</cell><cell>0.5444</cell><cell>0.7375</cell></row><row><cell>DBScan</cell><cell>0.8767</cell><cell>0.7529</cell><cell>0.7913</cell><cell>0.8838</cell></row><row><cell>Spectral clustering</cell><cell>0.5218</cell><cell>0.0246</cell><cell>0.4142</cell><cell>0.5867</cell></row><row><cell>CSM</cell><cell>0.5658</cell><cell>0.0585</cell><cell>0.5468</cell><cell>0.7333</cell></row><row><cell>CHAMELEON</cell><cell>0.5235</cell><cell>0.0279</cell><cell>0.5107</cell><cell>0.7007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Clustering performances on IRIS.</figDesc><table><row><cell>Method</cell><cell>Rand</cell><cell>Adjusted Rand</cell><cell>Jaccard</cell><cell>FM</cell></row><row><cell>SAM</cell><cell>0.9495</cell><cell>0.8858</cell><cell>0.8578</cell><cell>0.9234</cell></row><row><cell>SAM-No-Pruning</cell><cell>0.9075</cell><cell>0.7436</cell><cell>0.7298</cell><cell>0.8548</cell></row><row><cell>SAM-2-MST</cell><cell>0.9495</cell><cell>0.8858</cell><cell>0.8578</cell><cell>0.9234</cell></row><row><cell>SAM-SL</cell><cell>0.9495</cell><cell>0.8858</cell><cell>0.8578</cell><cell>0.9234</cell></row><row><cell>K-means</cell><cell>0.8797</cell><cell>0.7302</cell><cell>0.6959</cell><cell>0.8208</cell></row><row><cell>Single linkage</cell><cell>0.7766</cell><cell>0.5638</cell><cell>0.5891</cell><cell>0.7635</cell></row><row><cell>DBScan</cell><cell>0.8834</cell><cell>0.7388</cell><cell>0.7044</cell><cell>0.8268</cell></row><row><cell>Spectral clustering</cell><cell>0.8115</cell><cell>0.5745</cell><cell>0.5571</cell><cell>0.7156</cell></row><row><cell>CSM</cell><cell>0.8859</cell><cell>0.7455</cell><cell>0.7119</cell><cell>0.8321</cell></row><row><cell>CHAMELEON</cell><cell>0.7783</cell><cell>0.5492</cell><cell>0.5680</cell><cell>0.7369</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Clustering performances on WDBC.</figDesc><table><row><cell>Method</cell><cell>Rand</cell><cell>Adjusted Rand</cell><cell>Jaccard</cell><cell>FM</cell></row><row><cell>SAM</cell><cell>0.8138</cell><cell>0.6269</cell><cell>0.6981</cell><cell>0.8223</cell></row><row><cell>SAM-No-Pruning</cell><cell>0.8003</cell><cell>0.6091</cell><cell>0.6772</cell><cell>0.8101</cell></row><row><cell>SAM-2-MST</cell><cell>0.8012</cell><cell>0.6190</cell><cell>0.6811</cell><cell>0.8197</cell></row><row><cell>SAM-SL</cell><cell>0.5308</cell><cell>0.0032</cell><cell>0.5219</cell><cell>0.7162</cell></row><row><cell>K-means</cell><cell>0.7504</cell><cell>0.4914</cell><cell>0.6499</cell><cell>0.7915</cell></row><row><cell>Single linkage</cell><cell>0.5326</cell><cell>0.0024</cell><cell>0.5315</cell><cell>0.7286</cell></row><row><cell>DBScan</cell><cell>0.8691</cell><cell>0.7367</cell><cell>0.7828</cell><cell>0.8782</cell></row><row><cell>Spectral clustering</cell><cell>0.7479</cell><cell>0.4945</cell><cell>0.6133</cell><cell>0.7604</cell></row><row><cell>CSM</cell><cell>0.5860</cell><cell>0.1335</cell><cell>0.5392</cell><cell>0.7201</cell></row><row><cell>CHAMELEON</cell><cell>0.8365</cell><cell>0.6703</cell><cell>0.7406</cell><cell>0.8514</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the anonymous reviewers for their constructive comments and suggestions which helped improve the quality of this paper. The work of C. Zhong was supported by Zhejiang Provincial Natural Science Foundation of China, No. Y1090851, and the Center for International Mobility (CIMO). The work of D. Miao was supported by the National Natural Science Foundation of China, No. 60970061, No. 61075056, and the Research Fund for the Doctoral Program of Higher Education, No. 20060247039.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic subspace clustering of high dimensional data for datamining applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-SIGMOD Conference on the Management of Data</title>
		<meeting>ACM-SIGMOD Conference on the Management of Data</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Immune k-means and negative selection algorithms for data analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bereta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="1407" to="1425" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Some new indexes of cluster validity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. B. Cybern</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust path-based spectral clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="191" to="203" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A divide-and-merge methodology for clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1499" to="1525" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A time-efficient pattern reduction algorithm for k-means clustering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="716" to="731" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Density conscious subspace clustering for high-dimensional data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="16" to="30" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing redundancy in subspace clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1432" to="1446" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Corman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial data sets with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of finite mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="381" to="396" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast agglomerative clustering using a k-nearest neighbor graph</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fr√§nti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Virmajoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautam√§ki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1875" to="1881" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FLAME, a novel fuzzy clustering method for the analysis of DNA microarray data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Medico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustering aggregation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Annila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Disc. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CURE: an efficient clustering algorithm for large databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 ACM-SIGMOD International Conference Management of Data (SIGMOD&apos;98</title>
		<meeting>the 1998 ACM-SIGMOD International Conference Management of Data (SIGMOD&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ROCK: a robust clustering algorithm for categorical attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Data Engineering</title>
		<meeting>the IEEE Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning, Data Mining, Inference and Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Knowledge Discovery and Data Mining</publisher>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical clustering of mixed data based on distance hierarchy</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="4474" to="4492" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data clustering: a user&apos;s dilemma</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Machine Intelligence</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3776</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CHAMELEON: a hierarchical clustering algorithm using dynamic modeling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterative split-and-merge algorithm for VQ codebook generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaukoranta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fr√§nti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nevalainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2726" to="2732" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Step-wise clustering procedures</title>
		<author>
			<persName><forename type="first">B</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="86" to="101" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An agglomerative clustering algorithm using a dynamic k-nearest-neighbor list</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="1722" to="1734" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data clustering by minimizing disconnectivity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olafsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="732" to="746" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustering high dimensional data: a graph-based relaxed optimization approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Za√Øane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="4501" to="4511" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combining partitional and hierarchical algorithms for robust and efficient data clustering with cohesion self-merging</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multi-prototype clustering algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="689" to="698" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Berkeley Symposium on Mathematics, Statistics and Probability</title>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mixture Models: Inference and Application to Clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Basford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The effect of cluster size, dimensionality, and the number of clusters on recovery of true cluster structure</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Milligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Soon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sokol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CLARANS: a method for clustering objects for spatial data mining</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1003" to="1016" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new cluster validity index for the fuzzy c-mean</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Rezaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H C</forename><surname>Reiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="27" to="64" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H A</forename><surname>Sneath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Sokal</surname></persName>
		</author>
		<author>
			<persName><surname>Taxonomy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Francisco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The relative neighborhood graph of a finite planar set</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="261" to="268" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Theodorodis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kouttoumbas</surname></persName>
		</author>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>fourth ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A maximum variance cluster algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Backer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">STING: a statistical information grid approach to spatial datamining</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muntz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Very Large Data Bases</title>
		<meeting>the International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the equivalence of Cohen&apos;s kappa and the Hubert-Arabie Adjusted Rand index</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Warrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classif</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An optimal graph theoretic approach to data clustering: theory and its application to image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Survey of clustering algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="645" to="678" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Clustering gene expression data using a graph-theoretic approach: an application of minimum spanning tree</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="536" to="545" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Model-based clustering and data transformations for gene expression data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fraley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="977" to="987" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph-theoretical methods for detecting and describing gestalt clusters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Zahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
