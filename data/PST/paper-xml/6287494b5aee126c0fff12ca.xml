<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Answer-level Calibration for Free-form Multiple Choice Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
							<email>sawankumar@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Answer-level Calibration for Free-form Multiple Choice Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LM), trained on large corpora, have been shown to exhibit few-shot and zero-shot learning capability <ref type="bibr" target="#b16">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref> using only text interactions, as opposed to finetuning the model parameters using task specific training examples. Relying purely on text interactions for few-shot ability shifts the fo-cus to designing and utilizing suitable task-specific natural language templates.</p><p>In this work, we focus on free-form multiple choice question answering (and commonsense reasoning tasks in particular), where given a context and a set of choices of unspecified lengths, a model is required to select the most suitable choice. To enable zero-shot learning, the typical approach is to form textual sequences by concatenating the context independently with each choice and then scoring the concatenated strings using a pre-trained LM.</p><p>While LM probabilities have been shown to provide useful estimates of choice probabilities given a context, there is no incentive to treat the choices as equal in the absence of the associated context. For example, the LM probabilities in a neutral context are likely to be determined by frequency. In this work, we explore the role of biases that are likely to be associated with the choices naturally due to the language modeling objective. We propose ALC<ref type="foot" target="#foot_0">1</ref> (Answer-Level Calibration), where we use a neutral context to model such biases and remove them using a scaling factor determined by how similarly a model handles the question context as compared to a neutral context.</p><p>Further, we show that popular datasets favor models which rely on easy cues which are context independent. We use a bias-specific F1 score to analyze such biases. Our results indicate the need for answer-level calibration for more accurate estimates of model capabilities, or equivalently the design of better datasets. We hope our work will be useful for further research in both those directions. Specifically, we analyze context-independent biases related to length, part-of-speech (POS) and neutral context probabilities of the choices.</p><p>In summary, we make the following contributions:</p><p>1. We present ALC, a model-agnostic approach to improve the unsupervised performance of pretrained LMs for free-form multiple choice question answering, including commonsense reasoning tasks. 2. We show that popular datasets favor models relying on context-independent easy cues and demonstrate the need for answer-level calibration to better estimate model capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prompts <ref type="bibr" target="#b11">Jiang et al. (2020)</ref> show that manually created templates can be sub-optimal in extracting knowledge from LMs, and propose mining and paraphrasing-based approaches using training examples. Schick and Sch√ºtze (2021) highlight the importance of selecting templates for enabling fewshot learning.</p><p>Calibration Probabilities output by neural networks are known to suffer from lack of calibration <ref type="bibr" target="#b7">(Guo et al., 2017)</ref>, including LM output probabilities <ref type="bibr" target="#b2">(Braverman et al., 2020)</ref>. <ref type="bibr" target="#b25">Zhao et al. (2021)</ref> use token-level calibration to improve on few-shot classification and generation tasks. In contrast, we show that answer-level calibration is more suitable for the multiple choice setting that we consider. While we focus on free-form multiple choice questions in this work, when the choices are single tokens, for example in a classification task where the choices are True and False, answer-level calibration would behave similar to token-level calibration. As a result, answer-level calibration can be seen to have a more general scope as also illustrated empirically through our experiments.</p><p>Further, our analysis (Section 3.4) shows that answer-level calibration provides a more reliable measure of model performance on datasets with potential biases.</p><p>Finally, <ref type="bibr" target="#b10">Jiang et al. (2021)</ref> explore supervised methods, including finetuning as well as post-hoc methods, to improve calibration using training examples. In this work, we focus mainly on unsupervised calibration. <ref type="bibr" target="#b3">Brown et al. (2020)</ref> generally perform length normalization over the token probabilities for a choice, while observing that for a select few tasks they obtain performance gains when using an answer-level calibration scheme (which corresponds to the unscaled version in Equation 3 of ALC). They use task specific development sets to choose between length normalization and answer-level calibration which is undesirable for few-shot learning <ref type="bibr" target="#b12">(Kann et al., 2019)</ref>, and specifically for zero-shot learning. In this work, we show that unscaled calibration (as in Equation <ref type="formula">3</ref>) is suboptimal, compared to our proposed scaled version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer-level calibration</head><p>More recently, Holtzman et al. ( <ref type="formula">2021</ref>) also arrive at a formulation equivalent to the unscaled version of ALC but are motivated differently. Specifically, they hypothesize that the possibility of different surface forms of the same concept causes a competition between surface forms when scored by the LM. In contrast, we are motivated by calibration concerns and the presence of context-independent biases. We justify this motivation through bias associated evaluation (Section 5.2) for both the unscaled and scaled versions of ALC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative approaches using enhanced context</head><p>One way to make the probability estimates of the choices more accurate is to enhance the context using more task-specific cues. For example, <ref type="bibr" target="#b3">Brown et al. (2020)</ref> show that with just a few in-context examples, significant gains in performance can be obtained. At the same time, it has been shown that the order of examples as well as token-level calibration in such prompts can be critical for getting good performance <ref type="bibr" target="#b25">(Zhao et al., 2021;</ref><ref type="bibr" target="#b13">Kumar and Talukdar, 2021)</ref>.</p><p>While the gains from enhanced context through additional examples may be complementary to answer-level calibration, we focus on the zeroshot setting in this work. In the zero-shot setting, <ref type="bibr" target="#b20">Shwartz et al. (2020)</ref>, working on the question answering format, propose generating textual clarifications using the pre-trained LM itself, to enhance the context and improve zero-shot performance of pre-trained LM on commonsense reasoning tasks.</p><p>While their method has a much higher computational cost, we use it as an unsupervised baseline and show improvement over it on most tasks we consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALC: Proposed Method</head><p>We introduce the problem setting and notation in Section 3.1. We briefly describe our motivation in Section 3.2 and discuss the core idea of removing context-independent biases in Section 3.3. We provide the natural language formatting used in our experiments in Section A.3. We discuss bias associated measures in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We consider a problem setting where an example consists of a textual context C and K textual choices (or options) O k , k ‚àà [K], and we need to predict which choice O k fits best in context C. For example, in the case of question answering, this amounts to answering a question contained in the context C. Additionally, we define an instanceindependent neutral context C œÜ , where we expect all choices to be equally likely.</p><p>Denoting the gold answer by Y , the evaluation data is comprised of N instances defined by the tuples</p><formula xml:id="formula_0">(C i , [O i k ], Y i ), k ‚àà [K], i ‚àà [N ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motivation</head><p>Our main motivation is to evaluate the suitability of pretrained LMs for free-form multiple choice question answering where we contend that raw conditional phrase probabilities do not satisfy a natural requirement for such tasks (Equation <ref type="formula" target="#formula_2">2</ref>). We suggest and evaluate modifications to meet this requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Removing Context-independent Biases</head><p>We aim to obtain a probabilistic model M which provides estimates P M (O|C), the probability of a choice O given the context C. Predictions y for an example can subsequently be made using:</p><formula xml:id="formula_1">y = argmax k (P M (O k |C))<label>(1)</label></formula><p>We wish to build such a model using a pretrained LM, e.g., GPT2. Such a LM, trained on the task of next word prediction, is expected to provide estimates of word probabilities given a textual context. For example, given the sequence of words w 1 w 2 ...w i , we expect GPT2 to provide probability estimates P L (w i+1 |w 1 w 2 ...w i ). Applying chain rule, we can obtain estimates of phrases given a textual context. For example, we could obtain estimates of P L (O|C).</p><p>Can P L (O|C) serve as a proxy for P M (O|C)? It is tempting to expect the LM probabilities P L (O|C) to serve as a proxy for P M (O|C) when we can format the task in natural language. However, under the assumption that all choices O k are equally likely given a neutral context C œÜ , this approximation can be sub-optimal. For it to be optimal, we would need</p><formula xml:id="formula_2">P L (O 1 |C œÜ ) = P L (O 2 |C œÜ )... = P L (O K |C œÜ )<label>(2)</label></formula><p>However, given that these are task and instance specific choices, there is no incentive in the language modeling objective to ensure this condition.</p><p>To address this, we define a new score S L (O|C) to behave as expected with a neutral context:</p><formula xml:id="formula_3">S L (O k |C) = log P L (O k |C) ‚àí log P L (O k |C œÜ )</formula><p>(3) Predictions can subsequently be made using:</p><formula xml:id="formula_4">y = argmax k (S L (O k |C))<label>(4)</label></formula><p>Scaling the bias term: Equation 3, while desirable, makes a strong assumption about how the bias is present in the LM. While valid unquestionably for the neutral context, the bias in a trained (on task-specific data, or on a task-independent pretraining corpus) model is likely to depend on the context as well. For instance, a longer or more familiar context (in terms of similarity to training contexts) may mean the model is less reliant on context-independent cues. We therefore define a scaled version for removing biases, where the function g outputs the scaling term (ranging in [0, 1]):</p><formula xml:id="formula_5">S L (O k |C) = log P L (O k |C) ‚àí g(C, C œÜ ) * log P L (O k |C œÜ )<label>(5)</label></formula><p>We would want this formulation to preserve the requirement in Equation 2 which was satisfied by the unscaled version in Equation <ref type="formula">3</ref>. Specifically, we want g(C œÜ , C œÜ ) = 1 which would assign an equal score to each choice O k given a neutral context.</p><p>To get a model-agnostic<ref type="foot" target="#foot_1">2</ref> estimate of g, we think of log P L (O k |C) and log P L (O k |C œÜ ) as outputs from different models M and M œÜ respectively, and g as a measure of similarity between the models. Note that while M uses the available context C, M œÜ uses only the neutral context C œÜ . The intuition is that if M and M œÜ are identical, there is no new information provided by M and we want to set g(C, C œÜ ) = 1, leading to S L (O k |C) = 0. On the other hand, if M and M œÜ are very dissimilar, we can rely on the contextual scores of M and set g(C, C œÜ ) = 0, leading to</p><formula xml:id="formula_6">S L (O k |C) = log P L (O k |C)</formula><p>. Specifically, to estimate g, we compute a similarity metric between the token probabilities (across the model's entire vocabulary) output by the two models.</p><formula xml:id="formula_7">g(C, C œÜ ) = Sim(p f L (C), p f L (C œÜ ))<label>(6)</label></formula><p>where p f L indicates the probability vector output by the model across the vocabulary for the first token given the corresponding context. In this work, we consider Total Variation Distance (TVD), and Bhattacharyya Coefficient (BC) <ref type="bibr" target="#b0">(Bhattacharyya, 1943)</ref>.</p><p>When using TVD, we subtract it from 1, to obtain a similarity estimate:</p><formula xml:id="formula_8">g TVD (C, C œÜ ) = 1 ‚àí 0.5 * ||p f L (C) ‚àí p f L (C œÜ )|| 1<label>(</label></formula><p>7) while we directly use BC:</p><formula xml:id="formula_9">g BC (C, C œÜ ) = V i=1 p f L (C)[i] * p f L (C œÜ )[i] (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bias Associated Measures</head><p>Consider an instance and choice specific attribute</p><formula xml:id="formula_10">A i (O i k ) which can take values a j , j ‚àà [J].</formula><p>If we expect the attribute to be uncorrelated with task performance, we expect a model to perform similarly when evaluating subsets with different distributions of attributes A i (O k ) = a j . If a model relies on specific values of the attribute and if the evaluation data has sufficient representation of that value, standard evaluation metrics which ignore this attribute may provide an erroneous estimate of the model capability. As an extreme example, consider A(.) to denote whether the selected choice corresponds to the shortest choice among all choice O k , k ‚àà [K], with the attribute values being true/false. Assume then that the evaluation data is dominated by instances where A i (Y i ) = true, i.e., with a high probability, the correct answer in the evaluation data is the shortest choice. Consider also a model which always chooses the shortest choice, irrespective of the content. The model would return close to perfect scores using standard evaluation metrics such as accuracy against gold labels.</p><p>To analyze the impact of such attributes, we use a macro F1 score which takes into account the partitions created by an attribute. Recalling that an instance is represented by the tuple</p><formula xml:id="formula_11">(C i , [O i k ], Y i ), i ‚àà [N ],</formula><p>and letting ≈∂ i be the model prediction, we define precision (P), recall (R) and F1 scores for each attribute value a j , and subsequently an attribute specific macro F1 score (F1 A ).</p><formula xml:id="formula_12">P (A,a j ) = #{(A i ( ≈∂ i ) = a j ) &amp; ( ≈∂ i = Y i )} #{A i ( ≈∂ i ) = a j } (9) R (A,a j ) = #{(A i ( ≈∂ i ) = a j ) &amp; ( ≈∂ i = Y i )} #{A i (Y i ) = a j } (10) F1 (A,a j ) = 2 * P (A,a j ) * R (A,a j ) P (A,a j ) + R (A,a j )<label>(11)</label></formula><formula xml:id="formula_13">F1 A = Average({F1 (A,a j ) })<label>(12)</label></formula><p>where #{.} denotes the count of the corresponding set. If the model performs similarly irrespective of the attribute value, the macro F1 score F1 A is equal to the standard measure of accuracy:</p><formula xml:id="formula_14">Accuracy = #{ ≈∂ i = Y i } N (13)</formula><p>4 Experimental Setup</p><p>The datasets used and the corresponding prompts are described in Section 4.1. The LMs used are described in Section 4.2 and the baseline approaches in Section 4.3. Experimental results and analyses are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We used a series of commonsense reasoning tasks and evaluated on the publicly available development sets. We used the same versions of the data as <ref type="bibr" target="#b20">Shwartz et al. (2020)</ref> to allow for a direct comparison -COPA <ref type="bibr" target="#b6">(Gordon et al., 2012)</ref>, Common-senseQA <ref type="bibr" target="#b22">(Talmor et al., 2019)</ref>, MCTACO <ref type="bibr" target="#b26">(Zhou et al., 2019)</ref>, SocialIQA <ref type="bibr" target="#b18">(Sap et al., 2019)</ref>, PIQA <ref type="bibr" target="#b1">(Bisk et al., 2020)</ref>, WinoGrande <ref type="bibr" target="#b17">(Sakaguchi et al., 2020)</ref>. We also report on the adversarially generated large-scale SWAG dataset <ref type="bibr" target="#b24">(Zellers et al., 2018)</ref>. Further, we report on the AI2 Reasoning Challenge (ARC) <ref type="bibr" target="#b4">(Clark et al., 2018)</ref>, which has Easy and Challenge versions.</p><p>As a representative dialog understanding task, we report on the DREAM <ref type="bibr" target="#b21">(Sun et al., 2019)</ref> dataset.</p><p>Finally, we report on a recent benchmark introduced for measuring multitask accuracy of pretrained models (referred to as Hendrycks in the following) <ref type="bibr" target="#b8">Hendrycks et al. (2020)</ref>.</p><p>For MCTACO, we used a reduced subset as provided by <ref type="bibr" target="#b20">Shwartz et al. (2020)</ref> where each question Table <ref type="table">1</ref>: Standard evaluation results on unsupervised commonsense question answering tasks: (Top) Dev set accuracies (unless specified otherwise) with gpt2-xl are presented for baselines and ALC along with an unscaled version of ALC where the bias term is not scaled. The highest accuracies are marked in bold font. Note that while the unscaled version provides gains over the uncalibrated baseline, on the CommonsenseQA and SocialIQA tasks, there is also a drop in performance on some datasets, notably on PIQA. The scaled version, on the other hand, outperforms the LM-Baseline on all datasets except WinoGrande (on which all models perform close to majority accuracy). While token calibration improves over the majority accuracy on all datasets, it performs worse than the uncalibrated baseline. Finally, ALC outperforms or is competitive with Self-talk, while being computationally more efficient. (Middle) We also report on the gain over the uncalibrated baseline over different gpt2 variants and observe similar trends. (Bottom) Finally, we report on few-shot evaluation with gpt2-xl and again observe similar trends. Please see Section 5.1 for more details.</p><p>is associated with only one correct choice. For COPA, we also report on the test split due to the small size of COPA dev set. The sizes of the datasets used are reported in Appendix Table <ref type="table" target="#tab_7">8</ref>. All datasets contain questions in English language. We briefly describe these datasets in Section A.2.</p><p>Examples for each dataset along with contextual (C) and neutral (C œÜ ) prompts used in this work are captured in Section A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>We experiment with GPT2 <ref type="bibr" target="#b16">(Radford et al., 2019)</ref> variants -distilgpt2, gpt-small, gpt-medium, gpt2large and gpt2-xl. The size of models used is reported in Appendix Table <ref type="table" target="#tab_8">9</ref>. While the gpt-* models have been trained similarly as described in Radford et al. ( <ref type="formula">2019</ref>), distilgpt2 has been pretrained</p><p>with the supervision of GPT2<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b23">(Wolf et al., 2020)</ref>. For most of our experiments, we utilize the gpt2-xl model.</p><p>Please refer Section A.1 for additional details about the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Uncalibrated: Predictions are made using uncalibrated probabilities from a LM, log P L (O|C), computed as the sum of conditional log-probabilities output by the model for the tokens in O. Length normalized: Predictions are made using length-normalized probabilities from a LM, log P L (O|C), computed as the mean of conditional log-probabilities output by the model for the tokens in O. Table <ref type="table">3</ref>: Length bias analysis: We consider subsets of data where the shortest/longest choice is correct, and report on P, R and F1 scores (lowest values are underlined). We compare ALC against the uncalibrated as well as length normalized baselines. While length normalization is commonly used to overcome length bias, we find that it overcompensates and severely penalizes short answers (see recall with Shortest=true; the recall is lower than that for a random baseline). On the other hand, the uncalibrated baseline severely penalizes longer answers as expected. ALC improves on both subsets and provides a better alternative to length normalization. Please see Section 5.2.1 for details.</p><p>Self-talk: We use the official code repository 4 of self-talk <ref type="bibr" target="#b20">(Shwartz et al., 2020</ref>) using gpt2-xl as both the scoring model and the knowledge source. Token calibration: Following Zhao et al. ( <ref type="formula">2021</ref>), we use the probability vector output, p f N by the model at the first token given the neutral context to calibrate the model probabilities. Specifically, each token probability p is offset by p f N and renormalized: p = softmax(p ‚àí p f N ). We also tried an alternative variant suggested by <ref type="bibr" target="#b25">Zhao et al. (2021)</ref> where p = softmax(p/p f N ) but this generally did worse and we skip the corresponding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We aim to answer the following questions: 4 https://github.com/vered1986/self_ talk Q1 How does ALC compare with baselines using standard evaluation (accuracy) on free-form multiple choice question answering tasks? (Section 5.1) Q2 Does the aforementioned evaluation reflect true model capability? To answer this question, we perform a series of bias associated evaluations (see Section 3.4) and also evaluate whether ALC helps overcome such biases. Specifically, we evaluate on biases related to answer length, POS tag and context-ignorant LM probability. (Section 5.2) Q3 Does ALC improve expected calibration error <ref type="bibr" target="#b7">(Guo et al., 2017)</ref>? (Section 5.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Standard Evaluation</head><p>The overall results for the commonsense reasoning tasks (considered by Shwartz et al. ( <ref type="formula">2020</ref>)) using standard evaluation of ALC, as well as the base- Table <ref type="table">4</ref>: POS bias analysis: We consider subsets of data using the POS tag of the first token and report on P, R and F1 scores (lowest values are underlined). We limit to the larger subsets of nouns, verbs and adjectives (adj). We note that the uncalibrated baseline does worse on adjectives when compared to nouns and verbs. Both length normalized and ALC provide more even scores across POS tags. Please see Section 5.2.2 for details.</p><p>lines, with gpt2-xl are presented in Table <ref type="table">1</ref> (top).</p><p>We also report on an unscaled ablation of ALC. Note that ALC outperforms the uncalibrated baseline on all datasets except WinoGrande (where all models perform poorly and we drop it from further discussions). Further, the significant gains compared to token calibration (which generally does worse than the uncalibrated baseline) show that answer-level calibration is more suited for unsupervised commonsense question answering when there is no constraint on the lengths of candidate choices. Finally, ALC outperforms or is competitive with self-talk 5 while being significantly less computationally intensive. ALC requires scoring two strings (context input and neutral input) for each choice, while self-talk requires generating hundreds of clarification texts using data-dependent templates and subsequently scoring them. We also report on the average gain over the uncalibrated baseline across gpt2 models of varying sizes (Table <ref type="table" target="#tab_8">9</ref>) in Table <ref type="table">1</ref> (middle) and observe similar trends as in the case of gpt2-xl.</p><p>While our focus is zero-shot unsupervised evaluation, we also perform few-shot (1-shot and 4-shot) evaluation In general, for k-shot evaluation, we sample 100 sets of size k from an unseen split 6 of the dataset. A few-shot context is obtained by concatenating training examples with a newline token. We report the average performance on the evaluation set in Table <ref type="table">1</ref> (bottom) and observe similar trends as before.</p><p>We present the standard zero-shot evaluation on additional datasets in Table <ref type="table" target="#tab_1">2</ref>. The trends are sim-5 Please see Section A.4 for a note explaining the unusually high relative performance of baselines on some tasks when compared to self-talk.</p><p>6 For few-shot evaluation, we sample from the training split for all except COPA and MCTACO datasets where we sample from the dev set and report on the test set. ilar except for the SWAG (see Section 5.2 for an explanation) and the Hendrycks datasets (see Table <ref type="table">11</ref>).</p><p>Finally, while our focus is causal language models, we also present results using RoBERTa-large (a masked language model) in Table <ref type="table">10</ref>. Again, we observe similar trends.</p><p>In the subsequent sections, we show that the evaluation using the accuracy metric may not reveal true model capabilities as the datasets may favor models which utilize easy cues for predicting the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bias Associated Evaluation</head><p>Next, to gain a better understanding of the model capabilities, we analyze the performance associated with undesirable biases related to length, POS tag and context-ignorant LM probability. Specifically, we define the following attributes (see Section 3.4):</p><formula xml:id="formula_15">Shortest Attribute A i (O i k ) is set to true if O i k is the shortest (number of tokens) choice among the choices O i k , k ‚àà [K].</formula><p>Otherwise, the attribute is set to false. Longest Defined similar to Shortest, but set to true if O i k is the longest answer and false otherwise.</p><formula xml:id="formula_16">POS Attribute A i (O i k )</formula><p>is set to the POS tag of the first token in the choice O i k . We don't consider POS tags which occur less than a threshold (25) in the evaluation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM-Best Attribute</head><formula xml:id="formula_17">A i (O i k ) is set to true if O i</formula><p>k is the most likely choice using context-ignore (neutral input) LM probability. Otherwise, it is set to false. LM-Worst Defined similar to LM-Best, but set to true when O i k is the least likely choice and false otherwise.</p><p>Finally, we consider length-normalized versions of LM-Best and LM-Worst, referred to as LM- Table <ref type="table">5</ref>: Context-ignorant LM bias analysis: We consider subsets of data where the correct choice corresponds to the best/worst choice as per the context-ignorant (neutral context) LM probability and report on P, R and F1 scores (lowest values are underlined). Note that the F1 performance of uncalibrated and length-normalized baselines on PIQA and ARC (Easy) is much higher when LM-Best=true, i.e., when the correct choice is also the most likely choice without considering the context. An important takeaway here is that while standard evaluation did not distinguish ALC from the baselines, ALC is not overly reliant on context-ignorant LM probabilities. Please see Section 5.2.3 for more details.</p><p>Norm-Best and LM-Norm-Worst respectively. Briefly, our experiments reveal that while the datasets considered don't share a similar bias pattern, each usually suffers from at least one bias considered in this work, i.e., there is a drop in performance when measured using the bias associated score. We present the detailed results for commonsense reasoning tasks in Appendix Table <ref type="table" target="#tab_1">12</ref>, using gpt2-xl model, while highlighting the key takeaways here. Recall that in the absence of biases in the model, the F1 score should match the accuracy score.</p><p>In the following sections, we provide a more directed analysis on the presence of such biases, on datasets where such biases are most prominent, and if ALC helps alleviate such biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Length</head><p>We create subsets of the CommonsenseQA and SocialIQA dev set with specific properties to evaluate if the LM-Baseline has the associated biases and if they are addressed by ALC. First, we create subsets of examples where the shortest/longest answer is the correct answer. We expect longer sentences to have lower probabilities than shorter sentences with the uncalibrated baseline. Additionally, with the length normalized variant, where the final score is obtained as the mean of conditional log-probabilities instead of the sum (as in the uncalibrated baseline), longer sentences could potentially be favored. We report the uncalibrated and ALC's performance in Table <ref type="table">3</ref>. Note that both uncalibrated baseline and the length-normalized variants favor one subset at the cost of the other, while ALC improves on both. In particular, the uncalibrated baseline has a much poorer recall when the longest answer is correct. On the other hand, the length normalized variant has a much poorer recall when the shortest answer is correct. The results indicate that ALC provides a viable alternative to length normalization for handling length biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">POS</head><p>We analyze potential part of speech (POS) tag biases in Table <ref type="table">4</ref>. Considering the CommonsenseQA dataset, we create subsets of the data where the correct answer is of the POS tag noun, verb or adjective. Note that ALC shows less variation in performance (F1) across these subsets when compared to uncalibrated baseline while improving on each subset. In particular, the maximum difference in F1 scores is 8.1 for the uncalibrated baseline while it is 5.03 for ALC (BC). ALC also improves over the length normalized variant for each subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Context-ignorant LM Probability</head><p>To understand how much of the unsupervised performance comes from context-independent LM biases, we analyze subsets where the correct answer is most/least likely without the context. We report the performance on the PIQA and ARC datasets in Table <ref type="table">5</ref> and show that such biases indeed exist. The key takeaway is that the standard evaluation metrics may not give an accurate estimate of performance and that ALC provides more reliable estimates. Finally, we report macro F1 scores for LM-Norm-Best and LM-Norm-Worst evaluation in Table 6 on PIQA and SWAG datasets. The results indicate that the datasets favours length normalization aware scoring irrespective of the context. When we measure the bias associated score, ALC generally performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expected Calibration Error</head><p>Given a score S(O k |C) for each choice O k , we can compute a confidence estimate conf(O k |C) as:   </p><formula xml:id="formula_18">conf(O k |C) = e S(O k |C) k ‚àà[K] e S(O k |C)<label>(14)</label></formula><formula xml:id="formula_19">ECE = R r=1 |B r | N |acc(B r ) ‚àí conf(B r )| (15)</formula><p>where acc() and conf() measure the accuracy and mean confidence respectively in a bin. We set the number of bins to be 20. We report the average difference in accuracy and ECE compared to the uncalibrated baseline across the evaluation datasets (except WinoGrande) in Table 7. When compared to the uncalibrated baseline, ALC provides gains in calibration error while also improving performance. Length-normalization also improves ECE, presumably by correcting for length bias. However, length-normalization does not improve performance on an average. The relative performance gains of ALC can be explained through the handling of additional biases beyond length bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose ALC (Answer-Level Calibration), an unsupervised method to improve performance of pretrained language models. We show that, when compared to existing baselines, ALC is more suitable for free-form multiple choice question answering, including commonsense reasoning tasks. We also show that popular datasets favor models which rely on easy cues for predictions, and that ALC provides more reliable estimates of model capabilities by getting rid of some of these biases. Data Size COPA <ref type="bibr" target="#b6">(Gordon et al., 2012)</ref> 100 COPA-test <ref type="bibr" target="#b6">(Gordon et al., 2012)</ref> 499 CommonsenseQA <ref type="bibr" target="#b22">(Talmor et al., 2019)</ref> 1221 MCTACO <ref type="bibr" target="#b26">(Zhou et al., 2019)</ref> 454 SocialIQA <ref type="bibr" target="#b18">(Sap et al., 2019)</ref> 1954 PIQA <ref type="bibr" target="#b1">(Bisk et al., 2020)</ref> 1838 Winogrande <ref type="bibr" target="#b17">(Sakaguchi et al., 2020)</ref> 1267 ALC (Easy) <ref type="bibr" target="#b4">(Clark et al., 2018)</ref> 570 ALC (Challenge) <ref type="bibr" target="#b4">(Clark et al., 2018)</ref> 299 SWAG <ref type="bibr" target="#b24">(Zellers et al., 2018)</ref> 20000 DREAM <ref type="bibr" target="#b21">(Sun et al., 2019)</ref> 2040 Hendrycks <ref type="bibr" target="#b8">(Hendrycks et al., 2020)</ref> 14042 We leverage the transformers library <ref type="bibr" target="#b23">(Wolf et al., 2020)</ref> for accessing the LMs. All experiments were conducted using a single Nvidia GeForce GTX 1080 Ti Graphics Card. There was no training required. A typical experiment using gpt2-xl for CommonsenseQA task took around 15 minutes. The model sizes are captured in Table <ref type="table" target="#tab_8">9</ref>. Size of evaluation datasets is captured in Table <ref type="table" target="#tab_7">8</ref>.</p><p>We used the nltk pos-tagger with the universal tagset for pos-tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Datasets</head><p>COPA: The COPA dataset <ref type="bibr" target="#b6">(Gordon et al., 2012)</ref> contains a premise associated with two alternatives where one has a more plausible causal connection with the premise. There are two types of examples, depending on whether the connection is of "effect" or "cause". CommonsenseQA: The CommonsenseQA dataset <ref type="bibr" target="#b22">(Talmor et al., 2019)</ref> contains common sense questions extracted from ConceptNet <ref type="bibr" target="#b15">(Liu and Singh, 2004)</ref>. The alternative choices are made challenging by selecting from related concepts in Concept-Net or through suggestions through crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCTACO:</head><p>The MCTACO dataset <ref type="bibr" target="#b26">(Zhou et al., 2019)</ref> contains common sense questions related to understanding of time. Difficult adversarial candidates are selected using BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> predictions. SocialIQA: The SocialIQA dataset <ref type="bibr" target="#b18">(Sap et al., 2019)</ref> contains questions about social interactions with crowdsourced answers. PIQA: The PIQA dataset <ref type="bibr" target="#b1">(Bisk et al., 2020)</ref> contains questions about common sense. The question corresponds to a goal derived from an instruction website and the answers were crowdsourced. WinoGrande: The WinoGrande dataset <ref type="bibr" target="#b17">(Sakaguchi et al., 2020)</ref> is based on the Winograd Schema Challenge <ref type="bibr" target="#b14">(Levesque et al., 2012)</ref>, where a pair of sentences differ in one or two words containing a referential ambiguity. ARC: The ARC dataset <ref type="bibr" target="#b4">(Clark et al., 2018)</ref> contains natural grade-school science questions. The authors provide Easy and Challenge splits. The Challenge version is created using examples where retrieval-based and word-occurrence based methods fail <ref type="bibr" target="#b4">(Clark et al., 2018)</ref>. The Easy version contains the remaining questions. DREAM: DREAM (Dialogue-based REAding comprehension exaMination) <ref type="bibr" target="#b21">(Sun et al., 2019)</ref> provides a benchmark for reading comprehension focusing on multi-turn multi-party dialog understanding. SWAG: SWAG (Situations With Adversarial Generations) <ref type="bibr" target="#b24">(Zellers et al., 2018)</ref> provides a largescale dataset for grounded commonsense inference where different possible endings of a context are provided where the correct answer is derived from video captions while alternatives are adversarially generated. Hendrycks: Hendrycks et al. ( <ref type="formula">2020</ref>) provide a test suite containing 57 tasks to test the multitask accuracy of pretrained models. The tasks are broadly categorized into Humanities, STEM, Social Sciences and Other. We run our experiments on subsets associated with these categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data Formatting</head><p>In this section, we provide the formatting used to convert task-specific examples into natural language prompts as used in our experiments. We first give examples of the Context (if any), the Question and Choices as present in the corresponding dataset, followed by the Context input and Neutral input as fed to the pretrained LM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc><ref type="bibr" target="#b7">Guo et al. (2017)</ref> compute expected calibration error (ECE) by partitioning N confidence predictions into R equal bins B r , r ‚àà [1, R] and computing the weighted average of the absolute difference between the confidence and accuracy in each bin:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Standard evaluation results on additional tasks: Dev set accuracies (unless specified otherwise) are reported. The trends are similar to Table1, except for SWAG (see Section 5.2 and Table 6 for an explanation). Please see Section 5.1 for more details.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Easy</cell><cell cols="2">ARC Challenge</cell><cell cols="3">DREAM SWAG</cell><cell cols="3">Hendrycks-test Humanities STEM Social sciences Other</cell></row><row><cell cols="2">Token calibration</cell><cell>35.09</cell><cell>20.40</cell><cell></cell><cell>40.20</cell><cell cols="2">29.53</cell><cell></cell><cell>23.38</cell><cell>22.70</cell><cell>25.45</cell><cell>25.51</cell></row><row><cell cols="2">Uncalibrated</cell><cell>58.25</cell><cell>27.76</cell><cell></cell><cell>48.14</cell><cell cols="2">49.30</cell><cell></cell><cell>26.99</cell><cell>24.16</cell><cell>31.52</cell><cell>31.55</cell></row><row><cell cols="3">Length normalized 50.70</cell><cell>29.43</cell><cell></cell><cell>48.77</cell><cell cols="2">65.36</cell><cell></cell><cell>29.33</cell><cell>26.47</cell><cell>30.84</cell><cell>32.85</cell></row><row><cell></cell><cell>Unscaled</cell><cell>53.33</cell><cell>33.11</cell><cell></cell><cell>52.99</cell><cell cols="2">57.04</cell><cell></cell><cell>31.05</cell><cell>29.13</cell><cell>32.76</cell><cell>35.26</cell></row><row><cell>ALC</cell><cell>TVD</cell><cell>60.00</cell><cell>29.43</cell><cell></cell><cell>52.50</cell><cell cols="2">53.77</cell><cell></cell><cell>28.80</cell><cell>25.98</cell><cell>32.24</cell><cell>33.07</cell></row><row><cell></cell><cell>BC</cell><cell>56.49</cell><cell>33.78</cell><cell></cell><cell>53.14</cell><cell cols="2">59.16</cell><cell></cell><cell>30.31</cell><cell>27.60</cell><cell>32.60</cell><cell>34.58</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>P</cell><cell cols="2">Shortest=true R</cell><cell>F1</cell><cell></cell><cell>P</cell><cell>Longest=True R</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Commonsenseqa</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>213</cell><cell></cell><cell></cell><cell></cell><cell>266</cell></row><row><cell></cell><cell cols="2">Uncalibrated</cell><cell></cell><cell cols="2">25.94</cell><cell>51.64</cell><cell cols="2">34.54</cell><cell>49.63</cell><cell>25.19</cell><cell>33.42</cell></row><row><cell></cell><cell cols="3">Length normalized</cell><cell cols="2">41.3</cell><cell>17.84</cell><cell cols="2">24.92</cell><cell>28.94</cell><cell>55.26</cell><cell>37.98</cell></row><row><cell></cell><cell cols="2">ALC (Unscaled)</cell><cell></cell><cell cols="2">48.91</cell><cell>31.46</cell><cell cols="2">38.29</cell><cell>46.2</cell><cell>57.14</cell><cell>51.09</cell></row><row><cell></cell><cell cols="2">ALC (BC)</cell><cell></cell><cell cols="2">42.27</cell><cell>38.5</cell><cell cols="2">40.29</cell><cell>51.16</cell><cell>49.62</cell><cell>50.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SocialIQA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>665</cell><cell></cell><cell></cell><cell></cell><cell>667</cell></row><row><cell></cell><cell cols="2">Uncalibrated</cell><cell></cell><cell cols="2">38.27</cell><cell>68.72</cell><cell cols="2">49.17</cell><cell>48.36</cell><cell>15.21</cell><cell>23.15</cell></row><row><cell></cell><cell cols="3">Length normalized</cell><cell cols="2">51.82</cell><cell>10.68</cell><cell cols="2">17.71</cell><cell>38.69</cell><cell>78.29</cell><cell>51.78</cell></row><row><cell></cell><cell cols="2">ALC (Unscaled)</cell><cell></cell><cell cols="2">49.2</cell><cell>27.82</cell><cell cols="2">35.54</cell><cell>40.76</cell><cell>58.94</cell><cell>48.19</cell></row><row><cell></cell><cell cols="2">ALC (BC)</cell><cell></cell><cell cols="2">47.92</cell><cell>38.05</cell><cell cols="2">42.41</cell><cell>44.06</cell><cell>52.58</cell><cell>47.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Expected Calibration Error: Mean (and standard deviation) of difference with the uncalibrated baseline in accuracy and ECE over different evaluation datasets are reported. ALC improves both ECE and accuracy. Please see Section 5.3 for more details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Number of examples in the datasets used</figDesc><table><row><cell>Model</cell><cell>Size (Million parameters)</cell></row><row><cell>distilgpt2</cell><cell>82</cell></row><row><cell>gpt2-small</cell><cell>117</cell></row><row><cell>gpt2-medium</cell><cell>345</cell></row><row><cell>gpt2-large</cell><cell>774</cell></row><row><cell>gpt2-xl</cell><cell>1558</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Size of models used</figDesc><table><row><cell>A Appendix</cell></row><row><cell>A.1 Experimental Setup</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">ALC source code is available at https://github.com/SawanKumar28/alc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">By model-agnostic, we mean we only access the probabilities output by the model and don't rely on any knowledge of the model architecture.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://huggingface.co/distilgpt2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3">Timo Schick and Hinrich Sch√ºtze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">https://github.com/vered1986/self_ talk/issues/1</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While it seems surprising that the self-talk results in Table <ref type="table">1</ref> are generally lower than the uncalibrated baseline, we note that we haven't underestimated the performance of self-talk. Self-talk performance was obtained using the official repository of the project and the results align well with those reported in the original work. What has changed is the performance of the baseline, which is higher here (which in turn shows the significane of the numbers reported in this work). We note two differences with respect to the self-talk repository. First, self-talk uses a length-normalized baseline, while we evaluate both uncalibrated and lengthnormalized baselines. Second, there is a bug in the self-talk repository regarding calculating baseline performance, also noted in a GitHub issue 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Results</head><p>We show results across commonsense reasoning datasets for bias-associated F1 scores in Table <ref type="table">12</ref>: Overall bias associated evaluation results: We present bias-associated F1 scores for each attribute considered. We note that ALC consistently performs better or as competitive with the baselines. Please see Section 5.2 for details.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On a measure of divergence between two statistical populations defined by their probability distributions</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Calcutta Math. Soc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Calibration, entropy rates, and memory in language models</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1089" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
				<meeting><address><addrLine>Montr√©al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012. SemEval 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surface form competition: Why the highest probability answer isn&apos;t always right</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.564</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7038" to="7051" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How can we know when language models know? on the calibration of language models for question answering</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00407</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="962" to="977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards realistic practices in lowresource natural language processing: The development set</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1329</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3342" to="3349" />
		</imprint>
	</monogr>
	<note>Bowman</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reordering examples helps during priming-based few-shot learning</title>
		<author>
			<persName><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.395</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4507" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conceptnet-a practical commonsense reasoning tool-kit</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8732" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social IQa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised commonsense question answering with self-talk</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4615" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dream: A challenge data set and models for dialogue-based reading comprehension</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1 (Long and Short Papers)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1 (Long and Short Papers)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">going on a vacation&quot; takes longer than &quot;going for a walk&quot;: A study of temporal commonsense understanding</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3363" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
