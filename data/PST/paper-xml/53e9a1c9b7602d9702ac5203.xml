<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Convex Variational Model for Restoring Blurred Images with Multiplicative Noise *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiqiu</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Mathematics and Computer Science</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<postCode>2800</postCode>
									<settlement>Kgs. Lyngby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tieyong</forename><surname>Zeng</surname></persName>
							<email>zeng@hkbu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Kowloon Tong, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">FRGs of Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Convex Variational Model for Restoring Blurred Images with Multiplicative Noise *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF592ACF4D84A50CCBBBFB91A6509553</idno>
					<idno type="DOI">10.1137/120870621</idno>
					<note type="submission">Received by the editors March 20, 2012; accepted for publication (in revised form) May 29, 2013; published electronically August 22, 2013. This work was supported by RGC 211710, RGC 211911, NSFC 11271049, and the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convexity</term>
					<term>deblurring</term>
					<term>multiplicative noise</term>
					<term>primal-dual algorithm</term>
					<term>total variation regularization</term>
					<term>variational model AMS subject classifications. 52A41</term>
					<term>65K10</term>
					<term>65K15</term>
					<term>90C30</term>
					<term>90C47</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a new variational model for restoring blurred images with multiplicative noise is proposed. Based on the statistical property of the noise, a quadratic penalty function technique is utilized in order to obtain a strictly convex model under a mild condition, which guarantees the uniqueness of the solution and the stabilization of the algorithm. For solving the new convex variational model, a primal-dual algorithm is proposed, and its convergence is studied. The paper ends with a report on numerical tests for the simultaneous deblurring and denoising of images subject to multiplicative noise. A comparison with other methods is provided as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>In real applications, degradation effects are unavoidable during image acquisition and transmission. For instance, the photos produced by astronomical telescopes are often blurred by atmospheric turbulence. In order to further improve image processing tasks, image deblurring and denoising continue to attract attention in the applied mathematics community. Based on the imaging systems, various kinds of noise were considered, such as additive Gaussian noise, impulse noise, and Poisson noise. We refer the reader to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> and references therein for an overview of those noise models and the restoration methods. However, multiplicative noise is a different noise model, and it commonly appears in synthetic aperture radar (SAR), ultrasound imaging, laser images, and so on <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. For a mathematical description of such degradations, suppose that an image û is a real function defined on Ω, a connected bounded open subset of R 2 with compact Lipschitz boundary, i.e., û : Ω → R. The degraded image f is given by <ref type="bibr">(1.1)</ref> f = (Aû)η, where A ∈ L(L 2 (Ω)) is a known linear and continuous blurring operator and η ∈ L 2 (Ω) represents multiplicative noise with mean 1. Here, f is obtained from û, which is blurred by the blurring operator A, and then is corrupted by the multiplicative noise η. Usually we assume that f &gt; 0. In this paper, we concentrate on the assumption that η follows a Gamma distribution, which commonly occurs in SAR. The deblurring process under noise is well known to be an ill-posed problem in the sense of Hadamard <ref type="bibr" target="#b29">[30]</ref>. Since the degraded image provides only partial restrictions on the original data, there exist various solutions which can match the observed degraded image under the given blurring operator. Hence, in order to utilize variational methods, the main challenge in image restoration is to design a reasonable and easily solved optimization problem based on the degradation model and the prior information on the original image. Until the past decade, a few variational methods have been proposed to handle the restoration problem with the multiplicative noise. Given the statistical properties of the multiplicative noise η, in <ref type="bibr" target="#b38">[39]</ref> the recovery of the image û was based on solving the following constrained optimization problem:</p><formula xml:id="formula_0">(1.2) inf u∈S(Ω) Ω |Du| subject to Ω f Au dx = 1, Ω f Au -1 2 dx = θ 2 ,</formula><p>where θ 2 denotes the variance of η, S(Ω) = {v ∈ BV (Ω) : v &gt; 0}, BV (Ω) is the space of functions of bounded variation (see <ref type="bibr" target="#b5">[6]</ref> or below), and the total variation (TV) of u is utilized as the objective function in order to preserve significant edges in images. In (1.2), only basic statistical properties, the mean and the variance, of the noise η are considered, which somehow limits the restored results. For this reason, based on the Bayes rule and Gamma distribution with mean 1, by using a maximum a posteriori (MAP) estimator, Aubert and Aujol <ref type="bibr" target="#b4">[5]</ref> introduced a variational model as follows:</p><p>(1.3) inf</p><formula xml:id="formula_1">u∈S(Ω) Ω log(Au) + f Au dx + λ Ω |Du|,</formula><p>where the TV of u is utilized as the regularization term and λ &gt; 0 is the regularization parameter which controls the trade-off between a good fit of f and a smoothness requirement due to the TV regularization. Below we refer to <ref type="bibr">(1.3)</ref> as the AA model. Since both (1.2) and (1.3) are nonconvex, the gradient projection algorithms proposed in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b4">[5]</ref> may stick at some local minimizers, and the restored results strongly rely on the initialization and the numerical schemes. To overcome this problem and provide a convex model, in <ref type="bibr" target="#b31">[32]</ref> Huang, Ng, and Wen introduced an auxiliary variable z = log u in (1.3), and in <ref type="bibr" target="#b40">[41]</ref> Shi and Osher modified (1.3) by adding a quadratic term. With convex models, these two methods both provide better restored results than the method proposed in <ref type="bibr" target="#b4">[5]</ref>, and they are independent of the initial estimations. In addition, Steidl and Teubner <ref type="bibr" target="#b41">[42]</ref> combined the I-divergence as the data fidelity term with the TV regularization or the nonlocal means to remove the multiplicative Gamma noise. A general patch-based denoising filter was proposed in <ref type="bibr" target="#b19">[20]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, the denoising problem was handled by using a L 1 fidelity term on frame coefficients. In <ref type="bibr" target="#b35">[36]</ref>, the approach with spatially varying regularization parameters in the AA model <ref type="bibr">(1.3)</ref> was considered in order to restore more texture details of the denoised image. In <ref type="bibr" target="#b30">[31]</ref>, the multiplicative noise Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php removal was addressed via a learned dictionary, and extensive experimental results illustrated the leading performance of this approach. However, all of the above methods work only on the multiplicative noise removal issue, and it is still an open question to extend them to the deblurring case.</p><p>In this paper, we focus on the restoration of images that are simultaneously blurred and corrupted by multiplicative noise. Since the nonconvexity of the model <ref type="bibr">(1.3)</ref> proposed in <ref type="bibr" target="#b4">[5]</ref> causes a uniqueness problem and the issue of convergence of the numerical algorithm, we introduce a new convex model by adding a quadratic penalty term based on the statistical properties of the multiplicative Gamma noise. Furthermore, we study the existence and uniqueness of a solution to the new model on the continuous, i.e., functional space, level. Here, we still use the TV regularization in order to preserve edges during the reconstruction. Evidently, it can be readily extended to some other modern regularization terms such as nonlocal TV <ref type="bibr" target="#b25">[26]</ref> or the framelet approach <ref type="bibr" target="#b11">[12]</ref>. The minimization problem in our method is solved by the primal-dual algorithm proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> instead of the gradient projection method in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. The numerical results in this paper show that our method has the potential to outperform the other approaches in multiplicative noise removal with deblurring simultaneously.</p><p>The rest of the paper is organized as follows. In section 2, we briefly review the TV regularization and provide its main properties. In section 3, based on the statistical properties of the multiplicative Gamma noise, we propose a new convex model for denoising and study the existence and uniqueness of a solution with several other properties. Then in section 4 we extend the model and those properties to the case of denoising and deblurring simultaneously. Section 5 gives the primal-dual algorithm for solving our restoration model based on the work proposed in <ref type="bibr" target="#b14">[15]</ref>. The numerical results shown in section 6 demonstrate the efficiency of the new method. Finally, conclusions are drawn in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of TV regularization.</head><p>In order to preserve significant edges in images, in their seminal work <ref type="bibr" target="#b39">[40]</ref> Rudin, Lions, and Osher, introduced TV regularization into image restoration. In this approach, they recover the image in BV (Ω), which denotes the space of functions of bounded variation (BV); i.e., u ∈ BV (Ω) iff u ∈ L 1 (Ω) and the BV-seminorm, (2.1)</p><formula xml:id="formula_2">Ω |Du| := sup Ω u • div(ξ(x))dx ξ ∈ C ∞ 0 (Ω, R 2 ), ξ L ∞ (Ω,R 2 ) ≤ 1 ,</formula><p>is finite. The space BV (Ω) endowed with the norm u BV = u L 1 + Ω |Du| is a Banach space. If u ∈ BV (Ω), the distributional derivative Du is a bounded Radon measure, and the above quantity defined in (2.1) corresponds to the total variation (TV). Based on the compactness of BV (Ω), in the two-dimensional case we have the embedding BV (Ω) → L p (Ω) for 1 ≤ p ≤ 2, which is compact for p &lt; 2. See <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A convex multiplicative denoising model.</head><p>To propose a convex multiplicative denoising model, we start from the multiplicative Gamma noise. Suppose that the random variable η follows a Gamma distribution; i.e., its probability density function (PDF) is</p><formula xml:id="formula_3">(3.1) p η (x; θ, K) = 1 θ K Γ(K)</formula><p>x K-1 e - where Γ is the usual Gamma-function, and θ and K denote the scale and shape parameters, respectively, in the Gamma distribution. Furthermore, the mean of η is Kθ, and the variance of η is Kθ 2 ; see <ref type="bibr" target="#b28">[29]</ref>. As multiplicative noise, in general we assume that the mean of η equals 1; then we have that Kθ = 1 and its variance is</p><formula xml:id="formula_4">1 K . Now, set a random variable Y = 1 √ η , whose PDF is (3.2) p Y (y) = 2 θ K Γ(K) y -2K-1 e -1<label>θy</label></formula><formula xml:id="formula_5">2</formula><p>for y ≥ 0.</p><p>We can prove the following properties of Y . Proposition 3.1. Suppose that the random variable η follows a Gamma distribution with mean 1.</p><formula xml:id="formula_6">Set Y = 1 √ η . Then the means of Y and Y 2 are E(Y ) = √ KΓ(K -1/2) Γ(K) and E(Y 2 ) = K K -1 ,</formula><p>respectively. Furthermore, we have</p><formula xml:id="formula_7">E(Y n ) = K n 2 Γ(K -n 2 ) Γ(K) for any n ∈ N.</formula><p>Proof. Based on the PDF of η shown in (3.1) and Kθ = 1, we obtain</p><formula xml:id="formula_8">E(Y ) = +∞ 0 1 √ x 1 θ K Γ(K) x K-1 e -x θ dx = Γ(K -1/2) √ θΓ(K) +∞ 0 1 θ K-1/2 Γ(K -1/2) x K-3/2 e -x θ dx = Γ(K -1/2) √ θΓ(K) +∞ 0 p η (x; θ, K -1/2) dx = √ KΓ(K -1/2) Γ(K) .</formula><p>Similarly, we have</p><formula xml:id="formula_9">E(Y 2 ) = +∞ 0 1 x 1 θ K Γ(K) x K-1 e -x θ dx = Γ(K -1) θΓ(K) +∞ 0 1 θ K-1 Γ(K -1) x K-2 e -x θ dx = K K -1 .</formula><p>Further, for any n ∈ N we can calculate E(Y n ) similarly, and obtain</p><formula xml:id="formula_10">E(Y n ) = K n 2 Γ(K -n 2 ) Γ(K)</formula><p>. According to the properties of the Gamma function <ref type="bibr" target="#b3">[4]</ref> and Proposition 3.1, if K is large, the mean of Y is close to 1. Furthermore, we have the following result.</p><p>Proposition 3.2. Suppose that the random variable η follows a Gamma distribution with mean 1. Set Y = 1 √ η . Then</p><formula xml:id="formula_11">(3.3) lim K→+∞ E((Y -1) 2 ) = 0.</formula><p>Proof. With Proposition 3.1, we readily have</p><formula xml:id="formula_12">E((Y -1) 2 ) = K K -1 - 2 √ KΓ(K -1/2) Γ(K) + 1.</formula><p>Based on the property of the Gamma function (see <ref type="bibr" target="#b3">[4]</ref>),</p><formula xml:id="formula_13">lim K→+∞ Γ(K + α) Γ(K)K α = 1 ∀α ∈ R; taking α = -1 2 , we can immediately obtain lim K→+∞ E((Y -1) 2 ) = 0.</formula><p>Proposition 3.2 ensures that the value of E((Y -1) 2 ) is small with a large K. On the other hand, from Table <ref type="table" target="#tab_1">1</ref> we can see that, even with a small K, its value is still rather small. Remark 3.3. Since p Y (y) defined in (3.2) is monotonic on either side of the single maximal point y = 1 -θ 2 , it is a strictly unimodal function. In statistics, a Gaussian distribution is commonly used to approximate the unimodal or other distributions; see <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. To further understand the relationship between Y and a Gaussian distribution, we need the concept of the Kullback-Leibler (KL) divergence. Suppose that P and Q are continuous random variables with the PDFs p(x) and q(x), respectively. The KL divergence of P and Q is defined as</p><formula xml:id="formula_14">D KL (P ||Q) := ∞ -∞ log p(x) q(x) p(x) dx,</formula><p>which is always nonnegative and is zero iff P = Q almost everywhere; see <ref type="bibr" target="#b6">[7]</ref>. Based on the PDF of the random variable Y , we can prove that the KL divergence of Y with the Gaussian distribution N (μ K , σ 2 K ) tends to 0 when K goes to infinity, where μ K and σ 2 K are the mean and variance of Y , respectively; i.e., <ref type="bibr">(3.4</ref>)  (i</p><formula xml:id="formula_15">μ K := E(Y ) and σ K := E(Y 2 ) -(E(Y ))</formula><formula xml:id="formula_16">) +∞ 0 p Y (y) log p Y (y)dy = log 2 -log( √ KΓ(K)) + 2K+1 2 ψ(K) -K, where ψ(K) := d log Γ(K) dK</formula><p>is the digamma function (see <ref type="bibr" target="#b1">[2]</ref>); (ii</p><formula xml:id="formula_17">) +∞ 0 p Y (y) log p N (μ K ,σ 2 K ) (y)dy = -1 2 log(2πeσ 2 K ), where p N (μ K ,σ 2 K ) (y) denotes the PDF of the Gaussian distribution N (μ K , σ 2 K ); (iii) lim K→+∞ D KL (Y ||N (μ K , σ 2 K )) = 0. Proof.</formula><p>For the proof, see Appendix I (section 8). Furthermore, let us consider a rescaled version of Y , the random variable Z := Y -μ K σ K . When K goes to infinity, Y degenerates to 1; however, Z tends to follow the Gaussian distribution N (0, 1). Further, based on Proposition 3.4, we have lim</p><formula xml:id="formula_18">K→+∞ D KL (Z||N (0, 1)) = lim K→+∞ D KL (Y ||N (μ K , σ 2 K )) = 0.</formula><p>Using Proposition 3.4 and the definition of the KL divergence, in Table <ref type="table" target="#tab_1">1</ref> we list the KL divergence between Y and the Gaussian distribution N (μ K , σ 2 K ) with respect to different values of K. Obviously, even with small K, D KL (Y ||N (μ K , σ 2 K )) is already very small. In addition, in Figure <ref type="figure" target="#fig_0">1</ref> we show the PDFs of Y and N (μ K , σ 2 K ) with K = 6, K = 10, and K = 30. We can see that the PDF of Y decreases quickly and is close to symmetry as K increases, which are the essential properties of Gaussian distribution.</p><p>In the denoising case, that is, where A is the identity operator, from the degradation model (1.1) we obtain that Y = 1 √ η = u f . Inspired by (3.3), we introduce a quadratic penalty term into the AA model (1.3), which turns out to be</p><formula xml:id="formula_19">(3.5) inf u∈ S(Ω) E(u) := Ω log u + f u dx + α Ω u f -1 2 dx + λ Ω |Du|</formula><p>with the penalty parameter α &gt; 0. In addition, we set</p><formula xml:id="formula_20">S(Ω) := {v ∈ BV (Ω) : v ≥ 0},</formula><p>which is closed and convex, and we define log 0 = -∞ and 1 0 = +∞. Note that as f &gt; 0, we need not define 0 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Existence and uniqueness of a solution.</head><p>For the existence and uniqueness of a solution to (3.5), we start with discussing the convexity of the model. Since the quadratic penalty term provides extra convexity, we prove that E(u) in (3.5) is convex if the parameter α satisfies certain condition.</p><p>Proposition 3.5.</p><formula xml:id="formula_21">If α ≥ 2 √ 6</formula><p>9 , then the model (3.5) is strictly convex. Proof. With t ∈ R + and a fixed α, we define a function g as</p><formula xml:id="formula_22">g(t) := log t + 1 t + α( √ t -1) 2 .</formula><p>Easily, we have that the second order derivative of g satisfies</p><formula xml:id="formula_23">g (t) = -t -2 + 2t -3 + α 2 t -3 2 .</formula><p>A direct computation shows that the function t 3 2 g (t) reaches its unique minimum, 9α-2</p><formula xml:id="formula_24">√ 6 18 , at t = 6. Hence, if α ≥ 2 √ 6</formula><p>9 , we have g (t) ≥ 0; i.e., g is convex. Furthermore, since the function g has only one minimizer, g is strictly convex when α ≥ 2</p><formula xml:id="formula_25">√ 6 9 . Setting t = u(x) f (x)</formula><p>for each x ∈ Ω, we obtain the strict convexity of the first two terms in <ref type="bibr">(3.5)</ref>. Based on the convexity of the TV regularization, we deduce that</p><formula xml:id="formula_26">E(u) in (3.5) is strictly convex if α ≥ 2 √ 6</formula><p>9 . Since the feasible set S(Ω) is convex, the assertion is an immediate consequence.</p><p>Based on Proposition 3.5, we see that, with a suitable α, (3.5) is a convex approximation of the nonconvex model (1.3) with A as the identity operator. Now, we argue the existence and uniqueness of a solution to (3.5) and the minimum-maximum principle.</p><p>Theorem 3.6.</p><formula xml:id="formula_27">Let f be in L ∞ (Ω) with inf Ω f &gt; 0; then the model (3.5) has a solution u * in BV (Ω) satisfying 0 &lt; inf Ω f ≤ u * ≤ sup Ω f. Moreover, if α ≥ 2 √ 6</formula><p>9 , the solution of (3.5) is unique. Proof. Set c 1 := inf Ω f , c 2 := sup Ω f , and let</p><formula xml:id="formula_28">E 0 (u) := Ω log u + f u dx + α Ω u f -1 2 dx.</formula><p>For each fixed x ∈ Ω, easily we have log t</p><formula xml:id="formula_29">+ f (x) t ≥ 1 + log f (x) with t ∈ R + ∪ {0}. Noting that E(u) is defined in (3.5), we thus have E(u) ≥ Ω log u + f u dx ≥ Ω (1 + log f ) dx.</formula><p>In other words, E(u) in (3.5) is bounded from below, and we can choose a minimizing sequence {u n ∈ S(Ω) : n = 1, 2, . . .}.</p><p>Since for each fixed x ∈ Ω the real function on R + ∪ {0},</p><formula xml:id="formula_30">g(t) := log t + f (x) t + α t f (x) -1 2 ,</formula><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_31">is decreasing if t ∈ [0, f (x)</formula><p>) and increasing if t ∈ (f (x), +∞), one always has g(min(t, M )) ≤ g(t) with M ≥ f (x). Hence, we obtain that</p><formula xml:id="formula_32">E 0 (inf(u, c 2 )) ≤ E 0 (u). Combining this with Ω |D inf(u, c 2 )| ≤ Ω |Du| (see Lemma 1 in section 4.3 of [34]), we have E(inf(u, c 2 )) ≤ E(u).</formula><p>In the same way we are able to get E(sup(u, c 1 )) ≤ E(u). Hence, we can assume that 0</p><formula xml:id="formula_33">&lt; c 1 ≤ u n ≤ c 2 , which implies that u n is bounded in L 1 (Ω).</formula><p>As {u n } is a minimizing sequence, we know that E(u n ) is bounded. Furthermore, Ω |Du n | is bounded, and {u n } is bounded in BV (Ω). Therefore, there exists a subsequence {u n k } which converges strongly in L 1 (Ω) to some u * ∈ BV (Ω), and {Du n k } converges weakly as a measure to Du * <ref type="bibr" target="#b5">[6]</ref>. Since S(Ω) is closed and convex, by the lower semicontinuity of the TV and Fatou's lemma, we get that u * is a solution of the model (3.5), and necessarily 0</p><formula xml:id="formula_34">&lt; c 1 ≤ u * ≤ c 2 . Moreover, if α ≥ 2 √ 6</formula><p>9 , uniqueness follows directly from the strict convexity of the function E.</p><p>As for Theorem 4.1 in <ref type="bibr" target="#b4">[5]</ref>, here we also need the assumption inf Ω f &gt; 0, which ensures the lower boundedness of E(u) defined in <ref type="bibr">(3.5)</ref>. In numerical practice, this assumption is always kept by using max(f, ) as the observed image with a very small positive value .</p><p>In <ref type="bibr" target="#b4">[5]</ref> a comparison principle was given concerning the model <ref type="bibr">(1.3)</ref>. With the α-term in (3.5), it is satisfied with certain conditions on α as well.</p><p>Proposition 3.7. Let f 1 and f 2 be in L ∞ (Ω) with a 1 &gt; 0 and a 2 &gt; 0, where</p><formula xml:id="formula_35">a 1 = inf Ω f 1 and a 2 = inf Ω f 2 . Further, set b 1 = sup Ω f 1 and b 2 = sup Ω f 2 . Assume f 1 &lt; f 2 . Suppose u * 1 (resp., u * 2 ) is a solution of (3.5) with f = f 1 (resp., f = f 2 ). Then when α &lt; a 1 a 2 b 1 b 2 -a 1 a 2 , we have u * 1 ≤ u * 2 . Proof. Referring to [5], we define u ∧ v = inf(u, v) and u ∨ v = sup(u, v). Since u *</formula><p>i is a minimizer of E(u) with f = f i , which is defined in (3.5), with respect to i = 1, 2, we have</p><formula xml:id="formula_36">E(u * 1 ∧ u * 2 ) + E(u * 1 ∨ u * 2 ) ≥ E(u * 1 ) + E(u * 2 ). Based on the result Ω |D(u * 1 ∧ u * 2 )| + Ω |D(u * 1 ∨ u * 2 )| ≤ Ω |Du * 1 | + Ω |Du * 2 | in [13, 27], we get Ω ⎡ ⎣ log(u * 1 ∧ u * 2 ) + f 1 u * 1 ∧ u * 2 + α u * 1 ∧ u * 2 f 1 -1 2 ⎤ ⎦ dx + Ω ⎡ ⎣ log(u * 1 ∨ u * 2 ) + f 2 u * 1 ∨ u * 2 + α u * 1 ∨ u * 2 f 2 -1 2 ⎤ ⎦ dx ≥ Ω ⎡ ⎣ log u * 1 + f 1 u * 1 + α u * 1 f 1 -1 2 ⎤ ⎦ dx + Ω ⎡ ⎣ log u * 2 + f 2 u * 2 + α u * 2 f 2 -1 2 ⎤ ⎦ dx.</formula><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_37">Writing Ω = {u * 1 &gt; u * 2 } ∪ {u * 1 ≤ u * 2 }, we easily deduce that {u * 1 &gt;u * 2 } (f 1 -f 2 )(u * 1 -u * 2 ) 1 u * 1 u * 2 + α f 1 f 2 - 2α √ f 1 f 2 ( √ f 1 + √ f 2 )( u * 1 + u * 2 ) ≥ 0.</formula><p>Based on Theorem 3.6, we have 0</p><formula xml:id="formula_38">&lt; a 1 ≤ u * 1 ≤ b 1 and 0 &lt; a 2 ≤ u * 2 ≤ b 2 , which imply 1 u * 1 u * 2 ≥ 1 b 1 b 2 and 2 √ f 1 f 2 ( √ f 1 + √ f 2 )( u * 1 + u * 2 ) - 1 f 1 f 2 ≤ 2 √ a 1 a 2 ( √ a 1 + √ a 2 ) 2 - 1 b 1 b 2 ≤ 1 a 1 a 2 - 1 b 1 b 2 .</formula><p>Hence, we find that  </p><formula xml:id="formula_39">1 u * 1 u * 2 + α f 1 f 2 - 2α √ f 1 f 2 ( √ f 1 + √ f 2 )( u * 1 + u * 2 ) &gt; 0, as soon as α &lt; a 1 a 2 b 1 b 2 -a 1 a 2 . Taking account of f 1 &lt; f 2 ,</formula><formula xml:id="formula_40">u n -m Ω (u n ) 2 ≤ C Ω |D(u n -m Ω (u n ))| = C Ω |Du n |,</formula><formula xml:id="formula_41">-m Ω (u n ) 2 is bounded for each n. Since A ∈ L(L 2 (Ω)) is continuous, {A(u n -m Ω (u n ))} must be bounded in L 2 (Ω) and in L 1 (Ω).</formula><p>On the other hand, according to the boundedness of E A (u n ), for each n, (</p><formula xml:id="formula_42">√ Aun f -1) 2 is bounded in L 1 (Ω)</formula><p>, which implies that Aun f 1 is bounded; then we obtain that Au n 1 is bounded. Moreover, we have</p><formula xml:id="formula_43">|m Ω (u n )| • A1 1 = A(u n -m Ω (u n )) -Au n 1 ≤ A(u n -m Ω (u n )) 1 + Au n 1 . Hence, |m Ω (u n )| • A1 1 is bounded.</formula><p>Thanks to A1 = 0, we obtain that m Ω (u n ) is uniformly bounded. Together with the boundedness of {u n -m Ω (u n )}, this leads to the boundedness of {u n } in L 2 (Ω) and thus in L 1 (Ω). Since S(Ω) is closed and convex, {u n } is bounded in S(Ω) as well.</p><p>Therefore, there exists a subsequence {u n k } which converges weakly in L 2 (Ω) to some u * ∈ L 2 (Ω), and {Du n k } converges weakly as a measure to Du * . Due to the continuity of the linear operator A, one must have that {Au n k } converges weakly to Au * in L 2 (Ω). Then, based on the lower semicontinuity of the TV and Fatou's lemma, we obtain that u * is a solution of the model (4.1).</p><p>Based on Proposition 4.1, when α ≥ 2 √ 6 9 , the model (4.1) is convex. Furthermore, if A is injective, (4.1) is strictly convex, and then its minimizer has to be unique.</p><p>As in Theorem 3.6, the assumption that inf Ω f &gt; 0 basically guarantees the well-posedness of the model. Moreover, in the discrete settings, adding a few more technicalities, the assumption that A is injective may be weakened to ker(A) ker(∇) = {0}, where ∇ denotes the discrete gradient operator.</p><p>Remark 4.3. In the proof of Theorem 4.2, because of the α-term, we obtain that the sequence {Au n } is bounded in L 1 (Ω). Thus, when α = 0, that is, in the case of the model (1.3), it is difficult to get the same result, and in <ref type="bibr" target="#b4">[5]</ref> the existence of a solution to <ref type="bibr">(1.3)</ref> is still an open question.</p><p>According to the constraint in (4.1), we find that the model's minimizer is nonnegative. Further, we have the following result.</p><p>Proposition 4.4. Suppose that u * is the solution of (4.1). Then there exists a constant</p><formula xml:id="formula_44">C 1 such that |{x ∈ Ω : (Au * )(x) ≤ f (x)}| ≤ 1 + log C 1 + Ω log f dx for any 0 &lt; &lt; 1.</formula><p>Proof. Suppose that C 1 is the minimal value of (4.1), which is independent of . Set w = Au * f ; then we have</p><formula xml:id="formula_45">|Ω| + Ω log f dx ≤ Ω log w + 1 w dx ≤ C 1 + Ω log f dx,</formula><p>where we have used the fact that, for each t &gt; 0, log t + Based on w = Au * f , we obtain the assertion. As a consequence, |{x ∈ Ω : (Au * )(x) = 0}| = 0; i.e., Au * is positive a.e. Especially, in the discrete situation, Au * is strictly positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bias correction.</head><p>In <ref type="bibr" target="#b21">[22]</ref>, a variational model was proposed in the log-image domain for multiplicative noise removal. In order to ensure that the mean of the restored image equals that of the observed image, the method ends up an exponential transform along with a bias correction. In addition, we recall that through the theoretical analysis in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, the classical ROF (Rudin-Osher-Fatemi) model in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr">(4.3)</ref> inf</p><formula xml:id="formula_46">u∈BV (Ω) Ω 1 2 (Au -f ) 2 dx + λ Ω |Du|,</formula><p>is proved to preserve the mean; i.e., m Ω (Au * ) = m Ω (f ) with u * as a solution. However, (4.3) is proposed to remove the additive white Gaussian noise. In this section, we consider our new model (4.1) through theoretical analysis similar to that in <ref type="bibr" target="#b13">[14]</ref> and try to find the relation between the observed image f and the solution u * . Proposition 4.5. Suppose that A1 = 1. Let u * be a solution of (4.1), and assume that we have inf Ω Au * &gt; 0. Then the following properties hold true:</p><formula xml:id="formula_47">(i) Ω f (Au * ) 2 -α 1 f - 1 √ f • Au * dx = Ω 1 Au * dx.</formula><p>(ii) If there exists a solution in the case of α = 0, then we have</p><formula xml:id="formula_48">Ω 1 f dx ≥ Ω 1 Au * dx.</formula><p>Proof.</p><p>(i) We define a function with single variable t ∈ (-inf Ω Au * , +∞):</p><formula xml:id="formula_49">e(t) := Ω log A(u * + t) + f A(u * + t) dx + α Ω A(u * + t) f -1 2 dx + λ Ω |D(u * + t)|.</formula><p>Concerning A1 = 1, we necessarily have</p><formula xml:id="formula_50">e(t) = Ω log(Au * + t) + f Au * + t dx + α Ω Au * + t f -1 2 dx + λ Ω |Du * |.</formula><p>Since t = 0 is a (local) minimizer of e(t), we have e (0) = 0, which leads to</p><formula xml:id="formula_51">Ω 1 Au * - f (Au * ) 2 + α 1 f - 1 √ f • Au * dx = 0.</formula><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><formula xml:id="formula_52">(ii) With α = 0, the result in (i) becomes Ω f (Au * ) 2 dx = Ω 1 Au * dx.</formula><p>Moreover, according to Hölder's inequality and the nonnegativity of Au * and f , we obtain</p><formula xml:id="formula_53">Ω f (Au * ) 2 dx • Ω 1 f dx ≥ Ω 1 Au * 2 dx.</formula><p>Combining both, we have</p><formula xml:id="formula_54">Ω 1 f dx ≥ Ω 1 Au * dx.</formula><p>Note that in the discrete situation the assumption that inf Ω Au * &gt; 0 is always satisfied, based on the conclusion of Proposition 4.4.</p><p>From Proposition 4.5 we cannot obtain the similar theoretical result as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>; that is, the mean of the observed image is automatically preserved for the model (4.1). In order to reduce the influence from the bias and keep the restored image in the same scale as f , we improve the model (4.1) as <ref type="bibr">(4.4)</ref> inf</p><formula xml:id="formula_55">{u∈ S(Ω):m Ω (u)=m Ω (f )} Ω log Au + f Au dx + α Ω Au f -1 2 dx + λ Ω |Du|.</formula><p>It is straightforward to show that the feasible set {u ∈ S(Ω) : m Ω (u) = m Ω (f )} is closed and convex, and then the existence and uniqueness of a solution to (4.4) are easily obtained by extending Theorem 4.2. Note that the bias-variance trade-off in statistics does not always advocate for unbiased estimators. It is possible to obtain better peak signal-to-noise ratio (PSNR) results with a small (but different from zero) bias. However, in our numerical simulations, we do observe the improvement of PSNR with our bias correction step. In (4.4), we implicitly suppose that</p><formula xml:id="formula_56">m Ω (u) ≈ m Ω (Au), m Ω ((Au)η) ≈ m Ω (Au).</formula><p>Under some independence conditions, the above assumptions are theoretically rooted in statistics. Moreover, in the practical simulations, we find that these two assumptions provide rather reasonable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Primal-dual algorithm.</head><p>Since the model (4.4) is convex, there are many methods that can be extended to solve the minimization problem in (4.4). For example, the alternating direction method <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>, which is convergent and is well suited to large-scale convex problems, and its variant, the split-Bregman algorithm <ref type="bibr" target="#b27">[28]</ref>, which is widely used to solve L 1 regularization problems such as the TV regularization. In this section, we apply the primal-dual algorithm proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref> to solve the minimization problem in <ref type="bibr">(4.4)</ref>. This algorithm can be used for solving a large family of nonsmooth convex optimization problems; see the applications in <ref type="bibr" target="#b14">[15]</ref>. It is simple and comes with the convergence guarantees. Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Now, we focus on the discrete version of (4.4). For the sake of simplicity, we keep the same notation from the continuous context. Then the discrete model reads as follows:</p><p>(5. <ref type="bibr" target="#b0">1)</ref> min</p><formula xml:id="formula_57">u∈X E A (u) := log Au, 1 + f Au , 1 + α Au f -1 2 2 + λ ∇u 1 ,</formula><p>where X = {v ∈ R n : v i ≥ 0 for i = 1, . . . , n, and</p><formula xml:id="formula_58">n i=1 v i = n i=1 f i }, n</formula><p>is the number of pixels in the images, f ∈ X is obtained from a two-dimensional pixel-array by concatenation in the usual columnwise fashion, and A ∈ R n×n . Moreover, the vector inner product u, v = n i=1 u i v i is used, and • 2 denotes the l 2 -vector-norm. The discrete gradient operator ∇ ∈ R 2n×n is defined by</p><formula xml:id="formula_59">∇v = ∇ x v ∇ y v</formula><p>for v ∈ R n , with ∇ x , ∇ y ∈ R n×n corresponding to the discrete derivatives in the x-direction and y-direction, respectively. In our numerics, ∇ x and ∇ y are obtained by applying finite difference approximations for the derivatives with symmetric boundary conditions in the respective coordinate directions. In addition, ∇v 1 denotes the discrete TV of v, which is defined as</p><formula xml:id="formula_60">∇v 1 = n i=1 (∇ x v) 2 i + (∇ y v) 2 i .</formula><p>Define the function G : X → R as</p><formula xml:id="formula_61">G(u) := log Au, 1 + f Au , 1 + α Au f -1 2 2 .</formula><p>Based on the definition of TV in section 2, we give the primal-dual formulation of (5.1):</p><p>(5.2) max</p><formula xml:id="formula_62">p∈Y min u∈X G(u) -λ u, div p ,</formula><p>where Y = {q ∈ R 2n : q ∞ ≤ 1}, q ∞ = max i∈{1,...,n} | √ q 2 i + q 2 i+n | denotes the l ∞ -vectornorm, p is the dual variable, and the divergence operator div = -∇ . This is a generic saddle-point problem, and we can apply the primal-dual algorithm proposed in <ref type="bibr" target="#b14">[15]</ref> to solve the above optimization task. The algorithm is summarized as follows.</p><p>Algorithm for solving the model (5.1). 1: Fixed σ, τ . Initialize u 0 = f , ū0 = f , and p 0 = (0, . . . , 0) ∈ R 2n . 2: Calculate p k+1 and u k+1 from</p><formula xml:id="formula_63">p k+1 = arg max p∈Y λ ūk , div p - 1 2σ p -p k 2 2 , (5.3) u k+1 = arg min u∈X G(u) -λ u, div p k+1 + 1 2τ u -u k 2 2 , (5.4) ūk+1 =2u k+1 -u k .</formula><p>(5.5) Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3: Stop; or set k := k + 1 and go to step 2. In order to apply the algorithm to (5.1), the main questions are how to solve the dual problem in (5.3) and the primal problem in <ref type="bibr">(5.4)</ref>. For (5.3), the solution can be easily given by <ref type="bibr">(5.6</ref>)</p><formula xml:id="formula_64">p k+1 i = π 1 λσ(∇ū k ) i + p k i for i = 1, . . . , 2n,</formula><p>where π 1 is the projector onto the l 2 -normed unit ball; i.e.,</p><formula xml:id="formula_65">π 1 (q i ) = q i max(1, |q i |)</formula><p>and</p><formula xml:id="formula_66">π 1 (q n+i ) = q n+i max(1, |q i |) for i = 1, . . . , n,</formula><formula xml:id="formula_67">with |q i | = √ q 2 i + q 2 i+n .</formula><p>Since the minimization problem in (5.4) is strictly convex and its objective function has the second derivative, it can be solved efficiently by the Newton method following with one projection step, (5.7)</p><formula xml:id="formula_68">u k i := n j=1 f j n j=1 max(u k j , 0) max(u k i , 0) for i = 1, . . . , n,</formula><p>to ensure that u k is nonnegative and preserves the mean of f . This projection is inspired by Proposition 2.1 of <ref type="bibr" target="#b17">[18]</ref> or Proposition 12 of <ref type="bibr" target="#b18">[19]</ref>.</p><p>Based on Theorem 1 in <ref type="bibr" target="#b14">[15]</ref>, we end this section with the convergence properties of our algorithm. The proof is given in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Proposition 5.1. The iterates (u k , p k ) of our algorithm converge to a saddle point of (5.2), provided that στ λ 2 ∇ 2 &lt; 1.</p><p>According to the result ∇ 2 ≤ 8 with the unit spacing size between pixels in <ref type="bibr" target="#b12">[13]</ref>, we need only στ λ 2 &lt; 1 8 in order to keep the convergent condition. In our numerical practice, λ is tuned empirically (and usually it is around 0.1; see next section for details), and we simply set σ = 3 and τ = 3, which in most cases ensures the convergence of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Numerical results.</head><p>In this section we provide numerical results to study the behavior of our method with respect to its image restoration capabilities and CPU-time consumption. Here, we compare our method with the one proposed in <ref type="bibr" target="#b4">[5]</ref> (AA method) by solving (1.3) and with the one in <ref type="bibr" target="#b38">[39]</ref> (RLO method, for the author's initials) by solving (1.2), and both of them are able to remove the multiplicative noise and deblurring simultaneously. In the denoising case, we also provide numerical results from <ref type="bibr" target="#b19">[20]</ref>, where a probabilistic patch-based (PPB) method was proposed for several kinds of noise, including multiplicative noise. Since in the AA and RLO methods the preservation of the mean of the observed image is not guaranteed, in order to compare fairly, we add the same projection step as in (5.7) before outputting the results. Indeed, assume that u o is the result of the AA method or the RLO method; then we revise it as (6.1)</p><formula xml:id="formula_69">u new i := n j=1 f j n j=1 max(u o j , 0) max(u o i , 0) for i = 1, . . . , n.</formula><p>Note that usually this will improve the result by around 0.1 to 0.3 dB, depending on images and noise levels. For the PPB filter, we numerically find that this correction is not necessary. For illustrations, the results for the 256-by-256 gray level images Phantom, Cameraman, and Parrot are presented; see the original test images in Figure <ref type="figure" target="#fig_1">2</ref>. The quality of the restoration results is compared quantitatively by means of the PSNR <ref type="bibr" target="#b9">[10]</ref>, which is a widely used image quality assessment measure. In addition, all simulations listed here are run in MAT-LAB 7.12 (R2011a) on a PC equipped with 2.40GHz CPU and 4G RAM memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Image denoising.</head><p>Although our method is proposed for the simultaneous deblurring and denoising of images subject to multiplicative noise, here we show that it also provides good results for noise removal only. However, since our model is rather basic, more advanced approaches such as the patch-based method <ref type="bibr" target="#b19">[20]</ref> or dictionary learning method <ref type="bibr" target="#b30">[31]</ref> for multiplicative noise removal could have better results. In our example, the test images are corrupted by multiplicative noise with K = 10 and K = 6, respectively. The results are shown in Figures 3-8. For the AA method and the RLO method, we use the time-marching algorithm to solve the minimization models, as proposed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. We set the step size to 0.1 in order to obtain a stable iterative procedure. The algorithms are stopped when the maximum number of iterations is reached. In addition, after many experiments with different λ-values in the AA model, the RLO model, and ours, the ones that provide the best PSNRs are presented here. In the PPB filter <ref type="bibr" target="#b19">[20]</ref>, we use the codes provided by the authors in <ref type="bibr" target="#b0">[1]</ref>, which are written as C++ mex-functions. Since the codes were written for removing multiplicative noise with Nakagami-Rayleigh distribution, a square root transform was applied in order to eliminate the gap between the Gamma and Nakagami-Rayleigh distributions. We use the suggested parameter values for the PBB filter except for Phantom, where the search window size has to be enlarged to get better result, which costs more time (see Table <ref type="table" target="#tab_7">2</ref>). In our method, we stop the iterative procedure as soon as the value of the objective function has no big relative decrease, i.e., (6.2)</p><formula xml:id="formula_70">E(u k ) -E(u k+1 ) E(u k ) &lt; ε.</formula><p>In the denoising case, we set ε = 5 × 10 -4 . Notice that the computing time of the PPB filter in <ref type="bibr" target="#b19">[20]</ref> is not comparable with the remaining three TV-based methods, since the programming languages are different. Comparing the results of the three TV-based methods, i.e., the AA, RLO, and our method in Figures <ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_4">4</ref><ref type="figure" target="#fig_5">5</ref><ref type="figure" target="#fig_7">6</ref>, we see that all of their objective function values are monotonically decreasing, and our method performs best visually with the fewest iterations. Note that in the restored results for the AA and RLO methods, much more noise remains compared with those of our method; see, e.g., the white boundary of Phantom and the background in Cameraman. Moreover, the contrasts of the details for the AA method and the RLO method are noticeably reduced because of oversmoothing during noise removal; however, our method preserves more details. In this respect, observe the tripod and the trousers in Cameraman, especially when recovering the images corrupted by high-level noise. In order to compare the capability of recovering details, in Figures <ref type="figure" target="#fig_8">7</ref> and<ref type="figure" target="#fig_6">8</ref> we show the results for denoising the image Parrot, which includes more details. Comparing the textures surrounding the eye and the background of the parrot, we can clearly see that our method suppresses noise successfully while preserving significantly more details. Since the PPB filter in <ref type="bibr" target="#b19">[20]</ref> is a patch-based method, which has a framework different from that of the TV-based methods, it can remove the multiplicative noise more effectively and preserve more details. But we still find that some unpleasant artifacts are noticeable in the results obtained by the PPB filter; see, e.g., the homogeneous regions in Cameraman, Phantom, and Parrot. Furthermore, some tiny structures in the images are completely missing; see, e.g., the spots in Phantom and the buildings in Cameraman. Since we utilize TV regularization in our method, those spurious artifacts are absent from our results, but at the same time might be some undesirable staircasing effects; see the beak and neck of the parrot.</p><p>For a comparison of the performance quantitatively and the computational efficiency, in Table <ref type="table" target="#tab_7">2</ref> we list the PSNR values of the restored results, the number of iterations, and the CPU-times. In the three TV-based methods, we observe that the PSNR values from our method are more than 0.65 dB higher than others. Due to a large step size, with many fewer iterations our method reaches the stopping rule and also spends much less CPU-time. However, in order to obtain a stable iterative procedure, the AA and RLO methods have to use a small step size, and then need more than 10 times more iterations to provide results with the best PSNRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Image deblurring and denoising.</head><p>In this section, we consider the restoration of the noisy blurred images. In our experiments, we test two blurring operators, which are motion blur with length 5 and angle 30, and Gaussian blur with a window size 7 × 7 and a standard deviation of 2. Further, after blurring, the test images are corrupted by multiplicative noise with K = 10. In the blurring case, we set ε = 10 -4 in (6.2) as the stopping rule for our method.</p><p>In Figures <ref type="figure" target="#fig_12">9</ref><ref type="figure" target="#fig_13">10</ref><ref type="figure" target="#fig_14">11</ref>, we show the degraded images and the restored results for all three TVbased methods, and Table <ref type="table" target="#tab_8">3</ref> lists the PSNR values, the number of iterations, and the CPUtimes. In contrast to the results for the AA and RLO methods, our method also performs best both visually and quantitatively, and it preserves more details; see, e.g., the tripod in Cameraman and the texture near the eye in Parrot. Due to the blurring, more iterations are needed in all three methods, but our method still provides the best results in many fewer Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php  iterations with the least CPU-times. In conclusion, our method turns out to be more efficient and outperforms the other methods which are able to deblur while removing multiplicative noise simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion.</head><p>In this paper, we propose a new variational model for restoring blurred images subject to multiplicative Gamma noise. In order to obtain the convexity, we add a quadratic penalty term according to the statistical properties of the multiplicative Gamma noise in the model proposed in <ref type="bibr" target="#b4">[5]</ref>, which combines an MAP estimator with the TV regularization. The existence and uniqueness of a solution to the new model are obtained. Furthermore, some other properties are studied, such as the minimum-maximum principle, bias correction, and so on. Due to the convexity, we are allowed to employ the primal-dual algorithm proposed in <ref type="bibr" target="#b14">[15]</ref> to solve the corresponding minimization problem in the new model, and its convergence is guaranteed. Compared to other recently proposed methods, our method appears to be very competitive with respect to image restoration capabilities and CPU-time consumption.  </p><formula xml:id="formula_71">B(K -1 2 ) B(K) = 2 + 1 2K + O 1 K 2 .</formula><p>Proof. Based on a property of the Gamma-function, we readily obtain log</p><formula xml:id="formula_72">B(K -1 2 ) 2B(K) = log K - 1 2 + 2 log Γ K - 1 2 -log Γ(K) .</formula><p>Then using the expansion of log Γ(x) with x &gt; 0 in <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_73">(8.3) log Γ(x) = x log x -x - 1 2 log x 2π + 1 12x + O 1 x 2 ,</formula><p>and the Taylor expansion of log(1 + x) for x ∈ (-1, 1), we get log</p><formula xml:id="formula_74">B(K -1 2 ) 2B(K) = 1 + (2K -1) log 1 - 1 2K + 1 12K(K -1 2 ) + O 1 K 2 = log 2 + 1 2K + O 1 K 2 .</formula><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php  Further, in view of the Taylor expansion of e x , we have  Let x = 1 y 2 . Note that the mean of η equals 1; then we get</p><formula xml:id="formula_75">(a) (b) (c) (d)<label>(e)</label></formula><formula xml:id="formula_76">B(K -1 2 ) B(K) = 2 + 1 2K exp O 1 K 2 = 2 + 1 2K + O 1 K 2 .</formula><formula xml:id="formula_77">+∞ -∞ p Y (y) log p Y (y)dy = +∞ -∞ p η (x) log 2 θ K Γ(K) + 2K + 1 2 log x -Kx dx, = log 2 θ K Γ(K) + 2K + 1 2 (ψ(K) -log K) -K, = log 2 -log( √ KΓ(K)) + 2K + 1 2 ψ(K) -K,</formula><p>where we use a property of the Gamma distribution, E η (log x) = ψ(K)log K (Theorem 2.1 in <ref type="bibr" target="#b32">[33]</ref>) and recall that ψ(K) := d log Γ(K) dK is the digamma function (see <ref type="bibr" target="#b1">[2]</ref>).     (iii) In view of the results in (i) and (ii), using the approximation of ψ(x) in <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_78">ψ(x) = log x - 1 2x + O 1 x 2 ,</formula><p>we have</p><formula xml:id="formula_79">D KL (Y ||N (μ K , σ 2 K )) = log 2 -log( √ KΓ(K)) (8.4) + 2K + 1 2 ψ(K) -K + 1 2 log(2πeσ 2 K ) = log 2 -log( √ KΓ(K)) + 2K + 1 2 log K - 1 2K -K + 1 2 log(2πeσ 2 K ) + O 1 K = log 2 + 1 2 log Kσ 2 K + O 1 K .</formula><p>Here, we obtain (8.4) by simplifying D KL (Y ||N (μ K , σ 2 K )) with (8.3). Moreover, based on the results in Proposition 3.1, we have</p><formula xml:id="formula_80">σ 2 K = E(Y 2 ) -E 2 (Y ) = KΓ(K -1) Γ(K) - KΓ 2 (K -1 2 ) Γ 2 (K) = K K -1 - K 2K -1 Γ 2 (K -1 2 ) Γ(2K -1) Γ(2K) Γ 2 (K) = K 2K -1 - K 2K -1 B(K -1</formula><p>2 ) B(K) . <ref type="bibr">(8.5)</ref> Substituting the result (8.5) into (8.4) and applying the expansion in Lemma 8.1, we get</p><formula xml:id="formula_81">D KL (Y ||N (μ K , σ 2 K )) = log 2 + 1 2 log K 2 K -1 - K 2 2K -1 2 + 1 2K + O 1 K 2 + O 1 K = log 2 + 1 2 log 1 4 + O 1 K + O 1 K = O 1 K ,</formula><p>which yield the assertion.</p><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparisons of the PDFs of Y and N (μK , σ 2 K ) with different K. (a) K = 6, (b) K = 10, (c) K = 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Original images. (a) "Phantom," (b) "Cameraman," (c) "Parrot."</figDesc><graphic coords="15,124.34,96.40,119.00,119.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Results of different methods when removing the multiplicative noise with K = 10. Bottom row: Plots of the objective function values versus iterations. (a) Noisy Phantom, (b) PPB filter, (c) AA method (λ = 0.1), (d) RLO method (λ = 0.12), (e) our method (λ = 0.11 and α = 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results of different methods when removing the multiplicative noise with K = 10. Bottom row: Plots of the objective function values versus iterations. (a) Noisy Cameraman, (b) PPB filter, (c) AA method (λ = 0.14), (d) RLO method (λ = 0.14), (e) our method (λ = 0.12 and α = 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of different methods when removing the multiplicative noise with K = 6. Bottom row: Plots of the objective function values versus iterations. (a) Noisy Phantom, (b) PPB filter, (c) AA method (λ = 0.13), (d) RLO method ( λ = 0.13), (e) our method (λ = 0.11 and α = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>8 .</head><label>8</label><figDesc>Appendix I: Proof of Proposition 3.4. In order to prove Proposition 3.4, we need the following lemma. Lemma 8.1. Let B(K) denote the Beta-function B(K, K) with K ≥ 1, i.e., B(K) := Γ 2 (K) Γ(2K) . Then, we have (8.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of different methods when removing the multiplicative noise with K = 6. Bottom row: Plots of the objective function values versus iterations. (a) Noisy Cameraman, (b) PPB filter, (c) AA method (λ = 0.2), (d) RLO method ( λ = 0.2), (e) our method (λ = 0.16 and α = 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Images restored by different methods for the image Parrot. (a) Noisy image with K = 10, (b) PPB filter, (c) AA method (λ = 0.14), (d) RLO method (λ = 0.14), (e) our method (λ = 0.11 and α = 16).</figDesc><graphic coords="21,124.34,232.72,119.00,119.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Now we are ready to prove Proposition 3 . 4 ,Figure 8 .</head><label>348</label><figDesc>Figure 8. Images restored by different methods for the image Parrot. (a) Noisy image with K = 6, (b) PPB filter, (c) AA method (λ = 0.18), (d) RLO method (λ = 0.18), (e) our method (λ = 0.12 and α = 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Based on (3.2) and θ = 1 K , we have +∞ -∞ p Y (y) log p Y (y)dy = +∞ 0 p Y (y) log 2 θ K Γ(K) -(2K + 1) log y -K y 2 dy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Downloaded 08/ 22 /</head><label>22</label><figDesc><ref type="bibr" target="#b12">13</ref> to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Results of different methods when restoring the degraded images corrupted by motion blur and then multiplicative noise with K = 10. Row 1: Degraded images. Rows 2 and 4: Restored images for different methods. Rows 3 and 5: Plots of the objective function values versus iterations. (a) Degraded Phantom, (b) degraded Cameraman, (c) AA method (row 2: λ = 0.05; row 4: λ = 0.06), (d) RLO method (row 2: λ = 0.05; row 4: λ = 0.06), (e) our method (row 2: λ = 0.09 and α = 16; row 4: λ = 0.09 and α = 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Results of different methods when restoring the degraded images corrupted by Gaussian blur and then multiplicative noise with K = 10. Row 1: Degraded images. Rows 2 and 4: Restored images for different methods. Rows 3 and 5: Plots of the objective function values versus iterations. (a) Degraded Phantom, (b) degraded Cameraman, (c) AA method (row 2: λ = 0.03; row 4: λ = 0.05), (d) RLO method (row 2: λ = 0.03; row 4: λ = 0.05), (e) our method (row 2: λ = 0.07 and α = 16; row 4: λ = 0.07 and α = 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Results for different methods for the image Parrot blurred by different kernels and then corrupted by multiplicative noise with K = 10 (row 2: by motion blur; row 3: by Gaussian blur). (a) Image degraded by motion blur, (b) image degraded by Gaussian blur, (c) AA method (row 2: λ = 0.05; row 3: λ = 0.04), (d) RLO method (row 2: λ = 0.05; row 3: λ = 0.04), (e) our method (row 2: λ = 0.08 and α = 16; row 3: λ = 0.07 and α = 16).</figDesc><graphic coords="25,124.34,352.84,119.00,119.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(</head><label></label><figDesc>ii) Based on the PDFs of Y and the Gaussian distributionN (μ K , σ 2 K ), we have +∞ -∞ p Y (y) log p N (μ K ,σ 2 K )Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpwhere we have utilized the definition of the variance, i.e.,∞ -∞p Y (y)(y -μ K ) 2 dy = σ 2 K .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The values of E((Y -1) 2 ) and the KL divergence of Y with the Gaussian distribution N (μK, σ 2</figDesc><table><row><cell>K ) for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 .</figDesc><table><row><cell></cell><cell></cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>p Y (y)</cell></row><row><cell></cell><cell></cell><cell>1.8</cell><cell>p N(μ,σ 2 ) (y)</cell></row><row><cell></cell><cell></cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell>1.4</cell></row><row><cell></cell><cell></cell><cell>1.2</cell></row><row><cell></cell><cell>p(y)</cell><cell>1</cell></row><row><cell>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</cell><cell cols="2">Proposition 3.4. Suppose that the random variable η follows a Gamma distribution with mean 1. Set Y = 1 √ η . Then we have the following: 0 0.5 1 1.5 2 2.5 3 0 0.2 0.4 0.6 0.8 y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>The extension to simultaneous deblurring and denoising.</head><label></label><figDesc>The model(3.5) is based on the statistical properties of Gamma distribution and is specifically devoted to multiplicative Gamma noise removal. In this section, we extend it to the simultaneous deblurring and denoising case, i.e., to restoring the image û in (1.1) with the blurring operator A. The restoration is processed by solving the optimization problem</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>(4.1)</cell><cell>inf u∈ S(Ω)</cell><cell>E A (u) :=</cell><cell>Ω</cell><cell>log Au +</cell><cell>f Au</cell><cell>dx + α</cell><cell>Ω</cell><cell>Au f</cell><cell>-1</cell><cell>dx + λ</cell></row></table><note><p><p><p><p>in this case we deduce that {u * 1 &gt; u * 2 } has a zero Lebesgue measure; i.e., u * 1 ≤ u * 2 a.e. in Ω.</p>4.</p>Ω |Du|,</p>where A ∈ L(L 2 (Ω)). As a blurring operator, we assume that A is nonnegative; i.e., A ≥ 0 in short. Then we have Au ≥ 0 with u ∈ S(Ω). As in Proposition 3.2, since A is linear, we can readily establish the following convexity result. Proposition 4.1. If α ≥ 2 √ 6 9 , then the model (4.1) is convex.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>4.1. Existence of a solution.</head><label></label><figDesc>Based on the properties of TV and the space of BV functions, we prove the existence and uniqueness of a solution to (4.1).Theorem 4.2. Recall that Ω ⊂ R 2 is a connected bounded set with compact Lipschitz boundary. Suppose that A ∈ L(L 2 (Ω)) is nonnegative and does not annihilate constant functions; i.e., A1 = 0. Let f be in L ∞ (Ω) with inf Ω f &gt; 0; then the model (4.1) admits a solution u * . Similar to the proof of Theorem 3.6, E A is readily bounded from below. We can choose a minimizing sequence {u n } ⊂ S(Ω) for (4.1). So { Ω |Du n |} with n = 1, 2, . . .</figDesc><table><row><cell>Moreover, if α ≥ 2 √ 9 and A is injective, then the solution is unique. 6</cell></row><row><cell>Proof. is</cell></row><row><cell>bounded. Using the Poincaré inequality (see Remark 3.50 of [3]), we obtain</cell></row><row><cell>(4.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where m Ω (u n ) = 1 |Ω| Ω u n dx and |Ω| denotes the measure of Ω. Further, C is a constant. Recalling that Ω is bounded, it follows that u n</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>log t + 1 t ≥ log + 1 , and we get that</cell><cell></cell><cell>1 t ≥ 1. Moreover, if t ≤ &lt; 1, then</cell></row><row><cell>|{x ∈ Ω : w(x) ≤ }| ≤</cell><cell>1 + log</cell><cell>C 1 +</cell></row></table><note><p>Ω log f dx .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2</head><label>2</label><figDesc>The comparisons of PSNR values, the number of iterations, and CPU-time in seconds for different methods for the denoising case.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>K = 10</cell><cell></cell><cell></cell><cell>K = 6</cell><cell></cell></row><row><cell>Images</cell><cell>Methods</cell><cell>PSNR(dB)</cell><cell cols="2">Iter Time(s)</cell><cell>PSNR(dB)</cell><cell cols="2">Iter Time(s)</cell></row><row><cell></cell><cell>AA</cell><cell>29.44</cell><cell>3000</cell><cell>30.39</cell><cell>27.20</cell><cell>3000</cell><cell>30.73</cell></row><row><cell>Phantom</cell><cell>RLO</cell><cell>29.37</cell><cell>3000</cell><cell>36.92</cell><cell>27.08</cell><cell>3000</cell><cell>37.77</cell></row><row><cell></cell><cell>PPB</cell><cell>33.32</cell><cell>4</cell><cell>43.20</cell><cell>29.58</cell><cell>4</cell><cell>42.17</cell></row><row><cell></cell><cell>Ours</cell><cell>30.44</cell><cell>132</cell><cell>6.09</cell><cell>28.05</cell><cell>174</cell><cell>8.24</cell></row><row><cell></cell><cell>AA</cell><cell>24.38</cell><cell>3000</cell><cell>31.09</cell><cell>23.20</cell><cell>3000</cell><cell>33.64</cell></row><row><cell>Cameraman</cell><cell>RLO</cell><cell>24.31</cell><cell>3000</cell><cell>37.99</cell><cell>23.11</cell><cell>3000</cell><cell>37.88</cell></row><row><cell></cell><cell>PPB</cell><cell>26.26</cell><cell>4</cell><cell>16.43</cell><cell>25.22</cell><cell>4</cell><cell>15.11</cell></row><row><cell></cell><cell>Ours</cell><cell>25.01</cell><cell>162</cell><cell>8.50</cell><cell>23.85</cell><cell>168</cell><cell>11.82</cell></row><row><cell></cell><cell>AA</cell><cell>24.53</cell><cell>3000</cell><cell>27.86</cell><cell>23.23</cell><cell>3000</cell><cell>29.14</cell></row><row><cell>Parrot</cell><cell>RLO</cell><cell>24.28</cell><cell>3000</cell><cell>36.60</cell><cell>22.96</cell><cell>3000</cell><cell>39.61</cell></row><row><cell></cell><cell>PPB</cell><cell>25.52</cell><cell>4</cell><cell>15.64</cell><cell>24.73</cell><cell>4</cell><cell>14.41</cell></row><row><cell></cell><cell>Ours</cell><cell>25.47</cell><cell>179</cell><cell>9.19</cell><cell>24.21</cell><cell>219</cell><cell>11.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>The comparisons of PSNR values, the number of iterations, and CPU-time in seconds for different methods for deblurring with denoising.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Motion blur</cell><cell></cell><cell cols="2">Gaussian blur</cell><cell></cell></row><row><cell>Images</cell><cell>Methods</cell><cell>PSNR(dB)</cell><cell cols="2">Iter Time(s)</cell><cell>PSNR(dB)</cell><cell cols="2">Iter Time(s)</cell></row><row><cell></cell><cell>AA</cell><cell>22.58</cell><cell>8000</cell><cell>223.36</cell><cell>21.12</cell><cell>10 4</cell><cell>289.23</cell></row><row><cell>Phantom</cell><cell>RLO</cell><cell>22.28</cell><cell>7000</cell><cell>216.14</cell><cell>20.81</cell><cell>10 4</cell><cell>311.00</cell></row><row><cell></cell><cell>Ours</cell><cell>24.68</cell><cell>182</cell><cell>84.78</cell><cell>22.59</cell><cell>331</cell><cell>152.85</cell></row><row><cell></cell><cell>AA</cell><cell>22.36</cell><cell>8000</cell><cell>224.71</cell><cell>21.36</cell><cell>10 4</cell><cell>298.34</cell></row><row><cell>Cameraman</cell><cell>RLO</cell><cell>22.28</cell><cell>8000</cell><cell>248.93</cell><cell>21.31</cell><cell>10 4</cell><cell>313.67</cell></row><row><cell></cell><cell>Ours</cell><cell>22.99</cell><cell>200</cell><cell>91.93</cell><cell>21.85</cell><cell>293</cell><cell>119.09</cell></row><row><cell></cell><cell>AA</cell><cell>22.35</cell><cell>9000</cell><cell>263.82</cell><cell>21.35</cell><cell>10 4</cell><cell>296.58</cell></row><row><cell>Parrot</cell><cell>RLO</cell><cell>22.15</cell><cell>9000</cell><cell>285.12</cell><cell>21.23</cell><cell>10 4</cell><cell>317.76</cell></row><row><cell></cell><cell>Ours</cell><cell>23.18</cell><cell>216</cell><cell>100.86</cell><cell>22.08</cell><cell>223</cell><cell>103.92</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 08/22/13 to 128.206.9.138. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. The authors would like to thank the reviewers for the careful reading of the manuscript and the insightful and constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Deledalle</surname></persName>
		</author>
		<ptr target="http://www.math.u-bordeaux1.fr/∼cdeledal/ppb.php" />
		<title level="m">Probabilistic patch-based filter (PPB), image analysis software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stegun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Standards</orgName>
		</respStmt>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Functions of Bounded Variation and Free Discontinuity Problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Askey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roy</surname></persName>
		</author>
		<title level="m">Special Functions</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A variational approach to removing multiplicative noise</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="925" to="946" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Partial Differential Equations and the Calculus of Variations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Sci</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Mathematical Problems in Image Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doksum</surname></persName>
		</author>
		<title level="m">Mathematical Statistics: Basic Ideas and Selected Topics</title>
		<meeting><address><addrLine>Pearson, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Total variation restoration of speckled images using a split-Bregman algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3717" to="3720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiplicative noise removal using variable splitting and constrained optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1720" to="1730" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<title level="m">Handbook of Image and Video Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. and Trends Mach. Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framelet-based image inpainting algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="131" to="149" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An algorithm for total variation minimization and application</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image recovery via total variation minimization and related problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="167" to="188" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm for convex problems with applications to imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="120" to="145" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Image Processing and Analysis: Variational, PDE, Wavelet, and Stochastic Methods</title>
		<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A variational approach to reconstructing images corrupted by Poisson noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="257" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nested iterative algorithms for convex constrained image recovery problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pustelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="730" to="762" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Douglas-Rachford splitting approach to nonsmooth convex variational signal recovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Selected Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="564" to="574" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Iterative weighted maximum likelihood denoising with probabilistic patch-based weights</title>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Deledalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2661" to="2672" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient primal-dual method for L1TV image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hintermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1168" to="1189" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiplicative noise removal using L1 fidelity on frame coefficients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="201" to="226" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Poisson and Gaussian approximation of weighted local empirical processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Einmahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Process Appl</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="31" to="58" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science</title>
		<author>
			<persName><forename type="first">E</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1015" to="1046" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Restoration of Poissonian images using alternating direction optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="3133" to="3145" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlocal operators with applications to image processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1005" to="1028" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minimal Surfaces and Functions of Bounded Variation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Giusti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Birkhäuser Boston</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The split Bregman algorithm for L1-regularized problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="323" to="343" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probability: An Introduction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Grimmett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Welsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Oxford Science Publications</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sur les problèmes aux dérivées partielles et leur signification physique</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hadamard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Princeton Univ. Bull</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="49" to="52" />
			<date type="published" when="1902">1902</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiplicative noise removal via a learned dictionary</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4534" to="4543" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new total variation method for multiplicative noise removal</title>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="20" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some properties of generalized gamma distribution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khodabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="9" to="28" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image sequence analysis via partial differential equations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="5" to="26" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A variational approach to reconstructing images corrupted by Poisson noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="257" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiplicative noise removal with spatially varying regularization parameters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An algorithm for minimizing the Mumford-Shah functional</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE</title>
		<meeting>the 12th IEEE</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1133" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normal approximations to discrete unimodal distributions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Macgillivray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1013" to="1018" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiplicative denoising and deblurring: Theory and algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lions</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geometric Level Set Methods in Imaging, Vision, and Graphics, S. Osher and N. Paragios</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A nonlinear inverse scale space method for a convex multiplicative noise model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Removing multiplicative noise by Douglas-Rachford splitting methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="168" to="184" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonlocal filters for removing multiplicative noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision 6667</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Restoration of images corrupted by mixed Gaussian-impulse noise via l1-l0 minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1708" to="1728" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the total variation dictionary model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="821" to="825" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
