<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade-BGNN: Toward Efficient Self-supervised Representation Learning on Large-scale Bipartite Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-27">27 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
							<email>chaoyang.he@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Xie</surname></persName>
							<email>xiet@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
							<email>shahabi@usc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tencent AI Lab Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascade-BGNN: Toward Efficient Self-supervised Representation Learning on Large-scale Bipartite Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-27">27 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:1906.11994v3[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>self-supervised learning</term>
					<term>graph embedding</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bipartite graphs have been used to represent data relationships in many data-mining applications such as in E-commerce recommendation systems. Since learning in graph space is more complicated than in Euclidian space, recent studies have extensively utilized neural nets to effectively and efficiently embed a graph's nodes into a multidimensional space. However, this embedding method has not yet been applied to large-scale bipartite graphs. Existing techniques either cannot be scaled to large-scale bipartite graphs that have limited labels or cannot exploit the unique structure of bipartite graphs, which have distinct node features in two domains. Thus, we propose Cascade Bipartite Graph Neural Networks, Cascade-BGNN, a novel node representation learning for bipartite graphs that is domain-consistent, self-supervised, and efficient. To efficiently aggregate information both across and within the two partitions of a bipartite graph, BGNN utilizes a customized Inter-domain Message Passing (IDMP) and Intra-domain Alignment (IDA), which is our adaptation of adversarial learning, for message aggregation across and within partitions, respectively. BGNN is trained in a selfsupervised manner. Moreover, we formulate a multi-layer BGNN in a cascaded training manner to enable multi-hop relationship modeling while improving training efficiency. Extensive experiments on several datasets of varying scales verify the effectiveness and efficiency of BGNN over baselines. Our design is further affirmed through theoretical analysis for domain alignment. The scalability of BGNN is additionally verified through its demonstrated rapid training speed and low memory cost over a large-scale real-world bipartite graph 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs have been used to capture and represent complex structural relationships among data items in various domains, including drug discovery <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, social networks analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, and visual understanding <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. Amongst their varying forms, bipartite graphs are prevalent in data mining applications. A bipartite graph (Fig. <ref type="figure" target="#fig_0">1</ref>) is a graph whose vertices are divided into two independent partitions such that every edge connects nodes from one partition to the other. For example, in an e-commerce recommendation system, the two distinct partitions are represented by users and products, and an edge from a member from one partition to a member of the other represents the user purchasing the product <ref type="bibr" target="#b12">[13]</ref>. The ability to utilize information from the graphical structure, such as node features in the two distinct partitions and topology information, plays an important role in the accuracy and effectiveness of services and tasks, such as classification, prediction, and recommendation.</p><p>Graph representation learning, also called graph embedding, is a machine learning paradigm that aims to solve the above problem by mapping structural information into a low-dimensional vector space which can then be used to improve the performance of downstream tasks <ref type="bibr" target="#b24">[25]</ref>. Early classical works include random walk-based methods (DeepWalk <ref type="bibr" target="#b16">[17]</ref>, Node2Vec <ref type="bibr" target="#b4">[5]</ref>), where only graph topology and node relations are embedded as vectors. In light of the rapid advancements of deep learning, Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">[19]</ref> have exhibited tremendous progress in representation learning for generic graphs (GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, AS-GCN <ref type="bibr" target="#b7">[8]</ref>). In general, GNNs recursively update each node's feature by aggregating its neighbors through message passing, by which not only the patterns of graph topology but also node features are captured. However, GNNs cannot embed the rich information contained in distinct node features from two domains and topology information into a single node presentation, especially for an extremely large-scale bipartite graph without sufficient node-wise labels. For example, in the e-commerce system (Fig. <ref type="figure" target="#fig_0">1</ref>), to precisely represent a user, hidden product attributes should also be extracted and incorporated into the final embedding vector because it can express a user's taste and interest, which is useful in accurately classifying users. This paper studies the above problem of node representation learning on large-scale bipartite graphs in a self-supervised manner. There are two main challenges. The first challenge with bipartite graphs is that features of nodes in each partition of a bipartite graph may follow different distributions in distinct feature domains (e.g., in Fig. <ref type="figure" target="#fig_0">1</ref>, users and products have different attributes). Therefore, representing bipartite graphs as generic graphs to leverage neighbor message passing, as is typically done with GNNs, fails to exploit the extra knowledge from two distinct node domains. One can deal with two different partitions for each domain by converting twohop neighbors to one-hop homogeneous connections within the same domain (note that each node and its two-hop neighbors are in the same partition). This approach is clearly unable to exploit the feature correlations across the two partitions. Alternatively, one can treat bipartite graphs as heterogeneous networks and use random walk-based methods such as Metapath2Vec <ref type="bibr" target="#b1">[2]</ref>. However, Metapath2Vec does not integrate node features into the embedding process. Additionally, a meta-path in a bipartite graph can be equated to a naive unbiased random walk in homogeneous graphs.</p><p>The second challenge is that the limited label issue and computational inefficiency become non-trivial when scaling up extremely large bipartite graphs. Limited labels prohibit supervised learning. For example, an e-commerce bipartite graph contains billions of users, and labeling every user requires tremendous effort, and is thus impractical. Normally, only an extremely small fraction of the billions of users is manually labeled, which is insufficient for supervised learning. Some unsupervised learning methods have been proposed to address this problem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. However, they either do not apply to bipartite graphs or are inefficient in scalability.</p><p>In terms of computational inefficiency, scaling up is difficult. Although sampling methods, such as GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, AS-GCN <ref type="bibr" target="#b7">[8]</ref> and FastGCN <ref type="bibr" target="#b0">[1]</ref>), have been proposed to deal with the scalability issue (uncontrollable neighborhood expansion across layers), when applied to bipartite, they are only able to propagate through multiple layers in the same feature domain by connecting indirect two-hop nodes, which significantly increases the edge number. Thus, their training speed and memory costs are still unsatisfied.</p><p>To address the limitations of existing methods, we propose cascade-BGNN (Bipartite Graph Neural Networks), a highly efficient graph neural network framework that we have developed and deployed in production. Cascade-BGNN outperforms current competitive baselines in terms of both effectiveness and efficiency in a large-scale bipartite graph where nodes and edges are on the order of millions or billions <ref type="foot" target="#foot_0">2</ref> -a graph that is 1000 times larger than typical applications of GNNs. These advantages are due to the three key designs of our Cascade-BGNN.</p><p>Domain-consistent. To represent the node features within different domains into a single representation, BGNN consists of two central operations: inter-domain message passing (IDMP) and intradomain alignment (IDA). As illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>, two node domains are represented as 𝑈 and 𝑉 , respectively. In each layer (depth) of BGNN, we formulate two simultaneous directions of information flow, one from 𝑈 to 𝑉 and the other from 𝑉 to 𝑈 , each of which is equipped with different weight filters. By aggregating information from another domain through the connected edges, IDMP can attain an inter-domain representation for each node, which is then fused with the raw feature in each node itself. For domain fusion, we propose an intra-domain alignment technique to minimize the divergence between raw features and the inter-domain representation by using adversarial models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, a tool that has been applied successfully for distribution matching.</p><p>Self-Supervised. Most notably, BGNN is tailored for a limitedlabel setting. BGNN is a self-supervised learning framework, which is a form of unsupervised learning in which the data itself provides supervision. One may argue that a straightforward method to fuse the two domains is to concatenate the input feature of each node with its corresponding inter-domain representation as an enhanced output. Nevertheless, such a method requires using the final labels as supervised signals for the training process, which is not feasible in the limited label setting.</p><p>Efficient in Large-Scale. Our model and training method codesign elegantly improve the scalability. We design a cascaded training method that allows for multi-stage training without supervision. That is, the training of the upper layer (depth 𝑘 + 1 in Fig. <ref type="figure" target="#fig_3">2</ref>) begins only after the lower one (depth 𝑘 in Fig. <ref type="figure" target="#fig_3">2</ref>) has been trained completely. Cascaded training is clearly more memory-efficient than conventional end-to-end training method since it does not require restoration of all intermediate activation maps of neural layers. Additionally, in cascaded training, the domain shift in lower layers (i.e. the discrepancy between two domain features, which always exists during the early training phase) is not passed to higher ones. In contrast, in end-to-end training, this type of error accumulates as the depth increases. More details are discussed in § 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… IDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-domain Message Passing</head><p>(IDMP)</p><formula xml:id="formula_0">Intra-domain Alignment (IDA) depth 1 D H k u!v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b L R S s R f 4 L 1 g q a g C k P U l i W D V l i G A = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 t F 8 F S S K u i x 6 K X H C v Y D 2 h g 2 2 2 2 7 d L M J u 5 N K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g F l y D 4 3 x b u Z X V t f W N / G Z h a 3 t n d 8 / e P 2 j o K F G U 1 W k k I t U K i G a C S 1 Y H D o K 1 Y s V I G A j W D I Y 3 U 7 8 5 Y k r z S N 7 B O G Z e S P q S 9 z g l Y C T f P s J V P 0 1 w R / H + A I h S 0 Q M e T e 6 H v l 1 0 S s 4 M e J m 4 G S m i D D X f / u p 0 I 5 q E T A I V R O u 2 6 8 T g p U Q B p 4 J N C p 1 E s 5 j Q I e m z t q G S h E x 7 6 e z + C T 4 1 S h f 3 I m V K A p 6 p v y d S E m o 9 D g P T G R I Y 6 E V v K v 7 n t R P o X X k p l 3 E C T N L 5 o l 4 i M E R 4 G g b u c s U o i L E h h C p u b s V 0 Q B S h Y C I r m B D c x Z e X S a N c c s 9 L 5 d u L Y u U 6 i y O P j t E J O k M u u k Q V V E U 1 V E c U P a J n 9 I r e r C f r x X q 3 P u a t O S u b O U R / Y H 3 + A K u c l e M = &lt; / l a t e x i t &gt; H k 1 v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 c K 2 J 5 H i P A O R X V L y T b w m / Z U f e c I = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B i 2 W 3 C n o s e u m x g v 2 Q d i 3 Z N N u G J t k l y R b K 0 l / h x Y M i X v 0 5 3 v w 3 p u 0 e t P X B w O O 9 G W b m B T F n 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R 0 l i t A G i X i k 2 g H W l D N J G 4 Y Z T t u x o l g E n L a C 0 d 3 M b 4 2 p 0 i y S D 2 Y S U 1 / g g W Q h I 9 h Y 6 b H W G z + l o w t v 2 i u W 3 L I 7 B 1 o l X k Z K k K H e K 3 5 1 + x F J B J W G c K x 1 x 3 N j 4 6 d Y G U Y 4 n R a 6 i a Y x J i M 8 o B 1 L J R Z U + + n 8 4 C k 6 s 0 o f h Z G y J Q 2 a q 7 8 n U i y 0 n o j A d g p s h n r Z m 4 n / e Z 3 E h D d + y m S c G C r J Y l G Y c G Q i N P s e 9 Z m i x P C J J Z g o Z m 9 F Z I g V J s Z m V L A h e M s v r 5 J m p e x d l i v 3 V 6 X q b R Z H H k 7 g F M 7 B g 2 u o Q g 3 q 0 A A C A p 7 h F d 4 c 5 b w 4 7 8 7 H o j X n Z D P H 8 A f O 5 w 9 N d J A U &lt; / l a t e x i t &gt; Adversarial Loss Cascaded Training IDMP U V H k u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 R 1 b 6 A 4 a M H g i N t C z a z L L Y N c J R 0 Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i l x 4 r m F p o Y 9 l s N + 3 S z S b s T o R S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M k m m G f d Z I h P d D q n h U i j u o 0 D J 2 6 n m N A 4 l f w h H t z P / 4 Y l r I x J 1 j + O U B z E d K B E J R t F K f q O X P Y 5 6 5 Y p b d e c g q 8 T L S Q V y N H v l r 2 4 / Y V n M F T J J j e l 4 b o r B h G o U T P J p q Z s Z n l I 2 o g P e s V T R m J t g M j 9 2 S s 6 s 0 i d R o m 0 p J H P 1 9 8 S E x s a M 4 9 B 2 x h S H Z t m b i f 9 5 n Q y j 6 2 A i V J o h V 2 y x K M o k w Y T M P i d 9 o T l D O b a E M i 3 s r Y Q N q a Y M b T 4 l G 4 K 3 / P I q a d W q 3 k W 1 d n d Z q d / k c R T h B E 7 h H D y 4 g j o 0 o A k + M B D w D K / w 5 i j n x X l 3 P h a t B S e f O Y Y / c D 5 / A K g J j p U = &lt; / l a t e x i t &gt; H u!v</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t S L 1 D q x Z F + o l l B j j t f 8 </p><formula xml:id="formula_1">I 0 I 3 o / z 8 = " &gt; A A A B / H i c b V B N S 8 N A E N 3 U r 1 q / o j 1 6 W S y C p 5 J U Q Y 9 F L z 1 W s B / Q h r D Z b t u l m 0 3 Y n V R C q H / F i w d F v P p D v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C 2 L B N T j O t 1 X Y 2 N z a 3 i n u l v b 2 D w 6 P 7 O O T t o 4 S R V m L R i J S 3 Y B o J r h k L e A g W D d W j I S B Y J 1 g c j f 3 O 1 O m N I / k A 6 Q x 8 0 I y k n z I K Q E j + X a 5 4 W c J 7 i s + G g N R K n r E 0 5 l v V 5 y q s w B e J 2 5 O K i h H 0 7 e / + o O I J i G T Q A X R u u c 6 M X g Z U c C p Y L N S P 9 E s J n R C R q x n q C Q h 0 1 6 2 O H 6 G z 4 0 y w M N I m Z K A F + r v i Y y E W q d h Y D p D A m O 9 6 s 3 F / 7 x e A s M b L + M y T o B J u l w 0 T A S G C M + T w A O u G A W R G k K o 4 u Z W T M d E E Q o m r 5 I J w V 1 9 e Z 2 0 a 1 X 3 s l q 7 v 6 r U b / M 4 i u g U n a E L 5 K J r V E c N 1 E Q t R F G K n t E r e r O e r B f r 3 f p Y t h a s f K a M / s D 6 / A H J A 5 T c &lt; / l a t e x i t &gt; H k v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a v g H g b h x E k P d r Q 7 H X t s o L L k k P P g = " &gt; A A A B 7 H i c b V B N T w I x E J 3 i F + I X 6 t F L I z H x R H b R R I 9 E L x w x c Y E E V t I t X W j o d j d t l 4 R s + A 1 e P G i M V 3 + Q N / + N B f a g 4 E s m e X l v J j P z g k R w b R z n G x U 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J S 8 e p o s y j s Y h V J y C a C S 6 Z Z 7 g R r J M o R q J A s H Y w v p / 7 7 Q l T m s f y 0 U w T 5 k d k K H n I K T F W 8 h r 9 y d O 4 X 6 4 4 V W c B v E 7 c n F Q g R 7 N f / u o N Y p p G T B o q i N Z d 1 0 m M n x F l O B V s V u q l m i W E j s m Q d S 2 V J G L a z x b H z v C F V Q Y 4 j J U t a f B C / T 2 R k U j r a R T Y z o i Y k V 7 1 5 u J / X j c 1 4 a 2 f c Z m k h k m 6 X B S m A p s Y z z / H A 6 4 Y N W J q C a G K 2 1 s x H R F F q L H 5 l G w I 7 u r L 6 6 R V q 7 p X 1 d r D d a V + l 8 d R h D M 4 h 0 t w 4 Q b q 0 I A m e E C B</formula><formula xml:id="formula_2">l M d v L s z y z f U / E 0 v 9 b 8 5 0 l 6 4 M R b Q = " &gt; A A A B / 3 i c b V B N S 8 N A E J 3 U r 1 q / o o I X L 4 t F 8 F S S K u i x 6 K X H C v Y D 2 h g 2 2 2 2 7 d L M J u 5 t K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g 5 k x p x / m 2 c i u r a + s b + c 3 C 1 v b O 7 p 6 9 f 9 B Q U S I J r Z O I R 7 I V Y E U 5 E 7 S u m e a 0 F U u K w 4 D T Z j C 8 m f r N E Z W K R e J O j 2 P q h b g v W I 8 R r I 3 k 2 0 e o 6 q c J 6 k j W H 2 g s Z f S A R p N 7 1 7 e L T s m Z A S 0 T N y N F y F D z 7 a 9 O N y J J S I U m H C v V d p 1 Y e y m W m h F O J 4 V O o m i M y R D 3 a d t Q g U O q v H R 2 / w S d G q W L e p E 0 J T S a q b 8 n U h w q N Q 4 D 0 x l i P V C L 3 l T 8 z 2 s n u n f l p U z E i a a C z B f 1 E o 5 0 h K Z h o C 6 T l G g + N g Q T y c y t i A y w x E S b y A o m B H f x 5 W X S K J f c 8 1 L 5 9 q J Y u c 7 i y M M x n M A Z u H A J F a h C D e p A 4 B G e 4 R X e r C f r x X q 3 P u a t O S u b O Y Q / s D 5 / A F O 0 l a k = &lt; / l a t e x i t &gt; X v X u H 1 u</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 8 7 l T X g f / + q N T y 9 g p m Z 8 u h w   We provide theoretical analysis and empirical verification to demonstrate the advantages of Cascade-BGNN. Our theoretical analysis proves that domain alignment is able to make the representation distribution closer. Such distribution approaching causes information from one domain to be incorporated into the other. In our experiments, we contrast the performance of our algorithm with several unsupervised representation learning baselines: Node2Vec <ref type="bibr" target="#b4">[5]</ref>, VGAE <ref type="bibr" target="#b9">[10]</ref>, GraphSAGE <ref type="bibr" target="#b5">[6]</ref>, and AS-GCN <ref type="bibr" target="#b7">[8]</ref>. We use a large-scale bipartite graph dataset from the Tencent Platform and also construct three synthesized datasets based on the citation networks Cora, Citeseer, and PubMed <ref type="bibr" target="#b20">[21]</ref>. For all benchmarks, BGNN outperforms other competitive baselines in terms of both effectiveness and efficiency, with a higher classification accuracy, faster training speed, and lower memory cost.</p><formula xml:id="formula_3">N o k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 6 b G C a Q t t L J v t p F 2 6 2 Y T d j V B C f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p o J r 4 7 r f z t r 6 x u b W d m m n v L u 3 f 3 B Y O T p u 6 S R T D H 2 W i E R 1 Q q p R c I m + 4 U Z g J 1 V I 4 1 B g O x z f z f z 2 E y r N E / l g J i k G M R 1 K H n F G j Z X 8 R j 9 7 9 P q V q l t z 5 y C r x C t I F Q o 0 + 5 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m 5 l 2 l M K R v T I X Y t l T R G H e T z Y 6 f k 3 C o D E i X K l j R k</formula><formula xml:id="formula_4">W i E R 1 Q q p R c I m + 4 U Z g J 1 V I 4 1 B g O x z f z f z 2 E y r N E / l g J i k G M R 1 K H n F G j Z X 8 R j 9 7 9 P q V q l t z 5 y C r x C t I F Q o 0 + 5 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m 5 l 2 l M K R v T I X Y t l T R G H e T z Y 6 f k 3 C o D E i X K l j R k</formula><formula xml:id="formula_5">W i E R 1 Q q p R c I m + 4 U Z g J 1 V I 4 1 B g O x z f z f z 2 E y r N E / l g J i k G M R 1 K H n F G j Z X 8 R j 9 7 9 P q V q l t z 5 y C r x C t I F Q o 0 + 5 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m 5 l 2 l M K R v T I X Y t l T R G H e T z Y 6 f k 3 C o D E i X K l j R k r v 6 e y G m s 9 S Q O b W d M z U g v e z P x P 6 + b m e g m y L l M M 4 O S L R Z F m S A m I b P P y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 n 7 I N w V t + e Z W 0 L m u e W / P u r 6 r 1 2 y K O E p z C G V y A B 9 d Q h w Y 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N K W Z O 4 A + c z x 9 O 0 4 5 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 8 7 l T X g f / + q N T y 9 g p m Z 8 u h w N o k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 6 b G C a Q t t L J v t p F 2 6 2 Y T d j V B C f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p o J r 4 7 r f z t r 6 x u b W d m m n v L u 3 f 3 B Y O T p u 6 S R T D H 2 W i E R 1 Q q p R c I m + 4 U Z g J 1 V I 4 1 B g O x z f z f z 2 E y r N E / l g J i k G M R 1 K H n F G j Z X 8 R j 9 7 9 P q V q l t z 5 y C r x C t I F Q o 0 + 5 W v 3 i B h W Y z S M E G 1 7 n p u a o K c K s O Z w G m 5 l 2 l M K R v T I X Y t l T R G H e T z Y 6 f k 3 C o D E i X K l j R k r v 6 e y G m s 9 S Q O b W d M z U g v e z P x P 6 + b m e g m y L l M M 4 O S L R Z F m S A m I b P P y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 n 7 I N w V t + e Z W 0 L m u e W / P u r 6 r 1 2 y K O E p z C G V y A B 9 d Q h w Y 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N K W Z O 4 A + c z x 9 O 0 4 5 X &lt; / l a t e x i t &gt; H 1 v!u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u X D f K y G i c F K 7 I V v 2 / v l k 0 R 9 r x X c = " &gt; A A A B / 3 i c b V B N S 8 N A E J 3 U r 1 q / o o I X L 4 t F 8 F S S K u i x 6 K X H C v Y D 2 h g 2 2 2 2 7 d L M J u 5 t K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g 5 k x p x / m 2 c i u r a + s b + c 3 C 1 v b O 7 p 6 9 f 9 B Q U S I J r Z O I R 7 I V Y E U 5 E 7 S u m e a 0 F U u K w 4 D T Z j C 8 m f r N E Z W K R e J O j 2 P q h b g v W I 8 R r I 3 k 2 0 e o 6 q c j 1 J G s P 9 B Y y u g B J Z N 7 1 7 e L T s m Z A S 0 T N y N F y F D z 7 a 9 O N y J J S I U m H C v V d p 1 Y e y m W m h F O J 4 V O o m i M y R D 3 a d t Q g U O q v H R 2 / w S d G q W L e p E 0 J T S a q b 8 n U h w q N Q 4 D 0 x l i P V C L 3 l T 8 z 2 s n u n f l p U z E i a a C z B f 1 E o 5 0 h K Z h o C 6 T l G g + N g Q T y c y t i A y w x E S b y A o m B H f x 5 W X S K J f c 8 1 L 5 9 q J Y u c 7 i y M M x n M A Z u H A J F a h C D e p A 4 B G e 4 R X e r C f r x X q 3 P u a t O S u b O Y Q / s D 5 / A F P C l a k = &lt; / l a t e x i t &gt; H k u!v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + A S Q / 1 i Q 1 X J L y s W J h 3 7 k z g g 4 v l k = " &gt; A A A C A X i c b V D L S s N A F J 3 U V 6 2 v q B v B z W A R X J W k C r o s u u m y g n 1 A G 8 N k O m m H T m b C z K R S Q t z 4 K 2 5 c K O L W v 3 D n 3 z h t s 9 D W A x c O 5 9 z L v f c E M a N K O 8 6 3 V V h Z X V v f K G 6 W t r Z 3 d v f s / Y O W E o n E p I k F E 7 I T I E U Y 5 a S p q W a k E 0 u C o o C R d j C 6 m f r t M Z G K C n 6 n J z H x I j T g N K Q Y a S P 5 9 h G s + 2 k C e 5 I O h h p J K R 7 g O L t P R 5 l v l 5 2 K M w N c J m 5 O y i B H w 7 e / e n 2 B k 4 h w j R l S q u s 6 s f Z S J D X F j G S l X q J I j P A I D U j X U I 4 i o r x 0 9 k E G T 4 3 S h 6 G Q p r i G M / X 3 R I o i p S Z R Y D o j p I d q 0 Z u K / 3 n d R I d X X k p 5 n G j C 8 X x R m D C o B Z z G A f t U E q z Z x B C E J T W 3 Q j x E E m F t Q i u Z E N z F l 5 d J q 1 p x z y v V 2 4 t y 7 T q P o w i O w Q k 4 A y 6 4 B D V Q B w 3 Q B B g 8 g m f w C t 6 s J + v F e r c + 5 q 0 F K 5 8 5 B H 9 g f f 4 A f z G W 7 w = = &lt; / l a t e x i t &gt; H k v!u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F 5 T D A N y X O C K H 1 n c p 8 O E 3 c 7 C R 5 5 k = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 U r 1 q / o o I X L 4 t F 8 F S S K u i x 6 K X H C v Y D 2 h g 2 2 2 2 7 d L M J u 5 N K i T 3 4 V 7 x 4 U M S r f 8 O b / 8 Z t m 4 O 2 P h h 4 v D f D z L w g F l y D 4 3 x b u Z X V t f W N / G Z h a 3 t n d 8 / e P 2 j o K F G U 1 W k k I t U K i G a C S 1 Y H D o K 1 Y s V I G A j W D I Y 3 U 7 8 5 Y k r z S N 7 B O G Z e S P q S 9 z g l Y C T f P s J V P x 3 h j u L 9 A R C l o g e c T O 6 H v l 1 0 S s 4 M e J m 4 G S m i D D X f / u p 0 I 5 q E T A I V R O u 2 6 8 T g p U Q B p 4 J N C p 1 E s 5 j Q I e m z t q G S h E x 7 6 e z + C T 4 1 S h f 3 I m V K A p 6 p v y d S E m o 9 D g P T G R I Y 6 E V v K v 7 n t R P o X X k p l 3 E C T N L 5 o l 4 i M E R 4 G g b u c s U o i L E h h C p u b s V 0 Q B S h Y C I r m B D c x Z e X S a N c c s 9 L 5 d u L Y u U 6 i y O P j t E J O k M u u k Q V V E U 1 V E c U P a J n 9 I r e r C f r x X q 3 P u a t O S u b O U R / Y H 3 + A K u q l e M = &lt; / l a t e x i t &gt; depth k H 1 v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T q m m 2 O l + j j 6 t r v t 9 0 D g y x c m 3 B / o = " &gt; A A A B 7 H i c b V B N T w I x E J 3 F L 8 Q v 1 K O X R m L i i e y i i R 6 J X j h i 4 g I J r K R b u t D Q b T d t l 4 R s + A 1 e P G i M V 3 + Q N / + N B f a g 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I t Q n k k v V C b G m n A n q G 2 Y 4 7 S S K 4 j j k t B 2 O 7 + d + e 0 K V Z l I 8 m m l C g x g P B Y s Y w c Z K f q M / e f L 6 5 Y p b d R d A 6 8 T L S Q V y N P v l r 9 5 A k j S m w h C O t e 5 6 b m K C D C v D C K e z U i / V N M F k j I e 0 a 6 n A M d V B t j h 2 h i 6 s M k C R V L a E Q Q v 1 9 0 S G Y 6 2 n c W g 7 Y 2 x G e t W b i / 9 5 3 d R E t 0 H G R J I a K s h y U Z R y Z C S a f 4 4 G T F F i + N Q S T B S z t y I y w g o T Y / M p 2 R C 8 1 Z f X S a t W 9 a 6 q t Y f r S v 0 u j 6 M I Z 3 A O l + D B D d S h A U 3 w g Q C D Z 3 i F N 0 c 4 L 8 6 7 8 7 F s L T j 5 z C n 8 g f P 5 A 1 G n j l w = &lt; / l a t e x i t &gt; U V m in i-b a t c h IDMP IDA IDMP IDA IDMP IDA IDMP IDA H k 1 v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S J I J 6 o C l e e Z W f Q B I / I H 8 b t y B X d A = " &gt; A A A B 8 n i c b V B N S w M x E M 3 6 W e t X 1 a O X Y B G 8 W H a r o M e i l x 4 r 2 A 9 o 1 5 J N s 2 1 o N l m S 2 U J Z 9 m d 4 8 a C I V 3 + N N / + N a b s H b X 0 w 8 H h v h p l 5 Q S y 4 A d f 9 d t b W N z a 3 t g s 7 x d 2 9 / Y P D 0 t F x y 6 h E U 9 a k S i j d C Y h h g k v W B A 6 C d W L N S B Q I 1 g 7 G 9 z O / P W H a c C U f Y R o z P y J D y U N O C V i p W + + n k + w p H V 9 6 W b 9 U d i v u H H i V e D k p o x y N f u m r N 1 A 0 i Z g E K o g x X c + N w U + J B k 4 F y 4 q 9 x L C Y 0 D E Z s q 6 l k k T M + O n 8 5 A y f W 2 W A Q 6 V t S c B z 9 f d E S i J j p l F g O y M C I 7 P s z c T / v G 4 C 4 a 2 f c h k n w C R d L A o T g U H h 2 f 9 4 w D W j I K a W E K q 5 v R X T E d G E g k 2 p a E P w l l 9 e J a 1 q x b u q V B + u y 7 W 7 P I 4 C O k V n 6 A J 5 6 A b V U B 0 1 U B N R p N A z e k V v D j g v z r v z s W h d c / K Z E / Q H z u c P F + a R I A = = &lt; / l a t e x i t &gt; H k 1 u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a i B l t P V 9 7 O i B J A u 3 3 i 3 w U D l F O d 8 = " &gt; A A A B 8 n i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B I v g x b J b B T 0 W v f R Y w X 5 A u 5 Z s m m 1 D s 8 m S z A p l 2 Z / h x Y M i X v 0 1 3 v w 3 p u 0 e t P X B w O O 9 G W b m B b H g B l z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t l G J p q x F l V C 6 G x D D B J e s B R w E 6 8 a a k S g Q r B N M 7 m Z + 5 4 l p w 5 V 8 g G n M / I i M J A 8 5 J W C l X m O Q J t l j O r n w s k G 5 4 l b d O f A q 8 X J S Q T m a g / J X f 6 h o E j E J V B B j e p 4 b g 5 8 S D Z w K l p X 6 i W E x o R M y Y j 1 L J Y m Y 8 d P 5 y R k + s 8 o Q h 0 r b k o D n 6 u + J l E T G T K P A d k Y E x m b Z m 4 n / e b 0 E w h s / 5 T J O g E m 6 W B Q m A o P C s / / x k G t G Q U w t I V R z e y u m Y 6 I J B Z t S y Y b g L b + 8 S t q 1 q n d Z r d 1 f V e q 3 e R x F d I J O 0 T n y 0 D W q o w Z q o h a i S K F n 9 I r e H H B e n H f n Y 9 F a c P K Z Y / Q H z u c P F l u R H w = = &lt; / l a t e x i t &gt; … output representation</formula><p>To our knowledge, this is the largest self-supervised representation learning framework for bipartite graphs. Our source code and large dataset are released for reproducibility and future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>We define Bipartite Graphs as follows: Let 𝐺 = (𝑈 , 𝑉 , 𝐸) be a bipartite graph (as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>), where 𝑈 and 𝑉 denote the set of the two domains of vertices (nodes). 𝑢 𝑖 and 𝑣 𝑗 denote the 𝑖-th and 𝑗-th vertex in 𝑈 and 𝑉 , respectively, where 𝑖 = 1, 2, ..., 𝑀 and 𝑗 = 1, 2, ..., 𝑁 . There are only inter-domain edges, which are defined as 𝐸 ⊆ 𝑈 × 𝑉 . 𝑒 𝑖 𝑗 represents the edge between 𝑢 𝑖 and 𝑣 𝑗 . The incidence matrix for set 𝑈 is 𝐵 𝑢 ∈ R 𝑀×𝑁 and 𝐵 𝑣 ∈ R 𝑁 ×𝑀 for set 𝑉 . 𝐵 𝑢 (𝑖,𝑗) = 1 if 𝑒 𝑖 𝑗 ∈ 𝐸, and 𝐵 𝑢 (𝑖,𝑗) = 0 if 𝑒 𝑖 𝑗 ∉ 𝐸. The features of two sets of nodes can be formulated as 𝑋 𝑢 and 𝑋 𝑣 , respectively, where 𝑋 𝑢 ∈ R 𝑀×𝑃 is a feature matrix with 𝑥 𝑢 (𝑖) ∈ R 𝑃 representing the feature vector of node 𝑢 𝑖 , and 𝑋 𝑣 ∈ R 𝑁 ×𝑄 is similarly defined.</p><p>A core assumption is that the number of nodes and edges in bipartite graphs might be extremely large (on the order of millions or billions) and that there are limited node-wise labels.</p><p>Our work focuses on designing a self-supervised node representation learning model that can exploit both topology information and distinct node features from two domains to improve the accuracy of downstream tasks (e.g., classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL: BGNN</head><p>In this section, we introduce Cascade-BGNN, Cascade Bipartite Graph Neural Networks, a self-supervised representation learning framework for large-scale bipartite graphs. We also summarize the overall algorithm and provide a theoretical analysis.</p><p>Overall Framework. In general, bipartite graph representation learning aims to learn the embedding representation</p><formula xml:id="formula_6">𝐻 𝑢 ∈ R 𝑃 ′ and 𝐻 𝑣 ∈ R 𝑄 ′</formula><p>for nodes in group 𝑈 and 𝑉 , respectively. Let 𝑓 𝑒𝑚𝑏 be a general bipartite graph embedding model with parameters 𝜃 . In order to embed distinct node features 𝑋 𝑢 and 𝑋 𝑣 , the representation of 𝐻 𝑢 and 𝐻 𝑣 is defined as follows:</p><formula xml:id="formula_7">𝐻 𝑢 , 𝐻 𝑣 = 𝑓 𝑒𝑚𝑏 (𝑋 𝑢 , 𝐵 𝑢 , 𝑋 𝑣 , 𝐵 𝑣 ; 𝜃 )<label>(1)</label></formula><p>The entire architecture of 𝑓 𝑒𝑚𝑏 is illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>. There are three key designs within. The first is inter-domain message passing (IDMP), which is represented in blue in Fig. <ref type="figure" target="#fig_3">2</ref>. Its goal is for one domain to aggregate information from the other domain through the connected edges. Formally, it can be expressed as:</p><formula xml:id="formula_8">𝐻 𝑣→𝑢 = 𝑓 𝑢 (𝑋 𝑣 , 𝐵 𝑢 ; 𝜃 𝑢 )<label>(2)</label></formula><formula xml:id="formula_9">𝐻 𝑢→𝑣 = 𝑓 𝑣 (𝑋 𝑢 , 𝐵 𝑣 ; 𝜃 𝑣 )<label>(3)</label></formula><p>where 𝑓 𝑢 and 𝑓 𝑣 are the IDMP function for these two domains respectively, 𝐻 𝑣→𝑢 (resp. 𝐻 𝑢→𝑣 ) represents aggregated information flowing from 𝑉 (resp. 𝑈 ) to 𝑈 (resp. 𝑉 ). More details of IDMP are provided in § 3.1.</p><p>Once the aggregated features from the opposite domain 𝐻 𝑣→𝑢 , 𝐻 𝑢→𝑣 are attached, we use intra-domain alignment (IDA) to fuse these two distinct features into a single representation. IDA is represented by orange in Fig. <ref type="figure" target="#fig_3">2</ref>. Formally, we express it as:</p><formula xml:id="formula_10">𝐿𝑜𝑠𝑠 𝑢 = 𝐿 𝑎𝑑𝑣 (𝐻 𝑣→𝑢 , 𝑋 𝑢 )<label>(4)</label></formula><formula xml:id="formula_11">𝐿𝑜𝑠𝑠 𝑣 = 𝐿 𝑎𝑑𝑣 (𝐻 𝑢→𝑣 , 𝑋 𝑣 )<label>(5)</label></formula><p>where 𝐿 𝑎𝑑𝑣 is specified as an adversarial loss. After one layer training through minimization of Eq. ( <ref type="formula" target="#formula_10">4</ref>)-( <ref type="formula" target="#formula_11">5</ref>) in a self-supervised manner, we obtain the representation of the two domains 𝐻 1 𝑢 and 𝐻 1 𝑣 . Further explanation for IDA is provided in § 3.2. We also provide a theoretical analysis in § 3.5 to explain our design choice of the domain alignment.</p><p>The embedding of 𝐻 1 𝑢 (resp. 𝐻 1 𝑣 ) merely captures the one-hop topology structure of 𝐵 𝑢 (resp. 𝐵 𝑣 ) as well as feature information from 𝑋 𝑢 and 𝑋 𝑣 . As presented in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, the onehop aggregation does not sufficiently characterize diverse graph structures, hence a multi-hop mechanism is required. Instead of leveraging the typical end-to-end training method, this paper develops a cascaded training method to drive multi-hop message passing. Additionally, cascaded training is far more scalable. We will detail the cascaded training in § 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inter-Domain Message Passing (IDMP)</head><p>Formally, the adjacency matrix of a bipartite graph is</p><formula xml:id="formula_12">𝐴 = 0 𝑢,𝑢 𝐵 𝑢 𝐵 𝑣 0 𝑣,𝑣<label>(6)</label></formula><p>where 𝐵 𝑢 and 𝐵 𝑣 are incidence matrices for two partitions, respectively. For stability, we normalize 𝐵 𝑢 as B𝑢 = 𝐷 −1 𝑢 𝐵 𝑢 , where 𝐷 𝑢 is the degree matrix of 𝐵 𝑢 . Similar normalization is done for 𝐵 𝑣 . The IDMP process is defined as</p><formula xml:id="formula_13">𝐻 (𝑘) 𝑣→𝑢 = 𝜎 ( B𝑢 𝐻 (𝑘) 𝑣 𝑊 (𝑘) 𝑢 ) 𝐻 (𝑘) 𝑢→𝑣 = 𝜎 ( B𝑣 𝐻 (𝑘) 𝑢 𝑊 (𝑘) 𝑣 )<label>(7)</label></formula><p>where 𝜎 denotes an activation function, such as ReLU,</p><formula xml:id="formula_14">𝐻 𝑘 𝑣→𝑢 ∈ R 𝑀×𝑄 ′ (resp. 𝐻 (𝑘) 𝑢→𝑣 ∈ R 𝑁 ×𝑃 ′</formula><p>) are hidden features of the nodes in set 𝑈 (resp. 𝑉 ) aggregated from the features in 𝑉 (resp. 𝑈 ), and 𝑘 indicates the depth index (note that when 𝑘 = 0, 𝐻</p><formula xml:id="formula_15">𝑢 = 𝑋 𝑢 , 𝐻<label>(0)</label></formula><p>𝑣 = 𝑋 𝑣 are actually input features).</p><p>As we can see from Eq.7, there are two distinctions between IDMP and conventional GCNs <ref type="bibr" target="#b5">[6]</ref>: 1. IDMP only performs aggregation on each node's neighbor nodes without involving the node itself, while conventional GCN methods usually consider the selfloop computation; 2. The propagation is only one-hop-neighbor aware. The first distinction motivates us to further design an intradomain alignment to take the self-input features into account, while the second one leads to our design in the cascaded training approach which enables multi-hop modeling and supports efficient training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-Domain Alignment (IDA)</head><p>We introduce IDA from the perspective of the domain 𝑈 . We design two types of alignment losses to align 𝐻 𝑣→𝑢 with 𝐻 𝑢 . Our first alignment employs adversarial learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. A discriminator is trained to discriminate between vectors randomly sampled from 𝐻 𝑣→𝑢 and 𝐻 𝑢 . Conversely, IDMP (Inter-Domain Message Passing) is trained as a generator to prevent the discriminator from predicting accurately. As a result, this becomes a two-player min-max game, where the discriminator aims to maximize the ability to identify two distinct feature representations, and IDMP aims to prevent the discriminator from doing so by making the encoded representation 𝐻 𝑣→𝑢 (source) to approach 𝐻 𝑢 (target). After training, they will reach a Nash equilibrium, successfully aligned 𝐻 𝑣→𝑢 and 𝐻 𝑢 .</p><p>Discriminator objective: we define parameters of IDA discriminator as 𝜃 and IDMP generator as 𝜙. We denote 𝑃 𝜃,𝜙 (source = 1|ℎ) as the probability that the input feature vector ℎ is from the source domain 𝐻 𝑣→𝑢 . Conversely, source = 0 signifies that ℎ is from the target domain 𝐻 𝑢 . The discriminator loss function is as follows:</p><formula xml:id="formula_17">𝐿 𝐷 (𝜃 |𝜙) = 1 𝑀 𝑀 ∑︁ 𝑖=1 log 𝑃 𝜃,𝜙 (source = 0|ℎ 𝑢 (𝑖) ) − 1 𝑁 𝑁 ∑︁ 𝑖=1 log 𝑃 𝜃,𝜙 (source = 1|ℎ 𝑣→𝑢 (𝑖) )<label>(8)</label></formula><p>Generator objective: in the generative setting, IDMP is trained to align the encoded representation 𝐻 𝑣→𝑢 (source) to 𝐻 𝑢 (target) so that the discriminator is unable to distinguish them:</p><formula xml:id="formula_18">𝐿 𝐺 (𝜙 |𝜃 ) = 1 𝑁 𝑁 ∑︁ 𝑖=1 log 𝑃 𝜃,𝜙 (source = 0|ℎ 𝑣→𝑢 (𝑖) )<label>(9)</label></formula><p>During training, the discriminator and the generator are trained successively with gradient updates to optimize the two networks, respectively. In experiment § 4.4, when using this method for domain alignment, we call our model BGNN-Adv.</p><p>Another intuitive approach is to utilize multi-layer perceptron (MLP) as IDA to project node features 𝐻 𝑢 and IDMP output 𝐻 𝑣→𝑢 into the same feature space. This approach is relatively straightforward, but we can compare it with BGNN-Adv to judge whether or not adversarial learning can align the domain effectively. Formally, we define the loss function for one set 𝑈 as</p><formula xml:id="formula_19">𝐿 𝑢 = ||MLP(𝐻 (𝑖) 𝑣→𝑢 ) − 𝐻 (𝑖−1) 𝑢 || 𝐹<label>(10)</label></formula><p>which is symmetric for set 𝑉 . The multi-layer perceptron takes the IDMP output as its input and minimizes it with original features in a Frobenius norm. In the experiment § 4.4, we term our model BGNN-MLP when using MLP as IDA. We show that BGNN-Adv outperforms BGNN-MLP on the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cascaded Training: Towards Efficient BGNN</head><p>H (1)   u</p><formula xml:id="formula_20">H (1) v X u X v IDMP IDA E-epochs IDMP IDA E-epochs H (2) u H (2) v … sampling sampling (a) Our Cascaded Training X u X v sampling layer 1 E-epochs H<label>(1)</label></formula><p>u We present the cascaded training design for our proposed BGNN. In Fig. <ref type="figure" target="#fig_4">3</ref>, we depict a detailed diagram to illustrate our cascaded training process in comparison with the conventional end-to-end training paradigm. In cascaded training, we regard one depth (layer) training as training on a basic BGNN block (IDMP with IDA). Each depth completes its training with one-hop embedding in 𝐸 epochs. Then, its trained embedding is used as the input for later training. This process is also illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>. Cascaded training is in contrast to the conventional end-to-end training paradigm (shown in Fig. <ref type="figure" target="#fig_4">3</ref>), which propagates through multiple depths for a fixed number of hops to train the final embedding.</p><formula xml:id="formula_21">H (1) v layer<label>2</label></formula><formula xml:id="formula_22">H (2) u H<label>(2)</label></formula><p>We argue that cascaded training can embed information from multi-hops in bipartite graphs, similarly to how GNN-based methods perform on general graphs. Additionally, cascaded training is more memory-efficient and also requires less training time. Systemwise advantages demonstrate our design choice:</p><p>Only one depth training is alive. This indicates that our method requires a significantly lower memory cost. In Fig. <ref type="figure" target="#fig_4">3</ref>, each depth in a cascaded architecture takes the final embedding from the previous depth as its input. This indicates that we can destroy model instance (release unused memory) in the previous depth and only keep one depth training alive throughout the entire training process.</p><p>Avoid uncontrollable neighborhood expansion. On a largescale graph, the uncontrollable neighborhood expansion of each node layer by layer leads to a low computational speed and high memory cost. Sampling methods like GraphSAGE <ref type="bibr" target="#b5">[6]</ref> and AS-GCN <ref type="bibr" target="#b7">[8]</ref> have been proposed to deal with this issue by reducing the number of neighborhood nodes in each layer. Compared to these methods, since the propagation of our architecture only happens one-hop, the neighborhood expansion issue is avoided, which consequently speeds up the training and reduces memory cost.</p><p>Robust in hyper-parameter tuning. Our cascaded architecture is robust in that it can be easily trained with minimal hyperparameter tuning effort. By using cascaded training, the domain shift (i.e, the discrepancy between two domain features, which always exists during the early training phase) in lower depths will not be passed to higher ones; while in end-to-end training this kind of error accumulates as the depth increases.</p><p>With out statistical performance sacrifice. Notably, systemwise optimization does not sacrifice the statistical performance: multi-hop topology information can also be preserved by cascaded training, and it simultaneously reduces the memory cost and training time on large-scale bipartite graphs.</p><p>We verified the advantages of cascaded training in § 4.5 and § 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithm</head><p>We summarize our overall implementation framework for Cascade-BGNN model in Algorithm 1, which is consistent with Fig. <ref type="figure" target="#fig_3">2</ref> and Fig. <ref type="figure" target="#fig_4">3</ref>. The processes for set 𝑈 and 𝑉 are symmetric. Each step in the outmost loop proceeds as follows, where 𝑘 represents the current layer and 𝐻</p><formula xml:id="formula_23">(𝑘) 𝑢 , 𝐻<label>(𝑘)</label></formula><p>𝑣 are hidden representations in layer 𝑘. For every epoch, sampling is conducted on these hidden representations to get a mini-batch as input. After several epochs of training, the embedding representation of depth 𝑘 can be learned and saved for 𝑘 + 1 layer training. The 𝑘th layer model instance and unused memory are released. The final representation can be extracted in the last layer 𝐾, which can then be used for downstream tasks. The time complexity per epoch for Cascade-BGNN is fixed at 𝑂 (|𝐸|) (|𝐸| denotes the number of edges), where there is no neighborhood expansion along with layer (depth) in traditional end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical Analysis</head><p>In this section, we show that the better the alignment of embedding space in IDA, the better the downstream model M performs on </p><formula xml:id="formula_24">(𝑘+1) 𝑢 ← IDA (𝑘) (ℎ (𝑘) 𝑢 , IDMP (𝑘) (ℎ (𝑘) 𝑣 ) ℎ (𝑘 ) 𝑣→𝑢 ); ℎ (𝑘+1) 𝑣 ← IDA (𝑘) (ℎ (𝑘) 𝑣 , IDMP (𝑘) (ℎ (𝑘) 𝑢 ) ℎ (𝑘 ) 𝑢→𝑣 ); end end Save(𝐻 (𝑘+1) 𝑢 , 𝐻 (𝑘+1) 𝑣</formula><p>) for (𝑘 + 1)th depth training Release(IDMP (𝑘) , IDA (𝑘) , 𝐻 𝑘 𝑢 , 𝐻 𝑘 𝑣 ) end</p><formula xml:id="formula_25">𝑍 𝑢 ← 𝐻 𝐾 𝑢 ; 𝑍 𝑣 ← 𝐻 𝐾 𝑣</formula><p>the node representation. We show this by theoretically analyzing the relationship between alignment effects and the difference between the loss function value on model M on these two domains representations when handling a node classification task. We denote the nodes' label by 𝑦 and its representation vector by ℎ. There are two domains, 𝐻 𝑢 and 𝐻 𝑣→𝑢 from output of IDMP. We assume that 𝑀 outputs the conditional distribution of a node's label 𝑦 based on its representation vector ℎ and model parameter 𝜃 , denoted as P (𝑦|𝑣; 𝜃 ). This gives the probability that a node has a label given the embedding vector of the node. We can write the loss function of the model 𝑀 on one domain 𝐻 𝑢 as 𝐿 𝑀,𝑢 :</p><formula xml:id="formula_26">𝐿 𝑀,𝑢 = E(𝐷 ( P𝑢 (𝑦|ℎ; 𝜃 ), 𝑃 (𝑦|ℎ)))<label>(11)</label></formula><p>where 𝑃 (𝑦|ℎ) is the ground truth, and 𝐷 (𝑃 1 , 𝑃 2 ) measures the distance of two distributions. P𝑢 (𝑦|ℎ; 𝜃 ) means that this prediction is based on training data from the 𝑈 domain. We can rewrite the expectation as:</p><formula xml:id="formula_27">𝐿 𝑀,𝑢 = ∑︁ ℎ 𝑝 𝑢 (ℎ) • 𝐷 ( P𝑢 (𝑦|ℎ; 𝜃 ), 𝑃 (𝑦|ℎ))<label>(12)</label></formula><p>where 𝑝 𝑢 (ℎ) is the distribution in the embedding space. Similarly, the performance of the same model based on another domain 𝐻 𝑣→𝑢 training data can be measured by the loss function:</p><formula xml:id="formula_28">𝐿 𝑀,𝑣→𝑢 = E(𝐷 ( P𝑣→𝑢 (𝑦|ℎ; 𝜃 ), 𝑃 (𝑦|ℎ))) = ∑︁ ℎ 𝑝 𝑣→𝑢 (ℎ) • 𝐷 ( P𝑣→𝑢 (𝑦|ℎ; 𝜃 ), 𝑃 (𝑦|ℎ))<label>(13)</label></formula><p>We introduce a theorem: Theorem 1. If following inequalities are satisfied:</p><formula xml:id="formula_29">𝐷 ( P𝑢 (𝑦|ℎ; 𝜃 ), P𝑣→𝑢 (𝑦|ℎ; 𝜃 )) &lt; 𝑑, ∀ℎ ∈ 𝐻 (14) |𝑝 𝑢 (ℎ) − 𝑝 𝑣→𝑢 (ℎ)| 𝑝 𝑣→𝑢 (ℎ) &lt; 𝜖, ∀ℎ ∈ 𝐻 (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>Then we will have following inequality:</p><formula xml:id="formula_31">𝐿 𝑀,𝑢 ≤ (1 + 𝜖)𝐿 𝑀,𝑣→𝑢 + 𝑑 (16)</formula><p>The Proof for Theorem 1 can be found in the appendix. This theorem shows that by optimizing the representation on the domain 𝐻 𝑣→𝑢 , we are also improving the representation on the domain 𝐻 𝑢 , which is in the form of achieving a lower loss bounded by 𝐿 𝑀,𝑣→𝑢 on the downstream classification model 𝑀 in equation Eq. <ref type="bibr" target="#b15">16</ref>. The closer loss of these two domains produces a similar embedding space, which captures information from both domains. Thus, in order to achieve this, we need to make 𝜖 and 𝑑 in Eq. 14 and 15 smaller. In other words, we need to force distribution 𝑝 𝑢 (ℎ) and 𝑝 𝑣→𝑢 (ℎ) to keep close in the same embedding space. Furthermore, if two domains are close in the embedding space, then models trained on these two domains should output similar classification distributions, which guarantees to have a smaller probability distance in Eq. 14. We achieve the above goals through adversarial learning in BGNN. Consequently, this distribution alignment by IDA is guaranteed to capture information from both domains and result in better representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We design our experiments with the goals of (i) providing a rigorous comparison of the graph representation performance between our BGNN model and state-of-art baselines, (ii) verifying domain alignment and cascaded training, evaluating the BGNN efficiency of space and time complexity on a large-scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In this section, we introduce datasets that we used for experiments and the methods that we prepossessed these datasets. Tencent -Social Networks. This is a large-scale real-world social network represented by a bipartite graph. Nodes in set 𝑈 are social network users, and nodes in set 𝑉 are social communities (e.g., a subset of social network users who share the same interests in electrical products may join the same shopping community). Both users and communities are described by dense off-the-shelf feature vectors. The edge connection between two sets indicates that the user belongs to the community. Note that this dataset provides classification labels for research purposes. In real-world applications, labeling every node is impractical.</p><p>Cora, Citeseer, PubMed -Citation Networks. These are synthetic bipartite graph datasets that are generated from citation networks (single graph) where documents and citation links between them are treated as nodes and undirected edges, respectively.</p><p>Data Preprocessing. The process that we synthesize the bipartite graphs from citation networks is as follows: We process the Cora, CiteSeer, and PubMed datasets similarly and treat the original graph as an undirected graph. First, we divide the paper documents of each class into two equal-sized subsets. Then, we combine the first half of all classes into the 𝑈 group and the second half into the 𝑉 group. We remove some of the features of papers in the 𝑉 group to introduce heterogeneity between 𝑈 and 𝑉 . Lastly, we only keep edges that connected a paper in 𝑈 group and a paper in 𝑉 group and remove all other edges to make the graph bipartite. All isolated nodes are removed. Note that the parameters used for preprocessing do not affect the fairness of the comparison: we have verified that the relative ranking of the performance comparison on different baselines will not change with differing split proportions and dimensional heterogeneity.</p><p>As the Tencent dataset is already a bipartite graph, there is no need to change the graphical structure. To simplify the data loading process, we maintain the same format as the citation network datasets. The statistics of our datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Baselines for comparison. We mainly compare our BGNN algorithm against four unsupervised node embedding baselines:</p><p>• Raw features: This indicates a naive solution in which only raw features are used as input for the classification model, without using any graph structure information incorporated. • Node2Vec <ref type="bibr" target="#b4">[5]</ref>: This approach is an extension of Word2Vec <ref type="bibr" target="#b14">[15]</ref> on graph, which learns a feature representation by simulating biased random walks on the graph. We run Node2Vec on the bipartite graph and then concatenate the node embeddings with their own features. • VGAE <ref type="bibr" target="#b9">[10]</ref>: This method is based on a variational autoencoder, where GCN is used as an encoder and a simple inner product as a decoder to embed the nodes into a lowdimensional feature space. • GraphSAGE-MEAN, GraphSAGE-GCN <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>: We implement two types of aggregator functions: GCN and MEAN aggregator. Node-wise sampling is used to address the scalability issue. • AS-GCN <ref type="bibr" target="#b7">[8]</ref>: This method uses adaptive sampling between each layer to deal with node explosion in large-scale graphs. Since this method is originally designed for supervised learning, we only compare its scalability in § 4.6.</p><p>Note that we do not compare our method with other baselines, e.g. Methpath2vec++, because these models are not tailored to bipartite graphs: they cannot embed distinct features in two domains into a single representation; meta-path in bipartite graphs is the same as unbiased random walk in one of two subgraphs. Therefore, node2vec is adequate to represent them as baselines.</p><p>We cannot directly apply GCN to bipartite graphs due to the inconsistency of the nodes' feature dimensions in the two bipartite partitions. To make the comparison available, we reconstruct the bipartite graph into two subgraphs, where each only contains nodes from one partition with their two-hops connection through the opposite partition, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Through this conversion, GCN based methods can be implemented on two partitions, each with the same feature dimensions respectively, but still containing the original connectivity information in bipartite graphs.</p><p>For each baseline model, we follow the open-source implementation from the authors' original paper (in the appendix, we introduced the source code we used). In order to provide a fair comparison, we also tune the hyper-parameters for every baseline and report the best results among them. For the adversarial learning in IDA, we use a hyperbolic function as the non-linear activation function in the graph convolution networks. The dropout and L2 regularization are applied to each layer to prevent overfitting. During training, we use mini-batch to reduce the memory and computational cost for the large-scale dataset. We found that the optimal batch size for all four data set is near 500, and it only requires around 3 epochs on each data set to quickly converge to the best result. More details about parameter settings for each dataset are shown in the appendix.</p><p>Experiments are conducted on a GPU server with 8 Tesla V100 cards. We describe more details about our tailored system for our BGNN in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Results</head><p>We evaluated our BGNN results on a classification downstream task with 𝐹 1 score <ref type="bibr" target="#b5">[6]</ref> which is a popular metric for classification. For binary classification on the Tencent dataset, we report 𝐹 1 scores. For other multi-classification tasks, we use both micro-and macroaveraged 𝐹 1 scores.</p><p>Performance comparison. From Table <ref type="table" target="#tab_3">2</ref> we can see that BGNN-Adv achieves the best performance on bipartite graph representation learning. BGNN surpasses other methods on both large and small data sets, suggesting its effectiveness in capturing both interdomain and intra-domain information. Particularly on the Tencent large-scale bipartite graph, BGNN-Adv achieves a 29% gain over the raw feature baseline. In the PubMed dataset, due to the balanced degree distribution, even incorporating the graph structure information into the model can only marginally improve the accuracy. However, BGNN still achieves the best results among other baselines, which verifies that it also has a better capacity to embed more information on different long-tail datasets and balanced datasets. The overall evaluation proves that Cascade-BGNN, using distinct features and topology information, is effective for graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Distribution Alignment</head><p>In this section, we conduct ablation studies for the IDA component. For domain fusion, one straightforward method is to concatenate the input feature of each node with its corresponding inter-domain representation as an enhanced feature vector. Nevertheless, such a simple concatenation requires supervising signals, which are not feasible for large-scale graphs that only have very limited node labels. Therefore, to demonstrate the effectiveness of IDA, our ablation studies still focus on unsupervised learning. Comparison with the feature concatenation method. To show the importance of distribution alignment, we compare BGNN with raw features (Raw features in Table <ref type="table" target="#tab_4">3</ref>) input and feature aggregated from another domain (Feature aggregation in Table <ref type="table" target="#tab_4">3</ref>) without any adversarial training. The result is shown in Table <ref type="table" target="#tab_4">3</ref>. BGNN-Adv significantly outperforms these two naive baselines that have no distribution alignment. Both baselines only contain part of the information from the entire graph, which provides a limited representation. This also corresponds with our theory that merging information from two domains in a bipartite graph will lead to improvement in the final representation.</p><p>BGNN-Adv v.s. BGNN-MLP. We replace the adversarial IDA into MLP for self supervision, which we introduced in § 3.2. Compared to BGNN-Adv, BGNN-MLP shows lower prediction results. The reason is that MLP only performs linear mapping in high dimensions between two domains distribution, which limits its ability in nonlinear type distribution alignment typically seen in high dimensions.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Cascaded Training</head><p>In this section, we verified our cascaded training design by comparing it with end-to-end training and evaluating the results on different amounts of cascaded layers. Self-supervised training loss. We demonstrate the training loss of cascaded architecture. During each layer of training, we wait until the loss functions converge before continuing to the next layer step. To reduce the memory cost, rather than training two domains simultaneously in each layer, we alternatively select one domain to train the model. The adversarial training loss function of BGNN is shown in Fig. <ref type="figure" target="#fig_5">4</ref> where we only plot training losses of the first and second layers due to limited space. The generator and discriminator losses clearly demonstrate that the IDA performs an intra-domain alignment in an adversarial way.</p><p>Comparison with end-to-end training. To compare cascaded training with end-to-end training, we specify an end-to-end training architecture in which there are multiple successive IDMP layers and one IDA output layer. In each IDMP layer, one domain's output, the hidden node vector, is the concatenation of raw features and another domain's aggregated node features obtained by IDMP operation. IDA is performed as the final output in an unsupervised manner. The results for comparison are shown in Table <ref type="table" target="#tab_6">5</ref>. We welltuned the IDMP layer number for end-to-end training and find that the architecture with two IDMP layers achieves the best performance. The results demonstrate that our cascaded architecture achieves better performance on multiple datasets. In other words, this experiment verifies that cascaded training does not sacrifice statistical performance. We can also see from the results that end-toend training runs out of memory (OOM) in the large-scale dataset Tencent. Since cascaded training also has system-wise benefits, such as faster training and lower memory cost (details are discussed in § 4.6), it is crucial for large-scale bipartite graphs.</p><p>Effect of cascaded layer number. With deeper cascaded training layers, the model can not only embed multi-hops information into the final representation but can also do so without expanding the memory cost and uncontrollable neighborhood expansion. The experiments in Fig. <ref type="figure" target="#fig_6">5</ref> show the resultsa of Micro 𝐹 1 score of the downstream classification task along with the increase in the number of cascaded layers. Our observations and analysis are as follows:</p><p>• Without cascaded architecture, a single layer BGNN has a relatively lower performance on all datasets. Particularly, a single layer BGNN means that there is no cascaded architecture during learning; only one-time optimization is performed. The reason is that a single layer only embeds one-hop neighbor information in the final representation. • Cascade-BGNN can achieve the best performance when there are two layers. Beyond two layers, the improvement is minimal, and even decreases slightly. This phenomenon is attributed to two reasons: 1. in the bipartite setting, nodes in two domains normally represent entirely different entities (e.g., user and community). Consequently, one node may not have enough of a correlation with its multi-hop connected node from the opposite domain. 2. Techniques that can train truly deep GNN are in demand. We suggest that future research may address this problem by incorporating/exploring the use of trained parameters from previous layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation the efficiency of BGNN on large-scale Bipartite Graphs</head><p>We developed a graph system based on Cascade-BGNN, which is introduced in the Appendix. In order to compare the scalability of BGNN against the baselines, we measure the training time and memory cost of algorithms on the Tencent large-scale dataset. As demonstrated in Fig. <ref type="figure" target="#fig_7">6</ref>, BGNN greatly outperforms baselines in terms of both space and time requirements. This final result is due to our experimental observations beforehand: (1) BGNN does not need to load the entire graph into memory-only one mini-batch is needed. However, all other methods require one to fill the graph into memory first, which corresponds to the huge increase in memory cost  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Graph representation learning methods typically can be classified into two groups, supervised and unsupervised learning.</p><p>Unsupervised methods <ref type="bibr" target="#b11">[12]</ref> traditionally use the adjacency matrix to define and measure the similarity among nodes for graph embedding, which is referred to as matrix factorization. Other works explore using random walks on graphs to learn representation with the skip-gram model. DeepWalk <ref type="bibr" target="#b16">[17]</ref> and Node2vec <ref type="bibr" target="#b4">[5]</ref> are typically representative of these methods to model homogeneous graphs. Some others extend this to heterogeneous graphs, where different nodes are in distinct feature domains, such as MethPath2Vec <ref type="bibr" target="#b1">[2]</ref> and PTE <ref type="bibr" target="#b21">[22]</ref>. Although all these methods do not require node labels in their representation learning, they are shallow embedding approaches and have the following drawbacks. First, the nodes' features and graph structure are independent in the learning methods. Second, they are not efficient when applied to large-scale graphs. Third, the embeddings are transductive hence unseen nodes cannot be embedded with the model being learned so far.</p><p>As for supervised methods, the state-of-art graph-based neural networks have been used to learn node representation with the guide of node labels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. In general, these methods perform a convolution by aggregating the neighbor nodes' information so that each node can learn a relationship between the nodes in the entire graph. However, these methods are task-specific: in another word, they require labels in downstream tasks to supervise the models. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> try to utilize GCN to do unsupervised learning on graphs by performing a random walk or matrix completion on the output of GCN embeddings. However, these approaches still face the same problems above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose Cascade-BGNN, a self-supervised node representation learning framework for bipartite graphs. Cascade-BGNN is domain-consistent, self-supervised, and efficient. Within Cascaded-BGNN, we propose Inter-domain Message Passing (IDMP) as the encoder and Intra-domain Alignment (IDA) by adversarial learning to address the node feature inconsistency issue. We further designed the cascaded training method to capture the multi-hop relationships in local bipartite structure, as well as to improve the scalability. Extensive experiments and theoretical analysis confirmed the effectiveness and efficiency of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>This supplementary material provides source code for reproductivity, more details of the dataset, hyper-parameter settings, more experiment results, the infrastructure, and the future extension to large-scale bipartite graph system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SOURCE CODE FOR REPRODUCTIVITY</head><p>Source code. For experiment results reproductivity, we store this paper's source code at http://bit.ly/Cascade-BGNN. Since we may refactor our code for further research, we maintain the original version of our code in this URL. We also provide the data that we use in this paper for running experiments. Besides the BGNN model, we also provide baseline codes we use in our experiments. Each model's code is organized in an independent directory. In order to help reproduce our results efficiently, in the README.md file at the root directory, we organize a table of scripts for training procedure.  We depict a detailed diagram to illustrate our cascaded training pipeline from the perspective of set 𝑈 , shown in Fig. <ref type="figure" target="#fig_9">7</ref>. The step order is shown as a circle within a number. In step 1, we sample a mini-batch of node feature vectors from group 𝑈 (e.g., 𝑢 1 and 𝑢 2 with red color). In step 2, the QUERY operation takes the sampled node vectors as input and queries their neighbor node vectors from the opposite set 𝑉 (e.g, the queried neighbor vectors are 𝑣 1 , 𝑣 2 , 𝑣 3 , and 𝑣 5 with green color). The inter-domain message passing is in step 3 where neighbor vectors are aggregated to ℎ 𝑣→𝑢 . Then in step 4, the IDA (intra-domain alignment), taking sampled node vectors in 𝑈 and their aggregated neighbor vectors ℎ 𝑣→𝑢 , as input is trained with an adversarial loss. After iterating all mini-batches with multiple epochs, the learned 𝐻 𝑢 is saved for (𝑘 + 1)th depth cascaded training. This pipeline is consistent with Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EFFICIENT TRAINING SYSTEM</head><formula xml:id="formula_32">x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ] x 1 x 2 x Q … [ ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOF OF THEOREM 1</head><p>Here we give a proof of Theorem 1.</p><p>Proof. Note that for 𝐷 (𝑃 1 , 𝑃 2 ) measuring the distance of two distributions we have:</p><formula xml:id="formula_33">𝐷 (𝑃 1 , 𝑃 3 ) ≤ 𝐷 (𝑃 1 , 𝑃 2 ) + 𝐷 (𝑃 2 , 𝑃 3 )<label>(17)</label></formula><p>We then have: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE EXPERIMENTAL EVALUATIONS</head><p>In this section, we provide more information related to our paper, including detailed analysis of datasets and models implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Model Implementation Details</head><p>Logistic regression. In order to evaluate our model output embedding performance, we use logistic regression to predict the nodes' label. We use the logistic SGDClassifier from scikit-learn Python package. We split the nodes into 80 percentage for training and the rest for testing (30% for validation). Model Implementation. We use the code of baselines published by the author of the original paper. We summarize the baseline code we use in Table <ref type="table" target="#tab_8">6</ref>. We follow the parameter settings in their original papers and fine-tuned on our bipartite datasets. The Node2Vec is a high-performance version (C++), so its running time is comparable to ours. Since all the baselines are not designed for heterogeneous bipartite graph, in order to make a fair comparison with our models, we first transform the bipartite graph into a simple connected graph. We multiply the incidence matrix with its transpose to extract all two-hops connection. Since it is a bipartite graph, the two-hops connection of one set will only contain nodes in the exact same set. Through this simple transformation, the graph becomes to a single homogeneous graph, and all the baselines can achieve on it.  Hyper-Parameters. We use a grid-search to tune our model on every dataset to find the best hyperparameters. Here, we list all the final hyperparameters of BGNN for different datasets.</p><p>As for epochs, we first search in a wide range and find that with small epochs size will achieve better performance. This also proves the reason why our model requires less training time. The BGNN-MLP model contains two dense layers with rectified activation layer and dropout layer in between. The output of the decoder is aligned in the range [−1, 1] using hyperbolic tangent, which is the same distribution as the input features. As for BGNN-Adv model, the discriminator also contains two dense layers but with leaky rectified activation layer, which can avoid sparse gradient problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 More Experimental Results on Large-Scale Dataset</head><p>Training loss. Here we further show the training loss of BGNN-MLP model versus iterations on Pubmed dataset in Fig. <ref type="figure" target="#fig_11">8</ref>. The reason is that PubMed is a medium-size dataset with balanced degree distribution, so it is a great dataset to illustrate. The loss function converges after around 500 iterations. The detail parameter settings can be found in table <ref type="table" target="#tab_9">7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Bipartite Graph in E-commerce system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r v 6 e y G m s 9 S Q O b W d M z U g v e z P x P 6 + b m e g m y L l M M 4 O S L R Z F m S A m I b P P y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 n 7 I N w V t + e Z W 0 L m u e W / P u r 6 r 1 2 y K O E p z C G V y A B 9 d Q h w Y 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N K W Z O 4 A + c z x 9 O 0 4 5 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 8 7 l T X g f / + q N T y 9 g p m Z 8 u h w N o k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 6 b G C a Q t t L J v t p F 2 6 2 Y T d j V B C f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p o J r 4 7 r f z t r 6 x u b W d m m n v L u 3 f 3 B Y O T p u 6 S R T D H 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r v 6 e y G m s 9 S Q O b W d M z U g v e z P x P 6 + b m e g m y L l M M 4 O S L R Z F m S A m I b P P y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 n 7 I N w V t + e Z W 0 L m u e W / P u r 6 r 1 2 y K O E p z C G V y A B 9 d Q h w Y 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N K W Z O 4 A + c z x 9 O 0 4 5 X &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 8 7 l T X g f / + q N T y 9 g p m Z 8 u h w N o k o = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I e i x 6 6 b G C a Q t t L J v t p F 2 6 2 Y T d j V B C f 4 M X D 4 p 4 9 Q d 5 8 9 + 4 b X P Q 1 g c D j / d m m J k X p o J r 4 7 r f z t r 6 x u b W d m m n v L u 3 f 3 B Y O T p u 6 S R T D H 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Illustration of Cascade-BGNN. Given the inputs of two domains 𝑋 𝑢 and 𝑋 𝑣 , we obtain their self-supervised representation as 𝐻 𝑢 and 𝐻 𝑣 via inter-domain message passing and intra-domain distribution alignment. To enable multi-hop neighbor information aggregation, we stack multiple layers to formulate a deep BGNN whose layers are trained in a cascading manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of cascaded training with traditional end-to-end training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Adversarial training loss of cascaded architecture. The 𝑥 axis denotes iteration numbers in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of training depth (number of layers) on downstream classification task. The 𝑥 axis denotes the number of BGNN layers and the 𝑦 axis is the Micro 𝐹 1 score .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Memory cost and training time on Tencent data. The 𝑥 axis in left figure denotes the wall-clock time in second, whereas the 𝑦 axis in both figures are the memory cost. The short blue line of BGNN and orange line of AS-GCN mean the training has finished, whereas the training time of GraphSAGE is too long to be shown.</figDesc><graphic url="image-277.png" coords="9,53.80,83.69,240.24,106.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Cascaded Training Pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>𝐿</head><label></label><figDesc>𝑀,𝑢 − 𝐿 𝑀,𝑣→𝑢 = ∑︁ ℎ 𝑝 𝑢 (ℎ) • 𝐷 (𝑃 𝑢 (𝑦|ℎ; 𝜃 ), P (𝑦|ℎ)) − ∑︁ ℎ 𝑝 𝑣→𝑢 (ℎ) • 𝐷 (𝑃 𝑣→𝑢 (𝑦|ℎ; 𝜃 ), P (𝑦|ℎ)) ≤ ∑︁ ℎ (𝑝 𝑢 − 𝑝 𝑣→𝑢 )(ℎ) • 𝐷 (𝑃 𝑣→𝑢 (𝑦|ℎ; 𝜃 ), P (𝑦|ℎ)) + ∑︁ ℎ 𝑝 𝑢 (ℎ) • 𝐷 (𝑃 𝑢 (𝑦|ℎ; 𝜃 ), 𝑝 𝑣→𝑢 (𝑦|ℎ; 𝜃 )) = 𝜖𝐿 𝑀,𝑣→𝑢 + 𝑑 (18) □</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: BGNN-MLP training loss on Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Cascade-BGNN algorithm Input: Graph 𝐺 (𝑈 , 𝑉 , 𝐸); input features {𝑋 𝑢 , 𝑋 𝑣 } Output: Node representation 𝑍 𝑢 and 𝑍 𝑣 𝐻 0 𝑢 ← 𝑋 𝑢 ; 𝐻 0 𝑣 ← 𝑋 𝑣 for 𝑘 = 1, ...𝐾 do for e in epochs do</figDesc><table><row><cell cols="2">Sampling batches (ℎ 𝑢 , ℎ (𝑘)</cell><cell>(𝑘) 𝑣 ) from (𝐻 𝑢 , 𝐻 (𝑘)</cell><cell>(𝑘) 𝑣 )</cell></row><row><cell>for ℎ 𝑢 , ℎ (𝑘)</cell><cell cols="2">(𝑘) 𝑣 as batches of input do</cell></row><row><cell>ℎ</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell>Dataset</cell><cell cols="4">Tencent Cora Citeseer PubMed</cell></row><row><cell>#Edges</cell><cell cols="2">991,734 1,802</cell><cell>1,000</cell><cell>18,782</cell></row><row><cell cols="2">U 619,030</cell><cell>734</cell><cell>613</cell><cell>13,424</cell></row><row><cell>#Nodes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">V 90,044</cell><cell>877</cell><cell>510</cell><cell>3,435</cell></row><row><cell>U</cell><cell>8</cell><cell>1,433</cell><cell>3,703</cell><cell>400</cell></row><row><cell>#Features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V</cell><cell>16</cell><cell>1,000</cell><cell>3,000</cell><cell>500</cell></row><row><cell>U</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>#Classes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>V</cell><cell>N/A</cell><cell>6</cell><cell>6</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Prediction results for the four datasets (𝐹 1 scores). Results for BGNN unsupervised nodes embedding are shown. (OOM means out of memory)</figDesc><table><row><cell></cell><cell>Tencent</cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell><cell cols="2">PubMed</cell></row><row><cell>Methods</cell><cell>𝐹 1</cell><cell>Micro 𝐹 1</cell><cell>Macro 𝐹 1</cell><cell>Micro 𝐹 1</cell><cell>Macro 𝐹 1</cell><cell>Micro 𝐹 1</cell><cell>Macro 𝐹 1</cell></row><row><cell>Raw features</cell><cell>0.497</cell><cell>0.789</cell><cell>0.758</cell><cell>0.707</cell><cell>0.621</cell><cell>0.838</cell><cell>0.843</cell></row><row><cell>Node2Vec</cell><cell>0.577</cell><cell>0.810</cell><cell>0.780</cell><cell>0.724</cell><cell>0.627</cell><cell>0.834</cell><cell>0.839</cell></row><row><cell>VGAE</cell><cell>OOM</cell><cell>0.782</cell><cell>0.754</cell><cell>0.732</cell><cell>0.645</cell><cell>0.823</cell><cell>0.828</cell></row><row><cell>GraphSAGE-GCN</cell><cell>0.529</cell><cell>0.782</cell><cell>0.763</cell><cell>0.715</cell><cell>0.627</cell><cell>0.838</cell><cell>0.843</cell></row><row><cell>GraphSAGE-MEAN</cell><cell>0.580</cell><cell>0.823</cell><cell>0.801</cell><cell>0.748</cell><cell>0.665</cell><cell>0.838</cell><cell>0.843</cell></row><row><cell>BGNN-Adv</cell><cell cols="7">0.622±0.017 0.859±0.005 0.831±0.007 0.768±0.004 0.698±0.005 0.857±0.005 0.860±0.005</cell></row><row><cell>% gain over feat.</cell><cell>25%</cell><cell>9%</cell><cell>9%</cell><cell>9%</cell><cell>12%</cell><cell>2%</cell><cell>2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparsion of result without domain alignment with BGNN-Adv (Micro 𝐹 1 score)</figDesc><table><row><cell></cell><cell cols="4">Tencent Cora Citeseer PubMed</cell></row><row><cell>Raw features</cell><cell>0.497</cell><cell>0.789</cell><cell>0.707</cell><cell>0.838</cell></row><row><cell>Feature aggregation</cell><cell>0.453</cell><cell>0.625</cell><cell>0.431</cell><cell>0.569</cell></row><row><cell>BGNN-Adv</cell><cell>0.622</cell><cell>0.859</cell><cell>0.768</cell><cell>0.857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparsion of BGNN-MLP with BGNN-Adv (Micro 𝐹 1 score)</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Tencent Cora Citeseer PubMed</cell></row><row><cell>.</cell><cell>BGNN-MLP</cell><cell>0.582</cell><cell>0.784</cell><cell>0.756</cell><cell>0.846</cell></row><row><cell></cell><cell>BGNN-Adv</cell><cell>0.622</cell><cell>0.859</cell><cell>0.768</cell><cell>0.857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of two layers end-to-end training with cascaded BGNN-Adv (Micro (Macro) 𝐹 1 score)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">End-to-end training Cascaded training</cell></row><row><cell></cell><cell>Tencent</cell><cell>OOM</cell><cell>0.622</cell></row><row><cell>.</cell><cell>Cora</cell><cell>0.837 (0.809)</cell><cell>0.859 (0.831)</cell></row><row><cell></cell><cell>Citeseer</cell><cell>0.685 (0.642)</cell><cell>0.768 (0.698)</cell></row><row><cell></cell><cell>PubMed</cell><cell>0.859 (0.859)</cell><cell>0.857 (0.860)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>at the start of training. (2) BGNN does not require sampling in each graph convolutional layer and thus avoids an additional timeconsuming procedure. (3) The unique unsupervised learning loss in BGNN based on adversarial learning does not require further computational processes. For example, in GraphSAGE, the unsupervised loss is based on random walks, causing it to increase significantly with the graph size. Additional nodes and a longer walk length are needed to maintain high performance, which consequently requires an even longer training time and a larger memory cost. (4) in § 4.5, we introduced that cascaded training saves memory and avoids uncontrollable neighborhood expansion, reducing the memory cost and expediting training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Reference for baselines codeBaseline</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for BGNN on four datasets</figDesc><table><row><cell></cell><cell>Hyperparameters</cell><cell cols="4">Tencent Citeseer Cora PubMed</cell></row><row><cell>BGNN-Adv</cell><cell>batch size</cell><cell>600</cell><cell>400</cell><cell>400</cell><cell>700</cell></row><row><cell></cell><cell>epochs</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell><cell>0.0004</cell></row><row><cell></cell><cell>weight decay</cell><cell>0.0005</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell></row><row><cell></cell><cell>dropout</cell><cell>0.4</cell><cell>0.35</cell><cell>0.35</cell><cell>0.35</cell></row><row><cell></cell><cell>encoder output dimensions</cell><cell>16</cell><cell>16</cell><cell>24</cell><cell>24</cell></row><row><cell>BGNN-MLP</cell><cell>batch size</cell><cell>500</cell><cell>64</cell><cell>128</cell><cell>128</cell></row><row><cell></cell><cell>epochs</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>3</cell></row><row><cell></cell><cell>learning rate</cell><cell>0.0003</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0001</cell></row><row><cell></cell><cell>weight decay</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0008</cell><cell>0.005</cell></row><row><cell></cell><cell>dropout</cell><cell>0.4</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell></cell><cell>encoder output dimensions</cell><cell>24</cell><cell>48</cell><cell>48</cell><cell>48</cell></row><row><cell></cell><cell>decoder hidden dimensions</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Because of privacy and regulatory restrictions, in this paper, we only release a millionlevel dataset for the experimental demonstration.</note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code link Node2Vec (high performance version) https://github.com/snap-stanford/snap VGAE https://github.com/tkipf/gae GraphSage https://github.com/williamleif/GraphSAGE GCN https://github.com/williamleif/GraphSAGE AS-GCN https://github.com/huangwb/AS-GCN</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<idno>arXiv: 1801.10247</idno>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<idno>arXiv: 1406.2661</idno>
		<title level="m">Generative Adversarial Networks</title>
				<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<idno>arXiv: 1706.02216</idno>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Junction Tree Variational Autoencoder for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<idno>arXiv: 1802.04364</idno>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<idno>arXiv: 1611.07308</idno>
		<title level="m">Variational Graph Auto-Encoders</title>
				<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Amazon.com recommendations: item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">N-gram graph: Simple unsupervised representation for graphs, with applications to molecules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8464" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarially Regularized Graph Autoencoder for Graph Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6652</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;14</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining -KDD &apos;18</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining -KDD &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collective Classification in Network Data</title>
				<imprint>
			<date type="published" when="2008-09">Sept. 2008</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">PTE: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation Learning for Scene Graph Completion via Jointly Structural and Visual Embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="949" to="956" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -KDD &apos;16<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
				<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
