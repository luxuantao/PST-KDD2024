<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Defending and Harnessing the Bit-Flip based Adversarial Weight Attack</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhezhi</forename><surname>He</surname></persName>
							<email>zhezhihe@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical, Computer and Energy Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adnan</forename><forename type="middle">Siraj</forename><surname>Rakin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical, Computer and Energy Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingtao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical, Computer and Energy Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaitali</forename><surname>Chakrabarti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical, Computer and Energy Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deliang</forename><surname>Fan</surname></persName>
							<email>dfan@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical, Computer and Energy Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Defending and Harnessing the Bit-Flip based Adversarial Weight Attack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a new paradigm of the adversarial attack on the quantized neural network weights has attracted great attention, namely, the Bit-Flip based adversarial weight attack, aka. Bit-Flip Attack (BFA). BFA has shown extraordinary attacking ability, where the adversary can malfunction a quantized Deep Neural Network (DNN) as a random guess, through malicious bit-flips on a small set of vulnerable weight bits (e.g., 13 out of 93 millions bits of 8-bit quantized ResNet-18). However, there are no effective defensive methods to enhance the fault-tolerance capability of DNN against such BFA. In this work, we conduct comprehensive investigations on BFA and propose to leverage binarizationaware training and its relaxation -piece-wise clustering as simple and effective countermeasures to BFA. The experiments show that, for BFA to achieve the identical prediction accuracy degradation (e.g., below 11% on CIFAR-10), it requires 19.3× and 480.1× more effective malicious bitflips on ResNet-20 and VGG-11 respectively, compared to defend-free counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the Deep Neural Networks (DNNs) achieve humansurpassing performance in multiple computer vision related tasks, its applications in the real-world scenarios are growing rapidly. In such a scenario, the fault-tolerance capability of the neural network is of great research interest for developing reliable neural networks against weak random fault and even strong malicious attacks. A significant amount of research effort has focused on DNNs being fooled by human-imperceptible input noise, aka. adversarial example. Figure <ref type="figure">1</ref>: The fault injection on the identified vulnerable weight bits can be physically conducted by Row-Hammer Attack (RHA) <ref type="bibr" target="#b18">[19]</ref>. Meanwhile, the DNN under defense has higher resistance against the malicious bit-flips.</p><p>(DRAM). Recent research advances have brought up the vulnerability issue of data stored in DRAM, where Row-Hammer Attack (RHA) <ref type="bibr" target="#b18">[19]</ref> has been shown to maliciously flip the memory bits in DRAM without being granted any data write privileges, as depicted in Fig. <ref type="figure">1</ref>. Unfortunately, DNNs stored in DRAM with floating-point representation can be easily hacked to fully malfunction, through single bit-flip (e.g. in an exponential bit of any weight) through RHA <ref type="bibr" target="#b7">[8]</ref>. Thanks to the DNN weight quantization technique, DNN is more compact since the weights are represented in a fixed-point format with constrained representation. Such a representation has been proven to significantly enhance the immunity of quantized DNN to such malicious bit-flips in <ref type="bibr" target="#b7">[8]</ref>. However, a newly proposed Bit-Flip Attack (BFA) <ref type="bibr" target="#b16">[17]</ref> whose progressive bit searching algorithm can successfully identify and flip an extremely small number of vulnerable weight bits (e.g., 13 out of 93 millions bits of ResNet-18 on ImageNet) to degrade a large scale 8-bit quantized DNN inference accuracy to as low as a random guess (i.e., from 69.8% to 0.1%). Up to now, there is still a lack of effective defensive approaches against such BFA, and so we propose a BFA countermeasure based on utilizing weight binarization and its relaxation -piece-wise clustering. The contributions in this work can be summarized as:</p><p>• A comprehensive investigation of bit-flip based adversarial weight attack (i.e., BFA) is conducted, and several insightful observations are obtained for understanding parameter vulnerability to these attacks.</p><p>• Weight binarization and its piece-wise clustering relaxation method are proposed as the effective defensive techniques against BFA.</p><p>• Additional adversarial attack defense methods (e.g., adversarial training, pruning) and conventional model regularization methods are examined as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bit-Flip based Adversarial Weight Attack</head><p>The bit-flip based adversarial weight attack, aka. Bit-Flip Attack (BFA) <ref type="bibr" target="#b16">[17]</ref>, is an adversarial attack variant which performs weight fault injection through flipping the bits. For the machine-imperceptible purpose, the BFA only flips the most vulnerable weight bits which are identified by Progressive Bit Search (PBS) algorithm with iterative interand intra-layer search.</p><p>Given a n q -bit quantized DNN parameterized by bits (i.e., quantized weights in binary), define tensor {B l } L l=1 , where l ∈ {1, 2, ..., L} is the layer index. The intralayer search that identifies the bit with highest gradient (arg max B l |∇ B l L|) as vulnerable bit candidate, where L is the inference loss. Then, the inter-layer searching compares the bit candidates selected by the intra-layer search through directly checking the loss increment. Thus, the bit searching in iteration i can be formulated as an optimization process <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_0">max { Bi l } L f x; { Bi l } L l=1 , t s.t. t = f (x; {B l } L l=1 ); L l=1 D( Bi l , B l ) ∈ {0, 1, ..., N b }<label>(1)</label></formula><p>where x and t denotes the selected input mini-batch and ground-truth labels. Bi l is the quantized bit tensor of l-th layer perturbed by BFA in i-th iteration. f (x; {B} L l=1 ) compute the outputs of DNN parameterized by {B l } L l=1 . t is the output of clean model as the soft-label, which replaces the ground-truth t to perform the attack. L(•, •) computes the loss. The attack efficiency is measured by the Hamming distance (i.e., effective bit-flips) between prior-and post-attack model parameters { Bi l } L l=1 and {B l } L l=1 given by D( Bi l , B l ). In general, the optimization goal of BFA is to cause the DNN to malfunction with least number of bit-flips (i.e., min D( Bi l , B l )).</p><p>Table <ref type="table">1</ref>: Threat model of Bit-Flip Attack (BFA).</p><p>Access Required ✓ Access NOT Required ✗</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model topology &amp; parameters</head><p>Hyper-parameters and other training configurations.</p><p>A mini-batch of sample data Complete train/test datasets.</p><p>It is noteworthy that the quantized weight in fixed-point format is magnitude constrained (i.e., max(B) = 2 nq−1 ) in comparison to the floating-point counterpart, which is not only more biologically plausible but also practically necessary for the acceleration of modern AI applications. To clarify, we use the same threat model as in prior work <ref type="bibr" target="#b16">[17]</ref>, which is listed in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Defense against Adversarial Example</head><p>As the BFA is an adversarial attack variant, the popular techniques used to defend adversarial example <ref type="bibr" target="#b4">[5]</ref> are investigated to seek potential BFA defense method.</p><p>Adversarial Training. Adversarial training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> is by-far the most successful adversarial example defense method, that optimizes the DNN parameters θ w.r.t both the clean input x and their adversary examples x as:</p><formula xml:id="formula_1">min θ L(f (x; θ), t) + α • L(f ( x; θ), t)<label>(2)</label></formula><p>where α is the hyper-parameter to balance the accuracy of the trained model on clean natural data and adversarial examples. t is the soft-label as in Eq. <ref type="bibr" target="#b0">(1)</ref>. Such adversarial training is also normally considered as a strong regularization technique.</p><p>Increasing model capacity. Prior works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> have experimentally confirmed the resistance improvement against the adversarial attack by increasing the model capacity. It is interpreted as that the robust classifiers would require a more complicated decision boundary <ref type="bibr" target="#b14">[15]</ref>, which is expected to benefit the defense against malicious weight change as well. Further in-depth analysis of model capacity and BFA resistance is discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DNN under BFA 101</head><p>To first understand, then to defend and harness the bitflip based adversarial weight attack, we conducted some preliminary investigations, along with several important observations as described below.  Observation 1 BFA is prone to flip bits of close-to-zero weights, and cause large weight shift.</p><p>As depicted in Fig. <ref type="figure" target="#fig_2">2</ref>, the progressive bit search proposed in BFA <ref type="bibr" target="#b16">[17]</ref> is prone to identify vulnerable bit in the weight whose absolute value has a small magnitude (i.e., |w| → 0) then modify it to be a large value (explained in the caption of Fig. <ref type="figure" target="#fig_2">2</ref>). Since the BFA performs the attack on the quantized weight encoded in two's complement, the possible weight magnitude shift is discretized as 2 i , i ∈ {0, 1, ..., n q }. Moreover, Fig. <ref type="figure" target="#fig_2">2</ref> also shows the model with larger capacity possesses higher resistance against BFA (i.e., require more bit-flips for same accuracy degradation).</p><p>Observation 2 BFA is prone to flip the weight bits in the front-end layers of the target neural network.</p><p>Fig. <ref type="figure">3</ref> shows the histogram of bit-flips across different modules 1 of DNN under BFA. All the trials show that most of the bits found by BFA are mostly in the front-end, along the forward propagation path. Such an observation can be explained as the error introduced by the bit-flips in the frontend can be easily accumulated and amplified during the forward propagation, which is similar to the linear explanation of adversarial example discussed in <ref type="bibr" target="#b4">[5]</ref>.</p><p>1 module index includes all modules within DNN, where small to large index denotes the location from front to rear along the inference path. It is intriguing to notice that, with the evolution of BFA, it forces almost all inputs to be classified into one output group. We also find that the dominant output group highly depends on the given attack sample data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Defense against BFA</head><p>To enhance the resistance of DNN against BFA, we propose and investigate two defense techniques, i.e., Binarization-aware training and its relaxation -piece-wise weight clustering, inspired by the observations in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Binarization-aware training</head><p>binarization-aware training is originally proposed as an extreme low bit-width model compression technique, which converts the weights from 32-bit floating-point to {-1,+1} binary format encoded by 1-bit <ref type="bibr" target="#b17">[18]</ref>. Here, the binarizationaware training is leveraged as a defense technique against BFA, which can be mathematically described as:</p><formula xml:id="formula_2">Forward : w b l,i = E(|W fp l |) • sgn(w fp l,i ) Backward : ∂L ∂w b l,i = ∂L ∂w fp l,i<label>(3)</label></formula><p>where sgn() is the sign function. w b l,i denotes the binarized weight from its floating-point counterpart w fp l,i . In general, weight binarization intrinsically achieves two goals: 1) reducing the bit-width to 1, and 2) clustering the weights to ±E(|W fp l |) as in Eq. ( <ref type="formula" target="#formula_2">3</ref>). The Straight Through Estimator (STE) <ref type="bibr" target="#b1">[2]</ref> is adopted to address the non-differential problem for the sign function as prior works <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, different from STE in <ref type="bibr" target="#b9">[10]</ref>, the gradient clipping constraint is omitted from the backward path, thanks to the presence of weight scaling coefficient E(|W fp l |). Our interpretation of the BFA resistance enhancement through binarization-aware training comes in twofold: 1)  </p><formula xml:id="formula_3">∆w fp = η • ∇ w fp L(f (x, {W fp }); t) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where η is the learning rate. Due to the presence of Eq. ( <ref type="formula" target="#formula_2">3</ref>), even small weight update on w fp (i.e., w fp − ∆w fp ) may directly change the corresponding binarized weight w b from -1 to +1 or the opposite as a bit-flip, when the following condition is meet:</p><formula xml:id="formula_5">sgn(w fp − ∆w fp ) = sgn(w fp )<label>(5)</label></formula><p>Therefore, the binarization-aware training involves massive bit-flips on the binarized weight W fp , which mimics injecting the bit-flips noise on the weights during training. Fig. <ref type="figure" target="#fig_6">6</ref> depicts the average number of bit-flips caused by the weight update when training a binarized ResNet-20, each iteration may cause around 300 bit-flips on the binarized weights even when the learning rate is 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Clustering as relaxation of binarization</head><p>Since weight binarization normally suffers from significant prediction accuracy degradation due to aggressive model capacity reduction, we propose a relaxation to the weight binarization, called Piece-wise Clustering (PC), to emit the fixed single bit-width constraint while retaining similar functionality of clustering, which we believe play an important role in defending BFA. The piece-wise clustering introduces an additional weight penalty to the inference loss L (e.g., cross-entropy), and the optimization can be formulated as:</p><formula xml:id="formula_6">min {W l } L l=1 E x L(f (x, {W l } L l=1 ), t)+ λ • L l=1 (||W + l − E(W + l )|| 2 + ||W − l − E(W − l )|| 2 )</formula><p>piece-wise clustering penalty term <ref type="bibr" target="#b5">(6)</ref> where λ is the clustering coefficient to tune the strength of the weight clustering penalty term. W + l and W − l denote the positive and negative weight subset of l-th layer weight tensor. The DNN model optimized as Eq. ( <ref type="formula">6</ref>) leads to a bi-modal weight distribution as depicted in Fig. <ref type="figure" target="#fig_5">5d</ref>. The piece-wise clustering proposed above can also be viewed as a variant of group Lasso, where the group is defined as the positive and negative weight subsets in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>Dataset and Network Architectures In this work, experiments are focused on visual dataset CIFAR-10 <ref type="bibr" target="#b11">[12]</ref>, which includes 60k 32 × 32 RGB images evenly sampled from 10 categories, with 50k and 10k samples for training and test respectively. The data augmentation technique is identical as reported in <ref type="bibr" target="#b5">[6]</ref>. The ResNet-20 <ref type="bibr" target="#b5">[6]</ref> and VGG-11 <ref type="bibr" target="#b19">[20]</ref> are the two networks studied in the work. We use the momentum-based stochastic gradient descent optimizer, with training batch-size and weigh decay as 128 and 3e-4 respectively. The initial learning rate is 0.1 that scaled by 0.1 at 80 and 120 epochs, and the total number of epochs is 160. Note that, all the experiments are conducted using Pytorch <ref type="bibr" target="#b15">[16]</ref>, running on NVIDIA Titan-XP GPUs.   BFA Configuration. To evaluate the effectiveness of the proposed defense methods, the code from <ref type="bibr" target="#b16">[17]</ref> is utilized with further modification. The number of bit-flips N BF that degrades the prediction accuracy below 11% is used as the metric to measure the BFA resistance, for CIFAR-10 dataset. Moreover, since BFA requires a set of data to perform the attack, we take 256 sample images from the training subset as the default BFA configuration and report the mean±std of N BF with 5 BFA trials. Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in <ref type="bibr" target="#b16">[17]</ref>, but with quantization-aware training instead of post-training quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Result Evaluation</head><p>The experiment results with different quantization bitwidth n q and clustering coefficient λ of piecewise clustering are summarized in Fig. <ref type="figure" target="#fig_8">7</ref>. It reports the BFA-free test accuracy and number of bit-flips N BF required for BFA to succeed. Note that, for weight binarization in Fig. <ref type="figure" target="#fig_8">7</ref>, we exclude the Piece-wise Clustering (PC) term through setting λ = 0, since binarization intrinsically performs the clustering as discussed in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of quantization bitwidth and clustering coefficient.</head><p>Based on the results reported in Fig. <ref type="figure" target="#fig_8">7</ref>, training the DNN with binarized weights roughly degrade the test accuracy by ∼ 4% and ∼ 2% in comparison to the 8-bit quantized counterpart, for ResNet-20 and VGG-11. As discussed in Section 4.2, our intention of proposing the piecewise clustering as the relaxation to weight binarization is to mitigate such accuracy drop. We do observe that using piece-wise clustering with proper λ can mitigate the accuracy degradation while improving the BFA resistance (i.e., requiring more bit-flips N BF for the same accuracy degradation). For ResNet-20 and VGG-11, the ideal configurations of λ are 0.001 and 0.0005 respectively, as the model with larger capacity benefits from relatively smaller λ.</p><p>BFA resistance of ResNet-20. The 8-bit quantized ResNet-20 (baseline) requires only an average of 28 bit-flips to hamper the functionality of an accurate DNN, while the weight binarization significantly improves the BFA resistance compared to the baseline. Binarization increases the average value of N BF to 541.2, which improve the BFA resistance by ∼ 19×. Nevertheless, considering the inevitable accuracy drop due to the drastic bit-width reduction (32-bit to 1-bit), as an alternative approach we explore the performance of PC on other bit-width configurations as well. With λ = 1e − 3, the average value of N BF was improved by 2.09×, 2.55× and 1.73× for 8, 6 and 4 bit-width respectively. In conclusion, our proposed piece-wise clustering improves the resistance to adversarial weight attack for all the cases of different bit-widths. Still, the binarized network emerges as the most successful defense against BFA.</p><p>BFA resistance of VGG-11. For VGG-11, our observation follows a similar pattern as described in the previous section. The baseline VGG-11 (e.g., n q = 8) requires an average N BF of 16.4. Again, weight binarization improves the network robustness significantly, yielding an average N BF of 7874; which is ∼ 480× improvement in comparison to the baseline. In the case of VGG-11, the lower Bit-Width defends BFA even better than ResNet-20; the main reason for this discrepancy can be the difference between the size of the network. For a larger network such as VGG-11, low Bit-Width and PC performs a proper regularization to successfully defend against BFA. The best performance of PC for VGG-11 was achieved for a 4-bit network with λ = 1e − 4 achieving an average value of N BF of 82.59.</p><p>In summary, both the binarization-aware training and its piecewise clustering relaxation can improve the BFA resistance of the target neural network, while the binarizationaware training can push the N BF to an extremely large value (e.g., N BF &gt; 7000 on over-parameterized VGG-11). The implication of such large value is noteworthy, as a larger value of N BF indicates the significant increase in difficulty to carry out a memory fault injection through the feasible cyber-physical attacks. For example, when using the rowhammer attack to perform the fault injection, the increased attack execution time might be detected by the operating system through the integrity check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison of Alternative Defense Methods</head><p>Adversarial weight attack <ref type="bibr" target="#b16">[17]</ref> is a recently developed security threat model for modern DNN. Subsequently, the development of defensive approaches in this field has not received much attention. Therefore, for the first time, we investigate an alarming parameter security concern -bitflip based adversarial weight attack, with corresponding defense method. Owing to the lack of competing methods in this research direction, we attempt to transfer several conventional defense methods of adversarial examples <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>, for providing a comprehensive comparison. Weight Pruning. Both the activation and weight pruning have been investigated as the defense against adversarial example <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>. Such pruning techniques involve the stochastic process during the inference which suffers from gradient obfuscation <ref type="bibr" target="#b0">[1]</ref> which is a common reason for the failure of adversarial input defenses. Nevertheless, we investigate the effectiveness of network pruning to resist adversarial weight attack as an alternative approach. To achieve this, we train a regular network with Lasso loss function to shrink most of the weights to an extremely low value. Thus, we can rewrite the loss function with additional L 1 -norm penalty as:</p><formula xml:id="formula_7">min {W} L(f (x, {W l }); t) + β • L l=1 ||W l || 1 (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where β is the coefficient to tune the pruning strength.</p><p>Through training with Eq. ( <ref type="formula" target="#formula_7">7</ref>), we expect the weight tenor to be in a highly sparse representation.</p><p>The intuition behind pruning working as an adversarial weight defense can be summarized as: In a sparse network, we consider these zero-valued weights will not have any physical connection (pruned) to conduct a bit-flip attack, thus making them immune from BFA. As a result, the attacker is left with only a few portions of the weights which he/she can alter to perform the BFA. Nevertheless, as shown in Table <ref type="table" target="#tab_1">2</ref>, such a sparse regularized network is even more vulnerable to adversarial weight attack, requiring on average just 6.8 bit-flips to hamper the functionality of target DNN. Since a large portion of the weights was pruned, the remaining weights contain large significance in maintaining accurate network performance. So altering any of the remaining non-zero weights still manages to degrade network performance significantly.</p><p>Adversarial Weight Training. Inspired by the adversarial training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, we attempt to adopt the same idea and create a BFA-based adversarial training as an alternative approach to compare with our proposed method. We modified the adversarial training objective in Eq. ( <ref type="formula" target="#formula_1">2</ref>) to serve the purpose of adversarial weight training:</p><formula xml:id="formula_9">min {B} L(f (x; {B}), t) + αL(f (x; {B} + {B BFA }), s.t. {B BFA } = { B} − {B} (8)</formula><p>where {B BFA } is the different between BFA-perturbed weight bits { B} and its BFA-free counterpart {B}. During the model training, {B BFA } is run-time generated as the additive constant offset on the model weights.</p><p>The result of adversarial training is shown in Table <ref type="table" target="#tab_1">2</ref>, where it does not show the improvement of BFA resistance from such adversarial weight training. Our interpretation of the defense failure is summarized in the following. When performing the adversarial training with the adversarial examples, each natural image owns similar adversarial examples even with a different random seed. However, for the bit-flip based adversarial weight attack, using a single natural image as the attack sample will lead to massive different combinations of vulnerable weight bits, while BFA just provides one combination in a greedy way. Thus, performing the adversarial weight training with all the vulnerable weight bit combinations is not a feasible approach. In the end, through the comparison of all the potential defense methods listed in Table <ref type="table" target="#tab_1">2</ref>, we conclude that the binarizationaware training and the piecewise clustering are the effective defense methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis</head><p>Effect of Network Width. In prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>, enhance the capacity of target DNN via increasing the layer width is recognized as an effective method to defend adversarial example. In this work, it is expected that the DNN capacity also plays a positive role in defending against BFA. We summarize the performance of BFA by varying the width of the network in Table <ref type="table" target="#tab_2">3</ref>. In the first row of Table <ref type="table" target="#tab_2">3</ref>, we observe that ResNet-20 becomes more resilient to BFA with network width 4× than the baseline. However, the difference between the performance of the baseline and network with 2× width is barely distinguishable. In the second row of Table <ref type="table" target="#tab_2">3</ref>, a similar pattern is also observed utilizing our proposed piece-wise clustering method. Our proposed method with 4× width requires on average 72 bit-flips to cause complete malfunction of ResNet-20 architecture. In conclusion, similar to the observations from adversarial example <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>, increasing the network capacity by a large amount will enhance the robustness against BFA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion 1 Increasing the network capacity improves the resistance to bit-flip based adversarial weight attack.</head><p>Effect of Batch-Normalization and Dropout Nowadays, the presence of Batch-Normalization (BN) layer in the deep neural network is customary to accelerate the training of DNN <ref type="bibr" target="#b10">[11]</ref>, by normalizing the hidden features that forwarded along the inference path. On the other hand, an adversarial weight attack introduces variance in the weight tensor through malicious bit-flips on the weight bits, which changes the hidden features correspondingly. Taken the batch normalization into consideration, we expect the BN layer to stabilize the hidden feature errors caused by the malicious weight bit-flips. As the result listed in Table <ref type="table" target="#tab_3">4</ref>, we remove the BN layer (case 2) from our baseline model (case 1). We observe that once the BN layer was removed from the VGG-11 network, it becomes more vulnerable to a weight attack requiring less than just 10 bit-flips to cause malfunction of DNN. Thus, we conclude Batch-Normalization is able to slightly stabilize the Bit-Flip errors.</p><p>In addition to the batch-normalization, the conventional DNN regularization technique, such as dropout, is also used to enhance the resistance against adversarial example <ref type="bibr" target="#b22">[23]</ref>. As shown in Table <ref type="table" target="#tab_3">4</ref>, we increase the dropout rate to 0.7 in Will adversarial input defense method improve robustness against adversarial weight attack? or Vice Versa?</p><p>In this section, we want to examine how adversarial input defense and weight parameter defense interplay in increasing the overall robustness of the network. To achieve this, we take the most popular adversarial input attack known as Projected Gradient Descent (PGD) <ref type="bibr" target="#b14">[15]</ref>. We conduct PGD attack on our PC weight defense and observe that the network test accuracy after the PGD attack drops to 0.51% in Table <ref type="table" target="#tab_4">5</ref>. Thus, our weight defense completely fails against a strong PGD input attack. Then, we reverse the role by attacking a strong input defense known as PGD Trained adversarial defense <ref type="bibr" target="#b14">[15]</ref> with strong weight attack BFA <ref type="bibr" target="#b16">[17]</ref>. Again, adversarial input defense fails to defend BFA, requiring even less number of N BF than the baseline shown in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion 3</head><p>Improvement of network parameter robustness against BFA does not provide any guarantee of improvement of robustness against the vulnerability of input attack, and vice versa. Directions to the Parameter Security of DNNs. Finally, we summarize the findings of this work in Table <ref type="table" target="#tab_5">6</ref>, which provides an enhanced guide of regulation to follow while constructing DNNs with higher BFA-resistance. Our weight binarization and its clustering relaxation provides the best solution so far in achieving network parameter immunity from BFA. Additionally, network width and certain key elements of the DNN architecture also build slight resilience against the BFA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we tried to develop a comprehensive investigation of adversarial parameter security enhancement and provide several insightful observations for the future struggle against DNN parameter vulnerability. Based on those observations, we highlight potential successful directions to develop adversarial weight defense. Finally, through the comprehensive experiments, our proposed methods, especially binarization-aware training, are proven to improve the resistance against the emerging bit-flip based adversarial weight attack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Concept illustration of quantized DNN under BFA. Accuracy vs. # bit-flips with/without defense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ResNet-20<ref type="bibr" target="#b5">[6]</ref> with 8-bit weight. Bit-flips NBF for BFA (mean±std):11.2 ± 1.9, total number of bits of weights: 2 millions. VGG-11<ref type="bibr" target="#b19">[20]</ref> with 8-bit weight. Bit-flips NBF for BFA (mean±std):56.6 ± 35.2, total number of bits of weights: 78 millions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Weight shift caused by BFA for (a) ResNet-20 [6] and (b) VGG-11 [20] on CIFAR-10 dataset. For both architectures, 5 trials are executed with different random seeds. Each colored dot depicts the weight shift (x-axis: prior-attack weight, y-axis: post-attack weight) w.r.t one iteration of BFA. The color bar indicates the corresponding accuracy (%) on the CIFAR-10 test data. The vertical distance between dot and the diagonal dashed line (i.e., y = x) represents the weight shift magnitude. Moreover, results reported in this figure use 8-bit post-training weight quantization [17].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Normalized histogram and Kernel Density Estimation (KDE) of #bit-flips versus module index in (top) ResNet-20 and (bottom) VGG-11 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Vanilla training W l=1 . (b) Binarization W fp l=1 . (c) Binarization W b l=1 . (d) Piecewise Clustering W l=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The evolution of weight distribution of ResNet-20 (first layer, l = 1), under various training configurations. x-axis: weight magnitude, y-axis: training epoch.</figDesc><graphic url="image-7.png" coords="4,309.42,155.98,113.85,69.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average #Bit-flips (y-axis) per weight update iteration of binarization-aware training vs. epochs (x-axis), with ResNet-20 on CIFAR10.</figDesc><graphic url="image-9.png" coords="5,81.61,72.00,173.25,108.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The BFA-free test accuracy, mean and standard deviation of N BF for 5-trials under different quantization bit-width n q ∈ {8, 6, 4, 1} and clustering penalty coefficient λ ∈ {0, 1e − 4, 5e − 4, 1e − 3, 5e − 3}, with (left column) ResNet-20 and (right column) VGG-11 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>90.39 89.05 88.2 90.23 89.48 89.59 88.3 88.26 86.94 85.82 86.19</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">VGG11, Top-1 test accuracy of Clean model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell>86.4</cell></row><row><cell></cell><cell cols="4">ResNet20, Mean of NBF</cell><cell></cell><cell></cell><cell cols="4">VGG11, Mean of NBF</cell><cell></cell></row><row><cell>8</cell><cell>28</cell><cell>46.4</cell><cell>58.79</cell><cell>43.5</cell><cell>500</cell><cell>8</cell><cell>16.4</cell><cell>14.2</cell><cell>29.79</cell><cell>30</cell><cell>7500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6000</cell></row><row><cell>6</cell><cell>29.3</cell><cell>46.4</cell><cell>74.8</cell><cell>45.5</cell><cell></cell><cell>6</cell><cell>20.6</cell><cell>31.4</cell><cell>11.4</cell><cell>11.6</cell><cell></cell></row><row><cell>Nq</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>300</cell><cell>Nq</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4500</cell></row><row><cell>4</cell><cell>40.8</cell><cell>55.3</cell><cell>70.4</cell><cell>45.3</cell><cell></cell><cell>4</cell><cell>49.3</cell><cell>82.59</cell><cell>78</cell><cell>52.79</cell><cell>3000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1500</cell></row><row><cell>1</cell><cell>541.2</cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell>1</cell><cell>7874</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="2">0.0005 0.001</cell><cell>0.005</cell><cell></cell><cell></cell><cell>0</cell><cell cols="3">0.0001 0.0005 0.001</cell><cell></cell></row><row><cell cols="5">ResNet20, standard deviation of NBF</cell><cell></cell><cell cols="5">VGG11, standard deviation of NBF</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>4.47</cell><cell>10.57</cell><cell>4.14</cell><cell>8.5</cell><cell></cell><cell>8</cell><cell>1.14</cell><cell>0.45</cell><cell>11.3</cell><cell>6</cell><cell>400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>320</cell></row><row><cell>6</cell><cell>2.38</cell><cell cols="3">11.43 15.59 8.734</cell><cell>32</cell><cell>6</cell><cell>0.89</cell><cell>4.24</cell><cell>3.6</cell><cell>2.07</cell><cell></cell></row><row><cell>Nq</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24</cell><cell>Nq</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>240</cell></row><row><cell>4</cell><cell>6.3</cell><cell>6.9</cell><cell>12.82</cell><cell>6.5</cell><cell></cell><cell>4</cell><cell>3.39</cell><cell cols="2">43.17 11.73</cell><cell>4.15</cell><cell>160</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell></row><row><cell>1</cell><cell>49.8</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>1</cell><cell>431.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="2">0.0005 0.001</cell><cell>0.005</cell><cell></cell><cell></cell><cell>0</cell><cell cols="3">0.0001 0.0005 0.001</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Alternative Methods Comparison. In this table, we report the prior-and post-attack test accuracy (%) and N BF of BFA. The 8-bit quantization is chosen as the baseline; Binary and PC-8bit is the proposed method. Moreover, comparison with Lasso-based pruning and adversarial weight training (adv. training) is included as well.</figDesc><table><row><cell>Methods</cell><cell>Prior-Attack Accuracy (%)</cell><cell>Post-Attack Accuracy (%)</cell><cell>N BF</cell></row><row><cell>8-bit</cell><cell>91.84</cell><cell>10.45</cell><cell>28.0±4.47</cell></row><row><cell>PC-8bit</cell><cell>90.02</cell><cell>10.07</cell><cell>58.79±4.14</cell></row><row><cell>Binary</cell><cell>88.36</cell><cell>10.13</cell><cell>541.2 ± 49.8</cell></row><row><cell>Lasso Pruning</cell><cell>88.11</cell><cell>10.12</cell><cell>6.8±0.44</cell></row><row><cell>Adv. Training</cell><cell>87.72</cell><cell>10.09</cell><cell>9.6±6.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of Network Width. The ResNet-20<ref type="bibr" target="#b5">[6]</ref> with different width configuration (1×,2× and 4×) are reported. All the networks use 8-bit quantization.</figDesc><table><row><cell></cell><cell>Baseline (1×)</cell><cell>2×</cell><cell>4×</cell></row><row><cell>ResNet-20</cell><cell>28.0 ± 4.47</cell><cell cols="2">26.2 ± 2.68 36.4 ± 12.44</cell></row><row><cell cols="3">ResNet-20 (PC) 58.79 ± 4.14 47.2 ± 8.04</cell><cell>72 ± 18.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of Dropout and Batch-Normalization (BN):In order to show this effect, we report N BF for three cases: Case1. With BN and dropout (p = 0.2, p is the dropout rate); this is the case we used throughout the experiments of other sections. Case 2. Without BN and dropout (p = 0.2) to examine the effect of BN layer on BFA. Case 3. With BN and dropout (p = 0.7) to examine the effect of dropout regularizer against BFA. For each of the three cases, we report N BF for both vanilla VGG-11 and VGG-11 with Proposed Piece-wise Clustering (PC).</figDesc><table><row><cell>Method:</cell><cell>w BN dropout (0.2)</cell><cell>w/o BN dropout (0.2)</cell><cell>w BN dropout (0.7)</cell></row><row><cell>VGG-11</cell><cell>16.4 ± 1.14</cell><cell>9.2 ± 0.42</cell><cell>24.6± 3.71</cell></row><row><cell cols="2">VGG-11 (PC) 29.79 ± 11.3</cell><cell>8.8 ± 0.44</cell><cell>32.79 ± 3.49</cell></row><row><cell cols="4">case 3. The effect of dropout in resisting BFA is more emi-</cell></row><row><cell cols="4">nent for vanilla VGG-11 architecture. However, for our case</cell></row><row><cell cols="4">of piece-wise clustering dropout does not improve the ro-</cell></row><row><cell cols="4">bustness significantly. Nevertheless, in general, regulariza-</cell></row><row><cell cols="4">tion techniques such as dropout are expected to prevent the</cell></row><row><cell cols="4">network from over-fitting [13], subsequently slightly im-</cell></row><row><cell cols="4">prove network resistance against both adversarial input [23]</cell></row><row><cell cols="2">and weight attacks [17].</cell><cell></cell><cell></cell></row><row><cell cols="4">Conclusion 2 Regularization techniques, such as batch-</cell></row><row><cell cols="4">normalization and dropout, are helpful in improving the re-</cell></row><row><cell cols="2">sistance against BFA.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>In this table we report the test accuracy after adversarial input attack (PGD<ref type="bibr" target="#b14">[15]</ref>) on both adversarial input defense<ref type="bibr" target="#b14">[15]</ref> and adversarial weight defense (proposed PC). Then Report N BF after conducting BFA on both adversarial weight defense and input defense.</figDesc><table><row><cell>Method:</cell><cell>Test Accuracy w/o Attack(%)</cell><cell>Test Accuracy After PGD attack (%)</cell><cell>N BF</cell></row><row><cell>Baseline</cell><cell>91.84</cell><cell>0.41</cell><cell>28.0 ± 4.47</cell></row><row><cell>Adversarial Training</cell><cell>85.51</cell><cell>40.07</cell><cell>16.2 ± 2.95</cell></row><row><cell>Piece-wise Clustering</cell><cell>90.02</cell><cell>0.51</cell><cell>58.79 ± 4.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Checklist of investigations to improve the BFA resistance.</figDesc><table><row><cell>Directions to improve the BFA resistance</cell><cell>Yes No</cell></row><row><cell>1. Perform Weight Clustering (i.e., Binarization &amp; PC)</cell><cell></cell></row><row><cell>2. Increase Network Capacity</cell><cell></cell></row><row><cell>3. Dropout, Batch-Norm Regularization</cell><cell></cell></row><row><cell>4. Adversarial Weight Training</cell><cell></cell></row><row><cell>5. Network Pruning</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported in part by the National Science Foundation under Grant No.2005209, No.1931871 and Semiconductor Research Corporation nCORE.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dumitras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01017</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoder and dropout together to prevent overfitting in deep neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 8th International Congress on Image and Signal Processing (CISP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="697" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2018. 2, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bit-flip attack: Crushing neural network with progressive bit search</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008">2019. 1, 2, 3, 5, 6, 8</date>
			<biblScope unit="page" from="1211" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flip feng shui: Hammering a needle in the software stack</title>
		<author>
			<persName><forename type="first">K</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th {USENIX} Security Symposium ({USENIX} Security 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Haq: Hardware-aware automated quantization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08886</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Defending dnn adversarial attacks with pruning and logits augmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1144" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Defensive dropout for hardening deep neural networks under adversarial attacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
				<meeting>the International Conference on Computer-Aided Design</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
