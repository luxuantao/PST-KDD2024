<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A static analysis for quantifying information flow in a simple imperative language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Kings College</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Hunt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">City University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pasquale</forename><surname>Malacaria</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>London</settlement>
									<region>Queen Mary</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A static analysis for quantifying information flow in a simple imperative language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83D42348BA8D30F5154837B0526289D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Security</term>
					<term>program analysis</term>
					<term>static analysis</term>
					<term>confidentiality</term>
					<term>information flow</term>
					<term>measurement</term>
					<term>information theory</term>
					<term>quantify interference</term>
					<term>while language</term>
					<term>leakage</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach to quantify interference in a simple imperative language that includes a looping construct. In this paper we focus on a particular case of this definition of interference: leakage of information from private variables to public ones via a Trojan Horse attack. We quantify leakage in terms of Shannon's information theory and we motivate our definition by proving a result relating this definition of leakage and the classical notion of programming language interference. The major contribution of the paper is a quantitative static analysis based on this definition for such a language. The analysis uses some non-trivial information theory results like Fano's inequality and the L 1 inequality to provide reasonable bounds for conditional statements. While-loops are handled by integrating a qualitative flow-sensitive dependency analysis into the quantitative analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mathematical quantitative tools (like probability theory and statistics) have played an increasing role both in the theory and practice of most sciences. However the theory (and theory based analysis) of software systems largely relies on logics and makes little use of quantitative mathematics.</p><p>Traditionally logic is a qualitative discipline, things are true or false, provable or not, typable or not. It is our belief however that some fundamental notions in theoretical computer science might benefit from a quantitative study.</p><p>Take the notion of interference <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> between program variables, informally the capability of variables to affect the value of other variables. Absence of interference (non-interference) is often used in proving that a system is well-behaving, whereas interference can lead to mysterious (mis-)behaviours. However the misbehaviour in the presence of interference will generally happen only when there is enough interference. Think in terms of electric current: non-interference between variables X, Y is the absence of a circuit involving X, Y ; interference is the existence of a circuit; this however doesn't imply that there is enough "current" in the circuit to affect the behaviour of the system.</p><p>Concrete examples of this are provided by access control based software systems. To enter such a system the user has to pass an identification stage; whatever the outcome of this stage (authorisation or failure) some information has been leaked (in the case of failure the search space for the right key has now become smaller). Hence these systems present interference <ref type="bibr" target="#b8">[9]</ref> so they are not "secure" in a qualitative sense. However, common sense suggests to consider them secure if the interference is very small.</p><p>The aim of this paper is to use Shannon's information theory <ref type="bibr" target="#b26">[27]</ref> to define a quantified notion of interference for a simple imperative language and to derive a program analysis based on this notion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Attack model</head><p>Classical covert channel analysis is motivated by military settings in which an insider has access to confidential information. The insider attempts to use some aspect of the computer system's behaviour to communicate that information to another user. In this model, the system under analysis is being viewed as a communication channel and the insider chooses inputs in such a way as to maximise the amount of information transmitted.</p><p>Our model is rather different. We consider situations in which a program has access to confidential data (the high inputs). These inputs may be controlled by the owner of the confidential data but we assume that the owner does not intentionally collude to leak the data. Our attacker is an outsider who may have control over the low inputs, but does not have any direct access to the confidential data or control over the high inputs. By contrast with the rather specialised military setting, our model addresses what are now far more common confidentiality problems involving every day use of the internet.</p><p>One example is the "spyware" (trojan horse) problem: a user downloads and executes untrusted software from the internet. In this situation the user has no interest in transmitting confidential data (quite the reverse) but the software may require access to that data in order to serve the user's intended purpose. Here the user needs guarantees that software does not transmit an unacceptable amount of confidential data to any untrusted outsider.</p><p>A second example is the remote login problem: to gain access to a service, users must authenticate using a password or PIN number. In this situation the authentication process necessarily leaks information from the confidential password database and we need assurances that this leakage remains within acceptable limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Input distributions</head><p>Our use of information theory to quantify information flow requires us to consider probability distributions on the inputs to a program. This naturally raises the question of how such distributions might be calculated and, where there is more than one "obvious" choice of distribution, how a choice might be made. Though there is no simple answer to this, we can offer a guiding principle: to give useful results, the distribution should be a reasonable model of the attacker's uncertainty about the inputs. As a motivating example, consider the case of the login process giving access to accounts on the computer system of a small organisation. Suppose there are just four accounts (Peter, Paul, John, Ringo) and the attacker is someone trying to gain unauthorized access to one of the accounts (say John's). The high input is John's password and the low input is the attacker's guess. Passwords are 8 character sequences of ASCII characters, thus representable in 56 bits. We will assume for simplicity that the login process allows just one attempt at login. Consider two alternative scenarios:</p><p>1. The attacker has inside knowledge. He doesn't know which user has chosen which password but he does know that they have each chosen a different word from the set: black, red, orange, yellow, green, blue, indigo, violet. To this attacker, each of the eight possibilities is equally likely to be John's password, so the appropriate distribution assigns probability 1/8 to each one. This distribution has an entropy of 3 bits. Our definitions assign an information flow of just over 0.54 bits. 2. The attacker has no inside knowledge but he does know from empirical studies at other (larger) organisations, that users tend to pick certain dictionary words more often than other sequences of characters. Let us suppose (a gross simplification) that 1/8 of all users studied have been found to choose passwords uniformly from a set of 2 8 dictionary words, and the other 7/8 of users to choose uniformly from the remaining 2 56 -2 8 possibilities. The entropy of this distribution is approximately 0.54 + 1 + 49 = 50.54 bits. Our definitions assign an information flow of just over 0.006 bits.</p><p>These scenarios show that is not always best to use the "real" distribution when analysing a system. Using the distribution from scenario 1 when attackers have only the generic knowledge of scenario 2 would lead to a needlessly pessimistic view of the system's security. In fact, both these distributions may give overly conservative results. If the company has a rigid policy of allocating automatically generated passwords, the choices may actually conform to a uniform distribution over all 2 56  possibilities. In this case our definitions assign an information flow of less than 10 -15 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Plan of the paper</head><p>Section 2 introduces the basics of information theory and program semantics we need. We then investigate which of Shannon's measures (entropy, conditional entropy, mutual information) is the right one for the task; in 2.4 we introduce our definition of leakage and in 2.5 we establish its relation to the classical programming language definition of non-interference. Section 3 presents a review of related work on quantifying information flow, starting from Denning's and Millen's pioneering work up to the most recent developments in the field such as Clarkson, Myers and Schneider's work.</p><p>Section 4 introduces a static analysis based on the theory presented in 2.4. Analysis rules are syntax (command and expression) based. A rule for a command provides lower and upper bounds for the leakage of one or more variables in the command. A correctness result is then proven which establishes that the bounds provided by the rules for a language statement are always consistent with the leakage of that statement according to Section 2. <ref type="bibr" target="#b3">4</ref>.</p><p>Section 5 presents improvements to the analysis of expressions and further justification for some of the analysis rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Relationship with previous work</head><p>In a previous paper <ref type="bibr" target="#b3">[4]</ref> we sketched an information theory based program analysis for a simple language without loops. The present work is also related with the workshop paper <ref type="bibr" target="#b2">[3]</ref>. In contrast to the latter work, which employs a Use Definition Graph (UDG) underlying the analysis, here we present a syntax directed analysis along the lines of that in <ref type="bibr" target="#b3">[4]</ref>. Apart from this complete recasting of the analysis and new proofs of correctness, the significant additions in the present paper are: theory: Formal relationships between non-interference for program variables, random variable independence and leakage of confidential data are established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>analysis of commands:</head><p>The analysis of conditionals is significantly refined, using powerful results from information theory (Fano's inequality and the L 1 inequality) to derive both upper and lower bounds on flows through ifstatements.</p><p>analysis of expressions: Improved bounds on general equality tests and arithmetic expressions are presented.</p><p>In addition, the review of other significant contributions is extended and updated, all relevant proofs are provided and all sections benefit from extended discussion and more technical details and examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Information theory and interference</head><p>In this section we will provide the foundations of our work and some background on information theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The language and its semantics</head><p>In a semantics-based analysis of security properties, there is a trade-off between tractability and accuracy. Any semantic model is, necessarily, an abstraction of its physical implementations and the limits of an analysis based on that model are determined by the nature of the abstraction. Put more concretely, a system which can be shown to be secure with respect to a semantic model may still be vulnerable to attacks which exploit precisely those aspects of its behaviour which are not modelled.</p><p>In this paper we consider a programming language with a simple denotational semantics and we analyse confidentiality properties based purely on the input-output behaviour which this semantics defines. The language is described in Table <ref type="table">1</ref>.</p><p>The guarantees provided by our analysis are correspondingly limited. In particular, our analysis addresses the question of how much an attacker may learn (about confidential information) by observing the input-output behaviour of a program, but does not tell us anything about how much can be learned from its running-time.</p><p>Given a set A, we write A ⊥ for the cpo obtained by the disjoint union of A and {⊥}, ordered by:</p><formula xml:id="formula_0">⊥ ⊥ ⊥ a a a iff a = a</formula><p>where a, a range over A (thus a, a =⊥). If f : A → B ⊥ and g : B → C ⊥ we write (g • f ) : A → C ⊥ for the strict composition of g and f , thus:</p><formula xml:id="formula_1">(g • f )(a) = g(f (a)) if f (a) =⊥ (g • f )(a) =⊥ if f (a) =⊥</formula><p>The functions A → B ⊥ also form a cpo when ordered pointwise.</p><p>Program variables are drawn from a finite set Var. Let V be the set of k-bit integers (that is, bit-vectors of length k interpreted as integers in the range -2 k-1 n &lt; 2 k-1 in twos-complement representation). The set of stores Σ is just the set of functions σ ∈ Var → V .</p><p>Table <ref type="table">1</ref> The language</p><formula xml:id="formula_2">C ∈ Com x ∈ Var E ∈ Exp B ∈ BExp n ∈ V C := skip | x = E | C 1 ; C 2 | if B C 1 C 2 | while B C E := x | n | E 1 + E 2 | E 1 -E 2 | E 1 * E 2 B := ¬B | B 1 ∧ B 2 | E 1 &lt; E 2 | E 1 == E 2 Table 2 Denotational semantics JskipK = λσ.σ Jx = EK = λσ.σ[x → JEKσ] JC 1 ; C 2 K = JC 2 K • JC 1 K Jif B C 1 C 2 K = λσ. JC 1 Kσ if JBKσ = 1 JC 2 Kσ if JBKσ = 0 Jwhile B CK = lfp F where F (f ) = λσ. (f • JCK)σ if JBKσ = 1 σ if JBKσ = 0</formula><p>An arithmetic expression E is interpreted as a function JEK : Σ → V , using the usual twos-complement interpretations of +, -, * .</p><p>A boolean expression B is interpreted as a function JBK : Σ → {0, 1} in the standard way (0 is false, 1 is true).</p><p>A command C is interpreted as a function JCK : Σ → Σ ⊥ (see Table <ref type="table">2</ref>). The semantics of while is given as the least fix point of a function</p><formula xml:id="formula_3">F : (Σ → Σ ⊥ ) → (Σ → Σ ⊥ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Degrees of interference</head><p>We suppose that the variables of a program are partitioned into two sets, H (high) and L (low). High variables may contain confidential information when the program is run, but these variables cannot be examined by an attacker at any point before, during or after the program's execution. Low variables do not contain confidential information before the program is run and can be freely examined by an attacker before and after (but not during) the program's execution. This raises the question of what an attacker may be able to learn about the confidential inputs by examining the low variable outputs.</p><p>One approach to confidentiality, quite extensively studied <ref type="bibr" target="#b8">[9]</ref>, is based on the notion of non-interference. This approach looks for conditions under which the values of the high variables have no effect on (do not "interfere" with) the values of the low variables when the program is run. We can formalise non-interference in the current setting as follows. A terminating program P is non-interfering if, whenever σ 1 x = σ 2 x for all x in L, then JP Kσ 1 = σ 1 and JP Kσ 2 = σ 2 with σ 1 x = σ 2 x for all x in L. If this condition holds, an attacker learns nothing about the confidential inputs by examining the low outputs.</p><p>Thus non-interference addresses the question of whether or not a program leaks confidential information. In the current work, by contrast, we address the question of how much information may be leaked by a program.</p><p>To help explore the difference between the approaches, consider the following two programs:</p><formula xml:id="formula_4">1. if (h == x) {y = 0} {y = 1} 2. if (h &lt; x) {y = 0} {y = 1}</formula><p>Here we specify that h is high while x and y are low. Clearly, neither of these programs has the non-interference property, since the final value of y is affected by the initial value of h. But are the programs equally effective from an attacker's point of view? Suppose we allow the attacker not only to examine but actually to choose the initial value of x. Suppose further that the attacker can run the program many times for a given choice of value for h. There are 2 k possible values which h may have and the attacker wishes to know which one it is. It is easy to see (below) that the second program is more effective than the first, but the significance of this difference depends on the distribution of the values taken by h.</p><p>At one extreme, all 2 k values are equally likely. Using the first program it will take the attacker, on average, 2 k-1 runs, trying successive values for x, to learn the value of h. Using the second program, the attacker can choose values of x to effect a binary search, learning the value of h in at most k runs.</p><p>At the other extreme, h may in fact only ever take a few of the possible values. If the attacker knows what these few values are, then both programs can clearly be used to find the actual value quickly, since the search space is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Background on information theory</head><p>We use Shannon's information theory to quantify the amount of information a program may leak and the way in which this depends on the distribution of inputs. Shannon's measures are based on a logarithmic measure of the unexpectedness, or surprise, inherent in a probabilistic event. An event which occurs with some non-zero probability p is regarded as having a "surprisal value" of:</p><formula xml:id="formula_5">log 1 p</formula><p>Intuitively, surprise is inversely proportional to likelihood. The base for log may be chosen freely but it is conventional to use base 2 (the rationale for using a logarithmic measure is given in <ref type="bibr" target="#b26">[27]</ref>). The total information carried by a set of n events is then taken as the weighted sum of their surprisal values:</p><formula xml:id="formula_6">H = n i=1 p i log 1 p i<label>(1)</label></formula><p>(if p i = 0 then p i log(1/p i ) is defined to be 0). This quantity is variously known as the self-information or entropy of the set of events. The events of interest for us are observations of the values of variables before and after the execution of (part of) a program. Suppose that the inputs to a program take a range of values according to some probability distribution. In this case we may use a random variable to describe the values taken (initially) by a program variable, or set of program variables.</p><p>For our purposes, a random variable is a total function X : D → R, where D and R are finite sets and D comes with a probability distribution μ (D is the sample space). We adopt the following conventions for random variables:</p><p>1. if X is a random variable we let x range over the set of values which X may take; if necessary, we denote this set explicitly by R(X); the domain of X is denoted D(X) 2. we write p(x) to mean the probability that X takes the value x, that is, p(x) def = d∈X -1 (x) μ(d); where any confusion might otherwise arise, we write this more verbosely as P (X = x) 3. for a vector of (possibly dependent) random variables (X 1 , . . . , X n ), we write p(x 1 , . . . , x n ) for the joint probability that the X i simultaneously take the values x i ; equivalently, we may view the vector as a single random variable</p><formula xml:id="formula_7">&lt;X 1 , . . . , X n &gt; with range R(X 1 ) × • • • × R(X n ) 4.</formula><p>when summing over the range of a random variable, we write x f (x) to mean x∈R(X) f (x); again, we use the more verbose form where necessary to avoid confusion Recall that the kernel of a function f : A → B is the equivalence relation = f on A defined by a 1 = f a 2 iff f (a 1 ) = f (a 2 ). When two random variables X 1 : D → R 1 and X 2 : D → R 2 have the same kernel, we say that they are observationally equivalent, written X 1 X 2 .</p><p>The entropy of a random variable X is denoted H(X) and is defined, in accordance with (1), as:</p><formula xml:id="formula_8">H(X) = x p(x) log 1 p(x)<label>(2)</label></formula><p>It is immediate from the definitions that X 1 X 2 implies H(X 1 ) = H(X 2 ) but not conversely. Because of possible dependencies between random variables, knowledge of one may change the surprise (hence information) associated with another. This is of fundamental importance in information theory and gives rise to the notion of conditional entropy. Suppose that Y = y has been observed. This induces a new random variable (X|Y = y) (X restricted to those outcomes such that Y = y) with the same range as X but with domain Y -1 (y) and P ((X|Y = y) = x) = P (X = x|Y = y), where</p><formula xml:id="formula_9">P (X = x|Y = y) = p(x, y) p(y)</formula><p>The conditional entropy of X given knowledge of Y is then defined as the expected value (i.e., weighted average) of the entropy of all the conditioned versions of X:</p><formula xml:id="formula_10">H(X|Y ) = y p(y)H(X|Y = y)<label>( 3 )</label></formula><p>A key property of conditional entropy is that H(X|Y ) H(X), with equality iff X and Y are independent. Notice also the relation between conditional and joint entropy (chain rule):</p><formula xml:id="formula_11">H(X, Y ) = H(X|Y ) + H(Y ) ( 4 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual information</head><p>Information theory provides a more general way of measuring the extent to which information may be shared between two sets of observations. Given two random variables X and Y , the mutual information between X and Y , written I(X; Y ) is defined as follows:</p><formula xml:id="formula_12">I(X; Y ) = x y p(x, y) log p(x, y) p(x)p(y)<label>(5)</label></formula><p>Routine manipulation of sums and logs yields three equivalent ways of defining this quantity:</p><formula xml:id="formula_13">I(X; Y ) = H(X) + H(Y ) -H(X, Y )<label>( 6 )</label></formula><formula xml:id="formula_14">I(X; Y ) = H(X) -H(X|Y )<label>( 7 )</label></formula><formula xml:id="formula_15">I(X; Y ) = H(Y ) -H(Y |X)<label>( 8 )</label></formula><p>As shown by <ref type="bibr" target="#b5">(6)</ref>, I(X; Y ) is symmetric in X and Y . This quantity is a direct measure of the amount of information carried by X which can be learned by observing Y (or vice versa). As with entropy, there are conditional versions of mutual information. The mutual information between X and Y given knowledge of Z, written I(X; Y |Z), may be defined in a variety of ways. In particular, ( <ref type="formula" target="#formula_13">6</ref>)- <ref type="bibr" target="#b7">(8)</ref> give rise to three equivalent definitions for I(X; Y |Z):</p><formula xml:id="formula_16">I(X; Y |Z) = H(X|Z) + H(Y |Z) -H(X, Y |Z) ( 9 ) I(X; Y |Z) = H(X|Z) -H(X|Y , Z) (10) I(X; Y |Z) = H(Y |Z) -H(Y |X, Z) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Quantifying interference using information theory</head><p>In this section we start by defining a measure of information flow appropriate in a quite general computational setting. We go on to consider the special case of flows in deterministic systems (allowing a purely functional semantics) where all inputs are accounted for, and show how the general definition simplifies in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformational Systems</head><p>A transformational system S is specified by the following:</p><p>1. D S -a finite set (the sample space) 2. a probability distribution on D S 3. a random variable I S : D S → R(I S ) which defines the inputs of the system 4. a random variable O S : D S → R(O S ) which defines the outputs of the system Note that, in any real system of interest, we would expect the outputs of the system to be determined, to some extent, by the inputs, and so I S and O S will not normally be independent. Note also that we could, without loss of generality, fix D S as R(I S ) × R(O S ), taking I S and O S to be the first and second projections, respectively. However, it is technically convenient not to do this, especially in the case of purely deterministic systems. Given a transformational system S, we are concerned with two classes of observation:</p><formula xml:id="formula_17">-An input observation is a surjective function X : R(I S ) → R(X). -An output observation is a surjective function Y : R(O S ) → R(Y ).</formula><p>Observations induce random variables X in : D S → R(X) and Y out : D S → R(Y ) by composition with the relevant component of S:</p><formula xml:id="formula_18">-X in def = X • I S -Y out def = Y • O S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><p>In the following, let N be some finite subset of the integers, such as those representable in a k-bit twos-complement representation.</p><p>1. Consider terminating programs written in a simple imperative language, defined as in Table <ref type="table">1</ref> but with the addition of a coin operator, which evaluates randomly to 0 or 1 with equal probability. Assume given a probability distribution on input states σ ∈ Σ, writing p(σ) for the probability that the input state is σ. Any such program P can be viewed as a transformational system, taking D S def = Σ ×Σ to be the space of input/output pairs, I S (σ, σ ) = σ and O S (σ, σ ) = σ . The probability distribution on D S is induced by the standard semantics of P . For example, if P is the program y := coin then we have:</p><formula xml:id="formula_19">p(σ, σ ) = p(σ)/2 if σ = σ[y → 0] or σ = σ[y → 1] p(σ, σ ) = 0 otherwise</formula><p>The obvious input and output observations to consider in this setting are projections on the state, that is, observations of the values of one or more program variables. For any program variable x, the corresponding observation is given by X : σ → σ(x). 2. Consider the system defined by the swap function λ(x, y).(y, x) restricted to N ×N . In this case it is natural to take D S to be N ×N and I S to be the identity. Given a probability distribution on N × N , we then have a transformational system where I S is the identity and O S is just swap. In this case we also have</p><formula xml:id="formula_20">R(O S ) = N × N = R(I S )</formula><p>. Possible input and output observations include the projections π 1 , π 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Flow</head><p>We are interested in flows of information from system inputs to system outputs. We use some input observation X : R(I S ) → R(X) and output observation Y : R(O S ) → R(Y ), as defined in Section 2.4, to pick out the parts of the input and output we wish to focus on. In this context, we refer to X as the information source. In the rest of this section, assume given some transformational system S.</p><p>A natural information-theoretic quantity to view as the flow from X to Y is the mutual information between the two corresponding random variables: I(X in ; Y out ). We say this seems natural because it is a direct formalisation of the idea that the quantity of information flowing from X to Y is the amount of information given by input observation X which is shared with output observation Y .</p><p>However, despite its intuitive appeal, this formalisation is flawed as it stands. To see why, consider the case of Y = X XOR Z (true when exactly one of the arguments is true) with X and Z independent random variables uniformly distributed over the booleans. Since Y is the value of a function with argument X, and since variation in X clearly can cause variation in Y , we might expect the presence of an information flow from X to Y , but this is not shown in I(X; Y ); indeed we have:</p><formula xml:id="formula_21">I(X; Y ) = H(X) + H(Y ) -H(X, Y ) = 1 + 1 -2 = 0</formula><p>At first sight this is surprising but the explanation is straightforward: XOR is here providing perfect encryption of X, with Z as the key. An observer can learn nothing about X from Y provided the observer does not know Z. This shows very clearly that a satisfactory definition of flow must take account of the observer's prior knowledge of the context. The right way to do this is via conditional mutual information. In the XOR example, if we assume knowledge of Z and account for this by conditioning on Z, we find:</p><formula xml:id="formula_22">I(X; Y |Z) = H(X|Z) -H(X|Y , Z)</formula><p>Since the knowledge of the output of XOR and one of its inputs allows us to determine the other input, we can express X as a function of Y and Z, so H(X|Y , Z) = 0. On the other hand X and Z are independent and X is a uniformly distributed Boolean so</p><formula xml:id="formula_23">H(X|Z) = H(X) = 1 and I(X; Y |Z) = 1 -0 = 1.</formula><p>Prior knowledge may also decrease the flow of information as measured by an observer. For example, an observer of the swap system who already knows Z learns nothing new about &lt;X, Z&gt; by observing π 1 (swap(X, Z)), whereas an observer who knew nothing to start with would measure a flow of H(Z) bits.</p><p>We therefore modify our definition of information flow accordingly. Let X and Z be input observations, let Y be an output observation. Then we define the information flow from X to Y given knowledge of Z as:</p><formula xml:id="formula_24">F Z (X Y ) def = I(X in ; Y out |Z in )<label>(12)</label></formula><p>There are two natural questions arising from this basic definition:</p><p>-What is the meaning of F Z (X Y ) = 0? This captures the special case when no information flows from X to Y . In Corollary 1 we show that, in the deterministic case, this is equivalent to the standard notion of non-interference between X and Y (provided X, Z constitute the whole input).</p><p>-What is the meaning of F Z (X Y ) = n for n &gt; 0? By basic information theoretic inequalities we know that n H(X in |Z in ). When n achieves this maximum, the observation is revealing everything: all the information in X is flowing to Y . When n falls short of the maximum, the observation is incomplete, leaving H(X in |Z in )n bits of information still unknown. One possible operational interpretation of this "gap" is that it provides a measure of how hard it remains to guess the actual value of X once Y is known. This is formalised in <ref type="bibr" target="#b15">[16]</ref> where it is shown that an entropy based formula provides a lower bound for the number of guesses required by a dictionary attack (note, however, that this "guessing game" is an idealised one in which it is assumed that the encoding of information about X in Y is invertible in constant time; it has nothing to say about the computational difficulty of recovering X from Y when this does not hold). For a more extensive discussion of this question we refer the reader to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deterministic Information Flow</head><p>We now restrict attention to the case of deterministic, transformational systems, by which we mean systems for which there exists a function f such that O S = f • I S . This will be the case, for example, in systems defined by programs written in a simple imperative language (without non-deterministic constructs) or in a functional language. Now consider flows of the form F Z (X Y ) in the special case that observations X and Z jointly determine the inputs, thus &lt;X, Z&gt; is injective. For example, in a security setting we may be interested in flows of the form F L (H L) where program variables are partitioned into the high-security set (input observation H) and the low-security set (input observation L). Such a flow measures what a low-security observer (who can only observe low-security variables) can learn about high-security inputs as a result of information flow into the low-security outputs. Since H, L partition the set of all program variables they jointly provide a complete observation of the input.</p><p>As shown by the following proposition, the deterministic case allows a simplified definition of flow: Proposition 1. Assume a deterministic system S. Let X and Z be input observations and let Y be an output observation. If &lt;X, Z&gt; is injective (thus R(I S ) ∼ = P for some set of pairs P ⊆ R(X) × R(Z)) then:</p><formula xml:id="formula_25">F Z (X Y ) = H(Y out |Z in )<label>(13)</label></formula><p>Proof. By determinism and injectivity of &lt;X, Z&gt;, we have</p><formula xml:id="formula_26">Y out = f •&lt;X in , Z in &gt;, for some f . In what follows, let A = X in , B = Y out , C = Z in .</formula><p>By the definitions (see <ref type="bibr" target="#b11">(12)</ref> and Section 2.3), we must show</p><formula xml:id="formula_27">H(A|C) + H(B|C) -H(A, B|C) = H(B|C), that is, we must show H(A|C) = H(A, B|C).</formula><p>Expanding both sides according to the definitions, we must show</p><formula xml:id="formula_28">H(A, B, C) -H(C) = H(A, C) -H(C). But B = f • &lt;A, C&gt; implies (A, B, C) (A, C</formula><p>), so we are done.</p><p>Given its relative simplicity, it may be tempting to consider <ref type="bibr" target="#b12">(13)</ref> as an alternative general definition of flow. However, in general, it is not adequate. Consider again the example program Y := coin from Section 2.4. Consider some other program variable X (the choice is arbitrary) and define X : σ → σ(X) and Y : σ → σ(Y) and let Z be any input observation. Clearly, no information flows from X to Y , since the value assigned to Y does not depend on any part of the store. This is confirmed using <ref type="bibr" target="#b11">(12)</ref>, which gives a flow of I(X in ; Y out |Z in ) = 0 (one of the basic identities of information theory, since X in and Y out are independent). By contrast, applying <ref type="bibr" target="#b12">(13)</ref> would give a flow of</p><formula xml:id="formula_29">H(Y out |Z in ) = H(Y out ) = 1 2 log 2 + 1 2 log 2 = 1.</formula><p>Another striking difference between the specialised setting and the more general probabilistic setting, is that in the specialised case an upper bound on the flow into a collection of outputs can be determined by considering the outputs separately: Proposition 2. Let S be a deterministic system. Let X and Z be input observations. Let Y 1 and Y 2 be output observations and let</p><formula xml:id="formula_30">Y = &lt;Y 1 , Y 2 &gt; (thus Y is also an output observation). If &lt;X, Z&gt; is injective then F Z (X Y ) F Z (X Y 1 ) + F Z (X Y 2 ).</formula><p>Proof. By Proposition 1, it suffices to establish the general inequality</p><formula xml:id="formula_31">H(A, B|C) H(A|C) + H(B|C)<label>(14)</label></formula><p>It is easy to show (for example, by Venn diagram, see <ref type="bibr" target="#b34">[35]</ref>) that H(A, B|C) =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(A|C)+H(B|C)-I(A; B|C)</head><p>. Since all of the Shannon measures are non-negative, the inequality follows.</p><p>But the conclusion of this proposition does not hold in general when the injectiveness condition is dropped. The reason is essentially the one used to motivate the use of conditional mutual information in the definition of flow in Section 2.4: knowledge of one input can increase the apparent flow from another. Consider the program λ(x, z).(x XOR z, z) defining a deterministic system with</p><formula xml:id="formula_32">D S = R(I S ) = R(O S ) = bool × bool.</formula><p>Let X and Z be the input observations π 1 and π 2 , respectively. Similarly, let Y 1 , Y 2 be the output observations π 1 , π 2 . Now suppose the distribution for S is such that X in and Z in are independent and uniform. We are concerned with flows having X as the information source. Instead of taking Z as the observer's prior knowledge (which would satisfy the injectiveness condition of the proposition) take some constant function W (representing ignorance), in which case, injectiveness clearly fails. Conditioning on a constant has no effect, thus</p><formula xml:id="formula_33">I(X in ; Y out i |W in ) = I(X in ; Y out i ), hence F W (X Y i ) = I(X in ; Y out i ).</formula><p>Simple calculations then give the following:</p><formula xml:id="formula_34">-F W (X Y 1 ) = 0 -F W (X Y 2 ) = 0 -F W (X &lt;Y 1 , Y 2 &gt;) = 1</formula><p>So in this case it is not sufficient to calculate the flows to the outputs separately. The reason is clear: the two outputs are, respectively, a perfect encryption of X and its key. Knowing either one by itself reveals nothing about X but knowing both reveals everything.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Non-interference</head><p>In this section we consider only deterministic systems S and we assume the inputs and outputs to be finite functions whose domains index the components, as in the case of the store for a simple imperative language. Thus we are assuming the existence of a set of input observations {X 1 , . . . , X n } such that I S = &lt;X 1 , . . . , X n &gt; in and a set of output observations {Y 1 , . . .</p><formula xml:id="formula_35">, Y m } such that O S = &lt;Y 1 , . . . , Y m &gt; out = f • &lt;X 1 , . . . , X n &gt; in , for some f .</formula><p>Note: it is a simple consequence of the definitions that &lt;X 1 , . . . , X n &gt; in = &lt;X in 1 , . . . , X in n &gt;, and similarly for output observations. In the setting of security, information flow is of particular relevance when considering confidentiality properties. Leakage of confidential information is a particular case of information flow where the source of information is a high-security part of the input and the target a low-security part of the output. In general when there is information flow from inputs to outputs, the inputs are said to interfere with the outputs, whereas the absence of any such flow is known as non-interference. One attraction of non-interference is its relative simplicity, since it is a binary property which can be defined without any explicit recourse to information theory <ref type="bibr" target="#b8">[9]</ref>. Roughly speaking, a deterministic program is said to satisfy non-interference if its low-security outputs depend only on its low-security inputs (hence not on its high-security inputs).</p><p>More formally, in the deterministic case, a generalised definition of noninterference can be formalised by modelling different categories of observation (e.g., high-security, low-security) by equivalence relations. Given relations R and S, a function</p><formula xml:id="formula_36">f is said to map R into S, written f : R ⇒ S, iff ∀x, x .(x, x ) ∈ R ⇒ (f (x), f (x )) ∈ S.</formula><p>In what follows, R and S will always be equivalence relations, though the general construction (known as logical relations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref>) does not require this. Given a set of components X, let = X be the equivalence relation which relates two inputs just when they agree on all components in X. Suppose that the input (resp. output) components are partitioned into the low security components L (resp. L ) and the high-security components H (resp. H ), i.e., both inputs and outputs satisfy the injectivity condition with respect to H and L. Then non-interference is defined as f : (= L ) ⇒ (= L ) ("low-equivalence" is mapped into "low-equivalence"). More generally a collection of input components X interferes with an output component</p><formula xml:id="formula_37">Y iff f : (= X ) ⇒ (= Y )</formula><p>, where X is the complement of X, i.e., all those input components not in X.</p><p>Here we go on to explore the relationship between non-interference and information theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-interference and Independence</head><p>First recall that two random variables X and Y are independent iff for all x, y, P (X = x, Y = y) = P (X = x)P (Y = y). An immediate consequence of the definition is that for two independent random variables X and Y , H(Y , X) = H(Y ) + H(X), which provides a proof for the following: Proposition 3. Random variables X and Y are independent iff I(Y ; X) = 0.</p><p>As the XOR example suggests, simple random variable independence is not enough to capture the absence of information flows. The correct probabilistic characterization of non-interference is via conditional independence: Proposition 4. Let Y be an output component, hence (given the assumptions of this section) Y out = f • &lt;X 1 , . . . , X n &gt; in for some f . Assume a probability distribution such that, for all (x 1 , . . . ,</p><formula xml:id="formula_38">x n ), P (X in 1 = x 1 , . . . , X in n = x n ) = 0. Let i n. Then X 1 , . . . , X i are non-interfering with Y iff I(Y out ; X in 1 , . . . , X in i |X in i+1 , . . . , X in n ) = 0</formula><p>Proof. The constraint that all inputs have non zero probability (i.e., p(x 1 , . . . , x n ) = 0) is to avoid f being a "constant in disguise", i.e., f could assume theoretically more than one value but in practice only one value is possible as the inputs for the other values have probability 0.</p><p>In the following we use</p><formula xml:id="formula_39">X (resp. Z) for X 1 , . . . , X i (resp. X i+1 , . . . , X n ).</formula><p>From Proposition 1 we know that</p><formula xml:id="formula_40">I(Y ; X|Z) = H(Y |Z) so all we have to prove is that Y , X are non-interfering iff H(Y |Z) = 0. (⇒) : Y , X are non-interfering means the set of values for Y is a function of X, Z (i.e., H(Y |X, Z) = 0) constant on the X component which implies H(Y |X, Z) = H(Y |Z).</formula><p>(⇐) : Assume Y , X are interfering, i.e., f (X, Z) is non constant on the X component. Then given only the Z component we will not know the value Y will assume, i.e., we will have uncertainty in Y given Z, i.e., H(Y |Z) &gt; 0.</p><p>Corollary 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">F</head><formula xml:id="formula_41">A (A Y ) = 0 iff A does not interfere with Y . 2. When n = 1 we have non-interference iff I(X in ; Y out ) = 0, that is, iff X in and</formula><p>Y out are independent random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A review of significant contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Denning's approach</head><p>In <ref type="bibr" target="#b7">[8]</ref>, Denning suggests a definition of information flow for programs, based on information theory. Given two program variables x, y in a program P and two states s, s in the execution of P , Denning suggests that there is a flow of information from x at state s to y at state s if uncertainty about the value of x at s given knowledge of y at s is less than uncertainty about the value of x at s given knowledge of y at s.</p><p>Using information theory, Denning translates existence of a flow thus defined into the following condition:</p><formula xml:id="formula_42">H(x s |y s ) &lt; H(x s |y s )<label>(15)</label></formula><p>So, if there is a flow (i.e., (15) holds) how much information is transferred? The quantitative answer provided by Denning is a simple consequence of <ref type="bibr" target="#b14">(15)</ref>:</p><formula xml:id="formula_43">H(x s |y s ) -H(x s |y s )<label>(16)</label></formula><p>i.e., the difference in uncertainty between the two situations. A major merit of Denning's work has been to explore the use of information theory as the basis for a quantitative analysis of information flow in programs. However it doesn't suggest how the analysis could be automated. Also we argue that there is a problem with Denning's definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Denning</head><p>In this section we identify a flaw in Denning's definition, propose a correction, and show that the modified definition coincides with our own. Suppose that, in state s, y = abs(x) where x takes any integer value in the range -16, . . . , 15, with uniform distribution. Then consider the following two programs:</p><formula xml:id="formula_44">(A) {s}if (x = 0) {y = 1} {y = 2}{s } (B) {s}if (x &lt; 0) {y = 1} {y = 2}{s }</formula><p>Calculating the quantities involved in <ref type="bibr" target="#b15">(16)</ref>   <ref type="formula" target="#formula_43">16</ref>) + (1/2)log( <ref type="formula" target="#formula_43">16</ref>)). Thus, according to <ref type="bibr" target="#b15">(16)</ref>, there is no flow from x to y in either case, since the uncertainty in x given y actually increases in both cases. Now it is implicit in Denning's definition that the quantity of flow depends, in part, on what is observed both before and after the program runs. The first term in <ref type="bibr" target="#b15">(16)</ref>, H(x s |y s ), represents the initial uncertainty in x given that y is observed, whereas the second term is intended to represent the final uncertainty in x, again given that y is observed. The problem lies with the second term: it accounts for the final observation of y but (effectively) assumes that the initial observation has been forgotten. This is a safe assumption only if we know that the observer has no memory. In general, however, we must assume that, in the end, the observer knows both the initial and final values of y. Modifying <ref type="bibr" target="#b15">(16)</ref> in line with this assumption, we obtain:</p><formula xml:id="formula_45">) = (1/2)H(x s |y = 1) + (1/2)H(x s |y = 2) = (1/2)log(</formula><formula xml:id="formula_46">H(x s |y s ) -H(x s |y s , y s )<label>(17)</label></formula><p>Note that H(X|Y , Z) H(X|Y ), for any random variables X, Y , Z, and thus (16) (17) will always hold. Applying <ref type="bibr" target="#b16">(17)</ref> to programs (A) and (B), we calculate: H(x s |y s , y s ) = 1 and H(x s |y s , y s ) = 0. So, using <ref type="bibr" target="#b16">(17)</ref>, we still find no flow for program (A) but for program (B) we have a flow of 1 bit. The crucial difference between (A) and (B) is that (A) reveals nothing new about x (knowing abs(x) we already know if x = 0) whereas (B) reveals the one thing we didn't know, namely the sign of x.</p><p>Applying <ref type="bibr" target="#b16">(17)</ref> to the case of a program with inputs H in , L in and outputs H out , L out , we obtain:</p><formula xml:id="formula_47">H(H in |L in ) -H(H in |L out , L in ) (18) Proposition 5. F L (H L) = (18).</formula><p>Proof. The result follows simply by rewriting both sides using the definitions of conditional entropy and conditional mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Millen's approach</head><p>We have seen that the work we describe in this paper is not the first attempt to apply information theory to the analysis of confidentiality properties. An early example is that of Jonathan Millen <ref type="bibr" target="#b18">[19]</ref> which points to the relevance of Shannon's use of finite state systems in the analysis of channel capacity.</p><p>Millen was, to the best of our knowledge, the first to establish a formal correspondence between noninterference and mutual information. What he proves is that, using a state machine model, the notion of non-interference in such a system is equivalent to the mutual information between random variables representing certain inputs and outputs being equal to zero. This is hence the first result similar to Corollary 1.</p><p>Millen uses this equivalence to measure interference in state machine systems, in particular to study the channel capacity for covert channels. Millen's pioneering work is very relevant to ours; however, in contrast to Millen's work the fundamental concern of the current paper is the static analysis of programming languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">McLean's Approach</head><p>According to McLean <ref type="bibr" target="#b17">[18]</ref>, the most stringent approach to information flow is Sutherland's Non-deducibility model <ref type="bibr" target="#b28">[29]</ref>. This model requires High and Low objects to be effectively independent. Non-deducibility, also called compartmentalisation, may be helpful to logicians to reason about independent subsystems of a system, however it doesn't capture the notion of non-interference as intended in a security context.</p><p>McLean argues that an analysis of the notion of secure information flow, as opposed to compartmentalisation, requires the introduction of time into the model. When this is done, only certain classes of dependency between Low and High are considered security violations.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows allowed and forbidden flows between High and Low objects at different times. The left figure expresses the fact p(H t |L t-1 ) = p(H t ), i.e., low information can contribute to high information, whereas the right figure expresses the requirement p(L t |H t-1 ) = p(L t ), i.e., high information cannot contribute to following low information; notice that p(L t |H t-1 ) = p(L t ) is equivalent to p(H t-1 |L t ) = p(H t-1 ) which justifies forbidding the upward arrow as well.</p><p>However flows in Fig. <ref type="figure" target="#fig_0">1</ref> don't disallow the possibility that by taking the combined knowledge of L t , L t-1 we may end up knowing something about H t-1 . To avoid this situation the requirement p(L t |(H t-1 , L t-1 )) = p(L t |L t-1 ) is introduced, illustrated by Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The main problem with this definition of non-interference is the inability to distinguish between statistical correlation of values in the high and low objects and causal relationships between high and low objects.   McLean's model is highly abstract. System behaviours are modelled by the sequences of values taken by the 'High and Low objects' of the system. His Flow Model states that a system is secure if p(L t |(H s , L s )) = p(L t |L s ), where L t describes the values taken by the Low system objects at time t while L s and H s are the sequences of values taken by the Low and High objects, respectively, at times preceding t. The condition is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Goguen-Meseguer approach</head><p>The Goguen-Meseguer non-interference model <ref type="bibr" target="#b8">[9]</ref> can be seen as a particular case of McLean's Flow Model, when specific assumptions about the system are made. In particular Goguen-Meseguer concentrate on deterministic programs which cannot generate High-level output from Low-level input.</p><p>McLean's Flow Model provides the right security model for a system with memory. However his work is qualitative and there is not enough machinery to implement an analysis based on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Gray's approach</head><p>Gray <ref type="bibr" target="#b33">[34]</ref> models general non-deterministic systems using a similar synchronous state machine model to Millen's <ref type="bibr" target="#b18">[19]</ref> but using probabilistic rather than nondeterministic transitions. This allows his model to include temporal and probabilistic covert channels. He assumes the system has finite sets of internal system states, communication channels, input signals and output signals as well as a single initial state. A probabilistic transition is given by a function that maps to a probability a tuple consisting of a source state, a target state, a vector of inputs indexed by the set of channels, and a vector of outputs indexed by the set of channels.</p><p>Inputs and outputs are partitioned into High and Low with the assumption that the only interaction between high and low environments is through the system via the inputs/outputs in their own partition. His model assumes that each environment has a memory of previous system inputs and outputs accessible to it.</p><p>Gray's information flow security condition is given as</p><formula xml:id="formula_48">P t (α L , β L , α H , β H ) &gt; 0 ⇒ P t (l t |α L , β L , α H , β H ) = P t (l t |α L , β L )</formula><p>for any time t, where l t is the Low output at time t while α H (α L ) is the history of High (Low) inputs up to and including time t -1 and β H (β L ) is the history of High (Low) outputs up to and including time t -1.</p><p>His system model and information flow security model can be seen as a more detailed elaboration of McLean's flow model.</p><p>Of interest from the point of view of our own work is that Gray, while mainly concerned with non-interference, is aware that his framework is sufficiently general to be used to examine non-zero leakage of classified information. Of even more interest is that he establishes a connection with information theory: he shows that, if the flow security condition holds, the channel capacity of the channel from High to Low is zero.</p><p>He makes a definition of channel capacity for a system with memory and internal state (Shannon's original definition was for a discrete memoryless system <ref type="bibr" target="#b26">[27]</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Gray's channel capacity). The channel capacity from</head><formula xml:id="formula_49">H to L is C ≡ lim n→∞ C n</formula><p>where C n is defined as</p><formula xml:id="formula_50">C n ≡ max H,L ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 1 n n i=1 I( In-Seq-Event H,i , Out-Seq-Event H,i ; Final-Out-Event L,i | In-Seq-Event L,i , Out-Seq-Event L,i ) ⎞ ⎟ ⎟ ⎟ ⎟ ⎠</formula><p>This definition says that the channel capacity is the limit of a sequence of approximations, each at a finite time n. Each approximation is calculated as the maximum over all possible High and Low behaviours of the following quantity: the average for each moment in time, i, up to the current one (n) of the mutual information between the High input and output event histories and the Low output at time i, given knowledge of the Low input and output event histories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">McIver and Morgan approach</head><p>In their 2003 paper <ref type="bibr" target="#b16">[17]</ref> Annabelle McIver and Carroll Morgan put forward an information theoretic security condition based on measurement of information flow for a sequential programming language enriched with probabilities. The language is the probabilistic guarded command language of <ref type="bibr" target="#b19">[20]</ref>. The context of their discussion is program refinement and the paper establishes a number of equivalent conditions for a sequential language program to meet their security condition. There is no notion of a static analysis based on syntax.</p><p>Their security condition seeks to prevent any change in the knowledge low has of high through the operation of the program (the assumption is that the operation of the program can only increase low's knowledge of high). Low may observe low variables as well as the program text and can draw conclusions about high variables at that point in the program. The resulting definition of flow has a history free flavour. In what follows we give our version of their flow quantity definition, specialised to the case that high and low partition the store: Definition 2 (McIver and Morgan Flow Quantity). Let h and l be the random variables corresponding respectively to the high security and low security partitions of the store at the beginning of the program. Let h and l be the random variables corresponding to these partitions at the end of the program. The information flow from high to low is given by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(h|l) -H(h |l )</head><p>They use this definition of flow quantity (or, as they term it, "information escape") to define a notion of channel capacity, much in the same way as Gray does, by taking the channel capacity of the program to be the least upper bound over all possible input distributions of the information flow quantity. They give both a general definition for this and a definition specialised to the case when high and low partition the store. When the channel capacity of the program is zero it is defined as secure.</p><p>They then develop some alternative formulations of this notion of security and show that these are equivalent. The alternative formulations are quite interesting. First they show that their definition means that a program can only permute high values (i.e., can't identify any of them as this will destroy entropy) as well as not communicating the value of high to low. They then show that this permutation condition is equivalent to the program preserving maximal entropy on high values (in fact a permutation of a set would preserve entropy).</p><p>Their security condition corresponding to the absence of any flows given by their definition is quite strong. In particular there does not need to be any dependency of low on high introduced by the program for a flow in their sense to exist. For example the program h := h mod 2 will in general introduce a flow when the size of the value space of h is greater than 2, simply because the assignment will in general reduce the amount of entropy in the final value of h. Their security condition also rules out flows from low to high (which McLean calls "audit flows" and argues should be allowed) in effect achieving a separability condition, since any flow from low to high will decrease the entropy in high at program's end, given knowledge of low at program's end.</p><p>In some sense their security condition embraces both the standard definition of confidentiality (that there are no flows from a secret input to publically observable outputs) and the standard definition of integrity (that there are no flows from a possibly tainted source, low, to the untainted one, high). These have long been recognised as formal duals of each other <ref type="bibr" target="#b1">[2]</ref>. McIver and Morgan state but do not prove a lemma that says that their security condition implies a weaker one in which the high input only is protected. It is not clear that this is correct because we feel there seem to be some problems with their definition of flow.</p><p>First, again considering a state space partitioned between high and low, if the program does not assign anything to high then their definition specialises to Denning's and suffers the same drawback: the lack of consideration of the history of low inputs. As such it can at best be a model for an attacker without memory. See the counter-example to Denning's definition above in Subsection 3.1.</p><p>Second, this history free flavour can lead to some odd results. Consider swapping high and low via a third (high security) variable, temp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>temp := l; l := h; h := temp</head><p>Suppose that h and l have the same type (i.e., the same value space), are uniformly distributed, and are independent of each other. Our definition of flow quantity gives</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(l |l) = H(l ) = H(h)</head><p>i.e., the entire secret is leaked. Since h and l are the same type and uniformly distributed their entropy is the same, i.e., H(h) = H(l) and so their definition will give a flow of 0:</p><formula xml:id="formula_51">H(h|l) -H(h |l ) = H(h) -H(l) = 0</formula><p>which does not mean that their results about the equivalence of the security conditions are incorrect (since the channel capacity takes the least upper bound of a non-negative set). However these two examples do call into question the usefulness of their definition of information flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Clarkson, Myers and Schneider approach</head><p>A recent work by Clarkson, Myers and Schneider <ref type="bibr" target="#b4">[5]</ref> proposes a new perspective and an exciting new basis for a quantitative definition of interference. The idea is to model attacker belief about the secret input as a probability distribution. This belief is then revised using Bayesian techniques as the program is run. The attacker can be seen as a gambler and his belief as the amount he would be prepared to bet that the secret is such and such. This belief revision point of view unveils a new notion of uncertainty depending on how strongly the attacker believes something to be true.</p><p>As an illustration, the authors of <ref type="bibr" target="#b4">[5]</ref> propose the following example: Suppose a password checking program and suppose there are three possible passwords A, B and C. The attacker believes that A is the real password (he is 99% confident about that) and he thinks B and C are equally likely with confidence 0.5% each. What happen if the attacker fails authorization using A? He is then more confused than before about what the password is, so his uncertainty has increased as he has now two possible passwords B and C each with a 50% chance to be the right one.</p><p>Uncertainty in terms of attacker beliefs allow for a study of the information gain in a single experiment (i.e., a single run of a program) and opens up investigation of flow policies for particular inputs.</p><p>One could see this approach as complementing ours. Indeed we can see our input distribution as the bookie view of the input. The bookie doesn't necessary know the secret (e.g., a bookie doesn't know in advance the result of a horse race) but he builds up a probabilistic model of his beliefs such that on average he can make a profit against all possible attackers. His view is statistical in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Other related work</head><p>Contemporary with our own work has been that of Di Pierro, Hankin and Wiklicky. Their interest has been to measure interference in the context of a probabilistic concurrent constraint setting where the interference comes via probabilistic operators. In <ref type="bibr" target="#b20">[21]</ref> they derive a quantitative measure of the similarity between agents written in a probabilistic concurrent constraint language. This can be interpreted as a measure of how difficult a spy (agent) would find it to distinguish between the two agents using probabilistic covert channels, with a measure of 0 meaning the two agents were indistinguishable. However in contrast to our work they do not measure quantities of information. Their approach does not deal with information in an information-theoretic sense although the implicit assumption in example 4 in that paper is that the probability distribution of the value space is uniform.</p><p>Other recent works include Gavin Lowe who has measured information flow in CSP by counting refusals <ref type="bibr" target="#b12">[13]</ref> and Volpano and Smith who have relaxed strict noninterference and developed type systems in which a well typed program will not leak its secret in polynomial time <ref type="bibr" target="#b32">[33]</ref>.</p><p>There has been also some interesting work regarding syntax directed analysis of non-interference properties. See particularly the work of Sands and Sabelfeld <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysing programs for leakage</head><p>We now develop a system of inference rules to be used in the analysis of information flow in the simple, deterministic, while language defined in Section 2.1. We consider the case of security (confidentiality) properties described in Section 2.4, where the program variables are partitioned into H and L. Let H = {x 1 , . . . , x n } and L = {y 1 , . . . , y m }. For vector H = &lt;v 1 , . . . , v n &gt; we write H = H to mean x i = v i , 1 i n, and similarly for L = L = &lt;w 1 , . . . , w m &gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Worst case assumptions</head><p>As described in Section 2.4 (Deterministic Information Flow), we are interested in flows of the form F L (H X) where X is the output observation for some program variable x. Since Proposition 1 applies, it suffices to calculate bounds on H(X out |L in ). However, this raises an important question about what knowledge of input distributions it is reasonable to assume. Until now we have (implicitly) assumed a probability distribution on the space of initial stores which is independent of the choice of program. There are two potential problems with this assumption:</p><p>1. while it is reasonable to assume that some knowledge will be available as to the distribution of the high inputs, it is likely that little or no knowledge will be available about the low inputs; 2. the distribution for low inputs may actually be in the control of the attacker; in this case it is conservative to assume that an attacker chooses L in to maximise leakage.</p><p>We deal with both of these problems by constructing our analysis to give results which are safe for all possible distributions on the low inputs. The approach is, essentially, to suppose that the low inputs take some fixed (but unknown) value L.</p><p>Then, rather than calculate bounds directly on H(X out |L in ), we calculate bounds on H(X out |L in = L). Our analysis calculates bounds which hold for all choices of L and this is conservative with respect to F L (H X), as confirmed by the following:</p><formula xml:id="formula_52">Proposition 6. (∀L . a H(X out |L in = L) b) =&gt; a F L (H X) b. Proof. By Proposition 1, F L (H X) = H(X out |L in )</formula><p>. By definition of conditional entropy, H(X out |L in ) is the sum over all L of P (L in = L)H(X out |L in = L). The result follows simply because a weighted average is bounded by the smallest and greatest terms.</p><p>The analysis requires initial assumptions to be made about bounds on the values of the entropy of the input values, H(X in |L in = L). Methods by which such bounds might be calculated are beyond the scope of this paper but we can make some more or less obvious remarks:</p><p>1. Assuming k-bit variables, the bounds 0 H(X in |L in = L) k are always valid, though the analysis is unlikely to produce accurate results starting from such loose bounds, except in extreme cases. 2. For all low-security variables X ∈ L, H(X in |L in = L) = 0. 3. In most cases, it will be reasonable to assume that the initial values of H and L are independent, in which case H(X in |L in = L) = H(X in ) for all high-security variables X ∈ H, so the problem of calculating useful initial assumptions reduces to the problem of finding good estimates of the entropies of the high security inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The analysis rules</head><p>The remainder of this section presents the syntax-directed analysis rules and gives their informal meaning and motivation. The formal statements and proofs of correctness are deferred to Section 5. We group the rules into the following five categories:</p><p>Expressions: rules dealing with boolean and arithmetic expressions. Logical: "logical" rules for the analysis of commands, allowing us to combine the results derived by overlapping rule sets. Data Processing: rules expressing the basic limit placed on information flow imposed by the Data Processing theorem <ref type="bibr" target="#b5">[6]</ref>; this category incorporates rules for a qualitative dependency analysis. Direct Flow: rules which track simple direct flows of information due to assignment between variables and sequential control flow. Indirect Flow: rules which deal with indirect flows arising from the influence of confidential data on the control flow through conditionals.</p><p>Eq( <ref type="formula" target="#formula_8">2</ref>)</p><formula xml:id="formula_53">Γ E 1 : [a, _] Γ E 2 : [_, b] Γ (E 1 == E 2 ) : [0, B(q)] 1 2 k q 1 2 , U k (q) (a -b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Expressions</head><p>The The first two rules are "logical" rules which allow us to combine derivations obtained by overlapping rule sets. For example rule <ref type="bibr">[EConj]</ref> states that if we can show that the leakage of an arithmetic expression E in context Γ is [a 1 , b 1 ] and also [a 2 , b 2 ] then we can conclude that the leakage of E in the context Γ cannot be lower than max(a 1 , a 2 ) and cannot exceed min(b 1 , b 2 ). Rule [BConj] says the same for boolean expressions.</p><p>Rule [k-Bits] says that the leakage of an arithmetic expression is between 0 and k. Rule  says that the leakage of a boolean expression is between 0 and 1.</p><p>The leakage of a constant is 0 (rule [Const]) whereas the leakage of a variable x is given by the context Γ , x : [a, b] (rule <ref type="bibr">[Var]</ref>).</p><p>Rule <ref type="bibr">[And]</ref> says that the leakage of the conjunction of two boolean expressions cannot exceed the sum of the upper bounds of the leakages of the expressions and rule <ref type="bibr">[Neg]</ref> says that the leakage of the negation of a boolean expression is the same as the leakage of the original expression.</p><p>Rule [Eq <ref type="bibr" target="#b0">(1)</ref>] is the same as rule <ref type="bibr">[And]</ref>. Rule [Plus] is the same as rule [And] but applied to arithmetic expressions.</p><p>The most interesting rule is [Eq <ref type="bibr" target="#b1">(2)</ref>]. This rule uses two functions, each of which returns an entropy given a single probability. One function is:</p><formula xml:id="formula_54">U k (q) def = q log 1 q + (1 -q) log 2 k -1 1 -q (19)</formula><p>This is easily shown to be the greatest entropy possible for any distribution on 2 k elements assuming that one element (think of this as the value of E 2 ) has probability q. In fact, this is the basic principle underlying Fano's inequality <ref type="bibr" target="#b5">[6]</ref>. The other function is:</p><formula xml:id="formula_55">B(q) def = q log 1 q + (1 -q) log 1 1 -q (20)</formula><p>which is just the entropy of the binary distribution {q, 1 -q}.</p><p>[Eq <ref type="bibr" target="#b1">(2)</ref>] is based on the following observation: in an equality test E 1 == E 2 , if E 1 has high entropy (lower bound a) and E 2 has low entropy (upper bound b), then the test will almost always evaluate to false. Informally, this is because the value of E 1 varies widely while the value of E 2 remains almost constant, hence their values can be equal only seldom. The probability that the test is true (q in the side condition) is therefore low. More precisely, the constraint U k (q) (ab) must hold. A more detailed discussion can be found in Section 5. (We leave unstated the companion rule, justified by commutativity of ==, which reverses the roles of E 1 and E 2 .)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Logical</head><p>In general, a command leakage judgement has the form   Rule <ref type="bibr">[Join]</ref> states that we can join disjoint final contexts.</p><formula xml:id="formula_56">Γ {C} Γ . When Γ (x) = [a</formula><formula xml:id="formula_57">Γ {C} x : [a 1 , b 1 ] Γ {C} x : [a 2 , b 2 ] Γ {C} x : [max(a 1 , a 2 ), min(b 1 , b 2 )] Join Γ {C} Γ 1 Γ {C} Γ 2 Γ {C} Γ 1 , Γ 2 dom(Γ 1 ) ∩ dom(Γ 2 ) = ∅</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Data processing</head><p>The [Dep] rule relies on the following well known basic result of information theory, which simply states the more or less obvious fact that the entropy of a function of a random variable cannot exceed the entropy of the random variable itself (you can't get out more entropy from a deterministic system than you put in):</p><p>Lemma 1 (Data Processing). Let X 1 , . . . , X n , Z be random variables with a common domain D and such that Z = f (X 1 , . . . , X n ), where f is any total function. (Equivalently, in function notation:</p><formula xml:id="formula_58">Z = f • (λd ∈ D.&lt;X 1 (d), . . . , X n (d)&gt;).) Then H(Z) H(X 1 , . . . , X n ) H(X 1 ) + • • • + H(X n ).</formula><p>Proof. Immediate by the Data Processing Theorem <ref type="bibr" target="#b5">[6]</ref>.</p><p>The <ref type="bibr">[Dep]</ref> rule is a "catch all" rule which allows us to apply this lemma wherever it may be useful. The rule makes use of a secondary derivation for qualitative dependency judgements. A dependency judgement has the form C : Δ, where Δ : Var → ℘(Var). The informal meaning of a dependency judgement is as follows: if C : Δ, then the value of any variable x immediately after execution of C depends on at most the values of the variables in Δ(x) immediately before execution of C. Dependency judgements are derived using the rules shown in Table <ref type="table">6</ref>.</p><p>In the Table we use Δ Δ for the map (Δ Δ )(x) = Δ(x) ∪ Δ (x) and the condition Δ Δ iff for all x Δ(x) ⊆ Δ (x).</p><p>The general form of a derivation using these rules is D Δ {C} Δ where D is a set of program variables. Such a judgement is to be read as: in a control context which depends at most on the variables in D, if dependencies Δ hold before execution of </p><formula xml:id="formula_59">C : Δ Γ {C} x : [0, b] b = y∈Δ(x) Γ + (y) Table 6 Dependency analysis DSkip D Δ {skip} Δ DAssign Δ E : D D Δ {x = E} Δ[x → D ∪ D ] DIf Δ B : D D ∪ D Δ {C i } Δ i i = 1, 2 D Δ {if B C 1 C 2 } Δ Δ = Δ 1 Δ 2 DWhile Δ B : D D ∪ D Δ {C} Δ D Δ {while B C} Δ Δ Δ , Δ Δ DSeq D Δ {C 1 } Δ D Δ {C 2 } Δ D Δ {C 1 ; C 2 } Δ</formula><p>C, then dependencies Δ hold afterwards. A judgment C : Δ corresponds to a derivation ∅ Δ 0 {C} Δ where Δ 0 (x) = {x}, for all x. In some of the rules we write Δ E : D to mean that expression E has a value which depends at most on the variables in D, assuming the variable dependencies captured by Δ.</p><p>The derivation system for dependency is taken from <ref type="bibr" target="#b9">[10]</ref>, where it is shown to be a De Morgan dual of the Hoare-style independence logic of <ref type="bibr" target="#b0">[1]</ref>. The logic of these rules is similar to that used by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> but more sophisticated (and precise) in that it is flow-sensitive and thus does not require each variable to be assigned a fixed security type. The formal meaning and correctness of the rules are covered by Definition 4 and Theorem 2 below.</p><p>Notice that the only rule in our analysis which is specific to while statements is the [DWhile] rule in Table <ref type="table">6</ref>. Bounds for loops can be found by applying rule [Dep]: the bounds so obtained are conservative, i.e., it is pessimistically assumed that all that can be leaked from the variables on which the loop depends will be leaked. A more refined treatment of loops can be found in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency analysis: an example</head><p>Consider the following Java program:</p><formula xml:id="formula_60">public static int foo(int high) { int n = 16; int low = 0; while (n &gt;= 0) { int m = (int)Math.pow(2,n); if (high &gt;= m) { low = low + m; high = high -m; } n = n -1; } return low; }</formula><p>Below is a data dependency analysis for the while loop using the rules in Table <ref type="table">6</ref>. Notice that since in Δ we have low → {low, n, high} then by using rule [Dep] the analysis will conclude that all content of high is leaked into low (because high ∈ Δ (low)). In fact, even if no direct flow is present between high and low, it can be easily seen that the above program will leak one bit of the secret at each iteration of the loop.</p><formula xml:id="formula_61">E def = 2 n C def = C 1 ; n = n -1 C 1 def = if (high E) C 2 skip C 2 def = low = low + E; high = high -E Δ def = Δ 0 [low → {low, n, high}][high → {high, n}] DWhile Δ (n 0) : {n} DSeq {n} Δ {C 1 } Δ {n} Δ {n = n -1} Δ ∅ ∪ {n} Δ {C} Δ ∅ Δ 0 {while (n 0) C} Δ where Δ 0 Δ , Δ Δ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Direct flow</head><p>Rule <ref type="bibr">[Assign]</ref> states that if in a context Γ , E leaks between a and b then we deduce that after running x := E under the context Γ x leaks between a and b, i.e., the context x : [a, b].</p><p>Rule <ref type="bibr">[NoAss]</ref> states that if a variable is not assigned to inside the command C (condition x ∈ Ass(C)) then the leakage of x after C is unchanged from Γ .</p><p>Rules [Skip] and [Seq] are standard Hoare-logic style rules for skip and sequential composition. We note that [Skip] is easily derived using [NoAss] and [Join] and so we do not give a direct proof of its correctness. </p><formula xml:id="formula_62">Γ E : [a, b] Γ {x = E} x : [a, b] Seq Γ {C 1 } Γ Γ {C 2 } Γ Γ {C 1 ; C 2 } Γ Skip Γ {skip} Γ NoAss Γ , x : [a, b] {C} x : [a, b] x ∈ Ass(C)</formula><p>2. The upper bound of the leakage of x in the false-branch. 3. The maximum leakage possible (k) multiplied by the upper bound for the probability of the test being true (q). The idea in using qk for the true-branch is that if the test is true very seldom then q will be very small and so qk will be a reasonable approximation to the leakage of the true-branch. Rule [If(4)] aims to provide lower bounds for conditionals when the guard is an equality test. It says that if s is a lower bound of the probability of the test being false and a is lower bound for the leakage of x in the false-branch, then a lower bound for the leakage of x after the conditional is given by s(a -Z(s)) where Z(s) = ( 1 ss)(k log( 1 ss)) will be formally justified later. The quantity a -Z(s) is a lower bound on the entropy of x in C 2 given knowledge that the equality test is false. As shown later this lower bound is a consequence of the information theoretic L 1 inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Correctness</head><p>Correctness of the analysis is stated with respect to a non-standard "angelic" denotational semantics in which non-termination is replaced by the behaviour of skip. In Section 5.3 we show that, for the purposes of calculating leakage, the angelic semantics gives a tight approximation to the standard semantics, in the sense that it underestimates the overall leakage by at most 1 bit.</p><p>The angelic semantics of a command C is denoted JCK A : Σ → Σ and is defined, like the standard semantics, by induction on the syntax. The definition is formally identical to the standard semantics with the exception of the case for while, which is as follows:</p><formula xml:id="formula_63">Jwhile B CK A = f A where: f A (σ) = f (σ) if f (σ) =⊥ σ if f (σ) =⊥ f = lfp F F (f ) = λσ. f (JCK A σ) if JBKσ = 1 σ if JBKσ = 0</formula><p>The angelic semantics "over approximates" the standard semantics, in the following sense:</p><formula xml:id="formula_64">Lemma 2. For all C, σ, if JCKσ =⊥ then JCKσ = JCK A σ.</formula><p>Proof. By a simple induction on C, observing that JCK A can trivially be viewed as a map of type Σ → Σ ⊥ , so the lemma says that JCK A JCK, then appeal to monotonicity of lfp.</p><p>Correctness then amounts to the following: if Γ {C} Γ then, for all input distributions which model Γ , the output distribution (as determined by the angelic semantics) models Γ . To make this precise, let G : Σ → V (think of G as the semantics of some expression) and let F : Σ → Σ (think of F as the angelic semantics of some initial part of the program being analysed). In the following we assume given a distribution on Σ, so that any such G and F may be considered as random variables.</p><formula xml:id="formula_65">Definition 3. Say that G models [a, b], written G |= [a, b], if a H(G|L in = L)</formula><p>b for all L. Say that F models Γ , written F |= Γ , if λσ.F (σ)(x) |= Γ (x) for all x in the domain of Γ . We write |= Γ as shorthand for λσ.σ |= Γ .</p><p>Informally λσ.F (σ)(x) maps a store σ into the value of the variable x in the store F (σ). We will use Fx (or simply F when x is clear from the context) for λσ.F (σ)(x). Similarly for G : Σ → V , Ĝ will stand for λσ.G(σ). To simplify notations we will often write Ĉ for a command C instead of JCK A . For example z = 3 x is the map which sends a store σ into σ(x) if x = z and into 3 if x = z.</p><p>We can now state correctness of the analysis rules.</p><p>Theorem 1 (Correctness).</p><p>1.</p><formula xml:id="formula_66">If F |= Γ and Γ E : [a, b] then JEK • F |= [a, b]. 2. If F |= Γ and Γ {C} Γ then JCK A • F |= Γ .</formula><p>Corollary 2. If |= Γ and Γ {C} Γ then, for the transformational system induced by</p><formula xml:id="formula_67">JCK A , Γ (x) = [a, b] ⇒ a F L (H X) b, where X = λσ.σ(x).</formula><p>In the proofs below we suppress explicit mention of the assumptions L in = L. The justification for this is that H(X|L in = L) is just H(X) for the residual distribution given L in = L and we are implicitly quantifying over all possible choices of L. We write, for example, H(X) b, as shorthand for ∀L.H(X|L in = L) b, and so on. The proof of the Correctness Theorem is by induction on the height of the derivations. The proof cases for Part 1 are very simple, except for the rule [Eq <ref type="bibr" target="#b1">(2)</ref>] which is dealt with in Section 5.1.</p><p>Proof (Correctness, Part 1). Note throughout that 0 is necessarily a correct lower bound since entropy is non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: EConj, BConj.</head><p>Immediate by inductive hypothesis.</p><p>Case: k-Bits, 1-Bit. These are absolute upper bounds determined by the sizes of the value spaces for expressions in the language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Const.</head><p>The entropy of any constant function is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Var.</head><p>Immediate from the definitions.</p><p>problem of refining bounds on H(X = Y ) (by H(X = Y ) we mean H(Z) where Z = true if X = Y , Z = false otherwise). We begin with a simple observation: when the distribution of values for E 1 is close to uniform (X has high entropy) and the distribution for E 2 is concentrated on just a few values (Y has low entropy), then most of the time, E 1 and E 2 will not be equal. Thus, qualitatively, we can see that high entropy for X and low entropy for Y implies a low probability that E 1 ==E 2 evaluates to true (a low value for P (X = Y )). If we can quantify this bound on P (X = Y ) we can use it to calculate a bound on H(X = Y ).</p><p>It turns out that a well known result from information theory provides just such a quantitative bound: Lemma 3 (Fano's inequality, <ref type="bibr" target="#b5">[6]</ref>). Let X and Y be random variables taking values in a set of size 2 k . Then</p><formula xml:id="formula_68">H(X|Y ) U k (P (X = Y )) where U k (q) def = q log 1 q + (1 -q) log 2 k -1 1-q .</formula><p>We note that Fano's inequality is normally stated in terms of the so called error probability P e , where P e def = P (X = Y ). Since P e = 1-P (X = Y ), our presentation is a trivial rewriting of the usual one.</p><p>Our justification of [Eq <ref type="bibr" target="#b1">(2)</ref>] proceeds in three steps:</p><p>1. [ <ref type="bibr">Lemma 4]</ref> We argue that the quantity a-b used in the side condition of [Eq <ref type="bibr" target="#b1">(2)</ref>] is a lower bound for H(X|Y ), hence U k (q) ab implies U k (q) H(X|Y ). 2. [Lemma 5] We use Fano's inequality to show that any q 1/2 k such that U k (q) H(X|Y ) is an upper bound for P (X = Y ). 3. [Lemma 6] For q 0.5, if q is an upper bound for P (X = Y ) then B(q) is an upper bound for H(X = Y ). Proof. By assumption a H(X) and b</p><formula xml:id="formula_69">H(Y ), hence a -b H(X) -H(Y ). Thus, since H(X) H(X, Y ), a -b H(X, Y ) -H(Y ) = H(X|Y ).</formula><p>Lemma 5. Let X and Y be random variables taking values in a set of size 2 k and let q 1/2 k . Then U k (q) H(X|Y ) implies q P (X = Y ).</p><p>Proof. We show the contrapositive. Suppose that P (X = Y ) &gt; q. Note that for q 1/2 k , U k (q) is a decreasing function of q (see, for example, Fig. <ref type="figure" target="#fig_6">4</ref>) hence P (X = Y ) &gt; q implies U k (P (X = Y )) &lt; U k (q). By Fano's inequality, H(X|Y ) U k (P (X = Y )), hence H(X|Y ) &lt; U k (q), as required. </p><formula xml:id="formula_70">Γ (h == 0) : [0, ] Δ 0 Δ 1 Γ {if(h == 0) (x = 0) (x = 1)} x : [0, + 0 + 0]</formula><p>then derive that the leakage from h to x will be in the interval [0, 0.78 × 10 -9 ]. Let Γ be such that Γ (h) = <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref> and let def = 0.78 × 10 -9 . The derivation is illustrated in Table <ref type="table" target="#tab_4">9</ref>. The use of [Eq <ref type="bibr" target="#b1">(2)</ref>] is justified by checking the side condition for q = 1 2 32 , a = 32, b = 0 (check U k (q) = 32 = 32 -0) and B(q) = . Derivation Δ 0 is the sub-derivation for the true-branch x = 0:</p><formula xml:id="formula_71">Assign Const Γ 0 : [0, 0] Γ {x = 0} x : [0, 0]</formula><p>Derivation Δ 1 , the sub-derivation for the false-branch x = 1, is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2</head><p>Consider the following schema for Java programs using n if statements, where each Bi(high) is a boolean expression which tests the ith bit of high. Such programs thus copy n bits of high (a 32 bit variable) into low and then test high and low for equality. We assume n &lt; 32. Note that high itself is not modified.</p><p>What will be the result of the analysis on the final test low == high? Assuming that the high values are uniformly distributed the result depends on n. By using the techniques described in the previous pages we can bound the leakage; a few values are shown in Table <ref type="table" target="#tab_5">10</ref>. We have assumed uniform distribution, hence H(high) = 32 and at the end of the program H(low) = n. In order to upper bound H(low == high) we use [Eq(2)], i.e., we first determine q such that H(high) -H(low) U 32 (q) and we then compute B(q).  We can set reasonable bounds on the leakage into a given variable as the result of the execution of an if statement (implicitly, this is a variable within the statement), assuming that we have the following:</p><p>bounds on the leakage due to evaluation of the control expression and bounds on the leakage into the variable as a result of the execution of the individual branches the control expression is an equality test the test is not true very often (i.e., the probability p(ê == ff) ≈ 1) Notation-wise in the following pages we are referring to the following command:</p><formula xml:id="formula_72">C = if e C 1 C 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper Bounds</head><p>From standard information theory</p><formula xml:id="formula_73">H( Ĉ) H( Ĉ, ê) = H( Ĉ|ê) + H(ê)</formula><p>Assuming we can calculate an upper bound for H(ê) as shown in the previous section, we want to find a bound on the other term, H( Ĉ|ê). Since we will be using the quantity p(ê == ff) extensively in what follows we give it a name, r. That is, p(ê == ff) def = r. We can expand H( Ĉ|ê) into a term summing the weighted contribution from each branch:</p><formula xml:id="formula_74">H( Ĉ|ê) = rH( Ĉ|ê == ff) + (1 -r)H( Ĉ|ê == tt) = rH( Ĉ2 |ê == ff) + (1 -r)H( Ĉ1 |ê == tt)</formula><p>We assume the entropy of the true-branch is bounded by k (the maximum entropy possible for x, assuming x is a k-bit variable, and the always-available worst case assumption). Since r is close to 1 this branch will make a small contribution when weighted. We can show the weighted other branch is bounded by H( Ĉ2 ).</p><formula xml:id="formula_75">rH( Ĉ2 |ê == ff) rH( Ĉ2 |ê == ff) + (1 -r)H( Ĉ2 |ê == tt) = H( Ĉ2 |ê) H( Ĉ2 )</formula><p>Hence we have</p><formula xml:id="formula_76">H( Ĉ) H(ê) + H( Ĉ2 ) + (1 -r)k</formula><p>The inference rule for this is given as rule [If <ref type="bibr" target="#b2">(3)</ref>] in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower Bounds</head><p>We discuss setting a lower bound on the leakage into the variable. Given the role that lower bounds play in determining upper bounds in the analysis of equality tests (see above), it is quite useful to be able to determine tight lower bounds. To bound the leakage from below we use the L 1 inequality proposition (this is well known in information theory and can be found in <ref type="bibr" target="#b5">[6]</ref>):</p><p>We state the proposition. First, we need to define the L 1 distance between two probability distributions on the same event space.</p><p>Definition 5 (L 1 Distance). Define the L 1 distance between two probability distributions, p and q on the same event space, written pq 1 , as follows</p><formula xml:id="formula_77">p -q 1 def = v |p(v) -q(v)|</formula><p>The following is proved in <ref type="bibr" target="#b5">[6]</ref> (Theorem 16.3.2)</p><formula xml:id="formula_78">Proposition 8 (L 1 Inequality). p -q 1 1 2 ⇒ |H(p) -H(q)| p -q 1 log |X| p -q 1</formula><p>where |X| is the size of the event space for both p and q.</p><p>We can manipulate the RHS of the inequality in the consequent as follows:</p><formula xml:id="formula_79">p -q 1 log |X| p -q 1 = p -q 1 (log |X| -log p -q 1 ) = p -q 1 (k -log p -q 1 )</formula><p>Let's now define the two following probability distributions:</p><formula xml:id="formula_80">p -q 1 (k -log p -q 1 ) ( 1 r -r)(k -log( 1 r -r)</formula><p>) is justified by Proposition 9 and the fact that λx.x(k-log(x)) is monotonic over the interval of interest, (0, 1  2 ], for k ∈ N + . This can be demonstrated by differentiating the function and determining the sign of the first derivative on the interval (0, 1  2 ] (always positive). Hence, by noticing that H(p) = H( Ĉ2 ) and H(q) = H( Ĉ2 |ê == ff) we get a lower bound on H( Ĉ2 |ê == ff):</p><formula xml:id="formula_81">H( Ĉ2 |ê == ff) H( Ĉ2 ) -Z(r)</formula><p>We can use this to obtain a lower bound for the command C as follows:</p><formula xml:id="formula_82">H( Ĉ) H( Ĉ|ê) = (1 -r)H( Ĉ|ê == tt) + rH( Ĉ|ê == ff) rH( Ĉ|ê == ff) = rH( Ĉ2 |ê == ff) r(H( Ĉ2 ) -Z(r)) s(a -Z(s))</formula><p>Where a is a lower bound for H( Ĉ2 ) and s r. To justify If(4) in Table <ref type="table">8</ref> we are left with finding a lower bound s r. In the rule this is given by solving a 1b 2 = U k (1s) where a 1 (resp. b 2 ) is a lower (resp. upper) bound for E 1 (resp. E 2 ). This is a consequence of Proposition 7. In the rule, we restrict our search for the lower bound to the region 1 2 s 1 -1 2 k . This is safe since s(a -Z(s)) is monotone in the region 1 -1 2 k to 1. Notice that the quantities involved in inference rule [If(4)] can be automatically computed using techniques discussed in the analysis of boolean expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>The importance of the lower bounds provided by If(4) can be seen in the following example. Let P be the program</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>if (h==n) {h=0} {y=h} if (y==m) {l=0} {l=1}</head><p>Assuming that the secret variable h is very unlikely to be equal to a constant n the true-branch of the first conditional will be chosen almost never and so almost always y=h. Hence statistically P is very similar to the program P if (h==m ) {l=0} {l=1} for an analysis to give similar results for P and P we need the analysis of y in P at the end of the first conditional to be similar to the analysis of the secret input h which is h: <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref> (we are assuming uniform distribution on the secret).</p><p>Using the same argument as in a previous example we have: [Eq2] h : <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref> </p><formula xml:id="formula_83">(h == n) : [0, ] where = B(1/2 32 ) ≈ 7.8 × 10 -9 .</formula><p>We also know that the analysis of the false-branch will be [Assign] h : <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref>{y = h} y : <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref> and so we get values q = 1/2 32 , a = 32 which we use in the conditional rule to deduce h : <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref>{C}y :[(1 -1/2 32 )(32 -4.2 -8 ), 32] where C = if (h==n) {h=0} {y=h}. Notice that this bound is extremely close to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref> and hence the bounds for the low variable l in P and P will be very close (in both cases around 7.8 × 10 -9 bits).</p><p>On the other hand, suppose we didn't have good lower bounds for y after the first conditional (suppose for example we had y : [0, 32]). Then testing (y==m) against a constant m will produce [Eq1] y : [0, 32] (y == m) : [0, min(1, 32)] = [0, 1]. This is because the side conditions for [Eq2] are not satisfied and so the only applicable rule is [Eq1] which will result in much higher bounds for l in P (1 bit) than in P (around 7.8 × 10 -9 bits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Observing non-termination</head><p>The so-called angelic semantics used in the correctness proof above has the effect of ignoring non-termination and hence any information flow which might flow as a result of variation in termination behaviour. Intuitively, this is dangerous. In reality, it is possible for a program's termination behaviour to vary according to its inputs, including the confidential ones. Moreover, given knowledge of the code, an observer can, in practice, observe that a program is never going to terminate. On the other hand, consider how an observer can tell that a program is stuck in an infinite loop: either by some low-level observation of the program state or by timing it. But our input-output model of observation -captured precisely by the simple statetransformer semantics -clearly excludes these possibilities. So there is a strong argument for ignoring non-termination in the proof: it falls outside our abstraction of the observer. Note, however, that this does not mean it is safe to ignore the possibility of such flows, only that modelling them would require a more fine-grained model of the observer (and hence a different semantics).</p><p>In fact, we can do a little better than this. Let us suppose that non-termination is observable. We can capture this by using the standard denotational semantics and treating ⊥ as an observable value, like any other. How would this affect our results?</p><p>The following result shows that, if we measure flows with respect to the standard denotational semantics, rather than the angelic semantics, our analysis under-estimates flows by at most one bit. Proposition 10. If |= Γ and Γ {C} Γ then, for the transformational system induced by JCK, Γ</p><formula xml:id="formula_84">(x) = [a, b] =&gt; F L (H X) (b + 1)</formula><p>, where X = λσ.σ(x).</p><p>Proof. Let T C : Σ → {t, f } be defined by:</p><formula xml:id="formula_85">T C σ = t if JCKσ =⊥ f if JCKσ =⊥</formula><p>Fix some program variable x and consider the corresponding output observations JCK and JCK A . By Shannon's inequalities, we have</p><formula xml:id="formula_86">H( JCK) H( JCK, T C ) = H( JCK|T C ) + H(T C )<label>(21)</label></formula><p>Since (Lemma 2) JCK A JCK, we have:</p><formula xml:id="formula_87">JCKσ = JCK A σ if T C σ = t ⊥ otherwise From this it follows that H( JCK|T C = t) = H( JCK A |T C = t), while H( JCK|T C = f ) = 0. Hence, by the definition of conditional entropy, H( JCK|T C ) H( JCK A |T C ). Thus H( JCK|T C ) H( JCK A |T C ) H( JCK A ). Furthermore, since T C is 2-valued, H(T C ) 1. Hence, by<label>(21)</label></formula><p>, H( JCK) H( JCK A ) + 1. The proposition then follows by Corollary 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis of arithmetic expressions</head><p>We can improve the analysis of leakage via arithmetic expressions by exploiting algebraic knowledge of the operations together with information about the operands acquired through supplementary analyses such as parity analysis, constant propagation analysis etc. The (binary) operations we consider are addition, subtraction, multiplication (+, -, * ) on the twos-complement representations of k bit integers with overflow.</p><p>We use for the binary operator while the random variables X, Y and Z range over the first and second inputs and the output of the operator respectively. They are related by </p><formula xml:id="formula_88">P (Z = z) = (x,y)∈ -1 (z) P (X = x, Y = y)</formula><formula xml:id="formula_89">OpMax] Γ E 1 : [-, b 1 ] Γ E 2 : [-, b 2 ] Γ E 1 E 2 : [0, b 1 + b 2 ] [AddMin] Γ E 1 : [a 1 , b 1 ] Γ E 2 : [a 2 , b 2 ] Γ E 1 + E 2 : [max(a 1 , a 2 ) -min(b 1 , b 2 ), k] [ConstAdd] E 1 is constant Γ E 2 : [a, b] Γ E 1 + E 2 : [a, b] [ZeroMult] E 1 is zero Γ E 2 : [a, b] Γ E 1 * E 2 : [0, 0] [OddMult] E 1 is an odd constant Γ E 2 : [a, b] Γ E 1 * E 2 : [a, b]</formula><p>We assume we know bounds on the entropy of the input space, H(X, Y ), and entropy of the projected input spaces, H(X) and H(Y ) and we aim to find bounds on the entropy of the output space, H(Z).</p><p>Since a binary arithmetic operation on twos-complement integers is a function from X × Y to Z and since functions can only reduce entropy or leave it the same we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">H(Z) H(X, Y )</head><p>In general we will not know H(X, Y ) but only H(X) and H(Y ). Since H(X, Y ) H(X) + H(Y ) we can use this sum as an upper bound. The upper bound observation is captured for an operation in the rule [OpMax] in Table <ref type="table" target="#tab_6">11</ref>.</p><p>Further improvements to either the upper or the lower bound depend on knowledge more specific to the operation and/or the expressions.</p><p>Before examining individual operations there is something we can say about the relationship between H(Z) and H(X, Y ) that holds for all operations which we exploit directly in Proposition 13.</p><p>We can use the functional relationship between inputs and outputs to show that the entropy of the output space is the entropy of the input space less the entropy of the input space given knowledge of the output space. This latter quantity is a measure of the entropy of the input space destroyed by the function . The idea is expressed formally in the following proposition: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Addition and subtraction</head><p>Addition and subtraction are essentially the same operation in twos-complement arithmetic so we restrict our attention to addition.</p><p>Bitwise addition (+) makes the set of numbers representable in twos-complement using k bits a cyclic additive group with identity 0 and generator 1 (so a + 1 has its usual meaning except when a = 2 k-1 -1, in which case a + 1 = -2 k-1 ). The inverse -a is given by the twos-complement operation (so -a has its usual meaning except for a = -2 k-1 , which is its own inverse). Proposition 12. Let T k = {-2 k-1 , . . . , 2 k-1 -1}, the set of integers representable by k bits in twos-complement. Bitwise addition (+) makes T k a cyclic additive group with identity 0 and generator 1.</p><p>We don't include a proof here but the proposition is straightforward to verify. As an immediate consequence, addition of a constant is just a permutation on T k and thus leaves entropy unchanged. This is captured by rule <ref type="bibr">[ConstAdd]</ref>. The symmetric rule which follows from commutativity of addition is left implicit.</p><p>The cyclic group structure further allows us to show that either operand of + is a function of the other operand and the result. This in turn allows us to establish a tighter lower bound for +: the entropy of the outcome, H(Z), is bigger than or equal to the entropy of the input space, H(X, Y ) less the smaller of the two projected entropies for that space, H(X) and H(Y ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. First we establish that H(Z) H(X, Y ) -H(X).</head><p>We can make arguments similar to the one employed to justify Proposition 11 and demonstrate that H(X, Y , Z) = H(X, Z).</p><p>By Proposition 11 we have H(Z) = H(X, Y ) -H(X, Y |Z) so it suffices to show that H(X, Y |Z) H(X)</p><formula xml:id="formula_90">H(X, Y |Z) = H(X, Y , Z) -H(Z) = H(X, Z) -H(Z) = H(X|Z) H(X)</formula><p>By a similar argument we can also establish H(Z) H(X, Y ) -H(Y ). These two inequalities establish the proposition.</p><p>Since H(X, Y ) max(H(X), H(Y )) we can safely replace H(X, Y ) with that quantity. This provides the rule [AddMin] in Table <ref type="table" target="#tab_6">11</ref> for calculating an improved lower bound for addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Multiplication</head><p>Multiplication is less straightforward to analyse than addition as the algebraic structure of the operation is more complex. We are not currently able to provide any general result for the operation. However, in the event that some subsidiary prior analysis is able to identify a useful property of one of the operands of the operation, we can get very good bounds on the entropy of the output, in particular when one of the operands is odd or when one of the operands is zero.</p><p>Multiplication by zero always has zero as the result, i.e., Z has value space singleton set {0} whose element has probability 1, so knowing that one operand is zero guarantees that H(Z) = 0. This observation is captured in the rule [ZeroMult] in Table <ref type="table" target="#tab_6">11</ref>. The symmetric rule which follows from commutativity of multiplication is left implicit.</p><p>To justify the rule [OddMult] recall that the order of a group is the number of its elements, hence the order of T k is 2 k . Furthermore, the order of any element a is defined to be the order of the cyclic subgroup &lt;a&gt; which it generates (with elements {n.a : n 0}). We denote the order of an element a by o(a). We can then state the following Proposition 14. For a ∈ T k where a is odd, o(a) = 2 k , i.e., any odd element is a generator for T k .</p><p>This can be demonstrated using some elementary group theory since every odd number does not divide the order of T k .</p><p>Because odd elements are generators for the whole set, multiplication by an odd constant (i.e., zero entropy in one component) can be viewed as an injective function from T k to T k and so the entropy of the output space can never be less than the entropy of the other operand. Proposition 15. Let Z = * (X, Y ) where ∀x ∈ X. x is an odd constant, then</p><formula xml:id="formula_91">H(Z) H(Y )</formula><p>The implication of this result for propagation of lower bounds during multiplication is captured in the rule <ref type="bibr">[OddMult]</ref> in Table <ref type="table" target="#tab_6">11</ref>. The symmetric rule which follows from commutativity of multiplication is left implicit.</p><p>Although we don't have anything we can say in general about leakage via multiplication we do have a theoretical result which may prove useful in future work: we know the size of the inverse image of an element of T k .</p><formula xml:id="formula_92">Theorem 3. For c in T k , | * -1 (c)| = (k + 1 -log o(c))2 k-1 if c = 0 (k + 2)2 k-1 if c = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and future work</head><p>The work presented in this paper is the first time information theory has been used to measure interference between variables in a simple imperative language with loops. An obvious and very desirable extension of the work would be to a language with probabilistic operators.</p><p>Incremental improvement of the analysis could be given by a subtler treatment of loops and by improved bounds on a wider range of expressions. A similar syntax based, security related analysis might be applied to queries on a secure database. Denning <ref type="bibr" target="#b7">[8]</ref> did work on information flow in database queries.</p><p>It would be also interesting to be able to provide a "backward" analysis where given an interference property that we want a program to satisfy we are able to deduce constraints on the interference of the input. A simple example of this scenario is provided by Shannon's perfect secrecy theorem where the property of noninterference on the output implies the inequality H(L) H(Out).</p><p>Timing issues like "rate of interference" could also be analysed by our theory allowing for a quantitative analysis of "timing attacks" <ref type="bibr" target="#b30">[31]</ref>.</p><p>On a more speculative level we hope that quantified interference could play a role in fields where modularity is an issue (for example model checking or information hiding). At the present modular reasoning seems to break down whenever modules interfere by means other than their interfaces. However if once quantified the interference is shown to be below the threshold that affects the desired behaviour of the modules, it could be possible to still use modular reasoning. An interesting development in this respect could be to investigate the integration of quantified interference with non-interference based logic <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In our work information theory and denotational semantics provide two different levels in the analysis. However recent work relating denotational semantics and entropy has shown that there is an interaction between these two disciplines: both theories model in different albeit related ways partiality/uncertainty. The choice of flat domains as a denotational semantics for our programming language was motivated by a desire to emphasise that our analysis assumes programs are not reactive but have a functional, or data transforming, semantics. It may well be an accident that these flat domains sit comfortably with entropic measures but the work of K. Martin <ref type="bibr" target="#b14">[15]</ref> begs the question whether there is a more fundamental relationship between the two being utilised in our work.</p><p>The authors would like to thank Peter O'Hearn and the anonymous referees for their helpful comments, corrections and suggestions for improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Allowed and disallowed flows.</figDesc><graphic coords="19,189.47,203.24,71.28,68.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Safety requirement on flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. McLean's flow model.</figDesc><graphic coords="19,189.56,309.80,71.05,128.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>expression analysis rules have the form Γ E : [a, b] where Γ is a partial function from Var to real-valued pairs (representing intervals) of the form [a, b] with a b. The meaning of a rule Γ E : [a, b] is that the expression E has leakage in the interval [a, b] assuming that the leakage of each variable x in E lies in the interval Γ (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, b] we write Γ -(x) for a and Γ + (x) for b. The meaning of Γ is as for expressions: Γ (x) = [a, b] asserts that, prior to execution of C, the amount of information leaked into x lies in the interval [a, b]. Γ then reflects the effects of any changes to variables caused by execution of C. Note that the domains of Γ and Γ need not be equal. In particular: many rules assert a leakage interval for just one variable on the right hand side, taking the form Γ {C} x : [a, b]; many rules assert</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>either a lower bound only (intervals of the form [a, k]) or an upper bound only (intervals of the form [0, b]). Rules [CConj] and [Join] in Table 4 then allow us to combine multiple such derivations thus constructing Γ with larger domains and specifying smaller intervals. Rule [CConj] is similar to [EConj], i.e., if x : [a 1 , b 1 ] and x : [a 2 , b 2 ] are derived under the same initial context and running the same command C then x : [max(a 1 , a 2 ), min(b 1 , b 2 )].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 4 .</head><label>4</label><figDesc>Let a H(X) and let b H(Y ). Then ab H(X|Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>public static void foo(int high) { int low = 0; int b; if (B1(high)) b = 2^0; else b = 0; low = low + b; ... if (Bn(high)) b = 2^(n-1); else b = 0; low = low + b; System.out.println(low == high); }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Proposition 11 .</head><label>11</label><figDesc>Let Z = (X, Y ) then H(Z) = H(X, Y ) -H(X, Y |Z).Proof. Using conditional entropy expressions we have this relationship between H(X, Y ) and H(Z): H(X, Y , Z) = H(Z) + H(X, Y |Z). However we also havep(x, y, z) = p(x, y) if (x, y) ∈ -1 (z) = 0 otherwise Hence H(X, Y , Z) =x,y,z p(x, y, z) log(p(x, y, z)) =x,y p(x, y) log(p(x, y)) = H(X, Y ) Then we have H(X, Y ) = H(Z) + H(X, Y |Z) and so H(Z) = H(X, Y ) -H(X, Y |Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proposition 13 .</head><label>13</label><figDesc>Let Z = +(X, Y ), then H(Z) H(X, Y )min(H(X), H(Y ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we find: H(x s |y s ) 1 (because H(x s |y s ) H(x s |abs(x))). H(x s |y s ) 4.8 (because H(x s |y s ) = (1/32)H(x s |y = 1). + (31/32)H(x s |y = 2) = 0 + (31/32)log(31)) H(x s |y s ) = 4 (because H(x s |y s</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 Leakage inference: logical CConj</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>Leakage inference: data processing Dep</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7</head><label>7</label><figDesc>Leakage inference: direct flow Assign</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Derivation for Example 1</cell></row><row><cell>Eq(2)</cell><cell>Var</cell><cell>Γ h : [32, 32]</cell><cell>Const</cell><cell>Γ 0 : [0, 0]</cell></row><row><cell>If(1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10 n</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell>H(high) -H(low)</cell><cell>q</cell><cell>B(q) H(low == high)</cell></row><row><cell>2</cell><cell>30</cell><cell>0.074</cell><cell>0.38</cell></row><row><cell>7</cell><cell>25</cell><cell>0.24</cell><cell>0.80</cell></row><row><cell>12</cell><cell>20</cell><cell>0.41</cell><cell>0.98</cell></row><row><cell cols="4">5.2. Bounding leakage into a variable via execution of an if statement</cell></row><row><cell cols="4">This section provides justifications for rules [If(3)] and [If(4)].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>Some refined analysis rules</cell></row><row><cell>[</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>D. Clark et al. / A static analysis for quantifying information flowTable 3 Leakage inference: expressions EConjΓ E : [a 1 , b 1 ] Γ E : [a 2 , b 2 ] Γ E : [max(a 1 , a 2 ), min(b 1 , b 2 )] BConj Γ B : [a 1 , b 1 ] Γ B : [a 2 , b 2 ] Γ B : [max(a 1 , a 2 ), min(b 1 , b 2 )] k-Bits Γ E : [0, k] 1-Bit Γ B : [0, 1] Const Γ n : [0, 0] Var Γ , x : [a, b] x : [a, b] And Γ B i : [_, b i ] i = 1, 2 Γ (B 1 ∧ B 2 ) : [0, b 1 + b 2 ] Neg Γ B : [a, b] Γ ¬B : [a, b] Plus Γ E i : [_, b i ] Γ (E 1 + E 2 ) : [0, b 1 + b 2 ] Γ E 1 : [_, b 1 ] Γ E 2 : [_, b 2 ] Γ (E 1 == E 2 ) : [0, b 1 + b 2 ]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Table 8 Leakage inference: indirect flow If(1) ΓB : [_, b] Γ {C i } x : [_, b i ] i = 1, 2 Γ {ifB C 1 C 2 } x : [0, b + b 1 + b 2 ] If(2) Γ B : [0, 0] Γ {C i } x : [a i , b i ]</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research partially supported by EPSRC grants EP/C545605/1, EP/C009746/1, and EP/C009967/1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Indirect flow</head><p>Rule [If <ref type="bibr" target="#b0">(1)</ref>] states that the leakage of x after a conditional cannot exceed the sum of the leakage of the guard and the leakage of x in both branches (a direct consequence of the Data Processing Lemma).</p><p>It may seem at first sight that this rule is too conservative and perhaps should be replaced by a rule that calculates an upper bound of b + max(b 1 , b 2 ). The following example demonstrates why b + max(b 1 , b 2 ) is not an upper bound in general.</p><p>If h is uniformly distributed over {0, 1, 2, 3} this program leaks all 2 bits of information in h to x.</p><p>Our analysis would use a conservative upper bound of 1 bit for the leakage due to h &lt; 2 and we can precisely calculate the leakage due to the two tests h == 0 and h == 2 as 1/4 log 4 + 3/4 log(4/3) = 0.8113 for each. So an upper bound on the leakage using b + max(b 1 , b 2 ) produces 1.8113 which is not safe. The upper bound calculated by [If <ref type="bibr" target="#b0">(1)</ref>] is 0.8113 + 0.8113 + 1 = 2.623.</p><p>Rule [If <ref type="bibr" target="#b1">(2)</ref>] states that if there is no indirect flow in a conditional then the leakage of x after the conditional is the worst possible choice of leakage along the two branches.</p><p>Rule [If <ref type="bibr" target="#b2">(3)</ref>] states that if a conditional has as guard an equality test then the upper bound of the leakage of x after the command is given by summing:</p><p>1. The upper bound for the equality test as given by [Eq <ref type="bibr" target="#b1">(2)</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: And, Plus, Eq(1).</head><p>Immediate by the Data Processing Lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Neg.</head><p>Permutation of the elements in a domain has no effect on entropy.</p><p>Case: Eq(2). See Section 5.1.</p><p>Before proving part 2 of the Correctness Theorem, we need to establish the correctness of the Dependency Analysis (Table <ref type="table">6</ref>). The property we require is that C : Δ implies λσ.JCK A σ(x) can be decomposed as G • F where F picks out just the vector of variables in Δ(x). We derive this as a corollary of a more general property of the Dependency Analysis, formalised using the following definition (which may be viewed as a qualitative analogue of Definition 3).</p><p>Theorem 2. Suppose D Δ {C} Δ . Then:</p><p>Proof. Proof of the theorem is by induction on C. This is essentially the same as the correctness proof in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof (Correctness, Part 2).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Dep.</head><p>We </p><p>Γ + (y i ), for 1 i n, as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: CConj.</head><p>The hypothesis is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Join.</head><p>The hypothesis is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: Seq.</head><p>By induction hypothesis </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: If(1).</head><p>Here the hypothesis is</p><p>The definition of the if statement in the denotational semantics is as a function IF of the semantics of the guard and the branches, i.e., Jif</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and by compositionality of the semantics</head><p>We hence apply the Data Processing theorem to conclude:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: If(2).</head><p>By applying the induction hypothesis to the premise Γ B : [0, 0], i.e., F |= Γ implies H(λσ.JBK • F (σ)) = 0 we deduce (by basic information theory) that, for each choice of a low input, the map JBK is a constant function. Hence for each choice of a low input IF(JBK, JC 1 K A , JC 2 K A ) is either JC 1 K A or JC 2 K A and hence the result follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case: If(3), If(4).</head><p>Correctness for these rules is proved in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Analysis of equality tests</head><p>This section will justify rule [Eq(2)], i.e., analysis of tests of the form E 1 ==E 2 . By associating random variables X and Y to the two expressions, this becomes a Fig. <ref type="figure">4</ref>. The upper entropy for q in 4 bits. Lemma 6. Let q 0.5. Then q P (X = Y ) implies B(q) H(X = Y ).</p><p>Proof. B(q) is an increasing function of q in the region 0 q 0.5 (see Fig. <ref type="figure">4</ref>).</p><p>Together, these three results provide:</p><p>When using [Eq(2)], as the third lemma above shows, smaller values for q give tighter upper bounds for H(X = Y ). So to find the best upper bound we need to solve equations of the form U k (q) -(ab) = 0, where a and b are known values. For this, simple numerical techniques suffice <ref type="bibr" target="#b11">[12]</ref>. Another computationally useful fact is that B(q)+(1-q)k is an upper bound for U k (q) and that this bound is very tight unless k is small. We note that [Eq(2)] will give useful results in the case that a is high and b is low, that is, when E 1 is known to contain a large amount of confidential information and E 2 is known to contain very little.</p><p>The way in which rule [Eq <ref type="bibr" target="#b1">(2)</ref>] can be applied is illustrated by the example shown in Fig. <ref type="figure">4</ref>. This plots U k (q) and B(q) against q for k = 4 and shows that for a lower bound of (ab) = 3.75, q is bounded by 0 q 0.25 (the precise upper bound is slightly lower than this).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1</head><p>Consider the program P :</p><p>if (h == 0) x = 0; else x = 1;</p><p>with h high-security. Suppose that k = 32 and the input distribution makes h uniform over its 2 32 possible values; by definition of entropy of the uniform distribution we can therefore analyse the program starting with the assumption H(h) = log 2 32 = 32. We can -p(v) = P ( Ĉ2 = v), i.e., Σ σ: Ĉ2 σ=v p(σ) -q(v) = P ( Ĉ2 = v|ê == ff), i.e., Σ σ: Ĉ2 σ=v p(σ|ê == ff) Hence p(v) is the sum of the probabilities of all stores where after the evaluation of C 2 , x = v and q(v) is the sum of the probabilities of all stores where after the evaluation of C 2 , x = v conditioned to such stores having e evaluated to false.</p><p>Recall that we assume p(ê == ff) ≈ 1.</p><p>We now show that pq 1 1 rr. This is useful because when r is near to 1 then 1 rr is less than 1 2 , satisfying the LHS of Proposition 8.</p><p>We first prove</p><p>We then prove Lemma 8. For all v the following is true:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This is proven by cases</head><p>. Notice now that q(v) q 0 (v) because q(v) = P ( Ĉ2 = v|ê == ff) P ( Ĉ2 = v, ê == ff) = q 0 (v) (conditional probability is greater than joint probability): hence we conclude.</p><p>(2) p(v) &lt; q(v): In this case |p(v)q(v)| = q(v)p(v). By definition q(v) = 1 r p(v, ê == ff) and by basic probability q 0 (v) = p(v, ê == ff) p(v) so we have q(v) 1 r p(v) which gives us q(v)p(v) 1 r p(v)p(v). Using again p(v) q 0 (v) we conclude q(v)p(v) 1  r p(v)p(v) 1 r p(v)q 0 (v). Combining the previous lemmas we now conclude the proof of the proposition: for all v |p(v)q(v)| 1 r p(v)q 0 (v) implies Σ v |p(v)-q(v)| Σ v ( 1 r p(v)-q 0 (v)) = 1 r Σ v p(v)-Σ v p(v, ê == ff) = 1 r -r.</p><p>Thus we can apply the L 1 inequality and we get:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information flow analysis in logical form</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amtoft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAS 2004 (11th Static Analysis Symposium</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Giacobazzi</surname></persName>
		</editor>
		<meeting><address><addrLine>Verona, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3148</biblScope>
			<biblScope unit="page" from="100" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Integrity considerations for secure computer systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Biba</surname></persName>
		</author>
		<idno>ESD-TR-76-372</idno>
		<imprint>
			<date type="published" when="1977-04">April 1977</date>
			<publisher>USAF Electronics Systems Division</publisher>
			<pubPlace>Bedford, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantified interference for a while language</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electronic Notes in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="149" to="166" />
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Quantitative analysis of the leakage of confidential data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
		<editor>A. Di Pierro and H. Wiklicky</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
	<note>Electronic Notes in Theoretical Computer Science</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Belief in information flow</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE Computer Security Foundations Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley Interscience</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A lattice model of secure information flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E R</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E R</forename><surname>Denning</surname></persName>
		</author>
		<title level="m">Cryptography and Data Security</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Security policies and security models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goguen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meseguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On flow-sensitive security types</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Principles of Programming Languages, 33rd Annual ACM SIGPLAN -SIGACT Symposium (POPL&apos;06)</title>
		<meeting>Principles of Programming Languages, 33rd Annual ACM SIGPLAN -SIGACT Symposium (POPL&apos;06)<address><addrLine>Charleston, South Carolina, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Isthiaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>O'hearn</surname></persName>
		</author>
		<title level="m">BI as an assertion language for mutable data structures</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
	<note>th POPL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Faires</surname></persName>
		</author>
		<title level="m">Numerical Analysis, PWS-KENT</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantifying information flow</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Automated Verification of Critical Systems</title>
		<meeting>the Workshop on Automated Verification of Critical Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing security threats of looping constructs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the 34th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</title>
		<meeting>cedings of the 34th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">January 17-19, 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entropy as a fixed point</title>
		<author>
			<persName><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Compututer Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="292" to="324" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guessing and entropy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Information Theory</title>
		<meeting>IEEE International Symposium on Information Theory<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic approach to information hiding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mciver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Methodology</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="441" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Security models and information flow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mclean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 IEEE Symposium on Security and Privacy</title>
		<meeting>the 1990 IEEE Symposium on Security and Privacy<address><addrLine>Oakland, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Covert channel capacity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Millen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1987 IEEE Symposium on Research in Security and Privacy</title>
		<meeting>1987 IEEE Symposium on Research in Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic predicate transformers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mciver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming and Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="353" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Di Pierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wiklicky</surname></persName>
		</author>
		<title level="m">Approximate non-interference, in: CSFW&apos;02 -15th IEEE Computer Security Foundation Workshop, I. Cervesato</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Lambda definability and logical relations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Plotkin</surname></persName>
		</author>
		<idno>Memorandum SAI-RM-4</idno>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
		<respStmt>
			<orgName>Department of Artificial Intelligence, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Separation logic: a logic for shared mutable data structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Invited Paper, LICS&apos;02</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Syntactic control of interference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Record 5th ACM Symp. on Principles of Programming Languages</title>
		<meeting><address><addrLine>Tucson, Arizona; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A per model of secure information flow in sequential programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sabelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Symposium on Programming</title>
		<meeting>European Symposium on Programming<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic noninterference for multi-threaded programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sabelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE Computer Security Foundations Workshop</title>
		<meeting>13th IEEE Computer Security Foundations Workshop<address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication, The Bell System Technical Journal 27</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
		<ptr target="http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html" />
		<imprint>
			<date type="published" when="1948-10">July and October. 1948</date>
			<biblScope unit="page" from="623" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Logical relations and the typed lambda calculus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Statman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A model of information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th National Computer Security Conference</title>
		<meeting>the 9th National Computer Security Conference</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Secure flow typing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volpano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Irvine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Security</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Safety versus secrecy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volpano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int&apos;l Symposium on Static Analysis</title>
		<meeting>6th Int&apos;l Symposium on Static Analysis</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="303" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eliminating covert flows with minimum typings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volpano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE Computer Security Foundations Workshop</title>
		<meeting>the 10th IEEE Computer Security Foundations Workshop<address><addrLine>Rockport, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="156" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Verifying secrets and relative secrecy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volpano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Symposium on Principles of Programming Languages</title>
		<meeting>27th ACM Symposium on Principles of Programming Languages<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="268" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward a mathematical foundation for information flow security</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 IEEE Symposium on Security and Privacy</title>
		<meeting>1991 IEEE Symposium on Security and Privacy<address><addrLine>Oakland, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="21" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new outlook on Shannon&apos;s information measures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="466" to="474" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
