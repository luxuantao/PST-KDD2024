<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Complexity of Polynomial Matrix Computations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2003-01-13">January 13, 2003</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Giorgi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claude-Pierre</forename><surname>Jeannerod</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gilles</forename><surname>Villard</surname></persName>
						</author>
						<title level="a" type="main">On the Complexity of Polynomial Matrix Computations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2003-01-13">January 13, 2003</date>
						</imprint>
					</monogr>
					<idno type="MD5">62AC014BDDB64D13268B9A19FFC98F56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Matrix polynomial</term>
					<term>minimal basis</term>
					<term>column reduced form</term>
					<term>matrix gcd</term>
					<term>determinant</term>
					<term>polynomial matrix multiplication Matrice polynomiale</term>
					<term>base minimale</term>
					<term>forme colonne réduite</term>
					<term>pgcd matriciel</term>
					<term>déterminant</term>
					<term>produit de matrices polynomiales</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the link between the complexity of polynomial matrix multiplication and the complexity of solving other basic linear algebra problems on polynomial matrices. By polynomial matrices we mean n × n matrices of degree d over K[x]  where K is a commutative field. Under the straight-line program model we show that multiplication is reducible to the problem of computing the coefficient of degree d of the determinant. Conversely, we propose algorithms for minimal approximant computation and column reduction that are based on polynomial matrix multiplication; for the determinant, the straightline program we give also relies on matrix product over K[x] and provides an alternative to the determinant algorithm of <ref type="bibr" target="#b15">[16]</ref>. We further show that all these problems can be solved in particular in O˜(n ω d) operations in K. Here the soft "O" notation indicates some missing log(nd) factors and ω is the exponent of matrix multiplication over K.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The link between matrix multiplication and other basic linear algebra problems is well known under the algebraic complexity model. For K a commutative field, we will assume that the product of two n × n matrices over K can be computed in O(n ω ) operations in K. Under the model of computation trees over K, we know that ω is also the exponent of the problems of computing the determinant, the matrix inverse, the rank, the characteristic polynomial (we refer to the survey in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Chap.16]</ref>) or the Frobenius normal form <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. On an algebraic Ram, all these problems can be solved with O˜(n ω ) operations in K, hence the corresponding algorihms are optimal up to logarithmic terms. Here and in the rest of the paper, for any exponent e 1 , O˜(n e1 ) denotes O(n e1 log e2 n) for any exponent e 2 .</p><p>Much less is known for polynomial matrices and even less for integer matrices under the bitcomplexity model. Difficulties come from the size of the data (and from carry propagation in the case of integer arithmetic) which make reductions between problems hard to obtain. In this paper we investigate the case of matrices of degree d in K[x] n×n . This is motivated both by the interest in studying more concrete domains than abstract fields and by the two results <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. Storjohann has established and algorithm of cost O˜(n ω d) for the determinant and the Smith normal form. We have shown that the matrix inverse can be computed by a straight-line program of length O˜(n 3 d) (which is almost the size of the output). Besides, the latter estimate gives an alternative for the determinant in O˜(n ω d) (see §4).</p><p>These two results first ask the following question: are problems on polynomial matrices -and especially the determinant problem -harder than polynomial matrix multiplication? By slightly extending the result of Baur &amp; Strassen <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">Cor. 5]</ref> for matrices over a field, we answer positively for the determinant. We show in section 4 that if there is a straight-line program of length D(n, d) over K which computes the coefficient of degree d of the determinant, then there is a straight-line program of length no more than 8D(n, d) which multiplies two n × n matrices of degree d.</p><p>Conversely, the second question is to know which polynomial matrix problems can be solved with roughly the same number of arithmetic operations than polynomial matrix multiplication. As seen above we already know that this is the case of the determinant problem <ref type="bibr" target="#b15">[16]</ref> on an algebraic Ram using O˜(n ω d) as an estimation of the complexity of matrix multiplication <ref type="bibr" target="#b5">[6]</ref>. We will show in §4 that a different approach, which we have developed independently <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, gives a straight-line program of length O˜(n ω d) for the same problem.</p><p>Before studying the determinant, we first give analogous cost estimates with Ram programs for two other problems: we show in section 2 how to compute minimal bases and order d matrix approximants in O˜(n ω d) operations in K; we show in section 3 that an invertible matrix can be column reduced in time O˜(n ω d) as well. Note that here column reduction is roughly lattice basis reduction for K[x]-modules.</p><p>We further study the complexities of each of the above problems in terms of general cost functions involving polynomial matrix multiplication. To do so, we denote by MM(n, d) the cost of multiplying two matrices of degree d in K[x] n×n (with MM(n) = MM(n, 0)) and assume without loss of generality that n and d are powers of two. As we shall see in section 2.2 our minimal basis algorithm works recursively on the degree and leads us to define the function</p><formula xml:id="formula_0">MM ′ (n, d) = log d i=0 2 i MM(n, 2 -i d).</formula><p>This will be used in sections 2 and 3 for expressing the costs of matrix approximation and column reduction. In the same way, the determinant algorithms of section 4 work recursively on the dimension and we give their complexities in terms of the function</p><formula xml:id="formula_1">MM ′′ (n, d) = log n i=0 2 i MM(2 -i n, 2 i d).</formula><p>For MM(n, d) = Θ(n ω d log d log log d) <ref type="bibr" target="#b5">[6]</ref> these two functions further simplify. We have</p><formula xml:id="formula_2">MM ′ (n, d) = O(MM(n, d) log d) for any value of ω and MM ′′ (n, d) = O(MM(n, d) log n) if ω = 2 and MM ′′ (n, d) = O(MM(n, d)) if ω &gt; 2. Therefore MM ′ (m, d) = O˜(MM(n, d)), MM ′′ (m, d) = O˜(MM(n, d))</formula><p>when taking MM(n, d) = Θ˜(n ω d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Minimal basis computation</head><p>Many problems on matrix polynomials reduce to computing minimal approximant bases (or σbases) <ref type="bibr" target="#b1">[2]</ref>. Given a matrix power series G ∈ K[[x]] m×n and an approximation order d ∈ N, these bases are nonsingular m × m polynomial matrices M such that</p><formula xml:id="formula_3">M G = O(x d ).</formula><p>(</p><p>Minimality is made precise in Definition 2.1 below. It essentially expresses the fact that M has the smallest possible row degrees. Minimal basis computations are motivated by the following two applications, which we shall develop later in the paper: in section 3 we will use the fact that the problem of column reducing a matrix can be solved by computing a Padé approximant and thus a minimal approximant basis; in section 4 we further use such approximants for recovering the polynomial matrix kernels that lead to the determinant. Note that a third application is the computation of minimal matrix polynomials of linearly generated matrix sequences as proposed in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b19">[20]</ref>.</p><p>Our purpose in this section is to introduce polynomial matrix multiplication into the existing approximation algorithms. We achieve this by first adapting in §2.1 the σ-basis algorithm of Beckermann and Labahn <ref type="bibr" target="#b1">[2]</ref> to exploit fast matrix mutiplication over K. Roughly, the algorithm of <ref type="bibr" target="#b1">[2]</ref> works iteratively and computes a σ-basis from a (σ -n)-basis via n Gaussian elimination steps on vectors of K m . How to replace these n steps on m × 1 vectors with a single step on an m × n matrix was unclear. We solve this problem by resorting to the blocking approach of Coppersmith <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> and using the matrix product-based LSP factorization algorithm of <ref type="bibr" target="#b8">[9]</ref>. Polynomial matrix product then arises in §2.2 with a divide-and-conquer version of the method of §2.1 that generalizes the previous studies in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>This divide-and-conquer version yields the cost of O˜(n ω d) which improves upon the cost of</p><formula xml:id="formula_5">O˜(n 3 d) in [2, Theorem 6.2].</formula><p>To define the type of approximants we compute, we consider as in <ref type="bibr" target="#b1">[2]</ref> the formal power series vector</p><formula xml:id="formula_6">f (x) = G(x n )[1, x, . . . , x n-1 ] T ∈ K[[x]] m ;</formula><p>we further call (approximation) order of v T ∈ K[x] m and denote by ord v the integer</p><formula xml:id="formula_7">ord v = sup{τ ∈ N : v(x n )f (x) = O(x τ )}.</formula><p>For σ ∈ N we then define σ-bases with respect to the rows of G as follows.</p><formula xml:id="formula_8">Definition 2.1 A σ-basis of G is a matrix polynomial M in K[x] m×m verifying: i) ord M (i, * ) ≥ σ for 1 ≤ i ≤ m; ii) every v ∈ K[x] m such that ord v ≥ σ admits a unique decomposition v T = m i=1 c (i) M (i, * ) where, for 1 ≤ i ≤ m, c (i) ∈ K[x] and deg c (i) + deg M (i, * ) ≤ deg v (minimality of the approximant).</formula><p>This definition coincides with [2, Definition 3.2, p.809] when the m components of the multiindex in <ref type="bibr" target="#b1">[2]</ref> are the same. Also, since i)</p><formula xml:id="formula_9">yields M (x n )G(x n )[1, x, . . . , x n-1 ] T = O(x σ</formula><p>), it suffices to take σ = nd to get approximant M (x) in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Via matrix multiplication</head><p>To introduce matrix multiplication into σ-basis computations of <ref type="bibr" target="#b1">[2]</ref>, we use the so-called LSP factorization [3, p. 103]: every matrix A ∈ K m×m of rank r can be written as A = LSP where L ∈ K m×m is lower triangular with ones on the diagonal, S ∈ K m×m has m -r zero rows and P ∈ K m×m is a permutation matrix; additionally, the nonzero rows of S form an r × m upper triangular matrix with nonzero diagonal entries. Let</p><formula xml:id="formula_10">1 ≤ i 1 &lt; i 2 &lt; • • • &lt; i r ≤ m</formula><p>be the indices of the nonzero rows of S. Each i j is then uniquely defined as the smallest index such that the first i j rows of A have rank j.</p><p>In Algorithm SigmaBasis below, we assume we compute LSP factorizations with the algorithm of <ref type="bibr" target="#b8">[9]</ref> as described in <ref type="bibr">[3, p.</ref>  </p><formula xml:id="formula_11">d i = x if i ∈ {i 1 , . . . , i r } and d i = 1 otherwise; M := DL -1 πM ; δ := δ + [d 1 (0) -1, . . . , d m (0) -1] T ; od; return M ; Lemma 2.2 Algorithm BlockSigmaBasis is correct. Its cost is O(MM(m)d 2 ) or O(n ω d 2 ) oper- ations in K.</formula><p>Proof. Let M 0 = I m and, for 1 ≤ k ≤ d, write M k (x) for the matrix M computed by step k. We see that the degree of M k in x is no more than k and, assuming the algorithm is correct, that M k G = O(x k ). The computation of ∆ at step k thus costs O(MM(m)k) field operations. This dominates the cost of step k, for both LSP factorization and the update of M require only O(MM(m)) operations in K. The overall complexity then follows.</p><p>To prove the algorithm is correct, note first that M 0 is a 0-basis of G(x). Then, assuming for k ∈ {1, . . . , m} that</p><formula xml:id="formula_12">M k-1 (x) is an n(k -1)-basis of G(x), we verify that M k (x) = D(x)L -1 πM k-1 (x) is an nk-basis of G(x).</formula><p>Let N k-1 (x) = πM k-1 (x) and recall that P is the permutation matrix in the LSP factorization at step k.</p><formula xml:id="formula_13">It follows that N k-1 (x) is an n(k -1)-basis of G(x)P -1 . Algorithm FPHPS of [2, p. 810] with input parameters m, n, F (x) = N k-1 (x n )G(x n )P -1 [1, x, . . . , x n-1 ]</formula><p>T and (0, . . . , 0) ∈ N m then returns an nk-basis of G(x)P -1 after n steps. We denote this basis by N k (x). (Uniqueness of the output of FPHPS is explained in <ref type="bibr">[2, p. 818</ref>].) As shown below, the two bases are related as</p><formula xml:id="formula_14">N k (x) = D(x)L -1 N k-1 (x)<label>(2)</label></formula><p>and hence M k = N k is an nk-basis of GP -1 and G as well.</p><p>We now prove identity <ref type="bibr" target="#b1">(2)</ref>. Let Λ = ∆P -1 and let Λ j be the jth column of Λ. Then</p><formula xml:id="formula_15">x -(k-1)n F (x) ≡ Λ 1 + xΛ 2 + • • • + x n-1 Λ n mod x n .</formula><p>Since the rows of N k-1 have been sorted by permutation π, the first step of FPHPS simply consists in picking the first nonzero entry of Λ 1 -say, λ 1 with row index h 1 -and zeroing the lower entries of Λ 1 by using pivot λ 1 . The h 1 st row is then multiplied by x. In other words,</p><formula xml:id="formula_16">N k-1 (x) is transformed into E 1 (x)T 1 N k-1 (x) where we define E 1 (x) = diag(I h1-1 , x, I m-h1</formula><p>) and</p><formula xml:id="formula_17">T 1 =   I h1-1 1 t 1 I m-h1   with t 1 ∈ K m-h1 .<label>(3)</label></formula><p>Recalling that i 1 is the index of the first nonzero row of S in factorization Λ = LS, we verify first that h 1 = i 1 : it follows from Appendix A that L and S can be partitioned as</p><formula xml:id="formula_18">Λ = LS =   I i1-1 1 l 1 L ′     λ 1 s T 1 S ′   , λ 1 ∈ K\{0}.</formula><p>Here 1) and λ 1 is indeed the first nonzero entry of Λ 1 .</p><formula xml:id="formula_19">L ′ ∈ K (m-i1)×(m-i1) , S ′ ∈ K (m-i1)×(n-</formula><p>Hence</p><formula xml:id="formula_20">h 1 = i 1 . Second, comparing the first column in both members of T 1 Λ = T 1 LS yields t 1 = -l 1 and the i 1 st column of T -1</formula><p>1 is thus equal to the i 1 st column of L. The first step of FPHPS yields eventually</p><formula xml:id="formula_21">x -(k-1)n E 1 (x n )T 1 F (x) ≡ xΛ ′ 2 + • • • + x n-1 Λ ′ n mod x n where [0|Λ ′ 2 | • • • |Λ ′ n ] = E 1 (0)T 1 Λ = 0 0 0 L ′ S ′ ∈ K m×n . (<label>4</label></formula><formula xml:id="formula_22">)</formula><p>Let h 2 be the pivot index at step 2 and let T 2 and E 2 (x) be the associated transformation matrices. It follows from (4) that h 2 &gt; i 1 . Hence T 2 has the form T 2 = diag(I i1 , T ′ 2 ) and E 1 (x) commutes with T 2 . Then, noticing that the ordering imposed by π is still the same, one can iterate by replacing T 1 , LS and i</p><formula xml:id="formula_23">1 &lt; • • • &lt; i r with respectively T ′ 2 , L ′ S ′ and i 2 -i 1 &lt; • • • &lt; i r -i 1 .</formula><p>We eventually get h j = i j for 1 ≤ j ≤ r. Therefore, defining for 1 ≤ j ≤ r matrices E j (x) and T j has done in (3) for j = 1, we have</p><formula xml:id="formula_24">N k (x) = E r (x) • • • E 2 (x)E 1 (x)T r • • • T 2 T 1 N k-1 (x). (<label>5</label></formula><formula xml:id="formula_25">) It follows that E r (x) • • • E 2 (x)E 1 (x) = D(x)</formula><p>and that the i j th column of T -1 j equals the i j th column of L. Noticing further that because of the structure of T j the i j th column of T -1 equals the i j th column of T -1 j , we have T -1 = L and (2) follows from (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Via polynomial matrix multiplication</head><p>To use polynomial matrix multiplication, we now give a divide-and-conquer version of Algorithm SigmaBasis. This version is based on the following "transitivity lemma", which may be seen as the counterpart of Theorem 6.1 in <ref type="bibr" target="#b1">[2]</ref> and can be shown in the same way.</p><formula xml:id="formula_26">Lemma 2.3 If M, M ′ , M ′′ are the output of Algorithm SigmaBasis for input (G, d), (G, d/2), (x -d/2 M ′ G, d/2) respectively then M = M ′′ M ′ . Algorithm BlockSigmaBasis(G, d) Input: G ∈ K[[x]] m×n with m ≥ n and d ∈ N. Output: a σ-basis M ∈ K[x] m×m with σ = nd. Condition: d = 0 or log d ∈ N. if d = 0 then M := I m ; else if d = 1 then M := SigmaBasis(G,d); else if d ≥ 2 then M ′ := BlockSigmaBasis(G, d/2); M ′′ := BlockSigmaBasis(x -d/2 M ′ G mod x d/2 , d/2); M := M ′′ M ′ ; fi; return M ; Theorem 2.4 Algorithm BlockSigmaBasis is correct. Its cost is 1.5MM ′ (m, d) + O(dMM(m)) or O˜(m ω d) operations in K.</formula><p>Proof. For correctness it suffices to show that the algorithm with input (G, d) uses only the first d coefficients of series G: when d = 1 this is true because of Algorithm SigmaBasis; if we assume this is true for a given d/2 then this is still true for d since x -d/2 M ′ G mod x d/2 depends only on G mod x d . Correctness then follows immediately from Lemma 2.3. Now about complexity. First, it follows from Algorithm SigmaBasis that deg </p><formula xml:id="formula_27">M ≤ d. Hence the product M ′′ M ′ costs MM(m, d/2). Second, since deg M ′ ≤ d/2, the coefficient in x i of x -d/2 M ′ G mod x d/2 is the coefficient in x i+d/2 of M ′ (G mod x d ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Column reduction</head><p>For A ∈ K[x] n×n we consider the problem of computing C ∈ K[x] n×n such that C = AU is column reduced, U being a unimodular matrix over K[x]. Column reduction is essentially lattice basis reduction for K[x]-modules. To define the reduction, let d j denote the degree of the jth column of C. The corresponding coefficient vector of x dj is the jth leading vector of C. We let [C] l be the matrix of these leading vectors.</p><formula xml:id="formula_28">Definition 3.1 A matrix C is column reduced if its leading coefficient matrix satisfies rank [C] l = rank C.</formula><p>We refer to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> and the references therein for discussions on previous reduction algorithms and applications of the form especially in linear algebra and in linear control theory. If r is the rank of A, the best previously known cost for reducing A was O(n 2 rd 2 ) operations in K <ref type="bibr" target="#b13">[14]</ref>. Thus in particular O(n 3 d 2 ) for a nonsingular matrix. Here we propose a different approach which takes advantage of fast polynomial matrix multiplication and gives in particular the complexity estimate O˜(n ω d).</p><p>We assume that A of degree d is nonsingular in K[x] n×n . The general case would require further developments. We compute a column reduced form of A by combining our techniques in <ref type="bibr" target="#b21">[22]</ref> to the high-order lifting and the integrality certificate in <ref type="bibr">[16, §9]</ref>. The main idea is to reduce the problem to the computation of a matrix Padé approximant whose side-effect is to normalize the involved matrices <ref type="bibr" target="#b21">[22]</ref>. Let us first recall the definition of right matrix greatest common divisors. Definition 3.2 A right matrix gcd of P ∈ K[x] m×n and A ∈ K[x] n×n is any full row rank matrix G such that</p><formula xml:id="formula_29">U P A = G 0 with U unimodular.</formula><p>Right gcd's are not unique, but if [P T A T ] T has full column rank -here this is true by assumption-then, for given matrices P and A, all the gcd's are nonsingular and left equivalent (up to multiplication by a unimodular matrix on the left) in K[x] n×n . This also leads to the notion of an irreducible matrix fraction description. (See for example <ref type="bibr" target="#b10">[11]</ref> for a detailed study of matrix gcd's and fractions.) Definition 3.3 If a right gcd of P and A is unimodular then we say that P and A are relatively prime and that P A -1 is an irreducible right matrix fraction description.</p><p>The whole algorithm for column reduction will be given in §3.3. The first step, detailed in §3.1, is to compute from A a strictly proper and irreducible right fraction description</p><formula xml:id="formula_30">H = RA -1 ∈ K(x) n×n , R ∈ K[x] n×n .<label>(6)</label></formula><p>We recall that strictly proper means that H tends to zero when x tends to infinity. This implies that the degree of the jth column of R must be strictly lower than the degree of the jth column of A. Since the degrees of R and A are bounded by d, the second step of the method, studied in §3.2, is to compute from the first 2d + 1 terms of the expansion of H a right matrix Padé approximant</p><formula xml:id="formula_31">H = T C -1</formula><p>of H. Such an approximant, obtained from the results of §2 and <ref type="bibr" target="#b1">[2]</ref>, will have the additional property that C is column reduced. We will see that by the equivalence of irreducible fractions, C will be a column reduced form of A.</p><p>Like the algorithms in <ref type="bibr" target="#b15">[16]</ref>, our column reduction algorithm is randomized Las Vegas since the first step requires that det A(0) = 0. Without loss of generality this may be assumed by choosing a random element x 0 in K and by computing a column reduced form C of A(x + x 0 ). Indeed, a column reduced form of A is then recovered as C(x -x 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A strictly proper and irreducible fraction</head><p>For a given A, its inverse A -1 may not be a strictly proper rational function, a case where R = I is not a suitable choice in <ref type="bibr" target="#b5">(6)</ref>. We show that the integrality certificate of <ref type="bibr">[16, §9]</ref> can be used here to find a target strictly proper function.</p><formula xml:id="formula_32">Lemma 3.4 Let A ∈ K[x] n×n of degree d be such that det A(0) = 0. For h &gt; (n -1)d define R ∈ K[x] m×n by I = A -1 mod x h A + x h R.<label>(7)</label></formula><p>The fraction RA -1 is strictly proper and irreducible. If h is the closest power of 2 greater than (n -1)d + 1, the 2d + 1 first terms of the expansion of RA -1 may be computed at the cost of O(MM(n, d) log n) + O˜(n 2 d) operations in K.</p><p>Proof. Identity ( <ref type="formula" target="#formula_32">7</ref>) is identity <ref type="bibr" target="#b11">(12)</ref> in <ref type="bibr" target="#b15">[16]</ref> with B = I and T = A. This is a Euclidean matrix division with coefficients in reverse order. The fraction RA -1 is strictly proper because</p><formula xml:id="formula_33">RA -1 = x -h A -1 -x -h A -1 mod x h<label>(8)</label></formula><p>and h &gt; (n -1)d ≥ deg A * where A * is the adjoint matrix of A. On the other hand, there is a unimodular U such that</p><formula xml:id="formula_34">x h R A = I -(A -1 mod x h ) 0 I I A = U I 0 .</formula><p>Hence matrices x h R and A are relatively prime (see Definition 3.2) and the same is true for R and A.</p><p>For h as in the statement, the 2d + 1 terms of the expansion of RA -1 may be computed by high-order x d -lifting <ref type="bibr">[16, §8]</ref> with input parameters A, I, h and 2d + 1. The corresponding cost in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Proposition 13]</ref> is O˜(n ω d). It can be seen from <ref type="bibr" target="#b15">[16]</ref> that the algorithm actually runs at a cost of O(MM(n, d) log n) + O˜(n 2 d) operations in K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Padé approximation and reduction</head><p>The key observation is that the descriptions T C -1 of H with C a column reduced form of A are those whose numerator and denominator matrices have minimal degrees (see Corollary 3.6 below). By definition they satisfy</p><formula xml:id="formula_35">H -I C T = 0 mod x 2d+1</formula><p>and, as we shall see, their minimality implies that they must appear in any σ-basis of G = [H -I] for σ = n(2d + 1). (Here we consider σ-bases with respect to the columns rather than the rows. Hence we transpose the matrices of section 2.) To describe the set of all matrices T and C we use the notion of minimal basis of a module. </p><formula xml:id="formula_36">For M ∈ K[x] n×m , m &gt; n, with rank n, let N ∈ K[x] m×(m-n) with</formula><formula xml:id="formula_37">′ 1 ≤ d ′ 2 ≤ • • • ≤ d ′ m-n</formula><p>of any other basis of ker M satisfy d ′ j ≥ d j for 1 ≤ j ≤ m -n. We say that the columns of N form a minimal basis of ker M .</p><formula xml:id="formula_38">Corollary 3.6 A basis [C T T T ] T of ker G = ker[H -I] is minimal if and only if C is a column reduced form of A. Proof. If [C T T T ]</formula><p>T is a minimal basis then H = T C -1 must be irreducible, otherwise the simplification of (T, C) by a right matrix gcd would lead to a basis with smaller degrees. The latter would contradicts Theorem 3.5. In addition since [C T T T ] T is column reduced then C is column reduced. Indeed, H being strictly proper implies that T has column degrees strictly lower that those of C which thus dominate. By <ref type="bibr" target="#b10">[11,</ref> we further know that two irreducible descriptions T C -1 and RA -1 of the same function H have equivalent denominators. This means that there exists a unimodular U such that C = AU . Hence C is a column reduced form of A. Conversely, if C in a basis [C T T T ] T is a column reduced form of A then by Theorem 6.5-4 cited above, T C -1 is an irreducible description of H. Since C is column reduced, the non-minimality of [C T T T ] T as a basis of ker G would then contradict its irreducibility.</p><p>We now show that, for σ large enough, a σ-basis with respect to the columns of [H -I] leads to a minimal basis [C T T T ] T as in the corollary, and hence to a column reduced form of A. We follow here the techniques in <ref type="bibr" target="#b9">[10]</ref> for computing a minimal basis of the kernel of a polynomial matrix. Proof. We first show that there may be at most one set of n columns of N of degree at most d. Then the minimality of the σ-basis will imply its existence and the fact that it leads to a fraction description of the form T C -1 .</p><p>If [Q T P T ] T is a set of n columns of N of degrees bounded by d then HQ -P ≡ 0 mod x 2d+1 .</p><p>If A -1 S is a left description of H, defined in the same way as RA -1 in Lemma 3.4, we get SQ -AP ≡ 0 mod x 2d+1 .</p><p>Since every matrix in the latter identity has degree at most d we deduce that</p><formula xml:id="formula_39">SQ -AP = 0.<label>(9)</label></formula><p>It follows from the columns of a σ-basis N being linearly independent over K(x) <ref type="bibr" target="#b1">[2]</ref> that [Q T P T ] T has full column rank. Since <ref type="bibr" target="#b8">(9)</ref> implies that</p><formula xml:id="formula_40">I 0 S -A Q P = Q 0 ,</formula><p>we see that Q is invertible and satisfies</p><formula xml:id="formula_41">P Q -1 = A -1 S = H.<label>(10)</label></formula><p>Another choice [Q T</p><formula xml:id="formula_42">1 P T 1 ] T of n such columns would give H = P Q -1 = P 1 Q -1 1</formula><p>. By [11, Theorem 6.5-4] the two descriptions would verify</p><formula xml:id="formula_43">Q P P 2 = Q 1 P 1</formula><p>and this would contradict the nonsingularity of the σ-basis. Hence the choice [Q T P T ] T must be unique as announced. Let d 1 , . . . , d n be the minimal degrees given by the columns of a minimal description [C T T T ] T and let v 1 , . . . , v n be the corresponding columns. From ii) in Definition 2.1, v 1 can be written as</p><formula xml:id="formula_44">v 1 = 2n j=1 c (j) 1 N j , with deg c (j) 1 + deg N j ≤ d 1</formula><p>where N j is the jth column of the σ-basis N . Thus one column of N has degree bounded by d 1 . Now assume that N has k -1 columns of degrees d 1 , . . . , d k-1 with v k not belonging to the corresponding submodule. As for k = 1, there exists a column of N , linearly independent with respect to the first k -1 chosen ones, of degree bounded by d k . Therefore N contains n distinct columns of degrees bounded by d 1 , . . . d n and, by <ref type="bibr" target="#b9">(10)</ref>, in the kernel of [H -I]. Lemma 3.7 shows in conclusion that these n columns give C, a column reduced form of A, in their first n rows.</p><p>We may notice that the result of the lemma would be true as soon as σ &gt; 2nd for the computation of an approximant of type (d -1, d) as defined in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cost of the reduction</head><p>Our column reduction algorithm can be stated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm ColumnReduction(A)</head><formula xml:id="formula_45">Input: A ∈ K[x] n×n of degree d. Output: C = AU a column reduced form of A. Condition: A is nonsingular.</formula><p>Choice of a random x 0 in K; if det A(x 0 ) = 0 then fail; /* A is probably singular */ B := A(x + x 0 );</p><formula xml:id="formula_46">h := (n -1)d + 1; H := B -1 -(B -1 mod x h ) /x h mod x 2d+1 ; T C -1 := a Padé approximant of H mod x 2d+1 ; return C(x -x 0 );</formula><p>Its complexity follows from Lemma 3.4 concerning the computation of the first terms of H, and from Theorem 2.4 concerning the computation of the n(2d + 1)-basis of Lemma 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.8 A column reduced form of a nonsingular matrix</head><formula xml:id="formula_47">A of degree d in K[x] n×n can be computed by a Las Vegas (certified) algorithm in O(MM ′ (n, d) + MM(n, d) log n) +O˜(n 2 d) or O˜(n ω d) operations in K.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Matrix product &amp; determinant</head><p>The link between matrix multiplication and determinant computation over a the field K is well known. We may refer to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Chap.16]</ref> for a survey of the question. If we have an algorithm for multiplying to matrices with MM(n) operations in K then we have an algorithm (algebraic Ram) for computing the determinant with O(MM(n)) operations in K <ref type="bibr" target="#b3">[4]</ref>. Conversely, the exponents (computation trees) of matrix multiplication and of determinant computation coincide <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1]</ref>. Furthermore, if we have a randomized Monte Carlo algorithm which computes the determinant with D(n) operations in K then we have a Monte Carlo algorithm for multiplying two matrices with O(D(n)) operations in K [8, <ref type="bibr">Theorem 1.3]</ref>.</p><p>In this section we show that similar results hold for polynomial matrices of degree d. In §4.1, using a slight extension of Baur &amp; Strassen's idea <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">Cor. 5]</ref>, we propose a reduction of polynomial matrix multiplication to determinant computation. Then in §4.2, based on the techniques in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref>, we investigate the reverse reduction.</p><p>We use two models of computation, algebraic straight-line programs or algorithms on an algebraic Ram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Polynomial matrix multiplication</head><p>Baur &amp; Strassen <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">Cor. 5]</ref> in conjunction with <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref> have shown that a straight-line program of length D(n) for computing the determinant of a matrix A in K n×n can be transformed into a program of length bounded by O(D(n)) for matrix multiplication. Indeed, the problem of multiplying two matrices can be reduced to matrix inversion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref>. Then matrix inversion is reduced to the problem of computing the determinant by differentiation of the program of length D(n) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>The complexity estimate O(D(n)) for matrix multiplication relies on the computation of the partial derivatives of the determinant as a function in K[a 1,1 , . . . , a i,j , . . . , a n,n ]. The a i,j 's are indeterminates standing for the entries of the input matrix. It was not clear how to extend the result to polynomial matrices. The output of a program of length D(n, d) over K which computes the determinant of a polynomial matrix is a function in K[x, a 1,1 , . . . , a i,j , . . . , a n,n ], that is, a set of functions in K[a 1,1 , . . . , a i,j , . . . , a n,n ]. A straightforward idea could be to differentiate at least d such functions, but it is not known how to do it without increasing the complexity estimate O(D(n, d)).</p><p>Here we remark that having only one particular coefficient of the polynomial matrix determinant is sufficient for recovering the first d + 1 coefficients of the polynomial entries of the adjoint matrix A * = (det A)A -1 ∈ K[x] n×n . Hence we first compute A * modulo x d+1 and from there, the multiplication of two matrices of degree d is easily deduced.</p><p>Let A ∈ K[x] n×n have degree d and denote its (i, j) entry by a i,j = d k=0 a i,j,k x k . Let further a * i,j = nd-d k=0 a * i,j,k x k be the (i, j) entry of the adjoint matrix A * of A and let ∆ = nd l=0 ∆ l x l be the determinant of A. We have the following relation between the partial derivatives of coefficient ∆ l and some of the a * i,j,k 's.</p><p>Lemma 4.1 The partial derivatives of the coefficients of the determinant and the coefficients of the adjoint matrix satisfy</p><formula xml:id="formula_48">a * j,i,l-k = ∂∆ l ∂a i,j,k , 0 ≤ l ≤ nd, 0 ≤ k ≤ d.</formula><p>where, by convention, a * j,i,k = 0 if k &lt; 0 or k &gt; nd -d.</p><p>Proof. By Cramer's rule and since ∂a i,j /∂a i,j,k = x k , we have ∂∆/∂a i,j,k = x k a * j,i . On the other hand, for 1 ≤ k ≤ d the coefficients ∆ 0 , . . . , ∆ k-1 do not depend on variable a i,j,k and thus ∂∆/∂a i,j,k = nd l=k ∂∆ l /∂a i,j,k x l . Therefore </p><formula xml:id="formula_49">a * j,i,d-k = ∂∆ d ∂a i,j,k , 0 ≤ k ≤ d.</formula><p>By computing the partial derivatives <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref> of the given program for the determinant coefficient ∆ d we thus have a program of length bounded by 4D(n, d) for computing A * mod x d+1 . We conclude by applying this result twice to the well known 3n × 3n matrix</p><formula xml:id="formula_50">A =   I n A 1 I n A 2 I n   with A 1 , A 2 ∈ K[x] n×n of degree d.</formula><p>The associated adjoint matrix is the matrix of degree 2d</p><formula xml:id="formula_51">A * =   I n -A 1 A 1 A 2 I n -A 2 I n   .</formula><p>One can thus recover A 1 A 2 mod x d+1 from A * mod x d+1 . To get higher order terms, notice that if </p><formula xml:id="formula_52">A 1 A 2 = x d H + L then H = A 1 A 2 mod x d+1 where M = d i=0 M d-i x i is the "mirror" polynomial matrix of M = d i=0 M i x i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polynomial matrix determinant</head><p>Over K, algorithms for reducing determinant computation to matrix multiplication work recursively in O(log n) steps. Roughly, step i involves n/2 i products of 2 i × 2 i matrices. (See for example <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4]</ref>.) When looking for the determinant of a polynomial matrix, both Storjohann's algorithm <ref type="bibr" target="#b15">[16]</ref> and the straight-line program we derive below from our previous studies in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref> also work in O(log n) steps. They involve polynomial matrices of dimensions 2 i × 2 i and degree nd/2 i (this accounts for the definition of function MM ′′ (n, d) in introduction).</p><p>In this section we study the costs of two different methods for computing the determinant of an n × n polynomial matrix of degree d. These costs are functions of MM ′′ (n, d) and MM ′ (n, d) and reduce both to O˜(n ω d) when taking MM(n, d) = Θ(n ω d log d log log d).</p><p>The first method is Storjohann's high order lifting on an algebraic Ram <ref type="bibr" target="#b15">[16]</ref>. We recall it briefly in §4.2.1 below for the sake of completeness. We then present in §4.2.2 an alternative approach for straight-line programs, which has been developed independently <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>The analysis of this algorithm is similar to the one of the inversion algorithm in <ref type="bibr" target="#b9">[10]</ref> and we simply recall the key point for complexity: although the minimal bases in <ref type="bibr" target="#b10">(11)</ref> can have degrees as large as nd, they generically have degrees equal to d -a property which carries over the next step -and can then be recovered from the rows of any σ-bases of A L , A R with σ ≥ n(2d + 1) [10, <ref type="bibr">Properties 1 &amp; 2]</ref>. Generically, matrices B (j) i,L and B (j) i,R thus have dimensions 2 1-i n × 2 -i n and degree 2 i-1 d; it then follows from Theorem 2.4 that minimal kernel bases U  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we reduced polynomial matrix multiplication to determinant computation and conversely, under the straight-line model. Under the algebraic Ram model, we reduced the tasks of computing a σ-basis and column reduced form to the one of multiplying square polynomial matrices; as we have seen, similar reductions follow from <ref type="bibr" target="#b15">[16]</ref> for the problems of computing the determinant and the Smith normal form. However, in K[x] n×n it is still unclear whether -Hermite and Frobenius normal forms, -associated transformation matrices (even for the column reduced form), -the characteristic polynomial, can be computed in O˜(MM(n, d)) or O˜(n ω d) operations in K as well. Another related question is to know whether the straight-line approach of section 4.2.2 yields a O˜(n 3 d) algorithm for computing the inverse of a polynomial matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>103]: L, S, P are computed in O(MM(m)) operations in K; furthermore, L, S are such that if the ith row of S is identically zero then the ith column of L is the ith unit vector. (See Appendix A.) Algorithm SigmaBasis(G, d) Input: G ∈ K[[x]] m×n with m ≥ n and d ∈ N. Output: a σ-basis M ∈ K[x] m×m with σ = nd. M := I m ; δ := 0 ∈ N m ; for k from 1 to d do δ := πδ where π ∈ K m×m sorts δ in decreasing order; ∆ := x -(k-1) πM G mod x; ∆ := ∆ augmented with m -n zero columns; Compute the LSP factorization of ∆; D := diag(d 1 , . . . , d m ) where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This product costs MM(m, d). The cost C(m, n, d) of Algorithm BlockSigmaBasis thus satisfies C(m, n, 1) = O(MM(m)) and, for d ≥ 2, C(m, n, d) ≤ 2C(m, n, d/2) + MM(m, d/2) + MM(m, d). This gives the bound 1.5MM ′ (m, d) + O(dMM(m)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>columns forming a basis of the K[x]-submodule ker M . We denote by d 1 , d 2 , . . . , d m-n the column degrees of N and assume they are ordered as d 1 ≤ d 2 ≤ • • • ≤ d m-n . Then we have the following theorem and consequence. Theorem 3.5 [11, §6.5.4]. If N is column reduced then the column degrees d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 3 . 7</head><label>37</label><figDesc>Let N ∈ K[x] 2n×2n be a σ-basis with respect to the columns of G = [H -I]. If σ ≥ n(2d + 1), then the n columns of N of degree at most d define an irreducible description T C -1 of H with C a column reduced form of A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 4 . 2</head><label>42</label><figDesc>the result follows by identifying the coefficients. The theorem below is given for a programs over K which compute the particular coefficient ∆ d . It thus remains valid for programs over K which compute the whole determinant in K[x]. If there is a straight-line program of length D(n, d) over K which computes the (d + 1)st coefficient of the determinant of an n × n matrix of degree d, then there is a straight-line program of length no more than 8D(n, d) which multiplies two n × n matrices of degree d. Proof. It follows from Lemma 4.1 with l = d that the first d + 1 coefficients of a * j,i are given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>in O(MM ′ (2 -i n, 2 i d)) operations in K. On the other hand, the cost of matrix updateB := U i B is O(2 i MM(2 p-i , 2 i d)) where MM(n, d) ≤ MM ′ (n, d).Hence the result below, recalling that the cost of computing det A(0) is bounded by O(MM(n)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 4 . 4</head><label>44</label><figDesc>The determinant of an n × n polynomial matrix of degree d can be computed by a straight-line program over K of length O(log n i=1 2 i MM ′ (2 -i n, 2 i d)) or O˜(n ω d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Theorem 1.3] we may state an analogous result for algorithms on an algebraic Ram: if we have a randomized Monte Carlo algorithm which computes ∆ d with D(n, d) operations in K then we have a Monte Carlo algorithm for multiplying two matrices of degree d with O(D(n, d)) operations in K.</figDesc><table /><note><p><p><p>Therefore H and thus H can be recovered from A * mod x d+1 .</p>Following Giesbrecht</p>[8,  </p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Lifting determinant algorithms</head><p>Storjohann has given in <ref type="bibr">[16,</ref> Proposition 24] a Las Vegas algorithm for computing the determinant of a polynomial matrix in O˜(n ω d) operations. Without going into details since there is no change of the method, we point out that Storjohann's result actually gives the following. Theorem 4.3 <ref type="bibr" target="#b15">[16]</ref> The determinant of an n × n polynomial matrix of degree d can be computed by a Las Vegas algorithm in O(MM ′′ (n, d)</p><p>This complexity estimate may be deduced from the lines of <ref type="bibr" target="#b15">[16]</ref>. The term in O(MM ′′ (n, d)) comes from the integrality certificate and the Smith form computations of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Propositions 17 &amp; 22]</ref>. The term in O(MM(n, d) log 2 n) comes from the high-order lifting of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Prop. 13]</ref> performed at each step of the O(log n) steps of the main iteration <ref type="bibr">[16, §13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Straight-line determinant</head><p>Given A ∈ K[x] n×n of degree d and sufficiently generic, the straight-line approach presented in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref> computes the inverse of A as A -1 = B -1 U where U ∈ K[x] n×n and B ∈ K[x] n×n is diagonal of degree nd. One can further recover the determinant of A from B alone as we explain now. By definition of the inverse, U = (det A)</p><p>-1 BA * where A * is the adjoint matrix of A. Generically, deg det</p><p>To compute B in Algorithm Determinant below, we proceed as for Algorithm Inverse in <ref type="bibr" target="#b9">[10]</ref>: we diagonalize the input matrix in log n steps, starting with</p><p>where A L , A R ∈ K[x] n×n/2 and where U , U ∈ K[x] n/2×n are minimal bases of the left kernels of A R , A L respectively. These minimal bases are as in Theorem 3.5, for left kernels.</p><p>Algorithm Determinant(A)</p><p>In Section 2 we assumed we use the LSP algorithm of <ref type="bibr" target="#b8">[9]</ref> as described in <ref type="bibr">[3, p. 103</ref>]. In general, LSP factorization is not unique: for example</p><p>However, with this choice of algorithm, some columns of L are prescribed as follows.</p><p>Property A.1 If the ith row of S is identically zero then the ith column of L is equal to the ith unit vector.</p><p>In <ref type="bibr" target="#b11">(12)</ref> the computed factor L is thus the one with * = 0.</p><p>Proof. We use the notation of the solution to Problem 2.7c in <ref type="bibr">[3, p.</ref> 103] and we assume without loss of generality that m is a power of two. The result is clear for m = 1 since we take L = P = 1 . For m ≥ 2, assume that the result holds for m/2 and recall that in [3, p. 103] factors L, S are computed as</p><p>where L 1 , S 1 = [S ′ 1 , B] and L 2 , S 2 , P 2 stem from LSP factorizations of respective dimensions m/2 × m and m/2 × (m -r) where r ≤ m/2 is the rank of S 1 . (We shall describe G later.) We consider two cases, depending on whether i &gt; m/2 or i ≤ m/2. If i &gt; m/2 then the (i -m/2)th row of S 2 is zero. By assumption, the (i-m/2)th column of L 2 is therefore equal to the (i-m/2)th unity vector and the result follows from the shape of L in <ref type="bibr" target="#b12">(13)</ref>. If i ≤ m/2 then the ith row of S 1 is zero, for S 1 = [S ′ 1 , B] and P 2 is a permutation matrix. Hence, by assumption, the ith column of L 1 is the ith unity vector. To prove that the ith column of G is zero, recall first how G is defined in <ref type="bibr">[3, p. 103</ref></p><p>where F is a matrix of order m/2 whose last m/2 -r columns are zero; additionally S (-1) 1 is a matrix of order m/2 that transforms S 1 into S (-1) 1</p><p>In particular S (-1) 1 contains a row permutation such that the ith row of S 1 corresponds to a row of index greater than r in S (-1) 1 S 1 . The same permutation acts on the columns of F through transformation G = F S (-1) 1 and the ith column of G is therefore set to zero by definition of F .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The complexity of partial derivatives</title>
		<author>
			<persName><forename type="first">W</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A uniform approach for the fast computation of matrix-type Padé approximants</title>
		<author>
			<persName><forename type="first">B</forename><surname>Beckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="804" to="823" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Bini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polynomial and Matrix Computations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1994">1994</date>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
	<note>Fundamental Algorithms. Birkhauser</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Triangular factorization and inversion by fast matrix multiplication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bunch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="231" to="236" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algebraic Complexity Theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bürgisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shokrollahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grundlehren der mathematischen Wissenschaften</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">315</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On fast multiplication of polynomials over arbitrary algebras</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Cantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaltofen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="693" to="701" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving homogeneous linear equations over GF(2) via block Wiedemann algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coppersmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">205</biblScope>
			<biblScope unit="page" from="333" to="350" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nearly optimal algorithms for canonical matrix forms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Giesbrecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A generalization of the fast LUP matrix decomposition algorithm and applications</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Straight-line computation of the polynomial matrix inverse</title>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Jeannerod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Villard</surname></persName>
		</author>
		<idno>2002-47</idno>
		<ptr target="http://www.ens-lyon.fr/LIP/Pub/rr2002.html" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Laboratoire LIP, ENS Lyon, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Linear systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Taylor expansion of the accumulated rounding errors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Linnainmaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix-free linear system solving and applications to symbolic computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comp. Sc., Rensselaer Polytech. Instit</title>
		<imprint>
			<date type="published" when="1995-12">Dec. 1995</date>
			<pubPlace>Troy, New York</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On lattice reduction for polynomial matrices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storjohann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Algorithms for Matrix Canonical Forms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Storjohann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
			<publisher>ETH-Zentrum</publisher>
			<pubPlace>Zurich, Switzerland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institut für Wissenschaftliches Rechnen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-Order Lifting (Extended Abstract)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Storjohann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Symbolic and Algebraic Computation</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002-07">July 2002</date>
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaussian elimination is not optimal</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vermeidung von Divisionen</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Reine Angew. Math</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="182" to="202" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast computation of linear generators for matrix sequences and application to the block Wiedemann algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Thomé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Symbolic and Algebraic Computation</title>
		<meeting><address><addrLine>London, Ontario</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Black box linear algebra with the LinBox library</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<pubPlace>Raleigh, NC USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computation of the inverse and determinant of a matrix (summary by E. Thomé)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Villard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INRIA</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Chyzak</surname></persName>
		</editor>
		<meeting><address><addrLine>Rocquencourt, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">may 2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computing Popov and Hermite forms of polynomial matrices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Villard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Symbolic and Algebraic Computation</title>
		<meeting><address><addrLine>Zurich, Suisse</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996-07">July 1996</date>
			<biblScope unit="page" from="250" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Further analysis of Coppersmith&apos;s block Wiedemann algorithm for the solution of sparse linear systems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Villard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Symbolic and Algebraic Computation</title>
		<meeting><address><addrLine>Maui, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
