<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">User Preferences Prediction Approach based on Embedded Deep Summaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">James</forename><surname>Chambua</surname></persName>
							<email>jchambua@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Dar-es-Salaam</orgName>
								<address>
									<postBox>P.O. Box 33335</postBox>
									<settlement>Dar-es-Salaam</settlement>
									<country key="TZ">Tanzania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Niu</surname></persName>
							<email>zniu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computing and Information</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Zhu</surname></persName>
							<email>zhuyifan@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>C E P T E D M A N U S C R I</surname></persName>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">User Preferences Prediction Approach based on Embedded Deep Summaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.eswa.2019.04.047</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>User preference prediction</term>
					<term>Text summarization</term>
					<term>Deep learning embedding Yifan Zhu -Conceptualization</term>
					<term>Data Curation</term>
					<term>Investigation</term>
					<term>Methodology</term>
					<term>Software</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Some existing preference prediction methods have utilized users' review texts to learn additional knowledge to support the prediction task. Such methods determine and represent users' preference knowledge by conducting user sentiment, aspect sentiment and topic analysis as recognized in the review texts. However, the discovered item topics from topic-based methods may not fit the preferences of most users while the discovered users' opinions and aspects' sentiments from sentiment based methods may not reflect each user's opinion. This paper proposes a hybrid approach to learn and represent users' preference knowledge from review texts and utilize the acquired representation to support rating prediction. Our approach assumes that user preferences are affected by relevant item aspects and majority preference which can be captured through proper summarization and representation of users' review texts. Thus, two deep learning practices are established: the recurrent neural network -Long Short-Term Memory (RNN-LSTM) architecture to learn users' preference knowledge along with item aspects which influence preferences and the Doc2Vec algorithm to convert the acquired knowledge to a suitable representation. The approach extends probabilistic matrix factorization (PMF) model by strengthening its latent factors predictions with the acquired preference knowledge which is used to regulate the predictions. Our experiments on the Amazon products dataset have revealed the capability of learning a suitable representation for users' preference knowledge and its impact on rating prediction as our proposed approach beats alternative methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p> A hybrid approach to learn and represent users' preference knowledge.  Users' preference knowledge representation supplements ratings prediction.  Extending latent factor model to include acquired preference knowledge.  Experiments on Amazon products datasets shows performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Users' preference prediction approaches establish a way of forecasting user's future preferences which may be considered as a central aspect in determining and recommending most preferred items. Predicting users' ratings on new or unrated items is the main task of preference prediction approaches which allows recommendation methods to support users in dealing with the information overload problem <ref type="bibr" target="#b48">(Tarus, Niu, &amp; Mustafa, 2018)</ref>. Content-based (CB) and collaborative filtering (CF) approaches have been utilized for the preference prediction task, however, such approaches are still suffering from data sparsity along with cold-start challenges <ref type="bibr" target="#b35">(Pan, 2016)</ref>.</p><p>The cold start dilemma raises when a prediction model has limited facts relating to a user or an item to accurately learn user preferences when it encounters a new user/item <ref type="bibr" target="#b25">(Ling, Lyu, &amp; King, 2014)</ref>. <ref type="bibr" target="#b26">McAuley &amp; Leskovec, (2013)</ref> exposes the severity of the cold-start problem as illustrated with the Amazon products dataset they used in which over 80% of the items had few ratings (less than 10) while over 70% of items had review texts at length (over 30 words). In contrast, the data sparsity case emerges when most users have hardly provided any ratings for items and most items have been rated only by a handful of users, hence the matrix comprising observed ratings contains lots of unknown/missing ratings <ref type="bibr" target="#b27">(Miha, Dunja, Blaz, &amp; Grobelnik, 2005)</ref>. With such a situation, preference prediction methods which rely on numerical ratings only may produce inaccurate results because they may fail to find correct list of utmost similar users or items.</p><p>To resolve the sparsity and information limitation issues faced by prediction methods, researchers have considered utilizing additional information for instance review texts to acquire more knowledge about users or items <ref type="bibr">(Tarus, Niu, &amp; Yousif, 2017)</ref>. Some existing methods learn user sentiments <ref type="bibr" target="#b57">(Yousif, Niu, Tarus, &amp; Ahmad, 2017)</ref>, aspect sentiments and item topics to expand models' knowledge regarding user preferences <ref type="bibr" target="#b52">(Wan &amp; Niu, 2018)</ref>, <ref type="bibr" target="#b47">(Tarus, Niu, &amp; Kalui, 2018)</ref>, <ref type="bibr" target="#b51">(Wan &amp; Niu, 2016)</ref>. Hybrid methods have also been developed in order to learn user interests and improve personalized services in different domains such as e-learning <ref type="bibr" target="#b53">(Wan &amp; Niu, 2019)</ref>, <ref type="bibr" target="#b5">(Chen, Niu, Zhao, &amp; Li, 2014)</ref> and digital library <ref type="bibr" target="#b42">(Song, Niu, Song, Yu, &amp; Shi, 2004)</ref>. Nevertheless, the discovered users' and aspects sentiments may not equally represent user's opinion individually while the discovered topics may possibly be unfit for most users' preferences. Hence, it's important to devise an approach that would have a deeper comprehension of user's preferences along with item aspects which influence preferences. Such an approach should be competent in constructing a full contextual representation of users' preference knowledge which supports prediction of missing ratings.</p><p>Our proposed approach in this study establishes a way of examining users' review texts so that better learning and effective representation of users' preference knowledge is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T accomplished. The knowledge collection and representation tasks involve deep learning practices <ref type="bibr" target="#b22">(Lecun, Bengio, &amp; Hinton, 2015)</ref> which facilitate computations consisting of numerous regulating layers. Such techniques support learning of complex representations from texts with several levels of generalization different from machine learning approaches whose competence in data management is limited <ref type="bibr" target="#b22">(Lecun et al., 2015)</ref>. Therefore, the engaged deep learning practices ensure that review texts investigation provide more insights into the reasons behind users' preferences and more awareness of item features they consider relevant <ref type="bibr" target="#b58">(Zhang et al., 2014)</ref>. The study by <ref type="bibr" target="#b34">Palangi et al., (2016)</ref> proposed a deep model employing recurrent neural networklong short-term memory (RNN-LSTM) architecture to imitate long range context information between user clicks statistics for web content retrieval task and <ref type="bibr" target="#b46">Tai, Socher, &amp; Manning, (2015)</ref> also introduced Tree-LSTM to predict semantic connection among documents and to perform sentiment classification. <ref type="bibr" target="#b56">Yousif, Niu, Chambua, &amp; Khan, (2019)</ref> developed a multitask learning model which combined convolution neural network (CNN) and Bi-LSTM to analyze citation sentiment and purpose. Their proposed model learns word order information and word-word relationships from citation sentences in order to capture opinionated words that can be used to identify citation sentiment and purpose.</p><p>The basic assumption of this study is that user preferences are affected by relevant item aspects and majority user preference. Hence, the proposed approach acquires such knowledge through summarizing all review texts towards a particular item by implementing the LSTM architecture. The architecture ensures that the generated item summaries capture rich dependencies among users, items and item aspects. The proposed approach also implements Doc2Vec algorithm for paragraph vector embedding which facilitates conversion of item summaries and each user's review texts into a numerical representation (vectors) while preserving their crucial meanings. Finally, the approach computes vector similarity between item summaries vectors and user review texts vectors to generate a similarity score which will be used to regulate predictions of the probabilistic matrix factorization (PMF) model.</p><p>Considering the following review text which highlights sample preference knowledge expressed in review texts as one of the motivating factors behind our approach: "The spicy version of this ketchup isn't bad, but is too expensive. The regular version is sweet as dessert. Horrible. XXX has a much better flavor and costs a lot less. Even YYY is a better deal. I gave it three stars instead of two because it is in glass, not plastic". The review text reveals preference knowledge (taste, price) that influence this particular user preference and furthermore highlights the aspect (packageglass or plastic) considered as relevant regardless of ketchup's taste and price.</p><p>The contribution of this work is: firstly, establishing a hybrid approach to learn and represent users' preference knowledge contained in the review texts. This is accomplished by RNN-LSTM architecture which ensures generation of deep summaries (which capture preference knowledge) for each item. The approach also converts item summaries and users' review texts to paragraph vectors for further analysis. Secondly,</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>extending the PMF approach by strengthening its latent factors predictions with the acquired preference knowledge through prioritization or penalization of the associated predictions. Finally, experimental evaluation on public dataset between our proposed approach and alternative methods is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This section assesses various studies which have used review texts to predict users' preferences while addressing challenges concerning sparsity of the matrix comprising observed ratings along with users/items with both few ratings and with none <ref type="bibr" target="#b43">(Stern, Herbrich, &amp; Graepel, 2009)</ref> which impact prediction performance of models. Our proposed Embedded Deep Summaries (EDS) method conquers the described challenges through considering deep learning -text mining approaches to analyze users review texts as such techniques provide high efficiency and scalability for predictive models which are dealing with vast quantity of data with complex relationships <ref type="bibr" target="#b18">(Indurkhya, 2015)</ref>. Deep learning practices have shown to offer a reliable option for deep networks-based representations of user behaviors and item aspects (X. <ref type="bibr" target="#b16">He et al., 2017)</ref>, <ref type="bibr" target="#b3">(Bengio, Ducharme, Pascal, &amp; Janvin, 2003)</ref>, <ref type="bibr" target="#b2">(Barkan, 2017)</ref>, <ref type="bibr" target="#b29">(Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013)</ref>. Hence, related works provide a synopsis of methods which considers deep learning practices to represent user properties and item aspects as expressed in review texts and underline their distinctive facets from our EDS approach.</p><p>H. <ref type="bibr" target="#b55">Wang, Wang, &amp; Yeung, (2015)</ref> proposed a collaborative deep learning (CDL) model to enrich the hidden depiction recognized by collaborative topic regression (CTR) (C. <ref type="bibr" target="#b54">Wang &amp; Blei, 2011)</ref> in solving the rating sparsity problem. The CDL model utilizes a Bayesian creation model called stacked denoising autoencoder (SDAE) to learn and represent content and then couples the SDAE depiction with collaborative filtering to predict ratings. With this, their CDL model can concurrently extract profound feature depiction from content and learn the connection and inherent relationship concerning items (and users).</p><p>Differently from the CDL model, we have focused on creating users' general preference representation towards items by exploiting deep learning practices to summarize and embed users' review texts towards an item for generating preference summaries vectors and users' review texts vectors. Eventually, our proposed model integrates the learnt deep representations with the numerical ratings by extending the Bayesian PMF model <ref type="bibr" target="#b40">(Salakhutdinov &amp; Mnih, 2008)</ref> to involve the deep learning knowledge in its collaborative filtering.</p><p>The work of <ref type="bibr" target="#b20">Kim, Park, Oh, Lee, &amp; Yu, (2016)</ref> integrated convolutional neural network (CNN) with PMF to learn facts about item description documents to improve predictions. Their model specifically assumed latent models distribution like PMF is not suitable to stipulate the latent model distribution for an item, instead such a distribution is generated from three variables; 1) internal weights of the CCN; 2) item representation from their descriptions; and 3) epsilon which is the Gaussian noise. In a nutshell, as opposed to</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>taking Gaussian prior defined by zero mean on items, they defined the restricted probability of items to be the Gaussian variable (set of item description documents) with mean a document hidden vector attained from the CNN and the Gaussian noise was used as variance, hence integrating CNN and PMF. <ref type="bibr" target="#b41">Seo, Huang, Yang, &amp; Liu, (2017)</ref> also used CNN with native and overall attention to model user preferences along with item features as expressed by review texts. The native attention focused on learning user's preferences or item properties while the overall attention ensured CNN focus regarding semantic meanings of the review texts. Their attention module allowed their model to interpret and visualize its functions and infer informative words linked to a user's rating. Some works have applied word embedding procedures like Word2Vec <ref type="bibr" target="#b29">(Mikolov, Sutskever, et al., 2013)</ref> to learn either vector space word representation or non-textual features to represent item features along with user characteristics for preference prediction. The work by <ref type="bibr" target="#b30">Musto, Semeraro, de Gemmis, &amp; Lops, (2011)</ref> employed Word2Vec to represent words in vector spaces and recognize a linear space depiction of the items and user profiles through summation of words along with document representations as appeared in documents and as enjoyed by users, respectively. Ozsoy, (2016) applied Word2Vec technique using check-ins data to model item and user profiles for recommending venues to visit/check-in. The main task of their model is to generate a list consisting top-k venues/locations (e.g. restaurant, cafe) that the target user is anticipated to visit/check-in. Their model maps the functioning of the Word2Vec paradigm to the fundamental matrix factorization model.</p><p>The difference between our proposed approach and models which employ either CNN or word embedding techniques to learn item and user features is that our approach uses RNN's LSTM architecture to learn, generate and represent general user preference and relevant item aspects which influence preferences. Also models employing convolutional neural networks implies that they applied a bag of words for their encoders hence disregarding the complete arrangement. The LSTM architecture is far more complex than the simple word embedding like latent semantic indexing (LSI) <ref type="bibr" target="#b9">(Deerwester, Dumais, Furnas, Landauer, &amp; Harshman, 1990)</ref>, random indexing (RI) <ref type="bibr" target="#b38">(Sahlgren, 2005)</ref> and Word2Vec. <ref type="bibr" target="#b0">Almahairi, Kastner, Cho, &amp; Courville, (2015)</ref> introduced two approaches to model review texts; language model regularized latent factor model (LMLF) and bag-of-words regularized latent factor model (BoWLF). The former model uses LSTM to implement the recurrent function of RNN to regularize the collaborative filtering matrix factorization using items representations. However, from their experiments they concluded that the way they applied the RNN to standardize the matrix factorization (MF) model did not result to improved performance. The latter model represents each review text as a set of terms and is fused with MF to predict ratings. The work of <ref type="bibr" target="#b1">(Bansal, Belanger, &amp; McCallum, 2016)</ref> also proposed a scheme which directs RNN to acquire an embedding which represents the revealing content carried by the items. Their proposed model implements bidirectional Gated Recurrent Unit (GRU) network to convert text series into</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>hidden vector trained on CF task. The described two models are somehow similar to our approach as they both use recurrent networks to model user review texts or item contents to leverage CF approaches in predicting ratings, the only difference is the operation of RNN to acquire the representations and how to take into account the concluded revealing representations.</p><p>In our previous study <ref type="bibr" target="#b4">(Chambua, Niu, Yousif, &amp; Mbelwa, 2018)</ref>, we considered the linguistic aspects contained in review texts to enhance ratings prediction. We leveraged review texts' semantic and lexical similarity as supplementary knowledge to improve predictions by developing a prediction approach called Review Text Tensor Factorization (RTTF). We conducted tensor factorization of user properties, item features along with review texts linguistic aspects. Even though the semantic and lexical similarity demonstrated significant prediction improvements, a more contextual approach to learn and represent preference knowledge is suggested. <ref type="bibr" target="#b10">Fu, Niu, Zhang, Ma, &amp; Chen, (2016)</ref> utilized asker-answerer (users) information to improve recommending best answers to a question in community inquiry and responding systems by developing BFMC-ARM model. Specifically, they introduced user reputation information for users who have formerly responded to inquiries and computed similarity between askers-answerers. Regardless of the validity of BMFC-ARM approach, it still ignores some abundant knowledge about the user sentiment and question aspects which may influence predictions.</p><p>The proposed approach is governed by the LSTM architecture to capture relevant preference knowledge and then exploit Doc2Vec algorithm to represent the acquired knowledge along with influential item aspects. Some of the considered related works utilize review texts to learn and represent item topics, features and description documents using CNN or Word2Vec (H. <ref type="bibr" target="#b55">Wang et al., 2015)</ref>, <ref type="bibr" target="#b20">(Kim et al., 2016)</ref>, <ref type="bibr" target="#b41">(Seo et al., 2017)</ref> while some other approaches utilized LSTM <ref type="bibr" target="#b0">(Almahairi et al., 2015)</ref> or RNN <ref type="bibr" target="#b1">(Bansal et al., 2016)</ref>. To the best of authors' knowledge, the manner in which our approach learns and represents preference knowledge is unique and our work is the only study which considers preference knowledge as obtained from majority users along with influential item aspects to regulate PMF predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>The idea behind our proposed approach is to learn a good representation of the users' preference knowledge through summarizing review texts of each item to capture majority preference in order to boost prediction performance of the PMF model. Such a representation considers relationships among words and uses this essential knowledge about users and items in the prediction task. Hence, the proposed approach comprises three parts presented in Fig. <ref type="figure">1</ref> (rectangles represent the three processes): firstly, summarizing users review texts to generate preference summaries for each item through RNN-LSTM architecture. When generating summaries, the LSTM architecture considers word order, their semantic relationships, word co-occurrence and text style <ref type="bibr" target="#b14">(Han, Wu, &amp; Niu, 2018)</ref> as well as frequent terms mentioned. Therefore, its assumed that the generated</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T</formula><p>summaries captured semantic relatedness between review texts according to the ability of the LSTM architecture. Secondly, the model embeds the summaries and corresponding users' review texts to generate vector representations which will enable computation of most similar users and items. Finally, it employs a factorization method on past numerical ratings and the computed vector similarities to predict preferences. Thus, preference knowledge representation was applied to enhance model's prediction in terms of accuracy.</p><p>The proposed approach which is illustrated in figure <ref type="figure">1</ref> starts by collecting review texts for each item and implements RNN-LSTM architecture to summarize the texts. The assumption behind is that the summary will represent feelings of majority users towards an item as they have expressed in the review texts. Then the generated item summaries and each user review text towards an item are converted to vector representations using the Doc2Vec algorithm. The conversion allows different vector manipulation and further analysis on the review texts content represented by real valued vectors. The proposed approach then compares the obtained vectors for each user's review text with the corresponding item summary vector to generate a vector similarity score of a particular user towards a specific item. The comparison provides an implicit score of user's preference towards an item compared to other users who have reviewed the same item. Finally, the computed similarity scores are used to regulate predictions of the PMF model by favoring predictions of users with high similarity score and penalizing those of users with low scores. This means that collaborative filtering in the PMF model is set to have more influence (set high weight) when predicting ratings for users with high similarity scores as they have very similar preference (review text vectors) with majority users. Otherwise, the prediction influence from collaborative users is set to low as the target user's similarity score is low. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Review Texts Summarization</head><p>The first phase of the proposed approach focuses on generating review texts summaries for each item, which capture general user preferences and learn important user features or item aspects which influence preference. The model implements LSTM architecture to find and exploit long term dependencies among review texts. This subsection provides details of the review texts summarization in three steps; 1) definition of review texts summarization task; 2) description of how RNN summarizes text; 3) LSTM structural design for review texts summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Summarization Task</head><p>Methods to summarize texts can be regarded as either extractive or abstractive.</p><p>Extractive techniques rely on mining features from the source sentences and paragraphs, subject them into classification models to decide whether input sentences are nominated to be in the summary <ref type="bibr" target="#b19">(Khatri, Singh, &amp; Parikh, 2018)</ref>. With abstractive techniques, the generated summary is not a merely choice of several phrases, sentences and paragraphs from the original document, but a compact rephrasing of the key subjects of the document, through terminologies which are not observed in the source document <ref type="bibr" target="#b31">(Nallapati, Zhou, Santos, Gulcehre, &amp; Xiang, 2016)</ref>. Abstractive summarization involves profound consideration and perception of the source documents to identify the obvious or implied meaning of each word, phrase, sentence and paragraph and make interpretations about their properties so that new sentences are generated which make up a summary <ref type="bibr" target="#b37">(Rossiello, 2016)</ref>.</p><p>The proposed approach considers users' review texts as a sequence to learn and represent general user preference through abstractive summarization. Hence, considering an input sequence * + of review texts towards a particular item. Then, the intention is to generate a substantial series * + which summarizes the input review texts. The central conception is to generate a diversity of the source document which is smaller in size, consisting few phrases or sentences which captures salient thoughts of all reviews towards an item while and preserving the meaning and the vital contents of the source text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Recurrent Neural Network</head><p>A recurrent neural network (RNN) is formed through extending basic neural network which can cope with input series of varying sizes and process them using several neurons from numerous concealed layers and yield some results <ref type="bibr" target="#b24">(Lin et al., 2018)</ref>. It handles variable-length series by having a recurrent concealed condition which can be activated at different times by controlling the preceding time <ref type="bibr" target="#b6">(Chung, Gulcehre, Cho, &amp; Bengio, 2014)</ref>. RNNs form a very powerful and expressive class of models for constructing text series <ref type="bibr" target="#b44">(Sutskever, Martens, &amp; Hinton, 2011)</ref>. This phase models RNN through their inner The following is a specification of RNN for review texts summarization task: given an input sequence ( ) of review texts towards a particular item, which is passed through connected hidden layers which compute series of concealed conditions ( ) and then produce an output sequence ( ) by iterating the following equations from and :</p><formula xml:id="formula_4">( ) ( ) ( ) ( )</formula><p>Where are weight matrices as follows: and are input-to-hidden weight matrix for 1 st and n th hidden layers, respectively, and are the hidden-tohidden (recurrent) weight matrix at first and at n th hidden layers. The represents bias vectors at different hidden layers and refers to the function for hidden layers.</p><p>The proposed approach uses the source sequence to compute a hidden state representation, with which it uses to compute the target sequence as follows:</p><formula xml:id="formula_5">̂ ∑ ( ) ( ̂ ) ( )</formula><p>Where is output bias vector, and is the function for the output layer.</p><p>The focus of proposed summarization is to compute the conditional probability distribution:</p><formula xml:id="formula_6">( ) ∏ ( ) ( )</formula><p>Where the product represents softmax results of the desired terminologies.</p><p>One of the drawbacks of RNN is that its ability to model long-range structure is very low and also spends eternity to determine how to persist information about past inputs through long periods via recurrent regressive transmission because of inadequate decomposition faults drift <ref type="bibr" target="#b17">(Hochreiter &amp; Schmidhuber, 1997)</ref>. The issue with such setting is that networks' predictions are only centred on recent few inputs which were themselves predicted by the network, hence it's difficult for the RNN to make any progress from preceding mistakes. The LSTM architecture <ref type="bibr" target="#b12">(Gers &amp; Schraudolph, 2002</ref>)</p><formula xml:id="formula_7">A C C E P T E D M A N U S C R I P T</formula><p>is amongst the solution to the drawback as it provides a better memory capable of keeping and accessing information and long dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">LSTM-RNN Architecture</head><p>The hidden layer of the LSTM network architecture comprises a memory block with memory cells and a couple of self-adjustable, reproductive gating components which regulate incoming and output to all cells in the block. The memory cells contain recurrently self-connected direct component known as the "Constant Error Carousel" (CEC) <ref type="bibr" target="#b11">(Gers, Schmidhuber, &amp; Cummins, 2000)</ref>. By recirculating activation and error signals indefinitely the CEC delivers temporary memory storage for prolonged intervals.</p><p>The input, forget, and output gate can be guided to determine, respectively, what facts to accumulate in the memory, for how long, and the right time to access it. Merging memory cells into chunks permits allocation of the similar gates consequently decreasing the quantity of adjustable constraints <ref type="bibr" target="#b12">(Gers &amp; Schraudolph, 2002)</ref>.</p><p>Fig. <ref type="figure">2</ref>, presents typical LSTM architecture whose target is to capture/learn users' preference knowledge (PK) {word meaning, semantic correlation and relevant item features along with long range contextual information} from the input review texts through hidden word and sentence attention levels. The task of the LSTM architecture illustrated in the figure is to predict probability of the next word/term of the input sequence (PK) through computations of its internal/hidden representations. The weights in the hidden layers which are used in computing word probabilities are not predefined by the LSTM architecture but rather are determined/learned from the pattern of the input sequences. This means the obtained probabilities depend on word order (pattern) of the input sequence. For this reason, the learning rate is set to be very low (in this study between 0.1 and 0.0125 as it is halved after every third epoch) to allow the architecture to learn slowly in order to avoid getting similar results.</p><p>Consider prediction of the next word using LSTM architecture for the following text: "The ketchup has a very spicy taste, but is too expensive compared to the regular ketchup which has a sweet ______". The intention is for the network to learn from the dependency of the term "ketchup" to predict one of its aspect "taste". The generated probabilities are the results of learning long-short term dependency within texts. Hence, the architecture is capable of forgetting unnecessary words or remembering important ones by using the sigmoid function described in equation ( <ref type="formula">6</ref>), ( <ref type="formula">7</ref>) and ( <ref type="formula">9</ref>) which outputs 0 or 1 for forgetting or remembering, respectively. After forgetting the unnecessary information, the hidden layers then have to decide and reserve relevant terms for new sequence in the layers. The network will decide whether to update the new information or ignore using equation (10). Finally, the architecture has to decide what output to give depending on the long-term dependency learnt. From our text, the architecture knows it's a term related to ketchup and will figure out that it's the word taste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: Long Short-Term Memory Architecture</head><p>The proposed LSTM architecture for the purpose of review texts summarization implements the hidden layer function in equation ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>) with composite function as described by <ref type="bibr" target="#b13">(Graves, 2013)</ref>. Every unit of the LSTM keeps a memory for time different from RNN which calculates a total which is weighted for the source texts <ref type="bibr" target="#b6">(Chung et al., 2014)</ref>. The activation and output of the architecture are calculated as follows:</p><formula xml:id="formula_8">( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )</formula><p>Where represents equation of the logistic sigmoid, and represent the input, forget, output gates, and cell input initiation vectors, respectively, same size as hidden vector . Respective weight matrices are represented by while are the corresponding bias terms.</p><p>The proposed model handles the summarization task by building decoder-encoder model using LSTM with tiered encoders and it incorporates attention into the encoder to determine string and phrase level consideration. Hence, the LSTM comprises of two mechanisms: 1) an converter which modifies the input series into a concealed condition representation 2) an interpreter which uses the concealed condition to generate the objective series .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Review Texts Embedding</head><p>To ensure correct identification and modeling of users' preferences as expressed in the review texts, we consider prediction based paragraph vector <ref type="bibr" target="#b8">(Dai, Olah, &amp; Le, 2015)</ref> embedding technique using the Doc2Vec algorithm. The algorithm implements a neural network which maps words to their objective variables by determining weights which represents word vectors. Hence, all users' review texts for a particular item will be considered as separate documents together with the generated preference summary from subsection 3.1, for document embedding to generate respective document vectors. The considered separate documents are then transformed into vector representation by using Doc2Vec algorithm <ref type="bibr" target="#b21">(Le &amp; Mikolov, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Doc2Vec</head><p>Algorithm for Review Texts Embedding <ref type="bibr" target="#b21">(Le &amp; Mikolov, 2014)</ref> proposed two paragraph vectors models with dispersed memory (DM) and dispersed bag of words (DBOW), respectively, which are unsupervised frameworks that determine continuous distribution vector depictions for text portions.</p><p>They have extended word models <ref type="bibr">(Mikolov, Chen, Corrado, &amp; Dean, 2013)</ref> to go beyond string level in order to attain sentence and paragraph representations as such models determine vector depictions of terms using neural networks likewise. The elementary concept behind the word models is either to predict context probability given a word or to predict the following term given a series of training words (context words). As with word vectors techniques where words are involved in predicting the subsequent word in a sentence, with paragraph vector technique, paragraphs are also involved in predicting the following word given rich contexts and word semantics appraised from the paragraph. The paragraph vector framework <ref type="bibr" target="#b21">(Le &amp; Mikolov, 2014)</ref> maps each word and each paragraph to a distinctive vector (as presented in fig. <ref type="figure">3</ref>), represented by column matrices and , respectively. Subsequent words in sentences are predicted through concatenating or taking average of word and paragraph vectors. Hence, given an input sequence of words from users' review texts the intention is to exhaust the possibilities of the mean log probability:</p><formula xml:id="formula_9">∑ ( ) ( ) With softmax multi-classifier prediction is done through ( ) ∑</formula><p>Where each is un-normalized log probability for each output word computed with:</p><formula xml:id="formula_10">( ) ( )</formula><p>Where , are the softmax parameters and is constructed from and . For each separate single document of users review texts and item preference summary, the input vector (sentences/paragraphs) will be processed through hidden layer activation calculations to obtain the desired output. After training, representation for the involved vectors of word and paragraph is done using the weights between the two layers (input and hidden).</p><p>One advantage of Doc2vec algorithm is that it provides aggregated semantics for a single word. For instance, words such as jam, season and novel, each will have two vector representations for them as follows jam the noun, a food product as in fruit jam; or jam the verb as in traffic jam; season the noun referring to periods of the year (winter, summer, spring or fall) and season the verb e.g. for food seasoning; and novel the noun as in a story book and novel the adjective e.g. this novel (new) recipe. Such example terms come from the grocery and gourmet food dataset, defined in subsection 4.1. Making such distinction is very important for the model's overall performance in learning users preferences from the review texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Computing Most Similar Vectors</head><p>After training the Doc2Vec algorithm described in subsection 3.2.1 the following two vectors are generated: 1) a word vector for each word; 2) a document vector for each document (for each review text and item summary). The created vectors have same dimension and the training tactic is to ensure that the vectors consists as much information from the documents as possible. The basic assumption behind the Doc2Vec</p><formula xml:id="formula_11">A C C E P T E D M A N U S C R I P T</formula><p>algorithm is that words that appear in analogous perspective tend to have related implications <ref type="bibr" target="#b50">(Turney &amp; Pantel, 2010)</ref>. This assumption implies that the rationality of a word is determined from its surrounding words in the sentences and represented in a vector of real values. Since the Doc2Vec procedure is trained with huge amount of words (review texts) then its distributed representations of the text (Embeddings) consist of real valued vectors associated to words with a lot of contextual and semantic information.</p><p>From such vectors, computation of vector similarity can be done to evaluate how review texts are semantically related.</p><p>The proposed model employs cosine similarity <ref type="bibr" target="#b23">(Levy &amp; Goldberg, 2014)</ref> between each embedded user review text towards a particular item and corresponding embedded item summary. More formally, let paragraph vector represent user's review text towards item whose embedded summary is represented by paragraph vector , then similarity concerning the vectors can be computed by:</p><formula xml:id="formula_12">( ) ( ) ‖ ‖‖ ‖ ( )</formula><p>Where is the similarity score between th user review text towards th item and deep summary of item .</p><p>From the vectors similarity computation, high similarity scores ( very close to 1) imply that the two vectors (the user's review text and general item preference) share very close meaning and are semantically related. Otherwise, low similarity scores imply that the users' review texts differ from item summaries in terms of meaning and semantic relatedness. The influence of such users with low similarity scores will be penalized to have low impact in rating predictions while high similarity scores will be prioritized to have high impact on predictions as described in the following subsection with equation (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Factorizing Embedded Review Texts with Numerical Ratings</head><p>Our proposed approach extends the PMF model <ref type="bibr" target="#b39">(Salakhutdinov &amp; Mnih, 2007)</ref> by involving the embedded vectors similarity scores from subsection 3.2.2, in order to reinforce its predictions. The concept of the PMF model is that user preferences can be governed by several concealed aspects of the available past numerical ratings. Hence, this subsection explains how we introduced the embedded vector similarities to reinforce the latent factors and provide Bayesian estimation to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Extending PMF model to include Embedded Vectors Similarities</head><p>Suppose that the observed numerical ratings matrix consists of users and items. Let represent the rating of the th user towards th item. Then the PMF model approximates the matrix ̂ , and V , which represent users and items concealed aspects, respectively, by minimizing summation of the squared difference from the original matrix . The latent factor vectors for the th user and for th</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>item are denoted by and respectively. Hence, The restricted possibility of the observed numerical ratings is defined as follows:</p><formula xml:id="formula_13">( ) ∏ ∏[ ( )] ( )</formula><p>Where ( ) represents the probability function with mean and variance and is an indicator, which equals to 1 if user have rated item , and otherwise equals to 0.</p><p>From the PMF model expressed with equation ( <ref type="formula">14</ref>), we introduce the similarity score generated by vectors similarity computation described in subsection 3.2.2. The score comprises substantial related evidence regarding preferences and relevant item features which influence user behavior. Hence, we modify the conditional probability specified in equation ( <ref type="formula">14</ref>) to take account of the similarity score as a weight which inversely regulate variations among the PMF approximations. This implies that user review texts which have high similarity with the general item preference summary are prioritized to have small variations from the predictions while user review texts with low similarity score are configured to have larger deviations from the PMF model. Formerly, this can be defined with the following equation:</p><formula xml:id="formula_14">( ) ∏ ∏ * ( )+ ( )</formula><p>Learning of the proposed model is performed by exhausting the possibilities of the logsubsequent through the users' aspects and item properties from the latent factor model and from the deep embedding techniques with the hyper-parameters corresponds to minimizing the summation of the squared difference equation with the regularization terms:</p><formula xml:id="formula_15">( ) ∑ ∑ * ( ) + ∑‖ ‖ ∑‖ ‖ ( )</formula><p>Where and and ‖ ‖ denotes the Frobenius norm.</p><p>Approximating user preferences with PMF model requires model training which involves minimization or maximization (depending on the loss/objective function used) of the objective function to estimate hyper-parameters contained in the function. Such parameter estimation which is accomplished by Maximum a-posteriori (MAP) technique causes the model to over-fit because MAP yields discrete approximations of the parameters rather than providing a distribution which ensures the maximum/minimum value of the objective function is achieved <ref type="bibr" target="#b40">(Salakhutdinov &amp; Mnih, 2008)</ref>. The extent to which the model over-fits depends on the discrete values selected for the hyper-</p><formula xml:id="formula_16">A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The prediction capabilities of the models were assessed in terms of ratings prediction on public Amazon Products Datasets (R. <ref type="bibr" target="#b15">He &amp; McAuley, 2016)</ref>. These datasets have been utilized by the compared alternative methods and many other approaches in conducting different experiments. The experiments concentrated on making correct ratings prediction given the fact that the datasets are large and sparse. The statistics of the used datasets (R. <ref type="bibr" target="#b15">He &amp; McAuley, 2016)</ref> are depicted in table <ref type="table" target="#tab_0">1</ref>: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Evaluation metric</head><p>The competency of our approach was evaluated against recent models which employ deep learning practices and/or model review texts in predicting user preferences. The first baseline we considered was the ConvMF+ by <ref type="bibr" target="#b20">Kim et al., (2016)</ref> which incorporates CNN into PMF to capture related evidence of item description documents to further improve correctness of the forecasts as conveyed in the related works. The second baseline considered was the attention (CNN-D-Attn) model proposed by <ref type="bibr" target="#b41">Seo et al., (2017)</ref>, which applies deep learning CNN to exploit user aspects or item properties to predict ratings.</p><p>We also included two other baselines, Language Model regularized Latent Factor (LMLF) and Bag of Words standardized Latent Factor (BoWLF) proposed by <ref type="bibr" target="#b0">Almahairi et al., (2015)</ref>. LMLF uses LSTM to implement the recurring equation of RNN to regularize the collaborative filtering matrix factorization using items representations while BoWLF method characterizes each review text as a collection of terms and is combined with MF to predict ratings. Ratings Meets Reviews (RMR) model by <ref type="bibr" target="#b25">Ling et al., (2014)</ref>, a topic based model was also included in the baselines. The considered baseline models have stated to beat similar alternative methods including CDL as stated by <ref type="bibr" target="#b20">Kim et al., (2016)</ref>, CNN as verified by <ref type="bibr" target="#b41">Seo et al., (2017)</ref>, and HFT and CTR confirmed by <ref type="bibr" target="#b25">Ling et al., (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation</head><p>Prior to the training of the summarization phase of the proposed model, review texts for each item were collected and treated as separate documents whose contexts and semantic relatedness will be composed in the generated summary. The summarization phase was conducted using deep LSTM with 4 layers similar to the work by <ref type="bibr">(Sutskever, Vinyals, &amp;</ref> A C C E P T E D M A N U S C R I P T <ref type="bibr">Le, 2014)</ref>, with 1,000 hidden units and 300 dimension word embedding. The LSTM parameters were initialized with the uniform distribution between -0.1 and 0.1. Model details and parameters settings which showed best results are specified in table 2: Initialized with uniform distribution between -0.1 and 0.1</p><p>For the review texts embedding phase, the proposed model utilized all users' review texts for training by considering each review text as a single separate document. Review texts (documents) consist from few words, single sentence to several sentences or several paragraphs. The documents were tagged with identifications of users and items (user ID and product ID) and associated numerical rating. This was possible as the data comprises anonymous identifiers for users, items and associated numerical ratings along with review texts from the users. The proposed model was trained with 285,644 review texts and 296,331 items, from the six Amazon products datasets.</p><p>The Doc2Vec procedure used for representing documents was instantiated with a dimensionality of 100 for the features vectors, 8 words window between the recent and expected word, a minimum word count was set to 2 and the model used 7 worker threads for training. The review texts summarization and embedding phases were both programmed using python on Genism Python library <ref type="bibr" target="#b36">(Rehurek &amp; Sojka, 2010)</ref>, which is the most vigorous and competent tools for recognizing unmoderated semantic modelling using textual reviews.</p><p>The final phase combined, past numerical ratings and similarity scores between users' review texts and item deep summaries, to forecast unknown ratings trained the PMF model using stochastic gradient descent with a parameter learning degree of 0.001. Other experimental settings in this phase corresponded with the work by <ref type="bibr" target="#b40">(Salakhutdinov &amp; Mnih, 2008)</ref>. Scripts to predict missing ratings were written by means of MATLAB. Experiments of our embedded deep summaries (EDS) approach concentrated on evaluating model's behavior as it attempts to enhance the latent factor model (PMF) with vector representation of review texts. Therefore, our approach examined the capability of review texts vector representations in learning and representing preference knowledge along with utilizing the knowledge to augment forecasting of the latent approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>The performances of various methods in our experiments were evaluated with the mean squared error (MSE), which computes the mean of the squared difference between the observed and predicted ratings. MSE is a well-known metric in the literature, for evaluating model performances. Our experimental results verify our initial assumption that proper modeling of users' review texts can enhance preference prediction. Specifically, the results validate the capability of LSTM-RNN in storing and accessing preference knowledge, in order to generate realistic summaries with accurate representation of users' preferences and relevant item aspects as expressed in the review texts. The results also express the capability of Doc2Vec paragraph vectors in generating numerical representations of the review texts. Such representations consist of the semantic relatedness and context for users and items which are one of the basis behind determination of users' preferences.  The MSE for the evaluated approaches are clarified in figure <ref type="figure" target="#fig_5">4</ref> where variation from one dataset to the next is revealed. The proposed EDS model shows best performance followed by the CNN-D-Attn model in which authors specifically omitted users or items with few ratings as their objective was to deal with interpretable models rather than cold start or data sparsity problem. ConvMF+ and CNN performances were also low compared to proposed model. The remaining models performances are far low compared to the proposed EDS approach. The obtained experimental results suggest that deep learning techniques allow efficient learning and representation of preference knowledge by summarizing users review texts towards items and converting them to paragraph vectors. The LSTM-RNN architecture allowed generation of useful summaries for each item's review texts while the Doc2Vec algorithm successfully converted the summaries to vector representations which were used to enhance ratings prediction.</p><p>Figure <ref type="figure" target="#fig_6">5</ref>, presents users' review texts embedding for Office Products and Gourmet Foods datasets after reducing the vectors to 2-dimesions. As seen in the figure, embeddings visualizations for most users' review texts are very similar which implies that they share similar meaning and are semantically related. This corresponds to this work's assumption of generating item summaries to represent general user preference as well as Doc2Vec assumption that words that occur in analogous context tend to have related implications, which further translates that relevant item aspects mentioned within review texts will also be involved in the embeddings.  The execution particulars of the EDS approach which are stipulated in subsection 4.3 in terms of numerous parameters such as feature vector size, window words and the number of latent factors used were set to be very similar to alternative models settings. Specifically, we tuned some parameters of the Doc2Vec algorithm to address the sparseness of the rating matrix and to ensure that neither user nor item information along with semantic correlation between them is lost. The least word count (min_count) and window parameters were set to 2 and 8, respectively. The interpretation of the former parameter is that the Doc2Vec model will ignore all words with a total frequency lower than the value of the min_count while the latter parameter is the amount of terms to be considered between the recent and expected word in a sentence. Accordingly, the window value signifies how many words to use to determine the semantic correlation between words.</p><p>The effect of varying feature vectors size on prediction capability of the EDS is revealed in Figures 6 (a) to (c), with the corresponding change of the MSE. The deviation implies that the dimension of review texts documents influences how much preference knowledge is learnt and represented by the generated vectors. Best results are realized with feature vector of size 40 for Musical Instruments and Office Products datasets while for Amazon Instant Video is at 60 size feature Vector. The MSE slightly increases from such points and at feature vector of size 100, it drops and remains relatively constant for higher feature vectors (in the range of 100 -300) for all datasets. Altering other remaining parameters does not result into significant change of the MSE of the proposed EDS approach. Nevertheless, the contribution of such parameters is included within the feature vectors. Vectors visualization projected in figure <ref type="figure" target="#fig_5">4</ref> verifies that review texts which are very similar are closely clustered. The visualization, MSE results and the effect of varying vector dimensions validate that correct understanding and representation of users' preference knowledge improves ratings prediction. Hence, review texts vector similarity has proved its effectiveness in strengthening latent factor approaches such as PMF in finding most similar users or items during prediction of unknown/missing ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The main objective of the proposed approach was to learn how to represent preference knowledge as perceived from users' review texts and utilize such knowledge to enhance rating prediction. To achieve this, we evaluated user preferences towards each item as it was assumed that such preferences are affected by majority user preference. Hence, the proposed model implemented RNN-LSTM architecture and Doc2Vec algorithm to summarize and embed review texts, respectively. The generated summaries are assumed to represent majority preference within review texts of particular item. Even though the associated preference knowledge representation mechanisms are not easily interpreted (less human-interpretable), they still provide promising results as far as preference prediction is concerned.</p><p>Preference prediction approaches generally attempt to predict future preferences of users based on their past experience towards items. In this study, we utilized users review texts to learn appropriate knowledge which portrays user preferences as well as relevant item aspects which influence such preferences. Experimental results verified that the LSTM architecture correctly captured general user preference towards items and the Doc2Vec algorithm facilitated converting the item summaries and users' review texts to numerical representation so as to enable vector similarity computation. The computed vector similarities were subsequently used to find most similar users/items by regulating probabilistic matrix factorization predictions of unknown/missing ratings.</p><p>Experimental results of the proposed approach displayed significant improvements compared to the results from the evaluated topic based models, sentiment based methods as well as recent deep learning practices for rating prediction. The results suggest that the LSTM architecture efficiently learns preference knowledge from review texts through summarization. They also suggest that the generated paragraph vectors for users' review texts and item summaries correctly represent preference knowledge. Future work will examine alternative efficient techniques of learning and representing user preferences to further improve models' performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig. 1: Proposed Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig. 3: Paragraph vector framework for review texts represantation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: MSE of the Evaluated Models on Different Amazon Products</figDesc><graphic url="image-49.png" coords="22,75.45,99.87,300.41,281.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 ,</head><label>5</label><figDesc>Figure5, presents users' review texts embedding for Office Products and Gourmet Foods datasets after reducing the vectors to 2-dimesions. As seen in the figure, embeddings visualizations for most users' review texts are very similar which implies that they share similar meaning and are semantically related. This corresponds to this work's assumption of generating item summaries to represent general user preference as well as Doc2Vec assumption that words that occur in analogous context tend to have related implications, which further translates that relevant item aspects mentioned within review texts will also be involved in the embeddings. Figure5, presents a 2D review texts embedding vectors obtained by applying Principal component analysis (PCA)(Collins, Dasgupta, &amp;    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 5: Sample Embeddings of Review Texts Vectors</figDesc><graphic url="image-50.png" coords="23,75.45,238.01,210.10,157.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. 6: Feature Vector Influence on MSE</figDesc><graphic url="image-53.png" coords="24,75.45,356.97,207.96,156.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Dataset Statistics</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Min</cell><cell>Max</cell><cell>Min</cell><cell>Max</cell><cell>Min</cell><cell>Max</cell><cell>Min</cell><cell>Max</cell></row><row><cell></cell><cell># users</cell><cell># users</cell><cell># items</cell><cell># items</cell><cell>#reviews</cell><cell>#reviews</cell><cell># ratings</cell><cell># ratings</cell></row><row><cell>Amazon</cell><cell>29,757</cell><cell cols="2">774,058 15,149</cell><cell cols="2">120,774 10,261</cell><cell>151,254</cell><cell cols="2">500,176 1,373,768</cell></row><row><cell>Products</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Parameter Setting for LSTM Summarization</head><label>2</label><figDesc></figDesc><table><row><cell>Parameters</cell><cell>Value Used</cell></row><row><cell cols="2">Source sequence length 300 tokens</cell></row><row><cell cols="2">Target summary length 30 tokens</cell></row><row><cell>Optimization method</cell><cell>Stochastic Gradient Descent</cell></row><row><cell></cell><cell>with momentum</cell></row><row><cell>Learning Rate</cell><cell>0.1, halved after every third</cell></row><row><cell></cell><cell>epoch</cell></row><row><cell>Batch size</cell><cell>128</cell></row><row><cell>LSTM parameters</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>MSE for the experimented Amazon Products, best results are highlighted From the results depicted in table 3, the embedded deep summaries approach outperforms, firstly, deep learning methods CNN-D-Att and CNN ConvMF+. It also outperforms two methods, BoWLF and LMLF which combine matrix factorization model with LSTM-RNN and collection of words methods, respectively. Lastly, the EDS approach considerably beats the RMR model, which applies topic model on review texts to enhance model's predictions. The RMR model reported improved performance compared to the Collaborative Topic Model (CTR) model (C.<ref type="bibr" target="#b54">Wang &amp; Blei, 2011)</ref> and the Hidden Factors as Topics (HFT) model<ref type="bibr" target="#b26">(McAuley &amp; Leskovec, 2013)</ref>. The CNN-D-Att approach showed slightly better results for two datasets as presented in table 3. However, their experiments focused on training interpretable models rather than dealing with data sparsity issue and new users or items with no/few ratings, hence they specifically selected dataset samples comprising more than 5 review texts for each user or item.</figDesc><table><row><cell>Dataset</cell><cell>RMR</cell><cell>BoWLF</cell><cell>LMLF</cell><cell>CNN</cell><cell>ConvMF+</cell><cell>CNN-</cell><cell>EDS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D-Att</cell><cell></cell></row><row><cell cols="2">M. Instruments 1.374</cell><cell>1.375</cell><cell>1.388</cell><cell>0.704</cell><cell>0.698</cell><cell>0.694</cell><cell>0.694</cell></row><row><cell>G. Food</cell><cell>1.465</cell><cell>1.464</cell><cell>1.478</cell><cell>1.005</cell><cell>1.002</cell><cell>0.997</cell><cell>0.965</cell></row><row><cell>O. Products</cell><cell>1.638</cell><cell>1.629</cell><cell>1.646</cell><cell>0.727</cell><cell>0.720</cell><cell>0.719</cell><cell>0.700</cell></row><row><cell>Automotive</cell><cell>1.403</cell><cell>1.419</cell><cell>1.428</cell><cell>0.785</cell><cell>0.780</cell><cell>0.779</cell><cell>0.538</cell></row><row><cell>Patio</cell><cell>1.669</cell><cell>1.674</cell><cell>1.680</cell><cell>1.008</cell><cell>1.001</cell><cell>0.996</cell><cell>1.007</cell></row><row><cell cols="2">A.Instant Video 1.270</cell><cell>1.184</cell><cell>1.206</cell><cell>0.952</cell><cell>0.950</cell><cell>0.946</cell><cell>0.927</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (No. 61370137), the National Basic Research Program of China (No.2012CB7207002), the Ministry of Education -China Mobile Research Foundation Project (2016/2-7) and the 111 Project of Beijing Institute of Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author 1: James Chambua -Conceptualization; Data Curation; Formal Analysis; Investigation; Methodology; Roles/Writingoriginal draft; software; visualization.</p><p>Author 2: Zhendong Niu -Methodology; Project administration; Resources; Supervision; Validation; Visualization; Writingreview &amp; editing.</p><p>parameters. Hence, it's important to specify a scheme which guarantees extreme values for the objective function are achieved for wide range of data. Bayesian estimation controls prior probabilities in approximating hyper-parameters and eventually regulates variations which may lead to overfitting <ref type="bibr" target="#b32">(Nowlan &amp; Hinton, 1992)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Bayesian Estimation of the Extended PMF Model</head><p>Bayesian estimation for the hyper-parameters of the PMF model specifies Gaussian-Wishart priors <ref type="bibr" target="#b40">(Salakhutdinov &amp; Mnih, 2008)</ref> on the user and item hyper-parameters given with equation ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>):</p><p>Where * + and * + and is the Wishart distribution with degrees of freedom and scale matrix (identity matrix for user and item hyperparameters) and .</p><p>With such priors for the parameters, the distribution which regulates prediction of the target preference is obtained through utilization of Markov chain Monte Carlo (MCMC) procedures <ref type="bibr" target="#b40">(Salakhutdinov &amp; Mnih, 2008)</ref>. The distribution is specified with equation ( <ref type="formula">21</ref>):</p><p>Where ( ) and ( ) are the model's samples, generated by running Markov chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to assess the prediction capability of our approach in learning and representing users' preference knowledge, relevant item properties and their corresponding correlations as expressed in the review texts in predicting unknown/missing ratings numerous experiments were undertaken. Section 4 provides a description of the datasets used, compared alternative methods and the implementation of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations from Reviews for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.1145/2792838.2800192</idno>
		<ptr target="https://doi.org/10.1145/2792838.2800192" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems -RecSys &apos;15</title>
				<meeting>the 9th ACM Conference on Recommender Systems -RecSys &apos;15<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ask the GRU: Multi-Task Learning for Deep Text Recommendations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959180</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959180" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems -RecSys &apos;16</title>
				<meeting>the 10th ACM Conference on Recommender Systems -RecSys &apos;16<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian Neural Word Embedding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3135" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v3/bengio03a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="932" to="938" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensor factorization method based on review text semantic similarity for rating prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chambua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mbelwa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2018.07.059</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2018.07.059" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="629" to="638" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hybrid recommendation algorithm adapted in e-learning environments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11280-012-0187-z</idno>
		<ptr target="https://doi.org/10.1007/s11280-012-0187-z" />
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1412.3</idno>
		<ptr target="http://arxiv.org/abs/1412.3555" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generalization of principal component analysis to the exponential family</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS</title>
				<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Press</publisher>
			<date type="published" when="2001">2002. 2001</date>
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Document Embedding with Paragraph Vectors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1507.0</idno>
		<ptr target="http://arxiv.org/abs/1507.07998" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9</idno>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual Cortex Inspired CNN Model for Feature Construction in Text Analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2016.00064</idno>
		<ptr target="https://doi.org/10.3389/fncom.2016.00064" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with {LSTM</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015015</idno>
		<ptr target="https://doi.org/10.1162/089976600300015015" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Precise Timing with LSTM Recurrent Networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v3/gers02a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661829.2661935</idno>
		<idno>CoRR, abs/1308.0</idno>
		<ptr target="https://doi.org/10.1145/2661829.2661935" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised automatic text style transfer using LSTM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-73618-1_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-73618-1_24" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883037" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052569</idno>
		<ptr target="https://doi.org/10.1145/3038912.3052569" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;17 Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emerging directions in predictive text mining</title>
		<author>
			<persName><forename type="first">N</forename><surname>Indurkhya</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1154</idno>
		<ptr target="https://doi.org/10.1002/widm.1154" />
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Abstractive and Extractive Text Summarization using Document Context Vector and Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<idno>CoRR, abs/1807.0</idno>
		<ptr target="https://doi.org/arXiv:1807.08000v1" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional Matrix Factorization for Document Context-Aware Recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2959100.2959165</idno>
		<ptr target="https://doi.org/10.1145/2959100.2959165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems RecSys &apos;16</title>
				<meeting>the 10th ACM Conference on Recommender Systems RecSys &apos;16<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
				<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Sparse and Explicit Word Representations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W14/W14-1618.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heterogeneous knowledge-based attentive neural networks for short-term music recommendations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mushonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2874959</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2018.2874959" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="58990" to="59000" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ratings meet reviews, a combined approach to recommend</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1145/2645710.2645728</idno>
		<ptr target="https://doi.org/10.1145/2645710.2645728" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender systems -RecSys &apos;14</title>
				<meeting>the 8th ACM Conference on Recommender systems -RecSys &apos;14<address><addrLine>Foster City, Silicon Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hidden Factors and Hidden Topics:Understanding Rating Dimensions with Review Text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2507157.2507163</idno>
		<ptr target="https://doi.org/10.1145/2507157.2507163" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems -RecSys &apos;13</title>
				<meeting>the 7th ACM conference on Recommender systems -RecSys &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data Sparsity Issues in the Collaborative Filtering Framework</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Blaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/11891321_4</idno>
		<ptr target="https://doi.org/10.1007/11891321_4" />
	</analytic>
	<monogr>
		<title level="m">Advances in Web Mining and Web Usage Analysis</title>
				<meeting><address><addrLine>WebKDD; Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="58" to="76" />
		</imprint>
	</monogr>
	<note>7th International Worrkshop on Knowledge Discovery on the Web</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1301</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word Embedding Techniques for Content-based Recommender Systems: An Empirical Evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lops</surname></persName>
		</author>
		<idno type="DOI">10.1145/2020408.2020480</idno>
		<ptr target="https://doi.org/10.1145/2020408.2020480" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/K/K16/K16-1028.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning</title>
				<meeting>the 20th {SIGNLL} Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying Neural Networks by Soft Weight-Sharing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1992.4.4.473</idno>
		<ptr target="https://doi.org/10.1162/neco.1992.4.4.473" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">From Word Embeddings to Item Recommendation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Ozsoy</surname></persName>
		</author>
		<idno>CoRR, abs/1601.0</idno>
		<ptr target="http://arxiv.org/abs/1601.01356" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Sentence Embedding Using Long Short-Term Memory Networks : Analysis and Application to Information Retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2016.2520371</idno>
		<ptr target="https://doi.org/10.1109/TASLP.2016.2520371" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of transfer learning for collaborative recommendation with auxiliary data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2015.11.059</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2015.11.059" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference LREC Workshop on New Challenges for NLP Frameworks</title>
				<meeting>the Language Resources and Evaluation Conference LREC Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural abstractive text summarization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rossiello</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1769" />
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
				<meeting><address><addrLine>Genova, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1769</biblScope>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An introduction to Random Indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<ptr target="http://eprints.sics.se/221/1/RI_intro.pdf" />
	</analytic>
	<monogr>
		<title level="m">Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390267</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390267" />
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems 20 (NIPS 07)</title>
				<meeting>Advances in Neural Information essing Systems 20 (NIPS 07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using Markov chain Monte Carlo</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390267</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390267" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
				<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2008. 2008. 2005</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3109859.3109890</idno>
		<ptr target="https://doi.org/10.1145/3109859.3109890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems -RecSys &apos;17</title>
				<meeting>the Eleventh ACM Conference on Recommender Systems -RecSys &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Usage of hybrid model based on concepts correlations in adaption to changes of user&apos;s interest</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-30544-6_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-30544-6_58" />
	</analytic>
	<monogr>
		<title level="m">Digital Libraries: International Collaboration and Cross-Fertilization, ICADL 2004</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="653" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matchbox : Large Scale Online Bayesian Recommendations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1526709.1526725</idno>
		<ptr target="https://doi.org/10.1145/1526709.1526725" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on World Wide Web (WWW&apos;09)</title>
				<meeting>the 18th International Conference on World Wide Web (WWW&apos;09)<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generating Text with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.icml-2011.org/papers/524_icmlpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning,ICML 2011</title>
				<meeting>the 28th International Conference on Machine Learning,ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P15/P15-1150.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
				<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A hybrid recommender system for e-learning based on context awareness and sequential pattern mining</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalui</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-017-2720-6</idno>
		<ptr target="https://doi.org/10.1007/s00500-017-2720-6" />
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2449" to="2461" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Knowledge-based recommendation: a review of ontology-based recommender systems for e-learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mustafa</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-017-9539-5</idno>
		<ptr target="https://doi.org/10.1007/s10462-017-9539-5" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A hybrid knowledge-based recommender system for e-learning based on ontology and sequential pattern mining</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yousif</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2017.02.049</idno>
		<ptr target="https://doi.org/10.1016/j.future.2017.02.049" />
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning:Vector Space Models of Semantics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.2934</idno>
		<ptr target="https://doi.org/10.1613/jair.2934" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A learner oriented learning recommendation approach based on mixed concept mapping and immune algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2016.03.022</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2016.03.022" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="28" to="40" />
		</imprint>
	</monogr>
	<note>Knowledge-Based Systems</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">An e-learning recommendation approach based on the selforganization of learning resource. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.06.014</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2018.06.014" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Hybrid E-learning Recommendation Approach Based on Learners&apos; Influence Propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2019.2895033</idno>
		<ptr target="https://doi.org/10.1109/tkde.2019.2895033" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2020408.2020480</idno>
		<ptr target="https://doi.org/10.1145/2020408.2020480" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;11</title>
				<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining -KDD &apos;11<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783273</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783273" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-task learning model based on recurrent convolutional neural networks for citation sentiment and purpose classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.01.021</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2019.01.021" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A survey on sentiment analysis of scientific citations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-017-9597-8</idno>
		<ptr target="https://doi.org/10.1007/s10462-017-9597-8" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Explicit factor models for explainable recommendation based on phrase-level sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609579</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609579" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval -SIGIR &apos;14</title>
				<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval -SIGIR &apos;14<address><addrLine>Gold Coast, Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
