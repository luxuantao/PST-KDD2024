<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PuDianNao: A Polyvalent Machine Learning Accelerator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daofu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">Tianshi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuehai</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">Yunji</forename><surname>Chen</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
							<email>chentianshi@ict.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shaoli Liu SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Jinhong Zhou USTC</orgName>
								<orgName type="institution">Shengyuan Zhou SKLCA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Olivier Teman Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Xiaobing Feng SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">CAS Center for Excellence in Brain Science</orgName>
								<orgName type="institution" key="instit1">SKLCA</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Architecture</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PuDianNao: A Polyvalent Machine Learning Accelerator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2694344.2694358</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energyefficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energyefficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.</p><p>In this study, we present an ML accelerator called Pu-DianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the era of data explosion, Machine Learning (ML) techniques have become pervasive tools in emerging large-scale commercial applications such as social network, recommendation system, computational advertising, and image recognition. Facebook generates over 10 Petabyte (PB) log data per month <ref type="bibr" target="#b5">[6]</ref>. Taobao.com, the largest online retailer in China, generates tens of Terabyte (TB) data every day <ref type="bibr" target="#b5">[6]</ref>. The increasing amount of data poses great challenges to ML techniques, as well as computer systems accommodating those techniques.</p><p>The most straightforward way to accelerate large-scale ML is to design more powerful general-purpose CPUs and GPUs. However, such processors must consume a large fraction of transistors to flexibly support diverse application domains, thus can often be inefficient for specific workloads. In this context, there is a clear trend towards hardware accelerators that can execute specific workloads with very high energy-efficiency or/and performance. For ML techniques that have broad yet important applications in both cloud servers and mobile ends, of course, there have been some successful FPGA/ASIC accelerators, but each of which of-ten targets at only a single ML technique or technique family. DianNao <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> is one such example, which effectively accelerates representative neural network algorithms to benefit different ML scenarios (e.g., regression and classification).</p><p>While valid to support different ML scenarios at different scales, DianNao still has disadvantages. To be specific, when using DianNao to solve an ML problem, one has no choice but to use a neural network. Although recent studies have reported the large success of neural network (e.g., deep neural network), they were often achieved at great computational efforts, and involve too many parameters to tune. Even if regardless of the high computational complexity, neural network can still be worse than other ML techniques under certain scenarios. For example, in the classification of linearlyseparable data, complex neural networks can easily become over-fitting, and perform worse than even a linear classifier. In application domains such as financial quantitative trading, linear regression is more widely-used than neural network due to the simplicity and interpretability of linear model <ref type="bibr" target="#b4">[5]</ref>. The famous no-free-lunch theorem from the ML domain is a good summary of the above situation: any learning technique cannot perform universally better than another learning technique <ref type="bibr" target="#b38">[39]</ref>. The insight here is that an ML accelerator should be able to support diverse ML techniques, in order to address needs under different scenarios.</p><p>However, unlike neural networks which share similar computational patterns, there is significant diversity among existing ML techniques, making it hard to design a pervasive ML accelerator. The diversity is two-fold. First, different ML techniques may differ a lot in their computational primitives. Here we take two classification algorithms, naive bayes and k-nearest neighbors (k-NN), as examples. The most time-consuming part of naive bayes is estimating the conditional probabilities, but that of k-NN is calculating distances between instances (each represented as a feature vector). Second, different ML techniques may differ a lot in their locality properties. For example, a large number of instances in k-NN may be frequently reused to classify unseen instances, while each feature (vector component) of an instance is not frequently reused in naive bayes. In this case, one must identify and factor in computational primitives and locality properties of representative ML techniques before deciding the overall accelerator architecture.</p><p>In this paper, we present an accelerator accommodating seven representative ML techniques, i.e., k-means, k-NN, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. We conduct a thorough analysis of the ML techniques to extract critical computational primitives and locality optimizations that need to be supported by the accelerator. We present PuDianNao, an architecture design at TSMC 65nm process. On 13 critical phases of 7 representative ML techniques, the accelerator, clocked at 1GHz, is 1.20x faster and 128.41x more energyefficient than the NVIDIA K20M GPU on average. Our contributions are the following. First, we thoroughly analyze several representative ML techniques to extract their key computational tasks and locality properties, which establishes a solid foundation for the design of ML accelerator. Second, we design novel functional units to cover common computational primitives of different ML techniques, as well as on-chip storage that comprehensively factors in locality properties of different ML techniques. Third, we present Pu-DianNao, an accelerator accommodating seven representative ML techniques and multiple ML scenarios (e.g., classification, regression and clustering).</p><p>The rest of this paper proceeds as the following. Section 2 conducts a thorough analysis on computational primitives and locality properties of seven ML techniques. Section 3 presents the accelerator architecture of PuDianNao. Section 4 introduces the control module and code format of PuDian-Nao. Section 5 and 6 provide the experimental methodology and experimental results, respectively. Section 7 presents the related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Analysis of Representative ML</head><p>Techniques: A Computer Architecture Perspective</p><p>In the machine learning community, an ML technique is conventionally characterized by its mathematical model (e.g., linear or non-linear), its learning style (e.g., supervised or unsupervised), its training algorithm (e.g., maximuma-posteriori or gradient descent) and so on. From a computer architecture perspective, however, an ML technique is mainly characterized by its computational primitives and locality property. In this section, we do not repeat too much the context of ML textbooks, but provide an architect's-eye analysis of seven ML techniques. For each ML technique, our analysis consists of two parts. In the first part, we identify the most time-consuming step of the technique. In the second part, we analyze the locality property of the identified time-consuming step of the ML technique, and leave the analysis/optimization on the rest of the technique to later sections. This divide-and-conquer idea well reflects the spirit of our accelerator (see Section 3.1): it accommodates the most time-consuming operations of ML techniques with dedicated hardware computational structures as well as optimized on-chip buffers, but supports the rest operations by integrating lightweight generalpurpose Arithmetic Logic Units (ALUs).</p><p>Based on our locality analysis, we also investigate how to reduce off-chip memory bandwidth requirements of different ML techniques, and empirically check potential offchip memory bandwidth reductions with an in-house cache simulator, which has 32KB cache (clocked at 1GHz) which has enough banks to support a 256-bit SIMD engine. To focus on memory behaviors, we assume that the SIMD engine can calculate any function with three 256-bit inputs (e.g., f (a, b, c)) at one cycle. We provide a sufficiently large num-// Na is the number of testing instances // Nb is the number of reference instances for ( i = 0; i &lt; Na ; i ++) { for ( j = 0; j &lt; Nb ; j ++) { Dis [i , j ] = dis ( t ( i ) ,r ( j ) ) ; } } ber of training/testing/reference instances (each is a 32x32bit floating-point vector) for each ML technique when evaluating the off-chip memory bandwidth requirement. <ref type="bibr" target="#b1">[2]</ref> is a simple yet widely-used learning algorithm that directly uses the k nearest neighbors of a testing instance to determine the label (classification or regression result) of the instance. For each testing instance, the k-NN has two major steps. First, it computes distances between the testing instance and reference instances. Second, it finds the k nearest reference instances to the testing instance, and assigns the testing instance the most frequent label among labels of the k nearest reference instances (classification), or the average label in the k nearest reference instances (regression). The most time-consuming operations in k-NN are calculating distances between instances. On UCI Gas datasets, distance calculations averagely account for 84.44% the computation time on an Intel Xeon E5-4620 CPU. Figure <ref type="figure" target="#fig_0">1</ref> presents the original code of distance calculations. We observe that a reference instance can be reused after N b − 1 distance calculations (N b is the number of testing instances), while a testing instance can always be reused in its own loop. When N b is very large, the reuse distance of a reference instance is also very large, and the chip cannot simultaneously store all reference instances, leading to frequent off-chip memory accesses that re-fetch the same reference instances for different testing instances. To exploit the locality of reference instances, we tile loops of both testing and reference instances, and define each tiling block to be distance calculations between T i testing instances and T j reference instances (see Figure <ref type="figure" target="#fig_2">3</ref>). Tiling significantly reduces the reuse distance of reference instances, and can significantly reduce off-chip memory transfers. In our experiment comparing the off-chip memory bandwidth require-ments before and after tiling, we set T i = T j = 32, i.e., T i + T j = 64 instances (each is a 32x32-bit floating-point vector) in a tiling block (64x32x4 Byte = 8KB) can be simultaneously stored in the cache (32KB). We observe that tiling reduces the off-chip memory bandwidth requirement of distance calculations by 93.9% (see Figure <ref type="figure" target="#fig_1">2</ref>). // Ti is the tiled testing block size // Tj is the tiled reference block size for ( i = 0; i &lt; Na / Ti ; i ++) { for ( j = 0; j &lt; Nb / Tj ; j ++) { // tiled block for ( ii = i * Ti ; ii &lt; ( i +1) * Ti ; ii ++) { for ( jj = j * Tj ; jj &lt; ( j +1) * Tj ; jj ++) { Dis [ ii , jj ] = dis ( t ( ii ) ,r ( jj ) ) ; } } } }  <ref type="bibr" target="#b15">[16]</ref> is an unsupervised ML technique which partitions N instances into k clusters. k-Means starts with k random cluster centroids, and iteratively performs two steps. First, it assigns each instance to the cluster whose current centroid (mean) is the nearest. Second, it computes the new mean of each cluster with the current members of the cluster, which then becomes the new centroid of the cluster. The most time-consuming operations in k-Means are distance calculations. On UCI Gas datasets, distance calculations account for 89.83% the computation time of k-Means on an Intel Xeon E5-4620 CPU. In locality optimization, distance calculations in k-Means can be treated in a way (i.e., tiling) similar to what is done to k-NN. The minor difference here is that reference instances and testing instances in the locality analysis of k-NN are respectively replaced with cluster centroids (means), and instances to be clustered. We follow the same experimental setting of tiling in k-NN (e.g., instance length and tiling block size), and observe that tiling both cluster centroids and instances can reduce the off-chip memory bandwidth requirement of distance calculations by 92.5% (see Figure <ref type="figure" target="#fig_3">4</ref>).</p><formula xml:id="formula_0">2.1 k-Nearest Neighbors k-Nearest Neighbors (k-NN)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">k-Means k-Means</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Neural Network</head><p>Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b11">[12]</ref> is a classical artificial neural network that can model the non-linear relationship between inputs and outputs. Over the past decade, an emerging type of MLP called Deep Neural Network (DNN) has attracted broad interests of both the machine learning community and industry <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>DNN is known to have a deep structure consisting of many hidden layers, and neurons in adjacent layers are often fully connected. A DNN has three computation modes, feedforward computation which computes the network output for each given input under the current network setting, pre-training which locally tune the synapses (connection weights) between each pair of adjacent layers, and global training which globally tune synapses with the Back Propagation (BP) algorithm <ref type="bibr" target="#b34">[35]</ref>.</p><p>We first analyze the feedforward computation, which computes neuron values of the (g + 1)-th layer with neuron values of the g-th layer (g ≥ 1). Without losing any generality, assume that we already know the neuron values of the g-th layer, a layer having N a neurons (x 1 , . . . , x Na ). We are going to compute the neuron values of the (g + 1)-th layer, a layer having N b neurons (y 1 , . . . , y N b ). The value of the i-th neuron in the (g + 1)-th layer, y i , is computed by y j = f Na i=1 w ij x i + s j , where f represents the activation function (e.g., sigmoid and tanh), w ij is the synapse between neurons x i and y j , and s j is the bias. The feedforward computation, which computes all N b neurons in the same layer, can be written in a matrix form Y = X ⊗ W , where</p><formula xml:id="formula_1">W =        s 1 s 2 • • • s N b w 11 w 12 • • • w 1N b w 21 w 22 • • • w 2N b . . . . . . . . . . . . w Na1 w Na2 • • • w NaN b       <label>(1)</label></formula><p>X = (1, x 1 , . . . , x Na ), Y = (y 1 , . . . , y N b ), and the operator ⊗ is similar to the conventional matrix/vector multiplication except that the dot product performed between each row vector of the left multiplier and each column vector of the right multiplier, say, p • q is now replaced with f (p • q). Here f is the activation function of neurons. It can be observed that neurons of the g-th layer (x[i] in Figure <ref type="figure" target="#fig_4">5</ref>) will be used for N b times to calculate neurons in the next layer (y[j] in Figure <ref type="figure" target="#fig_4">5</ref>), while each synapse (w[i, j] in Figure <ref type="figure" target="#fig_5">6</ref>) is only be used once. In the original code, however, x[i] can be reused after a total of 2N a floating-point operations (multiplications and additions). When the number of neurons in the vector X (i.e., N a ) is very large, the reuse distance is also very large. In the meantime, the cache of a chip may not be sufficiently large to simultaneously store values of all N a neurons, and have to re-fetch them when reusing. In order to reduce memory transfers, we tile the loop with respect to neurons in the vector X (see Figure <ref type="figure" target="#fig_6">7</ref>). We also empirically evaluate the impact of tiling on feedforward computations, where we set N a = 16384 (16384 neurons need a total of 16384×4Byte=64KB). We observe that tiling reduces the memory bandwidth requirement of feedforward computation by 46.7%.</p><p>// x is the input neurons , y is output neurous // w is the weights , w [0 , i ] = s [ i ] y ( all ) = 0; // initialize all neurons for ( i = 0; i &lt; Nb ; i ++) { for ( j = 0; j &lt; ( Na +1) ; j ++) { y // T is the tiled block size y ( all ) = 0; // initialize all output neurons for ( j = 0; j &lt; ( Na +1) / T ; j ++) { // tiled block for ( i = 0; i &lt; Nb ; i ++) { for ( jj = j * T ; jj &lt; T ; jj ++) { y  <ref type="bibr" target="#b20">[21]</ref>, in which the most time-consuming steps are iterative feedforward computation and backforward computation (which includes similar operations to feedforward computation) <ref type="foot" target="#foot_0">1</ref>invoked by Gibbs sampling. Global training uses the Back Propagation (BP) algorithm <ref type="bibr" target="#b34">[35]</ref> to globally tune synapses, and the most-time consuming step is still analogous to feedforward/backforward computation. Therefore, the locality optimization done for feedforward computation also provides hints to pre-training and global training. We do not repeat details for the sake of brevity.</p><formula xml:id="formula_2">[ i ] += w [j , i ]* x [ j ]; if ( j == Na ) y [ i ] = f ( y [ i ]) ; } }</formula><formula xml:id="formula_3">[ i ] += w [ jj , i ]* x [ jj ]; if ( jj == Na ) y [ i ] = f ( y [ i ]) ; } } }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Linear Regression</head><p>Linear Regression (LR) <ref type="bibr" target="#b13">[14]</ref> is a supervised learning technique which models the relationship between a scalar response variable y and a d-dimensional vector variable x = (x 1 , x 2 x d ) T with a linear function y = d i=0 θ i x i (where x 0 always equals to 1). LR consists of two phases, the training phase which establishes the linear function from data, and the prediction phase which uses the existing linear model to predict the response (label) of each testing instance. We starts with the prediction phase as it is simpler and more fundamental. At this phase, predicted responses of n different testing instances can be computed as</p><formula xml:id="formula_4">Y = θX,<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">X =         1 1 1 • • • 1 x (1) 1 x (2) 1 x (3) 1 • • • x (n) 1 x (1) 2 x (2) 2 x (3) 2 • • • x (n) 2 . . . . . . . . . . . . . . . x (1) d x (2) d x (3) d • • • x (n) d         ,<label>(3)</label></formula><p>θ = (θ 0 , θ 1 , . . . , θ d ), and Y = (y (1) , . . . , y (n) ) are predicted responses of testing instances x (1) , . . . , x (n) , respectively. This can be viewed as a multiplication between a vector and a matrix. The locality analysis is similar to that for feedforward computations in DNN (see Section 2.3). Briefly, the coefficients θ need to be reused, and there is no reuse of data in X. When the number of coefficients in a linear model is very large, we need to tile θ to reduce the reuse distance of those coefficients. Our empirical study on a setting of d = 16384 reveals that tiling reduces the memory bandwidth requirement of LR prediction phase by 46.7%. At the training phase, the aim is to find appropriate coefficients θ = (θ 0 , . . . , θ d ) minimizing the Mean Square Error (MSE) over m training instances {x (1) , y (1) }, . . . , {x (m) , y (m) }, where y (i) is observed response (label) of training instance x (i) . Gradient descent is a common solution to this training task <ref type="bibr" target="#b40">[41]</ref>. Briefly, gradient descent starts with an initial values of θ = (θ 0 , . . . , θ d ), and iteratively updates θ along the negative gradient direction. In gradient descent, the most-time consuming operations are calculating θx (i) (i = 1, . . . , m) with the latest θ. Such operations can also be written as Eq. ( <ref type="formula" target="#formula_4">2</ref>). Therefore, the locality optimization for the prediction phase is also applicable here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Support Vector Machine</head><p>Support Vector Machine (SVM) is a supervised learning technique for classification and regression <ref type="bibr" target="#b10">[11]</ref>. The central idea of SVM is finding a hyperplane in a high-dimensional space, such that the distance between the hyperplane and the nearest training instance (i.e., geometric margin) is the largest. Like other supervised learning techniques, SVM has two phases, the training phase which constructed a model from the training data, and the prediction phase which use the trained model to predict the label of each testing instance. The SVM model, which predicts the label (y) of input instance x, has the form y = N i=1 α i y (i) k(x, x (i) ) + b, where N is the number of training instances, x (i) and y (i) are the i-th training instance and its label respectively, α i is a constant coefficient, k(•, •) is a kernel function (e.g., radial basis function, tanh function) serves as the surrogate of computing the dot product between two instances at a high-dimensional space, and b is a constant. When x (i) is not a support vector<ref type="foot" target="#foot_1">2</ref> , the corresponding coefficient α i equals to 0.</p><p>At the SVM training phase, the goal is finding appropriate coefficients α i that maximize the geometric margin, and a common training algorithm is Sequential Minimal Optimization (SMO) <ref type="bibr" target="#b31">[32]</ref>. The most time-consuming step in SMO is to compute the N × N kernel matrix K (N is the total number of training instances), a symmetric matrix recording kernel function values of all possible pairs of training instances. Let k ij (i, j = 1, . . . , N ) be the entry at the i-th row and j-th column of matrix K, which is the kernel function value of training instances x (i) and x (j) , i.e., k ij = k(x i , x j ). Kernel matrix computation shares a similar locality property to distance calculations in k-NN, except that for each pair of instances, kernel matrix computation computes the value of kernel function instead of computing the distance. Therefore, we reuse the tiling mechanism designed for distance calculations. Tiled kernel matrix computation reduces the memory bandwidth requirement by 93.9%.</p><p>Assume that we have already constructed an SVM model involving N a support vectors (see Footnote 2 ). At the prediction phase, we use the constructed SVM model to predict labels of N b testing instances. The most time-consuming computations at this phase are calculating kernel function values between each pair of support vector and testing in- stance. The locality analysis is similar to that of distance calculations in k-NN, and the minor differences are that reference instances in k-NN are replaced with support vectors in the context of SVM, and for each pair of support vector and testing instance we calculate the kernel function value instead of their distance. Similarly, we can tile both support vectors and testing instances to reduce memory bandwidth requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Naive Bayes</head><p>Naive Bayes (NB) is a probabilistic classifier based on Bayes' theorem and the Maximum-a Posteriori (MAP) paradigm <ref type="bibr" target="#b22">[23]</ref>. It also relies on a strong assumption that instance features are independent with each other. NB has two phases: the training phase which estimates individual conditional probabilities from training instances, the prediction phase which classifies testing instances. The goal of the training phase is to construct probability tables for estimating the posterior probability p(C|F 1 , . . . , F d ), where C represents the label (class index), F i (i = 1, . . . , d) represents the i-th feature. Here C and F i are random variables, not specific values. By applying the Bayes' theorem and the independence assumption, p(C|F 1 , . . . , F d ) can be written as (1/Z)p(C)</p><formula xml:id="formula_6">d i=1 p(F i |C)</formula><p>, where Z is a scaling factor that does not influence the results of NB. The central task of the training phase is to estimate conditional probabilities p(F i |C) (i = 1, . . . , d). Here we consider the discrete version of NB, thus those conditional probabilities can be obtained by performing frequency estimate on the training data. Assume that F i (i = 1, . . . , d) can only take the value from {f ij } a j=1 , C can only take the value from {c k } b k=1 , then each conditional probability can be written as p(F i = f ij |C = c k ), and there are a total of d × a × b such items. NB maintains a temporary counter for each item to assist the frequency estimate. By streaming in features and label of training instances, NB completes all frequency estimates, and normalize the frequencies to get all conditional probabilities. In training an NB classifier, each dimension (feature value) of an instance is fetched to cache for a single time, but could be reused for multiple times when it is involved in, for example, multiple bitwise AND operations for comparing it with several candidate values that the feature can take (comparisons are due to conditional assignment/branching statements for choosing the right counter).</p><p>Because each reuse of a feature value happens almost immediately after the last use, there is no need to tile instances as well as their feature values. During counting, however, temporary counters have to be frequently updated. When there are too many conditional probabilities to estimate, it is possible that the cache cannot store all temporary counters. To reduce memory transfers of temporary counters, one can pre-process training instances so that they are grouped according to their labels. Moreover, it is better to fetch from off-chip memory the corresponding features of different instances, rather than fetching each instance as a whole.</p><p>The prediction phase is relatively simpler. For each testing instance x = ( f1 , . . . , fd ), NB calculates posterior probabilities p(C = c k |F 1 = f1 , . . . , F d = fd ) (k = 1, . . . , b), and assigns the class index that maximize the posterior probability as the label of instance X. The most time-consuming operations are multiplications of d numbers. Moreover, there is no significant locality here, as no one can predict the feature values of an unseen instance (which determine the data required by the follow-up multiplications).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Classification Tree</head><p>Classification Tree (CT) is a type of supervised learning techniques which construct tree-like classifiers from training instances. CT consists of two phases, the training phase which constructs a tree-like classifiers from training instances, and the prediction phase which predicts labels of testing instances.</p><p>At the training phase, CT starts with the root node representing the whole set of training instances, and recursively split non-leaf nodes into smaller nodes under the guide of a learning metric. Different CTs may use different learning metrics. For example, CART uses Gini impurity <ref type="bibr" target="#b2">[3]</ref>, ID3 uses information gain <ref type="bibr" target="#b32">[33]</ref>, and C4.5 uses information gain ratio <ref type="bibr" target="#b33">[34]</ref>. However, the most time-consuming operations of all CTs are counting, which are indispensable steps for computing values of different learning metrics. There are often three types of counting tasks: counting the number of instances whose specific feature takes a specific value (for discrete feature space), counting the number of instances whose specific feature exceeds a threshold (for continuous feature space), and counting the number of instances having a specific label value. In each round of counting, the same feature value (vector component) of the same training instance (vector) will be frequently reused, because it is involved in, for example, multiple bitwise AND operations for comparing it with several candidate values that the feature can take (comparisons are due to conditional assignment/branching statements for choosing the right counter). Because each reuse of a feature value happens almost immediately after the last use, there is no need to tile instances as well as their feature values. In different rounds of counting, the same training instance could also be reused, but the reuse distance depends on data characteristics (instead of algorithm character-istics), thus is not deterministic. Therefore, we cannot expect a predefined tiling strategy that can reduce the corresponding memory accesses.</p><p>At the prediction phase, the constructed CT classifier predicts the label of each testing instance. In this process, each testing instance needs to travel along the path from the root node to a leaf node, and the leaf node assigns the testing instance its label. When the size of the CT is very large, it is possible that the cache cannot store the whole tree, and there is an unacceptable cost if we reload the whole tree for each testing instance. To address this issue, we can follow the divide-and-conquer strategy and decompose the tree into sub-trees, each of which can be stored by cache. When a subtree is stored in the cache, it processes all testing instances that have not yet been labeled. This strategy can also be interpreted as tiling the tree, which avoids to frequently reload the whole tree. We provide a brief summary to this section. First, we identify some critical computational primitives of ML techniques, such as distance calculation, dot product, counting, as well as some non-linear functions, providing hints to implementing functional units of the accelerator. Second, and more importantly, we discover from our locality analysis that tiling can effectively exploit the data locality of k-NN, k-Means, DNN, LR, and SVM, but is not very helpful to NB and CT since they do not exhibit remarkable or predictable data locality. Figure <ref type="figure" target="#fig_9">10</ref> illustrates the average reuse distance (i.e., the average number of instructions between two consecutive accesses) of each variable (excluding loop variables) in k-NN (tiled distance calculations) and NB (training) on an x86 machine. As illustrated in Figure <ref type="figure" target="#fig_13">10a</ref>, variables of distance calculations in k-NN naturally cluster into three classes in terms of their average reuse distances (similar behaviors can be observed from k-Means, DNN, LR, and SVM). This observation motivates us to use three separate on-chip buffers in the accelerator, each buffer stores variables having similar reuse distances (see Section 3.2 for details). Moreover, we observe from Figure <ref type="figure" target="#fig_13">10b</ref> that variables in NB also cluster into two classes (similar behaviors can be observed from CT). The left class corresponds to feature and label values of training instances, whose reuse distances are always 1 (see Section 2.6 for more details). The right class corresponds to temporary counters, whose average reuse distances spread over a large interval. This is because the reuse of a temporary counter happens only when a specific feature of the current instance takes a specific value, which is a stochastic event decided by data characteristics instead of algorithm characteristics. For ML techniques like NB and CT, we need at least two on-chip buffers to fit two levels of reuse distances. When designing the accelerator, we carefully factor in observations made in this section. In follow-up sections, we will present the concrete accelerator architecture and show how the architecture accommodates different ML techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Accelerator Architecture</head><p>In this section, we present the detailed architecture of Pu-DianNao, a hardware accelerator supporting seven representative ML techniques. As illustrated in Figure <ref type="figure" target="#fig_10">11</ref>, the Pu-DianNao consists of several Functional Units (FUs), three data buffers (HotBuf, ColdBuf, and OutputBuf), an instruction buffer (InstBuf), a control module, and a DMA. In the rest of this section, we will elaborate the microarchitecture of each component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Functional Units</head><p>Functional Units (FUs) are the basic execution units of the PuDianNao ML accelerator. Specifically, each FU consists of two parts, a Machine Learning functional Unit (MLU) and an Arithmetic Logic Unit (ALU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Machine Learning Unit (MLU)</head><p>The MLU is designed to support several basic yet important computational primitives which are common in representative ML techniques, including dot product (LR, SVM, and DNN), distance calculations (k-NN and k-Means), counting (ID3 and NB), sorting (k-NN and k-Means), non-linear functions (e.g., sigmoid and tanh) and so on. As illustrated in Figure <ref type="figure" target="#fig_11">12</ref>, the MLU is divided into 6 pipeline stages (Counter, Adder, Multiplier, Adder tree, Acc, and Misc).</p><p>In the Counter stage, each pair of inputs will be fed to a bitwise-AND unit or be compared by a comparer unit, and the value will then be added to an accumulator. After that, results of accumulators will be directly forwarded to the output buffer rather than the next stage. This stage is used to accelerate counting operations in naive bayes and classification tree. When there is no need to execute counting operations, the counter stage will be bypassed.</p><p>The Adder stage is leveraged to operate vector addition, which is a very common operation in ML techniques. The adder stage may also be bypassed (e.g., in DNN), and results can be forwarded to either the next stage or the output port.</p><p>The Multiplier stage is leveraged to operate vector multiplication. Inputs of the multiplier stage can be the outputs of the previous stage (gray arrows in Figure <ref type="figure" target="#fig_11">12</ref>) or data directly read from the input buffers (red and blue arrows in Figure <ref type="figure" target="#fig_11">12</ref>). Results of the multiplier stage can be forwarded to either the next stage or the output port.</p><p>The Adder tree stage adds up results of all multipliers in the previous stage. The Adder tree stage and the Multiplier stage together support the dot product operation, an operation widely used in ML techniques (e.g. LR, SVM, and DNN). When the dimension of instance is larger than the size of adder tree, results of this stage are partial sums, which will be accumulated in the Acc stage. After obtaining the final result of dot product, the Acc stage will forward it to either the next stage or the output port.</p><p>The Misc stage integrates two modules, linear interpolation module and k-sorter module. The linear interpolation module is used to approximatively calculate non-linear functions involved in ML techniques (e.g. sigmoid and tanh in neural network). Different non-linear functions correspond to different interpolation tables. The k-sorter module is used to find the smallest k values from the outputs of Acc stage, which is a common operation in k-Means and k-NN. Results of linear interpolation module and k-sorter module will be forwarded to the output port respectively. Finally, the output port will select one out of the six collected results as the final output of the MLU module, as illustrated in Figure <ref type="figure" target="#fig_11">12</ref>.</p><p>Furthermore, to reduce the area/power consumption of the PuDianNao accelerator, we implement 16-bit floatingpoint arithmetic units for stages Adder, Multiplier, and Adder tree. In the meantime, we still implement 32-bit floating-point units for the rest three stages (Counter, Acc, and Misc) to avoid potential overflow. We empirically evaluate the impact of the above hardware optimizations on the accuracies of k-NN, k-Means, SVM, LR, and MLP. We observe that it incurs negligible accuracy loss to these ML techniques (see Table <ref type="table" target="#tab_0">1</ref>), but significantly reduces the area of MLU. For example, we implemented the verilog of both 16-bit and 32-bit floating-point multipliers, placed/routed them, and found that the area of the 16-bit multiplier is only 20.07% the area of the 32-bit multiplier. Besides, here we do not evaluate naive bayes and classification tree, because they are not involved in stages Adder, Multiplier, and Adder tree, and will not be influenced by the 16-bit arithmetic units at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Arithmetic Logic Unit (ALU)</head><p>In addition to computational primitives discussed above, in some ML techniques there are some other miscellaneous operations that are not supported by the MLU (e.g., division and conditional assignment). Although such operations are not very frequent in ML techniques, executing them on a host general-purpose core will still cause long-distance data movements as well as synchronization overheads. Therefore, we add a small Arithmetic Logic Unit (ALU) in each FU, which contains an adder, a multiplier, and a divider, as well as the converter of 32-bit float to 16-bit float and 16bit float to 32-bit float. In addition, to support the log function required in training classification trees, we use ALU to compute approximations with the Taylor expansion of log(1 − x). Our experiments of the ID3 classification tree on UCI datasets <ref type="bibr" target="#b0">[1]</ref> suggest that computing log(•) with the first 10 items of the Taylor series have been sufficient to remove the accuracy loss brought by approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">On-Chip Data Buffers</head><p>We reveal in Section 2 that tiling can effectively exploit the locality of many ML techniques, and we also observe that average reuse distances of variables in tiled ML techniques cluster into two or three classes (see Section 2.8). Motivated by this observation, we put three separate on-chip data buffers in the PuDianNao accelerator: HotBuf (8KB), Cold-Buf (16KB) and OutputBuf (8KB). HotBuf stores the input data which have short reuse distance, and ColdBuf stores the input data with relative longer reuse distance. OutputBuf stores output data or temporary results.</p><p>In addition to the locality property, read width is another factor that motivates us to use multiple buffers. Here we take tiled distance calculations in k-Means as an example. Suppose that each time an MLU can only process f features (dimensions) of a testing instance and f features of a centroid. As the accelerator has a total of u MLUs, it can calculate partial square distances between one centroid and u testing instances at each cycle. Correspondingly, the MLUs need to read f × 16 bits of centroids and u × f × 16 bits of testing instances at each cycle. In other words, the read width of centroids and testing instances are different. Therefore, we set two separate buffers (HotBuf and ColdBuf) to reduce the overhead brought by different read widths. HotBuf stores centroids, thus the read width is f × 16 bits. HotBuff stores testing instances, thus the read width is u × f × 16 bits.</p><p>All three buffers are connected to the same DMA. Moreover, we use single-port SRAMs to construct HotBuf and ColdBuf, which can reduce the area and power consumption. As FUs may read (e.g. partial sum) or write the OutputBuf, we use dual-port SRAM to construct the OutputBuf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Control and Code Generator</head><p>The simplest way of controlling the accelerator is to hardwire all seven ML techniques. However, if the user wants to use another ML technique that is only slightly different from a hardwired ML technique, we might have to provide the user a new accelerator. To improve the flexibility of the accelerator, we use control instructions to support potential minor changes made on ML techniques. The accelerator only needs a single control module because all FUs will synchronously execute the same operations. The control module fetches instructions from the InstBuf, decodes the instructions, and sends operation signals to all FUs.</p><p>For each ML technique, the execution can be decomposed into several instructions. Table <ref type="table" target="#tab_1">2</ref> presents the instruction format. More specifically, each instruction contains five slots: CM, HotBuf, ColdBuf, OutputBuf, and FU. This instruction format is customized for the PuDianNao architecture, thus a programmer needs to know implementation details of the accelerator. In order to facilitate programmers, we implement a code generator to generate instructions for different ML techniques.</p><p>We provide an example of generated k-Means code in Table <ref type="table" target="#tab_2">3</ref>. In this example, the number of features in each instance is f = 16, the number of centroids is k = 1024, and the number of testing instances is N = 65536. Centroids will be stored in the HotBuf (8KB), and testing instances will be stored in the ColdBuf (16KB). In order to hide the DMA memory access behind the computation, we leverage the HotBuf and ColdBuf in the ping-pong manner. Specifically, in the first instruction, the accelerator LOADs 128 centroids (4KB) and 256 testing instances (8KB) from the memory via the DMA, which occupy half of the HofBuf and ColdBuf, respectively. Then the accelerator dis-  tances between the loaded centroids and testing instances. At the same time, another 256 testing instances are loaded to the other half of the ColdBuf, which will be leveraged by the second instruction. In the second instruction, the 128 centroids loaded in the first instruction will be reused, which are READ from the HotBuf. When distance calculations between the 128 centroids and all 65536 testing instances have been processed (after the 256-th instruction), a new block of 128 centroids will be loaded (in the 257th instruction). The process is repeated until distance calculations between all centroids and testing instances have been done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Methodology</head><p>Evaluation. We use both Verilog and C simulator implementations of PuDianNao to measure the performance and power.</p><p>Verilog design. The design is synthesized by Synopsys Design Compiler with TSMC 65nm GP process in standard VT. The synthesized design is also placed and routed with the Synopsys ICC compiler. We then simulate and verify the design with the Synopsys VCS, and estimate the power consumption with Synopsys Prime-Tame PX based on the simulated Value Change Dump (VCD) file.</p><p>C simulator. Due to the slow simulation speed of the VCS, it is extremely time-consuming to run large-scale datasets on the verilog design of PuDianNao. To alleviate this problem, we implemented an in-house cycle-by-cycle C simulator of PuDianNao, and carefully calibrated it to the verilog design and TSMC library on small-scale datasets with &lt;0.3% performance errors and &lt;7% energy errors across all scenarios. The simulator allows a memory band- width of up to 250GB/s, and is used to estimate the performance of PuDianNao on large-scale datasets.</p><p>Benchmarks. We evaluate PuDianNao and baseline processor on 7 ML techniques, including k-NN, k-Means, deep neural network, linear regression, support vector machine, naive bayes, and classification tree (ID3 <ref type="bibr" target="#b32">[33]</ref>). We use MNIST <ref type="bibr" target="#b24">[25]</ref> and UCI data <ref type="bibr" target="#b0">[1]</ref> as benchmarks of ML techniques, see Table <ref type="table" target="#tab_3">4</ref>.</p><p>Baseline. GPUs have been widely used to support ML in industry due to their speed advantage over SIMD CPU. Hence, we take a modern GPU card (NVIDIA K20M, 3.52 TFlops peak, 5GB GDDR5, 208GB/s memory bandwidth, 28nm technology, CUDA SDK5.5) as the baseline. To validate the performance of the GPU baseline on ML applications, we compare it against a CPU with 256-bit SIMD (Intel Xeon E5-4620 Sandy Bridge-EP, 2.2GHz, 1TB memory, gcc compilation flag "-O3 -ftree-vectorize -march=native") over all benchmarks introduced above. As shown in Figure <ref type="figure" target="#fig_12">13</ref>, our GPU baseline achieves an average speedup of 17.74x with respect to the SIMD CPU implementation. This result complies with two recent investigations which reported GPU to have 15x-49x <ref type="bibr" target="#b37">[38]</ref> and 10x-60x <ref type="bibr" target="#b8">[9]</ref> speedups over SIMD CPU for ML applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section, we present characteristics of PuDianNao after layout, and quantitatively compare PuDianNao against the GPU baseline. However, the area and power consumption of PuDian-Nao are remarkably smaller than those of a modern GPU, by two orders of magnitude. According to the DC and ICC reports, the total area of PuDianNao is 3.51 mm 2 , and the total power consumption is 596 mW . The critical path delay of PuDianNao is 0.99ns, suggesting that the accelerator can work at 1GHz frequency. The concrete area/power breakdowns are listed in Table <ref type="table" target="#tab_4">5</ref>. The on-chip buffers consume 62.64% and 31.37% of the total area and power. The combinational logic only consumes 21.97% area and 29.02% power. Among functional blocks, the most area-consuming part is ColdBuf (33.22%). All 16 FUs uses 19.38% area and 35.57% power. The concrete layout of PuDianNao is presented in Figure <ref type="figure" target="#fig_14">14</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance and Energy Consumption</head><p>We compare the performance of PuDianNao with GPU baseline across 13 phases of 7 ML techniques. <ref type="foot" target="#foot_3">4</ref> As shown in  We also compare energy consumptions of PuDianNao and the GPU baseline. As shown in Figure <ref type="figure" target="#fig_16">16</ref>, PuDianNao averagely reduces the energy consumption of the GPU by 128.41x. This observation is not surprising, because PuDian-Nao is equipped with dedicated functional units and on-chip buffers optimized for the ML techniques. The largest energy reduction of PuDianNao is achieved on k-NN (262.20x), which is mainly because PuDianNao has efficiently supported sorting, a frequent and time-consuming operation (each finds top 20 instances out of 60000 reference instances in our experiment) in k-NN. In contrast, the GPU consumes remarkable energy on sorting with its general-purpose functional units. The smallest energy reduction of PuDianNao is achieved on CT(ID3) prediction (50.32x). That is because PuDianNao frequently reconfigures its DMA to support irregular memory accesses (e.g., linked list) for loading components of the ID3 classification tree. The above observations and discussions provide hints for further optimizations of the PuDianNao accelerator.</p><p>We do not factor in the energy/area of memory controller and off-chip memory of PuDianNao, but our energy/area comparison is still rather conservative: PuDianNao uses a 65nm process, while the GPU baseline uses a 28nm process. Although the energy/area reported for the GPU baseline does include the memory controller, this is far from compensating for the area ratio between 28nm and 65nm processes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>Machine learning has a broad application scope, but has to be accommodated by powerful computer systems to achieve high efficiency. Although it is straightforward to run ML techniques on general-purpose CPUs and GPUs (e.g., <ref type="bibr" target="#b17">[18]</ref>), the efficiency is limited, because they spend too many efforts on flexibly supporting diverse application domains. For example, their functional units and memory hierarchies are not specifically designed to support ML applications.</p><p>There have been some successful ML accelerators that manage to implement dedicated hardware to accelerate machine learning. Yeh et al. designed a k-NN accelerator on FPGA <ref type="bibr" target="#b39">[40]</ref>. Manolakos and Stamoulias designed two highperformance parallel array architectures for k-NN <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. There are also many dedicated accelerators for k-Means <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> or SVM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, due to their broad applications in industry. Recent debates on deep learning <ref type="bibr" target="#b23">[24]</ref> even triggers the rebirth of hardware neural network <ref type="bibr" target="#b36">[37]</ref>, a hot topic in 1990s <ref type="bibr" target="#b18">[19]</ref>. In this trend, a number of successful neural network accelerators have been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>While remarkably increasing the energy-efficiency and performance compared with general-purpose processors, each of the above accelerators only accommodates a single technique or technique family (e.g.,neural network). When the ML task shifts from, for example, classification to clustering, or the user favors another ML technique having better accuracy/efficiency, such an accelerator might easily become useless. Recently, Majumdar et al. proposed an accelerator called MAPLE which can accelerate matrix/vector operation and ranking used in five ML technique families (including neural network, SVM, k-means) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, there are still many widely-used ML techniques (e.g., decision tree and naive bayes) whose major computational primitives are neither matrix nor vector operations (e.g., counting, linear interpolation), and such techniques cannot be supported by MAPLE. Furthermore, MAPLE does not excavate the locality properties of tiled ML algorithms, thus its memory bandwidth requirement might become the main bottleneck that prevents MAPLE from achieving high energyefficiency/performance in large-scale ML applications common in industry.</p><p>Unlike previous investigations on ML accelerators which do not simultaneously address (a)computational primitives and (b)locality properties of (c)diverse representative ML techniques, we factor in all three dimensions during the design of PuDianNao accelerator. Although PuDianNao has not yet supported all representative ML techniques, the design methodology may shed some light on developing general-purpose machine learning accelerator in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper we present a machine learning accelerator called PuDianNao, which supports multiple ML scenarios (e.g., classification, regression and clustering) as well as multiple ML techniques (k-NN, k-Means, linear regression, SVM, DNN, naive bayes, classification tree). When executing the seven ML techniques, PuDianNao is 1.20x faster and 128.41x more energy-efficient than the NVIDIA K20M GPU on average. Compared with previous ML accelerators designed for narrow ranges of ML techniques, PuDianNao is more robust when the data characteristics change, or the application scenario is altered, because it provides users a basket of candidate techniques. This is somewhat similar to a common strategy adopted by Wallstreet traders: they usually invest to a basket of currencies to reduce the overall risk. Our future work on this topic includes extending PuDianNao to support some other mature ML techniques, as well as a tape-out of PuDianNao.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Original code of distance calculations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Memory bandwidth requirements for distance calculations of k-NN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tiled code of distance calculations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory bandwidth requirements for distance calculations of k-Means (k = 64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Memory bandwidth requirements for DNN feedforward computation (N a = 16384).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Original code of DNN feedforward computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Tiled code of DNN feedforward computation. Pre-training locally tunes synapses (weights) between each pair of adjacent layers, such synapses then serves as the initial synapses of global training. Pre-training can be done by training Restricted Boltzmann Machines (RBMs)<ref type="bibr" target="#b20">[21]</ref>, in which the most time-consuming steps are iterative feedforward computation and backforward computation (which includes similar operations to feedforward computation)1   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Memory bandwidth requirements for prediction phase of LR (N a = 16384).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Memory bandwidth requirements for kernel matrix computation of SVM (d=32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Average reuse distances of variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Accelerator architecture of PuDianNao.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Implementation of MLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Performance comparison between GPU and SIMD CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>6. 1</head><label>1</label><figDesc>Characteristics after Layout The current version of PuDianNao has 16 MLUs, each MLU can process 16 instance features (dimensions) at each cycle. Each MLU contains 16+16+15+1+1=49 adders, coming from the Counter stage, the Adder stage, the Adder tree stage, the Acc stage, and the Misc stage respectively. Each MLU contains 16+1=17 multipliers, coming from the Multiplier stage and the Misc stage, respectively. With 16 MLUs 3 , PuDianNao can achieve a peak performance of 16 × (49 + 17) × 1 = 1056 Gop/sec at 1GHz frequency, almost approaching the performance of a modern GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Layout of PuDianNao. CM, FU, HB, CB, and OB stand for Control Module, Functional Unit, HotBuf, ColdBuf, and OutputBuf, respectively.</figDesc><graphic url="image-2.png" coords="10,373.33,440.05,126.31,125.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Performance speedup of PuDianNao over GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Energy reduction of PuDianNao over GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training accuracy of ML techniques under different sizes of arithmetic units (normalized to all 32bits).</figDesc><table><row><cell>ML techniques</cell><cell cols="2">accuracy</cell></row><row><cell></cell><cell>all 16bits</cell><cell>32bits&amp;16bits</cell></row><row><cell>SVM</cell><cell>37.7%</cell><cell>98.2%</cell></row><row><cell>k-NN</cell><cell>99.9%</cell><cell>100%</cell></row><row><cell>k-Means</cell><cell>93.9%</cell><cell>100.1%</cell></row><row><cell>LR</cell><cell>78.2%</cell><cell>99.0%</cell></row><row><cell>DNN</cell><cell>99.4%</cell><cell>100.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Instruction format of PuDianNao.</figDesc><table><row><cell>CM</cell><cell></cell><cell cols="4">HotBuf</cell><cell cols="4">ColdBuf</cell><cell></cell><cell></cell><cell cols="5">OutputBuf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FU</cell></row><row><cell>Inst Name</cell><cell cols="2">READ OP</cell><cell>READ ADDR</cell><cell>READ STRIDE</cell><cell>READ ITER</cell><cell>READ OP</cell><cell>READ ADDR</cell><cell>READ STRIDE</cell><cell>READ ITER</cell><cell>READ OP</cell><cell>WRITE OP</cell><cell>READ ADDR</cell><cell>WRITE ADDR</cell><cell>READ STRIDE</cell><cell cols="2">WRITE STRIDE</cell><cell cols="2">READ ITER</cell><cell>WRITE ITER</cell><cell>MLU-1 OP</cell><cell>MLU-2 OP</cell><cell>MLU-3 OP</cell><cell>MLU-4 OP</cell><cell>MLU-5 OP</cell><cell>MLU-6 OP</cell><cell>ALU OP</cell></row><row><cell>CM</cell><cell cols="4">HotBuf</cell><cell></cell><cell cols="4">ColdBuf</cell><cell></cell><cell></cell><cell cols="5">OutputBuf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FU</cell></row><row><cell>k-means</cell><cell>LOAD</cell><cell cols="2">0</cell><cell>16</cell><cell>128</cell><cell>LOAD</cell><cell>16384</cell><cell>256</cell><cell>16</cell><cell>NULL</cell><cell>STORE</cell><cell>NULL</cell><cell>1064960</cell><cell cols="2">NULL</cell><cell cols="2">16</cell><cell cols="2">NULL</cell><cell>16</cell><cell>NULL</cell><cell>SUB</cell><cell>MULT</cell><cell>ADD</cell><cell>NULL</cell><cell>SORT</cell><cell>NULL</cell></row><row><cell>k-means</cell><cell>READ</cell><cell cols="2">0</cell><cell>16</cell><cell>128</cell><cell>LOAD</cell><cell>20480</cell><cell>256</cell><cell>16</cell><cell>NULL</cell><cell>STORE</cell><cell>NULL</cell><cell>1065216</cell><cell cols="2">NULL</cell><cell cols="2">16</cell><cell cols="2">NULL</cell><cell>16</cell><cell>NULL</cell><cell>SUB</cell><cell>MULT</cell><cell>ADD</cell><cell>NULL</cell><cell>SORT</cell><cell>NULL</cell></row><row><cell>k-means</cell><cell>READ</cell><cell cols="2">0</cell><cell>16</cell><cell>128</cell><cell>LOAD</cell><cell>24576</cell><cell>256</cell><cell>16</cell><cell>NULL</cell><cell>STORE</cell><cell>NULL</cell><cell>1065472</cell><cell cols="2">NULL</cell><cell cols="2">16</cell><cell cols="2">NULL</cell><cell>16</cell><cell>NULL</cell><cell>SUB</cell><cell>MULT</cell><cell>ADD</cell><cell>NULL</cell><cell>SORT</cell><cell>NULL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">• • • • ••</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k-means</cell><cell>LOAD</cell><cell cols="2">2048</cell><cell>16</cell><cell>128</cell><cell>LOAD</cell><cell>16384</cell><cell>256</cell><cell>16</cell><cell>LOAD</cell><cell>STORE</cell><cell>1064960</cell><cell>1064960</cell><cell cols="2">16</cell><cell cols="2">16</cell><cell cols="2">16</cell><cell>16</cell><cell>NULL</cell><cell>SUB</cell><cell>MULT</cell><cell>ADD</cell><cell>NULL</cell><cell>SORT</cell><cell>NULL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">• • • • ••</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>An example of k-Means code (f = 16, k = 1024, N = 65536).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Benchmarks and dataset used in this paper.</figDesc><table><row><cell>ML technique</cell><cell>Dataset</cell><cell>Problem Size</cell></row><row><cell>k-NN</cell><cell>MNIST</cell><cell>60000 reference instances,</cell></row><row><cell></cell><cell></cell><cell>10000 testing instance</cell></row><row><cell></cell><cell></cell><cell>784 features, k=20</cell></row><row><cell>k-Means</cell><cell>MNIST</cell><cell>60000 instances, 784 features, k=10</cell></row><row><cell>DNN</cell><cell>MNIST</cell><cell>data size same with k-NN,</cell></row><row><cell></cell><cell></cell><cell>L1=784,L2=L3=L4=L5=4096,L6=10</cell></row><row><cell>LR</cell><cell>MNIST</cell><cell>data size same with k-NN</cell></row><row><cell>SVM</cell><cell>MNIST</cell><cell>data size same with k-NN</cell></row><row><cell>NB</cell><cell>UCI Nursery</cell><cell>12960 instances, 8 features, 5 classes</cell></row><row><cell>Classification</cell><cell>UCI Covertype</cell><cell>522000 training instances,</cell></row><row><cell>Tree(ID3)</cell><cell></cell><cell>59012 testing instances</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Characteristics of PuDianNao layout. PuDianNao outperforms GPU on 6 phases out of the total 13 phases, and the average speedup of PuDianNao over the GPU is 1.20x. The maximal speedup of PuDianNao is achieved on SVM prediction (2.92x) , and an important reason is that PuDianNao provides dedicated linear interpolation functionality to efficiently compute kernel functions. The worst speedup of PuDianNao is achieved on NB prediction (0.37x). This phase needs frequent multiplications of different numbers (conditional probabilities) to obtain the overall posterior probabilities. PuDianNao does not have a large register file as the GPU, thus has to frequently move data between FUs and on-chip buffers, resulting in the observed performance loss. In contrast, the other phase of NB (NB training) does not have such operations, thus PuDian-Nao achieves a 2.22x speedup over the GPU on NB training.</figDesc><table><row><cell>Component</cell><cell>Area</cell><cell></cell><cell>Power</cell><cell>Critical</cell></row><row><cell>or Block</cell><cell>in µm 2</cell><cell cols="2">(%) in mW</cell><cell>(%) path in ns</cell></row><row><cell cols="2">ACCELERATOR 3,513,437</cell><cell></cell><cell>596</cell><cell>0.99</cell></row><row><cell>Combinational</cell><cell cols="2">771,943 (21.97%)</cell><cell cols="2">173 (29.02%)</cell></row><row><cell cols="3">On-chip buffers 2,201,138 (62.64%)</cell><cell cols="2">187 (31.37%)</cell></row><row><cell>Registers</cell><cell cols="2">200,196 (14.23%)</cell><cell cols="2">86 (16.10%)</cell></row><row><cell>Clock network</cell><cell cols="2">40,154 (1.14%)</cell><cell cols="2">143 (23.99%)</cell></row><row><cell>Function Units</cell><cell cols="2">681,012 (19.38%)</cell><cell cols="2">117 (35.57%)</cell></row><row><cell>ColdBuf</cell><cell cols="2">1,167,232 (33.22%)</cell><cell cols="2">78 (16.44%)</cell></row><row><cell>HotBuf</cell><cell cols="2">578,829 (16.47%)</cell><cell cols="2">47 (9.56%)</cell></row><row><cell>OutputBuf</cell><cell cols="2">586,361 (16.68%)</cell><cell cols="2">51 (10.23%)</cell></row><row><cell cols="3">Control Module 481,737 (13.71%)</cell><cell cols="2">127 (21.30%)</cell></row><row><cell>Other</cell><cell cols="2">18,266 (0.52%)</cell><cell cols="2">41 (0.06%)</cell></row><row><cell>Figure 15,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Given two adjacent layers A, B and synapses between them, a feedforward pass computes neuron values of layer B with neuron values of layer A and the synapses, while a backforward pass computes neuron values of layer A with neuron values of layer B and the same synapses. From a computer architecture perspective, they are the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">A support vector is one of the training instances that are nearest to the decision hyperplane of SVM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">When calculating the peak performance, we voluntarily ignore contributions of ALUs, because ALU cannot work with MLU simultaneously.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">4 Many ML techniques have two phases each (training and prediction phases), but k-NN and k-Means only have one phase, and DNN has two different training phases, pre-training and global training.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by the NSF of China (under Grants 61100163, 61133004, 61222204, 61221062, 61303158, 61473275, 61432016, 61472396), the 973 Program of China (under Grants 2015CB358800, 2011CB302500), the Strategic Priority Research Program of the CAS (under Grant XDA06010403), the International Collaboration Key Program of the CAS (under Grant 171111KYSB20130002), a Google Faculty Research Award, the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), the 10,000 talent program, and the 1,000 talent program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://archive.ics.uci.edu/ml/" />
		<title level="m">UC Irvine Machine Learning Repository</title>
				<imprint>
			<date type="published" when="2014-07">July-2014</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An introduction to kernel and nearestneighbor nonparametric regression</title>
		<author>
			<persName><surname>Naomi S Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cart: Classification and regression trees</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><surname>Colla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page">156</biblScope>
			<pubPlace>Wadsworth: Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A massively parallel fpga-based coprocessor for support vector machines</title>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkata</forename><surname>Jakkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murugan</forename><surname>Sankaradass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field Programmable Custom Computing Machines, 2009. FCCM&apos;09. 17th IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithmic trading: winning strategies and their rationale</title>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">625</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Yin Zhang, and Victor CM Leung. Big Data-Related Technologies, Challenges and Future Prospects</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diannao: a smallfootprint high-throughput accelerator for ubiquitous machinelearning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural support for programming languages and operating systems</title>
				<meeting>the 19th International Conference on Architectural support for programming languages and operating systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;14)</title>
				<meeting>the 47th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;14)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">Ueli</forename><surname>Dan C Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1237</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to linear regression and correlation</title>
		<author>
			<persName><forename type="first">Allen L</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neuflow: A runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berin</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Akselrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency versus interpretability of classifications</title>
		<author>
			<persName><surname>Edward W Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hyperspectral images clustering on reconfigurable hardware using the k-means algorithm</title>
		<author>
			<persName><surname>Ac Frery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haglay</forename><surname>De Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Alice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliana</forename><forename type="middle">A</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoel</forename><surname>Loureiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mdas</forename><surname>Eusebio De Lima</surname></persName>
		</author>
		<author>
			<persName><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><surname>Horta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integrated Circuits and Systems Design, 2003. SBCCI 2003. Proceedings. 16th Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast k nearest neighbor search using gpu</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Debreuve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops, 2008. CVPRW&apos;08. IEEE Computer Society Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Overview of neural hardware. Neurocomputers for Brain-Style Processing. Design, Implementation and Application</title>
		<author>
			<persName><forename type="first">Jan Nh</forename><surname>Heemskerk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fpga implementation of k-means algorithm for bioinformatics application: An accelerated approach to clustering microarray data</title>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Hanaa M Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huseyin</forename><surname>Benkrid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><forename type="middle">T</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Hardware and Systems (AHS), 2011 NASA/ESA Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An analysis of bayesian classifiers</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerating neuromorphic vision algorithms for recognition</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Al Maashri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandhini</forename><surname>Chandramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijaykrishnan</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitali</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Design Automation Conference</title>
				<meeting>the 49th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="579" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A massively parallel, energy efficient programmable accelerator for learning and classification</title>
		<author>
			<persName><forename type="first">Abhinandan</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Becchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Srimat T Chakradhar</surname></persName>
		</author>
		<author>
			<persName><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An energy-efficient heterogeneous system for embedded learning and classification. Embedded Systems Letters</title>
		<author>
			<persName><forename type="first">Abhinandan</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><surname>Srimat</surname></persName>
		</author>
		<author>
			<persName><surname>Chakradhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="45" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ip-cores design for the knn classifier</title>
		<author>
			<persName><forename type="first">S Manolakos</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Stamoulias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4133" to="4136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time k-means clustering for color images on reconfigurable hardware</title>
		<author>
			<persName><forename type="first">Tsutomu</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="816" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A heterogeneous fpga architecture for support vector machine training</title>
		<author>
			<persName><forename type="first">Markos</forename><surname>Papadonikolakis</surname></persName>
		</author>
		<author>
			<persName><surname>Bouganis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field-Programmable Custom Computing Machines (FCCM), 2010 18th IEEE Annual International Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="211" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large margin dags for multiclass classification</title>
		<author>
			<persName><forename type="first">Nello</forename><surname>John C Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nips</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="547" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Induction of decision trees. Machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Ross</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bagging, boosting, and c4. 5</title>
		<author>
			<persName><forename type="first">Quinlan</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="725" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><surname>De Rummelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parallel architectures for the knn classifier-design of soft ip cores and fpga implementations</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Stamoulias</surname></persName>
		</author>
		<author>
			<persName><surname>Elias S Manolakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The rebirth of neural networks</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coordinating the use of gpu and cpu for improving performance of compute intensive applications</title>
		<author>
			<persName><forename type="first">George</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Sachetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olcay</forename><surname>Sertel</surname></persName>
		</author>
		<author>
			<persName><surname>Metin N Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umit</forename><surname>Meira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Catalyurek</surname></persName>
		</author>
		<author>
			<persName><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing and Workshops, 2009. CLUSTER&apos;09. IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The lack of a priori distinctions between learning algorithms</title>
		<author>
			<persName><surname>David H Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fpga implementation of knn classifier based on wavelet transform and partial distance search</title>
		<author>
			<persName><forename type="first">Yao-Jung</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Jyi</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiung-Yao</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
				<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
