<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class imbalance revisited: a new experimental setup to assess the performance of treatment methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ronaldo</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
							<email>ronaldo.prati@ufabc.edu.br</email>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centro de Matemática</orgName>
								<orgName type="institution" key="instit1">Computação e Cognição</orgName>
								<orgName type="institution" key="instit2">Universidade Federal do ABC</orgName>
								<address>
									<addrLine>Santo André</addrLine>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Instituto de Ciências Matemáticas e de Computação</orgName>
								<orgName type="institution" key="instit2">Universidade de São Paulo</orgName>
								<address>
									<settlement>São Carlos</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Class imbalance revisited: a new experimental setup to assess the performance of treatment methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9557890EA799DB08352628FF16954CDC</idno>
					<idno type="DOI">10.1007/s10115-014-0794-3</idno>
					<note type="submission">Received: 26 September 2013 / Revised: 17 April 2014 / Accepted: 4 October 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Class imbalance</term>
					<term>Experimental setup</term>
					<term>Sampling methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as "Are all learning paradigms equally affected by class imbalance?", "What is the expected performance loss for different imbalance degrees?" and "How much of the performance losses can be recovered by the treatment methods?". In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data set with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We apply such experimental design in a large-scale experimental evaluation with 22 data set and seven learning algorithms from different paradigms. We also propose a statistical procedure aimed to evaluate the relative degradation and recoveries, based on confidence intervals. This procedure allows a simple yet insightful visualization of the results, as well as provide the basis for drawing statistical conclusions. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5 %) for the most balanced distributions up to 10 % of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20 % for 1 % of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the treatment methods only partially recover the performance losses. On average, typically, about 30 % or less of the performance that was lost due to class imbalance was recovered by these methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition tasks, as it has been extensively documented in the literature with applications such as diagnostics of rare diseases <ref type="bibr" target="#b8">[9]</ref>, fraud detection <ref type="bibr" target="#b26">[27]</ref>, identification of oil spills in satellite radar images <ref type="bibr" target="#b22">[23]</ref> and many others. Literally, hundreds of research papers have reported that imbalanced data lead to performance losses, and some sort of treatments-such as sampling <ref type="bibr" target="#b23">[24]</ref>, cost-sensitive learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, ensembles <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>, among others-are able to improve classification.</p><p>Although the relationship between class imbalance and performance loss is well documented, we argue that it is under-comprehended. For instance, we have been inquired several times by different researchers about "from what distribution can a data set be considered imbalanced?". Although it is a quite naïve question, we were never able to fully answer it, since we are not aware of any definitive study relating performance loss, degree of class imbalance and learning systems.</p><p>Virtually, every paper about class imbalance has the exact same experimental setup. A proposed method is compared against one or two competing methods over a dozen or so data set. Although this experimental setup is reasonable to support an argument that the new method is as good as or better than the state of the art, it still leaves many unanswered questions. For sake of clarity, let us use an example: suppose that for a given application, a classifier over imbalanced data results in 80 % AUC, and after the application of a certain treatment method, we obtain 90 % AUC. It is indeed a significant improvement in classification; however, "were we able to fully recover the performances losses caused by class imbalance?".</p><p>We believe this question is extremely relevant and completely unanswered by the current research in class imbalance. The literature has provided dozens of methods to treat class imbalance, and there is (or will be) no single method able to provide the best performance for all data set. Therefore, given a problem in which we still need to improve the classification performance after the application of a treatment method A, we would like to know whether it is worth seeking for a different method B to apply instead of A, hoping that B will outperform A.</p><p>The reader should have anticipated that these two questions are directly related. If we knew the relationship between imbalance degree and performance loss, we could evaluate when a treatment method has recovered most of the loss. Unfortunately, no existing analysis is able to provide the answers we are looking for every pair of data set and learning system. However, we can answer these questions in terms of expected performance.</p><p>In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods and classifiers. This experimental setup uses real data set with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We divided the evaluation into two parts. The first part consists in inducing a classifier for each class distribution and measuring the performance loss compared to the balanced distribution. We found out that Support Vector Machine is the classifier paradigm that is the least affected by class imbalance, being insensitive to almost all but the most imbalanced distributions.</p><p>The second part consists of applying a treatment method and inducing a classifier for each class distribution. This time we measured the percentage of the performance loss that was recovered by the treatment method. In other words, 100 % represents that the classifier induced after the application of the treatment method obtained the same performance of the classifier induced over (original) balanced data. We used two well-known over-sampling methods, random over-sampling and SMOTE, as well as two SMOTE variations, Borderline-SMOTE and ADASYN. We also used a MetaCost, a general cost-sensitive procedure that can be applied to any learning algorithm. We show that the expected performance recovery for all methods is typically about 30 % or less for the most imbalanced distributions. MetaCost featured negatively among the treatment methods, not being able to recover most of the losses.</p><p>To support the draw of statistical conclusions, we also propose a statistical procedure to evaluate the results. This procedure constructs confidence intervals about the performance loss relative to the balanced class distribution. A confidence interval is a range of values, calculated from the observed data, which is likely to contain the true value at a specified probability chosen by the researcher. Confidence intervals provide information that may be used to test hypotheses, as well as can be visualized in a graph for a simple although flexible and insightful visualization of the results.</p><p>We believe that papers that propose new treatment methods for class imbalance should adopt the proposed experimental setup. The proposed setup allows not just comparing a new method to competing methods, but also to compare the new method to a reference performance provided by the balanced data set.</p><p>This work is organized as follows: Sect. 2 describes the experimental design proposed in this work; Sect. 3 analyses how learning systems belonging to different paradigms perform under different class imbalance degrees; Sect. 4 analyses the ability of two sampling techniques to recover the performance lost due to class imbalance; Sect. 5 presents the proposed statistical procedure based on confidence intervals. Section 6 discusses possible limitations of our approach; Sect. 7 compares the proposed approach to other experimental setups in the literature; finally, Sect. 8 presents our conclusion and suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental design</head><p>Due to the lack of space, we assume the reader is familiar with the class imbalance problem and methods. In the case this assumption does not hold, the literature has several comprehensible surveys, and we recommend <ref type="bibr" target="#b19">[20]</ref>. For the same reason, we are not able to include here all numerical results obtained in our experiments. Therefore, we decided to create a paper Web site <ref type="bibr" target="#b28">[29]</ref> that has detailed results, including tables, data and scripts we have used to calculate and plot the confidence intervals; however, we note this paper is totally self-contained.</p><p>Our experimental design is inspired by the design used in <ref type="bibr" target="#b35">[36]</ref>. The central idea is to generate several training set distributions with increasing degrees of class imbalance while maintaining the training sets with a fixed number of examples. Our training set distributions range from the balanced distribution (denoted as 50/50<ref type="foot" target="#foot_0">1</ref> ) to the imbalanced distributions of 1/99 and 99/1. In order to generate these class distributions, we restricted the training sets to have a total number of instances equals to the number of instances of the minority class. We avoid using extremely imbalanced data set; in particular, the combination of a small data set with a large class imbalance would result in a training set with too few examples. We return to this discussion in Sect. 6, where we comment possible limitations of this work.</p><p>The test sets have the naturally occurring class distributions. We reserved 25 % of the original data as test set using a stratified sample and did not touch this portion of the data until the final evaluation; the remaining 75 % was used as training set, with class distributions manipulated as previously described. In order to reduce variance and increase statistical significance, we repeated the whole process a hundred times using different train and test sample partitions.</p><p>For this specific study, we assembled a database with twenty-two data set. Since we want to promote reproducibility, most of them are public domain benchmark data set available in repositories such as UCI Machine Learning Repository <ref type="bibr" target="#b13">[14]</ref> or used in projects like Statlog <ref type="bibr" target="#b25">[26]</ref>. A few data set are not public domain, but we decided to include them since they are frequently used in studies involving class imbalance. These data set include tumor identification in mammography images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. We also included data set obtained in past research, and we make them publicly available for the first time in the paper Web site.</p><p>We use the area under the ROC curve (AUC) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref> as the main measure to assess our results. Since we chose AUC and also because we want to control the degree of class imbalance, we converted all non-binary-class data set into binary class, associating a positive label to one of the classes (usually the minority one) and aggregating all remaining classes into a negative class. Table <ref type="table" target="#tab_0">1</ref> presents a summarized description of the data set included in our study. The table lists the data set full names, as well as a short identifier used in this paper, the number of instances and attributes, labels associated with the positive and negative classes and attribute class distribution. The data set are listed in increasing order of class imbalance.</p><p>Two data set resulted in two entries each in Table <ref type="table" target="#tab_0">1</ref>, because different classes were used as positive class. For the Letter data set, Letter-a is the variation in which the positive class is the original letter "a" class; and Letter-vowel has the positive class composed by all classes that represent vowels. For the Splice-junction Gene Sequences data set, one entry has the intronexon ("ie") boundaries as positive class and the other has the exon-intron ("ei") boundaries. The final number of data set is 22, considering the four entries generated from these two data set.</p><p>We included at least one representative of each major learning paradigm. We selected C4.5 (decision trees), C4.5Rules (rules extracted from decision trees), CN2 and RIPPER (decision rules), Back-propagation Neural Network (connectionism), Naïve Bayes (Probabilistic) and Support Vector Machines (Statistical Learning). Whenever possible, we used the original implementations of the inducers that is the case for C4.5, C4.5Rules, CN2 and Ripper.</p><p>C4.5 and C4.5Rules are the original implementation provided by Quinlan <ref type="bibr" target="#b30">[31]</ref>. We used the default parameters, with exception of pruning. Several research papers have pointed out that C4.5 pruning is hardly beneficial in imbalanced domains. Therefore, we induced unpruned trees. CN2 is the original implementation provided by Clark and Boswell <ref type="bibr" target="#b7">[8]</ref>; we used it with default parameters. The same occurred to Ripper, which is the implementation provided by Cohen <ref type="bibr" target="#b9">[10]</ref>.</p><p>For Naïve Bayes and Neural Networks, we used the implementation provided by Borgelt <ref type="bibr" target="#b3">[4]</ref>. In order to estimate conditional probabilities, Naïve Bayes uses a frequency table for symbolic attributes and normal distribution for continuous attributes. For Neural Networks, most domains required a simple configuration with no hidden layer, and the learning lasted for 1,000 epochs. For SVM, we used LIBSVM <ref type="bibr" target="#b4">[5]</ref> with radial basis kernel with degree 3. For MetaCost, we drew 50 bootstrap samples, as suggested in <ref type="bibr" target="#b10">[11]</ref>.</p><p>We use this experimental design to answer the questions raised in abstract and introduction. We start discussing the expected performance loss for different learning paradigms in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Class imbalance, performance loss and learning paradigms</head><p>The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. Due to the lack of space, we only present the average results (with the corresponding standard deviation in the following line, between brackets) over all data set. The interested reader can find detailed results in the paper Web site <ref type="bibr" target="#b28">[29]</ref>. Each reported value is a performance loss relative to the balanced distribution, as defined by Eq. 1: </p><formula xml:id="formula_0">L = B -I B (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where B stands for the performance obtained with the balanced distribution and I for the performance obtained with an imbalanced distribution. As previously described, B and I are measured in this work as the area under the ROC curve (AUC).</p><p>At this point, we can review the question "from what distribution can a data set be considered imbalanced?" Technically, the answer is every non-balanced distribution, since most learning systems (except SVM) show some degree of performance loss for every nonbalanced distribution. Obviously, some practitioners might consider small losses insignificant; in that case, the distributions in the range 20/80-80/20 usually present small losses, on average bellow 2 %. Imbalanced distributions above and including 10/90 (and 90/10) tend to have more expressive losses, above 5 %; and the most imbalanced distributions in our study (1/99 and 99/1) had losses around 20 % on average. From the practical standpoint, we can answer the previous question by saying that for most learning systems the losses due to class imbalance start to be significant when the minority class represents 10 % of the data set or less.</p><p>We also posed the following question: "are all learning paradigms equally affected by class imbalance?" As we can see from the results in Table <ref type="table" target="#tab_1">2</ref>, the answer is clearly no. RIPPER is the learning algorithm that was the most affected by class imbalance. In contrast, SVM is very little affected by all but the most imbalanced distributions, obtaining even negative performance losses, i.e., small performance gains compared to the balanced distribution. A possible explanation is that SVM frequently uses few support vectors to determine the separation between classes, as previously observed by Japkowicz and Stephen <ref type="bibr" target="#b20">[21]</ref>. However, SVM still suffer the consequences of severe class imbalance, as noted by Wu and Chang <ref type="bibr" target="#b36">[37]</ref>. The authors conducted an experiment using two data set with class ratios of 10:1 and 10,000:1. In the second data set, the edge separation between classes tended to become more located over the space belonging to the minority class in comparison with the first data set. Therefore, the SVM inducer tended to classify more test cases as belonging to the majority class. In extreme situations, when the number of training examples from the minority class is not enough to characterize the decision space of this class, SVM might classify every instance as belonging to the majority class.</p><p>In the next section, we use the same experimental design to measure the capacity of treatment methods to recover the performance losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Class imbalance, performance recovery and treatment methods</head><p>So far we were able to characterize the expected performance losses for different class distributions and learning systems. These results lead to another question: "how much of the performance losses can be recovered by the treatment methods?" In this section, we use the same experimental design to answer this question.</p><p>A myriad of methods has been proposed to treat class imbalance. Unfortunately, due to the lack of space, we can only include the results of a few of them in this paper. We chose to analyze random over-sampling and SMOTE <ref type="bibr" target="#b5">[6]</ref> for the following reasons: first, our previous experience with sampling methods <ref type="bibr" target="#b0">[1]</ref> shows that these two methods outperform several other (over and under) sampling methods; second, SMOTE has became one of the most popular sampling methods for class imbalance, and it is frequently used as counterpart in empirical evaluations and has several proposed extensions and variations. We also included two of these variations: Borderline-SMOTE <ref type="bibr" target="#b17">[18]</ref> and ADASYN <ref type="bibr" target="#b18">[19]</ref>. Besides the sampling methods, we also evaluated how cost-sensitive learning could help with class imbalances. We chose MetaCost <ref type="bibr" target="#b10">[11]</ref>, a general cost-sensitive procedure that can be applied to any classifier. This choice was motivated by the fact that not all learning algorithms used in our experiments have cost-sensitive implementations, neither is trivial to directly transform them to cope with costs.</p><p>Random over-sampling compensates the imbalanced class distribution by randomly replicating instances from the minority class. SMOTE, on the other hand, tries to accomplish this compensation by introducing synthetic examples. These synthetic examples are created by a linear interpolation between a minority class instance and its nearest neighbors. Borderline-SMOTE and ADASYN are two variations of SMOTE, which tries to focus on specific regions in the feature space.</p><p>Borderline MetaCost wraps a cost-minimization procedure around a learning algorithm. The idea is to draw bootstrap samples from the training set and to learn a (non-cost sensitive) classifier for each sample. For each instance in the training set, the predicted class probability for each class is averaged over all learned classifiers, and this probability is combined with a cost matrix to relabel the instances in the training set. The general idea is that examples with uncertain classification (for instance, with probability score near 0.5) are more likely to be relabeled according to the cost setting. The relabeled training set is then used to (indirectly) train a cost-sensitive classifier.</p><p>These treatment methods were applied in the same experimental setup used in the previous section. We saved all data partitions and applied the treatment methods in the exact same data used to measure the performance loss. The sampling methods were applied to all nonbalanced data set, and new minority class examples were created until the training set became perfectly balanced. For MetaCost, the costs were defined so they also matched a perfectly balanced distribution. Once again, the test sets were not touched, and they keep the naturally occurring class distributions.</p><p>Our assessment measure is the performance recovery, measured as percentage of the performance loss defined by Eq. 1. The idea is to calculate the performance loss for the treated data set in the same way we did in the previous section for the imbalanced data. We include the equation here for sake of clarity:</p><formula xml:id="formula_2">L T = B -T B (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where B stands for the performance obtained with the balanced distribution and T for the performance obtained with the treated data, both measured in AUC.</p><p>In a second step, we calculate the performance recovery as a fraction of the performance loss for treated data over the performance loss for imbalanced data:</p><formula xml:id="formula_4">R = L -L T L<label>(3)</label></formula><p>A simple way to understand Eq. 3 is to think that R = 100 % when L T = 0 %, or in other words, the performance recovery will reach 100 % when there is no performance loss for treated data. In that case, the classifier performance for treated data is the same as the performance for the balanced distribution. In contrast, R = 0 % when L T = L, or in other words, the treated data classifier had the same performance as the one induced with imbalanced data.</p><p>Although the semantics of the recovery measure is straightforward, its interpretation might be hindered in situations of small losses. Small loss values in the denominator of Eq. 3 may lead to high recovery rates due to spurious performance variations by chance. In other words, we can easily achieve a 1,000 % recovery rate if the treatment method obtains a marginal 1 % AUC improvement over a 0.1 % performance loss due class imbalance. In order to avoid large recovery rates due to small losses, we only analyze recovery rates for significant losses. The following tables show the recovery rates for combinations of data set, inducer and imbalance rate that had 10 % or greater performance loss in the experiment of Sect. 3. The interested reader can find all detailed results, including for loss rates smaller than such threshold in the paper Web site.</p><p>We believe this procedure leads to an more focused analysis of the results, avoiding distractions with spurious results. In general, we are interested in situations of higher performance losses, in which it is worth rebalancing the training data. Table <ref type="table" target="#tab_3">3</ref> presents the expected performance recovery for random over-sampling.</p><p>Certainly, the most interesting values are the performance recovery associated with large class imbalance. For the class distributions of 1/99, 5/95 and 10/90 (and their negative class counterparts), we noticed that although some scattered number present recoveries above 50 %, most results are bellow 30 %. In our opinion, these numbers represent a rather modest performance recovery, especially if we consider that random over-sampling frequently provides competitive results with other sampling methods.</p><p>The results also show that not all learning systems are equally benefited by the replication of minority class examples. For example, the performance of Naïve Bayes has negative recovery rates for several class distributions. Apparently, the simple replication of examples does not help to correct the conditional probability estimates made by Naïve Bayes. These results contribute to the common criticism against the early class imbalance research that (inadvertently) considered fair to extrapolate to other inducers the results obtained with C4.5.</p><p>Table <ref type="table">4</ref> presents the expected performance recovery for SMOTE. This method does not replicate examples from the minority class, but interpolates cases of this class in order to expand its decision space. A first observation about SMOTE is that it does not outperform random over-sampling by a large margin for any inducers. There is some improvements in performance in several configurations, although we can also notice that some configurations were the recovery of Random Oversampling is higher. We should note that SMOTE is considerably more computationally expensive than random over-sampling. Therefore, this performance gain obtained with some inducers comes with the cost of additional computational time.</p><p>Tables <ref type="table" target="#tab_4">5</ref> and<ref type="table">6</ref> present results for Borderline-SMOTE and ADASYN, two variations of SMOTE. Inspecting these tables, we can observe that there is no clear advantage of preferring them in spite of SMOTE. Again, there is some improvements in some configurations. How-     ever, we can also notice that some configurations were SMOTE or random over-sampling leads to a higher recovery. Table <ref type="table" target="#tab_5">7</ref> presents results for MetaCost. Overall, the performance of MetaCost is quite poor, when compared to the sampling methods. A possible explanation is that, as MetaCost internally uses the prediction of the classifiers induced over imbalanced data, the costs were not able to compensate from the imbalance.</p><p>In general, we can say that the performance recoveries for both methods were rather modest. Although some scattered results present gains above 50 %, most of the performance recoveries are around or bellow 30 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Using confidence intervals to visualize results and draw statistical conclusions</head><p>The use of performance loss and performance recovery enables the construction of confidence intervals in a consistent way over data set, treatment methods and learning paradigms. Confidence interval provides a range of values within which the population parameter is likely to lie. The general expression of confidence intervals is ρ ± , where ρ is the estimated parameter and defines a range of values (interval) that act as good estimates of the unknown population parameter as a function of the degree of uncertainty the users assumes for the parameter.</p><p>It has been argued recently that confidence intervals are a better approach to statistically analyze classification performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> than reporting p values or the results of null hypothesis significance testing, due to the ability to interpret the confidence interval in a proper way, depending the effects we would like to analyze. These confidence intervals also allow the visualizations of the results in a graph, helping to draw statistical conclusions about the questions posed in this paper.</p><p>To this end, we adopted the pairedCI <ref type="bibr" target="#b14">[15]</ref> package from the CRAN repository<ref type="foot" target="#foot_1">2</ref> to compute the confidence intervals. This package contains functions that can be used to construct parametric and nonparametric confidence intervals for ratios of values. In this paper, we used the nonparametric version <ref type="bibr" target="#b1">[2]</ref> to construct 95 % confidence intervals for performance losses of original data and their counterpart treated by the class imbalance treatment methods.</p><p>The confidence intervals were constructed in three different ways: first, with performance loss for each class distribution over all data set and learning algorithms (Fig. <ref type="figure" target="#fig_3">1</ref>); second, with the performance loss for each learning algorithm, over all data set and class distributions (Fig. <ref type="figure">2</ref>); and third, with performance loss for each combination of learning algorithm and class distribution, over all data set (Fig. <ref type="figure">3</ref>).</p><p>The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the α% level, so it can be used to infer statistical significance at that level. In our setting, if the confidence interval for the performance loss does not include 0 %, then the performance of the classifier/treatment method significantly differs from the balanced distribution. Furthermore, when comparing two classifiers/treatment methods, if the confidence intervals of them do not overlap, then there is significantly statistical difference between the methods.</p><p>To gain some insight into the question about the performance loss for different imbalance degrees, Fig. <ref type="figure" target="#fig_3">1</ref> shows the 95 % confidence intervals for the average performance loss for each class distribution, averaged over all learning algorithms. The confidence interval for the  original (non-treated data) is shown in red marked with a dot, random over-sampling in brawn marked with a triangle, SMOTE in green marked with a filled square, Borderline SMOTE in cyan marked with a plus signal, Adasyn in blue marked with a square, MetaCost in pink marked with an asterisk. Interestingly, the performance of learning algorithms forms an "U" shaped pattern (flipped 90 • clockwise), with performance loss closer to 0 % near balanced distributions, going upward the more unbalanced the class distribution is. Also notice that this pattern occurs for both non-treated and treated data, although the nontreated data (original) diverges more accentuated from the performance from the balanced distribution than the treated data in all but MetaCost case. MetaCost had a poor performance for the most imbalanced configurations. This can be explained by the fact that MetaCost uses the classifiers' outputs from the bootstrap samples, and the pre-defined costs were not able to compensate from the losses for the most severe class imbalanced distributions. Indeed, in several cases, a trivial (majority class) classifier was induced in these extremes, increasing the performance loss rather than diminishing it. This is a known side effect of imbalanced data set in cost-sensitive classification <ref type="bibr" target="#b24">[25]</ref>.</p><p>As discussed in the previous section, this suggests three conclusions. First, class imbalance does harm classification performance, as the performance loss increases whenever the class Fig. <ref type="figure">2</ref> Confidence intervals for average performance loss and recovery for each learning algorithm distribution diverges from the balanced one. Second, the treatment methods do contribute to alleviate the problem, although they are not able to fully recover the performance from the original balanced distribution. It is important to notice that even though the recovery is not close to the maximum, they are significant comparing to the original distribution for the most unbalanced distributions (in all but one case where the class distribution is equal or lower than 10/90 or 90/10 the confidence intervals do not overlap, indicating a significant result at 95 % confidence level). Third, it is difficult to use cost-sensitive learning, particularly MetaCost due to the influence of class imbalance in the cost reweighing process.</p><p>As a final observation, the performance of random over-sampling is surprisingly good compared to more sophisticated methods. For several class distributions, including the most imbalanced ones, it outperforms several competing methods with statistical significance.</p><p>We now turn to the question if all the learning algorithms are equally affected by class imbalance. Figure <ref type="figure">2</ref> shows 95 % confidence intervals for the average performance losses for each learning algorithm averaged over all the class distributions. We use the same color and shape combination as in the previous figure. From this picture, we can see that the most affected learning algorithms are Ripper and C4.5Rules, with average performance losses around 10 and 5 %, respectively. In contrast, these methods are also the most benefitted Fig. <ref type="figure">3</ref> Confidence intervals for average performance loss and recovery for each learning algorithm 123 by the sampling treatment. The sampling methods produce results statistically significant when compared to the original non-treated data, as the confidence intervals do not overlap. MetaCost had a very poor performance with Ripper, statistically worst than the non-treated distribution. The treatment methods also seem to be good for C4.5, although there is no significant differences. Interestingly, the SMOTE variations seem to harm the performance of SVMs and Naïve Bayes.</p><p>Although Figs. 1 and 2 provide insightful information about these two questions, they may conclave some details as they average over all learning algorithms and class distributions, respectively. Figure <ref type="figure">3</ref> is similar to Fig. <ref type="figure" target="#fig_3">1</ref>, but now there is a graph for each inducer separately. This figure shows the 95 % confidence intervals for the average performance loss for each class distribution and for each learning algorithms, averaged over all data set. We use the same color and shape combination as in the previous figures.</p><p>Even though the overall trend similar for all inducers (the performance of learning algorithms forms an "U" shaped pattern (flipped 90 • clockwise), with performance loss closer to 0 % near balanced distributions, going upward the more unbalanced the class distribution is.), there are some interesting particularities. For the original (non-treated) distributions, the divergence from the balanced distribution is much more accentuated for Ripper, followed by C4.5Rules and C4.5. For these three inducers, there are significant differences for the distributions above 30/70 and 70/30. Naïve Bayes, CN2 and the Neural Network have a flatten curve, with a significant loss only for the distributions above 10/90 and 90/10. SVM, on the other hand, only has significant differences for the distributions 1/99 and 99/1. There are indeed distributions which seems to be better than the balanced one (an "negative" performance loss in the graph).</p><p>The treatment methods are beneficial for Ripper and C4.5Rules for almost all class distributions (with a few exceptions for C.45Rules) although, as discussed before, the recovery is only partial. C4.5 also presents some benefit, although it is not possible to indicate significative differences. SMOTE seems not to be beneficial for Naïve Bayes for very skewed distributions (1/99 and 99/1), although it is not possible to detect significative differences. For CN2, Neural Net and SVM, the few improvements are not significant, and in some cases worse than the original (without significant differences). MetaCost performed quite poorly for Ripper, CN2 and the Neural Network. Overall, there is very little difference for the SMOTE and its variations (Borderline-SMOTE and Adasyn).</p><p>A possible explanation for the differences in performance by the different learning algorithms are the way they explore the search space and the mechanism they adopt aiming to avoid overfitting. The most affected learning algorithms, Ripper and C4.5Rules, adopt rule pruning mechanisms in order to improve accuracy and reduce the rule set size. To this end, they use a separated "pruning set" taken from the training data set. Although this strategy may improve the overall accuracy (in the training set), this improvement could be rather artificial as it can be achieved by predicting the most frequent class in detriment of the less frequent, a phenomena well known in the literature <ref type="bibr" target="#b29">[30]</ref>. On the other hand, as stated before, the less affected algorithm, SVM, requires only the properly definition of the support vectors in order to build the models. Only the most severe imbalanced class distributions would have a strong influence in this definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>We reserve this section to discuss some limitations of the proposed experimental setup. The most obvious limitation is that this experimental setup is restricted to binary-class problems.</p><p>Although it is possible to extend the approach to more than two classes, the number of results to be analyzed would increase exponentially. In contrast, binary-class problems are quite common in imbalanced domains, in which frequently the positive class is assigned to a class of interest and the negative class is associated to all remaining objects.</p><p>A less obvious limitation is the size of the training sets. Since we generated several distributions, from the positive class being a 1 % minority class up to the positive class being a 99 % majority class, we had to restrict the size of the training set as the size of the minority class. In order to partially overcome this limitation, we looked for more balanced data set or imbalanced data set with larger number of instances. However, this limitation prevented us to use in our experimental evaluation some imbalanced data set frequently present in imbalanced data papers.</p><p>A possible criticism is that the restricted sizes of the training set might be biasing the results in some way. For instance, some inducers that better deal with smaller data set might be favored. However, notice that we do not report any absolute performance results; all results are relative to the performance obtained with the balanced distribution. In addition, the training sets have a constant number of instances for all class distributions.</p><p>We should note, however, that the restricted size of the training sets may lead to very few minority class examples for the most imbalanced distributions. It is not uncommon to find data set in which the minority class is represented by a dozen or so examples under these class distributions. We must note that this is not a very uncommon situation, and it known as absolute rarity <ref type="bibr" target="#b33">[34]</ref>. The literature has several examples of application domains that had class imbalance of the order of 1:1000, 1:10,000, or superior. In these domains, the learning systems almost invariantly have to deal with similar absolute number of minority class examples.</p><p>Finally, it is important to keep in mind that the results presented in this paper are averaged out over a series of data set. This is because the main point we want to highlight with our proposed methodology is a way to evaluate general trends about learning algorithms and treatment methods. Obviously, results can be very different for particular data set. The proposed method is quite general though and can used for a single data set as well. The paper Web site contains detailed results, including confidence interval plots for each data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>The literature on class imbalance has several experimental papers with large-scale comparisons of learning algorithms or treatment methods. In this section, we compare the experimental setups of these papers to the one used here.</p><p>As we previously observed, our experimental design is inspired by the setup used in <ref type="bibr" target="#b35">[36]</ref>. Weiss and Provost varied the training set class distribution in order to identify which distribution should be used when a limited number of examples is available. They conclude that, when AUC measure is used, although no single class distribution is able to provide the best-performing classifier, the balanced distribution performs well.</p><p>We used the results of Weiss and Provost to elect the balanced distribution as the reference distribution in our experiments. Although the results in <ref type="bibr" target="#b35">[36]</ref> were obtained with the C4.5 decision tree inducer only, our results conform with their results. As the reader can observe in Table <ref type="table" target="#tab_1">2</ref>, the average results for all inducers present positive losses for all non-balanced distributions. The exception is SVM, in which the best distribution was 20/80 with an average improvement of 4.21 % over the balanced distribution. These are average results over all data sets; we invite the interested reader to check the paper Web site for detailed (per data set) results.</p><p>An interesting counterpart to the experimental design of this paper is present in <ref type="bibr" target="#b6">[7]</ref>. In that work, Cieslak and Chawla investigate classifier performance when testing class distributions change substantially. For instance, a disease outbreak may change the class prior probability considerably, making it differ substantially from the class distribution used in training. Therefore, in their experimental setup, a classifier is compared against several test sets with different class distributions.</p><p>Another experimental work is presented in <ref type="bibr" target="#b21">[22]</ref>. <ref type="bibr">Khoshgoftaar</ref>  All these papers, however, report results in terms of absolute performance values. As we argued in the introduction, this approach cannot answer relevant question about the class imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>This paper proposes an experimental design to evaluate the influence of class imbalance in classifiers performance. This experimental design allows to evaluate the performance loss caused by different degrees of class imbalance, as well as to measure the performance recovery obtained by treatment methods. We also proposed a statistical procedure based on confidence intervals to help draw statistical conclusions about the experiments.</p><p>We conducted an experiment to answer some open-ended questions about class imbalance and concluded that all evaluated classifiers are affected by the class imbalance problem. All classifiers, except SVM, present some loss of performance for all class distributions. This loss tends to be more expressive as the classes become more imbalanced. Additionally, SVM seems to be only susceptible to absolute rarity. For SVM, performance loss occurred for the proportions 1/99 and 99/1. In these proportions, the data set have only 1 % minority class cases and, considering the restricted size of the training sets, the absolute number of minority class examples is very limited, characterizing absolute rarity.</p><p>We also made experiments with treated data by sampling methods and a cost-sensitive approach. We measured how much class imbalance performance loss was recovered by artificially rebalancing the data. The over-sampling methods evaluated were able to occasionally recover a significant proportion (between 50 and 60 %) of the performance lost. However, for the majority of the executions, the performance recovery was below 30 %, which can be considered a quite modest recovery rate.</p><p>As future work, we consider to evaluate additional sampling methods as well as other approaches to treat imbalanced data, such as other sampling algorithms, ensembles and costsensitive learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Table 4</head><label>4</label><figDesc>Performance recovery for SMOTE in AUC (percentage) Inducer Class distribution (positive/negative)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Table 6</head><label>6</label><figDesc>Performance recovery for ADASYN in AUC (percentage) Inducer Class distribution (positive/negative)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Confidence intervals for average performance loss and recovery for each class distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>et al. are interested in studying classification performance when one class is rare. In their experimental setup, the positive class is represented by 5, 10, 20 or 40 instances, and the negative class is set up in such a way that the positive class represents from 65 to 1 % of the total number of examples. For instance, when there are 5 positive examples, the negative examples can be as low as 3 examples (65 % positive class) up to 495 (1 % positive class). Khoshgoftaar et al. conclude that the balanced distribution is outperformed by the distributions 2:1 (negative:positive) for 10, 20 and 40 positive examples, and by the distribution 3:1 for 5 positive examples. An interesting feature of the experimental design used in [22] is to allow the analysis of classification performance factorized by class rarity and class distribution. Our experimental design does not allow the same analysis since, for instance, a 1 % class distribution may represent different absolute number of positive examples in different data set. In contrast, our experimental design leads to training sets with fixed number of examples, and therefore, the results are not influenced by the training sets sizes. The Khoshgoftaar et al. design cannot completely factor out the influence of training set sizes. This influence might even help to explain why the most rare configuration (with 5 positive examples) required a 3:1 class distribution when the other less rare distributions required only 2:1. We note that in such design, the most balanced distributions also have smaller training sets sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Data set description</figDesc><table><row><cell>Data set name</cell><cell>Identifier</cell><cell cols="2">#Examples #Attributes</cell><cell>Classes</cell><cell>Classes %</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(num., nom.)</cell><cell>(min., maj.)</cell><cell></cell></row><row><cell>Chess-King-</cell><cell>Kr-versus-kp</cell><cell>3,196</cell><cell>36 (0, 36)</cell><cell>(nowin, won)</cell><cell>(47.77, 52.23)</cell></row><row><cell>Rook versus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>King-Pawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Contraceptive</cell><cell>CMC</cell><cell>1,472</cell><cell>9 (2,7)</cell><cell>(1, others)</cell><cell>(42.73, 57.27)</cell></row><row><cell>method choice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Liver disorders</cell><cell>Bupa</cell><cell>345</cell><cell>6 (6, 0)</cell><cell>(1, 2)</cell><cell>(42.02, 57.98)</cell></row><row><cell>Tokyo SGI server</cell><cell>Tokyo1</cell><cell>958</cell><cell>43 (43,0)</cell><cell>(good, bad)</cell><cell>(36.12, 63.88)</cell></row><row><cell>performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAGIC gamma</cell><cell>Magic</cell><cell>19,019</cell><cell>10 (10,0)</cell><cell>(h, g)</cell><cell>(35.16, 64.84)</cell></row><row><cell>telescope data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Breast cancer</cell><cell>Breast</cell><cell>699</cell><cell>10 (10, 0)</cell><cell>(benign,</cell><cell>(34.99, 65.01)</cell></row><row><cell>Wisconsin</cell><cell></cell><cell></cell><cell></cell><cell>malignant)</cell><cell></cell></row><row><cell>Pima Indians</cell><cell>Pima</cell><cell>768</cell><cell>8 (8, 0)</cell><cell>(1, 0)</cell><cell>(34.77, 65.23)</cell></row><row><cell>diabetes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tic-Tac-Toe</cell><cell>Tic-Tac-Toe</cell><cell>958</cell><cell>9 (0, 9)</cell><cell>(positive,</cell><cell>(34.65, 65.35)</cell></row><row><cell>endgame</cell><cell></cell><cell></cell><cell></cell><cell>negative)</cell><cell></cell></row><row><cell>German credit data</cell><cell>German</cell><cell>1,000</cell><cell>20 (7, 13)</cell><cell>(Bad, good)</cell><cell>(30.00, 70.00)</cell></row><row><cell>Splice-junction gene</cell><cell>Splice-ie</cell><cell>3,176</cell><cell>60 (0, 60)</cell><cell>(ie, others)</cell><cell>(24.09, 75.91)</cell></row><row><cell>sequences-ie</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Splice-junction gene</cell><cell>Splice-ei</cell><cell>3,176</cell><cell>60 (0, 60)</cell><cell>(ei, others)</cell><cell>(23.99, 76.01)</cell></row><row><cell>sequences-ei</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vehicle silhouettes</cell><cell>Vehicle</cell><cell>946</cell><cell>18 (18, 0)</cell><cell>(van, others)</cell><cell>(23.52, 76.48)</cell></row><row><cell>Letter recognition-</cell><cell>Letter-vowel</cell><cell>20,000</cell><cell>16 (16, 0)</cell><cell cols="2">(vowels, others) (19.39, 80.61)</cell></row><row><cell>vowel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pen-based recognition</cell><cell>Pen-digits</cell><cell>10,991</cell><cell>16 (16,0)</cell><cell>(2, others)</cell><cell>(10.41, 89.59)</cell></row><row><cell>of handwritten</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>digits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Page blocks</cell><cell>Page-blocks</cell><cell>5,472</cell><cell>10 (10,0)</cell><cell>(others, text)</cell><cell>(10.22, 89.78)</cell></row><row><cell>classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optical recognition of</cell><cell>Opt-digits</cell><cell>5,619</cell><cell>63 (63,00)</cell><cell>(2, others)</cell><cell>(9.91, 90.09)</cell></row><row><cell>handwritten digits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Landsat satellite</cell><cell>Satimage</cell><cell>6,435</cell><cell>36 (36, 0)</cell><cell>(4, others)</cell><cell>(9.73, 90.27)</cell></row><row><cell>Hoar-frost detection</cell><cell>Hoar-frost</cell><cell>3,043</cell><cell cols="2">236 (200, 36) (positive,</cell><cell>(6.11, 93.9)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>negative)</cell><cell></cell></row><row><cell>Letter recognition-A</cell><cell>Letter-a</cell><cell>20,000</cell><cell>16 (16, 0)</cell><cell>(a, others)</cell><cell>(3.95, 96.05)</cell></row><row><cell>Abalone data set</cell><cell>Abalone</cell><cell>4,176</cell><cell>8 (7,1)</cell><cell>(15, others)</cell><cell>(2.72, 97.28)</cell></row><row><cell>Nursery</cell><cell>Nursery</cell><cell>12,960</cell><cell>8 (8, 0)</cell><cell>(not-recom,</cell><cell>(2.55, 97.45)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>others)</cell><cell></cell></row><row><cell>Microcalcifications in</cell><cell cols="2">Mammography 11,182</cell><cell>6 (6, 0)</cell><cell>(2, 1)</cell><cell>(2.32, 97.68)</cell></row><row><cell>mammography</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Performance loss in AUC (percentage)</figDesc><table><row><cell></cell><cell>99/1</cell><cell>23.23</cell><cell>(11.56)</cell><cell>25.08</cell><cell>(11.74)</cell><cell>16.40</cell><cell>(10.56)</cell><cell>13.80</cell><cell>(9.79)</cell><cell>22.81</cell><cell>(15.42)</cell><cell>29.53</cell><cell>(8.07)</cell><cell>11.77</cell><cell>(16.45)</cell><cell>20.37</cell></row><row><cell></cell><cell>95/5</cell><cell>8.99</cell><cell>(5.58)</cell><cell>11.01</cell><cell>(6.68)</cell><cell>6.54</cell><cell>(6.52)</cell><cell>5.63</cell><cell>(6.69)</cell><cell>6.08</cell><cell>(9.40)</cell><cell>16.61</cell><cell>(9.25)</cell><cell>0.78</cell><cell>(9.94)</cell><cell>7.95</cell></row><row><cell></cell><cell>90/10</cell><cell>4.75</cell><cell>(3.37)</cell><cell>6.71</cell><cell>(4.53)</cell><cell>3.44</cell><cell>(3.87)</cell><cell>3.12</cell><cell>(4.83)</cell><cell>3.09</cell><cell>(7.85)</cell><cell>11.02</cell><cell>(7.72)</cell><cell>-2.18</cell><cell>(11.06)</cell><cell>4.28</cell></row><row><cell></cell><cell>80/20</cell><cell>1.84</cell><cell>(1.73)</cell><cell>2.90</cell><cell>(2.63)</cell><cell>1.50</cell><cell>(1.62)</cell><cell>0.92</cell><cell>(1.98)</cell><cell>1.08</cell><cell>(4.16)</cell><cell>5.47</cell><cell>(5.50)</cell><cell>-3.16</cell><cell>(12.54)</cell><cell>1.51</cell></row><row><cell></cell><cell>70/30</cell><cell>0.88</cell><cell>(1.14)</cell><cell>1.45</cell><cell>(1.53)</cell><cell>0.83</cell><cell>(1.15)</cell><cell>0.32</cell><cell>(1.25)</cell><cell>0.31</cell><cell>(2.00)</cell><cell>2.18</cell><cell>(3.28)</cell><cell>-3.18</cell><cell>(13.25)</cell><cell>0.40</cell></row><row><cell></cell><cell>60/40</cell><cell>0.23</cell><cell>(0.32)</cell><cell>0.48</cell><cell>(0.68)</cell><cell>0.37</cell><cell>(0.77)</cell><cell>0.04</cell><cell>(0.51)</cell><cell>0.10</cell><cell>(0.59)</cell><cell>0.29</cell><cell>(2.23)</cell><cell>-2.93</cell><cell>(13.70)</cell><cell>-0.20</cell></row><row><cell></cell><cell>50/50</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell><cell>(0.00)</cell><cell>0.00</cell></row><row><cell></cell><cell>40/60</cell><cell>0.19</cell><cell>(0.74)</cell><cell>0.35</cell><cell>(0.89)</cell><cell>-0.03</cell><cell>(0.66)</cell><cell>0.05</cell><cell>(0.53)</cell><cell>0.16</cell><cell>(0.60)</cell><cell>0.89</cell><cell>(1.11)</cell><cell>-2.87</cell><cell>(13.91)</cell><cell>-0.18</cell></row><row><cell></cell><cell>30/70</cell><cell>0.63</cell><cell>(0.97)</cell><cell>1.18</cell><cell>(1.54)</cell><cell>0.22</cell><cell>(0.89)</cell><cell>0.64</cell><cell>(0.87)</cell><cell>0.70</cell><cell>(1.31)</cell><cell>2.71</cell><cell>(2.57)</cell><cell>-4.72</cell><cell>(16.02)</cell><cell>0.19</cell></row><row><cell></cell><cell>20/80</cell><cell>1.71</cell><cell>(1.67)</cell><cell>2.84</cell><cell>(2.79)</cell><cell>1.01</cell><cell>(1.60)</cell><cell>1.73</cell><cell>(2.02)</cell><cell>1.64</cell><cell>(3.86)</cell><cell>5.88</cell><cell>(4.06)</cell><cell>-5.50</cell><cell>(16.64)</cell><cell>1.33</cell></row><row><cell>Class distribution (positive/negative)</cell><cell>1/99 5/95 10/90</cell><cell>22.74 9.15 4.65</cell><cell>(9.69) (5.75) (3.94)</cell><cell>24.12 10.98 6.55</cell><cell>(11.45) (7.03) (5.64)</cell><cell>19.79 6.50 3.15</cell><cell>(11.07) (5.91) (2.75)</cell><cell>16.76 7.34 4.69</cell><cell>(9.51) (6.45) (5.32)</cell><cell>25.53 7.07 4.09</cell><cell>(14.75) (9.32) (7.78)</cell><cell>31.83 18.83 12.31</cell><cell>(8.45) (8.04) (5.75)</cell><cell>11.39 0.42 -3.19</cell><cell>(18.09) (11.72) (12.86)</cell><cell>21.74 8.61 4.61</cell></row><row><cell>Inducer</cell><cell></cell><cell>C4.5</cell><cell></cell><cell>C4.5Rules</cell><cell></cell><cell>CN2</cell><cell></cell><cell>Neural</cell><cell>Network</cell><cell>Naïve Bayes</cell><cell></cell><cell>Ripper</cell><cell></cell><cell>SVM</cell><cell></cell><cell>Average</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>-SMOTE calculates the k nearest neighbors for each instance of the minority class. Let m ≤ k be the number of nearest neighbors belonging to the majority class. If m = k, the minority class example is considered noise and is discarded. If m/2 ≤ m &lt; k, the example of the minority class is marked as borderline. Borderline-SMOTE creates synthetic examples by interpolating borderline examples with their neighbors. Thus, the algorithm strengthens border regions, favoring the minority class. ADASYN uses a density distribution as a criterion for deciding the number of synthetic examples generated from each minority class example. This factor is calculated according to the proportion of majority instances among the nearest neighbors of each minority example. ADASYN creates a larger number of synthetic examples for minority class examples that have higher factors, i.e., greater number of majority examples among the nearest neighbors. This strategy tends to create more minority class examples in borderline regions than in internal class regions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Performance recovery for Random Over-sampling in AUC (percentage)</figDesc><table><row><cell></cell><cell>99/1</cell></row><row><cell></cell><cell>95/5</cell></row><row><cell></cell><cell>0/10</cell></row><row><cell></cell><cell>80/20</cell></row><row><cell></cell><cell>70/30</cell></row><row><cell></cell><cell>60/40</cell></row><row><cell></cell><cell>50/50</cell></row><row><cell></cell><cell>40/60</cell></row><row><cell></cell><cell>30/70</cell></row><row><cell></cell><cell>20/80</cell></row><row><cell>Class distribution (positive/negative)</cell><cell>1/99 5/95 10/90</cell></row><row><cell>Inducer</cell><cell>C4.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Performance recovery for Borderline-SMOTE in AUC (percentage)</figDesc><table><row><cell></cell><cell>99/1</cell><cell>17.57</cell><cell>(32.38)</cell><cell>16.46</cell><cell></cell></row><row><cell></cell><cell>95/5</cell><cell>53.53</cell><cell>(32.38)</cell><cell>41.85</cell><cell></cell></row><row><cell></cell><cell>90/10</cell><cell>74.74</cell><cell>(106.34)</cell><cell>52.49</cell><cell></cell></row><row><cell></cell><cell>80/20</cell><cell>-</cell><cell>-</cell><cell>69.11</cell><cell></cell></row><row><cell></cell><cell>70/30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>60/40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>50/50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>40/60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>30/70</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell>20/80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>Class distribution (positive/negative)</cell><cell>1/99 10/90 5/95</cell><cell>6.36 69.07 44.77</cell><cell>(27.35) (63.08) (43.59)</cell><cell>-2.63 55.87 39.56</cell><cell>(56.37) (37.89)</cell></row><row><cell>Inducer</cell><cell></cell><cell>C4.5</cell><cell></cell><cell>C4.5Rules</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Performance recovery for MetaCost in AUC (percentage)</figDesc><table><row><cell></cell><cell>99/1</cell></row><row><cell></cell><cell>95/5</cell></row><row><cell></cell><cell>90/10</cell></row><row><cell></cell><cell>80/20</cell></row><row><cell></cell><cell>70/30</cell></row><row><cell></cell><cell>60/40</cell></row><row><cell></cell><cell>50/50</cell></row><row><cell></cell><cell>40/60</cell></row><row><cell></cell><cell>30/70</cell></row><row><cell></cell><cell>20/80</cell></row><row><cell>Class distribution (positive/negative)</cell><cell>1/99 5/95 10/90</cell></row><row><cell>Inducer</cell><cell>C4.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use the notation X/Y , with X + Y = 100 to denote that for a set of 100 instances, X belongs to the positive class and Y belongs to the negative class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>CRAN (http://cran.r-project.org) is a network of Web servers distributed around the world that store versions of code and documentation for the statistical software R, as well as community contributed packages.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their comments on the draft of this paper. We also thank Nitesh Chawla for providing the Microcalcifications in Mammography data set. This work was funded by FAPESP award 2012/07295-3.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study of the behavior of several methods for balancing machine learning training data</title>
		<author>
			<persName><forename type="first">Geapa</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Confidence limits for a ratio using Wilcoxon&apos;s signed rank test</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="234" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Significance tests or confidence intervals: which are preferable for the comparison of classifiers?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berrar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<ptr target="http://www.ingentaconnect.com/content/tandf/teta/2013/00000025/00000002/art00003" />
	</analytic>
	<monogr>
		<title level="j">J Exp Theor Artif Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</author>
		<ptr target="http://www.borgelt.net/" />
		<title level="m">Christian borgelt web page</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Libsvm-a library for support vector machines</title>
		<author>
			<persName><forename type="first">C-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing pets on imbalanced datasets when training and testing class distributions differ</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia conference on advances in knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rule induction with CN2: some recent improvements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European working session on machine learning</title>
		<imprint>
			<biblScope unit="page" from="151" to="163" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data in surveillance of nosocomial infection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hilario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hugonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geissbhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Med</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="18" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Metacost: a general method for making classifiers cost-sensitive</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classification accuracy comparison: Hypothesis tests and the use of confidence intervals in evaluations of difference, equivalence and non-inferiority</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Foody</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0034425709000923" />
	</analytic>
	<monogr>
		<title level="j">Remote Sens Environ</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1658" to="1663" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuncion</forename><forename type="middle">A</forename></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Confidence intervals for the ratio of locations and for the ratio of scales of two paired samples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Froemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hothorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://cran.r-project.org/web/packages/pairedCI/index.html" />
	</analytic>
	<monogr>
		<title level="m">The Comprehensive R Archive Network</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barrenechea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bustince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part C</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="484" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data sets with boosting and data generation: the databoost-im approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Viktor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B-H</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1007/11538059_91</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on advances in intelligent computing. Lecture notes in computer science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adasyn: adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international joint conference on neural networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The class imbalance problem: a systematic study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="429" to="449" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with limited minority class data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napolitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Folleco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning and applications</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine learning for the detection of oil spills in satellite radar images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="195" to="215" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploratory under-sampling for class-imbalance learning</title>
		<author>
			<persName><forename type="first">X-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on data mining</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="965" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The influence of class imbalance on cost-sensitive learning: an empirical study</title>
		<author>
			<persName><forename type="first">X-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="970" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine learning, neural and statistical classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Ellis Horwood</publisher>
			<pubPlace>New york</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minority report in fraud detection: classification of skewed data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Phua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on graphical methods for classification predictive performance evaluation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geapa</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1601" to="1618" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geapa</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<ptr target="http://sites.labic.icmc.usp.br/ClassImbalanceRevisited/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Paper website</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The case against accuracy estimation for comparing induction algorithms</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="445" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">C4.5: programs for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>Los Altos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Class imbalance, redux</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trikalinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on data mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting algorithms for imbalanced multiinstance datasets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian conference on artificial intelligence</title>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Zilles</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7884</biblScope>
			<biblScope unit="page" from="174" to="186" />
		</imprint>
	</monogr>
	<note>of lecture notes in computer science</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining with rarity: a unifying framework</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning vs. sampling: which is best for handling unbalanced classes with unequal error costs?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zabar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on data mining</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning when training data are costly: the effect of class distribution on tree induction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="315" to="354" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Class-boundary alignment for imbalanced dataset learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced Datasets in international conference on machine learning Ronaldo C. Prati received a B.Sc. degree in Computer Science in 2001 and a M.Sc. and a Ph.D. degrees in Computer Science and Computational Mathematics in 2003 and 2006, respectively, from University of</title>
		<meeting><address><addrLine>São Paulo, campus of São Carlos, São Paulo, Brazil; Santo André, São Paulo, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>His research interests include machine learning, data and text mining, and their applications</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
