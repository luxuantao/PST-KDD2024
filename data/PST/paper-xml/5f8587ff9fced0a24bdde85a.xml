<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory-Augmented Relation Network for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Interconnected Systems Laboratory of Anhui Province</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Interconnected Systems Laboratory of Anhui Province</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueliang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Interconnected Systems Laboratory of Anhui Province</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
							<email>iexumingliang@zzu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhengzhou University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Interconnected Systems Laboratory of Anhui Province</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory-Augmented Relation Network for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413811</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies â†’ Image representations</term>
					<term>Object recognition</term>
					<term>Transfer learning few-shot learning</term>
					<term>semi-supervised learning</term>
					<term>object recognition</term>
					<term>metric-learning</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Metric-based few-shot learning methods concentrate on learning transferable feature embedding which generalizes well from seen categories to unseen categories under limited supervision. However, most of the methods treat each individual instance separately without considering its relationships with the others in the working context. We investigate a new metric-learning method to explicitly exploit these relationships. In particular, for an instance, we choose the samples that are visually similar from the working context, and perform weighted information propagation to attentively aggregate helpful information from the chosen samples to enhance its representation. We further formulate the distance metric as a learnable relation module which learns to compare for similarity measurement, and equip the working context with memory slots, both contributing to generality. We empirically demonstrate that the proposed method yields significant improvement over its ancestor and achieves competitive or even better performance when compared with other few-shot learning approaches on the two major benchmark datasets, i.e. miniImagenet and tieredImagenet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep artificial agents have achieved impressive performance on various computer vision tasks like classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>, object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref> and semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>For classification, deep recognizers have outperformed humans on visual recognition challenge for years <ref type="bibr" target="#b11">[12]</ref>. However, the success hinges on the ability to apply gradient-based optimization routines to high-capacity models when given large-scale training samples as supervision <ref type="bibr" target="#b32">[33]</ref>. When the training samples are scarce, it becomes challenging to learn to recognize new concepts due to overfitting. In contrast, we human can learn new concepts fast with only few examples. The gap here presents few-shot visual recognition task.</p><p>Few-shot visual recognition, also termed few-shot learning, aims to learn novel visual concepts from one or a few labelled instances. It was first introduced by Fei-Fei et al. <ref type="bibr" target="#b3">[4]</ref> in 2003, and has attracted lots of attention ever since. Data augmentation is a straightforward method to tackle the task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref>, but it does not solve the low-data problem. A promising trend is to transfer knowledge from known categories (i.e. seen categories with abundant training examples) to new categories (i.e. novel categories with few examples) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>, simulating the human learning process <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">44]</ref>. To this end, a variety of approaches have been proposed to learn transferable knowledge in various forms, among which metric-learning methods that concentrate on learning transferable feature embedding fall into one of the main strands.</p><p>Given an unlabeled query sample and a limited number of labelled support samples, a metric-learning method first maps all the input samples into a latent space in which similarities between the query sample and each support sample are estimated according to a predefined distance metric, e.g. cosine distance metric <ref type="bibr" target="#b36">[37]</ref> or Euclidean distance metric <ref type="bibr" target="#b33">[34]</ref>. It then labels the query sample as same as the support sample that gets the highest similarity score <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>. While promissing, most metric-learning methods merely consider instant pairwise query-support relationships as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b) but fail to explore support-support relationships among the labelled support samples, let alone that among the unlabeled ones. The single-image-estimated 1 features they used for few-shot learning are normally not discriminative enough to bring a considerable performance improvement. Moreover, we doubt the fact that the predefined metric suits few-shot learning best. A predefined metric may tailor feature extractor to generate a latent space that matches this very metric perfectly but is sub-optimal for few-shot learning.</p><p>To address the above problems, we extend <ref type="bibr" target="#b34">[35]</ref> to explicitly explore the relationships between each two samples and propose to propagate information from relevant samples for embedding enhancement. Particularly, we adopt a parameterized relation module to estimate the relationship between two samples. The module takes two samples as input and tells to what extent they belong to the same category. It is trained to produce close relationships for similar samples and distant relationships for dissimilar samples. With this module, we aim to learn a generic distance metric simutaneously when searching for the optimal latent space for few-shot learning. A weighted fully connected relation graph as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(a) then is constructed by applying the learnt metric for pairwise comparison in the working context. We propose to explore this relation graph for embedding enhancement to facilitate few-shot learning. For an instance in the graph, we retrieve its neighbors and perform weighted information propagation to attentively aggregate visual information from the neighbors to enhance its representation. Such an information propagation reduces noise in class representations and expands decision boundaries, actually serving as a manifold smoothing regularization. Additionally, we equip the working context with memory slots which control the number of samples that are available in the graph construction and information propagation. Given its flexible memory capacity, the memory can easily extend to hold unlabeled query samples or unlabeled auxiliary samples, making it generalize well to transductive setting or semi-supervised setting.</p><p>We dub the proposed method Memory-Augmented Relation Network (MRN). Our main contributions are as follows: 1) A generic distance metric is learned for few-shot learning. By designing the distance metric as a parameterized relation module, we learn transferable representations and distance metric end to end. 2) We propose to enhance representations via attentively aggregating information from the neighborhood context. The enhanced representations are shown to be more discriminative. 3) We propose a general memory-based framework for few-shot learning. It comes with tightly coupled structure in which the propagation procedure and the classifier share the same learnable distance metric. 4) We evaluate the proposed MRN on two benchmark dataset, namely miniImagenet <ref type="bibr" target="#b33">[34]</ref> and tieredImagenet <ref type="bibr" target="#b27">[28]</ref>. It achieves great improvements over existing methods in few-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Approaches that aim to tackle few-shot learning can be roughly divided into three groups: meta-learning approaches, metric-learning approaches, and hallucination based approaches. Recently, transductive learning has drawn much attention because of the performance boost. In this section, we briefly review relevant work for each of the groups and point out their relevance to our method. Meta-learning approaches Meta-learning approaches usually utilize optimization-based meta-learning or the learning-to-learn paradigm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> to train a meta-learner for few-shot learning. The meta-learner either learns an optimizer for training another linear classifier <ref type="bibr" target="#b26">[27]</ref> or learns an optimal initialization state for the base classifier <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>, from which fast learning is feasible with few examples. The meta-learner is usually difficult to train due to its complicated memory architectures (e.g. RNNs, recurrent neural networks) and the temporally-linear hidden state dependency <ref type="bibr" target="#b23">[24]</ref>. Besides, an additional fine-tuning on the target few-shot learning task is further required. In contrast to those methods, the proposed MRN can be easily trained end to end from scratch without bothering to introduce any complicated memory architecture, and can adapt to new tasks directly with no need for fine-tuning. Metric-learning approaches Metric-learning approaches mainly aim at learning transferable representations for few-shot learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. To this end, Vinyals et al. <ref type="bibr" target="#b36">[37]</ref> presented matching networks which use an attention mechanism to derive a weighted k-NN classifier, and devised an episodic metalearning mechanism to train the classifier for fast adaptation. Snell et al. <ref type="bibr" target="#b33">[34]</ref> proposed to take the mean representation of support samples in each class as prototype and recognize a query sample according to its square euclidean distances against the prototypes. Li et al. <ref type="bibr" target="#b20">[21]</ref> advanced this image-to-category measure by replacing image-level representation with a bunch of local descriptors.</p><p>To overcome local connectivity, Wu et al. <ref type="bibr" target="#b38">[39]</ref> also exploited local descriptors to deal with inherent local connectivity with a dual correlation attention mechanism. They proposed to concatenate four globally related features derived from cross-correlation attention and self-correlation attention to yield a more representative feature. Different from <ref type="bibr" target="#b38">[39]</ref> that focuses on digging a more representative representation out of one single image, in this work, we propose to enhance the representation of a sample via aggregating information from its neighborhood, i.e. the other samples. Both <ref type="bibr" target="#b38">[39]</ref> and our MRN are descendants of relation network <ref type="bibr" target="#b34">[35]</ref>, a deep model which extends siamese network <ref type="bibr" target="#b17">[18]</ref> by adopting the innovative episodic meta-training mechanism proposed in <ref type="bibr" target="#b36">[37]</ref>. Hallucination based approaches For a recognizer that has a feature extractor and a classifier as two distinctive components, hallucination based approaches propose to improve performance from two aspects: data augmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> and classifier weights hallucination <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44]</ref>. The data augmentation approaches commonly focus on expanding intra-class variations by hallucinating new training samples. The classifier weights hallucination approaches, on the other hand, propose to directly set the weight vector for a novel category in the classifier. Such approaches are inspired by the close relationships between classifier weights and the feature representations associated with the same category produced by the feature extractor. Though straightforward, the imprinting process surprisingly provides an instant good classification performance on novel categories and an initialization for future fine-tuning. Transductive learning In few-shot learning, the recognition of each query sample is carried out independently one by one with only a few labelled support samples. Transductive few-shot learning proposes to feed all the query samples once at a time and predict them as a whole. In this setting, the relationships among all samples, both labelled and unlabeled ones, can be considered to improve performance. For example, Liu et al. <ref type="bibr" target="#b22">[23]</ref> presented a model that utilizes the entire query set for transductive inference, which propagates labels from labelled samples to unlabeled ones based on visual affinities. Other representatives are <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref>. In <ref type="bibr" target="#b39">[40]</ref>, Ye et al. trained an attention-based transformer to transform task-agnostic embeddings to task-specific embeddings for few-shot learning. The proposed FEAT selects relevant instances and combines their transformed embeddings to obtain a new task-specific embedding. Li et al. <ref type="bibr" target="#b19">[20]</ref> followed the feature propagation idea and proposed to aggregate information from neighborhood in a tree graph. Likewise, we also propose to aggregate information from the neighborhood context for representation enhancement. The key difference between <ref type="bibr" target="#b19">[20]</ref> and our MRN is that we extend <ref type="bibr" target="#b34">[35]</ref> to jointly learn a generic distance metric and update representations with information from samples selected by this very distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>A few-shot learning task T is generally composed of two sets of instances, the support set</p><formula xml:id="formula_0">S = {(x s i , y s i )} N S</formula><p>i=1 that contains N S labelled instances and the query set</p><formula xml:id="formula_1">Q = {x q i } N Q i=1 which contains N Q unlabeled instances.</formula><p>For simplicity, we consider the well-organized K-shot C-way few-shot setting, where the support set S is prepared by sampling K instances from each of the C categories. Instances in query set Q are sampled from these C categories as well but must share no individual instance with S, i.e. S âˆ© Q = âˆ…. Few-shot learning aims to classify each unlabeled instance in Q with labelled instances in S as supervision.</p><p>Intuitively, one can train a classifier on the support set in hope that it generalizes well to the query set. Unfortunately, this straightforward proposal suffers from severe overfitting due to the data scarcity problem. The community usually resorts to an auxiliary meta-train set D base to learn transferable knowledge to improve generalization on Q. The set D base contains abundant labelled examples from C base categories and has a disjoint label space w.r.t. the target few-shot learning task, i.e. C base âˆ© C = âˆ…. An effective way to exploit this meta-train set is to perform episodic meta-training with a large amount of sampled training episodes, as proposed in <ref type="bibr" target="#b36">[37]</ref>.</p><p>To set up a training episode T , we first randomly sample C categories from C base . Then, for each of the C categories, we randomly sample K labelled examples to serve as the support set Åœ. A fraction of the remainder examples of the C categories are selected to act as the query set Q. Obviously, the training episode is actually mimicking the target K-shot C-way few-shot learning task T . Training a model over tens of thousands of such training episodes with the objective defined in Equation ( <ref type="formula">1</ref>) would yield one model that generalizes well when applied to tasks that contain novel categories. (1)   In Equation <ref type="bibr" target="#b0">(1)</ref>, Î¸ are the parameters of the model, P Î¸ (y|x, Åœ) denotes the probability of sample x being from category y, and R(Î¸ ) is the standard regularization. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, we adopt the episodic meta-training strategy in this work.</p><formula xml:id="formula_2">Î¸ = arg max Î¸ E T 1 | Q | (x,y)âˆˆ Q log P Î¸ (y|x, Åœ) + R(Î¸ )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>We present a memory-augmented relation network (MRN) that has the pipeline as illuastrated in Figure <ref type="figure">2</ref> to solve few-shot learning problem. It basically consists of four components: a feature embedding function f Ï† : R W Ã—H Ã—3 â†’ R d for mapping similar images to nearby points in latent space, an information aggregation component together with an episodic memory for representation enhancement, a relation module Ð´ Ï• : R d â†’ R that learns to tell whether two images are from the same category, and a metric-based classifier for making the final predictions. In the following sections, we explain the information aggregation component and the metricbased classifier in detail and give a brief description of the relation module. The embedding function f Ï† is left untouched because MRN is capable of working with all possible backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Metric-based Classifier</head><p>We first introduce our parameter-free metric-based classifier in order to make a clear impression. The classifier, in one word, takes an unlabeled query image and a bunch of labelled support images as input, and goes through an image-to-category measure to assign the query image to the category that it most likely belongs to. Let f = {f 1 , . . . , f | T | } be the feature embeddings of instances in a given  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-shot C-way training episode T , it imposes the cross entropy loss</head><formula xml:id="formula_3">L i = âˆ’ C t y i,t log( exp(âˆ’D C (f i , c t )) C j exp(âˆ’D C (f i , c j )) )<label>(2)</label></formula><p>where c t represents the t-th class centroid that is computed as the mean value of all support images in the category as defined in <ref type="bibr" target="#b33">[34]</ref>, y i,t is the index label with y i,t = 1 if the image f i belongs to the t-th category and y i,t = 0 otherwise, and D C denotes a certain distance metric. Likewise, let f = {f 1 , . . . , f | T | } be the feature embeddings of instances in the target K-shot C-way few-shot learning task T , in the test phase, it labels each query image according to its distance to the class centroids as defined in Equation <ref type="formula" target="#formula_4">3</ref>.</p><formula xml:id="formula_4">Å·i = arg min t D C (f i , c t ), t = 1, 2, . . . , C<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Embedding Enhancement within Episodic Memory</head><p>The metric-based classifier highly relies on discriminative representations to predict correct labels. However, in few-shot learning, the embedding function is usually not sufficiently trained to model representative semantics in the training data. When feeding these raw features into a metric-based classifier, it leads to inferior performance. To alleviate the problem, we propose to enhance a sample by adding extra information propagated from other similar or relevant samples to its representation. Specifically, when working with episodic meta-training, for each image x i in a training episode T = Åœ âˆª Q, we select its k nearest neighbors A i âˆˆ R k Ã—d from available samples M âˆˆ R mÃ—d and update its feature embedding via gathering information from these neighbors as in Equation <ref type="formula" target="#formula_5">4</ref>that followed.</p><formula xml:id="formula_5">f i = Î»f i + (1 âˆ’ Î») f j âˆˆA i w i, j f j , f i A i (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">w i, j = exp(âˆ’D G (f i , f j )) f k âˆˆA i exp(âˆ’D G (f i , f k )) , iff f j , f k âˆˆ A i<label>(5)</label></formula><p>where f i denotes the representation of the instance x i , w i, j is the coefficient that controls information flow during the aggregation as defined in Equation <ref type="formula" target="#formula_7">5</ref>, D G represents some distance metric, and the hyperparameter Î» indicates the proportion of information preserved to avoid disturbance from neighborhood.</p><p>The information propagation could be naturally simulated by message passing in graph. We therefore organize all instances {f Ï† (x i )} âˆª M as a weighted relation graph G = (V , E) as defined in Equation <ref type="formula" target="#formula_8">6</ref>:</p><formula xml:id="formula_8">v i = f i , e i, j = D G (f i , f j ) âˆ€f i , f j âˆˆ {f Ï† (x i )} âˆª M<label>(6)</label></formula><p>where the edge e i, j reflects the visual affinity between sample x i and x j . For each node v i âˆˆ G, we select its k-nearest neighbors based on edge weights and perform the aforementioned weighted information aggregation to update its node embedding, which accordingly updates image representation f i because the two are identical. Theoretically, the information aggregation can be performed iteratively:</p><formula xml:id="formula_9">v d i = AGGREGATE(A dâˆ’1 i )<label>(7)</label></formula><p>where d denotes the current search depth. By increasing d, we can aggregate information from more samples, like the neighbors of neighbors. Consequently, the deeper we go into the graph, the denser the updated embeddings are. However, a big d hurts the classification performance. Imagining the situation where d is big enough, all images collapse to the same point in the latent space which makes the classification impossible.</p><p>Undoubtedly, the number of potentially exploitable instances in M matters in the propagation procedure. In this work, we keep an episodic memory to serve as M and consider a more controllable propagation procedure for embedding enhancement. The memory has an adjustable memory size in case it can hold flexible number instances. As depicted in Figure <ref type="figure">2</ref>, we initialize the memory with features extracted by the embedding function f Ï† , and update the memory whenever the information aggregation changes the feature stored in one memory slot. With the help of the memory cache, our embedding enhancement adapts very easily to transductive fewshot learning or semi-supervised few-shot learning by initializing the memory to hold unlabeled samples that we intend to exploit during the weighted information propagation. Different from other work that adopt memory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, the memory in our work lives as long as an episode exists but cannot survive across episodes. When a new episode arrives, the old memory is destroyed and a new memory initialized with new data is created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation Module: Learning to Compare</head><p>We design the relation module as simple as the parameterized CNN model depicted in Figure <ref type="figure" target="#fig_2">3</ref>. Like euclidean distance metric, it takes two input instances and estimates the distance between them. For example, let f Ï† (x i ) and f Ï† (x j ) be the feature embeddings of sample x i and x j , the distance between them can be computed by Equation <ref type="formula" target="#formula_10">8</ref>as follows:</p><formula xml:id="formula_10">d i, j = Ð´ Ï• (O(f Ï† (x i ), f Ï† (x j ))), âˆ€ i, j<label>(8)</label></formula><p>where O(â€¢, â€¢) denotes the preprocessing before pairwise comparison. We use the simple difference operation as defined in Equation <ref type="formula" target="#formula_11">9</ref>for preprocessing:</p><formula xml:id="formula_11">O(f Ï† (x i ), f Ï† (x j )) = f Ï† (x i ) âˆ’ f Ï† (x j ), âˆ€ i, j<label>(9)</label></formula><p>As shown in Figure <ref type="figure">2</ref>, the relation module is utilized as a shared submodule in the information aggregation procedure for relation reasoning and the metric-based classifier for classification respectively. That is:</p><formula xml:id="formula_12">D C = Ð´ Ï• â€¢ O D G = Ð´ Ï• â€¢ O<label>(10)</label></formula><p>We will show in our experiments that the use of relation module in the information propagation and metric-based classifier boosts performance in comparison to that with plain euclidean distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluated and compared our MRN with state-ofthe-art approaches on two few-shot learning benchmark datasets, i.e. miniImagenet <ref type="bibr" target="#b33">[34]</ref> and tieredImagenet <ref type="bibr" target="#b27">[28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Network architecture For a fair comparison with existing methods, we took the widely-used four-layer convolutional network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>  Hyperparameters We estimated the hyperparameters by cross validation on miniImagenet. In all our experiments, we set the neighborhood discovery hyperparameter k = 20, information aggregation hyperparameter/search depth d = 1, and information preservation hyperparameter Î» = 0.2. Namely, we selected the 20-nearest neighbors of an example as its neighborhood, and only propagated information from its 1-order neighbors for embedding enhancement even through information aggregation from neighborhood with arbitrary depth is possible. After the aggregation procedure in Equation <ref type="formula" target="#formula_5">4</ref>, an embedding consists of 20 percents of old information and 80 percents of new information aggregated from its neighborhood. Evaluation We batched 15/10 query images per category in each episode for evaluation in 1-shot/5-shot 5-way classification. In all settings, we conducted few-shot classification on 1000 randomly sampled episodes from the test set and reported the mean accuracy together with the 95% confidence interval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Methods</head><p>We mainly compare with state-of-the-art methods that concentrate on representation learning or metric learnning to demonstrate the efficiency of our proposed MRN. Since we aim to extend Relation Network (RN) <ref type="bibr" target="#b34">[35]</ref> to exploit pairwise relationships for embedding enhancement, RN and its variant PARN <ref type="bibr" target="#b38">[39]</ref> would serve as the direct baselines. We also take TPN <ref type="bibr" target="#b22">[23]</ref>, MNE <ref type="bibr" target="#b19">[20]</ref>, FEAT <ref type="bibr" target="#b39">[40]</ref>, GNN <ref type="bibr" target="#b5">[6]</ref> and MM-Net <ref type="bibr" target="#b1">[2]</ref> as baselines because our MRN shares some commonalities with them. To put it in a nutshell, we all propose to exploit the working context, the labelled instances or the unlabeled instances or them both, to boost few-shot learning in either a transductive manner or a non-transductive manner. Among them, the transductive methods, i.e. TPN <ref type="bibr" target="#b22">[23]</ref>, MNE <ref type="bibr" target="#b19">[20]</ref> and FEAT <ref type="bibr" target="#b39">[40]</ref>, taking advantage of both labelled and unlabeled instances, provide quite strong baselines.</p><p>We additionally provide MRN-Zero and MRN-Euclid as two baselines. MRN-Zero represents the variant that discards the working memory to avoid information propagation. With no memory, it loses the ability to aggregate information from other instances and inevitably falls back to the plain relation work. But MRN-Zero is slightly different from RN in that its relation module fuses two embeddings via differentiation preprocessing before pairwise comparison, rather than the concatenation in RN. MRN-Euclid shares a similar structure with our MRN except that it utilizes euclidean distance metric for relation estimation during the information aggregation procedure instead of the learnt one, which decouples the information propagation from distance metric learning. By introducing MRN-Euclid, we aim to demonstrate the superiority of the proposed MRN that has a compact structure and tightly coupled workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head><p>We validate the effectiveness of the proposed MRN on standard miniImagenet and tieredImagenet datasets, and report the main results in Table <ref type="table">1</ref>.</p><p>Comparison to previous state-of-the-arts As shown in Table <ref type="table">1</ref>, MRN achieves 57.83%/71.13% classification accuracy for 1-shot/5shot 5-way tasks on miniImagenet, and achieves 62.65%/74.20% classification accuracy for 1-shot/5-shot 5-way tasks on tieredImagenet. It outperforms non-transductive methods, such as ProtoNet, RN, GNN, DN4 and PARN, by a large margin on both datasets, which comfirms that exploring the working context by explicitly aggregating visual information from unlabeled instances benefits few-shot learning a lot. Thus the effectiveness of our MRN is proved. When compared to transductive methods, MRN largely outperforms TPN and its high-shot variant in both 1-shot and 5-shot learning on two datasets, and compares favorably with FEAT in 1-shot setting. MNE provides a strong baseline that is better than other previous work. In comparison, the performance of MRN is inferior to that of MNE on miniImagenet. But, our MRN gets more than 2%/0.5% improvement over MNE in 1-shot/5-shot setting on tieredImagenet and achieves state-of-the-art performance. It should be noted that the MRN tends to achieve higher improvements or better results on tieredImagenet than on miniImagenet. We conjecture this is primarily due to there existing more categories and more training examples in the training split of tieredImagenet dataset which in turn presents richer intra-class variations that facilitates model training. We also observe that MRN consistently makes more gains for 1-shot learning in comparison to its 5-shot counterpart, which indicates that the proposed embedding enhancement in Section 4.2 is more helpful when training data is extremely scarce. Comparison among MRN variants Among the three MRN variants, MRN-Zero refuses to propagate information from other instances for embedding enhancement and consequently achieves inferior performance when compared to previous transductive methods. This fits in with expectation because transductive methods consistently outperform non-transductive methods in few-shot learning. When compared to RN, we can observe that it gets competitive results on miniImagenet and better results on tieredImagenet, which demonstrates the effectiveness of differentiation preprocessing used in our relation module as described in Section 4.3. MRN-Euclid outperforms MRN-Zero by a large margin on miniImagenet and tieredImagenet for 1-shot learning, but surprisingly falls behind MRN-Zero on both datasets by around 1.3% for 5-shot learning. We conclude this is mostly due to its decoupled workflow where euclidean distance is adopted for relation estimation during embedding enhancement and decouples the embedding enhancement from distance metric learning. In contrast, MRN consistently gets improvements, demonstrating the superiority of our compact model. Visualization We visualize the similarity matrices learned by RN and our MRN under 5-shot 5-way setting on miniImagenet. In detail, we randomly sampled a 5-shot 5-way task from the test split. For each category in the task, we select 10 query instances as in the training phase. Then all instances are fed into the target models for generating the matrices. Given the feature representations extracted by the backbone, RN computes its similarity matrix based on the pairwise similarities estimated by the learnt distance metric. Our MRN performs weighted information propagation as described in Section 4.2 to enhance representations right before it estimates the similarities. We visualize the 75 Ã— 75 matrices in Figure <ref type="figure" target="#fig_3">4</ref>. It can be seen that MRN generates a matrix that is much closer to the ground truth. We also sampled a 1-shot 5-way task with 59 query images per each category, extracted the features and visualized them using  <ref type="table">1</ref>: 5-way classification accuracies of the proposed MRN and state-of-the-art methods on miniImagenet and tieredImagenet, with 95% confidence intervals. Approaches fall into differented groups are separated, and top results are highlighted. C+: trained with higher way, K+: trained with higher shot, â€ : transductive method, -: not reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>To understand MRN better, we carried out several controlled experiments to examine how each part affects the final performance.</p><p>Sensitivities to hyperparameters The number of neighbors, i.e. the neighborhood discovery hyperparameter k, is vital for information propagation. In Figure <ref type="figure">6</ref>(a), we investigate the setting of k. It can be observed that MRN with k &gt; 0 always surpass MRN-Zero (the baseline model or the plain relation network with k = 0). We conclude that embedding enhancement by aggregating information from the neighborhood helps. But, we also notice that the gain drops when too few (k &lt; 10) or too many (k &gt; 30) neighbors are exploited. This is mainly because a tiny neighborhood can only provide limited amount of information and a huge neighborhood imposes irrelevant information which makes it difficult for our MRN to further boost performance.</p><p>In Figure <ref type="figure">6</ref>(b), we compare different values of d, the information aggregation hyperparameter used in Equation 7 that controls how deep into the relation graph we go. By increasing d, we iteratively collect more information from less relevant instances. From the results, we can see that the performance tends to decrease as the search depth d increases. For 5-shot classification, the performance of MRN with d = 3 is even worse than that of the plain relation network MRN-Zero with d = 0, though it still achieves superior performance for 1-shot classification. Impact of information preservation In Equation <ref type="formula" target="#formula_5">4</ref>, we set the hyperparameter Î» for information preservation. A smaller Î» means more information aggregated from the neighborhood, and correspondingly a bigger Î» means more information from the instance itself. To see how the performance changes along with Î», we conducted sensitivity experiments as shown in Figure <ref type="figure">6(c</ref>). As can be seen from the results, the performance drops slightly in 5-shot setting and drastically in 1-shot setting when less and less information is aggregated from the neighborhood. However, our MRN is able to perform neck to neck with MRN-Zero even when Î» = 0.99 in which situation nearly no information is aggregated. Quite unexpectedly, we find that the MRN outperforms MRN-Zero when only one percent of the original information is preserved, with 99 percents of the information aggregated from the neighbors. This indicates the fact that the proposed MRN is effective enough to discriminate relevant examples from irrelevant ones and borrow information from the relevant examples to form more discriminative embeddings.  Effectiveness of weighted embedding aggregation General aggregation strategies like mean-pooling and max-pooling can be used in the information aggregation procedure. In Table <ref type="table" target="#tab_5">2</ref>, we compared the proposed weighted embedding aggregation with mean-pooling and max-pooling feature aggregation methods. MRN-mean is the model that employs mean pooling aggregation strategy during the information aggregation procedure. MRN-max denotes the one that uses max pooling aggregation strategy. The performance drops drastically for both MRN-mean and MRN-max. For MRN-mean, aggregating information equally from all neighbors hurts performance because the aggregation imposes too much irrelevant information. For MRN-max, the performance drop results from information loss when only the max values are selected and merged into the target representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we tackled few-shot learning problems from the perspective of metric learning. Different from previous work that adopt a predefined metric, such as euclidean distance or cosine distance, in the metric-based classifier for similarity measurement, we learn a generic distance metric to compare images for the same purpose. We further propose to propagate information from relevant instances in working context based on their visual affinities for embedding enhancement. It is empirically demonstrated that the learnt distance metric suits few-shot learning better than the predefined ones and the enhanced embeddings are more discriminative which in turn boosts performance. We found that the proposed MRN benefited greatly from its tightly coupled workflow in which a shared learnable distance metric was adopted for embedding enhancement and classification simutaneously. MRN achieved state-of-the-art results on two benchmark datasets. For future work, we plan to extend MRN to deeper models. We also notice that relation propagation is a promissing direction and will study it as a supplement to our visual information propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Major motivation. Conventional metric learning methods for few-shot learning focus on the query-support relations in (b) to recognize a query sample but ignore the support-support relations in (a). We notice that explicitly exploiting these relations results in more discriminative features, as shown in (c), which in turn improves performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of our relation module. It consists of two convolutional blocks and two fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Similarity matrices under 5-shot 5-way setting on miniImagenet. From left to right: RN, our MRN and the ground truth. A brighter color indicates a higher estimated similarity score.</figDesc><graphic url="image-31.png" coords="7,54.60,369.89,65.58,65.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: t-SNE visualization under 1-shot 5-way setting on miniImagenet. From left to right: (a) RN, (b) MRN before propagation and (c) MRN after propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure2: The overall framework of the proposed MRN model. In this illustration, we present a 2-shot 2-way problem as an example. Circles with solid line denote nodes corresponding to support images, and the ones with dashed line are the query images to be classified. Two different colors represent two categories. For brevity, we leave the edges in graph not drawn. The distance between two nodes reflects their relationship. Please also notice we illustrate two times of information aggregation</figDesc><table /><note>(i.e. d = 2) just in order to demonstrate the feasibility of iteratively applying this aggregation process many times (i.e. d &gt; 1). The final d used in our experiments is determined via cross-validation. Best viewed in color.on each image for network training:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The dataset contains more than 700,000 images in total. The average number of examples in each class is more than 1200. Each of its 608 classes is derived from 34 high-level categories in Imagenet. The 34 high-level categories are further divided into 20 training (351 classes), 6 validation (97 classes) and 8 test (160 classes) categories. This high-level split strategy provides a more challenging and realistic few-shot setting where the training classes are distinct from the test classes semantically.</figDesc><table><row><cell>miniImagenet The miniImagenet dataset, originally introduced by</cell></row><row><cell>Vinyals et al. [37], is derived from the larger ILSVRC-12 dataset [31].</cell></row><row><cell>It consists of 60,000 color images of size 84 Ã— 84 that are divided into</cell></row><row><cell>100 categories with 600 examples each. We follow [27, 34] to split</cell></row><row><cell>the dataset into 64 base categories for training, 16 novel categories</cell></row><row><cell>for validation, and 20 novel categories for testing, respectively. In</cell></row><row><cell>few-shot learning , the target model is trained on images from the</cell></row><row><cell>64 training categories and evaluated on the 20 novel categories. The</cell></row><row><cell>validation set is used for monitoring generalization performance</cell></row><row><cell>only.</cell></row></table><note>tieredImagenet The tieredImagenet<ref type="bibr" target="#b27">[28]</ref> is another subset of the ILSVRC-12 dataset<ref type="bibr" target="#b30">[31]</ref>, but it has a much larger class cardinality (608 classes) than that of miniImagenet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Conv-4-64 as the basic feature extractor in our model. The backbone contains 4 convolutional blocks. Each block has a 64-filter convolutional layer with kernel size 3, a batch normalization layer, a relu activation layer, and a 2 Ã— 2 max pooling layer. We utilize the relation module depicted in Figure3(as mentioned in Section 4.3) for relation reasoning. The number of filters in each block of the relation module is set to 64 (i.e., H = 64) in order to work with the feature extractor.</figDesc><table /><note>Episodic trainingWe adopt the episodic meta-training strategy proposed in<ref type="bibr" target="#b36">[37]</ref> for rapid learning in our K-shot C-way experiments. In each training episode, besides the K Ã— C support images, the 1-shot 5-way episode contains 15 query images per each of the C sampled categories. The number in the 5-shot 5-way episode is 10 accordingly. It means one episode in total has 15 Ã— 5 + 1 Ã— 5 = 80 training images for 1-shot 5-way experiments. All images are resized to 84 Ã— 84. In the training phase, we also do basic data augmentations like random cropping and horizontal flipping to increase intra-class variations. The model is optimized with Adam<ref type="bibr" target="#b4">[5]</ref> optimizer end-to-end from scratch. The learning rate is initially set to 0.001, and weight decay is set to 1 Ã— 10 âˆ’6 . For experiments on miniImagenet, we trained the model over 200,000 randomly sampled episodes and reduced the learning rate by half for every 50,000 episodes. As for experiments on tieredImagenet, we trained the model over 500,000 randomly sampled training episodes and reduced the learning rate by half for every 100,000 episodes. All our experiments are implemented in Pytorch with a GeForce GTX 1080 Ti Nvidia GPU card.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc><ref type="bibr" target="#b4">5</ref>-way classification results on miniImagenet with different information aggregation strategies.</figDesc><table><row><cell>5-way Acc (%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Key R&amp;D Program of China under Grant 2019YFA0706203, and by the National Natural Science Foundation of China (NSFC) under grants 61722204, 61932009, 61976076 and 61632007. We would like to thank Haoran Zhang and Wei Qin for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory Matching Networks for One-Shot Image Recognition</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00429</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00429" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
				<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18">2018. June 18-22. 2018</date>
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fe-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
				<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning and learning by analogy: Introduction</title>
		<author>
			<persName><forename type="first">Dedre</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating Classification Weights With GNN Denoising Autoencoders for Few-Shot Learning</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00011</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00011" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. June 16-20, 2019</date>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE, 21-30</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edge-Labeling Graph Neural Network for Few-shot Learning</title>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-Learning With Differentiable Convex Optimization</title>
		<author>
			<persName><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01091</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.01091" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. June 16-20, 2019</date>
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memory-Based Neighbourhood Embedding for Visual Recognition</title>
		<author>
			<persName><forename type="first">Suichan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6102" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning</title>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories using dependent gaussian processes</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-Learning with Latent Embedding Optimization</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=BJgklhAcK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PARN: Position-Aware Relation Networks for Few-Shot Learning</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27">2019. October 27 -November 2, 2019</date>
			<biblScope unit="page" from="6658" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions</title>
		<author>
			<persName><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attribute-based transfer learning for object categorization with zero/one training example</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="127" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-shot Learning via Saliency-guided Hallucination of Samples</title>
		<author>
			<persName><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2770" to="2779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to Learn Image Classifiers With Visual Analogy</title>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11497" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
