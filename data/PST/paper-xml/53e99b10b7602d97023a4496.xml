<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation of Color Textures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
							<email>m.mirmehdi@cs.bris.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Maria</forename><surname>Petrou</surname></persName>
							<email>m.petrou@ee.surrey.ac.uk</email>
						</author>
						<author>
							<persName><forename type="middle">M</forename><surname>Mirmehdi</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">M</forename><surname>Petrou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Engineering, Information Technology and Math</orgName>
								<address>
									<postCode>GU2 5XH</postCode>
									<settlement>Guildford, Surrey</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Segmentation of Color Textures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9A2B9A4DF38D82E113ACE838E67ABCC</idno>
					<note type="submission">received 10 May 1997; accepted 8 Dec. 1999.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index TermsÐColor segmentation</term>
					<term>probabilistic relaxation</term>
					<term>perceptual smoothing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐThis paper describes an approach to perceptual segmentation of color image textures. A multiscale representation of the texture image, generated by a multiband smoothing algorithm based on human psychophysical measurements of color appearance is used as the input. Initial segmentation is achieved by applying a clustering algorithm to the image at the coarsest level of smoothing. The segmented clusters are then restructured in order to isolate core clusters, i.e., patches in which the pixels are definitely associated with the same region. The image pixels representing the core clusters are used to form 3D color histograms which are then used for probabilistic assignment of all other pixels to the core clusters to form larger clusters and categorise the rest of the image. The process of setting up color histograms and probabilistic reassignment of the pixels to the clusters is then propagated through finer levels of smoothing until a full segmentation is achieved at the highest level of resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE topics of texture segmentation and color segmenta- tion have attracted the attention of many researchers. Hundreds of papers have been written on the former and certainly dozens on the latter topic. At first sight it might seem trivial to solve the problem of color texture segmentation, as it may appear that the obvious route would be to combine the knowledge gained from the research in the texture area with that gained from the research in the color area. However, there is a fundamental property which characterizes color texture and which has just emerged from the research in psychophysics <ref type="bibr" target="#b0">[1]</ref>: Human perception of color depends on the spatial frequency of the color component. In other words, the perceptual response of the human visual system to a certain part of the electromagnetic spectrum depends on the frequency with which this stimulus is spatially distributed. Thus, colors that appear in a multicolor pattern are perceived differently from colors that form uniform areas. (For example, it has been shown <ref type="bibr" target="#b1">[2]</ref> that any colored pattern with frequency higher than eight cycles per I of visual angle is seen as black.) Zhang and Wandell <ref type="bibr" target="#b0">[1]</ref> proposed a new color system, called SCIE-Lab, which takes into consideration exactly this property of the visual system. These results have important effect on the segmentation of color textures. For example, color attributes which characterize a color texture at a certain resolution may be entirely different from the color attributes that characterize the same texture at another resolution.</p><p>An important characteristic of the human vision system is that it works as a process, with the analysis of a frame relying on a previous grosser analysis. This is achieved either with the help of peripheral vision followed by foveation to the area of interest, or by increasing the physical proximity of the viewed object. The former approach relies on the mechanism of switching sensors (going from the information obtained by the rods to that obtained by the cones). The latter approach, however, relies on using the same sensor, but changing the number of degrees of visual angle the object occupies in the field of view, in other words, changing the spatial frequency a pattern extends to the eye. Both approaches are characterized by causality: The information flows from the coarse level to the finer level of resolution as the coarser analysis precedes the one performed at the finer level.</p><p>The reason we are concerned with the human color perception is because the criteria by which we judge a segmentation to be good or not are subjective. In the absence of a specific application requirement, we expect the segmentation of an image to agree with that performed by our own vision system. Thus, our interest in color texture perception here concerns the way different textures can be perceived as separate homogeneous regions in the preattentive stage of vision.</p><p>Inspired by the above observations, in this paper, we are proposing a mechanism of segmenting color textures, by constructing a causal, multiscale tower of image versions based on perceptual considerations. The reason we call it ªtowerº and not a ªpyramidº is because we do not perform subsampling and, thus, we preserve the same number of pixels at all levels. The levels of the tower are constructed with the help of blurring masks put forward by Zhang and Wandell <ref type="bibr" target="#b0">[1]</ref>, by assuming that the same color-textured object is seen at 1, 2, 3, ... meters distance. Hence, each coarser version of the image imitates the blurred version the human vision system would have ªseenº at the corresponding distance. The analysis of the image starts at the coarsest level and proceeds toward the finest level, just like it would have happened if a person was slowly approaching a distant object. The mechanism with which information is transferred from a coarse to a finer level is probability theory that makes use of causality. We do not advocate that this is actually the mechanism deployed by humans; we use this approach because it is a sound mathematical tool that allows the incorporation of both features and preliminary conclusions that refer to many different levels of analysis.</p><p>The originality of the work presented is twofold: While multiresolution pyramids have been proposed and successfully utilized for several tasks, including texture segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, it is the first time that a multiscale/ multilevel representation of the image which emulates the human color perception, and which, most significantly, takes into consideration the change in spatial frequency of the perceived pattern, is used for segmentation. Second, although the issue of transfer of information from one level of resolution to the next has been tackled by several researchers and probabilistic relaxation has been used in multiresolution pyramid representations of data <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, it is the first time that a probabilistic relaxation theory, appropriate for operating across different levels of scale and exploiting a dictionary of permissible label configurations appropriate for region labeling, as opposed to edge or line labeling <ref type="bibr" target="#b9">[10]</ref>, is developed. We wish to stress here that we are referring to the object-centered approaches where the purpose is to get right the label of a single object given all available information, and not to the message-centered approaches where a global cost function is optimized to obtain the best overall labeling at the (possible) expense of mislabeling a few individual objects. The reader interested in the multiresolution message-centered approaches (including MRFs) is referred, for example, to <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>. In the next section, we shall give a brief literature review of the relevant issues. In Section 3, we shall describe the method by which the perceptual tower of images is created. In Section 4, we shall present our probabilistic framework of propagating information in the causal direction across the levels of the tower and, in Section 5, the application of the probabilistic framework to color texture images will be described. In Section 6, we shall present results of our approach when used to segment several color texture images. We shall compare them with the results obtained by a recently proposed method <ref type="bibr" target="#b17">[17]</ref>. Our conclusions are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LITERATURE SURVEY</head><p>The main consideration of texture perception in computer vision literature has been with the derivation of descriptive features of the underlying texture. For example, Julesz and Bergen <ref type="bibr" target="#b18">[18]</ref> used descriptions such as color, widths, lengths, and orientations of local features, namely textons, to explain differences in artificially generated images. Also, Malik and Perona <ref type="bibr" target="#b19">[19]</ref> provided a comparison of their computational model of human texture perception with psychophysical data obtained in texture discrimination experiments, while Tamura et al. <ref type="bibr" target="#b20">[20]</ref> approximated computationally some basic textural features such as coarseness, directionality, line-likeness, contrast, roughness, and regularity, which correspond to human visual perception. Rao and Lohse <ref type="bibr" target="#b21">[21]</ref> identified three high level features for texture perception: repetition, orientation, and complexity, by applying hierarchical cluster analysis and multidimensional scaling, and by using textures classified by human subjects. Most of these studies did not consider color information and, moreover, their features are useful when viewing a scene from a fixed distance only.</p><p>Segmentation of texture images is a major field of research in computer vision. Textures may be regular or randomly structured and various structural, statistical, and spectral approaches have been proposed toward segmenting them <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. Some specific examples of recent techniques are by Jain and Farrokhnia <ref type="bibr" target="#b25">[25]</ref>, who presented a texture segmentation algorithm focused on a multichannel Gabor filtering approach which is believed to characterize the processing of visual information in the early stages of the human visual system. Multiscale approaches for texture analysis are few and far between. Unser and Eden <ref type="bibr" target="#b26">[26]</ref> extracted texture energy measures from the image and smoothed the output of the extraction filter bank using Gaussian smoothing at different scales. The features in these multiscale planes are reduced, by diagonalizing scatter matrices evaluated at two different spatial resolutions, and thresholded to yield texture segmentation. Matalas et al. <ref type="bibr" target="#b27">[27]</ref> used a B-spline transform in order to obtain images at several smoothing levels to calculate vector dispersion and gradient orientation at different scales. A small disparity function was then applied to segment textures. Won and Derin <ref type="bibr" target="#b28">[28]</ref> proposed an unsupervised segmentation algorithm using a modified maximum a posteriori criterion for maximizing the posterior distribution of the texture model parameters conditioned on observed image data. The texture was modeled by Gaussian Markov Random Fields. Roan et al. <ref type="bibr" target="#b29">[29]</ref> described a method for classification of textured surfaces viewed at different resolutions, i.e., viewed at different scales or distances, while the image size remains constant. They used gray level co-occurrence matrices and the Fourier power spectrum of an unknown texture image, taken at one of several resolutions, to classify it as one of six known textures.</p><p>None of the above approaches is concerned with color textures. On the other hand, there is a vast forum of work on color image segmentation, e.g., <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>. In general, most color texture representation schemes either use a combination of gray level texture features together with pure color features or they derive texture features computed separately in each of the three color spectral channels. For example, Coleman and Andrews <ref type="bibr" target="#b38">[38]</ref> used K-means clustering in each color band and maximized a cluster fidelity parameter for a more psychovisually acceptable segmented image. Song et al. <ref type="bibr" target="#b39">[39]</ref> proposed a two-tier color segmentation technique for random textures in which image pixels are segregated into a large number of clusters in the RGB space, where the noise is assumed to be homogeneous, and then merged to form super-clusters in the perceptually uniform Luv space, where the noise is inhomogeneous. Tan and Kittler <ref type="bibr" target="#b40">[40]</ref> used eight DCT texture features computed from the intensity image and six color features derived from the color histogram of a textured image for classification. Scharcanski and Hovis <ref type="bibr" target="#b41">[41]</ref> built a color codebook as a standard representation of the characteristic colors found in their problem prototypes. The codebook was then deployed to describe the color information in textured images.</p><p>Campbell and Thomas <ref type="bibr" target="#b37">[37]</ref> examined a combination of Gabor filters and low-pass color filters for segmentation of natural scenes. Their system was implemented as a selforganizing feature map which was trained on a large number of hand-segmented outdoor scenes. Another example of a Gabor-based technique is that of Ma and Manjunath <ref type="bibr" target="#b17">[17]</ref> who combined color intensity and Gabor texture features (including magnitude and phase information) characterized by a predictive coding method to detect and separate color texture regions. We will use Ma and Manjunath's method for comparative analysis later, in Section 6, since Gabor filtering is believed to characterize the processing of visual information in the early stages of the human visual system <ref type="bibr" target="#b25">[25]</ref> and this makes it a relevant technique for comparison purposes.</p><p>Markov Random Field (MRF) models have also been used for color texture segmentation. Panjwani and Healey <ref type="bibr" target="#b14">[14]</ref> presented an unsupervised segmentation technique based on MRFs which clustered a color image in the RGB space and made use of the spatial interaction of RGB pixels within each color plane and the interaction between different color planes. Huang et al. <ref type="bibr" target="#b33">[33]</ref> clustered the histogram for each band of a color image and used scale space filtering for a coarse segmentation of the histogram followed by a MRF process for fine segmentation. Krishnamachari and Chellappa <ref type="bibr" target="#b16">[16]</ref> also applied a multiresolution MRF model for image texture analysis. Such models as used in <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b33">[33]</ref>, however, belong to a totally different class of approaches, namely, the message centered approaches. In these approaches, the problem is formulated as one of global optimization with the Markov parameters expressing the local contextual dependencies. Then, one tries to identify the optimal solution on ªaverage,º possibly at the expense of the correct identity of some individual pixels, by optimizing a global consistency function. Our approach belongs to the complementary class, namely that of object centered approaches, where one seeks to make the best possible decision for each individual entity/object/pixel, given all available information. Then, global consistency is achieved through propagation of the information by iterative steps. For the relationship between the message and the object centered approaches, the reader is referred to the work of Stoddart et al. <ref type="bibr" target="#b42">[42]</ref>.</p><p>None of the above approaches or any other color segmentation work known to the authors have taken into consideration the interaction between color and spatial frequency of patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BUILDING THE PERCEPTUAL TOWER</head><p>Before we proceed in this section, it is necessary to clarify the difference between ªmultiresolutionº and ªmultiscaleº in the context of this work. The resolution of an image signifies the area in physical units a pixel corresponds to, e.g., I pixel Q Â Qmm P in the scene. When the same physical object is seen at a different distance, the resolution of the image changes, for example, I pixel Q Â Qm P . At the same time, the number of ªpixelsº the image of the object occupies in the retina reduces. Each pixel now carries the (blurred) information from several other pixels in the finer resolution version. Thus, when one blurs the image to imitate human vision, one should subsequently subsample the image as well. This way, a pyramid of image resolutions is created. We chose not to perform this subsampling, hence we create a tower of images (Fig. <ref type="figure" target="#fig_0">1</ref>) instead of a pyramid. The reason is dual: 1) We like to keep the redundant information in the coarse levels to increase the robustness of the system, 2) we maintain a direct correspondence between the pixels across the resolution/scale levels. As we do not perform subsampling, the sizes of the blurring masks we use become larger and larger in number of pixels as we proceed to compute the coarser levels of the tower. Seen in this way, our approach is multiscale as filters of various scale sizes are employed. Therefore, throughout this paper, we do not distinguish between the terms multiresolution and multiscale.</p><p>The response characteristics of the human visual mechanism are functions of not only the spectral properties of the stimuli, but also of the temporal and spatial variations of these stimuli. When an observer deals with multicolored objects, with fine textures, their color matching behavior is affected by the spatial properties of the observed pattern. Furthermore, the human visual system will experience loss of detail at increasing distances away from the object. It perceives colored textures at a large distance as areas of fairly uniform color, whereas variations in luminance, e.g., at the borders between two textured areas, are still perceived. Therefore, it is necessary to introduce a multiscale smoothing algorithm that smoothes an image according to human perception. Zhang and Wandell <ref type="bibr" target="#b0">[1]</ref> recently studied systematically the color perception of human subjects for different frequencies of spatial color variation. They proposed an algorithm for perceptual smoothing appropriate for evaluating image coding schemes. It is based on measurements in psychophysical studies which showed that discrimination and appearance of small-field or fine patterned colors differ from similar measurements made using large uniform fields. The human eye perceives high spatial frequencies of color as a uniform color instead of being able to separate these colors. An algorithm which takes this into account must smooth the image in luminance and chrominance color planes separately with different filter matrices for the planes. Zhang and Wandell <ref type="bibr" target="#b0">[1]</ref> advocated the use of the opponent color space, which consists of three different color planes, y I , y P , y Q , representing the luminance, the red-green, and the blueyellow planes, respectively. (Similar ideas for opponent spaces not in color only, but also in orientation token space, have been introduced by Papathomas and his coworkers in <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>.) In the y I y P y Q color space, each of the planes is smoothed separately with two-dimensional spatial kernels, defined as sums of Gaussian functions with different values of standard deviation '. The result of this operation is that the luminance plane is blurred lightly, whereas the redgreen and the blue-yellow planes are blurred more strongly. This spatial processing technique is pattern-color separable. Zhang and Wandell's filtered representation was then transformed back to CIE-XYZ and then to CIE-Lab resulting in their Spatial CIE-Lab space, namely SCIE-Lab. They used this space for their coding-error analysis.</p><p>In this application, we transform the image data from the y I y P y Q to the Luv space and use it as input in the ensuing steps of the algorithm. The input RGB image can be transformed into the y I y P y Q opponent color space in one step, but, for clarity, we show this transformation via the XYZ space: </p><formula xml:id="formula_0">I m i w i n i e À x P y P ' P i X Q</formula><p>The values for w i Y ' i which have been determined from psychovisual measurements of color appearance on human subjects are given in Table 1 <ref type="bibr" target="#b0">[1]</ref>. Divisor n i in ( <ref type="formula">3</ref>) is introduced to normalize the sum of the matrix elements of each individual Gaussian kernel before the weighted sum is applied. Divisor m normalizes the sum of the elements of the final matrix to I.</p><p>In their subsequent coding error analysis in <ref type="bibr" target="#b0">[1]</ref>, Zhang and Wandell experimented for an output device resolution of 90dpi ($ Q À R pixels/mm) at a viewing distance of IVin for I of visual angle. We obtained the sizes of the convolution matrices for different distances as follows: Consider Fig. <ref type="figure">2</ref>. For of visual angle, a particular distance away from the scene being viewed results in a viewing area</p><p>x units wide. Given the output device resolution of r pixels/mm, then we can compute the value of x (and, similarly, y) to be used in (3) in pixels, using x r x, i.e.,</p><p>x r tn % IVH X R Some typical convolution kernels are illustrated in Fig. <ref type="figure" target="#fig_2">3a</ref>, Fig. <ref type="figure" target="#fig_2">3b</ref>, Fig. <ref type="figure" target="#fig_2">3c</ref>. For comparison, Fig. <ref type="figure" target="#fig_2">3d</ref> shows the corresponding Gaussian kernel, computed so that it has the same size as the Zhang and Wandell masks with ' being such that, at the cutoff size, its value is I percent of its central value. Once the kernels are applied to the image in the opponent color space, the image can be converted to CIE-Luv, which is a perceptually uniform space and, therefore, more suitable for carrying out color measurements. Fig. <ref type="figure" target="#fig_3">4</ref> shows a real texture collage and its associated smoothed images at varying distances for both perceptual and Gaussian smoothing. As the masks we are using are large for large distances, in order to avoid having an ever increasing border, we extended the image in two ways; we either assumed that it is embedded in a much larger image of black pixels or a much larger image of white pixels. These Fig. <ref type="figure">2</ref>. The visible width x for a particular distance at of visual angle.</p><p>two cases correspond to the physical situations where the viewed object is seen against a black background or against a white background, respectively. The overall color impression that results in the two cases, for both perceptual and Gaussian smoothing, is very different, as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. However, the segmentation results obtained did not differ significantly for either background. Therefore, in all remaining experiments, we adopt the option of padding the image with zeros to deal with the border effects. It is clear from Fig. <ref type="figure" target="#fig_3">4</ref> that the perceptually smoothed images provide a more realistic representation and blurring of an ªobjectº viewed at varying distances than the Gaussian. Most particularly, the Gaussian has vastly mixed and smoothed the color values when convolved with each of the three color channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTISCALE PROBABILISTIC RELAXATION</head><p>The problem of multiscale probabilistic labeling of the input image using a set of perceptually blurred versions of the image can be stated as follows: Let l indicate the levels of coarsening with l IY F F F Y v, representing the levels from full resolution to the coarsest level. Let iY i IY F F F Y x be a pixel and x l i , the associated measurement vector for that pixel at resolution level l. We define a label set Y f3 I Y 3 P Y F F F Y 3 m g, which contains all possible labels of the image for m possible perceptual categories. Thus, each pixel i has label i that can take on values from .</p><p>We wish to choose for pixel i the most probable label i given all the available information. In other words, we wish to set:</p><formula xml:id="formula_1">i rgfmx k i 3 k j x l j Y VjY VlgX S</formula><p>For simplicity and clarity of exposition, we shall restrict ourselves to considering only two successive levels of resolution l and l I. Then, using Bayes's rule, we have:</p><formula xml:id="formula_2">i 3 k j x l j Y x lI j Y Vj i 3 k Y x l j Y x lI j Y Vj x l j Y x lI j Y Vj X T</formula><p>We can expand the terms in the numerator and the denominator by applying the theorem of total probability:</p><formula xml:id="formula_3">i 3 k j x l j Y x lI j Y Vj 3 I XX 3 iÀI 3 iI XX 3 x I 3 I Y P 3 P YXXY i 3 k YXXY x 3 x Yx l j Yx lI j YVj 3 I 3 P XX 3 i XX 3 x I 3 I YXXY i 3 i YXXY x 3 x Yx l j Yx lI j YVj X U</formula><p>The joint probability that appears in ( <ref type="formula">7</ref>) can be factorized as follows:  </p><formula xml:id="formula_4">I 3 I Y P 3 P Y F F F Y x 3 x Y x l j Y x lI j Y Vj x lI I Y x lI P Y F F F Y x lI x j I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x Â I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x X V</formula><p>As we try to emulate perceptual segmentation here, we can imagine that, due to causality, measurements obtained at level l I (the coarser level) cannot possibly depend on measurements obtained at level l. This is because, in a causal model, the measurement values obtained at the coarse level have been obtained before the fine resolution image becomes available, so they can be assumed to be independent of the fine resolution measurements. In our case, of course, we do not have available a sequence of images of increasing spatial resolution and, in practice, we have derived the coarse measurements from the fine ones.</p><p>In a sense, we use the fine resolution image to play the role of the ªviewed sceneº and, from it, we create the sequence of images we need, imitating the way the human vision system would have made the measurements from the scene and created such a sequence. Hence, we make the assumption of the independence of the coarse measurements from the fine ones. Thus, the first factor on the right hand side of ( <ref type="formula">8</ref>) can be simplified as follows:</p><formula xml:id="formula_5">x lI I Y x lI P Y F F F Y x lI x j I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x x lI I Y x lI P Y F F F Y x lI x j I 3 I Y P 3 P Y F F F Y x 3 x X W</formula><p>We also expect that the measurement concerning a certain pixel depends on the identity of that pixel alone and on nothing else. This is the independence assumption which, in practice, is not correct. However, it allows us to separate the factors that contain the contextual information from the factors that contain the intrinsic to the pixel information. It is a commonly made assumption by probabilistic relaxation schemes, e.g., <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, through necessity. The error it introduces is subsequently corrected by the incorporation of the contextual information through some extra iterative steps performed at each resolution level. This is because the independence assumption ignores the influence one pixel exerts on its neighboring pixels. By iterating the solution, we allow the neighboring pixels to influence the label of the pixel under revision and, thus, we effectively correct for the violation of the independence assumption. Therefore, we can further write:</p><formula xml:id="formula_6">x lI I Y x lI P Y F F F Y x lI x j I 3 I Y P 3 P Y F F F Y x 3 x j</formula><p>x lI j j j 3 j j j 3 j j x lI j p j 3 j px lI j Y IH where px lI j is the prior probability of measurements x lI j arising and p j 3 j is the prior probability of label 3 j . Now, consider the second factor on the righthand side of <ref type="bibr" target="#b7">(8)</ref>. We can write it as:</p><formula xml:id="formula_7">I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x x l I j I 3 I Y P 3 P Y F F F Y x 3 x Y x l P Y F F F Y x l x Â 3 I Y P 3 P Y F F F Y x 3 x Y x l P Y F F F Y x l x X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II</head><p>We can further expand the second term on the righthand side of (11) to write:</p><formula xml:id="formula_8">I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x x l I j I 3 I Y P 3 P Y F F F Y x 3 x Y x l P Y F F F Y x l x Â x l P j I 3 I Y P 3 P Y F F F Y x 3 x Y x l Q Y F F F Y x l x Â F F F Â x l x j I 3 I Y P 3 P Y F F F Y x 3 x Â I 3 I Y P 3 P Y F F F Y x 3 x X IP</formula><p>For the same reasons explained earlier, we expect that the measurement obtained for a particular object depends on the identity of the object itself and on nothing else. Thus, all factors on the righthand side of <ref type="bibr" target="#b11">(12)</ref>, except the last one, can be simplified to express dependence only on the identity of the object they refer to. The last factor is the joint probability of a certain label assignment to arise. So, we have:</p><formula xml:id="formula_9">I 3 I Y P 3 P Y F F F Y x 3 x Y x l I Y x l P Y F F F Y x l x j x l i j j 3 j Â I 3 I Y P 3 P Y F F F Y x 3 x X IQ</formula><p>Now, by substituting from ( <ref type="formula">10</ref>) and ( <ref type="formula">13</ref>) in <ref type="bibr" target="#b7">(8)</ref>, we obtain:</p><formula xml:id="formula_10">I 3 I Y P 3 P Y F F F Y x 3 x Y x l j Y</formula><p>x lI j Y Vj j I p j 3 j j 3 j j x lI j px lI j x l j j j 3 j Â</p><formula xml:id="formula_11">I 3 I Y P 3 P Y F F F Y x 3 x X IR</formula><p>Then, upon substitution in <ref type="bibr" target="#b6">(7)</ref>:</p><formula xml:id="formula_12">i 3 k j x l i Y x lI j Y Vj x l i j i 3 k i 3 k j x lI i px lI i j px lI j i 3 k 3 i x l i j i 3 i i 3 i j x lI i px lI i j px lI j i 3 i Y IS where i 3 i I p i 3 i 3 I Á Á Á 3 iÀI 3 iI Á Á Á 3 x j</formula><p>x l j j j 3 j j 3 j j x lI j p j 3 j Â</p><formula xml:id="formula_13">I 3 I Y P 3 P Y F F F Y x 3 x X IT</formula><p>In the above expression, px lI i is independent of the summation indices and cancels in the numerator and denominator. Therefore, <ref type="bibr" target="#b15">(15)</ref> further simplifies to:</p><formula xml:id="formula_14">i 3 k j x l j Y x lI j Y Vj x l i j i 3 k i 3 k j x lI i i 3 k 3 i x l i j i 3 i i 3 i j x lI i i 3 i X IU</formula><p>At the finest resolution, ( <ref type="formula">5</ref>) and ( <ref type="formula">17</ref>) give the final labeling result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION TO COLOR TEXTURE SEGMENTATION</head><p>In the previous two sections, we presented the basic ingredients of our algorithm. The core of the multilevel probabilistic relaxation lies in the implementation of ( <ref type="formula">16</ref>) and ( <ref type="formula">17</ref>) and the estimation of the quantities that appear in them. The method works in a bootstrapping manner to estimate the various quantities needed. Thus, it is wholly unsupervised. We use a K-means clustering algorithm with very large K to initialize the segmentation at the coarsest level. After the first segmentation, the small clusters are merged to form a smaller number of clusters. Each pair of regions with a common border is tested for merging. The average vuv color of the border pixels for each of the two regions is computed. If the Euclidean distance of these two vuv colors is less than a certain threshold, the two regions are merged. The merging process stops when no pair of regions that can satisfy the merging criterion can be found. Fig. <ref type="figure" target="#fig_4">5</ref> shows a schematic representation of the whole approach. In what follows, we shall describe how each quantity that appears in ( <ref type="formula">16</ref>) and ( <ref type="formula">17</ref>) is estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Core Clusters</head><p>The core clusters describe groups of pixels which can be confidently associated with the same region of texture in the image. The core clusters form the basis for setting up the color histograms at different levels. To derive core clusters from the initial clusters, we need to fuzzify the segmentation/classification result obtained at the coarsest initialization level. As this is only a step to help start the iteration process, we are adopting a rather simplistic approach: We first calculate the standard deviation ' of each cluster Y IY XXY g, where g is the total number of clusters. Then, we associate with every pixel a confidence, p i , with which it may be associated with each cluster: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViY VY IV</head><p>where d i P is the squared distance of pixel i from the mean of cluster in Luv color space. Note that this formula has the property of giving a confidence higher than 50 percent to pixels that are closer than one ' from the mean of the cluster to belong to that cluster. Each core cluster is formed from the pixels that can be associated with it with a confidence of at least 80 percent. Fig. <ref type="figure" target="#fig_6">6</ref> shows an example image with both its initial clusters and the subsequently derived core clusters.</p><p>Quantities p i are also used to initialize the values of i 3 i j x lI i which appear in ( <ref type="formula">16</ref>) and ( <ref type="formula">17</ref>). Thus, we set:</p><formula xml:id="formula_15">i 3 j x v i p i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViY VX IW</head><p>At all other levels l `v, these quantities are the probability label assignments computed for each pixel at level l I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prior Probabilities</head><p>The relative sizes of the core clusters are used as measures of the prior probabilities of the cluster labels, i.e., quantity p j 3 j appearing in <ref type="bibr" target="#b16">(16)</ref>. This is based on the observation that the larger clusters will appear most dominant when a texture mosaic is viewed from a large distance and, at the same time, the prior probability of a pixel to belong to each cluster is proportional to the size of each cluster, in the absence of any other information concerning the pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">3D Color Histograms</head><p>The core clusters formed at resolution level l I are mapped back into the image at resolution l and, using the color pixel values in those regions, a three-dimensional color histogram is set up (dynamically) for each region. This provides a statistical characterization for each different texture at each resolution. From these color histograms, the likelihood of a pixel i at smoothing stage l to have label 3 k can be calculated using the color of this pixel. This likelihood is represented by x l i j i 3 k . Note that, this way, the distribution of the features that characterize a texture at each resolution level can be derived.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Q-Function Pattern Dictionary</head><p>Equation ( <ref type="formula">16</ref>) involves a summation over all possible labels of all pixels other than the pixel under consideration. Clearly, such a summation is impossible due to the enormous number of combinations one would have to consider. We prune the number of possibilities by imposing a limit to the number of pixels we shall consider as influencing the labeling of the pixel under consideration. Thus, instead of examining all other x À I pixels, we handle only a subset of them constituting a local neighborhood around the pixel. We restrict this to be a Q Â Q neighborhood. This allows us then to introduce a dictionary of permissible label configurations within each Q Â Q patch. As junctions are rare events in images, in most cases, we have only one or at most two regions present in any Q Â Q patch. Hence, we restrict the entries of the dictionary to be of the form presented in Fig. <ref type="figure" target="#fig_7">7</ref>, where A and B stand for any pair of cluster labels present in the image. All entries of the dictionary are assigned equal probability, thus factor I 3 <ref type="formula">16</ref>) becomes a constant and, therefore, redundant. Label combinations that do not appear in the dictionary have zero probability of existing and, so, they do not enter the summation on the righthand side of <ref type="bibr" target="#b16">(16)</ref>. In view of the above, (16) simplifies to</p><formula xml:id="formula_16">I Y P 3 P Y F F F Y x 3 x in (</formula><formula xml:id="formula_17">i 3 i I p i 3 i ` a d</formula><p>x l j j j 3 j j 3 j j x lI j p j 3 j Y PH where `is the set of all patterns in the dictionary, a is the set containing all possible combinations of two labels where the center pixel has label 3 i , and d is the set of Q Â Q pixel neighborhood entries in the dictionary. An improvement of the Q-function is possible by expanding the pattern dictionary to patterns with more than two different labels. It is also possible to calculate the Q-function for a neighborhood larger than Q Â Q pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>In this section, we present our results on a number of color texture images. All segmentation results are shown using arbitrarily selected colors to highlight different regions. There are three parameters involved in our processing which can significantly affect the segmentation results. We consider these three parameters briefly and then perform experiments on a number of collages of real textures to determine acceptable ranges of values for them. At this stage, the segmentations are evaluated by comparing each result with its ground-truth and recording the error. The values for the three parameters are then fixed and used to perform segmentation on a number of real scenes which are subjectively judged.</p><p>The training database of images consists of PU images of collages of real textures, from natural textures to ceramic tiles and granite stones. They vary in size from IHH Â IHH to PHH Â PHH and are all shown in Fig. <ref type="figure" target="#fig_8">8</ref>. Some of the textures used for producing these collages are from the MIT Media Lab VISTEX database <ref type="bibr" target="#b47">[47]</ref>.</p><p>The fundamental issue in the proposed method is when to stop the multiscale tower growing, i.e., to what ªdistanceº should we smooth the image. Hence, the first parameter (P1) we considered is the distance parameter from which the processing will begin. The range of values tested is between P and IH meters with steps of P meters.</p><p>At the beginning of Section 5 we discussed a merging process, applied after the K-means segmentation: We merge the large number of small clusters found to form a smaller number of more significant clusters. We set K in the K-means processing to be large and control the final result via two parameters (P2 and P3) that influence the post-Kmeans merging. More specifically, each pair of regions whose average vuv differs by less than a threshold (P2) are merged. Then, any region which has fewer pixels than a certain percentage (P3) of the total number of image pixels is ignored and left unclassified. The consequence of this last stage is that the ignored pixels will never form a new region and can only join one of the bigger regions at a finer resolution level. The remaining regions will then be used to form core-clusters prior to the commencement of the relaxation process. The range of values used for P2 are T to IT units (with steps of P units) and for P3 are 5 percent to 15 percent (with steps of 1 percent).</p><p>The range of values chosen for each parameter arises from experience while developing this work, based primarily on the quality of the results obtained. For example, we found that P1 values below 2 generally produce a highly oversegmented result due to the lack of smoothing, while values above 10 perform too much smoothing and cause undersegmentation. Hence, we chose 2-10 as the most appropriate range.</p><p>The total parameter space of P1, P2, and P3 was explored by testing with all combinations of values in Table <ref type="table" target="#tab_1">2</ref> presents the results of all these runs in summary. For each value of a parameter, some statistics of the errors are calculated over all results produced by all values of the other parameters, over all images. For example, for P1 = 6 the statistics presented are calculated over the results of 1,782 experiments. The statistics computed are: the mean error, the mode error, and the median value of the error. Looking at Table <ref type="table" target="#tab_1">2</ref>, we observe that promising values of the parameters are in the following ranges: P1 = 8-10, P2 = 10-12, P3 = 6-9. We performed further tests and investigated these values and their immediate neighbourhoods. Here, we produce some of the more salient results and then draw our conclusions. Table <ref type="table" target="#tab_2">3</ref> shows the error statistic for the range of values of P1 when P2 = 8 and P3 = 8, and when P2 = 10 and P3 = 8. Table <ref type="table" target="#tab_3">4</ref> shows the error statistic for the range of values of P2 when P1 = 8 and P3 = 8 and when P1 = 10 and P3 = 8. Table <ref type="table" target="#tab_4">5</ref> shows the error statistic for the range of values of P3 when P1 = 8 and P2 = 10 and when P1 = 10 and P2 = 10. Table <ref type="table" target="#tab_5">6</ref> shows the error statistic for the range of values of P3 when P1 = 8 and P2 = 8 and when P1 = 10 and P2 = 8. These tables demonstrate that the system is least sensitive to the value of parameter P3. It seems to be rather  sensitive to the values of the other two parameters, with good results obtained for P1 and P2 both in the ranges 8-10. We conclude that the best set of values to use are P1 = 10, P2 = 8, and P3 = 8.</p><p>We now compare our results with the recent color segmentation technique of Ma and Manjunath <ref type="bibr" target="#b17">[17]</ref> (hereafter referred to as MM97). Four example test cases (T1 to T4) are shown in the left column of Fig. <ref type="figure" target="#fig_9">9</ref>. The middle column illustrates the results obtained by MM97 and the right column shows the results of the proposed segmentation approach using P1 = 10, P2 = 8, and P3 = 8. MM97 examine the direction of change in color, texture, and phase at each image location and construct an edge-flow vector. Region boundaries are then detected by iteratively propagating the edge-flow until some stabilization criteria are satisfied. The Gaussian function (with its user-determined scale parameter) is used both for smoothing and Gabor feature extraction. Contextual neighborhood information is incorporated through the predictive coding analysis of neighbors in different directions for the edge-flow propagation.</p><p>Table <ref type="table" target="#tab_6">7</ref> gives the error of the best result that could be obtained by MM97 and the proposed method compared for  each of the 27 training images. In most cases, the proposed method has the least error. In most test cases, MM97's technique does well to locate the relevant regions, however, by the nature of its edge-flow approach, it fails to locate accurate boundaries separating the regions. This is entirely in accordance to results reported in <ref type="bibr" target="#b17">[17]</ref> for similar images.</p><p>Next, we applied the proposed method to a number of real images. The segmentation of textures in these images is subjective. In Figs. <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref>, and 13, we show the segmentation by MM97 and by the proposed method for a forest scene, an image of the painting La seine aÁ argenteuil by Claude Monet, an aerial scene, and an infant monkey among branches and leaves. It should be noted that the MM97 results are obtained by tuning the parameters of their algorithm for each image individually. The results presented from the proposed scheme were all obtained with the default values P1 = 10, P2 = 8, and P3 = 8. In some cases, better results could have been obtained if we had finely tuned the parameters to the individual image. Nevertheless, in all cases, the segmented regions seem to agree well with the regions we would perceive as distinct if we tried not to make use of semantics (i.e., high level knowledge) like ªsky,º ªgrass,º etc. In all these images, there are no nice straight edges that MM97's technique or our Q-function could take advantage of to give a ªcleanº segmentation.</p><p>We also performed experiments using Gaussian smoothing instead of perceptual smoothing. The segmentation results for the Gaussianly smoothed images were simply very wrong since the similar amount of smoothing applied to each image band, using the same Gaussian kernel, was more and more destructive at increasing distances. To save space, we do not report those results here and the reader is referred to <ref type="bibr">[48]</ref>.</p><p>The relaxation process of the proposed method can be iterated not only across the tower of images, but also at each level of the tower in order to increase the influence of the model imposed via the dictionary. Naturally, this would add to the computational cost. At present, the smoothing stage demands a high computational cost due to the size of the convolution filters. However, the clustering and the relaxation process take approximately 60 seconds on a Silicon Graphics R10000 processor for a IPV Â IPV image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSIONS</head><p>Color is an important parameter in the human visual experience. Most work in the past on texture analysis and segmentation has been concerned with deriving structural descriptors of texture, e.g., coarseness, regularity, blobiness, orientation, etc., with the color information perhaps used as an extra cue.</p><p>In this paper, we treated the interplay of colors and their spatial distribution in an inseparable way as they are actually perceived during the preattentive stage of human color vision. To do this, we developed a tower of blurred versions of an image created by masks imitating the blurring the human vision sensors experience for scenes viewed at different distances and allowed the information in this tower to flow in a causal direction, from the most blurred level to the most focused.</p><p>The creation of the tower made use of the latest results of psychophysics research, while the framework developed for the causal transfer of information is quite general and can be applied for image segmentation where the features used could be other than color. Fig. <ref type="figure" target="#fig_14">14</ref> shows the result of the proposed algorithm applied to the forest image of Fig. <ref type="figure" target="#fig_10">10</ref> with starting distances P À IRm. These results seem to correlate well with the way we perceive this scene from various distances.</p><p>Finally, the probabilistic relaxation methodology developed works in the opposite sense to classical probabilistic relaxation schemes: There, the flow of information starts from the immediate neighbors of a pixel and, as the iteration steps progress, the influence of more distant pixels is incorporated through the succession of immediate neighbor interactions. In our case, probabilistic relaxation works in the same sense as all other multiresolution schemes, where, first, the information of long-range interaction is absorbed, followed by information from the shorter range interaction. As we do not perform subsampling when we create the levels of the multiscale tower and we keep only the same immediate neighbors as contextual neighborhood of a pixel, it may appear that we lack the mechanism to incorporate information from distant pixels. This is not so because, through the increasing size of the blurring masks which we use to create the multiscale tower, the information from increasingly larger distances is ªsmearedº into the immediate neighbors of a pixel and, through interaction with them, is incorporated into the pixel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The view-distance-based image tower.</figDesc><graphic coords="3,318.84,63.77,191.79,192.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>convolution matrices are set up for the color planes. Each of these consists of a weighted sum of Gaussian kernels. The matrices are computed according to<ref type="bibr" target="#b0">[1]</ref>:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convolution kernels for (a)-(c) perceptual smoothing and (d) Gaussian smoothing. (a) Luminance plane. (b) Red-green plane. (c) Blue-yellow plane. (d) Gaussian.</figDesc><graphic coords="5,72.91,61.17,420.66,338.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top: A real texture collage image. Second and third rows: Perceptual and Gaussian smoothing with the ªblack background.º Fourth and fifth rows: Perceptual and Gaussian smoothing with the ªwhite background.º The three columns correspond to 1, 5, and 10 meters of viewing distance, from left to right.</figDesc><graphic coords="6,72.91,61.17,420.66,584.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overall schematic view of the algorithm.</figDesc><graphic coords="8,96.83,61.17,372.81,420.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Real texture collage image with initial clusters and derived core clusters.</figDesc><graphic coords="9,76.31,170.30,413.91,116.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Entries in the pattern dictionary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The thumbnails of images used in the experiments.</figDesc><graphic coords="9,73.30,312.66,419.92,125.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Four test cases (T1-T4) and their segmentation by applying (middle column) Ma and Manjunath and (right column) proposed methods.</figDesc><graphic coords="12,56.35,61.17,453.83,523.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Forest image, segmentation by Ma and Manjunath, and by proposed method.</figDesc><graphic coords="13,312.38,61.17,204.77,604.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Monet's painting, segmentation by Ma and Manjunath, and by proposed method.</figDesc><graphic coords="14,42.12,61.17,219.17,431.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Land image, segmentation by Ma and Manjunath, and by proposed method.</figDesc><graphic coords="14,311.64,61.17,206.19,605.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Monkey image, segmentation by Ma and Manjunath, and by proposed method.</figDesc><graphic coords="15,52.21,61.17,198.99,602.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Forest image, segmentation by the proposed method from distances of 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14m from top left.</figDesc><graphic coords="16,54.14,61.17,458.14,524.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,67.63,111.68,168.26,347.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,140.49,245.36,285.56,182.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,139.01,468.90,288.45,179.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Values for Weight and Spread (in Degrees of Visual Angle) of the Convolution Kernels Determined from Psychovisual Measurements of Color Appearance<ref type="bibr" target="#b0">[1]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Each</head><label>2</label><figDesc>Percentage Error Statistic for a Single Variable Has Been Computed over All Experiments with All Possible Values of the Other Two Parameters</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Mean</head><label>3</label><figDesc>Percentage Errors, Mode of Percentage Errors, and Median Percentage Errors for P1 Values at Fixed P2 and P3 Values</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 Mean</head><label>4</label><figDesc>Percentage Errors, Mode of Percentage Errors, and Median Percentage Errors for P1 Values at Fixed P2 and P3 Values</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Mean</head><label>5</label><figDesc>Percentage Errors, Mode of Percentage Errors, and Median Percentage Errors for P1 Values at Fixed P2 and P3 Values</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 Mean</head><label>6</label><figDesc>Percentage Errors, Mode of Percentage Errors, and Median Percentage Errors for P1 Values at Fixed P2 and P3 Values</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 Error</head><label>7</label><figDesc>Percentages of Incorrectly Classified Pixels</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the Associate Editor and all the referees for their helpful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ªA Spatial Extension of CIELAB for Digital Color Image Reproduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
		<ptr target="ftp://white.stanford.edu/scielab/spie97.ps.gz" />
	</analytic>
	<monogr>
		<title level="m">Proc. Soc. Information Display Symp</title>
		<meeting>Soc. Information Display Symp</meeting>
		<imprint>
			<publisher>WWW address</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">ªSCIELAB: A Metric to Predict the Discriminability of Colored Patterns,º Proc. Ninth Workshop Image and Multidimensional Signal Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="11" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Avnir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªMultiple Resolution Texture Analysis and Classification</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ªMultiple Resolution Representation and Probabilistic Matching of 2-D Gray-Scale Shape</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ªMultiple Resolution Segmentation of Textured Images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ip</surname></persName>
		</author>
		<title level="m">ªStructural Texture Segmentation Using Irregular Pyramid,º Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Glazer</surname></persName>
		</author>
		<title level="m">ªMultilevel Relaxation in Low-Level Computer Vision,º Multiresolution Image Processing and Analysis</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="312" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªImage Analysis Using Multigrid Relaxation Methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="139" />
			<date type="published" when="1986-02">Feb. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<title level="m">ªMultiresolution Relaxation: Experiments and Evaluations,º Proc. Int&apos;l Conf. Pattern Recognition</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="712" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ªMultiresolution Edge Labelling Using Herarchical Relaxation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haindl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Pattern Recognition</title>
		<meeting>Int&apos;l Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªA Renormalization Group Approach to Image Processing Problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="164" to="180" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ªMultiscale Markov Random Fields and Constrained Relaxation in Low-Level Image Analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th</title>
		<meeting>17th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<idno>III-61-III-64</idno>
		<title level="m">Conf. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªMultimodal Estimation of Discontinuous Optical Flow Using Markov Random Fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="217" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Panjwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
		<title level="m">Segmentation of Textured Color Images Using Markov Random Field Models,º Proc. Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="776" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<title level="m">ªMultiscale Minimization of Global Energy Functions in Some Visual Recovery Problems,º Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ªMultiresolution Gauss-Markov Random-Field Models for Texture Segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ªEdge Flow: A Framework of Boundary Detection and Image Segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Vision and Pattern Recognition</title>
		<meeting>Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="744" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bergen, ªTextons, the Fundamental Elements in Preattentive Vision and Perception of Textures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Systems Technical J</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">645</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ªA Computational Model of Texture Perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CSD-89-491</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªTextural Features Corresponding to Visual Perception</title>
		<imprint>
			<date type="published" when="1978-06">June 1978</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="460" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Lohse</surname></persName>
		</author>
		<title level="m">ªIdentifying High Level Features of Texture Perception,º Graphical Models and Image Processing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="218" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<title level="m">ªStatistical and Structural Approaches to Texture,º Proc. IEEE</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="786" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dewaele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oosterlinck</surname></persName>
		</author>
		<title level="m">ªTexture Analysis anno 1983,º Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="336" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buf</surname></persName>
		</author>
		<title level="m">ªA Review of Recent Texture Segmentation and Feature Extraction Techniques,º CVGIP: Image Understanding</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="359" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farrokhnia</surname></persName>
		</author>
		<title level="m">ªUnsupervised Texture Segmentation Using Gabor Filters,º Pattern Recognition</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ªMultiresolution Feature Extraction and Selection for Texture Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="717" to="728" />
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ªA Set of Multiresolution Texture Features Suitable for Unsupervised Image Segmentation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hatzakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Signal Processing VIII, Theories and Applications</title>
		<meeting>Signal essing VIII, Theories and Applications</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page">498</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Derin</surname></persName>
		</author>
		<title level="m">ªUnsupervised Segmentation of Noisy and Textured Images Using Markov Random Fields,º CVGIP: Graphical Models and Image Processing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="306" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Roan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">ªMultiple Resolution Imagery and Texture Analysis,º Pattern Recognition</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<title level="m">ªColor Information for Region Segmentation,º Computer Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="222" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<title level="m">ªColor Indexing,º PhD thesis</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Rochester</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ªSegmenting Images Using Normalized Color</title>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">ªColor Images&apos; Segmentation Using Scale Space Filter and Markov Random Field,º Pattern Recognition</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">229</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Physical Approach to Color Image Understanding</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Klinker</surname></persName>
		</author>
		<editor>A.K. Peters</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Wellesley, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ªMultiresolution Color Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Skarbek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<title level="m">ªColour Image SegmentationÐA Survey,º technical report</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Technical Univ., Berlin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">ªSegmentation of Natural Images Using Self-Organising Feature Maps,º Proc. Seventh British Machine Vision Conf</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="222" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Andrews</surname></persName>
		</author>
		<title level="m">ªImage Segmentation by Clustering,º Proc. IEEE</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="773" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">ªDefect Detection in Random Color Textures,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="667" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">ªColour Texture Classification Using Features from Color Histogram,º Proc. Eighth Scandinavian Conf. Image Processing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Scharcanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hovis</surname></persName>
		</author>
		<title level="m">ªRepresenting the Color Aspect of Texture Images,º Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="191" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ªProbabilistic Relaxation as an Optimiser</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stoddart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Vision</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ªSimilarities between Texture Grouping and Motion Perception: The Role of Color, Luminance, and Orientation,º Int</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Kashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Papathomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">l J. Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="91" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ªA Human Vision-Based Computational Model for Chromatic Texture Segregation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Papathomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Kashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ªEdge Labelling Using Dictionary-Based Relaxation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ªStructural Matching in Computer Vision Using Probabilistic Relaxation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="749" to="764" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<ptr target="http://www-white.-media.mit.edu/vismod/imagery/VisionTexture/vistex.html" />
		<title level="m">ªVistex Texture Database,ºMIT Media Lab</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
