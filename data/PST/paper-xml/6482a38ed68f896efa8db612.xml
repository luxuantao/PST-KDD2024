<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple and Controllable Music Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-08">8 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
							<email>jadecopet@meta.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
							<email>felixkreuk@meta.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Itai</forename><surname>Gat</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tal</forename><surname>Remez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Kant</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
							<email>defossez@meta.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meta</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Yossi Adi is Affiliated with both The Hebrew University of Jerusalem &amp; MetaAI Preprint. Under review</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple and Controllable Music Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-08">8 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.05284v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the task of conditional music generation. We introduce MUSICGEN, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MUSICGEN is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MUSICGEN can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MUSICGEN. Music samples, code, and models are available at github.com/facebookresearch/audiocraft.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-to-music is the task of generating musical pieces given text descriptions, e.g., "90s rock song with a guitar riff". Generating music is a challenging task as it requires modeling long range sequences. Unlike speech, music requires the use of the full frequency spectrum <ref type="bibr" target="#b0">[M?ller, 2015]</ref>. That means sampling the signal at a higher rate, i.e., the standard sampling rates of music recordings are 44.1 kHz or 48 kHz vs. 16 kHz for speech. Moreover, music contains harmonies and melodies from different instruments, which create complex structures. Human listeners are highly sensitive to disharmony <ref type="bibr" target="#b1">[Fedorenko et al., 2012</ref><ref type="bibr" target="#b2">, Norman-Haignere et al., 2019]</ref>, hence generating music does not leave a lot of room for making melodic errors. Lastly, the ability to control the generation process in a diverse set of methods, e.g., key, instruments, melody, genre, etc. is essential for music creators.</p><p>Recent advancements in self-supervised audio representation learning <ref type="bibr" target="#b3">[Balestriero et al., 2023]</ref>, sequential modeling <ref type="bibr" target="#b4">[Touvron et al., 2023]</ref>, and audio synthesis <ref type="bibr" target="#b5">[Tan et al., 2021]</ref> provide the conditions to develop such models. To make audio modeling more tractable, recent studies proposed representing audio signals as multiple streams of discrete tokens representing the same signal <ref type="bibr" target="#b6">[D?fossez et al., 2022]</ref>. This allows both high-quality audio generation and effective audio modeling. However, this comes at the cost of jointly modeling several parallel dependent streams. <ref type="bibr" target="#b7">Kharitonov et al. [2022]</ref>, <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> proposed modeling multi-streams of speech tokens in parallel following a delay approach, i.e., introduce offsets between the different streams. <ref type="bibr" target="#b9">Agostinelli et al. [2023]</ref> proposed representing musical segments using multiple sequences of discrete tokens at different granularity and model them using a hierarchy of autoregressive models. In parallel, <ref type="bibr" target="#b10">Donahue et al. [2023]</ref> follows a similar approach but for the task of singing to accompaniment generation. Recently, <ref type="bibr" target="#b11">Wang et al. [2023]</ref> proposed tackling this problem in two stages: (i) modeling the first  In this work, we introduce MUSICGEN, a simple and controllable music generation model, which is able to generate high-quality music given textual description. We propose a general framework for modeling multiple parallel streams of acoustic tokens, which serves as a generalization of previous studies (see Figure <ref type="figure">1</ref>). To improve controllability of the generated samples, we additionally introduce unsupervised melody conditioning, which allows the model to generate music that matches a given harmonic and melodic structure. We conduct an extensive evaluation of MUSICGEN and show the proposed method is superior to the evaluated baselines by a large margin, with a subjective rating of 84.8 out of 100 for MUSICGEN against 80.5 for the best baseline. We additionally provide an ablation study which sheds light on the importance of each of the components on the overall model performance. Lastly, human evaluation suggests that MUSICGEN yields high quality samples which are better melodically aligned with a given harmonic structure, while adhering to a textual description.</p><p>Our contribution: (i) We introduce a simple and efficient model to generate high quality music at 32 kHz. We show that MUSICGEN can generate consistent music with a single-stage language model through an efficient codebook interleaving strategy. (ii) We present a single model to perform both text and melody-conditioned generation and demonstrate that the generated audio is coherent with the provided melody and faithful to the text conditioning information. (iii) We provide extensive objective and human evaluations on the key design choices behind our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>MUSICGEN consists in an autoregressive transformer-based decoder <ref type="bibr" target="#b12">[Vaswani et al., 2017]</ref>, conditioned on a text or melody representation. The (language) model is over the quantized units from an EnCodec <ref type="bibr" target="#b6">[D?fossez et al., 2022]</ref> audio tokenizer, which provides high fidelity reconstruction from a low frame rate discrete representation. Compression models such as <ref type="bibr" target="#b6">[D?fossez et al., 2022</ref><ref type="bibr" target="#b13">, Zeghidour et al., 2021]</ref> employ Residual Vector Quantization (RVQ) which results in several parallel streams. Under this setting, each stream is comprised of discrete tokens originating from different learned codebooks. Prior work, proposed several modeling strategies to handle this issue <ref type="bibr" target="#b7">[Kharitonov et al., 2022</ref><ref type="bibr">, Agostinelli et al., 2023</ref><ref type="bibr">, Wang et al., 2023]</ref>. In this work, we introduce a novel modeling framework, which generalizes to various codebook interleaving patterns, and we explore several variants. Through patterns, we can leverage the internal structure of the quantized audio tokens. Finally, MUSICGEN supports conditional generation based on either text or melody.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Audio tokenization</head><p>We use EnCodec <ref type="bibr" target="#b6">[D?fossez et al., 2022]</ref>, a convolutional auto-encoder with a latent space quantized using Residual Vector Quantization (RVQ) <ref type="bibr" target="#b13">[Zeghidour et al., 2021]</ref>, and an adversarial reconstruction loss. Given a reference audio random variable X ? R d?fs with d the audio duration and f s the sample rate, EnCodec encodes it into a continuous tensor with a frame rate f r ? f s . This representation is then quantized into Q ? {1, . . . , N } K?d?fr , with K being the number of codebooks used in RVQ and N being the codebook size. Notice, after quantization we are left with K parallel discrete tokens sequences, each of length T = d ? f r , representing the audio sample. In RVQ, each quantizer encodes the quantization error left by the previous quantizer, thus quantized values for different codebooks are in general not independent, and the first codebook is the most important one.</p><p>2.2 Codebook interleaving patterns (see Figure <ref type="figure">1</ref>)</p><p>Exact flattened autoregressive decomposition. An autoregressive model requires a discrete random sequence U ? {1, . . . , N } S with S the sequence length. By convention, we will take U 0 = 0, a deterministic special token indicating the beginning of the sequence. We can then model the</p><formula xml:id="formula_0">distribution ?t &gt; 0, p t (U t-1 , . . . , U 0 ) ? P [U t |U t-1 , . . . , U 0 ] .<label>(1)</label></formula><p>Let us define recursively ?0 = 0 and for all t &gt; 0,</p><formula xml:id="formula_1">?t &gt; 0, P ?t = p t ?t-1 , . . . , ?0 .<label>(2)</label></formula><p>Then, we immediately have that U and ? follow the same distribution. This means that if we can fit a perfect model p of p, then we can fit exactly the distribution of U .</p><p>As stated before, the main issue with the representation Q we obtained from the EnCodec model is that there are K codebooks for each time step. One solution would be to flatten out Q, thus taking S = df s ? K, e.g. first predicting the first codebook of the first time step, then the second codebook of the first time step, etc. Then, using (1) and (2), we could theoretically fit an exact model of the distribution of Q. The downside however is the increased complexity, with part of the gain coming from the lowest sample rate f r being lost.</p><p>More than one possible flattening exists, and not all the pt functions need to be estimated through a single model. For instance, MusicLM <ref type="bibr" target="#b9">[Agostinelli et al., 2023]</ref> uses two models, one modeling the flattened first K/2 codebooks, and a second one the other K/2 flattened codebooks, conditioned on the decision of the first model. In that case, the number of autoregressive steps is still df s ? K.</p><p>Inexact autoregressive decomposition. Another possibility is to consider an autoregressive decomposition, where some codebooks are predicted in parallel. For instance, let us define another sequence with V 0 = 0 and for all t ? {1, . . . , N }, k ? {1, . . . , K}, V t,k = Q t,k . When dropping the codebook index k, e.g. V t , we mean the concatenation of all the codebooks at time t.</p><formula xml:id="formula_2">p t,k (V t-1 , . . . , V 0 ) ? P [V t,k |V t-1 , ?, . . . , V 0 ] .<label>(3)</label></formula><p>Let's define again recursively ?0 = 0 and for all t &gt; 0, ?t &gt; 0, ?k, P ?t,k = p t,k ?t-1 , . . . , ?0 .</p><p>Unlike in (2), we no longer have in the general case that ? follows the same distribution as V , even assuming we have access to the exact distribution p t,k . In fact, we would only have a proper generative model if for all t, (V t,k ) k are independent conditionally on V t-1 , . . . , V 0 . As t increases, the errors will compound and the two distributions can grow further apart. Such a decomposition is inexact, but allows to keep the original frame rate which can considerably speed up training and inference, especially for long sequences. Interestingly, the VALL-E speech generator model <ref type="bibr" target="#b11">[Wang et al., 2023]</ref> uses an inexact autoregressive decomposition. It first predicts sequentially the first codebook for all time steps, then predicts in parallel all the remaining codebooks, e.g. implicitly assuming they are independent conditionally on the first codebook for all time steps.</p><p>Arbitrary codebook interleaving patterns. In order to experiment with various such decompositions, and measure exactly the impact of using an inexact decomposition, we introduce codebook interleaving patterns. Let us consider ? = {(t, k) : {1, . . . , d ? f r }, k ? {1, . . . , K}} be the set of all pairs of time steps and codebook indexes. A codebook pattern is a sequence P = (P 0 , P 1 , P 2 , . . . , P S ), with P 0 = ?, and for all 0 &lt; i ? S, P i ? ?, such that P is partition of ?. We model Q by predicting in parallel all the positions in P t , conditionally on all the positions in P 0 , P 1 , . . . , P T . Pragmatically, we restrict ourselves to patterns where each codebook index appears at most once in any of the P s .</p><p>We can now easily define a number of decompositions, for instance the "parallel" pattern given by</p><formula xml:id="formula_4">P s = {(s, k) : k ? {1, . . . , K}}.<label>(5)</label></formula><p>Similarly, a VALL-E <ref type="bibr" target="#b11">[Wang et al., 2023]</ref> inspired pattern can be defined as</p><formula xml:id="formula_5">P s = {(s, 1)} if s ? T, P s = {(s, k) : k ? {2, . . . , K}} otherwise. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>It is also possible to introduce a "delay" between the codebooks, as in <ref type="bibr" target="#b7">Kharitonov et al. [2022]</ref>, e.g.,</p><formula xml:id="formula_7">P s = {(s -k + 1, k) : k ? {1, . . . , K}, s -k ? 0}.<label>(7)</label></formula><p>Through empirical evaluations, we show the benefits and drawbacks of various codebook patterns. Shedding light on the importance of exact modeling of the parallel codebook sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model conditioning</head><p>Text conditioning. Given a textual description matching the input audio X, we compute a conditioning tensor C ? R T C ?D with D being the inner dimension used in the autoregressive model. Generally, there are three main approaches for representing text for conditional audio generation. <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> proposed using a pretrained text encoder, specifically T5 <ref type="bibr" target="#b14">[Raffel et al., 2020]</ref>. <ref type="bibr" target="#b15">Chung et al. [2022]</ref> show that using instruct-based language models provide superior performance. Lastly, <ref type="bibr" target="#b9">Agostinelli et al. [2023]</ref>, <ref type="bibr" target="#b16">Liu et al. [2023]</ref>, <ref type="bibr">Huang et al. [2023a]</ref>, <ref type="bibr" target="#b18">Sheffer and Adi [2023]</ref> claimed that joint text-audio representation, such as CLAP <ref type="bibr">[Wu* et al., 2023]</ref>, provides better-quality generations. We experiment with all of the above, respectively: T5 encoder, FLAN-T5, and CLAP.</p><p>Melody conditioning. While text is the prominent approach in conditional generative models nowadays, a more natural approach for music is conditioning on a melodic structure from another audio track or even whistling or humming. Such an approach also allows for an iterative refinement of the model's output. To support that, we experiment with controlling the melodic structure via jointly conditioning on the input's chromagram and text description. In preliminary experiments, we observed that conditioning on the raw chromagram often led to reconstructing the original sample, resulting in overfitting. To reduce it, we introduce an information bottleneck by choosing the dominant time-frequency bin in each time step. While a similar capability was shown in <ref type="bibr" target="#b9">Agostinelli et al. [2023]</ref>, the authors used supervised proprietary data, which is tedious and costly to collect. In this work, we take an unsupervised approach, eliminating the requirement for supervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model architecture</head><p>Codebook projection and positional embedding. Given a codebook pattern, only some codebooks are present at each pattern step P s . We retrieve from Q the values corresponding to the indices in P s .</p><p>As noted in Section 2.2, each codebook is present at most once in P s or not at all. If it is present, we use a learned embedding table with N entries and dimension D to represent the associated value from Q. Otherwise, we use a special token indicating its absence. We sum the contribution from each codebook after this transformation. As P 0 = ?, the first input is always the sum of all the special tokens. Finally, we sum a sinusoidal embedding to encode the current step s <ref type="bibr" target="#b12">[Vaswani et al., 2017]</ref>.</p><p>Transformer decoder. The input is fed into a transformer with L layers and a dimension D. Each layer consists of a causal self-attention block. We then use a cross-attention block that is fed with the conditioning signal C. When using melody conditioning, we instead provide the conditioning tensor C as a prefix to the transformer input. The layer ends with a fully connected block consisting of a linear layer from D to 4?D channels, a ReLU, and a linear layer back to D channels. The attention and fully connected blocks are wrapped with a residual skip connection. Layer normalization <ref type="bibr" target="#b20">[Ba et al., 2016]</ref> is applied to each block before being summed with the residual skip connection ("pre-norm").</p><p>Logits prediction. The output from the transformer decoder at pattern step P s is transformed into logits prediction for the values of Q taken at the indices given by P s+1 . Each codebook is present at most once in P s+1 . If a codebook is present, the logits prediction is obtained by applying a codebook specific linear layer from D channels to N .</p><p>3 Experimental setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models and hyperparameters</head><p>Audio tokenization model. We use a non-causal five layers EnCodec model for 32 kHz monophonic audio with a stride of 640, resulting in a frame rate of 50 Hz, and an initial hidden size of 64, doubling at each of the model's five layers. The embeddings are quantized with an RVQ with four quantizers, each with a codebook size of 2048. We follow <ref type="bibr" target="#b6">D?fossez et al. [2022]</ref> to train the model on one-second audio segments cropped at random in the audio sequence.</p><p>Transformer model. We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters. We use a memory efficient Flash attention <ref type="bibr" target="#b21">[Dao et al., 2022]</ref> from the xFormers package <ref type="bibr" target="#b22">[Lefaudeux et al., 2022]</ref> to improve both speed and memory usage with long sequences. We study the impact of the size of the model in Section 4. We use the 300M-parameter model for all of our ablations. We train on 30-second audio crops sampled at random from the full track. We train the models for 1M steps with the AdamW optimizer <ref type="bibr" target="#b23">[Loshchilov and Hutter, 2017]</ref>, a batch size of 192 examples, ? 1 = 0.9, ? 2 = 0.95, a decoupled weight decay of 0.1 and gradient clipping of 1.0.</p><p>We further rely on D-Adaptation based automatic step-sizes <ref type="bibr" target="#b24">[Defazio and Mishchenko, 2023]</ref> for the 300M model as it improves model convergence but showed no gain for the bigger models. We use a cosine learning rate schedule with a warmup of 4000 steps. Additionally, we use an exponential moving average with a decay of 0.99. We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision. More specifically, we use float16 as bfloat16 was leading to instabilities in our setup. Finally, for sampling, we employ top-k sampling <ref type="bibr" target="#b25">[Fan et al., 2018]</ref> with keeping the top 250 tokens and a temperature of 1.0.</p><p>Text preprocessing. <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> proposed a text normalization scheme, in which stop words are omitted and the remaining text is lemmatized. We denote this method by text-normalization. When considering musical datasets, additional annotations tags such as musical key, tempo, type of instruments, etc. are often available. We also experiment with concatenating such annotations to the text description. We denote this approach by condition-merging. Finally, we explored using word dropout as another text augmentation strategy. For the final models, we used condition-merging with a probability of 0.25. Upon merging, we apply a text description dropout with a probability of 0.5. We use a word dropout with a probability of 0.3 on the resulting text. A full comparison of the different text preprocessing strategies can be found in Appendix A.2.</p><p>Codebook patterns and conditioning. We use the "delay" interleaving pattern from Section 2.2, This translates 30 seconds of audio into 1500 autoregressive steps. For text conditioning, we use the T5 <ref type="bibr" target="#b14">[Raffel et al., 2020]</ref> text encoder, optionally with the addition of the melody conditioning presented in Section 2.3. We also experiment with FLAN-T5 <ref type="bibr" target="#b15">[Chung et al., 2022]</ref>, and CLAP <ref type="bibr">[Wu* et al., 2023]</ref> and compare the performance of MUSICGEN using each of these text encoders in the Appendix A.2. For melody conditioning, we compute the chromagrams with a window size of 2 14 and a hope size of 2 12 . Using a large window prevents the model from recovering fine temporal details. We further quantize the chromagram by taking the argmax at each time step. We follow a similar approach to <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> and implement classifier-free guidance when sampling from the model's logits. Specifically, during training we drop the condition with a probability of 0.2 and during inference we use a guidance scale of 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>Training datasets. We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data Evaluation datasets. For the main results and comparison with prior work, we evaluate the proposed method on the MusicCaps benchmark <ref type="bibr" target="#b9">[Agostinelli et al., 2023]</ref>. MusicCaps is composed of 5.5K samples (ten-second long) prepared by expert musicians and a 1K subset balanced across genres. We report objective metrics on the unbalanced set, while we sample examples from the genre-balanced set for qualitative evaluations. For melody evaluation and the ablation studies, we use samples from an in-domain held out evaluation set of 528 music tracks, with no artist overlap with the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Baselines. We compare MUSICGEN to two baselines for text-to-music generation: Riffusion [Forsgren and Martiros] and Mousai <ref type="bibr" target="#b27">[Schneider et al., 2023]</ref>. We use the open source Riffusion model to run inference<ref type="foot" target="#foot_1">3</ref> . For Mousai, we train a model using our dataset for a fair comparison, using the open source implementation provided by the authors<ref type="foot" target="#foot_2">4</ref> . Additionally, when possible, we compare to MusicLM <ref type="bibr" target="#b9">[Agostinelli et al., 2023]</ref> and Noise2Music <ref type="bibr">[Huang et al., 2023b]</ref>.</p><p>Evaluation metrics. We evaluate the proposed method using objective and subjective metrics. For the objective methods, we use three metrics: the Fr?chet Audio Distance (FAD), the Kullback-Leiber Divergence (KL) and the CLAP score (CLAP). We report the FAD <ref type="bibr" target="#b29">[Kilgour et al., 2018]</ref> using the official implementation in Tensorflow with the VGGish model<ref type="foot" target="#foot_3">5</ref> . A low FAD score indicates the generated audio is plausible. Following <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref>, we use a state-of-the-art audio classifier trained for classification on AudioSet <ref type="bibr" target="#b30">[Koutini et al., 2021]</ref> to compute the KL-divergence over the probabilities of the labels between the original and the generated music. The generated music is expected to share similar concepts with the reference music when the KL is low. Last, the CLAP score <ref type="bibr">[Wu* et al., 2023</ref><ref type="bibr">, Huang et al., 2023a]</ref> is computed between the track description and the generated audio to quantify audio-text alignment, using the official pretrained CLAP model<ref type="foot" target="#foot_4">6</ref> .</p><p>For the human studies, we follow the same setup as in <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref>. We ask human raters to evaluate two aspects of the audio samples (i) overall quality (OVL), and (ii) relevance to the text input (REL). For the overall quality test, raters were asked to rate the perceptual quality of the provided samples in a range of 1 to 100. For the text relevance test, raters were asked to rate the match between audio and text on a scale of 1 to 100. Raters were recruited using the Amazon Mechanical Turk platform. We evaluate randomly sampled files, where each sample was evaluated by at least 5 raters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>We start by presenting results of the proposed method on the task of text-to-music generation and compare MUSICGEN to prior work in the field. Next, we evaluate the ability of the proposed method to generate music conditioned on melodic features. We conclude with an ablation study. Music samples, code, and models are available at github.com/facebookresearch/audiocraft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the baselines</head><p>Table <ref type="table" target="#tab_1">1</ref> presents the comparison of the proposed method against Mousai, Riffusion, MusicLM, and Noise2Music. As there is no official implementation of Noise2Music, nor pre-trained model, we report only FAD on MusicCaps, which is reported in the Noise2Music manuscript. Similarly, MusicLM implementation is not public. We use the MusicLM public demo<ref type="foot" target="#foot_6">8</ref> for our subjective tests while reporting the FAD as reported by the authors. While the original MusicLM model is trained on data with vocals, the model behind the API is instrument-only. For the human study, we restrict ourselves to 40 instrument-only samples from MusicCaps. To prevent leakage in MUSICGEN trained with chromagram, we sample chromagrams at random from a held-out set during test time.</p><p>Results suggest that MUSICGEN performs better than the evaluated baselines as evaluated by human listeners, both in terms of audio quality and adherence to the provided text description. Noise2Music performs the best in terms of FAD on MusicCaps, followed by MUSICGEN trained with text conditioning. Interestingly, adding a melody conditioning degrades the objective metrics, however, it does not significantly affect human ratings, while still being superior to the evaluated baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Melody evaluation</head><p>We evaluate MUSICGEN, conditioned jointly on textual and melodic representations, using objective and subjective metric on the held out evaluation set. For the objective evaluation, we introduce a new metric: chroma cosine-similarity, which measures the average cosine-similarity between frames corresponding to the same time steps, taken from the quantized chroma of the reference and the generated samples. We evaluate using 1000 randomly sampled files from a held-out set. To better evaluate the relation between the conditioned melody to the generated music, we introduce another human study. To that end, we present human raters with a reference musical piece, followed by a set of generated pieces. For each generated sample, the listeners are asked to rate how well the melody of the generated piece matches that of the reference on a scale of 1 to 100. We use 40 samples of 10 seconds at random from the held-out set. Results are reported in Table <ref type="table" target="#tab_2">2</ref>. Results suggest that MUSICGEN trained with chromagram conditioning successfully generates music that follows a given melody. Thus, allowing for better control over the generated output. Interestingly, MUSICGEN is robust to dropping the chroma at inference time with both OVL. and REL. staying roughly the same.</p><p>Table <ref type="table">3</ref>: Codebook patterns. We compare different codebook interleaving patterns on 30-seconds, audio sequences. The "flattening" pattern achieves the best scores. The "delay" and "partial flattening" patterns achieve similar scores, while "parallel" and "VALL-E" obtain worse scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation</head><p>This section provides an ablation study for the different codebook patterns, together with results for model scales. Additionally, we present results for different text augmentation strategies and text encoders in Appendix A.2. All ablations are performed using 1K samples of 30 seconds, randomly sampled from the held-out evaluation set.</p><p>The effect of the codebook interleaving patterns. We evaluate various codebook patterns using the framework from Section 2.2, with K = 4, given by the audio tokenization model. Table <ref type="table" target="#tab_1">1</ref> reports results with the "delay" pattern. The "partial delay" consists in delaying by the same amount the codebooks 2, 3, and 4. The "parallel" pattern predicts all the codebooks from the same time step in parallel. The "VALL-E" pattern is inspired by the work of <ref type="bibr" target="#b11">Wang et al. [2023]</ref>. It first predicts codebook 1 for all steps, then predicts in parallel codebooks 2, 3, and 4. Thus, this pattern has twice the steps compared to other patterns. "Partial flattening" is similar, but instead of sampling first codebook 1 for all steps, it interleaves them with the parallel sampling of codebooks 2, 3, and 4. Finally, the "flattening" pattern consists in flattening all the codebooks, similar to MusicLM <ref type="bibr" target="#b9">[Agostinelli et al., 2023]</ref>. We report objective and subjective evaluations in Table <ref type="table">3</ref>. While flattening improves generation, it comes at a high computational cost and similar performance can be reached for a fraction of this cost using a simple delay approach.</p><p>The effect of model size. In Table <ref type="table" target="#tab_3">4</ref> we report results for different model sizes, namely 300M, 1.5B, and 3.3B parameter models. As expected, scaling the model size results in better scores, however this comes at the expense of longer training and inference time. Regarding subjective evaluations, the overall quality is optimal at 1.5B, but a larger model can better understand the text prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>In this work, we propose a music language model. Our method is related to recent advancements in audio neural representation and modeling. In the following, we review recent works in those fields.</p><p>Audio representation. In recent years, the prominent approach is to represent the music signals in a compressed representation, discrete or continuous, and apply a generative model on top of it. <ref type="bibr" target="#b33">Lakhotia et al. [2021]</ref> proposed quantizing speech representation using k-means to construct speech language models. Recently, <ref type="bibr" target="#b6">D?fossez et al. [2022]</ref>, <ref type="bibr" target="#b13">Zeghidour et al. [2021]</ref> proposed to apply a VQ-VAE directly on the raw waveform using residual vector quantization. Later, several studies used such representation for text-to-audio generation. Next, we discuss the recent research in audio generation.</p><p>Music generation. Music generation has been long studied under various setups. <ref type="bibr" target="#b34">Dong et al. [2018]</ref> proposed a GAN-based approach for symbolic music generation. <ref type="bibr" target="#b35">Bassan et al. [2022]</ref> proposed an unsupervised segmentation for symbolic music which can be later used for generation. <ref type="bibr" target="#b36">Ycart et al. [2017]</ref> proposed modeling polyphonic music modeling using recurrent neural networks. Ji et al.</p><p>[2020] conducted a comprehensive survey therein for deep learning methods for music generation. <ref type="bibr" target="#b38">Dhariwal et al. [2020]</ref> proposed representing music samples in multiple streams of discrete representations using a hierarchical VQ-VAE. Next, two sparse transformers applied over the sequences to generate music. <ref type="bibr" target="#b39">Gan et al. [2020]</ref> proposed generating music for a given video, while predicting its midi notes. Recently, <ref type="bibr" target="#b9">Agostinelli et al. [2023]</ref> proposed representing music using multiple streams of "semantic tokens" and "acoustic tokens". Then, they applied a cascade of transformer decoders conditioned on a textual-music joint representation <ref type="bibr" target="#b40">[Huang et al., 2022]</ref>. <ref type="bibr" target="#b10">Donahue et al. [2023]</ref> followed a similar modeling approach, but for the task of singing-to-accompaniment generation.</p><p>An alternative approach is using diffusion models, which naturally apply over continuous representations. <ref type="bibr" target="#b27">Schneider et al. [2023]</ref>, <ref type="bibr">Huang et al. [2023b]</ref>, <ref type="bibr" target="#b41">Maina [2023]</ref>, Forsgren and Martiros proposed using a latent diffusion model for the task of text-to-music. <ref type="bibr" target="#b27">Schneider et al. [2023]</ref> proposed using diffusion models for both audio encoder-decoder and latent generation. <ref type="bibr">Huang et al. [2023b]</ref> proposed a cascade of diffusion model to generate audio and gradually increase its sampling rate. Forsgren and Martiros proposed fine-tuning Stable Diffusion <ref type="bibr" target="#b42">Rombach et al. [2022]</ref> using spectrograms to generate five-second segments, then, using image-to-image mapping and latent interpolation to generate long sequences.</p><p>Audio generation. Several studies were proposed for text-to-audio (environmental sounds) generation. <ref type="bibr" target="#b43">Yang et al. [2022]</ref> proposed representing audio spectrograms using a VQ-VAE, then applying a discrete diffusion model conditioned on textual CLIP embeddings for the generation part <ref type="bibr" target="#b44">[Radford et al., 2021]</ref>. <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> proposed applying a transformer language model over discrete audio representation, obtained by quantizing directly time-domain signals using EnCodec <ref type="bibr" target="#b6">[D?fossez et al., 2022]</ref>. <ref type="bibr" target="#b18">Sheffer and Adi [2023]</ref> followed a similar approach to <ref type="bibr" target="#b8">Kreuk et al. [2022]</ref> for image-to-audio generation. <ref type="bibr">Huang et al. [2023a]</ref>, <ref type="bibr" target="#b16">Liu et al. [2023]</ref> proposed using latent diffusion models for the task of text-to-audio, while extending it to various other tasks such as inpainting, image-to-audio, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We introduced MUSICGEN, a state-of-the-art single stage controllable music generation model that can be conditioned on text and melody. We demonstrated that simple codebook interleaving strategies can be used to achieve high quality generation while reducing the number of autoregressive time steps compared to the flattening approach. Furthermore, we provided a comprehensive study of the impact of model sizes, conditioning methods, and text pre-processing techniques. We also introduced a simple chromagram-based conditioning for controlling the melody of the generated audio.</p><p>Limitations Our simple generation method does not allow us to have fine-grained control over adherence of the generation to the conditioning, we rely mostly on CF guidance. Also, while it is relatively straightforward to do data augmentation for text conditioning, conditioning on audio warrants further research on data augmentation, types and amount of guidance.</p><p>Ethical aspects. Large scale generative models present ethical challenges. We first ensured that all the data we trained on was covered by legal agreements with the right holders, in particular through an agreement with ShutterStock. A second aspect is the potential lack of diversity in the dataset we used, which contains a larger proportion of western-style music. However, we believe the simplification we operate in this work, e.g., using a single stage language model and a reduced number of auto-regressive steps, can help broaden the applications to new datasets.</p><p>Generative models can represent an unfair competition for artists, which is an open problem. Open research can ensure that all actors have equal access to these models. Through the development of more advanced controls, such as the melody conditioning we introduced, we hope that such models can become useful both to music amateurs and professionals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Model details Codebook interleaving patterns. Figure A.1 provides a visual description of the additional codebook patterns introduced for the ablation in Section 4, namely "partial flattening" and "partial delay" patterns. The intuition behind such patterns is driven by the fact that the first codebook from RVQ is the most important one and predicting the rest of the codebooks in parallel would allow to limit the introduced flattening or delay while maintaining good modeling performance.  <ref type="figure">Figure A</ref>.1: Visualizing partial flattening and partial delays codebook patterns applied on a sequence with 4 parallel streams of quantized values (corresponding to k 1 , . . . , k 4 ) and N time steps (t 1 , . . . , k n ). "Partial flattening" separates the first codebook in dedicated steps and interleaves them with the parallel sampling of codebooks 2, 3, and 4, leading the number of interleaved sequences steps M to be twice the number of original steps N . The "partial delay" pattern consists in delaying by the same amount the codebooks 2, 3, and 4, in our case we use a delay of 1. The total number of steps of the interleave sequences is N (excluding the last step for simplicity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial</head><p>Melody conditioning. In this work, we provide an unsupervised approach for melody conditioning through conditioning on the chromagram representation. As shown in Figure A.2, chromagram-based conditioning successfully preserves the melodic structure when generating novel music samples.</p><p>In preliminary experiments, we noticed that the chromagram tends to be dominated by the lower frequency instruments, mainly by the drums and bass. To mitigate that, we used Demucs <ref type="bibr" target="#b45">[D?fossez et al., 2019]</ref> to first decompose the reference track into four components: drums, bass, vocals, and other. Next, we omit the drums and bass to recover the melodic structure of the residual waveform. Finally, we extract the quantized chromagram to create the conditioning that is later fed to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional experimental results</head><p>We provide further ablation studies on the core components of MUSICGEN, namely the text encoder used for text conditioning described in Section 2.3 and text augmentation strategies presented in Section 3.1. We report results on the MusicCaps dataset to better understand out-of-domain generalization abilities of the different approaches. Finally, we share additional experimental results on optimization methods.</p><p>The effect of text encoder. We investigate the impact of the text encoder, comparing three different encoders: T5 <ref type="bibr" target="#b14">[Raffel et al., 2020]</ref> 9 , Flan-T5 <ref type="bibr" target="#b15">[Chung et al., 2022]</ref> 10 and CLAP <ref type="bibr">[Wu* et al., 2023]</ref> 11 with a quantization bottleneck. For the CLAP-based encoder, similarly to <ref type="bibr" target="#b9">Agostinelli et al. [2023]</ref> we rely on the music embeddings during training and the text embeddings at inference time and we train a RVQ layer on top of the extracted embeddings. More specifically, we use RVQ with 12 quantizers, each with a codebook size of 1024. Quantizing the CLAP embeddings leads to a homogeneous representation with the discrete tokens further reducing the gap between the audio encoding used at train time and text encoding at test time. We report results for the different text encoders in Table <ref type="table" target="#tab_6">A</ref>.1. Both T5 and Flan-T5 perform similarly in terms of objective metrics, the overall quality being slighly better for T5. The CLAP-based model however suffers worse objective and subjective metrics, with the exception of the CLAP score which rely on the same underlying audio and text encoder. 9 https://huggingface.co/t5-base 10 https://huggingface.co/google/flan-t5-base 11 https://huggingface.co/lukewys/laion_clap/blob/main/music_audioset_epoch_15_esc_ 90.14.pt Table <ref type="table" target="#tab_6">A</ref>.1: Text encoder results. We report results for T5, Flan-T5, and CLAP as text encoders. We observe similar results for T5 and Flan-T5 on all the objective metrics. CLAP encoder performs consistently worse on all the metrics but CLAP score. The effect of text augmentations. We examine the impact of text augmentation strategies for the proposed method. In particular, we study the use of condition merging (i.e. concatenating additional metadata to the text description), text normalization (text-norm.) and word dropout. We report objective metrics for the different augmentation strategies in Table <ref type="table" target="#tab_6">A</ref>.2. We observe a gain in FAD and KL when leveraging the additional metadata with condition merging. However, neither text normalization or word dropout improves the results in terms of objective metrics.  <ref type="bibr">[2023]</ref>. We observed improved convergence for the 300M parameter model, although for larger models, e.g. 1.5B and 3.3B, we observed the automated rule led to deteriorated performance, both on the train and validation set. Further investigation is required to better understand the effect of D-Adaptation, and whether it can scale to the largest model. The convergence for both methods can be observed on the train and validation set in   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure A.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure A. 2 :</head><label>2</label><figDesc>Figure A.2: Visualization of quantized chromagram bins over time from reference melody (left), generated music conditioned on chroma and text (middle) and generated music with text-only conditioning (right).Each row is generated using a different chroma condition, all rows share the same text condition: "90s rock song with electric guitar and heavy drums". We observe strong adherence to the input melody for the music samples generated with chroma conditioning while rendering novel styles guided by the input text.</figDesc><graphic url="image-1.png" coords="15,108.00,120.01,396.02,217.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure Comparison of Adam and Adam with D-Adaptation<ref type="bibr" target="#b24">[Defazio and Mishchenko, 2023]</ref>. While D-Adaptation provided consistent gains for the 300M parameters model, we observed worse convergence both on the train (left) and validation (right) set for the 1.5B parameters model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Text-to-Music generation. We compare objective and subjective metrics for MUSICGEN against a number of baselines. We report both mean and CI95 scores. The Mousai model is retrained on the same dataset, while for MusicLM we use the public API for human studies. We report the original FAD on MusicCaps for Noise2Music and MusicLM. "MUSICGEN w. random melody" refers to MUSICGEN trained with chromagram and text. At evaluation time, we sample the chromagrams at random from a held-out set. respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.</figDesc><table><row><cell>MUSICCAPS Test Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Melody evaluation. We report cosine similarity between reference and generated melody (SIM.) and subjective metrics including alignment with the melody (MEL.). All results are reported using MUSICGEN 1.5B.We use the CrowdMOS package 7 to filter noisy annotations and outliers. We remove annotators who did not listen to the full recordings, annotators who rate the reference recordings less than 85, and the rest of the recommended recipes from CrowdMOS<ref type="bibr" target="#b31">[Ribeiro et al., 2011]</ref>. For fairness, all samples are normalized at -14dB LUFS [ITU-R, 2017].</figDesc><table><row><cell>In Domain Test Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Model scale. We compare 3 scales for our method, and evaluate it on an internal test set to limit the impact of the out of domain prediction issues we observed with MusicCaps. In terms of objective metrics we observe a continuous improvement of the metrics, although subjective quality stop improving at 1.5B parameters. A 3.3B model however seems to better fit the text prompt.</figDesc><table><row><cell>In Domain Test Set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A .</head><label>A</label><figDesc>2: Text augmentations strategies results. We report objective metrics using only the original text description (no augmentation) and for different text augmentation strategies: using condition merging to augment the text description with metadata, using text-normalization (text-norm.) and applying word dropout on the resulting text. All models are trained for 500K steps. Condition merging improves the result over training only over the original text description. Other augmentations perform worst on all metrics. -Adaptation is a novel automated way of selecting the overall learning rate of the Adam optimizer, i.e. its ? parameter, dynamically throughout the training, introduced by Defazio and Mishchenko</figDesc><table><row><cell>No augmentation</cell><cell>3.68</cell><cell>1.28</cell><cell>0.31</cell></row><row><cell>Condition merging</cell><cell>3.28</cell><cell>1.26</cell><cell>0.31</cell></row><row><cell>Condition merging + Text-norm.</cell><cell>3.78</cell><cell>1.30</cell><cell>0.29</cell></row><row><cell>Condition merging + Word dropout</cell><cell>3.31</cell><cell>1.31</cell><cell>0.30</cell></row><row><cell>Condition merging + Text-norm. + Word dropout</cell><cell>3.41</cell><cell>1.39</cell><cell>0.30</cell></row><row><cell>Effect of D-Adaptation.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>MUSICCAPS Test Set</p>CONFIGURATION</p>FAD vgg ? KL ? CLAP scr ? D</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>www.shutterstock.com/music and www.pond5.com   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Using riffusion-model-v1 from github.com/riffusion/riffusion-app (on May 10, 2023)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Implementation from github.com/archinetai/audio-diffusion-pytorch(March 2023)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>github.com/google-research/google-research/tree/master/frechet_audio_distance</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/LAION-AI/CLAP</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>http://www.crowdmos.org/download/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements.</head><p>The authors would like to thank <rs type="person">Mary Williamson</rs> and <rs type="person">Joelle Pineau</rs> for supporting this project, thank <rs type="person">Justin Luk</rs>, <rs type="person">Prash Jain</rs>, <rs type="person">Sidd Srinivasan</rs>, <rs type="person">Rode Duenes</rs>, and <rs type="person">Philip Woods</rs> for the dataset, and thank the xformers team: <rs type="person">Daniel Haziza</rs>, <rs type="person">Francisco Massa</rs>, and <rs type="person">Michael Ramamonjisoa</rs> for the technical discussions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fundamentals of music processing: Audio, analysis, algorithms, applications</title>
		<author>
			<persName><forename type="first">Meinard</forename><surname>M?ller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sensitivity to musical structure in the human brain</title>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Fedorenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Norman-Haignere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3289" to="3300" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Divergence in the functional organization of human and macaque auditory cortex revealed by fmri responses to harmonic tones</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Sam V Norman-Haignere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><surname>Bevil R Conway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1057" to="1060" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A cookbook of selfsupervised learning</title>
		<author>
			<persName><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Sobal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12210</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15561</idno>
		<title level="m">A survey on neural speech synthesis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.13438</idno>
		<title level="m">High fidelity neural audio compression</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-free prosodyaware generative spoken language modeling</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8666" to="8681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15352</idno>
		<title level="m">Textually guided audio generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zal?n</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Verzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Caillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11325</idno>
		<title level="m">Generating music from text</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Caillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Esling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Verzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12662</idno>
		<title level="m">Generating musical accompaniments from singing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural codec language models are zero-shot text to speech synthesizers</title>
		<author>
			<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.02111</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von</surname></persName>
		</editor>
		<editor>
			<persName><surname>Luxburg</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Soundstream: An end-to-end neural audio codec</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Audioldm: Text-to-audio generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12503</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12661</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">I hear your true colors: Image guided audio generation</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation</title>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Caggiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Naren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Tintore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/xformers" />
		<title level="m">xformers: A modular and hackable transformer modelling library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning-rate-free learning by d-adaptation</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Mishchenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.07733</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m">Hierarchical neural story generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Riffusion-stable diffusion for real-time music generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Martiros</surname></persName>
		</author>
		<ptr target="https://riffusion.com/about" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mo\?usai: Text-to-music generation with long-context latent diffusion</title>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11757</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Noise2music: Text-conditioned music generation with diffusion models</title>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03917</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fr\&apos;echet audio distance: A metric for evaluating music enhancement algorithms</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sharifi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08466</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient training of audio transformers with patchout</title>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05069</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowdmos: An approach for crowdsourcing mean opinion score studies</title>
		<author>
			<persName><forename type="first">Fl?vio</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinei</forename><surname>Flor?ncio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2416" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Algorithms to measure audio programme loudness and true-peak audio level</title>
		<author>
			<persName><surname>Itu-R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On generative spoken language modeling from raw audio</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu-Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1336" to="1354" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment</title>
		<author>
			<persName><surname>Hao-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Chia</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Musegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised symbolic music segmentation using ensemble temporal prediction errors</title>
		<author>
			<persName><forename type="first">Shahaf</forename><surname>Bassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Rosenschein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00760</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study on lstm networks for polyphonic music sequence modelling</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ycart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISMIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Shulei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06801</idno>
		<title level="m">A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Foley music: Learning to generate music from videos</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="758" to="775" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel Pw</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><surname>Mulan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12415</idno>
		<title level="m">A joint embedding of music audio and natural language</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Kinyugo</forename><surname>Maina</surname></persName>
		</author>
		<author>
			<persName><surname>Msanii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.06468</idno>
		<title level="m">High fidelity music synthesis on a shoestring budget</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Diffsound</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09983</idno>
		<title level="m">Discrete diffusion model for text-to-sound generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
