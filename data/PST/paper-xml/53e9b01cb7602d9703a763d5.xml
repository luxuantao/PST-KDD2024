<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attribute-augmented Semantic Hierarchy Towards Bridging Semantic Gap and Intention Gap in Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yang.yang@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Smeaton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">Institute of Intelligent Machines</orgName>
								<orgName type="department" key="dep3">Department of Electrical Computer Engineering</orgName>
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attribute-augmented Semantic Hierarchy Towards Bridging Semantic Gap and Intention Gap in Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DEDFA4AC6195DE707A3ACB5B3919812</idno>
					<idno type="DOI">10.1145/2502081.2502093</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval Model Algorithms</term>
					<term>Experimentation</term>
					<term>Performance image retrieval</term>
					<term>attribute</term>
					<term>semantic hierarchy User Search Engine Data Query Intention Gap Semantic Gap</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel Attribute-augmented Semantic Hierarchy (A 2 SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in Contentbased Image Retrieval (CBIR). A 2 SH organizes the semantic concepts into multiple semantic levels and augments each concept with a set of related attributes, which describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. A hierarchical semantic similarity function is learnt to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedbacks on attributes and images. These feedbacks are then used to refine the search results based on A 2 SH. We develop a content-based image retrieval system based on the proposed A 2 SH. We conduct extensive experiments on a large-scale data set of over one million Web images. Experimental results show that the proposed A 2 SH can characterize the semantic affinities among images accurately and can shape user search intent precisely and quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Content-based Image Retrieval (CBIR), a technique for retrieving images from a large database of digital images based on visual content, has been studied extensively since the early 1990s <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b23">23]</ref>. It has gained increasing importance in both the academia and industry, in the current era of social media, because of the explosive growth of images shared in cyberspace and the compelling demands in various multimedia applications for Web and mobile clients. In spite of the remarkable progress made in the last two decades, CBIR remains challenging mainly due to two critical scientific problems for image retrieval as shown in Figure <ref type="figure" target="#fig_0">1:</ref> (a) the Semantic Gap between the low-level visual features and high-level semantics <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b2">2]</ref>; and (b) the Intention Gap between user's search intent and the query <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b11">11]</ref>, which hinders the understanding of user's intent behind a query.</p><p>Recent studies, especially those on TRECVID <ref type="bibr" target="#b21">[21]</ref>, have shown that a promising route to narrowing the semantic gap is to exploit a set of concepts to form the semantic description of visual content <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b19">19]</ref>. As the amount and scope of semantic concepts increase, semantic hierarchy, such as Im-ageNet <ref type="bibr" target="#b4">[4]</ref> and LSCOM <ref type="bibr" target="#b20">[20]</ref>, has been developed to organize the semantic concepts from general to specific and essentially partition the semantic space hierarchically, towards better addressing the semantic gap problem. The semantic hierarchy has been found to be encouraging in improving the understanding of visual content recently <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b18">18]</ref>. However, there still lacks the correspondences between visual features and semantics, due to the intra-concept variations and interconcept similarities on visual properties. On the other hand, to address the intention gap problem, Relevance Feedback (RF) has been introduced into image retrieval. RF collects user feedbacks on candidate images, indicating them as "relevant" or "irrelevant" and lets the system to infer users' search intent from the labeled images <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b1">1]</ref>. Due to the discrepancy between user's intent and low-level visual cues, RF is often ineffective in narrowing search to target in practice. Motivated by the above observations, in this paper, we propose a novel Attribute-augmented Semantic Hierarchy (A 2 SH), towards narrowing both the semantic and intention gaps, and demonstrate its effectiveness in both automatic and interactive Content-based Image Retrieval. Here, attributes refer to semantic descriptions of concepts such as the visual appearances (e.g., "round" as shape, "metallic" as texture), sub-components (e.g., "has wheel", "has leg"), and various discriminative properties (e.g., "properties that dog has but cat do not"). Figure <ref type="figure" target="#fig_1">2</ref> shows an illustration of A 2 SH. A semantic hierarchy consisting of semantic concepts is augmented by a pool of attributes. Each semantic concept is linked to a set of related attributes. For example, "car" is augmented by the attributes "window" and "metallic", etc. These attributes are specifications of the multiple facets of the corresponding concept and can act as an intermediate bridge connecting the concept and low-level visual cues. Moreover, they span a local semantic space in the context of the concept. On the other hand, the same attribute may have different semantics in the context of different concepts. For example, the attribute "wing" of concept "bird " refers to appendages that are feathered; while the same attribute refers to metallic appendages in the context of "jet". Hence, associating an attribute to concepts can reveal the heterogeneous meanings of the same attribute. We equip A 2 SH with a set of concept classifiers and attribute classifiers. The concept classifier is used to predict the presence of a concept in images, while the attribute classifier aims to predict the presence of the attributes in the context of its associated concept. As a result, A 2 SH is able to interpret the semantics of image content with a hierarchial semantic representation. In particular, an image can be represented as the responses from the concept classifiers as well as the linked attribute classifiers, leading to a hierarchical interpretation consisting of multiple levels of semantic granulations. Based on such interpretation, we develop a hierarchical semantic similarity function to precisely characterize the semantic similarities between images. The semantic similarity between any two images is computed as a hierarchical aggregation of their similarities in the local semantic spaces of their common semantic concepts at multiple levels. In the local semantic space of each concept, a local semantic metric is learnt to capture the semantic affinities between images in the context of the concept.</p><p>Based on the above A 2 SH, we develop a content-based image retrieval system, which supports both automatic retrieval and interactive retrieval with user feedbacks. We expect A 2 SH to both effectively narrow the semantic gap by structurlizing the semantics of image content with semantically meaningful hierarchical representations in terms of concepts and attributes, as well as the intention gap by shaping user search intent accurately from the feedbacks. The system flowchart is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. In the offline part, we first learn the concept classifiers, attribute classifiers, and hierarchial semantic similarity function to equip A 2 SH. We next use A 2 SH to process the database images and obtain their hierarchical semantic representations. All the images are indexed hierarchically based on their semantic paths in the hierarchy to enable efficient large-scale retrieval. In the online part, a given query image is first processed by A 2 SH, getting its hierarchical semantic representation, based on which a collection of candidate images are returned from the database according to the indexing. Similar images are then retrieved from the candidate set based on their hierarchical semantic similarities to the query. After the automatic retrieval, we present the results to solicit user feedbacks. We enable a broad channel of feedback to help user deliver search intent by providing hybrid feedbacks on attributes and images. While the image feedbacks collect positive and negative samples of user intent, the feedbacks on attributes compose a clearer semantic description of the intent <ref type="bibr" target="#b39">[39]</ref>, such as "has head and leg, not furry." These hybrid feedbacks are analyzed by A 2 SH, leading to a precise semantic interpretation of user intent, and are used to refine the search results. We expect the hybrid feedbacks leading to better search results with less interaction effort.</p><p>We evaluate the proposed system on a large-scale corpus of over one million Web images. The experimental results have demonstrated the superiority of the proposed system over state-of-the-arts CBIR approaches. The main contributions of this paper are summarized as follows:</p><p>• We propose a novel Attribute-augmented Semantic Hierarchy (A 2 SH), in which each concept is augmented by a set of related attributes. A 2 SH models the seman-tics of images in the form of a hierarchical semantic representation, which is semantically meaningful.</p><p>• We develop a CBIR system based on the proposed A 2 SH and demonstrate the effectiveness of A 2 SH in narrowing the semantic and intention gaps in image retrieval over a large-scale image data set.</p><p>• We learn a hierarchical semantic similarity function, which is able to accurately characterize the semantic affinities among images. Moreover, we develop a hybrid feedback mechanism to collect feedbacks on both attributes and images, which can help capture users' search intent more precisely based on A 2 SH.</p><p>The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 describes the elementary building blocks of the proposed A 2 SH, including the concept classifiers, attribute classifiers, and hierarchical semantic similarity function. Section 4 elaborates the automatic and interactive image retrieval based on the proposed A 2 SH. Experimental results and analysis are reported in Section 5, followed by conclusions and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Hierarchy</head><p>A semantic hierarchy is a formally defined taxonomy or ontology, where each node represents a semantic concept, such as WordNet <ref type="bibr" target="#b10">[10]</ref>, ImageNet <ref type="bibr" target="#b4">[4]</ref>, and LSCOM <ref type="bibr" target="#b20">[20]</ref> etc. It organizes semantic concepts from general to specific and has been shown to be effective in boosting visual recognition <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b18">18]</ref> and retrieval <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b3">3]</ref>. Most of the recent works of exploiting semantic hierarchy for image retrieval focus on designing a semantic similarity function that embeds hierarchical information. In the method proposed by Deselaers and Ferrari <ref type="bibr" target="#b5">[5]</ref>, given two images, their visual nearest neighbor images were first found, and their semantic distance was computed as a distance between the concepts of their neighbors. Deng et al. <ref type="bibr" target="#b3">[3]</ref> developed a hierarchical bilinear similarity function for image retrieval. They first represented an image as a semantic vector z consisting of its relevances to a set of concepts, and defined the bilinear similarity between any two images as z T i Szj , where S is a matrix encoding the pairwise semantic affinities among the concepts. They have shown that this method achieves the state-of-the-art performance of image retrieval on ImageNet. Recently, Verma et al. <ref type="bibr" target="#b35">[35]</ref> proposed to associate separated visual similarity metrics for every concept in a hierarchy and then learn the metrics jointly through an aggregated hierarchical metric. Different from existing works based on conventional semantic hierarchy, our work augments semantic hierarchy with a pool of attributes. The augmented hierarchy has better capacity in modeling the semantics of images. We characterize the semantic similarities among images by a hierarchical similarity function composed by a set of local semantic metrics learnt in the semantic spaces spanned by attributes in the context of various concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attributes</head><p>Attributes have attracted increasing attentions recently. Attributes refer to semantic descriptions of concepts such as sub-components (e.g., "has wheel"), the visual appearances (e.g., "round" as shape), and various discriminative properties (e.g., "properties that dog has but cat do not"). Attributes are semantically meaningful as opposed to lowlevel visual features, and they are relatively easier to recognize automatically instead of the full concepts (e.g., "dog", "car") <ref type="bibr">[8]</ref>. Attributes can be exploited as intermediate-level descriptors of images to boost visual recognition <ref type="bibr">[8,</ref><ref type="bibr" target="#b17">17]</ref> and retrieval <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b9">9]</ref>. For example, Jaimes et al. <ref type="bibr" target="#b12">[12]</ref> proposed a conceptual framework for indexing visual information based on semantic concepts and attributes. Douze et al. <ref type="bibr" target="#b6">[6]</ref> proposed to represent an image by the concatenation of visual features and its response from attribute classifiers, and have shown that such representation can improve image retrieval significantly compared to pure visual features. Scheirer et al. <ref type="bibr" target="#b26">[26]</ref> developed a probabilistic normalization method to normalize the responses from attribute classifiers, and have shown that the normalized representation is more effective. While most existing works represented images in the form of a flat semantic representations in terms of attributes, our work associate attributes to the concepts in a semantic hierarchy and represent images in the form of a hierarchical semantic representations, which are more semantically meaningful. Moreover, as aforementioned, associating an attribute to concepts can reveal the heterogeneous meanings of the same attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Feedback</head><p>Relevance Feedback (RF) is a key technique to narrow down the intention gap in image retrieval <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b2">2]</ref>. It attempts to capture users' search intent by iteratively collecting users' feedbacks on the retrieved images and refining the retrieval based on the feedbacks. A wealth of RF methods has been proposed to refine the original query or learn a relevance ranking function from the feedbacks, i.e., the "relevant" and "irrelevant" images labeled by users. For example, the Query Point Movement (QPM) method <ref type="bibr" target="#b24">[24]</ref> gradually refines the query point by moving it towards the "relevant" images and away from the "irrelevant" images. Tong et al. <ref type="bibr" target="#b34">[34]</ref> proposed to learn a Support Vector Machine (SVM) from the "relevant" and "irrelevant" images, and then rank the images according to their responses from the SVM classifier. Tao et al. <ref type="bibr" target="#b33">[33]</ref> proposed a asymmetric bagging and random subspace SVM method to learn a robust classifier from user feedbacks. Recently, Zhang et al. <ref type="bibr" target="#b39">[39]</ref> developed an attribute feedback scheme, which collects user feedbacks on semantic attributes and ranks images according to the presence probabilities of the attributes in the images. Kovashka et al. <ref type="bibr" target="#b13">[13]</ref> proposed to collect the user's relative judgments on attributes, such as "show me shoes that are more formal than these and shinier than those," to improve textbased image retrieval. Compared to these works, our work enables a broad channel of feedback to help user better delivery search intent by providing hybrid feedbacks on attributes and images. By modeling the feedbacks based on A 2 SH, our work leads to a better understanding of user intent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ATTRIBUTE-AUGMENTED SEMANTIC HIERARCHY</head><p>In this section, we first present a definition of the proposed Attribute-augmented Semantic Hierarchy (A 2 SH) as follows:</p><p>Definition 1. Attribute-augmented Semantic Hierarchy is a directed acyclic graph H = (C, A, EC, ECA), consisting of a set of concepts C = {c}, a pool of attributes A = {a}, a set of concept-concept edges EC , where an edge is an ordered pair of concepts in C ×C, and a set of concept-attribute edges ECA, where an edge is an unordered pair of a concept and an attribute in C×A. The set of attributes linked to concept c is Ac.</p><p>A 2 SH organizes semantic concepts from general to specific, where each concept is augmented with a set of related attributes. These attributes comprehensively describe the multiple semantic facets of the concept, and span a local semantic space tailored to the concept. Next, we equip A 2 SH with concept classifiers, attribute classifiers, and a hierarchical semantic similarity function. In particular, each of the concept classifiers predicts the presence of a semantic concept c in images. The attribute classifiers for the attributes Ac linked to concept c predict the presence of the attributes in the context of c. With such hierarchy, a given image can be represented as the responses from the concept classifiers as well as the linked attribute classifiers, leading to a hierarchical semantic interpretation consisting of semantics at multiple levels. The similarity between two images is computed by the hierarchical semantic similarity function, which aggregates their local similarities in the context of their common semantic concepts at multiple levels. A local semantic metric is learnt in the local semantic space of each concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept Learning</head><p>A concept classifier fc : X → {-1, +1} predicts whether an image belongs to concept c, where X is an arbitrary feature space. Generally, given the concept classifiers in a hierarchy, the semantic path of an image can be efficiently predicted by the classifiers in a top-down fashion <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b18">18]</ref>. Here, a semantic path P is a set of multi-level semantics P = (c0 → ... → cn) from the root c0 and satisfies ∀i &gt; 0, fc i (x) = +1. Next, we introduce the learning of the concept classifiers.</p><p>One way to learn each concept classifier is to use the conventional "one-vs-all" strategy, that is, learning the classifier by the images are from the concept as positive samples and images from others as negative samples. However, this strategy neglects the hierarchical relation among concepts, resulting in classifiers ineffective for hierarchical classification. Another way for concept classifier learning is to locally train the classifier for a concept by using the images from its siblings as negative samples <ref type="bibr" target="#b18">[18]</ref>. However, this local training strategy results in classifiers that suffer from the "error prorogation" problem. To address the above problems, we here use the "hierarchical one-vs-all" strategy <ref type="bibr" target="#b32">[32]</ref> to learn concept classifiers by exploiting the hierarchical relation among concepts and collecting training samples globally in the hierarchy. In particular, the positive training set P os(c) and the negative training set N eg(c) are constructed as follows:</p><formula xml:id="formula_0">P os(c) = {Ii, s.t. L(Ii) ∩ (c ∪ descend(c))}, N eg(c) = {Ii, s.t. Ii / ∈ P os(c)},<label>(1)</label></formula><p>where L(Ii) ⊆ C is the set of concept labels for sample Ii.</p><p>For each concept c, the positive training set P os(c) consists of images labeled as either the concept itself or one of its descendant concepts; while the negative training set N eg(c) contains images which are not in P os(c). Based on P os(c) and N eg(c), we train a binary linear Support Vector Machine (SVM) as the concept classifier fc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute Learning</head><p>As aforementioned, we augment the concepts in a semantic hierarchy using a pool of attributes. Here, we exploit two types of attributes, including nameable attributes and unnameable attributes. Nameable attributes refer to the attributes that are human-nameable, such as the visual appearances and sub-components of a concept <ref type="bibr" target="#b39">[39]</ref>. Moreover, a bunch of discriminative properties among concepts, such as "properties that dog has but cat do not" are automatically discovered. These discriminative properties are termed as unnameable attributes, since they are hard to be articulated explicitly by human. Such unnameable attributes are important in depicting a concept especially when the concept shares most of the nameable attributes with many others concepts. For example, "dog" and "cat" may share many nameable attributes, such as "furry", "tail", etc. while unnameable attributes need to be learned to differentiate the fur of cats from that of dogs using image examples. Together they offer a comprehensive description of the multiple facets of a concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Nameable Attribute Learning</head><p>A nameable attribute classifier f c a : V → {-1, +1} predicts the presence of a nameable attribute a of concept c in an image, where V is an arbitrary visual feature space. This classifier is learnt in the context of c, i.e., using the positive samples of c with ground truth labelings of attribute a. Note that attributes normally correspond to partial visual cues of the whole image. For example, a component attribute may only appear at one or more regions in the image, and an appearance attribute may correspond to only partial channels of visual descriptors. Hence, the visual feature V describing the whole image may not characterize the attributes well. This motivates us to perform feature selection towards selecting the most informative feature V c a for learning the attribute classifier f c a . We propose a hierarchical feature selection mechanism to select features in a bottom-up fashion <ref type="bibr">[8,</ref><ref type="bibr" target="#b39">39]</ref>. Without loss of generality, we start with selecting visual features V c a for attribute a in the context of concept c. Suppose we have already selected the most informative features V c ′ a for attribute a in the context of c ′ ∈ child(c). We merge these features to form the following base features V c a :</p><formula xml:id="formula_1">V c a = c ′ ∈child(c) V c ′ a .<label>(2)</label></formula><p>The base features serve as a set of candidate features, from which we perform feature selection to discover the most informative features for learning a in the context of c. Intuitively, selecting features from the base features V c a instead of the raw feature V leads to more informative features for learning attribute a. For example, it is more effective to select features for the attribute "head" of "animal" from the union of the features for 'head" of "dog", "cat", etc. Given the positive samples Ic of c labeled with/without attribute a, we train an ℓ1-norm linear regressor to select V c a from V c a . For attributes appearing in most of Ic, there are insufficient negative samples of a left. In order to learn an effective linear regressor, we instead access the images of c's ancestors until sufficient negative samples of a are collected. The regression results in a sparse set of model parameters where the nonzero elements correspond to feature dimensions that are selective for a. By selecting the nonzero dimensions of V c a , we finally get the most informative feature V c a , based on which we train a linear SVM as the attribute classifier f c a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Unnameable Attribute Discovery</head><p>Since unnameable attributes are usually hard to be articulated by human, we cannot manually label images being positive/negative to an unnameable attribute and thus cannot obtain unnameable attributes classifiers by supervised learning. Hence, we propose to automatically discover unnameable attributes in an unsupervised fashion. Inspired by <ref type="bibr" target="#b22">[22]</ref>, we define unnameable attributes as the hypotheses that help in distinguishing a concept and its siblings. Next, we detail an iterative approach for discovering unnameable attributes in the context of concept c.</p><p>At each iteration t, we maintain an attribute set At containing nameable attributes of the concepts Cc = c∪sibling(c), and unnameable attributes discovered thus far. We are concerned with the classification among the images of the concepts in Cc, denoted as I, represented by the responses from the attribute classifiers of At. We use a nearest neighbor classifier to classify I into Cc. Based on the classification result, we construct a symmetric confusion matrix<ref type="foot" target="#foot_0">2</ref> , which can be viewed as a fully connected graph whose nodes correspond to Cc. A strong edge weight indicates high confusion between the concepts linked by the edge. Next, we perform a spectral clustering algorithm on this graph to obtain several clusters. Each cluster is a subset of concepts that are most confused with each other. For images in the i-th cluster, we employ an unsupervised max-margin clustering algorithm <ref type="bibr" target="#b40">[40]</ref> to generate a hyperplane separating them into two classes. The hyperplane serves as a hypothesis that helps in distinguishing the most confused concepts in the i-th cluster. We regard the hypothesis as an unnameable attribute ai. Then, we learn a linear SVM classifier f c a i for ai using the images of the two classes split by ai. Finally, suppose that we have discovered m unnameable attributes at iteration t, we add them to the attribute set At+1 ← {At, a1, ..., am}, which is in turn used for the next iteration. The discovery process ends if no more new hypotheses can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Semantic Similarity Learning</head><p>From the concept and attribute classifiers learnt above, we can generate a hierarchical semantic representation of an image as {(c0 → ... → cn); (z c 0 , ..., z cn )}, where (c0 → ... → cn) is the semantic path predicted by concept classifiers, c0 is the root of the hierarchy and (z c 0 , ..., z cn ) is the local semantic representations in terms of attributes along the path. Specifically, z c is composed by the responses from the attribute classifiers of concept c as follows:</p><formula xml:id="formula_2">z c = f c a 1 (x), f c a 2 (x), ..., f c a |Ac| (x) T ,<label>(3)</label></formula><p>where f c a is the classifier for attribute a ∈ Ac. We normalize f c a into the range of [0, 1] by a probabilistic strategy <ref type="bibr" target="#b26">[26]</ref>.</p><p>With such hierarchical semantic representation of images, we formulate a hierarchical semantic similarity function to precisely characterize the semantic similarities between images by aggregating their local similarities along their common semantic paths. The hierarchical semantic similarity between any two images is defined as follows:</p><formula xml:id="formula_3">S(Ii, Ij) = c∈P ij s(Ii, Ij; c),<label>(4)</label></formula><p>where Pij is the common semantic path of image Ii and Ij, s(Ii, Ij; c) is the local similarity between Ii and Ij in the context of c along the path Pij .</p><p>There are two conventional ways to define s(Ii, Ij; c). The first is to set s(Ii, Ij; c) to 1, such that S(Ii, Ij) is reduced to the length of the common path of Ii and Ij. This lacks the fine characterization of the semantic affinities between the images along the path. The second is to calculate s(Ii, Ij; c) as the visual similarity. This measurement suffers from the discrepancy between visual similarity and semantic similarity. Hence they are both unable to characterize the semantic affinities between images well. In order to precisely characterize the semantic similarities between images, we propose to learn a local semantic metric in the local semantic space of each concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Local Semantic Metric Learning</head><p>We define the local semantic distance between two images in the local semantic space of concept c as:</p><formula xml:id="formula_4">d z c i , z c j ; c = (z c i -z c j ) T Mc(z c i -z c j ), (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where Mc is a positive semi-definite symmetric matrix of size |Ac| × |Ac|. Mc is the local semantic metric, which needs to be learnt to bring together the images of the same concept as close as possible and sperate the images of different concepts as far as possible. In particular, with Mc, we expect the neighbor samples within the same semantic class c to be as close as possible, towards preserving the fine neighborhood relation within the class, and the samples from the siblings of c to be separated away with a large margin. To achieve this, for image Ii ∈ P os(c), we require the distance between Ii and its K-nearest neighbors Ij ∈ P os(c) as small as possible. P os(c) are the images belong to concept c. We denote j i as such neighborhood. Moreover, the distance between Ii and Ij should be smaller than that between Ii and any image I k from sibling concepts. Let S(c) denote the set of images of sibling concepts, we can have a set of training triples as T = {(i, j, k) : j i, Ii ∈ P os(c), I k ∈ S(c)}, based on which we formulate the metric learning objective as follows: min</p><formula xml:id="formula_6">Mc j i d 2 (z c i , z c j ; c) + λ (i,j,k)∈T ξ ijk s.t. ∀(i, j, k) ∈ T , d 2 (z c i , z c k ; c) -d 2 (z c i , z c j ; c) ≥ 1 -ξ ijk , ξ ijk ≥ 0, Mc 0,<label>(6)</label></formula><p>where λ &gt; 0 is the regularization constant. We employ the LMNN solver <ref type="bibr" target="#b37">[37]</ref> modified with the above defined training triplets T to solve the metric learning problem. Note that solving the above problem is very efficient since the local semantic space is compact, i.e., the dimension of Mc is low. With Mc we can compute the local semantic similarity between images as:</p><formula xml:id="formula_7">s(Ii, Ij; c) = exp(-d(z c i , z c j ; c)),<label>(7)</label></formula><p>which is in turn used to compose the hierarchical semantic similarities in Eq. ( <ref type="formula" target="#formula_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMAGE RETRIEVAL WITH A 2 SH</head><p>In this section, we develop a content-based image retrieval system based on A 2 SH. The system enables efficient and effective automatic retrieval and interactive retrieval with hybrid feedbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic Retrieval with Hierarchical Indexing</head><p>A 2 SH provides a much more efficient similarity search due to the aforementioned compact hierarchical semantic representations. However, the cost of linear scan of the entire database can be very high even for such a compact representation especially for large-scale databases. In order to support efficient large-scale image retrieval, we develop a hierarchical indexing strategy. All the images are indexed hierarchically based on their semantic paths in the hierarchy. We define an index file as follows:</p><formula xml:id="formula_8">Indc := &lt; c, child(c), Ic &gt;,<label>(8)</label></formula><p>where child(c) is the children of the concept c, and Ic is the set of database images whose predicted semantic paths terminate at c.</p><p>Given a query image Iq, the retrieval with the hierarchical indexing is as follows. First, we generate the hierarchical semantic representation of Iq along its semantic path c0 → ... → cn based on A 2 SH. Next, we perform fast retrieval of candidate images by looking-up the index file Indc n . The candidate images consist of the images indexed by cn and its children child(cn). Note that the number of the candidate images is significantly reduced compared to the size of entire database. These candidate images are then ranked according to their hierarchical semantic similarities to the query as in Eq. ( <ref type="formula" target="#formula_3">4</ref>). In practice, as the semantic path prediction may not be perfect, cn may not be exactly the same as the ground truth semantic path terminal of the query image. To address this problem, we set a look-back level b (b = 3 in the experiments) and retrieve more candidate images by the index file Indc n-b . Note that when b = n, it retrieves all the database images as candidates, degenerating to linear scan.</p><p>Next, we analyze the time complexity of the retrieval, including three major steps: 1) semantic path prediction, 2) looking-up the index file to obtain candidate images and 3) generating Top K results according to the hierarchical semantic similarities of the candidate images. Denote the averaged fan-out (i.e., averaged number of the children of a concept in the hierarchy) of A 2 SH as F , the averaged leaf depth (i.e., averaged depth over all the leaves) as D, and the concept classifier prediction cost as C. Therefore, we can estimate cost of the semantic path prediction at O(nF C), where n ≤ D+b is the average depth of the predicted path, and the candidate images retrieval cost at O(F D-n+b ), which is the cost for sub-hierarchy traversal. Note that C is a small constant, D and F are 6.3 and 3.8 in our ImageNet hierarchy, respectively. Hence, the prediction cost and the candidate retrieval cost are very small, and the time cost for for retrieval is mainly from the third step, i.e., ranking candidate images, which has a time complexity of O(ndNc+Nc log Nc), where d is the average dimensions of local semantic spaces, and Nc is the number of the candidate images, which is much smaller than the size of the entire database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interactive Retrieval with Hybrid Feedback</head><p>Because of the presence of intention gap that hinders the understanding of user search intent by the system, the results from automatic retrieval often do not satisfy users' information needs. We therefore execute interactive retrieval by involving users' interaction with the system. We pro-pose a Hybrid Feedback (HF) mechanism to help user deliver search intent by providing hybrid feedbacks on both the attributes and images. In particular, we allow a user to give "yes"/"no" feedbacks on attributes to state which attributes are in or not in his/her search intent, as well as relevance judgements on images to indicate which images are "relevant" or "irrelevant" to the intent. These hybrid feedbacks are then used to generate a precise semantic interpretation of user intent based on the proposed A 2 SH. By iteratively collecting user feedbacks and refining the retrieval, the system can shape user intent more accurately and narrow the search to target gradually.</p><p>Suppose we are at the t-th feedback iteration. The system records the "relevant" images as Rt and the "irrelevant" images as Rt, as well as the "yes" attributes as Bt and the "no" attributes as Bt. Suppose the hierarchical semantic representation of a query image is {Q = (c0 → ... → cn); Z = (z c 0 , ..., z cn )}, where Q is the semantic path and Z is the set of local semantic representations along the path. We refine the query representation at iteration t: Zt, tailoring it to user intent by incorporating semantic descriptions delivered by image feedbacks (i.e., Rt and Rt), and attribute feedbacks (i.e., Bt and Bt). More specifically, we refine the query Zt to make it close to the semantic representation of relevant images while away from that of the irrelevant ones. This refinement is carried out along the semantic path for every local semantic representation of the query, leading to a hierarchical semantic interpretation of user intent. Formally, for ∀c ∈ Q, we have</p><formula xml:id="formula_9">z c t+1 [a] =z c t [a] + β i∈Rt (z c i [a] -z c t [a])/|Rt| -γ j∈Rt (z c j [a] -z c t [a])/|Rt|,<label>(9)</label></formula><p>where β and γ are trade-off parameters. Through image feedbacks, the semantic representation of the query is shaped closer towards the semantic representations of relevant images as well as farther away from those of irrelevant ones. User feedbacks on attributes Bt and Bt state the desired and undesired attributes, respectively. That is to say, the attributes in Bt are expected be included in the query, while the attributes in Bt are not. Hence, we refine the query Zt by setting the values on the dimensions corresponding to Bt as 1 and the values on the dimensions for Bt as 0. For ∀c ∈ Q, we have</p><formula xml:id="formula_10">∀a ∈ Ac, z c t+1 [a] =    1, a ∈ Bt, 0, a ∈ Bt, z c t [a], otherwise.<label>(10)</label></formula><p>The resultant query Zt+1 is then used to generate the new search results based on the aforementioned hierarchical semantic similarity function. Here, we emphasize the semantic dimensions corresponding to the attributes in Bt and Bt to make them contribute more to the similarity, since they encapsulate users' clear intent on the attributes. Recall the distance function based on the local semantic metric in Eq. ( <ref type="formula" target="#formula_4">5</ref>). We notice that emphasizing the dimensions is equivalent to giving large weights to the corresponding rows of the metric matrix Mc in similarity calculation. In our experiments, we set the weight to 0.7 for the rows corresponding to the attributes in Bt and Bt, and 0.3 for the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we systematically evaluate the proposed Attribute-augmented Semantic Hierarchy (A 2 SH) in contentbased image retrieval. We first evaluate the elementary building blocks of A 2 SH. Then, we investigate the effectiveness of A 2 SH in automatic and interactive image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Dataset Description</head><p>We conducted experiments on ImageNet <ref type="bibr" target="#b4">[4]</ref>, which is a large-scale corpus of images organized according to the Word-Net hierarchy. Each concept in the hierarchy is depicted by hundreds to thousands of images collected from the Web. We used a subset of ImageNet with 1,860 concepts and 1.27 million images, which were used for ILSVRC 2012 4 . This data set contains a partial WordNet hierarchy and some isolated nodes outside WordNet. We used the WordNet hierarchy for evaluation. This hierarchy consists of 1.22 million images with 1,730 concepts, including 958 leaf concepts. Its maximum depth is 19. We merged the non-leaf nodes with no siblings into their parents since they are the sole heir to the semantics of their parents. This gives rise to a compressed hierarchy with maximum depth of 11, consisting of 1,322 concepts and the original amount of leaf concepts and images. We augmented this hierarchy with a pool of attributes, including nameable and unnameable attributes. We defined 33 nameable attributes 5 based on the attribute pool used in <ref type="bibr">[8]</ref>. These attributes were linked to the concepts in a bottom-up manner. We first associated each leaf concept with its related attributes. Each non-leaf concept was then linked to the union of the attributes from its children. The unnameable attributes were automatically discovered for each concept as described in Section 3.2.2.</p><p>We randomly split the image set into a training subset with 50% of images for each concept, and a subset with the remaining images for testing. We generated ground truth on the images as follows. Based on the labeling of leaf concepts provided by ImageNet, we generated the labeling for each non-leaf concept in a bottom-up manner. A non-leaf concept was regarded as positive to an image if its any child is positive, otherwise, negative. For attributes, we conducted manual labeling. Since manual labeling is labor-intensive and time-consuming, we randomly selected 100 images of each leaf concept from the training subset for ground truth labeling. The attribute labeling in the context of non-leaf concepts was also generated in a bottom-up manner. We conducted image retrieval on the testing subset. We randomly selected 100 images from each of the 958 leaf concepts, giving rise to a total of 95,800 experimental queries. 4 http://www.image-net.org/challenges/LSVRC/2012/index 5 We removed the concept-specific attributes in <ref type="bibr">[8,</ref><ref type="bibr" target="#b39">39]</ref>, such as "jet-engine", since in our work, we have such conceptspecific descriptions by linking the attributes (e.g., "wing") to concepts (e.g., "jet"). We also added seven color attributes because of their effectiveness in image retrieval <ref type="bibr" target="#b25">[25]</ref>. As a result, we have 33 attributes as follows: black, blue, brown, cylinder, furry, glass, gray, green, handle, head, leg, metallic, plastic, rectangular, red, round, scale, screen, shiny, skin, smooth, spotted, stripped, tail, triangle, vegetation, wet, wheel, white, window, wing, wooden, and yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Visual Features</head><p>To represent image content, we extracted four types of visual features: edge, color, texture, and Scale Invariant Feature Transform (SIFT) descriptors <ref type="bibr" target="#b16">[16]</ref>. While edge descriptor was extracted globally from the entire image, the other descriptors were extracted locally. In particular, edges were found using the standard canny detector and their orientations were quantized into 9 unsigned bins. This gives rise to a 9-D edge descriptor for each image. Color descriptors as the 3-channel LAB values were densely extracted from each pixel. Texture descriptors were computed for each pixel as the 48-D responses of texton filter banks <ref type="bibr" target="#b14">[14]</ref>. SIFT descriptors were densely extracted from image patches at multiple scales of {8 × 8, 12 × 12, 16 × 16}-pixel size, with 4-pixel step. For the color, texture, and SIFT descriptors, we adopted the state-of-the-art locality-constrained linear sparse coding (LLC) <ref type="bibr" target="#b36">[36]</ref> method with max-pooling strategy to generate the global representations of images. We used a 512-D codebook for color and texture, 4,096-D for SIFT. As a result, we had 5,129-D global feature representation for each image. Since attributes usually correspond to image regions but not the entire image. To get features better characterizing attributes, we split each image into 2 × 3 grids, and extracted the above features from each grids. Finally, we obtained a 35,903-D (i.e., 5,129 × 7) feature vector for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Experimental Setting</head><p>We learnt linear SVM classifiers for concepts and attributes by employing LIBLINEAR toolbox<ref type="foot" target="#foot_1">1</ref> . We learnt ℓ1 linear logistic regressors to select informative features for learning attributes. For local metric learning, we deployed the LMNN toolbox <ref type="bibr" target="#b37">[37]</ref> with training triplets configuration as described in Section 3.3.1. The algorithmic parameters of the above models were tuned through five-fold cross validation. We applied the Weibull distribution <ref type="bibr" target="#b26">[26]</ref> to normalize the responses from attribute classifiers.</p><p>To evaluate the effectiveness of the proposed A 2 SH in automatic retrieval, we compared it against the following five representative retrieval solutions, including two flat methods and three hierarchical ones. a) fVisual retrieves images based on visual similarities with Eculidean metric; b) fSemantic represents each image into a flat semantic representation composed by the responses from the 1,322 concept classifiers and the 33 attribute classifiers of the root concept. It retrieves images based on such representation using the ℓ1 distance; c) hPath performs retrieval based on the length of the common semantic path of an image and the query; d) hVisual computes the similarities between any two images by aggregating their visual similarities along their common semantic path, then conducts retrieval based on such similarity, and e) hBilinear <ref type="bibr" target="#b3">[3]</ref> retrieves images by the recently proposed bilinear semantic metric which was reported achieving the state-of-the-art performance on Im-ageNet dataset.</p><p>To evaluate the effectiveness of A 2 SH in interactive retrieval, we compared it to the following three interactive retrieval methods. a) QPM <ref type="bibr" target="#b24">[24]</ref>: Query Point Movement method updates the query based on image feedbacks, and refines search results using the new query; b) SVM <ref type="bibr" target="#b34">[34]</ref>: this approach learns a SVM classifier from the "relevant" and "irrelevant" images and ranks images according to their responses from the classifiers; and c) AF <ref type="bibr" target="#b39">[39]</ref>: the recently proposed Attribute Feedback approach collects user feedbacks on attributes and then ranks images according to the presence probabilities of the attributes in the images. Note that our approach enables hybrid feedbacks on attributes and images. For the sake of fair comparison, we incorporated image feedbacks into AF, such that it also uses hybrid feedbacks. Moreover, all the baseline methods were performed on the flat semantic representations of the images, rather than the low-level visual descriptors.</p><p>We conducted the evaluation in two settings with a fixed number of feedbacks and a fixed time limit, respectively. In the first setting, we conducted five feedback iterations with 20 feedbacks per iteration. For the QPM and SVM methods, 20 feedbacks on top 20 images were collected in each iteration. For the AF and our A 2 SH methods, the same number of feedbacks were collected, including 5 attribute feedbacks and 15 image feedbacks. Five informative attributes were suggested in each iteration for soliciting attribute feedbacks. We here employed the suggestion strategy in <ref type="bibr" target="#b39">[39]</ref>. Given a query, the feedback process was simulated by the computer according to the ground truth of the query category on the images and the association between the attributes and the category. In the setting of fixed time limit, we invited 25 novice users to interact with the system through the above four feedback methods, respectively. We did not constrain the numbers of feedbacks and iterations and allowed the users to interact with the system in a free way. Since it is time-consuming to and labor-intensive for users to evaluate a large number of queries. We randomly selected 10 images from each leaf concept as queries, giving rise to 9,580 queries in total, and assigned these queries to the users approximately evenly with no overlap between them. We set the time limit to 2 minutes in the experiments. For a given query in all the above evaluations, we used the search results from the best automatic retrieval method, i.e., the proposed A 2 SH, as the initial results for interactive retrieval.</p><p>All the experiments were conducted on a server with Intel(R) Xeon(R) CPU X5650 at 2.67 GHz on 24 cores, 48GB RAM and 64-bit Centos 5.4 operating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Performance Metric</head><p>We adopted the widely used metric AUC (area under ROC curve) value for classification performance evaluation. We adopted Average Precision at top K retrieved images (AP@K) for retrieval performance evaluation <ref type="bibr" target="#b21">[21]</ref>. Denote R as the number of relevant images in the database. At any ranked position j (1 ≤ j ≤ K), let Rj be the number of relevant images in the top j results and let Ij = 1 if the j-th image is relevant and 0 otherwise, then AP@K is defined as</p><formula xml:id="formula_11">1 min(R,K) K j=1 R j j × Ij.</formula><p>Moreover, we also used the following hierarchical Average Precision at top K for retrieval performance evaluation. The hMAP@K is defined as</p><formula xml:id="formula_12">1 min(R,K) K j=1 D * jq /D * q j</formula><p>, where D * jq is the depth of the lowest common ground truth ancestor of ground-truth concept of the image ranked at position j and the query, D * q is the depth of the ground truth concept of the query. The intuition of hAP@K is that if a returned image does not exactly match the query, it is expected to be as semantically close to the query as possible, in order for a better user experience. We averaged the AP@K and hAP@K over all the queries to compute the MAP@K and hMAP@K, which are overall performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluations of Concept Classifiers, Attribute</head><p>Classifiers, and Local Semantic Metrics  <ref type="bibr" target="#b4">[4]</ref>. From these results, we can see that the concept classifiers at most levels achieve an average AUC above 0.9 except those at depth 8 and 9 with average AUC of 0.89 and 0.88, respectively. The performance of attribute classifiers is illustrated in Figure <ref type="figure" target="#fig_3">3(b)</ref>, from which we can see that the attribute classifiers at every level obtain an average AUC higher than 0.9. These results demonstrate the effectiveness of our concept and attribute classifiers in capturing the semantics of image content, leading to precise hierarchial semantic representations of images.</p><p>A bunch of unnameable attributes are discovered to complement the nameable attributes for better characterizing a concept. Figure <ref type="figure" target="#fig_3">3(c)</ref> shows the average number of unnameable attributes discovered for the concepts at different depth levels. As aforementioned, we have no ground truth of the unnameable attributes on images and thus cannot evaluate the performance of their classifiers directly. Alternatively, we evaluated the effectiveness of unnameable attributes in improving the accuracy of distinguishing sibling concepts. In particular, for each set of sibling concepts in the hierarchy, we used the nearest neighbor classifier to classify their images based on the local semantic representations with or without unnameable attributes. The classification performance is illustrated in Figure <ref type="figure" target="#fig_3">3(d)</ref>. From the results, we can see that the discovered unnameable attributes can improve the classification performance significantly. This indicates that the unnameable attributes can help to provide a more comprehensive and discriminative description of the multiple facets of a concept.</p><p>We evaluated the effectiveness of the local semantic metrics as follows. We used the local semantic metric of each concept to help classifying the images of the concept from those of all its siblings by using the 5-nearest neighbor classifier <ref type="bibr" target="#b37">[37]</ref>. We compared the local semantic metrics again the widely used ℓ1 distance. The classification performance comparison is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>(e). We can see that the proposed local semantic metric outperforms the ℓ1 distance significantly at every depth level. It achieves relative improvement 10.7% at various depth levels on average. This demonstrates the effectiveness of the local semantic metric and its capacity in composing an effective hierarchical semantic similarity to precisely characterize the semantic affinities among images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluations of Automatic Retrieval</head><p>Figure <ref type="figure" target="#fig_4">4</ref> illustrates the performance comparison between the proposed A 2 SH and the other five automatic retrieval methods. We can see that A 2 SH achieves the best retrieval performance in terms of both MAP and hMAP at all the top K results as compared to the other methods. The performance improvements of A 2 SH over the other methods are significant. For example, A 2 SH improves the performance by 22.4%, 23.1%, 41.5%, and 46.0% relatively in terms of MAP at the top 50 results as compared to the hBilinear, fSemantic, hPath, and hVisual methods, respectively. The corresponding performance improvements in terms of hMAP are 7.3%, 20.2%, 31.3%, and 26.5%, respectively. These results demonstrate the effectiveness of A 2 SH in image retrieval. The superiority of A 2 SH to the other methods arises from the following aspects: a) A 2 SH models the semantics of images in the form of a hierarchical semantic representation consisting of multiple levels of concepts, each of which is associated with a local semantic representation in terms of related attributes. Such hierarchical semantic representation provides a more comprehensive and more precise interpretation of image semantics; and b) The hierarchical similarity function in A 2 SH more accurately characterizes the semantic similarities among images by ensembling the local semantic metrics in the context of various concepts. Table <ref type="table" target="#tab_0">1</ref> lists the average retrieval time per query of the five automatic retrieval over the 95,800 queries by the five approaches. We can observe that A 2 SH provides highly efficient retrieval. It significantly reduces the retrieval time by several orders of magnitude compared to the other methods. The reasons are two folds. First, A 2 SH represents images in the form of a compact hierarchical semantic representation, enabling to fast similarity computation. Second, the hier- archical indexing in A 2 SH significantly reduces the size of the search space. For the sake of fair comparison, we also accelerated the other four retrieval methods using indexing techniques. In particular, hVisual was carried out based on the hierarchical indexing in our A 2 SH system. hBilinear was accelerated using the indexing technique in <ref type="bibr" target="#b3">[3]</ref>. Here, we do not list the time cost of the hPath method, since it is a sub-procedure of A 2 SH and hVisual, i.e., retrieving candidate images from the hierarchical index files. The fSemantic method was accelerated by indexing the semantic concepts and attributes using inverted files. We also indexed the lowlevel visual features, which are high-dimensional and sparse as described in Section 5.1.2, by inverted files to accelerate the visual retrieval in fVisual and hVisual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Evaluations of Interactive Retrieval with Hybrid Feedbacks</head><p>Figure <ref type="figure" target="#fig_5">5</ref> illustrates the performance of interactive retrieval with five feedback iterations in terms of MAP and hMAP at the top 20, 50, and 100 search results, respectively. From these results, the following observations can be obtained: a) The proposed A 2 SH based interactive retrieval approach outperforms the other three methods at every iteration and all the top 20, 50, and 100 results; b) A 2 SH significantly reduces the interaction efforts while it achieves comparable performance to the other three methods. For example, consider the MAP at top 20 results, A 2 SH obtains a comparable performance at the 3rd, 2nd, and 2nd iteration as compared to AF, QPM and SVM at the last round, respectively. In other words, A 2 SH can reduce labeling efforts by about 40%, 60%, and 60% as compared to the three methods, respectively; and c) The performance improvements of A 2 SH and AF over QPM and SVM indicate the effectiveness of attribute feedbacks in delivering user search intent. The superiority of A 2 SH to AF demonstrates that A 2 SH can infer user intent more accurately from the feedbacks.</p><p>Table <ref type="table" target="#tab_1">2</ref> lists the performance of the four interactive retrieval approaches with a fixed time limit of 2 minutes. From these results, we can see that the proposed A 2 SH achieves the best performance in terms of both MAP and hMAP at all the top 20, 50, and 100 results. This demonstrates that A 2 SH shapes user intent more precisely and quickly within the same interaction time and can generate more accurate search results as compared to the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we proposed a novel Attribute-augmented Semantic Hierarchy (A 2 SH) which organizes semantic concepts from general to specific, and augments each semantic concept with a set of related attributes, which are specifications of the multiple facets of the concept and act as an intermediate bridge connecting the concept and low-level visual features. We learned the concept classifiers, attribute classifiers, and hierarchical similarity function to equip A 2 SH. Based on the proposed A 2 SH, we developed a content-based image retrieval system supporting both automatic retrieval and interactive retrieval with user feedbacks. A hybrid feedback mechanism was developed to collect broad array of feedbacks on attributes and images. These feedbacks were then utilized to improve the retrieval based on A 2 SH. We systematically evaluated the A 2 SH based image retrieval system on a large-scale corpus of over one million Web images. The experimental results demonstrated the effectiveness of A 2 SH in bridging the semantic and intention gaps, leading to more accurate results compared to the state-ofthe-arts CBIR approaches. We will continue our future works in two directions. First, we will study how to build or refine the A 2 SH automatically by mining concepts, attributes, and their intrinsic relations from online user-generated content. Second, we will apply the proposed A 2 SH to other applications, such as the usergenerated content organization and web video retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the semantic gap and intention gap in image retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed Attribute-augmented Semantic Hierarchy (A 2 SH) and the image retrieval system developed on A 2 SH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance at different depth levels measured by Average AUC: (a) concept classifers; (b) nameable attribute classifiers; (d) classification by exploiting unnameable attributes; (e) local semantic metrics. The average AUC at a depth level is obtained by averaging the AUC values of all the classifiers at that level. The average number of unnameable attribute discovered at different depth levels is shown in the subfigure (c). The depth of the root is 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 (</head><label>3</label><figDesc>Figure 3(a)  shows the average AUC values of the concept classifiers at different depth levels in the hierarchy<ref type="bibr" target="#b4">[4]</ref>. From these results, we can see that the concept classifiers at most levels achieve an average AUC above 0.9 except those at depth 8 and 9 with average AUC of 0.89 and 0.88, respectively. The performance of attribute classifiers is illustrated in Figure3(b), from which we can see that the attribute classifiers at every level obtain an average AUC higher than 0.9. These results demonstrate the effectiveness of our concept and attribute classifiers in capturing the semantics of image content, leading to precise hierarchial semantic representations of images.A bunch of unnameable attributes are discovered to complement the nameable attributes for better characterizing a concept. Figure3(c)shows the average number of unnameable attributes discovered for the concepts at different depth levels. As aforementioned, we have no ground truth of the unnameable attributes on images and thus cannot evaluate the performance of their classifiers directly. Alternatively, we evaluated the effectiveness of unnameable attributes in improving the accuracy of distinguishing sibling concepts. In particular, for each set of sibling concepts in the hierarchy, we used the nearest neighbor classifier to classify their images based on the local semantic representations with or without unnameable attributes. The classification performance is illustrated in Figure3(d). From the results, we can see that the discovered unnameable attributes can improve the classification performance significantly. This indicates that the unnameable attributes can help to provide a more comprehensive and discriminative description of the multiple facets of a concept.We evaluated the effectiveness of the local semantic metrics as follows. We used the local semantic metric of each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Performance of automatic image retrieval over the 95,800 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of interactive retrieval with five feedback iterations over the 95, 800 queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average retrieval time per query of automatic image retrieval over the 95,800 queries.</figDesc><table><row><cell>Method</cell><cell>fVisual</cell><cell>fSemantic</cell><cell>hVisual</cell><cell cols="2">hBilinear A 2 SH</cell></row><row><cell cols="5">Time (ms) 1.18 × 10 4 3.62 × 10 3 7.42 × 10 2 4.47 × 10 2</cell><cell>70.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of interactive retrieval with 2-minute time limit over the 9, 580 queries.</figDesc><table><row><cell></cell><cell></cell><cell>MAP(%)</cell><cell></cell><cell></cell><cell>hMAP(%)</cell><cell></cell></row><row><cell>RF Methods</cell><cell>@20</cell><cell>@50</cell><cell>@100</cell><cell>@20</cell><cell>@50</cell><cell>@100</cell></row><row><cell>A 2 SH</cell><cell cols="6">24.67 22.80 22.03 68.37 66.04 64.08</cell></row><row><cell>AF</cell><cell>22.59</cell><cell>21.38</cell><cell>20.63</cell><cell>62.84</cell><cell>60.20</cell><cell>58.54</cell></row><row><cell>QPM</cell><cell>21.24</cell><cell>20.53</cell><cell>19.52</cell><cell>58.00</cell><cell>56.73</cell><cell>55.83</cell></row><row><cell>SVM</cell><cell>21.56</cell><cell>20.08</cell><cell>19.15</cell><cell>58.50</cell><cell>57.18</cell><cell>55.45</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>It is constructed by the sum of the original confusion matrix and its transposition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>http://www.csie.ntu.edu.tw/ cjlin/liblinear/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work was supported by NUS-Tsinghua Extreme Search (NExT) project under the grant No.: R-252-300-001-490.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relevance feedback for image retrieval: a short survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DELOS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical semantic indexing for large scale image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual and semantic similarity in imagenet</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining attributes and fisher vectors for efficient image retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating concept ontology and multitask learning to achieve more effective classifier training for multilevel image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weak attributes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Wordnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Applications of Ontology: Computer Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intent and its discontents: the user at the wheel of the online video search engine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A conceptual framework for indexing visual information at multiple levels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Internet Imaging</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Whittlesearch: Image search with relative attribute feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Content-based multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Complex event detection via multi-source video attributes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic hierarchies for visual object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On image auto-annotation with latent space models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale concept ontology for multimedia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tesic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Qu ĺ ֒ eenot. Trecvid 2012 -an overview of the goals, tasks, data, evaluation mechanisms and metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRECVID</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactively building a discriminative vocabulary of nameable attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image retrieval: Current techniques, promising directions, and open issues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relevance feedback: a power tool for interactive content-based image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attribute learning in large-scale datasets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-attribute spaces: Calibration for attribute fusion and similarity search</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The state of the art in image and video retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualseek: a fully automated content-based image query system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adding semantics to detectors for video retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Concept-based video retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FTIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Taxonomic classification for web-based videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Support vector machine active learning for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning hierarchical similarity metrics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual query suggestion</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attribute feedback</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Maximum margin clustering made practical</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
