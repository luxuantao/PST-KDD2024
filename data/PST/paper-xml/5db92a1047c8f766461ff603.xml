<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Defense Against Adversarial Images: Turning a Weakness into a Strength</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution. † Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengyuan</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal Contribution. † Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A New Defense Against Adversarial Images: Turning a Weakness into a Strength</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural images are virtually surrounded by low-density misclassified regions that can be efficiently discovered by gradient-guided search -enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed, they are easily bypassed when the adversary has full knowledge of the detection mechanism and adapts the attack strategy accordingly. In this paper, we adopt a novel perspective and regard the omnipresence of adversarial perturbations as a strength rather than a weakness. We postulate that if an image has been tampered with, these adversarial directions either become harder to find with gradient methods or have substantially higher density than for natural images. We develop a practical test for this signature characteristic to successfully detect adversarial attacks, achieving unprecedented accuracy under the white-box setting where the adversary is given full knowledge of our detection mechanism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advance of deep neural networks has led to natural questions regarding its robustness to both natural and malicious change in the test input. For the latter scenario, the seminal work of Biggio et al. <ref type="bibr" target="#b2">[3]</ref> and Szegedy et al. <ref type="bibr" target="#b48">[49]</ref> first suggested that neural networks may be prone to imperceptible changes in the input -the so-called adversarial perturbations -that alter the model's decision entirely. This weakness not only applies to image classification models, but is prevalent in various machine learning applications, including object detection and image segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">55]</ref>, speech recognition <ref type="bibr" target="#b7">[8]</ref>, and deep policy networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The threat of adversarial perturbations has prompted tremendous effort towards the development of defense mechanisms. Common defenses either attempt to recover the true semantic labels of the input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref> or detect and reject adversarial examples <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b55">56]</ref>. Although many of the proposed defenses have been successful against passive attackers -ones that are unaware of the presence of the defense mechanism -almost all fail against adversaries that have full knowledge of the internal details of the defense and modify the attack algorithm accordingly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. To date, the success of existing defenses have been limited to simple datasets with relatively low variety of classes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Recent studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref> have shown that the existence of adversarial perturbations may be an inherent property of natural data distributions in high dimensional spaces -painting a grim picture for defenses. However, in this paper we propose a radically new approach to defenses against adversarial attacks that turns this seemingly insurmountable obstacle from a weakness into a strength: We use the inherent property of the existence of valid adversarial perturbations around a natural image as a signature to attest that it is unperturbed.</p><p>2 Background Attack overview. Test-time attacks via adversarial examples can be broadly categorized into either black-box or white-box settings. In the black-box setting, the adversary can only access the model as an oracle, and may receive continuous-valued outputs or only discrete classification decisions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>. We focus on the white-box setting in this paper, where the attacker is assumed to be an insider and therefore has full knowledge of internal details of the network. In particular, having access to the model parameters allows the attacker to perform powerful first-order optimization attacks by optimizing an adversarial loss function.</p><p>The white-box attack framework can be summarized as follows. Let h be the target classification model that, given any input x, outputs a vector of probabilities h(x) with h(x) y = p(y |x) (i.e. the y -th component of the vector h(x)) for every class y . Let y be the true class of x and L be a continuous-valued adversarial loss that encourages misclassification, e.g., L(h(x ), y) = −cross-entropy(h(x ), y).</p><p>Given a target image x for which the model correctly classifies as arg max y h(x) y = y, the attacker aims to solve the following optimization problem:</p><formula xml:id="formula_0">min x L(h(x ), y) , s.t. x − x ≤ τ.</formula><p>Here, • is a measure of perceptible difference and is commonly approximated using the Euclidean norm • 2 or the max-norm • ∞ , and τ &gt; 0 is a perceptibility threshold. This optimization problem defines an untargeted attack, where the adversary's goal is to cause misclassification. In contrast, for a targeted attack, the adversary is given some target label y t = y and defines the adversarial loss to encourage classification to the target label: L(h(x ), y t ) = cross-entropy(h(x ), y t ).</p><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>For the remainder of this paper, we will focus on the targeted attack setting but any approach can be readily augmented for untargeted attacks as well.</p><p>Optimization. White-box (targeted) attacks mainly differ in the choice of the adversarial loss functions L and the optimization procedures. One of the earliest attacks <ref type="bibr" target="#b48">[49]</ref> used L-BFGS to optimize the cross-entropy adversarial loss in Equation 1. Carlini and Wagner <ref type="bibr" target="#b6">[7]</ref> investigated the use of different adversarial loss functions and found that the margin loss L(Z(x ), y t ) = max</p><formula xml:id="formula_2">y =yt Z(x ) y − Z(x ) yt + κ +<label>(2)</label></formula><p>is more suitable for first-order optimization methods, where Z is the logit vector predicted by the model and κ &gt; 0 is a chosen margin constant. This loss is optimized using Adam <ref type="bibr" target="#b25">[26]</ref>, and the resulting method is known as the Carlini-Wagner (CW) attack. Another class of attacks favors the use of simple gradient descent using the sign of the gradient <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, which results in improved transferability of the constructed adversarial examples from one classification model to another.</p><p>Enforcing perceptibility constraint. For common choices of the measures of perceptibility, the attacker can either fold the constraint as a Lagrangian penalty into the adversarial loss, or apply a projection step at the end of every iteration onto the feasible region. Since the Euclidean norm • 2 is differentiable, it is commonly enforced with the former option, i.e.,</p><formula xml:id="formula_3">min x L(h(x ), y t ) + c x − x 2</formula><p>for some choice of c &gt; 0. On the other hand, the max-norm • ∞ is often enforced by restricting every coordinate of the difference x − x to the range [−τ, τ ] after every gradient step. In addition, since all pixel values must fall within the range [0, 1], most methods also project x to the unit cube at the end of every iteration <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>. When using this option along with the cross entropy adversarial loss, the resulting algorithm is commonly referred to as the Projected Gradient Descent (PGD) attack<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detection Methods and Their Insufficiency</head><p>One commonly accepted explanation for the existence of adversarial examples is that they operate outside the natural image manifold -regions of the space that the model had no exposure to during training time and hence its behavior can be manipulated arbitrarily. This view casts the problem of defending against adversarial examples as a robust classification or anomaly detection problem. The former aims to project the input back to the natural image manifold and recover its true label, whereas the latter only requires determining whether the input belongs to the manifold and reject it if not.</p><p>Detection methods. Many principled detection algorithms have been proposed to date <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b55">56]</ref>. The most common approach involves testing the input against one or several criteria that are satisfied by natural images but are likely to fail for adversarially perturbed images. In what follows, we briefly describe two representative detection mechanisms.</p><p>Feature Squeezing <ref type="bibr" target="#b55">[56]</ref> applies a semantic-preserving image transformation to the input and measures the difference in the model's prediction compared to the plain input. Transformations such as median smoothing, bit quantization, and non-local mean do not alter the image content; hence the model is expected to output similar predictions after applying these transformations. The method then measures the maximum L 1 change in predicted probability after applying these transformations and flags the input as adversarial if this change is above a chosen threshold.</p><p>Artifacts <ref type="bibr" target="#b13">[14]</ref> uses the empirical density of the input and the model uncertainty to characterize benign and adversarial images. The empirical density can be computed via kernel density estimation on the feature vector. For the uncertainty estimate, the method evaluates the network multiple times using different random dropout masks and computes the variance in the output. Under the Bayesian interpretation of dropout, this variance estimate encodes the model's uncertainty <ref type="bibr" target="#b15">[16]</ref>. Adversarial inputs are expected to have lower density and higher uncertainty than natural inputs. Thus, the method predicts the input as adversarial if these criteria are below or above a chosen threshold.</p><p>Detectors that use multiple criteria (such as Feature Squeezing and Artifacts) can combine these criteria into a single detection method by either declaring the input as adversarial if any criterion fails to be satisfied, or by training a classifier on top of them as features to classify the input. Other notable useful features for detecting adversarial images include convolutional features extracted from intermediate layers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>, distance to training samples in pixel space <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>, and entropy of non-maximal class probabilities <ref type="bibr" target="#b36">[37]</ref>.</p><p>Bypassing detection methods. While the approaches for detecting adversarial examples appear principled in nature, the difference in settings from traditional anomaly detection renders most techniques easy to bypass. In essence, a white-box adversary with knowledge of the features used for detection can optimize the adversarial input to mimic these features with gradient descent. Any nondifferentiable component used in the detection algorithm, such as bit quantization and non-local mean, can be approximated with the identity transformation on the backward pass <ref type="bibr" target="#b0">[1]</ref>, and randomization can be circumvented by minimizing the expected adversarial loss via Monte Carlo sampling <ref type="bibr" target="#b0">[1]</ref>. These simple techniques have proven tremendously successful, bypassing almost all known detection methods to date <ref type="bibr" target="#b5">[6]</ref>. Given enough gradient queries, adversarial examples can be optimized to appear even "more benign" than natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detection by Adversarial Perturbations</head><p>In this section we describe a novel approach to detect adversarial images that relies on two principled criteria regarding the distribution of adversarial perturbations around natural images. In contrast to the shortcomings of prior work, our approach is hard to fool through first-order optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Criterion 1: Low density of adversarial perturbations</head><p>The features extracted by convolutional neural networks (CNNs) from natural images are known to be particularly robust to random input corruptions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref>. In other words, random perturbations applied to natural images should not lead to changes in the predicted label (i.e. an adversarial image).</p><p>Our first criterion follows this intuition and tests if the given input is robust to Gaussian noise:</p><p>C1: Robustness to random noise. Sample ∼ N (0, σ<ref type="foot" target="#foot_1">2</ref> I) (where σ 2 is a hyperparameter) and compute ∆ = h(x) − h(x + ) 1 . The input x is rejected as adversarial if ∆ is sufficiently large.           </p><formula xml:id="formula_4">Q v B O M r 3 O / 8 8 i 1 E b G 6 w 0 n C / Y g O l Q g F o 2 i l h 1 5 E c R S E 2 d O 0 X 6 2 5 d X c G s k y 8 g t S g Q L N f / e o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P K 7 3 U 8 I S y M R 3 y r q W K R t z 4 2 S z x l J x Y Z U D C W N u n k M z U 3 x s Z j Y y Z R I G d z B O a R S 8 X / / O 6 K Y a X f i Z U k i J X b P 5 R m E q C M c n P J w O h O U M 5 s Y Q y L W x W w k Z U U 4 a 2 p I o t w V s 8 e Z m 0 z + q e W / d u z 2 u N q 6 K O M h z B M Z y C B x f Q g B t o Q g s Y K H i G V</formula><formula xml:id="formula_5">Q v B O M r 3 O / 8 8 i 1 E b G 6 w 0 n C / Y g O l Q g F o 2 i l h 1 5 E c R S E 2 d O 0 X 6 2 5 d X c G s k y 8 g t S g Q L N f / e o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P K 7 3 U 8 I S y M R 3 y r q W K R t z 4 2 S z x l J x Y Z U D C W N u n k M z U 3 x s Z j Y y Z R I G d z B O a R S 8 X / / O 6 K Y a X f i Z U k i J X b P 5 R m E q C M c n P J w O h O U M 5 s Y Q y L W x W w k Z U U 4 a 2 p I o t w V s 8 e Z m 0 z + q e W / d u z 2 u N q 6 K O M h z B M Z y C B x f Q g B t o Q g s Y K H i G V</formula><formula xml:id="formula_6">Q v B O M r 3 O / 8 8 i 1 E b G 6 w 0 n C / Y g O l Q g F o 2 i l h 1 5 E c R S E 2 d O 0 X 6 2 5 d X c G s k y 8 g t S g Q L N f / e o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P K 7 3 U 8 I S y M R 3 y r q W K R t z 4 2 S z x l J x Y Z U D C W N u n k M z U 3 x s Z j Y y Z R I G d z B O a R S 8 X / / O 6 K Y a X f i Z U k i J X b P 5 R m E q C M c n P J w O h O U M 5 s Y Q y L W x W w k Z U U 4 a 2 p I o t w V s 8 e Z m 0 z + q e W / d u z 2 u N q 6 K O M h z B M Z y C B x f Q g B t o Q g s Y K H i G V 3 h z j P P i v D s f 8 9 G S U + w c w h 8 4 n z / 9 O Z E c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P / 7 8 W o K P q T P M A m r n J 7 K 8 s b y W L s Q = " &gt; A A A B 8 X i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e i F 4 8 V b C u 2 o W y 2 m 3 b p Z h N 2 X 8 Q S + i + 8 e F D E q / / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0 T Z x q x l s s l r G + D 6 j h U i j e Q o G S 3 y e a 0 y i Q v B O M r 3 O / 8 8 i 1 E b G 6 w 0 n C / Y g O l Q g F o 2 i l h 1 5 E c R S E 2 d O 0 X 6 2 5 d X c G s k y 8 g t S g Q L N f / e o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P K 7 3 U 8 I S y M R 3 y r q W K R t z 4 2 S z x l J x Y Z U D C W N u n k M z U 3 x s Z j Y y Z R I G d z B O a R S 8 X / / O 6 K Y a X f i Z U k i J X b P 5 R m E q C M c n P J w O h O U M 5 s Y Q y L W x W w k Z U U 4 a 2 p I o t w V s 8 e Z m 0 z + q e W / d u z 2 u N q 6 K O M h z B M Z y C B x f Q g B t o Q g s Y K H i G V</formula><formula xml:id="formula_7">+ g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U u O m X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q t 3 k c R T i B U z g H D 6 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B k t 2 M x Q = = &lt;</formula><formula xml:id="formula_8">+ g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U u O m X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q t 3 k c R T i B U z g H D 6 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B k t 2 M x Q = = &lt;</formula><formula xml:id="formula_9">+ g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U u O m X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q t 3 k c R T i B U z g H D 6 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B k t 2 M x Q = = &lt;</formula><formula xml:id="formula_10">v E V Q 4 5 5 X g B 3 R H d t T R W p I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O p F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s J b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l d q 9 T y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l G G M x g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m i o 5 9 k v E V Q 4 5 5 X g B 3 R H d t T R W p I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O p F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s J b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l d q 9 T y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l G G M x g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m i o 5 9 k v E V Q 4 5 5 X g B 3 R H d t T R W p I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O p F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s J b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l d q 9 T y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l G G M x g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m i o 5 9 k v E V Q 4 5 5 X g B 3 R H d t T R W p I = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O p F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p W R + U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s J b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l d q 9 T y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K</formula><formula xml:id="formula_11">T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w j G N 7 n / 8 M i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l b i + i O A r C 7 G l 6 2 q / W 3 L o 7 A 1 k m X k F q U K D Z r 3 7 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 5 V e a n h C 2 Z g O e d d S R S N u / G w W e U p O r D I g Y a z t U 0 h m 6 u + N j E b G T K L A T u Y R z a K X i / 9 5 3 R T D K z 8 T K k m R K z b / K E w l w Z j k 9 5 O B 0 J y h n F h C m R Y 2 K 2 E j q i l D 2 1 L F l u A t n r x M 2 u d 1 z 6 1 7 d x e 1 x n V R R x m O 4 B j O w I N L a M A t N K E F D G J</formula><formula xml:id="formula_12">T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w j G N 7 n / 8 M i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l b i + i O A r C 7 G l 6 2 q / W 3 L o 7 A 1 k m X k F q U K D Z r 3 7 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 5 V e a n h C 2 Z g O e d d S R S N u / G w W e U p O r D I g Y a z t U 0 h m 6 u + N j E b G T K L A T u Y R z a K X i / 9 5 3 R T D K z 8 T K k m R K z b / K E w l w Z j k 9 5 O B 0 J y h n F h C m R Y 2 K 2 E j q i l D 2 1 L F l u A t n r x M 2 u d 1 z 6 1 7 d x e 1 x n V R R x m O 4 B j O w I N L a M A t N K E F D G J</formula><formula xml:id="formula_13">T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w j G N 7 n / 8 M i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l b i + i O A r C 7 G l 6 2 q / W 3 L o 7 A 1 k m X k F q U K D Z r 3 7 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 5 V e a n h C 2 Z g O e d d S R S N u / G w W e U p O r D I g Y a z t U 0 h m 6 u + N j E b G T K L A T u Y R z a K X i / 9 5 3 R T D K z 8 T K k m R K z b / K E w l w Z j k 9 5 O B 0 J y h n F h C m R Y 2 K 2 E j q i l D 2 1 L F l u A t n r x M 2 u d 1 z 6 1 7 d x e 1 x n V R R x m O 4 B j O w I N L a M A t N K E F D G J</formula><formula xml:id="formula_14">T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w j G N 7 n / 8 M i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l b i + i O A r C 7 G l 6 2 q / W 3 L o 7 A 1 k m X k F q U K D Z r 3 7 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 5 V e a n h C 2 Z g O e d d S R S N u / G w W e U p O r D I g Y a z t U 0 h m 6 u + N j E b G T K L A T u Y R z a K X i / 9 5 3 R T D K z 8 T K k m R K z b / K E w l w Z j k 9 5 O B 0 J y h n F h C m R Y 2 K 2 E j q i l D 2 1 L F l u A t n r x M 2 u d 1 z 6 1 7 d x e 1 x n V R R x m O 4 B j O w I N L a M A t N K E F D G J 4 h l d 4 c 9 B 5 c d 6 d j / l o y S l 2 D u E P n M 8 f Y Z i R T Q = = &lt; / l a t e x i t &gt;</formula><p>x 00</p><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B w 8 1 3 a l 9 o q M g I D u T t v H S W 9 1 y C 7 w = "  This style of reasoning has indeed been successfully applied to defend against black-box and gray-box 2 attacks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54]</ref>. Figure <ref type="figure" target="#fig_12">1</ref> shows a 2D cartoon depiction of the high dimensional decision boundary near a natural image x. When the adversarial attack perturbs x slightly across the decision boundary from A to an incorrect class B, the resulting adversarial image x can be easily randomly perturbed to return to class A and will therefore fail criterion C1.</p><formula xml:id="formula_15">&gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k X q q s y I o M u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d O s X h 9 U a 0 7 D m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 L O R c N 1 G u 7 9 Z a 1 5 U 9 R R h h M 4 h X N w 4 Q q a c A c t a A O F B J 7 h F d 5 Q i l 7 Q O / p Y j J Z Q s X M M f 4 A + f w D G E Z F + &lt; / l</formula><formula xml:id="formula_16">V D L S g M x F L 2 p r 1 p f V Z d u g k X q q s y I o M u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d O s X h 9 U a 0 7 D m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 L O R c N 1 G u 7 9 Z a 1 5 U 9 R R h h M 4 h X N w 4 Q q a c A c t a A O F B J 7 h F d 5 Q i l 7 Q O / p Y j J Z Q s X M M f 4 A + f w D G E Z F + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B w 8 1 3 a l 9 o q M g I D u T t v H S W 9 1 y C 7 w = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k X q q s y I o M u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d O s X h 9 U a 0 7 D m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 L O R c N 1 G u 7 9 Z a 1 5 U 9 R R h h M 4 h X N w 4 Q q a c A c t a A O F B J 7 h F d 5 Q i l 7 Q O / p Y j J Z Q s X M M f 4 A + f w D G E Z F + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B w 8 1 3 a l 9 o q M g I D u T t v H S W 9 1 y C 7 w = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k X q q s y I o M u i G 5 c V 7 A M 6 Q 8 m k m T Y 0 k x m S j F i G / o Y b F 4 q 4 9 W f c + T d m 2 l l o 6 4 H A 4 Z x 7 u S c n S A T X x n G + U W l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 d J w q y t o 0 F r H q B U Q z w S V r G 2 4 E 6 y W K k S g Q r B t M b n O / + 8 i U 5 r F 8 M N O E + R E Z S R 5 y S o y V P C 8 i Z h y E 2 d O s X h 9 U a 0 7 D m Q O v E r c g N S j Q G l S / v G F M 0 4 h J Q w X R u u 8 6 i f E z o g y n g s 0 q X q p Z Q u i E j F j f U k k i p v 1 s n n m G z 6 w y x G G s 7 J M G z 9 X f G x m J t J 5 G g Z 3 M M + p l L x f / 8 / q p C a / 9 j M s k N U z S x a E w F d j E O C 8 A D 7 l i 1 I i p J Y Q q b r N i O i a K U G N r q t g S 3 O U v r 5 L O R c N 1 G u 7 9 Z a 1 5 U 9 R R h h M 4 h X N w 4 Q q a c A c t a A O F B J 7 h F d 5 Q i l 7 Q O / p Y j J Z Q s X M M f 4 A + f w D G E Z F + &lt; / l a t e x i t &gt;</formula><p>However, we emphasize that this criterion alone is insufficient against white-box adversaries and can be easily bypassed. In order to make the adversarial image also robust against Gaussian noise, the attacker can optimize the expected adversarial loss under this defense strategy <ref type="bibr" target="#b0">[1]</ref> through Monte Carlo sampling of noise vectors during optimization. This effectively produces an adversarial image x (see Figure <ref type="figure" target="#fig_12">1</ref>) that is deep inside the decision boundary.</p><p>More precisely, for a natural image x with correctly predicted label y and target label y t , let h(x) be the predicted class-probability vector. Let us define p adv to be identical to h(x) in every dimension, except for the correct class y and the target y t , where the two probabilities are swapped. Consequently, dimension y t is the dominant prediction in p adv . We redefine the adversarial loss of the (targeted) PGD attack to contain two terms:</p><formula xml:id="formula_17">L = L 1 + L 2 where: L 1 = L(h(x ), p adv ) misclassify x as yt</formula><p>, and</p><formula xml:id="formula_18">L 2 = E ∼N (0,σ 2 I) [ h(x ) − h(x + ) 1 ] bypass C1 ,<label>(3)</label></formula><p>where L(•, •) denotes the cross-entropy loss. For the first term, we deviate from standard attacks by targeting the probability vector p adv instead of the one-hot vector corresponding to label y t .</p><p>Optimizing against the one-hot vector would cause the adversarial example to over-saturate in probability, which artificially increases the difference ∆ = h(x ) − h(x + ) 1 and makes it easier to detect using criterion C1.</p><p>We evaluate this white-box attack against criterion C1 using a pre-trained ResNet-101 <ref type="bibr" target="#b20">[21]</ref> model on ImageNet <ref type="bibr" target="#b10">[11]</ref> as the classification model. We sample 1,000 images from the ImageNet validation set and optimize the adversarial loss L for each of them using Adam <ref type="bibr" target="#b25">[26]</ref> with learning rate 0.005 for a maximum of 400 steps to construct the adversarial images. The variation in ∆ under Gaussian perturbations (C1; left plot) and numbers of steps K t to the decision boundary (C2t; right plot) for adversarial images constructed using different numbers of gradient iterations. Gray-box attacks (orange) can be detected easily with criterion C1 alone (left plot, the orange line is significantly higher than the gray line). For white-box attacks (blue), C1 alone is not sufficient (the blue line overlaps with the gray line) -however C2 (right plot) separates the two lines reliably when C1 does not.</p><p>the error bars show the range of values between the 30th and 70th quantiles. When the attacker is agnostic to the detector (orange line), i.e., only optimizing L 1 , ∆ does not decrease throughout optimization and can be used to perfectly separate adversarial and real images (gray line). However, in the white-box attack, the adversarial loss explicitly encourages ∆ to be small, and we observe that indeed the blue line shows a downward trend as the adversary proceeds through gradient iterations.</p><p>As a result, the range of values for ∆ quickly begins to overlap with and fall below that of real images after 100 steps, which shows that criterion C1 alone cannot be used to detect adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Criterion 2: Close proximity to decision boundary</head><p>The intuitive reason why the attack strategy described above in section 4.1 can successfully fool criterion C1 is that it effectively pushes the adversarial image far into the decision boundary of the target class (e.g. x in Figure <ref type="figure" target="#fig_12">1</ref>) -an unlikely position for a natural image, which tends to be close to adversarial decision boundaries. Indeed, Fawzi et al. <ref type="bibr" target="#b12">[13]</ref> and Shafahi et al. <ref type="bibr" target="#b42">[43]</ref> have shown that adversarial examples are inevitable in high-dimensional spaces. Their theoretical arguments suggest that, due to the curse of dimensionality, a sample from the natural image distribution is close to the decision boundary of any classifier with high probability. Hence, we define a second criterion to test if an image is close to the decision boundary of an incorrect class: C2(t/u): Susceptibility to adversarial noise. For a chosen first-order iterative attack algorithm A, evaluate A on the input x and record the minimum number of steps K required to adversarially perturb x. The input is rejected as adversarial if K is sufficiently large.</p><p>Criterion C2 can be further specialized to targeted attacks (C2t) and untargeted attacks (C2u), which measures the proximity (i.e. number of gradient steps) to either a chosen target class or to an arbitrary but different class. We denote these quantities as K t and K u , respectively. In this paper we choose A in C2 to be the targeted/untargeted PGD attack, but our framework can plausibly generalize to any first-order attack algorithm. Figure <ref type="figure">2</ref> (right) shows the effect of optimizing the adversarial loss L on K t . Again, the center line shows the median value of K t across 1,000 images and the error bars indicate the 30th and 70th quantiles. As expected, real images (gray line) require very few steps to reach the decision boundary of any random target class. When the adversary does not seek to bypass criterion C1 (orange line), the constructed adversarial images lie very close to the decision boundary and are indistinguishable from real images with C2 alone (however here C1 is already sufficient).</p><p>On the other hand, when the attacker minimizes ∆ to fool criterion C1, the adversarial image moves away from the decision boundary in order to be robust to random Gaussian noise. This results in an increase in the number of steps K t to reach the decision boundary of a random target class. At 400 steps, there is almost no overlap between the 30-70th quantiles of values of K t for real and adversarial images. This separation begins almost precisely as the value of ∆ for adversarial images (left plot) begins to overlap with that of natural images at 100 steps. Thus, C2t becomes an effective criterion to detect adversarial images that optimize against C1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Detection strategy</head><p>The fact that natural images can simultaneously satisfy criteria C1 and C2 can be regarded as almost paradoxical: While the minimum distance from a natural input to the decision boundary of any incorrect class is small, the density of directions that can lead to a decision boundary within a short distance is also very low. We postulate that this behavior of natural images is difficult to imitate even for an adaptive, white-box adversary.</p><p>Our detection strategy using the two criteria can be summarized as follows. Given an input x (which might be an adversarial example already), we compute (∆, K t , K u ) and compare these quantities to chosen thresholds (t C1 , t C2t , t C2u ), corresponding to criteria C1, C2t, and C2u. We reject x as an adversarial example if at least one of the three (sub-)criteria is not satisfied, i.e., if any measurement is larger than the corresponding threshold. Details on hyperparameter selection can be found in the Supplementary Material.</p><p>Best effort white-box adversary. Based on our proposed detection method, we define a white-box adversary that aims to cause misclassification while passing the detection criteria C1 and C2. Let L be the adversarial loss for the defense-agnostic (targeted) attack (e.g. Equation <ref type="formula" target="#formula_1">1</ref>). We define loss functions L 1 and L 2 as in Equation <ref type="formula" target="#formula_18">3</ref>following the same strategy used in section 4.1 to bypass C1. Since the criterion C2t is discrete, it is difficult to optimize directly. Instead, we encourage the constructed adversarial image to change prediction to any class y = y t after a single gradient step towards y . As natural images require very few gradient steps to cross the decision boundary, the resulting adversarial image will appear real to criterion C2t. Let</p><formula xml:id="formula_19">δ y = ∇ x L A (h(x ), y )</formula><p>denote the gradient of the cross-entropy loss w.r.t. x<ref type="foot" target="#foot_2">3</ref> . The loss term to bypass C2t can be defined as</p><formula xml:id="formula_20">L 3 = E y ∼Uniform,y =yt [L(h(x − αδ y ), y )],</formula><p>which encourages x − αδ y -the one-step move towards class y at step size α -to be close to or cross the decision boundary of class y for every randomly chosen class y = y t . Similarly, to bypass criterion C2u, we simulate one gradient step at step size α away from the target class y t (which the defender perceives as the predicted class) as x + αδ yt . We then encourage this resulting image to be classified as not y t via the loss term: L 4 = −L(h(x + αδ yt ), y t ).</p><p>Gradients for L 3 and L 4 can be approximated using Backward Pass Differentiable Approximation (BPDA) <ref type="bibr" target="#b0">[1]</ref>. As a result of optimizing L 3 and L 4 , the produced image x will admit both a targeted and an untargeted "adversarial example" within one or few steps of the attack algorithm A, therefore bypassing C2. Combining all the components, the modified adversarial loss L for white-box attack against our detector becomes</p><formula xml:id="formula_21">L = λL 1 + L 2 + L 3 + L 4 .<label>(4)</label></formula><p>The inclusion of additional loss terms hinders the optimality of L 1 and may cause the attack to fail to generate a valid adversarial example. Thus, we include the coefficient λ so that L 1 dominates the other loss terms and guarantees close to 100% success rate in constructing adversarial examples to fool h. We optimize the total loss L using Adam <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We test our detection mechanism against the white-box attack defined in section 4.3 in several different settings, and release our code publicly for reproducibility <ref type="foot" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Datasets and target models. We conduct our empirical studies on ImageNet <ref type="bibr" target="#b10">[11]</ref> and CIFAR-10 <ref type="bibr" target="#b26">[27]</ref>. We sample 1,000 images from ImageNet (validation) and CIFAR-10 (test): each class has 1 or 100 images. We use the pre-trained ResNet-101 model <ref type="bibr" target="#b20">[21]</ref> in PyTorch for ImageNet and train a VGG-19 model <ref type="bibr" target="#b43">[44]</ref> with a dropout rate of 0.5 <ref type="bibr" target="#b46">[47]</ref> for CIFAR-10 as target models. We additionally include detection results using an Inception-v3 model <ref type="bibr" target="#b47">[48]</ref> on ImageNet in the Supplementary Material.</p><p>Table <ref type="table">1</ref>: Detection rates of different detection algorithms against white-box adversaries on ImageNet. Worst-case performance against all evaluated attacks is underlined for each detector.</p><p>Table <ref type="table">2</ref>: Detection rates of different detection algorithms against white-box adversaries on CIFAR-10. Worst-case performance against all evaluated attacks is underlined for each detector. stronger adversary at τ = 0.1, both detectors perform significantly worse, but our combined detector still achieves a non-trivial detection rate.  To further substantiate our claim that the criteria C1 and C2t/u are mutually exclusive, we plot the value of different components of the adversarial loss L * throughout optimization for the whitebox attack (PGD) on ImageNet. The center lines in Figure <ref type="figure" target="#fig_15">4</ref> show the average loss for each L i over 1,000 images and the shaded areas indicate standard deviation. Since the primary goal is to cause misclassification, the term L 1 (blue line) shows steady descending trend throughout optimization and its value has stabilized after 50 iterations. L 2 (orange line) begins at a low value due to the initialization being a natural image (and hence it is robust against Gaussian noise), and after 50 iterations it returns back to the initial level, which shows that the adversary is successful at bypassing criterion C1. However, this success comes at the cost of L 3 (red line) failing to reduce to a sufficiently low level due to inherent conflict with L 2 (and L 1 ), hence criterion C2t can be used to detect the resulting adversarial image. One drawback of our method is its (relatively) high computation cost. Criteria C2t/u require executing a gradient-based attack until either label change or for a specified number of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adversarial loss curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Detection times</head><p>To limit the number of false positives, the upper threshold on the number of gradient steps must be sufficiently high, dominating the running time of the detection algorithm. Table <ref type="table" target="#tab_0">4</ref> shows the average per-image detection time for both real and (targeted) adversarial images on ImageNet and CIFAR-10. On both datasets, the average detection time for real images is approximately 5 seconds and is largely due to a large threshold for C2u. The situation is similar for adversarial images: As the CW attack optimizes the margin loss, taking the adversarial images much farther into the decision boundary, it takes longer (many more steps to undo via C2t/u) to detect it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that our detection method achieves substantially improved resistance to white-box adversaries compared to prior work. In contrast to other detection algorithms that combine multiple criteria, the criteria used in our method are mutually exclusive -optimizing one will negatively affect the other -yet are inherently true for natural images. While we do not suggest that our method is impervious to white-box attacks, it does present a significant hurdle to overcome and raises the bar for any potential adversary.</p><p>There are, however, some limitations to our method. The running time of our detector is dominated by testing criterion C2, which involves running an iterative gradient-based attack algorithm. The high computation cost could prohibit the suitability of our detector for deployment. Furthermore, it is fair to say that the false positive rate remains relatively high due to a large variance in the statistics ∆, K t and K u for the different criteria, hence a threshold-based test cannot completely separate real and adversarial inputs. Future research that improve in either front can certainly ameliorate the performance of our method to be more practical in real world systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>x</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P / 7 8 W o K P q T P M A m r nJ 7 K 8 s b y W L s Q = " &gt; A A A B 8 X i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e i F 4 8 V b C u 2 o W y 2 m 3 b p Z h N 2 X 8 Q S + i + 8 e F D E q / / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 dr e 2 d 2 r 7 h + 0 T Z x q x l s s l r G + D 6 j h U i j e Q o G S 3 y e a 0 y i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3 h z j P P i v D s f 8 9 G S U + w c w h 8 4 n z / 9 O Z E c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P / 7 8 W o K P q T P M A m r n J 7 K 8 s b y W L s Q = " &gt; A A A B 8 X i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e i F 4 8 V b C u 2 o W y 2 m 3 b p Z h N 2 X 8 Q S + i + 8 e F D E q / / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0 T Z x q x l s s l r G + D 6 j h U i j e Q o G S 3 y e a 0 y i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3 h z j P P i v D s f 8 9 G S U + w c w h 8 4 n z / 9 O Z E c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P / 7 8 W o K P q T P M A m r n J 7 K 8 s b y W L s Q = " &gt; A A A B 8 X i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I o M e i F 4 8 V b C u 2 o W y 2 m 3 b p Z h N 2 X 8 Q S + i + 8 e F D E q / / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0 T Z x q x l s s l r G + D 6 j h U i j e Q o G S 3 y e a 0 y i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3 h z j P P i v D s f 8 9 G S U + w c w h 8 4 n z / 9 O Z E c &lt; / l a t e x i t &gt; A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B A e V O B C 5 O b W q G C F k 5 2 K l P 7 h c w R g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P V i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B A e V O B C 5 O b W q G C F k 5 2 K l P 7 h c w R g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P V i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B A e V O B C 5 O b W q G C F k 5 2 K l P 7 h c w R g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P V i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>/ l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B A e V O B C 5 O b W q G C F k 5 2 K l P 7 h c w R g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P V i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U u O m X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q t 3 k c R T i B U z g H D 6 6 g B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B k t 2 M x Q = = &lt; / l a t e x i t &gt; B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m i o 5 9 k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l G G M x g = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 9 k 1 J v 5 / A r F a N o a l X d 7 N E F z r f u c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O x F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o J J f e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A / K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y u 1 u z y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l e W M x w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 9 k 1 J v 5 / A r F a N o a l X d 7 N E F z r f u c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O x F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W yx W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o J J f e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A / K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N iU b g r f 6 8 j p p X 1 U 9 t + o 1 r y u 1 u z y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l e W M x w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 9 k 1J v 5 / A r F a N o a l X d 7 N E F z r f u c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O x F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o J J f e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A / K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y u 1 u z y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l e W M x w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / 9 k 1J v 5 / A r F a N o a l X d 7 N E F z r f u c = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G O x F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o J J f e 5 3 n l B p H s s H M 0 3 Q j + h I 8 p A z a q z U r A / K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G t n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N iU b g r f 6 8 j p p X 1 U 9 t + o 1 r y u 1 u z y O I p z B O V y C B z d Q g 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A l e W M x w = = &lt; / l a t e x i t &gt; D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N s L o s 3 l a W 3 W l k y b C 0 / e U H S Y S J S 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G N R D x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 5 f r r h V d w 6 y S r y c V C B H v V / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X U k k j 1 H 4 2 P 3 R K z q w y I G G s b E l D 5 u r v i Y x G W k + i w H Z G 1 I z 0 s j c T / / O 6 q Q m v / Y z L J D U o 2 W J R m A p i Y j L 7 m g y 4 Q m b E x B L K F L e 3 E j a i i j J j s y n Z E L z l l 1 d J 6 6 L q u V W v c V m p 3 e R x F O E E T u E c P L i C G t x D H Z r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H l 2 m M y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N s L o s 3 l a W 3 W l k y b C 0 / e U H S Y S J S 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G N R D x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 5 f r r h V d w 6 y S r y c V C B H v V / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X U k k j 1 H 4 2 P 3 R K z q w y I G G s b E l D 5 u r v i Y x G W k + i w H Z G 1 I z 0 s j c T / / O 6 q Q m v / Y z L J D U o 2 W J R m A p i Y j L 7 m g y 4 Q m b E x B L K F L e 3 E j a i i j J j s y n Z E L z l l 1 d J 6 6 L q u V W v c V m p 3 e R x F O E E T u E c P L i C G t x D H Z r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H l 2 m M y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N s L o s 3 l a W 3 W l k y b C 0 / e U H S Y S J S 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G N R D x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 5 f r r h V d w 6 y S r y c V C B H v V / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X U k k j 1 H 4 2 P 3 R K z q w y I G G s b E l D 5 u r v i Y x G W k + i w H Z G 1 I z 0 s j c T / / O 6 q Q m v / Y z L J D U o 2 W J R m A p i Y j L 7 m g y 4 Q m b E x B L K F L e 3 E j a i i j J j s y n Z E L z l l 1 d J 6 6 L q u V W v c V m p 3 e R x F O E E T u E c P L i C G t x D H Z r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H l 2 m M y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N s L o s 3 l a W 3 W l k y b C 0 / e U H S Y S J S 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G N R D x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R T W 1 j c 2 t 4 r b p Z 3 d v f 2 D 8 u F R S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 5 n f f k K l e S w f z C R B P 6 J D y U P O q L F S 4 6 5 f r r h V d w 6 y S r y c V C B H v V / + 6 g 1 i l k Y o D R N U 6 6 7 n J s b P q D K c C Z y W e q n G h L I x H W L X U k k j 1 H 4 2 P 3 R K z q w y I G G s b E l D 5 u r v i Y x G W k + i w H Z G 1 I z 0 s j c T / / O 6 q Q m v / Y z L J D U o 2 W J R m A p i Y j L 7 m g y 4 Q m b E x B L K F L e 3 E j a i i j J j s y n Z E L z l l 1 d J 6 6 L q u V W v c V m p 3 e R x F O E E T u E c P L i C G t x D H Z r A A O E Z X u H N e X R e n H f n Y 9 F a c P K Z Y / g D 5 / M H l 2 m M y A = = &lt; / l a t e x i t &gt; x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 e e G Q 1 T B t 6 l Q b n f b r N g e k y p 8 t 4 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y i p 5 K I o M e i F 4 8 V b C 2 0 o W y 2 m 3 b p Z h N 2 X 8 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>4 h l d 4 c 9 B 5 c d 6 d j / l o y S l 2 D u E P n M 8 f Y Z i R T Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 e e G Q 1 T B t 6 l Q b n f b r N g e k y p 8 t 4 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y i p 5 K I o M e i F 4 8 V b C 2 0 o W y 2 m 3 b p Z h N 2 X 8 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>4 h l d 4 c 9 B 5 c d 6 d j / l o y S l 2 D u E P n M 8 f Y Z i R T Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 e e G Q 1 T B t 6 l Q b n f b r N g e k y p 8 t 4 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y i p 5 K I o M e i F 4 8 V b C 2 0 o W y 2 m 3 b p Z h N 2 X 8 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>4 h l d 4 c 9 B 5 c d 6 d j / l o y S l 2 D u E P n M 8 f Y Z i R T Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + 3 e e G Q 1 T B t 6 l Q b n f b r N g e k y p 8 t 4 = " &gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y i p 5 K I o M e i F 4 8 V b C 2 0 o W y 2 m 3 b p Z h N 2 X 8 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m S K Q w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h + 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B w 8 1 3 a l 9 o q M g I D u T t v H S W 9 1 y C 7 w = " &gt; A A A B 8 3 i c b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic illustration of the shape of adversarial regions near a natural image x.</figDesc><graphic url="image-1.png" coords="4,385.55,315.32,118.27,113.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 (Figure 2 :</head><label>22</label><figDesc>Figure 2 (left) shows the effect of the number of gradient iterations on ∆ when optimizing the adversarial loss L . The center line shows median values of ∆ across 1,000 sample images, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of different components of the adversarial loss L * . See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 :</head><label>4</label><figDesc>Running time of different components of our detection algorithm on ImageNet and CIFAR-10. See text for details.</figDesc><table><row><cell></cell><cell>Real</cell><cell>PGD</cell><cell>CW</cell></row><row><cell></cell><cell cols="3">C1 0.074s 0.091s 0.107s</cell></row><row><cell>ImageNet</cell><cell cols="3">C2t 0.403s 1.057s 3.46s</cell></row><row><cell></cell><cell cols="3">C2u 4.512s 0.138s 0.241s</cell></row><row><cell></cell><cell cols="3">C1 0.011s 0.013s 0.012s</cell></row><row><cell>CIFAR-10</cell><cell cols="3">C2t 0.379s 0.128s 0.27s</cell></row><row><cell></cell><cell cols="3">C2u 5.230s 0.055s 9.631s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Some literature also refer to the iterative Fast Gradient Signed Method (FGSM)<ref type="bibr" target="#b16">[17]</ref> as PGD<ref type="bibr" target="#b32">[33]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In gray-box attacks, the adversary has full access to the classifier h but is agnostic to the defense mechanism.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We denote the adversarial loss of the Algo. A in our detector by LA to differentiate it from L of the attacker.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/s-huu/TurningWeaknessIntoStrength</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attack algorithms. We evaluate our detection method against the white-box adversary defined in section 4.3.</p><p>Since the adversary may vary in the choice of the surrogate loss (cf. L in Equation <ref type="formula">3</ref>), we experiment using both targeted and untargeted variants of two representative loss functions: the margin loss defined in the Carlini-Wagner (CW) attack <ref type="bibr" target="#b6">[7]</ref> (see Equation <ref type="formula">2</ref>), and the cross-entropy loss used in the Projected Gradient Descent (PGD) attack <ref type="bibr" target="#b0">[1]</ref>. The L ∞ -bound for all attacks is set to τ = 0.1, which is very strong and often produces images with noticeable visual distortion. See Figure <ref type="figure">3</ref> for an illustration. We further experiment with boundary attack <ref type="bibr" target="#b3">[4]</ref> for attacking the target model and detection mechanism as a black box in the Supplementary Material.</p><p>All attacks optimize the adversarial loss using Adam <ref type="bibr" target="#b25">[26]</ref>. We set λ = 2 (cf. Equation <ref type="formula">4</ref>) for ImageNet and λ = 3 for CIFAR-10 to guarantee close to 100% attack success rate. We found that changing the maximum number of iterations has little effect on the attack's ability to bypass our detector, and thus we fix to a reasonable value of 50 steps for ImageNet (which is sufficient to guarantee convergence; see Figure <ref type="figure">4</ref>) and 200 steps for CIFAR-10. The learning rate has a more noticeable effect and we evaluate our detector against different chosen values. See the Supplementary Material for detection results against variants of these attacks, including untargeted attacks and τ = 0.03.</p><p>Baselines. We compare our detector against two strategies: Feature Squeezing <ref type="bibr" target="#b55">[56]</ref> and Artifacts <ref type="bibr" target="#b13">[14]</ref>. These detection algorithms are the most similar to ours -using a combination of different criteria as features for the detector. We modify the Artifacts defense slightly to use the density and uncertainty estimates directly by thresholding rather than training a classifier on top of these features, which has been shown in prior work <ref type="bibr" target="#b5">[6]</ref> to remain effective against adversaries that are agnostic to the defense. With a false positive rates (FPR) of 0.1, Feature Squeezing attains a detection rate of 0.737 on ImageNet and 0.892 on CIFAR-10, while Artifacts attains a detection rate of 0.587 on CIFAR-10.</p><p>We adopt the same strategy as in section 4.3 to formulate white-box attacks against these detectors, adding a term in the adversarial loss for each criterion and using Backward Pass Differentiable Approximation (BPDA) to compute the gradient of non-differentiable transformations <ref type="bibr" target="#b0">[1]</ref>. Details on these modifications can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Detection results</head><p>ImageNet results. 0.1 on ImageNet. This result is a considerable improvement over similar detection methods such as Feature Squeezing, where the detection rate is close to 0, i.e. the adversarial images appear "more real" than natural images. We emphasize that given the strong adversary that we evaluate against (τ = 0.1), these detection rates are very difficult to attain against white-box attacks.</p><p>Ablation study. We further decompose the components of our detector to demonstrate the trade-offs the adversary must make when attacking our detector. When using different learning rates, the adversary switches between attempting to fool criteria C1 and C2. For example, at LR = 0.01, the PGD adversary can be detected using criterion C1 substantially better than using criterion C2t due to under-optimization of the value ∆. On the other hand, at LR = 0.1, the adversary succeeds in bypassing criterion C1 at the cost of failing C2t. The criterion C2u does not appear to be effective here as it consistently achieves a detection rate of close to 0. However, it is a crucial component of our method against untargeted attacks (see Supplementary Material). Overall, our combined detector achieves the best worst-case detection rate across all attack scenarios.</p><p>CIFAR-10 results. The detection rates for our method are slightly worse on CIFAR-10 (Table <ref type="table">2</ref>) but still outperforming the Feature Squeezing and Artifacts baselines, which are close to 0 in the worst case. For this dataset, criterion C2u becomes ineffective due to the over-saturation of predicted probabilities for clean images, causing untargeted perturbation to take excessively many steps.</p><p>Furthermore, the CIFAR-10 dataset violates both of our hypotheses regarding the distribution of adversarial perturbations near a natural image. Models trained on CIFAR-10 are much less robust to random Gaussian noise due to lack of data augmentation and poor diversity of training samples -the VGG-19 model could only tolerate a Gaussian noise of σ = 0.01 as opposed to σ = 0.1 for ResNet-101 on ImageNet. Furthermore, CIFAR-10 is much lower-dimensional than ImageNet, hence natural images are comparatively farther from the decision boundary <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. Given this observation, we suggest that our detector be used only in situations where these two assumptions can be satisfied. Gray-box detection results. Despite the fact that our detection mechanism is formulated against white-box adversaries, we evaluated against a graybox adversary with knowledge of the underlying model but not of the detector for completeness.</p><p>Table <ref type="table">3</ref> shows detection rates for graybox attacks at FPR of 0.05 and 0.1 on ImageNet. At perceptibility bound τ = 0.03, the combined detector is very successful at detecting the generated adversarial images, achieving a detection rate of 97.6% at 5% FPR. In comparison, Feature Squeezing could only achieve a detection rate of 30.4% against the CW attack. Against the much</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1802.00420</idno>
		<imprint>
			<date type="published" when="2018">2018. 1, 3, 4, 6, 7, 13</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vulnerability of deep reinforcement learning to policy induction attacks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Behzadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munir</surname></persName>
		</author>
		<idno>CoRR, abs/1701.04143</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML</title>
				<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno>CoRR, abs/1712.04248</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2017. 1, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards Evaluating the Robustness of Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio adversarial examples: Targeted attacks on speech-to-text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1801.01944</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-03">November 3, 2017. 2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<idno>CoRR, abs/1707.05373</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2009. 2, 4, 6</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2018. 2018, 3-8 December 2018. 2018. 1, 2, 5, 8</date>
			<biblScope unit="page" from="1186" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Detecting Adversarial Samples from Artifacts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<title level="m">On the (Statistical) Detection of Adversarial Examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simple black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>CoRR, abs/1905.07121</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<title level="m">Countering Adversarial Images using Input Transformations. International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2016. 2, 4, 6</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adversarial attacks on neural network policies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>CoRR, abs/1702.02284</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Prior convictions: Black-box adversarial attacks with bandits and priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno>CoRR, abs/1807.07978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarial Logit Pairing. ArXiv e-prints</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2007">2014. 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Adversarial Machine Learning at Scale. International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial examples detection in deep networks with convolutional filter statistics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
				<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random selfensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
				<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>CoRR, abs/1611.02770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N R</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Magnet: A two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-30">2017. October 30 -November 03, 2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhancing Robustness of Machine Learning Systems via Data Transformations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Nitin</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cullina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sitawarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Conference on Information Sciences and Systems (CISS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards robust detection of adversarial examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4579" to="4589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Practical blackbox attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-02">2017. April 2-6, 2017. 2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18">2018. June 18-22. 2018. 2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Certified Defenses against Adversarial Examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The odds are odd: A statistical test for detecting adversarial examples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kilcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>CoRR, abs/1805.06605</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Are adversarial examples inevitable?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>CoRR, abs/1809.02104</idno>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 5, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Certifying some distributional robustness with principled adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>CoRR, abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>CoRR, abs/1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Autozoom: Autoencoderbased zeroth order optimization method for attacking black-box neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<idno>CoRR, abs/1805.11770</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15. 2018. 2018</date>
			<biblScope unit="page" from="5032" to="5041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<title level="m">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. Network and Distributed Systems Security Symposium (NDSS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
