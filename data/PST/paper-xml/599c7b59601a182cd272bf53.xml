<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft-DTW: a Differentiable Loss Function for Time-Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
							<email>&lt;marco.cuturi@ensae.fr&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">CREST</orgName>
								<orgName type="institution" key="instit1">ENSAE</orgName>
								<orgName type="institution" key="instit2">Université Paris-Saclay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
							<email>&lt;mathieu@mblondel.org&gt;.</email>
							<affiliation key="aff1">
								<orgName type="institution">NTT Communication Science Laboratories</orgName>
								<address>
									<addrLine>Seika-cho</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Mathieu Blondel</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft-DTW: a Differentiable Loss Function for Time-Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines <ref type="bibr" target="#b17">(Petitjean et al., 2011)</ref>. Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of supervised learning is to learn a mapping that links an input to an output objects, using examples of such pairs. This task is noticeably more difficult when the output objects have a structure, i.e. when they are not vectors <ref type="bibr" target="#b1">(Bakir et al., 2007)</ref>. We study here the case where each output object is a time series, namely a family of observations indexed by time. While it is tempting to treat time as yet another feature, and handle time series of vectors as the concatenation of all these vectors, several practical Proceedings of the 34 th International Conference on Machine <ref type="bibr">Learning, Sydney, Australia, PMLR 70, 2017.</ref> Copyright 2017 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Output</head><p>Figure <ref type="figure">1</ref>. Given the first part of a time series, we trained two multi-layer perceptron (MLP) to predict the entire second part.</p><p>Using the ShapesAll dataset, we used a Euclidean loss for the first MLP and the soft-DTW loss proposed in this paper for the second one. We display above the prediction obtained for a given test instance with either of these two MLPs in addition to the ground truth. Oftentimes, we observe that the soft-DTW loss enables us to better predict sharp changes. More time series predictions are given in Appendix F.</p><p>issues arise when taking this simplistic approach: Timeindexed phenomena can often be stretched in some areas along the time axis (a word uttered in a slightly slower pace than usual) with no impact on their characteristics; varying sampling conditions may mean they have different lengths; time series may not synchronized.</p><p>The DTW paradigm. Generative models for time series are usually built having the invariances above in mind: Such properties are typically handled through latent variables and/or Markovian assumptions <ref type="bibr">(Lütkepohl, 2005, Part I, §18)</ref>. A simpler approach, motivated by geometry, lies in the direct definition of a discrepancy between time series that encodes these invariances, such as the Dynamic Time Warping (DTW) score <ref type="bibr" target="#b23">(Sakoe &amp; Chiba, 1971;</ref><ref type="bibr" target="#b24">1978)</ref>. DTW computes the best possible alignment between two time series (the optimal alignment itself can also be of interest, see e.g. <ref type="bibr" target="#b10">Garreau et al. 2014)</ref> of respective length n and m by computing first the n × m pairwise distance matrix between these points to solve then a dynamic program (DP) using Bellman's recursion with a quadratic (nm) cost.</p><p>The DTW geometry. Because it encodes efficiently a useful class of invariances, DTW has often been used in a discriminative framework (with a k-NN or SVM classifier) to predict a real or a class label output, and engineered to run faster in that context <ref type="bibr" target="#b27">(Yi et al., 1998)</ref>. Recent works by <ref type="bibr" target="#b17">Petitjean et al. (2011)</ref>; <ref type="bibr" target="#b16">Petitjean &amp; Ganc ¸arski (2012)</ref> have, however, shown that DTW can be used for more innovative tasks, such as time series averaging using the DTW discrepancy (see Schultz &amp; Jain 2017 for a gentle introduction to these ideas). More generally, the idea of synthetising time series centroids can be regarded as a first attempt to output entire time series using DTW as a fitting loss. From a computational perspective, these approaches are, however, hampered by the fact that DTW is not differentiable and unstable when used in an optimization pipeline.</p><p>Soft-DTW. In parallel to these developments, several authors have considered smoothed modifications of Bellman's recursion to define smoothed DP distances <ref type="bibr" target="#b0">(Bahl &amp; Jelinek, 1975;</ref><ref type="bibr" target="#b19">Ristad &amp; Yianilos, 1998)</ref> or kernels <ref type="bibr" target="#b21">(Saigo et al., 2004;</ref><ref type="bibr" target="#b8">Cuturi et al., 2007)</ref>. When applied to the DTW discrepancy, that regularization results in a soft-DTW score, which considers the soft-minimum of the distribution of all costs spanned by all possible alignments between two time series. Despite considering all alignments and not just the optimal one, soft-DTW can be computed with a minor modification of Bellman's recursion, in which all (min, +) operations are replaced with (+, ×). As a result, both DTW and soft-DTW have quadratic in time &amp; linear in space complexity with respect to the sequences' lengths.</p><p>Because soft-DTW can be used with kernel machines, one typically observes an increase in performance when using soft-DTW over DTW <ref type="bibr" target="#b6">(Cuturi, 2011)</ref> for classification.</p><p>Our contributions. We explore in this paper another important benefit of smoothing DTW: unlike the original DTW discrepancy, soft-DTW is differentiable in all of its arguments. We show that the gradients of soft-DTW w.r.t to all of its variables can be computed as a by-product of the computation of the discrepancy itself, with an added quadratic storage cost. We use this fact to propose an alternative approach to the DBA (DTW Barycenter Averaging) clustering algorithm of <ref type="bibr" target="#b17">(Petitjean et al., 2011)</ref>, and observe that our smoothed approach significantly outperforms known baselines for that task. More generally, we propose to use soft-DTW as a fitting term to compare the output of a machine synthesizing a time series segment with a ground truth observation, in the same way that, for instance, a regularized Wasserstein distance was used to compute barycenters <ref type="bibr" target="#b7">(Cuturi &amp; Doucet, 2014)</ref>, and later to fit discriminators that output histograms <ref type="bibr" target="#b28">(Zhang et al., 2015;</ref><ref type="bibr" target="#b20">Rolet et al., 2016)</ref>. When paired with a flexible learning architecture such as a neural network, soft-DTW allows for a differentiable end-to-end approach to design predictive and generative models for time series, as illustrated in Figure <ref type="figure">1</ref>. Source code is available at https: //github.com/mblondel/soft-dtw.</p><p>Structure. After providing background material, we show in §2 how soft-DTW can be differentiated w.r.t the locations of two time series. We follow in §3 by illustrating how these results can be directly used for tasks that require to output time series: averaging, clustering and prediction of time series. We close this paper with experimental results in §4 that showcase each of these potential applications.</p><p>Notations. We consider in what follows multivariate discrete time series of varying length taking values in Ω ⊂ R p .</p><p>A time series can be thus represented as a matrix of p lines and varying number of columns. We consider a differentiable substitution-cost function δ : R p × R p → R + which will be, in most cases, the quadratic Euclidean distance between two vectors. For an integer n we write n for the set {1, . . . , n} of integers. Given two series' lengths n and m, we write A n,m ⊂ {0, 1} n×m for the set of (binary) alignment matrices, that is paths on a n × m matrix that connect the upper-left (1, 1) matrix entry to the lower-right (n, m) one using only ↓, →, ց moves. The cardinal of A n,m is known as the delannoy(n − 1, m − 1) number; that number grows exponentially with m and n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The DTW and soft-DTW loss functions</head><p>We propose in this section a unified formulation for the original DTW discrepancy <ref type="bibr" target="#b24">(Sakoe &amp; Chiba, 1978)</ref> and the Global Alignment kernel (GAK) <ref type="bibr" target="#b8">(Cuturi et al., 2007)</ref>, which can be both used to compare two time series x = (x 1 , . . . , x n ) ∈ R p×n and y = (y 1 , . . . , y m ) ∈ R p×m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Alignment costs: optimality and sum</head><p>Given the cost matrix ∆(x, y) := δ(x i , y j ) ij ∈ R n×m , the inner product A, ∆(x, y) of that matrix with an alignment matrix A in A n,m gives the score of A, as illustrated in Figure <ref type="figure">2</ref>. Both DTW and GAK consider the costs of all possible alignment matrices, yet do so differently:</p><formula xml:id="formula_0">DTW(x, y) := min A∈An,m A, ∆(x, y) , k γ GA (x, y) := A∈An,m e − A,∆(x,y) /γ . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>DP Recursion. <ref type="bibr" target="#b24">Sakoe &amp; Chiba (1978)</ref> showed that the Bellman equation ( <ref type="formula">1952</ref>) can be used to compute DTW. That recursion, which appears in line 5 of Algorithm 1 (disregarding for now the exponent γ), only involves (min, +) operations. When considering kernel k γ GA and, instead, its integration over all alignments (see e.g. Lasserre 2009), <ref type="bibr">Cuturi et al. (2007, Theorem 2)</ref> and the highly related formulation of <ref type="bibr" target="#b21">Saigo et al. (2004</ref><ref type="bibr">Saigo et al. ( , p.1685</ref>) use an old algorithmic appraoch <ref type="bibr" target="#b0">(Bahl &amp; Jelinek, 1975)</ref> which consists in (i) replacing all costs by their neg-exponential; (ii) replace (min, +) operations with (+, ×) operations. These two recursions can be in fact unified with the use of a soft-y 1 y 2 y 3 y 4 y 5 y 6 <ref type="table" target="#tab_3">1 δ 2,2 δ 2,3 δ 2,4 δ 2,5 δ 2,6  δ 3,1 δ 3,2 δ 3,3 δ 3,4 δ 3,5 δ 3,6  δ 4,1 δ 4,2 δ 4,3 δ 4,4 δ 4,5 δ 4,6   Figure 2</ref>. Three alignment matrices (orange, green, purple, in addition to the top-left and bottom-right entries) between two time series of length 4 and 6. The cost of an alignment is equal to the sum of entries visited along the path. DTW only considers the optimal alignment (here depicted in purple pentagons), whereas soft-DTW considers all delannoy(n − 1, m − 1) possible alignment matrices.</p><formula xml:id="formula_2">x 1 x 2 x 3 x 4 δ 1,1 δ 1,2 δ 1,3 δ 1,4 δ 1,5 δ 1,6 δ 2,</formula><p>minimum operator, which we present below.</p><p>Unified algorithm Both formulas in Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be computed with a single algorithm. That formulation is new to our knowledge. Consider the following generalized min operator, with a smoothing parameter γ ≥ 0:</p><formula xml:id="formula_3">min γ {a 1 , . . . , a n } := min i≤n a i , γ = 0, −γ log n i=1 e −ai/γ , γ &gt; 0.</formula><p>With that operator, we can define γ-soft-DTW:</p><formula xml:id="formula_4">dtw γ (x, y) := min γ { A, ∆(x, y) , A ∈ A n,m }.</formula><p>The original DTW score is recovered by setting γ to 0. When γ &gt; 0, we recover dtw γ = −γ log k γ GA . Most importantly, and in either case, dtw γ can be computed using Algorithm 1, which requires (nm) operations and (nm) storage cost as well . That cost can be reduced to 2n with a more careful implementation if one only seeks to compute dtw γ (x, y), but the backward pass we consider next requires the entire matrix R of intermediary alignment costs. Note that, to ensure numerical stability, the operator min γ must be computed using the usual log-sum-exp stabilization trick, namely that log i e zi = (max j z j ) + log i e zi−maxj zj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Differentiation of soft-DTW</head><p>A small variation in the input x causes a small change in dtw 0 (x, y) or dtw γ (x, y). When considering dtw 0 , that change can be efficiently monitored only when the optimal alignment matrix A ⋆ that arises when computing dtw 0 (x, y) in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is unique. As the minimum over a finite set of linear functions of ∆, dtw 0 is therefore locally differentiable w.r.t. the cost matrix ∆, with gradient A ⋆ , a fact that has been exploited in all algorithms designed to Algorithm 1 Forward recursion to compute dtw γ (x, y) and intermediate alignment costs 1: Inputs: x, y, smoothing γ ≥ 0, distance function δ 2: r 0,0 = 0; r i,0 = r 0,j = ∞; i ∈ n , j ∈ m 3: for j = 1, . . . , m do 4:</p><p>for i = 1, . . . , n do 5:</p><formula xml:id="formula_5">r i,j = δ(x i , y j ) + min γ {r i−1,j−1 , r i−1,j , r i,j−1 } 6:</formula><p>end for 7: end for 8: Output: (r n,m , R) average time series under the DTW metric <ref type="bibr" target="#b17">(Petitjean et al., 2011;</ref><ref type="bibr" target="#b25">Schultz &amp; Jain, 2017)</ref>. To recover the gradient of dtw 0 (x, y) w.r.t. x, we only need to apply the chain rule, thanks to the differentiability of the cost function:</p><formula xml:id="formula_6">∇ x dtw 0 (x, y) = ∂∆(x, y) ∂x T A ⋆ ,<label>(2)</label></formula><p>where ∂∆(x, y)/∂x is the Jacobian of ∆ w.r.t. x, a linear map from R p×n to R n×m . When δ is the squared Euclidean distance, the transpose of that Jacobian applied to a matrix B ∈ R n×m is (• being the elementwise product):</p><formula xml:id="formula_7">(∂∆(x, y)/∂x) T B = 2 (1 p 1 T m B T ) • x − yB T .</formula><p>With continuous data, A ⋆ is almost always likely to be unique, and therefore the gradient in Eq. ( <ref type="formula" target="#formula_6">2</ref>) will be defined almost everywhere. However, that gradient, when it exists, will be discontinuous around those values x where a small change in x causes a change in A ⋆ , which is likely to hamper the performance of gradient descent methods.</p><p>The case γ &gt; 0. An immediate advantage of soft-DTW is that it can be explicitly differentiated, a fact that was also noticed by <ref type="bibr" target="#b22">Saigo et al. (2006)</ref> in the related case of edit distances. When γ &gt; 0, the gradient of Eq. ( <ref type="formula" target="#formula_0">1</ref>) is obtained via the chain rule,</p><formula xml:id="formula_8">∇ x dtw γ (x, y) = ∂∆(x, y) ∂x T E γ [A],<label>(3)</label></formula><p>where</p><formula xml:id="formula_9">E γ [A] := 1 k γ GA (x, y) A∈An,m e − A,∆(x,y)/γ A,</formula><p>is the average alignment matrix A under the Gibbs distribution p γ ∝ e − A,∆(x,y) /γ defined on all alignments in A n,m . The kernel k γ GA (x, y) can thus be interpreted as the normalization constant of p γ . Of course, since A n,m has exponential size in n and m, a naive summation is not tractable. Although a Bellman recursion to compute that average alignment matrix E γ [A] exists (see Appendix A) that computation has quartic (n 2 m 2 ) complexity. Note that this stands in stark contrast to the quadratic complexity obtained by <ref type="bibr" target="#b22">Saigo et al. (2006)</ref> for edit-distances, which is due to the fact the sequences they consider can only take values in a finite alphabet. To compute the gradient of soft-DTW, we propose instead an algorithm that manages to remain quadratic (nm) in terms of complexity. The key to achieve this reduction is to apply the chain rule in reverse order of Bellman's recursion given in Algorithm 1, namely backpropagate. A similar idea was recently used to compute the gradient of ANOVA kernels in <ref type="bibr" target="#b3">(Blondel et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Algorithmic differentiation</head><p>Differentiating algorithmically dtw γ (x, y) requires doing first a forward pass of Bellman's equation to store all intermediary computations and recover R = [r i,j ] when running Algorithm 1. The value of dtw γ (x, y)-stored in r n,m at the end of the forward recursion-is then impacted by a change in r i,j exclusively through the terms in which r i,j plays a role, namely the triplet of terms r i+1,j , r i,j+1 , r i+1,j+1 . A straightforward application of the chain rule then gives</p><formula xml:id="formula_10">∂rn,m ∂ri,j ei,j = ∂rn,m ∂ri+1,j ei+1,j ∂r i+1,j ∂ri,j + ∂rn,m ∂ri,j+1 ei,j+1 ∂r i,j+1 ∂ri,j + ∂rn,m ∂ri+1,j+1 ei+1,j+1 ∂r i+1,j+1 ∂ri,j</formula><p>, in which we have defined the notation of the main object of interest of the backward recursion: e i,j := ∂rn,m ∂ri,j . The Bellman recursion evaluated at (i + 1, j) as shown in line 5 of Algorithm 1 (here δ i+1,j is δ(x i+1 , y j )) yields : r i+1,j = δ i+1,j + min γ {r i,j−1 , r i,j , r i+1,j−1 }, which, when differentiated w.r.t r i,j yields the ratio: ∂r i+1,j ∂ri,j = e −ri,j /γ / e −ri,j−1/γ + e −ri,j /γ + e −ri+1,j−1/γ .</p><p>The logarithm of that derivative can be conveniently cast using evaluations of min γ computed in the forward loop:</p><formula xml:id="formula_11">γ log ∂r i+1,j ∂ri,j = min γ {r i,j−1 , r i,j , r i+1,j−1 } − r i,j = r i+1,j − δ i+1,j − r i,j .</formula><p>Similarly, the following relationships can also be obtained:</p><formula xml:id="formula_12">γ log ∂r i,j+1 ∂ri,j = r i,j+1 − r i,j − δ i,j+1 , γ log ∂r i+1,j+1 ∂ri,j = r i+1,j+1 − r i,j − δ i+1,j+1 .</formula><p>We have therefore obtained a backward recursion to compute the entire matrix E = [e i,j ], starting from e n,m = ∂rn,m ∂rn,m = 1 down to e 1,1 . To obtain ∇ x dtw γ (x, y), notice that the derivatives w.r.t. the entries of the cost matrix ∆ can be computed by ∂rn,m ∂δi,j = ∂rn,m ∂ri,j ∂ri,j ∂δi,j = e i,j • 1 = e i,j , and therefore we have that</p><formula xml:id="formula_13">∇ x dtw γ (x, y) = ∂∆(x, y) ∂x T E,</formula><p>where E is exactly the average alignment E γ [A] in Eq. ( <ref type="formula" target="#formula_8">3</ref>). These computations are summarized in Algorithm 2, which, once ∆ has been computed, has complexity nm in time and space. Because min γ has a 1/γ-Lipschitz continuous gradient, the gradient of dtw γ is 2/γ-Lipschitz continuous when δ is the squared Euclidean distance.</p><p>Algorithm 2 Backward recursion to compute ∇x dtwγ(x, y) for i = n, . . . , 1 do</p><formula xml:id="formula_14">1: Inputs: x, y, smoothing γ ≥ 0, distance function δ 2: (•, R) = dtw γ (x, y), ∆ = [δ(x i , y j )] i,j 3: δ i,m+1 = δ n+1,j = 0, i ∈ n , j ∈ m 4: e i,m+1 = e n+1,j = 0, i ∈ n , j ∈ m 5: r i,m+1 = r n+1,j = −∞, i ∈ n , j ∈ m 6: δ n+1,m+1 = 0, e n+1,</formula><formula xml:id="formula_15">9: a = exp 1 γ (r i+1,j − r i,j − δ i+1,j ) 10: b = exp 1 γ (r i,j+1 − r i,j − δ i,j+1</formula><p>)</p><formula xml:id="formula_16">11: c = exp 1 γ (r i+1,j+1 − r i,j − δ i+1,j+1</formula><p>)</p><p>12:</p><formula xml:id="formula_17">e i,j = e i+1,j • a + e i,j+1 • b + e i+1,j+1 • c 13:</formula><p>end for 14: end for 15: Output: ∇ x dtw γ (x, y) = ∂∆(x,y) ∂x T E 3. Learning with the soft-DTW loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Averaging with the soft-DTW geometry</head><p>We study in this section a direct application of Algorithm 2 to the problem of <ref type="bibr">computing Fréchet means (1948)</ref> of time series with respect to the dtw γ discrepancy. Given a family of N times series y 1 , . . . , y N , namely N matrices of p lines and varying number of columns, m 1 , . . . , m N , we are interested in defining a single barycenter time series x for that family under a set of normalized weights λ 1 , . . . , λ N ∈ R + such that N i=1 λ i = 1. Our goal is thus to solve approximately the following problem, in which we have assumed that x has fixed length n:</p><formula xml:id="formula_18">min x∈R p×n N i=1 λ i m i dtw γ (x, y i ).<label>(4)</label></formula><p>Note that each dtw γ (x, y i ) term is divided by m i , the length of y i . Indeed, since dtw 0 is an increasing (roughly linearly) function of each of the input lengths n and m i , we follow the convention of normalizing in practice each discrepancy by n × m i . Since the length n of x is here fixed across all evaluations, we do not need to divide the objective of Eq. ( <ref type="formula" target="#formula_18">4</ref>) by n. Averaging under the soft-DTW geometry results in substantially different results than those that can be obtained with the Euclidean geometry (which can only be used in the case where all lengths n = m e i,j+1</p><formula xml:id="formula_19">1 = • • • = δ i,j δ i+1,j δ i,j+1 δ i+1,j+1 r i−1,j−1 r i−1,j r i−1,j+1 r i,j−1 r i,j r i,j+1 r i+1,j−1 r i+1,j r i+1,</formula><p>Figure <ref type="figure">3</ref>. Sketch of the computational graph for soft-DTW, in the forward pass used to compute dtwγ (left) and backward pass used to compute its gradient ∇x dtwγ (right). In both diagrams, purple shaded cells stand for data values available before the recursion starts, namely cost values (left) and multipliers computed using forward pass results (right). In the left diagram, the forward computation of ri,j as a function of its predecessors and δi,j is summarized with arrows. Dotted lines indicate a min γ operation, solid lines an addition.</p><p>From the perspective of the final term rn,m, which stores dtwγ(x, y) at the lower right corner (not shown) of the computational graph, a change in ri,j only impacts rn,m through changes that ri,j causes to ri+1,j, ri,j+1 and ri+1,j+1. These changes can be tracked using Eq. (2.3,2.3) and appear in lines 9-11 in Algorithm 2 as variables a, b, c, as well as in the purple shaded boxes in the backward pass (right) which represents the recursion of line 12 in Algorithm 2.</p><p>m N are equal), as can be seen in the intuitive interpolations we obtain between two time series shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>Non-convexity of dtw γ . A natural question that arises from Eq. ( <ref type="formula" target="#formula_18">4</ref>) is whether that objective is convex or not. The answer is negative, in a way that echoes the non-convexity of the k-means objective as a function of cluster centroids locations. Indeed, for any alignment matrix A of suitable size, each map x → A, ∆(x, y) shares the same convexity/concavity property that δ may have. However, both min and min γ can only preserve the concavity of elementary functions <ref type="bibr">(Boyd &amp; Vandenberghe, 2004, pp.72-74)</ref>. Therefore dtw γ will only be concave if δ is concave, or become instead a (non-convex) (soft) minimum of convex functions if δ is convex. When δ is a squared-Euclidean distance, dtw 0 is a piecewise quadratic function of x, as is also the case with the k-means energy (see for instance Figure <ref type="figure">2</ref> in Schultz &amp; Jain 2017). Since this is the setting we consider here, all of the computations involving barycenters should be taken with a grain of salt, since we have no way of ensuring optimality when approximating Eq. ( <ref type="formula" target="#formula_18">4</ref>).</p><p>Smoothing helps optimizing dtw γ . Smoothing can be regarded, however, as a way to "convexify" dtw γ . Indeed, notice that dtw γ converges to the sum of all costs as γ → ∞. Therefore, if δ is convex, dtw γ will gradually become convex as γ grows. For smaller values of γ, one can intuitively foresee that using min γ instead of a minimum will smooth out local minima and therefore provide a better (although slightly different from dtw 0 ) optimization landscape. We believe this is why our approach recovers better results, even when measured in the original dtw 0 discrepancy, than subgradient or alternating minimization approaches such as DBA <ref type="bibr" target="#b17">(Petitjean et al., 2011)</ref>, which can, on the contrary, get more easily stuck in local minima. Evidence for this statement is presented in the experimental section.  <ref type="formula" target="#formula_18">4</ref>) with (λ1, λ2) set to (0.25, 0.75), (0.5, 0.5) and (0.75, 0.25).</p><p>The soft-DTW geometry leads to visibly different interpolations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Clustering with the soft-DTW geometry</head><p>The (approximate) computation of dtw γ barycenters can be seen as a first step towards the task of clustering time series under the dtw γ discrepancy. Indeed, one can naturally formulate that problem as that of finding centroids x 1 , . . . , x k that minimize the following energy:</p><formula xml:id="formula_20">min x1,...,x k ∈R p×n N i=1 1 m i min j∈[[k]]</formula><p>dtw γ (x j , y i ).</p><p>(5)</p><p>To solve that problem one can resort to a direct generalization of <ref type="bibr">Lloyd's algorithm (1982)</ref> in which each centering step and each clustering allocation step is done according to the dtw γ discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning prototypes for time series classification</head><p>One of the de-facto baselines for learning to classify time series is the k nearest neighbors (k-NN) algorithm, combined with DTW as discrepancy measure between time series. However, k-NN has two main drawbacks. First, the time series used for training must be stored, leading to potentially high storage cost. Second, in order to com-pute predictions on new time series, the DTW discrepancy must be computed with all training time series, leading to high computational cost. Both of these drawbacks can be addressed by the nearest centroid classifier <ref type="bibr">(Hastie et al., 2001, p.670)</ref>, <ref type="bibr" target="#b26">(Tibshirani et al., 2002)</ref>. This method chooses the class whose barycenter (centroid) is closest to the time series to classify. Although very simple, this method was shown to be competitive with k-NN, while requiring much lower computational cost at prediction time <ref type="bibr" target="#b18">(Petitjean et al., 2014)</ref>. Soft-DTW can naturally be used in a nearest centroid classifier, in order to compute the barycenter of each class at train time, and to compute the discrepancy between barycenters and time series, at prediction time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multistep-ahead prediction</head><p>Soft-DTW is ideally suited as a loss function for any task that requires time series outputs. As an example of such a task, we consider the problem of, given the first 1, . . . , t observations of a time series, predicting the remaining (t + 1), . . . , n observations. Let x t,t ′ ∈ R p×(t ′ −t+1) be the submatrix of x ∈ R p×n of all columns with indices between t and t ′ , where 1 ≤ t &lt; t ′ &lt; n. Learning to predict the segment of a time series can be cast as the problem</p><formula xml:id="formula_21">min θ∈Θ N i=1 dtw γ f θ (x 1,t i ), x t+1,n i ,</formula><p>where {f θ } is a set of parameterized function that take as input a time series and outputs a time series. Natural choices would be multi-layer perceptrons or recurrent neural networks (RNN), which have been historically trained with a Euclidean loss <ref type="bibr">(Parlos et al., 2000, Eq.5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Throughout this section, we use the UCR (University of California, Riverside) time series classification archive <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>. We use a subset containing 79 datasets encompassing a wide variety of fields (astronomy, geology, medical imaging) and lengths. Datasets include class information (up to 60 classes) for each time series and are split into train and test sets. Due to the large number of datasets in the UCR archive, we choose to report only a summary of our results in the main manuscript. Detailed results are included in the appendices for interested readers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Averaging experiments</head><p>In this section, we compare the soft-DTW barycenter approach presented in §3.1 to DBA <ref type="bibr" target="#b17">(Petitjean et al., 2011)</ref> and a simple batch subgradient method.</p><p>Experimental setup. For each dataset, we choose a class at random, pick 10 time series in that class and compute their barycenter. For quantitative results below, we repeat this procedure 10 times and report the averaged results. For each method, we set the maximum number of iterations to 100. To minimize the proposed soft-DTW barycenter objective, Eq. ( <ref type="formula" target="#formula_18">4</ref>), we use L-BFGS.</p><p>Qualitative results. We first visualize the barycenters obtained by soft-DTW when γ = 1 and γ = 0.01, by DBA and by the subgradient method. Figure <ref type="figure">5</ref> shows barycenters obtained using random initialization on the ECG200 dataset. More results with both random and Euclidean mean initialization are given in Appendix B and C.</p><p>We observe that both DBA or soft-DTW with low smoothing parameter γ yield barycenters that are spurious. On the other hand, a descent on the soft-DTW loss with sufficiently high γ converges to a reasonable solution. For example, as indicated in Figure <ref type="figure">5</ref> with DTW or soft-DTW (γ = 0.01), the small kink around x = 15 is not representative of any of the time series in the dataset. However, with soft-DTW (γ = 1), the barycenter closely matches the time series. This suggests that DTW or soft-DTW with too low γ can get stuck in bad local minima.</p><p>When using Euclidean mean initialization (only possible if time series have the same length), DTW or soft-DTW with low γ often yield barycenters that better match the shape of the time series. However, they tend to overfit: they absorb the idiosyncrasies of the data. In contrast, soft-DTW is able to learn barycenters that are much smoother.</p><p>Quantitative results. claim that the smoothness of soft-DTW leads to an objective that is better behaved and more amenable to optimization by gradient-descent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">k-means clustering experiments</head><p>We consider in this section the same computational tools used in §4.1 above, but use them to cluster time series.</p><p>Experimental setup. For all datasets, the number of clusters k is equal to the number of classes available in the dataset. Lloyd's algorithm alternates between a centering step (barycenter computation) and an assignment step. We set the maximum number of outer iterations to 30 and the maximum number of inner (barycenter) iterations to 100, as before. Again, for soft-DTW, we use L-BFGS.</p><p>Qualitative results. Figure <ref type="figure" target="#fig_3">6</ref> shows the clusters obtained when runing Lloyd's algorithm on the CBF dataset with soft-DTW (γ = 1) and DBA, in the case of random initialization. More results are included in Appendix E. Clearly, DTW absorbs the tiny details in the data, while soft-DTW is able to learn much smoother barycenters.</p><p>Quantitative results. Table <ref type="table" target="#tab_2">2</ref> summarizes the percentage of datasets on which soft-DTW barycenter achieves lower k-means loss under DTW, i.e. Eq. ( <ref type="formula">5</ref>) with γ = 0. The actual loss values achieved by all methods are indicated in Appendix I and Appendix J. The results confirm the same trend as for the barycenter experiments. Namely, as γ decreases, soft-DTW is able to achieve lower loss than other methods on a large proportion of the datasets. Note that we have not run experiments with smaller values of γ than 0.001, since dtw 0.001 is very close to dtw 0 in practice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Time-series classification experiments</head><p>In this section, we investigate whether the smoothing in soft-DTW can act as a useful regularization and improve classification accuracy in the nearest centroid classifier.</p><p>Experimental setup. We use 50% of the data for training, 25% for validation and 25% for testing. We choose γ from 15 log-spaced values between 10 −3 and 10.</p><p>Quantitative results. Each point in Figure <ref type="figure">7</ref> above the diagonal line represents a dataset for which using soft-DTW for barycenter computation rather than DBA improves the accuracy of the nearest centroid classifier. To summarize, we found that soft-DTW is working better or at least as well as DBA in 75% of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multistep-ahead prediction experiments</head><p>In this section, we present preliminary experiments for the task of multistep-ahead prediction, described in §3.4.</p><p>Experimental setup. We use the training and test sets predefined in the UCR archive. In both the training and test sets, we use the first 60% of the time series as input and the remaining 40% as output, ignoring class information. We then use the training set to learn a model that predicts the outputs from inputs and the test set to evaluate results with both Euclidean and DTW losses. In this experiment, we focus on a simple multi-layer perceptron (MLP) with one hidden layer and sigmoid activation. We also experimented with linear models and recurrent neural networks (RNNs) but they did not improve over a simple MLP.</p><p>Implementation details. Deep learning frameworks such as Theano, TensorFlow and Chainer allow the user to specify a custom pass for their function. Implementing such a backward pass, rather than resorting to automatic differentiation (autodiff), is particularly important in the case of soft-DTW: First, the autodiff in these frameworks is designed for vectorized operations, whereas the dynamic program used by the forward pass of Algorithm 2 is inherently element-wise; Second, as we explained in §2.2, our backward pass is able to re-use log-sum-exp computations from the forward pass, leading to both lower computational cost and better numerical stability. We implemented a custom backward pass in Chainer, which can then be used to plug soft-DTW as a loss function in any network architecture. To estimate the MLP's parameters, we used Chainer's implementation of Adam <ref type="bibr">(Kingma &amp; Ba, 2014)</ref>.</p><p>Qualitative results. Visualizations of the predictions obtained under Euclidean and soft-DTW losses are given in Figure <ref type="figure">1</ref>, as well as in Appendix F. We find that for sim- Quantitative results. A comparison summary of our MLP under Euclidean and soft-DTW losses over the UCR archive is given in Table <ref type="table" target="#tab_3">3</ref>. Detailed results are given in the appendix. Unsurprisingly, we achieve lower DTW loss when training with the soft-DTW loss, and lower Euclidean loss when training with the Euclidean loss. Because DTW is robust to several useful invariances, a small error in the soft-DTW sense could be a more judicious choice than an error in an Euclidean sense for many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose in this paper to turn the popular DTW discrepancy between time series into a full-fledged loss function between ground truth time series and outputs from a learning machine. We have shown experimentally that, on the existing problem of computing barycenters and clusters for time series data, our computational approach is superior to existing baselines. We have shown promising results on the problem of multistep-ahead time series prediction, which could prove extremely useful in settings where a user's actual loss function for time series is closer to the robust perspective given by DTW, than to the local parsing of the Euclidean distance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>m+1 = 1, r n+1,m+1 = r n,m 7: for j = m, . . . , 1 do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Interpolation between two time series (red and blue) on the Gun Point dataset. We computed the barycenter by solving Eq. (4) with (λ1, λ2) set to (0.25, 0.75), (0.5, 0.5) and (0.75, 0.25). The soft-DTW geometry leads to visibly different interpolations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Clusters obtained on the CBF dataset when plugging our proposed soft barycenter and that of DBA in Lloyd's algorithm. DBA absorbs the idiosyncrasies of the data, while soft-DTW can learn much smoother barycenters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Percentage of the datasets on which the proposed soft-DTW barycenter is achieving lower DTW loss (Equation (4) with γ = 0) than competing methods.</figDesc><table><row><cell></cell><cell>Random</cell><cell>Euclidean mean</cell></row><row><cell></cell><cell>initialization</cell><cell>initialization</cell></row><row><cell cols="2">Comparison with DBA</cell><cell></cell></row><row><cell>γ = 1</cell><cell>40.51%</cell><cell>3.80%</cell></row><row><cell>γ = 0.1</cell><cell>93.67%</cell><cell>46.83%</cell></row><row><cell>γ = 0.01</cell><cell>100%</cell><cell>79.75%</cell></row><row><cell>γ = 0.001</cell><cell>97.47%</cell><cell>89.87%</cell></row><row><cell cols="3">Comparison with subgradient method</cell></row><row><cell>γ = 1</cell><cell>96.20%</cell><cell>35.44%</cell></row><row><cell>γ = 0.1</cell><cell>97.47%</cell><cell>72.15%</cell></row><row><cell>γ = 0.01</cell><cell>97.47%</cell><cell>92.41%</cell></row><row><cell>γ = 0.001</cell><cell>97.47%</cell><cell>97.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1 summarizes the percentage of datasets on which the proposed soft-DTW barycenter achieves lower DTW loss when varying the smoothing parameter γ. The actual loss values achieved by different methods are indicated in Appendix G and Appendix H. As γ decreases, soft-DTW achieves a lower DTW loss than other methods on almost all datasets. This confirms ourFigure 5. Comparison between our proposed soft barycenter and the barycenter obtained by DBA and the subgradient method, on the ECG200 dataset. When DTW is insufficiently smoothed, barycenters often get stuck in a bad local minimum that does not correctly match the time series.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Percentage of the datasets on which the proposed soft-DTW based k-means is achieving lower DTW loss (Equation (5) with γ = 0) than competing methods. Each point above the diagonal represents a dataset where using our soft-DTW barycenter rather than that of DBA improves the accuracy of the nearest nearest centroid classifier. This is the case for 75% of the datasets in the UCR archive.</figDesc><table><row><cell></cell><cell>Random</cell><cell>Euclidean mean</cell></row><row><cell></cell><cell>initialization</cell><cell>initialization</cell></row><row><cell cols="2">Comparison with DBA</cell><cell></cell></row><row><cell>γ = 1</cell><cell>15.78%</cell><cell>29.31%</cell></row><row><cell>γ = 0.1</cell><cell>24.56%</cell><cell>24.13%</cell></row><row><cell>γ = 0.01</cell><cell>59.64%</cell><cell>55.17%</cell></row><row><cell>γ = 0.001</cell><cell>77.19%</cell><cell>68.97%</cell></row><row><cell cols="3">Comparison with subgradient method</cell></row><row><cell>γ = 1</cell><cell>42.10%</cell><cell>46.44%</cell></row><row><cell>γ = 0.1</cell><cell>57.89%</cell><cell>50%</cell></row><row><cell>γ = 0.01</cell><cell>76.43%</cell><cell>65.52%</cell></row><row><cell>γ = 0.001</cell><cell>96.49%</cell><cell>84.48%</cell></row><row><cell>Figure 7.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Averaged rank obtained by a multi-layer perceptron (MLP) under Euclidean and soft-DTW losses. Euclidean initialization means that we initialize the MLP trained with soft-DTW loss by the solution of the MLP trained with Euclidean loss. one-dimensional time series, an MLP works very well, showing its ability to capture patterns in the training set. Although the predictions under Euclidean and soft-DTW losses often agree with each other, they can sometimes be visibly different. Predictions under soft-DTW loss can confidently predict abrupt and sharp changes since those have a low DTW cost as long as such a sharp change is present, under a small time shift, in the ground truth.</figDesc><table><row><cell>Training loss</cell><cell>Random initialization</cell><cell>Euclidean initialization</cell></row><row><cell cols="2">When evaluating with DTW loss</cell><cell></cell></row><row><cell>Euclidean</cell><cell>3.46</cell><cell>4.21</cell></row><row><cell>soft-DTW (γ = 1)</cell><cell>3.55</cell><cell>3.96</cell></row><row><cell>soft-DTW (γ = 0.1)</cell><cell>3.33</cell><cell>3.42</cell></row><row><cell>soft-DTW (γ = 0.01)</cell><cell>2.79</cell><cell>2.12</cell></row><row><cell>soft-DTW (γ = 0.001)</cell><cell>1.87</cell><cell>1.29</cell></row><row><cell cols="2">When evaluating with Euclidean loss</cell><cell></cell></row><row><cell>Euclidean</cell><cell>1.05</cell><cell>1.70</cell></row><row><cell>soft-DTW (γ = 1)</cell><cell>2.41</cell><cell>2.99</cell></row><row><cell>soft-DTW (γ = 0.1)</cell><cell>3.42</cell><cell>3.38</cell></row><row><cell>soft-DTW (γ = 0.01)</cell><cell>4.13</cell><cell>3.64</cell></row><row><cell>soft-DTW (γ = 0.001)</cell><cell>3.99</cell><cell>3.29</cell></row></table><note>ple</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. MC gratefully acknowledges the support of a chaire de l'IDEX Paris Saclay.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="404" to="411" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Predicting Structured Data. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Aj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svn</forename><surname>Vishwanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the theory of dynamic programming</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="719" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higher-order factorization machines</title>
		<author>
			<persName><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName><surname>Akinori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The ucr time series classification archive</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><surname>Eamonn</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><surname>Nurjahan</surname></persName>
		</author>
		<author>
			<persName><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mueen</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename></persName>
		</author>
		<ptr target="www.cs.ucr.edu/˜eamonn/time_series_data/" />
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast global alignment kernels</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
				<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast computation of Wasserstein barycenters</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
				<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel for time series based on global alignments</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><surname>Jean-Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oystein</forename><surname>Birkenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP&apos;07</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Les éléments aléatoires de nature quelconque dans un espace distancié</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annales de l&apos;institut Henri Poincaré</title>
				<imprint>
			<date type="published" when="1948">1948</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="215" to="310" />
		</imprint>
	</monogr>
	<note>Presses universitaires de France</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Metric learning for temporal sequence alignment</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName><surname>Rémi</surname></persName>
		</author>
		<author>
			<persName><surname>Arlot</surname></persName>
		</author>
		<author>
			<persName><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1817" to="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization</title>
				<imprint>
			<publisher>Springer New York Inc</publisher>
			<date type="published" when="2001">2001. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Linear and integer programming vs linear integration and counting: a duality viewpoint</title>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">B</forename><surname>Lasserre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">New introduction to multiple time series analysis</title>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Lütkepohl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-step-ahead prediction using dynamic recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Parlos</surname></persName>
		</author>
		<author>
			<persName><surname>Rais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="765" to="786" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Summarizing a set of time series by averaging: From steiner sequence to compact multiple alignment</title>
		<author>
			<persName><forename type="first">Franc</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganc</forename><surname>¸ois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>¸arski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="91" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A global averaging method for dynamic time warping, with applications to clustering</title>
		<author>
			<persName><forename type="first">Franc</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><surname>¸ois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Ketterlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ganc ¸arski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="678" to="693" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic time warping averaging of time series allows faster and more accurate classification</title>
		<author>
			<persName><forename type="first">Franc</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><surname>¸ois</surname></persName>
		</author>
		<author>
			<persName><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><forename type="middle">E</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="470" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning stringedit distance</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ristad</surname></persName>
		</author>
		<author>
			<persName><surname>Sven</surname></persName>
		</author>
		<author>
			<persName><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast dictionary learning with a smoothed Wasserstein loss</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS&apos;16</title>
				<meeting>AISTATS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Protein homology detection using string alignment kernels</title>
		<author>
			<persName><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName><surname>Hiroto</surname></persName>
		</author>
		<author>
			<persName><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><surname>Jean-Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobuhisa</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Akutsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1682" to="1689" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing amino acid substitution matrices with a local alignment kernel</title>
		<author>
			<persName><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName><surname>Hiroto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Akutsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">246</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dynamic programming approach to continuous speech recognition</title>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seibi</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Congress on Acoustics</title>
				<meeting>the Seventh International Congress on Acoustics<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seibi</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech, and Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brijnesh</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06393</idno>
		<title level="m">Nonsmooth analysis and subgradient methods for averaging in dynamic time warping spaces</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diagnosis of multiple cancer types by shrunken centroids of gene expression</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramanian</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6567" to="6572" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient retrieval of similar time sequences under time warping</title>
		<author>
			<persName><forename type="first">Byoung-</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jagadish</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings., 14th International Conference on</title>
				<meeting>14th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
	<note>Data Engineering</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with a Wasserstein loss</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Araya-Polo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
