<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Peking University d AI for Science Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijie</forename><surname>Chen</surname></persName>
							<email>chenwj@dp.tech</email>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Shen</surname></persName>
							<email>shenf@dp.tech</email>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hangrui</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
							<email>zhanglf@dp.tech</email>
							<affiliation key="aff0">
								<orgName type="department">DP Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2022.08.04.502811</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent breakthroughs on protein structure prediction, namely AlphaFold, have led to unprecedented new possibilities in related areas. However, the lack of training utilities in its current open-source code hinders the community from further developing or adapting the model. Here we present Uni-Fold as a thoroughly open-source platform for developing protein folding models beyond AlphaFold. We reimplemented AlphaFold and AlphaFold-Multimer in the PyTorch framework, and successfully reproduced their from-scratch training processes with equivalent or better accuracy. Based on various optimizations, Uni-Fold achieves about 2.2 times training acceleration compared with AlphaFold under similar hardware configuration. On a benchmark using recently released multimeric protein structures, Uni-Fold outperforms AlphaFold-Multimer by approximately 2% on the TM-Score. Uni-Fold is currently the only open-source repository that supports both training and inference of multimeric protein models. The source code, model parameters, test data, and web server of Uni-Fold are publicly available 3 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the three-dimensional (3D) structures of proteins is important for studying their functionalities and thus the mechanisms of biological activities. Predicting how proteins fold via computational methods has long been a fundamental yet most challenging problem in life science. Along with the development of artificial intelligence, a recent breakthrough on in silico protein folding, namely AlphaFold <ref type="bibr" target="#b0">[1]</ref>, unprecedentedly achieved "near experimental accuracy" on a majority of monomeric proteins. To briefly summarize, this method directly predicts the atomic coordinates of a protein using a combination of its amino acid sequence, multiple sequence alignment (MSA), and solved homologous structures. In AlphaFold, the sequence and MSA information is encoded via Evoformer, an attention-based deep neural network. The predicted structure is decoded via a structure module, which predicts the local frames and torsion angles of all residues.</p><p>Adapted from AlphaFold, AlphaFold-Multimer <ref type="bibr" target="#b1">[2]</ref> was later developed by the same team via training AlphaFold on multimeric protein structures. AlphaFold-Multimer supports the prediction of protein complex structures with significantly better performances compared with traditional docking methods.</p><p>The occurrence of the AlphaFold system undoubtedly shed light on countless new possibilities of life science exploration. Well discussed in <ref type="bibr" target="#b2">[3]</ref>, these possibilities include assistance in solving experimental structures, structure-based drug discovery, and protein designing, etc. Meanwhile, the system is not yet perfect. For instance, it does not work well in predicting structures of membrane proteins, anti-bodies, and the combinations of proteins and ligands. In addition, a more discouraging fact is that the complexity of the AlphaFold system together with some other inconveniences made it almost impossible for smaller research groups to re-train the system. This expels them from the power of further developing the system, or adapting it to other applications. The aforementioned inconveniences include: 1) the current open-source code of AlphaFold does not contain any training scripts or utilities of the model; 2) the code of AlphaFold is based on JAX framework, which is limited to a community currently much smaller than TensorFlow and PyTorch; and 3) the original AlphaFold was designed and trained on Google Tensor Processing Unit (TPU), which is hardly accessible to the majority of the research community.</p><p>In order to encourage wider collaborations in the area, we present Uni-Fold as a thoroughly opensource platform for developing protein folding models beyond AlphaFold. Uni-Fold supports the training and inference of both monomeric and multimeric models with high accuracy and efficiency. In particular, we reimplemented both AlphaFold and AlphaFold-Multimer in the PyTorch framework, and reproduced their from-scratch training processes on larger training data. To summarize, Uni-Fold made the following contributions:</p><p>â€¢ Uni-Fold is an open-source platform that welcomes community contributions. We proved the correctness of the implementation by reproducing the from-scratch training process of AlphaFold and AlphaFold-Multimer with equivalent or better performances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Besides reimplementing AlphaFold and AlphaFold-Multimer according to the official code, we made several alterations and improvements in Uni-Fold. In the rest of this paper, we refer to the monomeric model as Uni-Fold Monomer, and the multimeric one as Uni-Fold Multimer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Protein Homology</head><p>In this subsection, we describe the process of searching homologous sequences and structures in Uni-Fold. Unless otherwise specified, the same pipeline is used in both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Genetic Search</head><p>We reused the genetic search protocol in AlphaFold and AlphaFold-Multimer. We used JackHMMER <ref type="bibr" target="#b3">[4]</ref> with MGnify <ref type="bibr" target="#b4">[5]</ref>, JackHMMER with UniRef90 <ref type="bibr" target="#b5">[6]</ref>, and HHBlits <ref type="bibr" target="#b6">[7]</ref> with Uniclust30 <ref type="bibr" target="#b7">[8]</ref> + BFD for monomers. For multimers, we additionally used JackHMMER with UniProt <ref type="bibr" target="#b8">[9]</ref> to search for sequences with species annotations. We used the same hyperparameters of the MSA search tools as AlphaFold. Identical sequences in the MSAs were deduplicated. The MSA block deletion and MSA clustering strategies of AlphaFold were implemented as is.</p><p>Cross-Chain Genetics It was widely demonstrated that MSAs with properly paired orthologs encode cross-chain co-evolutionary information of protein heteromers and thus serve as strong indicators of the complex structure. In Uni-Fold Multimer, we adopted MSA pairing, a technique proposed in <ref type="bibr" target="#b9">[10]</ref> and later used in AlphaFold-Multimer to build cross-chain genetics. For homomeric chains in a complex, we simply concatenated the duplicated MSAs of each chain; for heteromeric chains, we ranked the MSA rows of each chain by the species similarities to the target sequence, and then concatenated rows of the same rank. Unpaired MSA rows, as well as those with no species annotations, were padded with gap symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template Search</head><p>The template search process of Uni-Fold was much like to that of AlphaFold. To be more specific, we used structure templates that were released before April 29th, 2020. In training, the templates were first filtered such that all templates were released before the target. Top n = 20 templates (if existed) modeled by the "sum_prob" output of HHSearch were kept and further sub-sampled to k = min(Uniform([0, n]), 4) templates. In inference, the top 4 templates were used. For multimers, templates were individually searched for each heteromeric chain and sampled together. We did not use cross-chain templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model and Loss Functions</head><p>In this subsection, we describe the model and loss functions of Uni-Fold, which were implemented closely following AlphaFold(-Multimer). Alterations are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alterations in Model Architecture</head><p>We globally replaced the ReLU activation in AlphaFold(-Multimer) to Gaussian Error Linear Units (GELUs) <ref type="bibr" target="#b10">[11]</ref>, calculated as</p><formula xml:id="formula_0">GELU (x) = 1 2 x 1 + tanh 2 Ï€ x + 0.044715x 3 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>As the OuterProductMean module in AlphaFold tended to produce large numerical values which led to training instability, we added a postprocessing layer to its output to lower its values:</p><p>x = Linear (LayerNorm (x)) .</p><p>(2) In most auxiliary heads but the predicted-LDDT head, AlphaFold(-Multimer) used a single linear projection (x = Linear (x)). We enhanced this with an additional activation function:</p><p>x = Linear (GELU (Linear (LayerNorm (x))))</p><p>Shuffled Multi-Chain Permutation Alignment In AlphaFold-Multimer, a greedy method was used to disambiguate homologous chains to their respective labels. The method first chose an anchor candidate with the least ambiguity, by which the predicted structure was superposed to the ground truth. A candidate permutation alignment was then derived by greedily minimizing the error of aligned chain centers. The method iterated over all anchor candidates to derive the optimal permutation alignment. In Uni-Fold Multimer, this process was further modified. Instead of greedily minimizing the chain center error, we minimized C Î± -RMSD under the superposition of the anchor candidates, so that tangled chains could be better disambiguated. Meanwhile, we shuffled the order of chains before applying the greedy algorithm, and output the best alignment among n shuffles.</p><p>Entity-sharing MSA Mask NaÃ¯vely applying random masks on the MSAs of homomeric sequences in multimers (as AlphaFold-Multimer did) would lead to data leakage in the masked MSA prediction task, as their MSAs were identical. In Uni-Fold Multimer, we addressed this problem with entitysharing MSA mask, where the same MSA masks were used for chains with identical sequences.</p><p>Violation Loss Different violation losses were used between AlphaFold and AlphaFold-Multimer.</p><p>In both Uni-Fold Monomer and Multimer, we followed the recipe of AlphaFold-Multimer, where the loss of steric clashes of non-bonded atoms was normalized by the number of clashing atom pairs, and the bond angle loss was scaled with weight 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Norm Loss</head><p>In both Uni-Fold Monomer and Multimer, we added representation norm losses to encourage numerical stability. The losses L msa and L pair punished the variations of the MSA and pair representations among recycling iterations:</p><formula xml:id="formula_3">L msa (m) = mean l,i max âˆ¥m l,i âˆ¥ âˆ’ d msa âˆ’ Ï„, 0 ,<label>(4)</label></formula><formula xml:id="formula_4">L pair (z) = mean i,j max âˆ¥z i,j âˆ¥ âˆ’ d pair âˆ’ Ï„, 0 ,<label>(5)</label></formula><p>where d msa and d pair are the dimensions of MSA and pair representations and Ï„ = 1 is a tolerance constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation and Acceleration</head><p>In this subsection, we describe the implementation details of Uni-Fold. Some acceleration techniques are specifically discussed in this subsection. <ref type="bibr" target="#b11">[12]</ref> is widely used to accelerate the training of large Transformers, where partial or whole calculations of the model is conducted in half-precision to save time and memory. Instead of storing all activations in bfloat16 format as AlphaFold(-Multimer) did, we used bfloat16 for most of the layers except for the input embedding layers, geometry-related operations, softmax activations, layer normalizations, and the calculation of all losses. In the specific implementation, parameters in both bfloat16 and float32 formats were maintained. After the gradients were calculated, they were copied into float32 format and then used to update the float32 parameters. Then, before the next forward process, the float32 parameters were copied back into bfloat16 ones. Parameters and gradients were flattened into large tensors during this process, which significantly reduced the time of kernel calls. When casting values from float32 to bfloat16 , we used stochastic rounding <ref type="bibr" target="#b12">[13]</ref> which was previously shown to encourage numerical robustness. Notably, float16 was also supported as a feature of Uni-Fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Precision Training Mixed precision training</head><p>Operator Fusion The idea of operator fusion is to merge multiple consecutive operators into one. This accelerates the calculation by reducing the vain cost of repeated global access of GPU memory. Similar to previous works on Transformer acceleration <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, we fused the softmax and layer normalization operators. This operator fusion was particularly important because its two components were done in float32 format. Fusing them saved not only the memory access time, but also the type converting cost on both time and memory. The operator fusing in Uni-Fold was based on an open-source repository <ref type="foot" target="#foot_0">4</ref> which was derived from the NVIDIA APEX package <ref type="foot" target="#foot_1">5</ref> . We further optimized its softmax kernel for large columns based on the softmax implementation of OneFlow <ref type="bibr" target="#b16">[17]</ref>.</p><p>Per-Sample Gradient Clipping A notable detail in AlphaFold is that the gradient clipping was applied to each sample instead of each batch. However, in most existing AlphaFold replicas such as OpenFold, per-batch gradient clipping was widely used. In Uni-Fold, we implemented both and found that per-sample gradient clipping displays a significant advantage. Detailed comparisons are shown in Section 3.4</p><p>Distributed Framework and Hardware We used a cluster of 128 NVIDIA A100 GPUs with 40GB memory for the distributed training of Uni-Fold. The data parallelism strategy of AlphaFold was used, where each GPU contained one training sample at each step. Meanwhile, as mixed-precision and per-sample gradient clipping were used, the distributed algorithm was slightly modified from the standard data parallel algorithm. Specifically, after backward, we copied the bfloat16 gradients to float32 ones and then performed per-sample gradient clipping on each GPU independently. An all-reduce operation was finally applied to the clipped gradients.</p><p>Training Data Compression In order to reduce run-time parsing costs, we preprocessed and stored sequence features (MSA, templates, etc.) and labels (coordinates from PDB and MMCIF files) as NumPy arrays. To reduce the storage and I/O costs, we adopted several data compression tricks.</p><p>The PDB dataset consisted of 600,000+ protein chains with only 130,000+ unique ones. Sharing MSAs and templates among identical sequences reduced the storage space to approximately 1/5. We also compressed the deletion matrices of MSAs into sparse matrices. This further reduced the storage space to approximately 1/6. The features were then compressed to GZIP format, which further reduced the storage space to 1/5. A combination of these tricks reduced the storage space from more than 300TB to approximately 2TB with negligible I/O expenses.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Uni-Fold</head><p>Training Protocol For the training of Uni-Fold Monomer, we used a much simpler two-stage scheme compared with the official AlphaFold. In the initial training stage, we followed the same setting as AlphaFold. In the finetuning stage, we skipped the first finetuning stage of AlphaFold and directly finetuned the model following the configurations of model 1.1.2 (Model 2). Uni-Fold Multimer adopted a similar two-stage scheme, where the initial training and the finetuning configurations of AlphaFold-Multimer were used. Details of the training protocol are summarized in Table <ref type="table" target="#tab_1">1</ref>, where we italicize our alterations. Training Data We collected all PDB structures released before January 16th, among which chain structures were used to train the monomer model, and assembly structures (including those with one chain), the multimer model. Following AlphaFold, we filtered out the structures with resolutions larger than 9 Ã…, and those with any single amino acid accounting for more than 80% of the sequence. For multimer training, besides the monomer filter, we further filtered out assemblies with more than 18 chains to encourage training stability.</p><p>Self-Distillation Uni-Fold adopted the self-distillation strategy of AlphaFold. Similarly, the selfdistillation dataset was constructed from Uniclust30 (version 2018_08). To balance data quality and computational cost, we first filtered the sequence clusters in the dataset such that all center sequences have lengths between 200 and 1,024. This left approximately 5 million clusters, which were further used to search for MSAs against Uniclust30, the dataset itself, with HHBlits. Default search parameters are used except for the number of iterations n = 3. Among the output MSAs, we first removed those with less than 200 sequences, then removed sequences that appeared in at least two other MSAs, yielding a final dataset of about 360,000 sequences. Predicted structures by an early version of Uni-Fold were used as labels, where structures of residues with Predicted LDDT lower than 50% were masked. Both Uni-Fold Monomer and Multimer used this self-distillation dataset. We did not use multimeric self-distillation samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Accuracy Benchmarks</head><p>Data We evaluated Uni-Fold and other baselines on recently released protein structures in the Protein Data Bank (PDB) <ref type="bibr" target="#b17">[18]</ref>. We collected a total of 1,181 PDB structures released between January 17th and July 14th, 2022. For monomer evaluations, we collected the structures of all sequences and kept those with less than 40% template identity <ref type="foot" target="#foot_2">6</ref> . The left sequences were further filtered so that all sequences have resolutions less than 3Ã… and lengths between 50 and 1024, yielding a total of 301 unique sequences with 876 structures <ref type="foot" target="#foot_3">7</ref> . For multimer evaluations, we collected assemblies with 2 or more chains, among which at least one chain had less than 40% template identity. The left was further filtered so that all assemblies have a resolution of less than 3.5 Ã… and the total number of residues between 50 and 1536, yielding a total of 162 assemblies. The PDB-IDs of the test dataset are publicly available<ref type="foot" target="#foot_4">8</ref> . To test the models' power of predicting structures of entire assemblies, we did not process the multimeric structures into contacted pairs of chains as AlphaFold-Multimer did.</p><p>The homology search process is described in Section 2.1. All baselines used the same features.</p><p>Baselines We compared Uni-Fold with AlphaFold(-Multimer) and OpenFold. For monomer evaluations, we report the performances of AlphaFold Model 2 and OpenFold Model 2, which displayed the best accuracy and robustness in early tests. The training of Uni-Fold Monomer also followed the setting of Model 2. For AlphaFold-Multimer on monomer tasks, we report the best performance among its 5 public models (Model 5). For AlphaFold-Multimer on multimer tasks, we report all performances of its 5 public models. We used the v2 parameters of AlphaFold-Multimer.</p><p>Metrics For monomer evaluations, as multiple ground-true structures ({T i }) may exist for a sequence, we calculated the metrics using a prediction (P) and its best-aligned structure. Taking TM-Score as an example,</p><formula xml:id="formula_5">TM(P, {T i }) â‰œ max j TM(P, T j ).<label>(6)</label></formula><p>For multimer evaluations, as we evaluated protein assemblies as a whole, using docking-based metrics such as DockQ might lead to confusion. Alternatively, we made a natural extension to adapt single-chain metrics to assemblies conceptually merging all chains into one. Specifically, for all baselines and metrics on multimeric tasks, the optimal alignment between a prediction and the ground truth was calculated on the entire assembly structure. The scores were then calculated by averaging over all CÎ± atoms. For TM-Score, we used the number of all residues in the assembly to calculate d 0 . This process was iterated over all possible permutation alignments and the best score was reported.</p><p>Results Table <ref type="table" target="#tab_2">2</ref> shows the results of evaluations. In general, Uni-Fold displays equivalent or better performances compared with AlphaFold and OpenFold on both monomer and multimer tasks. We also evaluated how well multimer models can predict monomer structures by using AlphaFold-Multimer and Uni-Fold Multimer directly on monomer tasks. An obvious drop in performance was observed, which is consistent with the results in <ref type="bibr" target="#b1">[2]</ref>. On both monomer and multimer tasks, Uni-Fold Multimer significantly outperformed AlphaFold-Multimer on RMSD and TM-Score. As two models shared the same data pipeline and model implementation, we would conclude that the elevation is obtained using the updated training datasets and the model and loss alterations discussed in Section 2.2. Nevertheless, according to the performances of GDT-HA and C Î± -LDDT, predictions from AlphaFold-Multimer may have more accurate localities compared with Uni-Fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficiency Benchmarks</head><p>this subsection, we introduce the efficiency of Uni-Fold compared with other existing implementations of AlphaFold. On all tasks and all models, mixed precision with bfloat16 was used. Except for AlphaFold(-Multimer) which used 128 TPU cores, all baselines used the same hardware configuration with 128 NVIDIA A100 GPUs.</p><p>Training We compared the end-to-end training time of Uni-Fold and other existing implementations of AlphaFold. On monomer tasks, we followed closely the model configurations in <ref type="bibr" target="#b18">[19]</ref> for fair comparison; on multimer tasks, we followed <ref type="bibr" target="#b1">[2]</ref>, where N extra_msa is inferred from the code. The detailed configurations are displayed in Table <ref type="table" target="#tab_3">3</ref>. As AlphaFold-Multimer introduced its training details too briefly, we did not know the exact finetuning steps. We chose 10,000 as the finetuning steps of Uni-Fold Multimer. Results of OpenFold and HelixFold are referred directly from <ref type="bibr" target="#b18">[19]</ref>; results of AlphaFold(-Multimer) are from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. We do not include the results of FastFold in this table     <ref type="table" target="#tab_4">4</ref> shows the results, in which acceleration ratios to official AlphaFold(-Multimer), "Accel. to AF", are also calculated. With a similar hardware configuration, the monomer benchmark performance for Uni-Fold (Uni-Fold (benchmark) is about 2.2 times faster than the official statistics of AlphaFold, leading to other recent implementations.</p><p>Under the real configuration of training Uni-Fold (Uni-Fold (real)) in Table <ref type="table" target="#tab_1">1</ref>, the total training time is approximately 4.13 days. Despite that we do not know the exact steps of AlphaFold-Multimer, we achieved equivalent or better performances with an about 1.9 time faster training process using configurations in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Inference Figure <ref type="figure" target="#fig_0">1</ref> shows the inference speed and memory usage of Uni-Fold and other baselines. The number of MSAs was set as 128. We did not use chunking in any baselines. Gray dash lines indicate that longer sequences lead to Out-of-Memory (OOM) error. Uni-Fold consistently outperforms all baselines on the inference speed. Uni-Fold also enjoys less peak GPU memory usage, indicating that with the same hardware, Uni-Fold can be used to predict longer sequences. We looked into the code of FastFold, and found that the fluctuation of inference time of FastFold at N res = 1, 024 was caused by a switch of computation kernel.</p><p>Evoformer Benchmark To further benchmark the efficiency of Uni-Fold against more baselines that do not include end-to-end training time, following <ref type="bibr" target="#b19">[20]</ref>, we tested the running time and peak GPU memory consumption of an Evoformer layer in both forward and backward propagation. We used a configuration of 128 MSAs and 256 residues. Gradient checkpointing was disabled, as single Evoformer layers are the minimal checkpointing units. We included OpenFold and its variant, OpenFold with Low Memory Attention (OpenFold LMA ), as well as FastFold as baselines under this setting <ref type="foot" target="#foot_5">9</ref> . Results are summarized in Table <ref type="table" target="#tab_5">5</ref>, where Uni-Fold displays the best performances on both speed and memory efficiency. A surprising observation is that OpenFold LMA is much slower and uses more memory compared with OpenFold. We double-checked our benchmark scripts and would conclude that the LMA optimization is possibly depreciated in the ongoing development of OpenFold.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effect of Per-Sample Gradient Clipping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>In Figure <ref type="figure" target="#fig_3">3</ref>, we present a demo case of Uni-Fold Multimer and AlphaFold-Multimer predictions. The assembly is a complex of 5 chains and a peptide, including a GPCR-G protein complex, a synthetic peptide ligand, and a nanobody (B9-scFv) to stabilize the complex. Both models correctly predicted the structure of the GPCR-G protein complex (the main structure), however, the B9-scFv was wrongly predicted by AlphaFold-Multimer as isolated to the main structure. This led to the low TM-Score of AlphaFold-Multimer. Meanwhile, as C Î± -LDDT focuses on the qualities of local structures, it remained relatively high for AlphaFold-Multimer. Uni-Fold correctly predicted the combination of the main structure and the B9-scFv. This example intuitively explains the different performances on C Î± -LDDT and TM-Score in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Plenty of efforts have been devoted to reimplementing or improving AlphaFold. RoseTTAFold <ref type="bibr" target="#b20">[21]</ref>, known as the earliest re-implementation of AlphaFold (before its release of code), achieved near performance to AlphaFold, while its developers also decided not to release the training code. Open-Fold <ref type="bibr" target="#b21">[22]</ref> is an open-source repository that includes the training utilities of the AlphaFold model, yet currently, it does not support the training and prediction of multimeric protein structures. Fast-Fold <ref type="bibr" target="#b19">[20]</ref>, on the other hand, proposed a model parallelism solution based on the implementation of OpenFold to accelerate training. However, the authors did not provide any details or results of from-scratch training except for efficiency. HelixFold <ref type="bibr" target="#b18">[19]</ref> is another recent implementation of AlphaFold that supports training and model parallelism under the PaddlePaddle framework. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Inference speed and memory usage of Uni-Fold and other baselines with regard to different sequence lengths. Gray dash lines indicate that the next data points are infeasible due to Out-of-Memory errors.</figDesc><graphic url="image-1.png" coords="7,108.00,309.71,396.01,265.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training accuracies of Uni-Fold Monomer under different gradient clipping strategies.</figDesc><graphic url="image-2.png" coords="8,137.70,72.00,336.59,156.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure2shows the training accuracies measured by C Î± -LDDT with regard to different gradient clipping strategies. By default, a per-sample gradient clipping by the global norm with value 0.1 was used. We compared this strategy with per-batch gradient clipping with value 0.1, 0.05, and 0.025. The intuition of reducing the value of per-batch clipping was that per-sample clipping always has smaller gradients than per-batch clipping under the same value. Results in Figure2indicate that per-sample clipping leads to consistently better model convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Case study of a protein complex (PDB-ID 7T6U).</figDesc><graphic url="image-3.png" coords="9,179.28,72.00,253.44,136.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5</head><label></label><figDesc>The Chronicle of Uni-Fold Development Released on December 8th, 2021, Uni-Fold v1.0.0 10 (Uni-Fold-JAX) was the first open-source repository (with training scripts) that reproduced the from-scratch training of AlphaFold with approaching accuracy. Currently, Uni-Fold-JAX is still the only open-source repository that supports the training of official AlphaFold implementation. On April 24th, 2022, we released Uni-Fold v1.1.0 as a service on Hermite. Compared with AlphaFold, Uni-Fold v1.1.0 enjoyed faster training and inference speed as well as better accuracy on newly (at then) released PDB structures. On May 26th, 2022, we released Uni-Fold v2.0.0 on Hermite. Uni-Fold v2.0.0 contained the first reproduction of from-scratch training of AlphaFold-Multimer, with slightly better accuracy on newly (at then) released PDB multimeric structures. On August 1st, 2022, we made Uni-Fold v2.0.0 publicly available on GitHub. The released code supported both training and inference of AlphaFold(-Multimer). Currently, this is the only open-source repository that supports the training of AlphaFold-Multimer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training protocol and time for Uni-Fold. Alterations are italicized.</figDesc><table><row><cell>Task</cell><cell cols="2">Monomer</cell><cell cols="2">Multimer</cell></row><row><cell>Model</cell><cell>Init. Training</cell><cell cols="2">Finetuning Init. Training</cell><cell>Finetuning</cell></row><row><cell>Parameters initialized from</cell><cell cols="2">Random Init. Training</cell><cell cols="2">Random Init. Training</cell></row><row><cell>Sequence crop size</cell><cell>256</cell><cell>384</cell><cell>384</cell><cell>384</cell></row><row><cell>Number of sequences (MSA + templ.)</cell><cell>128</cell><cell>512</cell><cell>128</cell><cell>256</cell></row><row><cell>Number of templates</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>Number of extra MSAs</cell><cell>1024</cell><cell>1024</cell><cell>1152</cell><cell>1152</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Peak learning rate</cell><cell>1e-3</cell><cell>5e-4</cell><cell>1e-3</cell><cell>5e-4</cell></row><row><cell>Warm-up steps</cell><cell>1,000</cell><cell>500</cell><cell>1,000</cell><cell>500</cell></row><row><cell>Decay steps</cell><cell>50,000</cell><cell>N/A</cell><cell>50,000</cell><cell>N/A</cell></row><row><cell>Decay ratio</cell><cell>0.95</cell><cell>N/A</cell><cell>0.95</cell><cell>N/A</cell></row><row><cell>Total training steps</cell><cell>80,000</cell><cell>5,000</cell><cell>80,000</cell><cell>10,000</cell></row><row><cell>Ratio of self-distillation samples</cell><cell>75%</cell><cell>50%</cell><cell>50%</cell><cell>50%</cell></row><row><cell>Violation loss weight</cell><cell>0.0</cell><cell>0.02</cell><cell>0.02</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy results of Uni-Fold and baselines on recently released PDB structures. *: Note that for multimer models on monomer tasks, the input is given as single sequences instead of assemblies.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="5">C Î± -RMSD (Ã…) TM-Score GDT-TS GDT-HA C Î± -LDDT</cell></row><row><cell></cell><cell>AlphaFold</cell><cell>4.669</cell><cell>0.776</cell><cell>0.686</cell><cell>0.531</cell><cell>0.879</cell></row><row><cell>Monomer (301 Seqs.)</cell><cell>OpenFold Uni-Fold Monomer AlphaFold-Multimer*</cell><cell>4.737 4.683 5.458</cell><cell>0.776 0.776 0.751</cell><cell>0.685 0.689 0.663</cell><cell>0.528 0.537 0.513</cell><cell>0.874 0.880 0.865</cell></row><row><cell></cell><cell>Uni-Fold Multimer*</cell><cell>5.142</cell><cell>0.756</cell><cell>0.658</cell><cell>0.496</cell><cell>0.864</cell></row><row><cell></cell><cell></cell><cell>8.263</cell><cell>0.763</cell><cell>0.607</cell><cell>0.458</cell><cell>0.883</cell></row><row><cell></cell><cell></cell><cell>8.115</cell><cell>0.764</cell><cell>0.605</cell><cell>0.456</cell><cell>0.882</cell></row><row><cell>Multimer</cell><cell>AlphaFold-Multimer</cell><cell>8.086</cell><cell>0.764</cell><cell>0.606</cell><cell>0.456</cell><cell>0.883</cell></row><row><cell>(162 PDBs)</cell><cell></cell><cell>8.141</cell><cell>0.764</cell><cell>0.611</cell><cell>0.462</cell><cell>0.885</cell></row><row><cell></cell><cell></cell><cell>8.238</cell><cell>0.761</cell><cell>0.609</cell><cell>0.461</cell><cell>0.882</cell></row><row><cell></cell><cell>Uni-Fold Multimer</cell><cell>7.025</cell><cell>0.783</cell><cell>0.619</cell><cell>0.460</cell><cell>0.872</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Configurations of the total training time benchmark. ) N templ N res N seq N extra_seq</figDesc><table><row><cell cols="3">Task samples (Ã—10 6 Monomer Init./ FT Initial Training 10.0 Finetuning 1.5</cell><cell>4 256 128 4 384 512</cell><cell>1024 5120</cell></row><row><cell>Multimer</cell><cell>Initial Training Finetuning</cell><cell>10.0 ?</cell><cell>4 384 128 4 384 256</cell><cell>1152 1152</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Training times (days) of Uni-Fold and other AlphaFold implementations. *: Not rigorous benchmark performance.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="3">Initial Training Finetuning Total Accel. to AF</cell></row><row><cell></cell><cell>AlphaFold</cell><cell>7.-</cell><cell>4.-11.-</cell><cell>1.00</cell></row><row><cell>Monomer</cell><cell>OpenFold HelixFold</cell><cell>8.05 5.29</cell><cell>2.80 10.85 2.26 7.55</cell><cell>1.01 1.46</cell></row><row><cell></cell><cell>Uni-Fold (benchmark)</cell><cell>3.30</cell><cell>1.72 5.02</cell><cell>2.19</cell></row><row><cell></cell><cell>Uni-Fold (real)*</cell><cell>3.38</cell><cell>0.73 4.11</cell><cell>2.68</cell></row><row><cell>Multimer</cell><cell>AlphaFold-Multimer* Uni-Fold Multimer*</cell><cell>14.-7.45</cell><cell>2.-16.-1.09 8.54</cell><cell>1.00 1.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Running time and GPU memory usage of an Evoformer Layer. The number of MSA is 128 and the number of residues is 256.</figDesc><table><row><cell>Model</cell><cell cols="4">Forward (ms) Backward (ms) Total (ms) Peak Memory (GB)</cell></row><row><cell>OpenFold LMA</cell><cell>20.911</cell><cell>39.007</cell><cell>59.918</cell><cell>2.138</cell></row><row><cell>OpenFold</cell><cell>17.198</cell><cell>23.703</cell><cell>40.901</cell><cell>2.019</cell></row><row><cell>FastFold</cell><cell>9.415</cell><cell>20.247</cell><cell>29.662</cell><cell>1.954</cell></row><row><cell>Uni-Fold</cell><cell>8.440</cell><cell>18.472</cell><cell>26.912</cell><cell>1.917</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">https://github.com/guolinke/fused_ops</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">https://github.com/NVIDIA/apex</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">The template identity refers to the maximum single template coverage using pipelines in Section 2.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">A sequence may have multiple solved structures in homomers or different assemblies.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4">https://github.com/dptech-corp/Uni-Fold</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5">We used OpenFold code with commit ID a44bbebbfa4dbb8b228e0c8d77338173cf78d699, FastFold code with commit ID 665e6c97a7d95d3db2df860d104fa3c456c71fe2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6">https://github.com/dptech-corp/Uni-Fold-jax</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Å½Ã­dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishub</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Berghammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pushmeet Kohli</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Antropova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Å½Ã­dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rishub Jain</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Kathryn Tunyasuvunakool</note>
	<note>Pushmeet Kohli, John Jumper, and Demis Hassabis. Protein complex prediction with alphafold-multimer. bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What&apos;s next for AlphaFold and the AI protein-folding revolution</title>
		<author>
			<persName><forename type="first">Ewen</forename><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">604</biblScope>
			<biblScope unit="page" from="234" to="238" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hidden markov model speed heuristic and iterative HMM search procedure</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Informatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MGnify: the microbiome analysis resource in 2020</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Beracochea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Burgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Cochrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Crusoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Sakharova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Scheremetjew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Korobeynikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Shlemov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kunyavskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alla</forename><surname>Lapidus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D570" to="D578" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">E</forename><surname>Baris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cathy</forename><forename type="middle">H</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uniprot</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HHblits: lightningfast iterative protein sequence searching by HMM-HMM alignment</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>SÃ¶ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniclust databases of clustered and deeply annotated protein sequences and alignments</title>
		<author>
			<persName><forename type="first">Milot</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Von Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clovis</forename><surname>Driesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">J</forename><surname>Galiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>SÃ¶ding</surname></persName>
		</author>
		<author>
			<persName><surname>Steinegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D170" to="D176" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UniProt: the universal protein knowledgebase in 2021</title>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D480" to="D489" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning reveals many more inter-protein residue-residue contacts than direct coupling analysis</title>
		<author>
			<persName><forename type="first">Tian-Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>arXiv, 1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Frederick Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>GarcÃ­a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno>arXiv, 1710.03740</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Pedram</forename><surname>Zamirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<idno>arXiv, 2010.06192</idno>
		<title level="m">Revisiting bfloat16 training</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06644</idno>
		<title level="m">Efficient denoising pretraining of large scale autoencoding language models with model generated signals</title>
				<meeting><address><addrLine>Metro</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Oneflow: Redesign the distributed deep learning framework from scratch</title>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Protein Data Bank (PDB): The single global macromolecular structure archive</title>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Stephen K Burley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><forename type="middle">J</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Kleywegt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haruki</forename><surname>Markley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><surname>Velankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">1607</biblScope>
			<biblScope unit="page" from="627" to="641" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Helixfold: An efficient implementation of alphafold2 using paddlepaddle</title>
		<author>
			<persName><forename type="first">Guoxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingfei</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv, 2207.05477</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Shenggan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin-Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>You</surname></persName>
		</author>
		<idno>arXiv, 2203.00854</idno>
		<title level="m">Fastfold: Reducing alphafold training time from 11 days to 67 hours</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">Minkyung</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justas</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rie</forename><surname>Gyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">N</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Dustin</forename><surname>Kinch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hahnbeom</forename><surname>MillÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carson</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><forename type="middle">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">H</forename><surname>Degiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andria</forename><forename type="middle">V</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberdina</forename><forename type="middle">A</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><surname>Ebrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Opperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Sagmeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tea</forename><surname>Buhlheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><forename type="middle">K</forename><surname>Pavkov-Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Rathinaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><forename type="middle">K</forename><surname>Dalwadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Christopher</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><forename type="middle">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Grishin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">6557</biblScope>
			<biblScope unit="page" from="871" to="876" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gustaf</forename><surname>Ahdritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazim</forename><surname>Bouatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kadyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gerecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><surname>Openfold</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
