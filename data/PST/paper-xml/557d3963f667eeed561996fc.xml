<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">487387A861F38BF5C682F4FBA976CEEF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and Synthesis of a Class of Discrete-Time</head><p>Neural Networks Described on Hypercubes Anthony N. Michel, Fellow,, Absfracf-In this paper, we first present a qualitative analysis for a class of synchronous discrete-time neural networks defined on hypercubes in the state space. Next, we utilize these analysis results to establish a design procedure for associative memories to he implemented by the present class of neural networks. To demonstrate the storage ability and flexibility of our synthesis procedure, several specific examples are considered.</p><p>The present design procedure has essentially the same desirable features as our earlier results for continuous-time neural networks: for a given system dimension, networks designed by the present method may have the ability to store more patterns (as asymptotically stable equilibria) than corresponding discrete-time networks designed by other techniques; the present design method guarantees to store all of the desired patterns as asymptotically stable equilibrium points; and the present method provides guidelines for reducing the number of spurious states and for estimating the extent of the domains of attraction for the patterns to he stored as asymptotically stable equilibrium points. In addition, the present results provide a means of implementing neural networks by serial processors and special digital hardware. Thus, the present results make possible efficient digital simulations of continuous-time neural networks designed by the present method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N the present paper, we investigate the qualitative properties I of neural networks described by a system of first-order linear ordinary difference equations, which are defined on a closed hypercube of the state space with solutions extended to the boundary of the hypercube. When solutions are located on the boundary of the hypercube, the system is said to be in a saturated mode. The class of systems considered herein retains the basic structure of the Hopfield model; however, whereas the discrete Hopfield model [ l ] is allowed to operate asynchronously, the present model will be required to operate in a synchronous mode.</p><p>For the present model, we develop an analysis method which will enable us to determine and characterize the set of asymptotically stable equilibrium points and the set of unstable equilibrium points. The latter set of equilibria can be used to estimate the domains of attraction for the elements of the former set.</p><p>The synthesis procedure which we develop for the present class of neural networks is motivated by our earlier work, presented in [6]- <ref type="bibr" target="#b6">[8]</ref>. Specific examples suggest that the present design method is more effective than existing design techniques for discrete neural networks 1161, [4], [ 5 ] , <ref type="bibr" target="#b7">[9]</ref>.</p><p>The class of systems considered herein can easily be implemented in digital hardware. In addition, when implemented by a serial processor (e.g., in digital simulations), the present class of neural networks offers considerable advantages over digital IEEE, Jie Si, and Gune Yen simulations of the differential equations used to represent the continuous-time neural networks considered in [6]- <ref type="bibr" target="#b6">[8]</ref>.</p><p>The applicability of the present results is demonstrated by means of several specific examples, including pattern recognition applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summary of the Results</head><p>We consider neural networks described by a system of firstorder linear difference equations, which are defined on a closed hypercube. We refer to such systems as "linear systems in a saturated mode" (LSSM). Specifically, we consider neural networks described by equations of the form  <ref type="figure">(S,</ref>) where 1 when 0 2 ! sat 0 = 0 when -1 &lt; 0 &lt; 1 -1 when0 5 -1 i</p><p>In vector notation, we can express system (S,) as a first-order vector difference equation (S ), where Tis an n X n matrix with real components T I / , x is a real n-vector with components x,, s a t ( x ) is a real n-vector with components s a r ( x , ) , and I is a real n-vector with components I,,</p><p>x ( k + 1) = s a t ( T x ( k ) + I ) ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( S )</head><p>The principal difference between system ( S ) and the usual systems described by linear difference equations is that the latter are defined on open subsets of R" while the former is defined on the closed set D" = { x E R " : -1 5 x, 5 I , i = 1, 1 * * , n } , We will show that an LSSM system may be viewed as a reduced-order linear system. In general, the solutions of system ( S ) will be in a saturated mode for some or all of the discrete points in time, k = 0, 1, 2 , . . . . System ( S ) can easily be implemented on a digital computer or in digital hardware.</p><p>The results which we establish for system ( S ) fall into two categories. One type of result addresses analysis while the other type pertains to synthesis procedures for system ( S ). In the following, we give a brief summary of the results developed herein.</p><p>Analysis: (These results are obtained under an appropriate set of assumptions, which we will give later.) a) First, a definition of solutions of system ( S ) is developed. On the boundary aD", solutions in the "saturated" mode are characterized. The solutions for ( S ) are unique and exist for all k = 0 , 1 , 2 ; . . . k = 0, 1, 2, . . . . 1045-9227/91/0100-0032$0l 00 c 1991 IEEE b) The concept of equilibrium point for ( S ) is made precise. An efficient algorithm is developed to determine the location and stability properties of each equilibrium point of ( S ).</p><p>c) It is shown that system ( S ) has at most 3" equilibrium points and at most 2" asymptotically stable equilibrium points. The distribution of the equilibrium points in the state space is discussed. d) An energy function E for system (S) is specified. We show that there is a one-to-one correspondence between the set of local minimum points of E and the set of asymptotically stable equilibrium points of ( S ).</p><p>e) We also show that along each solution of (S ), the energy function E decreases monotonically, and system ( S ) will not exhibit periodic solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesis:</head><p>We define the set B" as B" = {x E R": x, = 1 or -1, i = 1, * , n 1. We will establish results along the following lines Given in vectors in B " , say aI, . . . , a,,,, we wish to design a system ( S ) such that 1) a i , . * . , a,,, are asymptotically stable equilibrium points of system ( S ).</p><p>2 ) The system has no periodic solutions.</p><p>3) The total number of asymptotically stable equilibrium , a,} is as small as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>The domain of attraction of each a, is as large as possible.</p><p>The synthesis procedure developed in [6]- <ref type="bibr" target="#b6">[8]</ref> is modified for system (S ). In doing so, we consider a class of LSSM systems given by points of ( S ) in the set B" = { al, * possible.</p><formula xml:id="formula_0">x ( k + 1) = sat(T,x(k) + IT), k = 0, 1, 2 , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( S T )</head><p>where x E D " , T, = r l T + -r 2 T -, r 1 and r2 are real parameters, and T + and Tare real symmetric matrices. The precise meaning of r l , r 2 , T ' , Tand I , will be made clear later. Let 15, be the affine subspace of R" generated by { a I , * . * , a,,, }, which contains { a,, . . . , CY,, } . With a proper choice of parameters T~ and r 2 , we show that a) System (S,) does not exhibit periodic solutions.</p><p>b) The set of asymptotically stable equilibrium points contained in B" is approximately equal to B" fl L,. In other words, an asymptotically stable equilibrium point having components _+ 1 is most likely contained in an affine subspace generated by</p><p>The above synthesis method is also generalized to system ( S ) when the given data set (i.e., the desired set of asymptotically stable equilibrium points) is contained in</p><formula xml:id="formula_1">A = { x E D": x, = 1, -1, or 0, i = 1, . . . , n } instead of B".</formula><p>The remainder of this paper is organized in an unusual manner in order to make the material more accessible to the reader. A summary of the synthesis procedure and specific examples designed by this synthesis procedure are presented in Section 11. These examples indicate that the method advanced herein is more effective than existing design techniques for discrete-time neural networks. Further comparisons of the present design method with existing results are made in the concluding section, Section 111.</p><p>The detailed mathematics, which justify the assertions made in Sections 1-111 and which constitute the main contribution to the paper, is presented in the Appendix. In Appendix I, we restate the problem on hand. In Appendix 11, we establish the necessary notation. In Appendix 111, we state and prove the results summarized above under the category Analysis while in Appendix IV, we establish the results and procedure summarized above under the category Synthesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">SYNTHESIS PROCEDURE A N D EXAMPLES</head><p>In this section, we first provide a statement of the synthesis procedure developed herein. Next, we consider several specific examples to demonstrate the applicability of this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Synthesis Procedure</head><p>, a, in B " , which Suppose we are given m vectors aI, . . . are to be stored as asymptotically stable equilibrium points for an n dimensional system ( S ). We proceed as follows. 2) Perform a singular value decomposition of Y and obtain the matrices U , V , and C such that Y = UC V T , where U and V are unitary matrices and where C is a diagonal matrix with the singular values of Yon its diagonal. (This can be accomplished by standard computer routines [15]). Let</p><formula xml:id="formula_2">y = [ Y l , . . . , Y,-ll, U = [Ill, . . * 1 and k = dimension of Span ( y , , . . . , Y , ~-I ).</formula><p>From the properties of singular value decomposition, we know that k = rank of E, { U,, . . . , u k } is an orthonormal basis of Span( yI, . . , ym-I ) and { U,, * . . , U,,} is an orthonormal basis of R".</p><p>3) Compute In the Appendix, we show that all vectors in L, fl B", where L, = Aspan(a,, . . . , a,), including a l , * . . , a,, will be stored as asymptotically stable equilibrium points in the LSSM system I, = T~CY, -T,a,,,.</p><formula xml:id="formula_3">k T + = [ T f ] = c u;uT, i = l and n T -= [ T -] = , c u,<label>uf.</label></formula><p>x ( k + 1) = sat(T,x(k) + I,), k = 0, 1, 2 , . . . . (S,)</p><p>5) With T~ fixed, choose the parameter r2 as large as possible. In the Appendix, we show that the set of the asymptotically stable equilibrium points contained in B" is approximately equal IO L, n BIZ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Specific Examples</head><p>We now design several specific neural networks by the above synthesis procedure and investigate the effects of the parameters 7, and r2 on the performance of system ( S , ) by means of simulations of these networks. In the study of these examples, we consider speed of convergence and the ability of the networks to recover desired patterns from inputs subjected to Gaussian and binary noise. Also, we compare the performance of networks designed by the present method with the performance of networks designed by the methods developed in [SI and <ref type="bibr" target="#b7">[9]</ref>. We conclude the present section with the design of a neural network for pattern recognition. The objective here is to synthesize a neural network which can recognize 27 key patterns (the 26 upper case letters in the alphabet plus a blank), each pattern being represented by a 9 x 9 pixel array.</p><p>Example 2.1: In this example, we illustrate the application of the synthesis procedure given above. The dimension of the system to be considered is n = 10. Given are rn = 6 vectors 1) Compute the n x (rn -1 ) = 10 X 5 matrix Y = [ y I , y r , y 3 , y,, y,] where specified by</p><formula xml:id="formula_4">a1 = a, = -- -1 1 -1 1 1 1 -1 1 1 . 1- -- 1 1 -1 1 -1 1 -1 1 1 1 -- a&gt; = as = . - 1 1 -1 -1 1 -1 1 -1 1 1 -- -1 -1 -1 1 1 1 -1 -1 -1 -- . - -1 1 1 1 -1 -1 1 -1 1 1 -1 -- -- -1 -1 -1 1 1 -1 1 1 -1 1 . - Y4 = -- 2 2 0 0 -2 2 -2 0 2 -0- &gt; Y2 = 3 y5 = -- 2 2 0 -2 0 0 0 -2 2 0 2 0 0 -2 0 2 0 -2 0 -- -- --2- Y3 = _ - 0 2 2 0 -2 0 0 -2 2 -2 --</formula><p>and wherey, = ai -ah for 1 5 i 5 5.</p><p>2) Perform a singular value decomposition of Y to obtain the matrices U , V , and C such that Y = U C V T . We have k = rank of C = 5.</p><p>3) Compute</p><formula xml:id="formula_5">T' = [ T i ] = U + ( U ' ) T = + . . . + U ~U :</formula><p>It is desired that these vectors be asymptotically stable equilibrium points of the LSSM system ( ST).</p><formula xml:id="formula_6">T -= [T,;] = U -( u -) ' = ugu; + . . . + U l 0 U f O .</formula><p>4) Choose the parameters T~ = 1.5, 72 = 0.5 and compute -Also, by Theorem 3.1, we determine that system (S,) has 158 unstable equilibrium points.</p><p>Example 2 . 2 : In order to ascertain how typical the results of Example 2.1 are, we evaluated the number of spurious states found for system (S,) by varying the parameters 71 and r2. In Fig. <ref type="figure" target="#fig_3">1</ref>, we fixed r , as 1.05, 1.5, and 10.5 ( r l &gt; l ) , and we determined the number of spurious states found in the system (S,) by varying the parameter r2 from 0.0 to 1. We were able to confirm Theorem 4.1, given in Appendix IV: as r2 increases, the number of spurious states decreases. In Fig. <ref type="figure" target="#fig_23">2</ref>, we fixed r2 as 0.05, 0.5, and 0.95 ( I r2 1 &lt; 1). We observe that curve 3</p><p>(7* = 0.95) reveals the best average performance with a minimum number of spurious states and the parameters ( r1 = 1.5, r2 = 0.95) yield the optimum solution under the given desired patterns (with n and m specified) to be stored. Fig. <ref type="figure">3</ref> provides details for the better cases.</p><p>Remark 2.1: Fig. <ref type="figure" target="#fig_3">1</ref> shows that, by using the present method, the a , , * * , a6 are always stored as stable output vectors. Also, as 72 increases ( -1 &lt; r2 &lt; 1 ), the number of spurious stable output vectors decreases.</p><p>Example 2.3: The same patterns as the ones considered in Example 1 were stored as asymptotically stable equilibrium points to determine a time index, which we define here as the convergence time (to an equilibrium) from a noisy input. A total of 10 000 random input vectors were generated as inputs for the network considered in <ref type="bibr" target="#b6">[8]</ref>, for the network treated in <ref type="bibr" target="#b7">[9]</ref>, and for network (S,) considered in the present paper. </p><formula xml:id="formula_7">x ( k + 1) = s a t ( T , x ( k ) + IT), k = 0, 1, 2, . . . . (ST)</formula><p>In accordance with Theorem 3.1, given in Appendix 111, system ( S,) has additional asymptotically stable equilibrium points in D" -{ a l , * . . . a6} given by . suggest that the present results are more efficient than the ones developed in <ref type="bibr" target="#b6">[8]</ref> and [91.</p><formula xml:id="formula_8">- -1 1 0 0 1 1 0 -1 1 -1 . - 1 1 . - 1 1 0 0 -1 1 0 -1 1 -1 , - -- -1 1 0 0 1 -1 1 -1 1 0 -- , - 1 1 0 0 -1 -1 1 -1 1 0 -- -1 1 0.87 1 -1 1 -0.30 -0.70 1 -1 . - -1 1 -0.87 1 1 -1 0.30 0.70 1 1 -- ai2 = a; = a; = CY; = CY; , CY; = , a; = a; = a;, = . - 1 -1 -1 1 -1 1 0 1 -1 0 -- 1 -1 -1 -0.49 1 -1 1 0.06 -1 1 -- --1- -1 -1 1 1 1 0 I -1 0 -- -- 1 1 -1 -1 1 1 0 -1 1 0 -- -1 -1 0.6 1 -1 -1 1 -0.2 -1 -1 -0.</formula><p>Example 2.4: A ten-neuron system ( n = 10) has 2" = 1024 possible binary inputs. For each design considered, each of the possible binary inputs was applied as an initial condition for the network. The network was allowed to evolve from its initial condition to a final state. We interpret this final state as the network's binary response to the given initial condition. The Hamming distance from the initial condition to the final binary response was used as a basis for estimating the basins of attraction of the stored patterns. Specifically, we wish to determine how the network's performance is affected by the number of patterns m to be stored. For each of the m designs (taken to be even between 2 and n = l o ) , 1024 trial runs were made for the network from <ref type="bibr" target="#b6">[8]</ref>, for the network from <ref type="bibr" target="#b7">[9]</ref>, and for (&amp;). The data in each entry shown in Tables 11-IV should be interpreted as the number of successes versus the number of failures (e.g., in Table <ref type="table">11</ref>, for the network from [SI, the data shown in the fourth entry of row one are 110/10. This indicates that for a specified desired input pattern, there are a total of 120 possible initial conditions with Hamming distance 3 to be applied to the network, and the results show that 110 of them are successful trials while IO trials resulted in failures.)</p><p>Example 2.5: In this example, we design an 81-neuron network. The given set of library vectors corresponds to the 26 upper case letters and a blank pattern, i. we employed in the simulations are Gaussian noise and binary noise (noise which randomly flips binary bits). For the former, the noisy input pattern is generated by adding zero-mean Gaussian noise with a given specific standard deviation (SD) to the key pattern (in Fig. <ref type="figure" target="#fig_23">2</ref>, SD = 1.15). For the latter, the noisy input pattern is generated by randomly flipping a certain number of binary bits of the key pattern to produce an input pattern at the desired Hamming distance (HD) from the desired output vector (in Fig. <ref type="figure">3</ref>, H D = 19). Typical time evolutions of the neural network are shown in Figs. <ref type="figure" target="#fig_23">2</ref> and<ref type="figure">3</ref> for Gaussian noise and binary noise, respectively. In each figure, the plot in the left upper comer represents the desired output pattern and the next pattern to the right is the noisy input pattern. The rest of the plots provide the time evolution results (the caption on the bottom of each plot indicates the current value of the time for each of the successive plots). The final result (i.e., the pattern to which the network converged) is shown in the plot of the last row furthest to the right.</p><p>Remark 2.2: Figs. 2 and 3 use small dots and large circles to represent the outputs of neurons with values of -1 and 1, respectively. Output values between these two extremes are represented by circles whose size is scaled between the large circles and the small dots.</p><p>Six different sets of simulation runs were made, three for Gaussian noise with different standard deviations and three for binary noise with different Hamming distances. For each case, we tested all of the key patterns twice (i.e., 2 X 27 = 54 trials). The results are summarized in Table <ref type="table" target="#tab_5">V</ref>.</p><p>-r --T     We conclude with a few observations concerning the strengths and weaknesses of several related existing results. This will enable us to point to the principal advantages and deficiencies of the method advanced herein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Relation of Present Results to Existing Works</head><p>1) The outer product method [ 11-[3] does not guarantee that a network designed by this method will necessarily store desired vectors as equilibrium points of the network. Furthermore, experience with numerous simulations has shown that networks designed by the outer product method can store effectively only up to 0 . 1 5 vectors as equilibria where n denotes the order of the network [12], <ref type="bibr" target="#b10">[13]</ref>.</p><p>2) The projection learning rule [4], [SI has two advantages over the outer product method. First, networks designed by the projection learning rule are capable of storing up to n equilibrium points. Secondly, this technique guarantees that a network designed by this method will always store a given vector as an equilibrium point. However, this equilibrium point need not be asymptotically stable.</p><p>3) The eigenstructure method, developed in  4) Networks designed by the outer product method, projection learning rule, and eigenstructure method are globally stable (i.e., all trajectories converge to some equilibrium point) and they have a symmetric interconnecting structure.</p><p>The design method developed in 191, [ 131, [ 141 yields neural networks with interconnecting structures that do not require symmetry; it yields networks that guarantee to store all desired patterns as asymptotically stable equilibrium points; and, as in the case of the projection learning rule, it can store up to n asymptotically stable equilibrium points. However, this design method does not guarantee global stability.</p><p>5) The eigenstructure method appears to offer several advantages over other existing design techniques of neural networks for associative memories. However, in their present form, the results in [6]-[ 81 are applicable only to continuous-time networks.</p><p>The present results, which extend the eigenstructure method to discrete-time neural networks, provide a basis for a) the implementation of neural networks by means of serial processors, and b) the implementation of neural networks by means of special digital hardware.</p><p>In particular, in view of a), the present results enable us to generate efficient digital simulations of the continuous-time neural networks designed by the results in [6]- <ref type="bibr" target="#b6">[8]</ref>. (Previously, this required lengthy simulations of high-order systems of ordinary differential equations representing the neural networks obtained by the results in [6]- <ref type="bibr" target="#b6">[8]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Summary</head><p>Summarizing, in this paper we first presented a qualitative analysis for a class of discrete-time neural networks defined on hypercubes. Next, we utilized these analysis results to establish a design procedure for the class of neural networks considered herein. To demonstrate the storage ability of our results, we presented several specific examples.</p><p>The present design procedure has the same desirable features as our earlier results [6]- <ref type="bibr" target="#b6">[8]</ref> for continuous-time neural networks: i) for a given system dimension, networks designed by the present method may have the ability to store more patterns (as asymptotically stable equilibria) than corresponding networks designed by other techniques [1]-[51, 191, [131, 1141 (see Remark 4.4 in Appendix IV and discussion in <ref type="bibr" target="#b6">[8]</ref>); ii) the present design method guarantees to store all of the desired patterns as asymptotically stable equilibrium points (see Theorems 3.1, 3.2, 3.3, and Remarks 3.4, 3.5 in Appendix 111); iii) the present method provides guidelines for reducing the number of spurious states and for estimating the extent of the domains of attraction for the stored patterns (see Remark 3.4.2 in Appendix I11 and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">--l -1'</head><p>Remark 4.1 and Lemma 4.2 in Appendix IV). In addition, the present results provide a means of implementing neural networks (designed by the eigenstructure method) by serial processors and digital hardware. In particular, the present results provide digital simulations of neural networks with vastly improved convergence speeds (when compared to the digital simulations for the differential equations that described the neural networks given in [6]-[81).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I PROBLEM STATEMENT AND SUMMARY OF RESULTS</head><p>We consider neural networks described by a system of firstorder difference equations</p><formula xml:id="formula_9">x ( k + 1 ) = sat(Tx(k) + I ) , k = 0, 1 , 2, . . . . ( S )</formula><p>where the symbols in (S ) are as defined in Section I.</p><p>In Appendix 111, we make precise the concepts of solution and equilibrium for (S ); we develop algorithms to determine the location, number, and stability properties of the equilibria of (S ); and we show that (S ) will not exhibit periodic solutions by making use of an energy-like function having certain properties.</p><p>In Appendix IV, we utilize the results established in Appendix 111 to develop a synthesis procedure for system (S ) having the following properties: the given vectors a I , * -* , CY, in B" are asymptotically stable equilibrium points of (S ); the system (S ) has no periodic solutions; the total number of spurious states (i.e., the total number of asymptotically stable equilibrium points of ( S ) in the set B"-{ CY,, . . -, a,,, 1 ) is as small as possible; the domain of attraction of each a , is as large as possible; and the set of asymptotically stable equilibrium points contained in B" is approximately equal to B" fl L,. In amving at the synthesis procedure, we make use of the parameterized system In amving at the results enumerated above, we will make use of the notation given in Appendix 11.</p><formula xml:id="formula_10">x ( k + 1) = sat(T,x(k) + IT), k = 0, 1, 2 , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I1</head><p>NOTATION Let V and W be arbitrary sets. Then V U W , V fl W , V -W , and V X W denote the union, intersection, difference, and Cartesian product of V and W , respectively. If V is a subset of W , we write V C Wand if x is an element of V , we write x E V. Iff is a function from V into W , we write f: V + W and we l e t f (</p><formula xml:id="formula_11">U ) = { f ( x ) E W: x E U } for U C V , a n d f -' ( y ) = { x E V : f ( x ) = y } for y E W .</formula><p>Let 0 denote the empty set, let R denote the set of real numbers, let R + = [ 0, +CO), and let Z + = { 0, 1, 2, . . . ] . If VI, . . . , V, are arbitrary sets, their Cartesian product is denoted by II:= l V, = V , x * * . x V,. If, in particular, V = V , = . . . = V,,, we write II;= I VI = V". Let R" be real n-space. If x E R", then x T = (xi, . . . , x,!) denotes the transpose of x. When using a norm for x E R", I x 1, we will have in mind I x 1 = a. If x E R" and Y C R", then x I Y will mean that x'y = 0 for all y E Y. If V C R", then v, V o , and aV represent the closure, interior, and boundary of V in R", respectively. Also, we let B ( x , r ) = { y E R": 1 xy 1 &lt; r } f o r x ~R a n d r &gt; O . L e t B " = { x ~R " : x , = l o r -l , i r -r ----= 1, . , n ) and D" = { x E R " : -1 I x, I 1, i = 1, min ( x ) = min { x , : i = 1, . , n } . IfA = [A,,] is an arbitrary matrix, then AT denotes the transpose of A and the norm of A is defined as 1 A I = sup, I { 1 Ax I }. If A is a symmetric matrix, by A &gt; 0 we mean that A is positive definite and by A L 0 we mean that A is positive semidefinite. Sym ( n ) denotes the symmetric group of order n. If {x,, . . . , x, } C R", then Span ( X I , * . . , x,") denotes the subspace of R" generated by x I , . . . , x, and Aspan (xl, , x,) denotes the affine subspace of R" generated by x l , . . . , x,. If xo E R" and L is a subspace of R", then L + x, denotes the affine subspace of R" produced by shifting L by xo, that is <ref type="figure">,</ref><ref type="figure">L ( X ,</ref><ref type="figure">y</ref> ) denotes the angle between x a n d y , i . e . , <ref type="figure">~( x ,</ref><ref type="figure">y</ref>   <ref type="figure">,</ref><ref type="figure">l,</ref><ref type="figure">l)T E A: t r ( ,</ref><ref type="figure">,</ref><ref type="figure">=</ref>  The number of elements in An, is equal to</p><formula xml:id="formula_12">L + xo = { y E R": y = x + xo, x E L } . Finally, if x, y E R"</formula><formula xml:id="formula_13">n! /m!(n -m)! 2" -#',</formula><p>and the number of elements in A = U A,, 0 5 m 5 n , is 3".</p><p>For each 4 E A, let</p><formula xml:id="formula_14">C ( 4 ) = { x = (x,, * . * , x , , ) ~E R": \ x i \ &lt; 1 if 4, = 0, andx, = 4 , if 4, # 0 ) . ( 3 . 1 ~)</formula><p>From the notation given above, we have the following results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3. I :</head><p>I ) A,, = B " and C(5) = { t } , for any 5 E Ao. Another choice of element in A2 is case, Remark 3. I :</p><formula xml:id="formula_15">2) A , , = { O } a n d C ( 0 ) = ( D " ) o = { x ~R " : -l &lt; x i &lt; 1, 3 ) a ( D " ) = U C([), 4 E U A,, 0 5 m &lt; n. 4) D" = U C ( 0 , 4 E U A,</formula><formula xml:id="formula_16">= ( I , 0 , -1, 0 ) ' . In this C ( ( ) = { X E R 4 : x 1 = I , Ix21 &lt; I , x , = -1, lx41 &lt; l } and p ( 1 ) = 2, p ( 2 ) = 4 , p ( 3 ) = 1, p ( 4 ) = 3</formula><p>1) For a given 5 E A,,,, there may exist different permutations in Sym ( n ) for which (3.2) is true. For these different permutations, the notation given above will be the same up to different orders in components. Thus, the analysis and conclusions will be identical for any of the permutations used.</p><p>2) If rn = n , we have T I , , = T , I, = I , El = t and TI,,,, T,,,,, Ill, Ell do not exist. If rn = 0, we have T,,,,, = T, I,, = I, Ill = t and TI,,, TI,.,, 4, 4, do not exist.</p><p>Definition 3.1: <ref type="figure"></ref>and<ref type="figure">(,(,,</ref> = 1 or -1, rn &lt; i I n .</p><formula xml:id="formula_17">1) Consider E A,,, 0 &lt; rn I n , with p E Sym ( n ) , such that l r c r ) = 0, 1 5 i I rn,</formula><p>Also, consider the linear difference system defined by . . .</p><formula xml:id="formula_18">' ~1 1 = ( V p ( m + I ) ,</formula><p>In particular, if .</p><p>$ E A,,,, rn &lt; n , the solution cp is said to be in a saturated mode.</p><p>Remark 3.2: <ref type="figure">,</ref><ref type="figure">(,</ref><ref type="figure">,</ref><ref type="figure">(k</ref></p><formula xml:id="formula_19">1) ( o ( { O , 1, . . . , k , } ) c C ( [ ) implies that -1 &lt; cp,</formula><formula xml:id="formula_20">) &lt; 1, i = 1, 1 . . , m, k E ( 0 , 1, * * . , kn}, P W i , ) ( k ) = t &amp; ( , ) , i = m + 1, . . . , n, k E (0, 1, . . . , kn), i.e., cpl,(k) = 5n when k E ( 0 , 1, . . -, kn}</formula><p>2) When rn = n , A, = { 0 ) . In this case, (3.4b) has no meaning and we only need to consider (3.4a). Furthermore, the (local) solutions in C(0) = (0")' = { x E R": -1 &lt; x, &lt; 1, i = 1, * . . , n } defined above for the LSSM system ( S ) will be identical to the usual solutions defined on the open subset C ( 0 ) for the linear system ( S ).</p><p>3) When rn = 0, An = B" and for 4 E An, C ( 5 ) = { t } . In this case, (3.4a) does not exist and we only need to consider (3.4b). Furthermore, the only function with the range in C ( t ) </p><formula xml:id="formula_21">is</formula><formula xml:id="formula_22">I, E Z', k, E Z+, Io = 0 such that E, n E, = 0 when i # j ,</formula><p>and U E, = Z i , and (o ( k ) restricted to each E, is a (local) solution as defined in Definition 3. l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dejinition 3.3:</head><p>1) A vector x, E D" is said to be an equilibrium of system ( S ) if the function (a = cp ( e , x,): Z + + D" defined by cp ( k , x,) = x, is a solution of (S ).</p><p>2) Let x, be an equilibrium point of system ( S ), i) x, is said to be stable if for every E &gt; 0, there is a 6 = 6 ( ~) &gt; O s u c h t h a t ( ( o ( k , x ) -x e l &lt; ~, f o r a l l k ~Z + w h e never ( x -x , I &lt; 6 ii) x, is said to be asymptotically stable if it is stable and if there is an 17 &gt; 0 such that lim, + I cp ( k , x )x, I = 0 whenever 1 xx, 1 &lt; 1.</p><p>iii) x, is said to be unstable if it is not stable. We will have occasion to make use of the following assumptions for system (S ).</p><p>Assumption 3. I: 1) T i s symmetric.</p><p>2) For any m, 0 I m 5 n , and for any E A,,,, the m x m matrix (E,,, -TI,l) = ( E , -T f i ( , ) , , l , l ) , 1 5 i, j 5 m is nonsingular, where E, is the m X m identity matrix, and p E Sym ( n ) , such that t f i l i ) = 0, 1 5 i 5 m , and E,,(!) = + I , m &lt; i 5 n .</p><p>For system ( S ) satisfying Assumption 3.1, we introduce the following notation. . . . all vectors I in R".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. For</head><p>3.1 and 3.2. Then, for any 4 E A,,,, 0 I m &lt; n , re # 1 . E A,,,, 0 &lt; m &lt; n , rE = 1. Then from the notation given in (3.5), there is j , 1 c: j 5 n m, such that the j t h component of the vector ( T l l , l x ~l + TIl,lI~II + 111) is equal to +1. Without loss of the generality, assume that j = 1. Take { E A,,,+l where tgCj = 0 for 1 5 i s m + 1 a n d 3 ; ( , , = ( , ( i , f o r m + 1 &lt; i 5 n . T h e n x E = x r E aC( E ) and this contradicts Assumption 3.2. Suppose that, for some E Ao, rE = 1. Then we can arrive at a contradiction in the same manner. Theorem 3.1: Suppose that system ( S ) satisfies Assumptions 3.1 and 3.2. Then for any m, 0 5 m I n , and for any . $ E Am, with the notation given above, we have the following results:</p><p>Proof: Suppose that for some </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pro05</head><p>Case I: In this case, = 0 and C([) = (D")', and from Definition 3.2, solutions of (S ) on C( 0 ) are identical to solutions of ( S ) on C ( 0 ) in the usual sense. Then the conclusions of Case I of the theorem follow directly from the theory of linear difference equations.</p><p>Case II: Consider 5 E A,, 0 &lt; m &lt; n , and a constant func- Therefore, if rs &gt; 0 and x E E C ( t ) , then xE is the unique equilibrium point of system ( S ) in C( 0. If rE &lt; 0, then clearly there is no equilibrium point of system ( S ) in C ( f ) . Also, if rt &gt; 0 and xs @ C( 0, then it is clear that there is no equilibrium point of system (S ) in C( ).</p><formula xml:id="formula_23">tion cp: Z f + C( E), cp(k) = f . If f is an equilibrium point of ( S ) in C ( E ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now suppose that xE is an equilibrium point of ( S ).</head><p>There is an open neighborhood U of x E in R" such that min ( ( <ref type="figure">T,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">x,</ref><ref type="figure">+  T,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">~,</ref><ref type="figure">,</ref><ref type="figure">+ I,</ref><ref type="figure">,</ref>) * Err) &gt; 1 for all x E U n D". If the eigenvalues of TI,, satisfy 1 X, 1 &lt; 1, i = 1, . . * , m, then it can be proved by induction and by the theory of linear difference equations that all solutions of ( S ) starting in U f l D" will monotonically converge to xE as k .+ 03. Thus, xE is an asymptotically stable equilibrium point of ( S ). On the other hand, if there is at least one i such that I A , 1 2 I , there is a h0 &gt; 0 such that B(xs, 6,) fl D" C U fl D " , and for any E , 0 &lt; E &lt; 6,, there is a function , and pl is a solution of the linear system (SE). Since (a ( { 0 , * * , k, } ) C U , pII satisfies (3.4b) and (o is a solution of system ( S ) . Thus, xF is an unstable equilibrium point of ( S ) by Definition 3.3. Finally, for the purpose of contradiction, assume that xE is an asymptotically stable equilibrium point of (S ) and assume that x( is another equilibrium point of ( S ) in m. -This concludes the proof of Theorem 3. I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3 . 4 :</head><p>1) Theorem 3.1 establishes an algorithm to locate all equilibrium points of ( S ) and to determine the stability properties of all equilibrium points.</p><p>2 ) In practice, due to noise, unstable equilibrium points can not be used as memory locations. However, by investigating the locations of the unstable equilibrium points of (S ), the domain of attraction of each asymptotically stable equilibrium point of ( S ) can be approximately determined.</p><p>3) If for some , $ E A, the corresponding TI,, is singular, Theorem 3.1 can still be applied to other vectors in A . If Lemma 3.2 is not true, we need to consider the case rF = 1. In fact, we can study this case using the same method as was employed in Theorem 3.1. In this case, the conclusions might be less straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 . 2 : If system ( S ) satisfies Assumption 3.1 and</head><p>1) There is at most one equilibrium point of (S ) in each re-2) There are at most 3" equilibrium points for system ( S ).</p><p>3) If for 4 E A the corresponding xt is an asymptotically stable equilibrium point for system (S ), then i</p><p>) xt is the unique equilibrium point of system ( S ) in ii) x F is the unique asymptotically stable equilibrium point</p><formula xml:id="formula_24">of system ( S ) in U C( {), where { E A such that xi E C( {).</formula><p>In particular, if 4 E A, = B" and xF = , $ is an asymptotically stable equilibrium point of system ( S ), then x F = , $ is the unique asymptotically stable equilibrium point of system ( S ) in the region {x E D " : -1 &lt; x,,$, I 1, I 5 i 5 n } .</p><p>4) There are at most 2" asymptotically stable equilibrium points for system ( S ). Remark 3 . 5 : Theorem 3.2 is not only very useful to simplify the algorithm given in Theorem 3.1 for checking the location of equilibrium points of system ( S ), but it is also useful to determine whether a given set of vectors can be synthesized as a set of asymptotically stable equilibrium points of system ( S ) or not (cf. synthesis procedure). Definition 3 . 4 : If T is symmetric, then 1) the energy function E: D" -+ R of system ( S ) is defined by</p><formula xml:id="formula_25">E ( x ) = -x T ( T -E,,)x -2 x T f ,</formula><p>where E, is an identity matrix 2 ) each nonequilibrium solution of ( S ) converges to an equilibrium point of ( S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>1) It is sufficient to consider the proof for local solutions of ( S ) as defined in Definition 3.1. Consider a nonconstant local solution of ( S ) given by (o: { I , , * . , I, + k , } -+ C ( 0 , , $ E A,,,. With the notation given above, we have, for k = I,,</p><formula xml:id="formula_26">, I, + k , , E ( k ) = --cp(kIT(T -E,&gt;) ~( k ) -2 ~( k ) ~I . . . = -q+qT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T(o(k) + I p ( k ) T p ( k ) -2p(k)%</head><p>Since T is symmetric, we can find an orthonormal matrix P such that T = P T Q P , where P T = P -' , Q is a diagonalized matrix, i.e., Q = diag { X I , . . . , X ,, } . Hence, we have of ordinary difference equations.</p><p>Remark 3.6: 1) Theorem 3.3 shows that system ( S ) has no oscillatory solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">)</head><p>In practice, due to noise, all solutions of system ( S ) will only converge to asymptotically stable equilibrium points of system (S ).</p><p>Theorem 3.4: Suppose that system (S ) satisfies all assumptions of Theorem 3.1 and 3.3. Then there is a one-to-one correspondence between the set of local minima of the energy function E and the set of asymptotically stable equilibrium points of system (S ).</p><p>Proofi Suppose that f is an asymptotically stable equilibrium point of system ( S ). Then there is an open neighborhood U of f in R" such that all solutions of ( S ) starting in U n D" f is an equilibrium point of system ( S ) . If f is not asymptotically stable, then by assumption, it is unstable. Thus, there is a 6, &gt; 0 such that B ( f , 6, ) C U and for any E , 0 &lt; E &lt; 6, , there is a solution p of ( S ) starting in B ( f , E ) fl D" which tends towards the exterior of B ( f , 6,) n D". Since along solution p, the energy function E decreases monotonically, we have that E ( f ) &gt; E ( x , ) for some point x , E ( a B ( f , 6 , ) ) n D" C U fl D". This contradiction shows that f is an asymptotically stable equilibrium point of system (S ).</p><p>Remark 3.7: Without A , &gt; -1, i = 1, . * , n, we can not guarantee the global stability of system ( S ) . As an example, let us consider the following system  His research interests are in the areas of dig-ita1 filtering, image processing, and neural networks.</p><p>Mr. Yen is a member of Sigma Xi.</p><p>--l -.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received March 23, 1990; revised July 20, 1990. This paper was supported in part by the National Science Foundation under Grant ECS The authors are with the Department of Electrical Engineering, Univer-IEEE Log Number 9039133. 88-02924. sity of Notre Dame, Notre Dame, IN 46556.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x , ( k + 1 ) = sat( , = I5 T , p , ( k )+ I , ) , k = 0, 1, 2, . . * , n i = 1, . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 )</head><label>1</label><figDesc>Compute the n x ( m -1 ) matrix Y = [ a 1a,,, . . * 3 %?,-Ia,,].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>" r = k + l 4 )</head><label>4</label><figDesc>Choose T~ and r2 with 71 &gt; 1 and r2 sufficiently large, I r2 I &lt; 1, and compute T, = TIT+ -T ~T -, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TaulFig. 1 .</head><label>1</label><figDesc>Fig. 1 . Effects of parameters 7 , and 72 on the number of spurious states for Example 2 . 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>9 Fig. 2 . 12 Fig. 3 .</head><label>92123</label><figDesc>Fig. 2 . Recovery of a key pattern from data corrupted by Gaussian noise (Example 2.5).Binary noise with 19 Hamming distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>H D = O H D = 1 H D = 2 H D = 3 H D = 4 H D = 5 H D = 6 H D = 7 H D = 8 H D = 9 H D = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>O H D = 1 H D = 2 H D = 3 H D = 4 H D = 5 H D = 6 H D = 7 H D = 8 H D = 9 H D = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>[6]-[ 8 ]</head><label>8</label><figDesc>for continuous-time models, has two advantages over the projection learning rule. First, networks designed by the eigenstructure method are capable of storing equilibrium points, which by far -r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(S,) with T, = r l T --r 2 T -. The parameters r1 and r2 and the matrices 7 ' ' and T -, as well as all of the other symbols in ( S,), are defined in Sections I and 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) = a r c c o s x T -y / ( x ( ( y l . . . . , n } . If x, y E R " , let x * y = ( x , y , , . . . , x,Y,)' and APPENDIX I11 ANALYSIS Before giving a proper definition of the solution for system ( S ), we need to introduce some additional concepts. For each integer m, 0 I m 5 n, let A, = { 4 = ( t l r . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>0 , 1 I i 5 m and C;r(ll = f l , m &lt; i I n, for some p E Sym ( n ) } , (3. l a ) where ( 3 . l b )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>7 .</head><label>7</label><figDesc>,,, 0 5 m I n. 5 ) For any 4 , 17 E 4 4 f 7 , C ( 0 fl C ( v ) = 0. Suppose that 4 E A,,, and p E Sym ( n ) such that, i = 1, . . . , n } . C;p(I, = 0 , 1 I i 5 m and t P ( , , = f l , m &lt; i 5 n. (3.2) Subsequently, we will make use of the notation TI,,, = [Tp(,,p(,,,], f o r m &lt; i I n , 1 I j 5 m (3.3a) and 1, = (I,,,), * . . 3 ~, i , d T ~ I,, = ( ~, O , ? l l ) &gt; . . . . I~~,,,)'., t I = ( t , , , , ? . . . 3 t,w,))7&gt; tII = ( t p ( J n + l ) &gt; . . . &gt; t P ( d Let n = 4 and rn = 2. One choice of element in A2 is E = ( 0 , 0, I , 1)'and C([) = { x € R 4 : lxII &lt; I , Ix2( &lt; 1,x3 = 1,x4 = l } and p ( I ) = I , p ( 2 ) = 2, p ( 3 ) = 3, p ( 4 ) = 4. In this case, we have and tl = ((1, I d T t I I = ( t 3 . 'U7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>x , ( k + 1) = Tl,Ixl(k) + Tl.ll~ll + 1,(S,)    where 3 xpc,,JT and . . . XI = (X,iI), . * . 3 .X,i,,JT&gt; XI, = (x,(,,lll)3 -1 &lt; xu(,, &lt; I , for 1 I i I rn. IEEE TRANSACTIONS ON NEURAL NETWORKS. VOL. 2. NO. I. JANUARY 1991System (S, ) is said to be the reduced linear system of system (S ) over the region C ( 4 ) .2) For any t E A,, a function cp: { 0, I , * . . k , } --t C ( t ) , is said to be a (local) solution of system ( S ) if the vector function pr containing the p ( i )-th components of cp, 1 I i I m, is a solution of the linear system ( S E ) , i.e., cpi(k + 1) = Ti,icpi(k) + ~i . i i t i i + 11, k E CO, 1, . . . kn} (3.4a) and min ((T,l,lcm(k) + T L l l t I l + 111) * tu) 2 1, k E ( 0 , 1, * * * , k o } (3.4b) where PI = (cpp(l), . . . 7 ~v ( m ) ) ~ and 3 P r ( n J T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>the constant function cp(k) = [. Dejinition 3.2: For x,, E D", a function cp = cp( ., no): Z t + D " is said to be a solution of (S ) starting at xg if a) ~( 0 , xo) = xn, and b) there are countably many sets E, = { I , , . . . , I, + k , } ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>[ = 0 E A,, let xE = ~x , + I E R" or xE = (E,, -T ) -' I (3.5a) E A,,, 0 &lt; m &lt; n and with TI,,, . . * 7 ~, I , l l ~ 4 , Ill where E, denotes the n X n identity matrix. defined as in (3.4), let 11. For xs = (xcl, * * . , x ~, ~) ' Xefi(,,&gt; . . . 3 x t f i ( m 1 ) ' = (E"Zh-7 6 , I I E I l + 11) XEll = (XEfi("2+I)? . . . &gt; Xs,(,,,)T = Ell and let E R , = min ((TILIXEI + TII.IIFII + 111) * 611). ( 3 . 5 ~) 111. F o r ( E A , = B " , C ( E ) = { l } , l e t XE = E (3.5d) and rs E R, rt = min ((TE + I ) * E ) .(3.5e) Assumption 3.2: Given Assumption 3. I , assume that for 4 E A,,,, 0 5 m &lt; n , x s @ a C ( t ) , i . e . , ~~~( ~) # k l , f o r i = 1, Remark 3.3: For fixed T , Assumption 3.2 is true for almost Lemma 3.2: Suppose that system ( S ) satisfies Assumptions , m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Case I: m = n , [ = 0 E A,,. 1) If xs @, C( 5 ) = (On)', there is no equilibrium point of 2) If xE E C( E) = (D")', xe is the unique equilibrium point system ( s ) in (D")'. of system ( S in ( D " )'. In particular, i) If the eigenvalues of T = satisfy I A, I &lt; 1, i = 1, , n , then xE is asymptotically s t a x a n d there are no other ii) If there is at least one i such that I A , 1 2 1, then xt is . . . equilibrium points of system (S ) in C( 5 ) = D"; and unstable. Case II: 0 &lt; m &lt; n , [ E A,,,. A) If rs &lt; 1, there is no equilibrium point of system ( S ) in B) If rt &gt; 1, we have that 1) If xt $ C( E ) , there is no equilibrium point of system ( S ) 2) If xE E C( E), xE is the unique equilibrium point of system i) If the eigenvalues of satisfy 1 A, 1 &lt; 1, i = 1, . . , m, then xt is asymptotically stable, and there are no other ii) If there is at least one i such that 1 A, 1 L 1, then xE is C ( E ) . in C ( 0 . ( S ) in C( [). In particular, equilibrium points in m; and unstable. Case III: A) If rE &lt; 1, x s = E is not an equilibrium point of system B) If rt &gt; 1, xE = 2: is an asymptotically stable equilibrium E A. = B". (Note: C ( E ) = { E}) ( S I . point of system ( S ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>then by Definition 3.1, we have that f l = TI.If1 + ~1.1,EII + 4 or f l = (E", -~, , l ) -l ( ~, , l l E I I + 4 ) = -%I and min ( ( ~~~. ~f ~ + ~1 ~, 1 ~~1 1 + 111) * (11) 5: 1 . Since f E C( E ) , then fIj = Ell and x+ I f E C( E ) , Also, rc Z 1 and by Lemma 3.2, rI &gt; I . Thus, if x is an equilibrium point of ( S ) in C([), t h e n 2 = xt and rE &gt; 1. On the other hand, if xc E C( 4 ) and rE &gt; I , let cp: Z + + C([) and cp(k) = xs. Then cp satisfies (3.4). By Definition 3.1, cp is a solution of ( S ) and by Definition 3.3, p = xE is an equil i b r i u m p o i n t o f ( S ) i n C ( i ) . T h e n , i f x E E C ( E ) a n d r s &gt; 1, then cp ( k ) = xE is an equilibrium point of (S ) in C( 4 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>p: ( 0 , . , k , } -+ C(,$) n U, k , I k,, such that p ( 0 ) E C(5) flB ( x , , E ) , I ( o ( k , )-xE 1 &gt; a, ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Since x F is the unique equilibrium point of ( S ) in C( ,$), we must have xy E C(,$) -C(,$). Then there is a k , 1 5 k 5 m , such that { w , k l = &amp;-1. By the theory of linear difference equations, the p (k)-th component of Tx, + I starting at x r points to xE. Thus, (Tx, + * l w l k , &lt; 1 and r y &lt; I . This contradiction shows that xE is the only equilibrium point of ( S ) in C( E ). Case I f f : If rt &lt; 1, (o = xF does not satisfy (3.4b) and x: is not a solution of (S ). If rt &gt; 1, then there is an open neighborhood U of xE in R" such that min ( ( T,,~lx, + Tll,l,~ll + I,,)* E,,) &gt; 1, for all x E U n D " . It can be proved by induction and by the theory of linear difference equations that all solutions of (S ) starting in U n D" will monotonically converge to xF as k + 00. Thus, xc is an asymptotically stable equilibrium point of ( S ) by Definition 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>3. 2 ,</head><label>2</label><figDesc>we have the following results. gion C( {), , $ E A. C ( 0 , and __ IEEE TRANSACTIONS ON NEURAL NETWORKS. VOL. 2 . NO. I. JANUARY 1991 Proof: Part 1 follows from Theorem 3.1. Part 2 follows from the fact that D" is separated into 3" regions by C( ,$), , $ E A. Part 3 has been proved in Theorem 3.1. Part 4 follows from Part 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>E D" is said to be a (local) minimum of the energy func-tion E if there is an open neighborhood U of x in R" such that E ( x ) 5 E ( y ) , f o r a n y y E D" n U .Theorem 3 . 3 : Suppose that system (S ) satisfies Assumptions 3.1 and 3.2, and all the eigenvalues of system (S ) satisfy XI &gt; -1, i = 1, . . * , T I . Then it is true that 1) along nonequilibrium solutions of system ( S ), the energy function E given in (3.6) decreases monotonically; and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>p</head><label></label><figDesc>( k + 1) = T ( o ( k ) + I = P -' Q P ( o ( k ) + I.LetP ( o ( k )= + ( k ) , PI = J , b(k + 1 ) = Q $ ( k ) + J , then or equivalently, ~$ ~( k + 1) = X , 4 , ( k ) + J , , i = 1, . * 1 n.E ( k ) = -~( k ) ~ PTQP(o(k) + ~( k ) ~ P T P p ( k )-2 q ~( k ) ~P ~P f= -b(k)T Q4(k) + b ( k ) T 4 ( k ) -24(k)TJ " T Moreover, E ( k + 1 ) -E ( k ) = 0 only at equilibrium points2 ) The proof of Part 2 of the theorem follows from the theory of ( S ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>will converge to f as k -+ W . Since along any solution p, the value of the energy function E decreases monotonically in k and the energy function E is continuous in x , we have that E ( f ) &lt; E ( x ) for x E U n D". Thus f is a local minimum of E . On the other hand, suppose that f is a local minimum of the energy function E . Then there is an open neighborhood U o f f in R" such that E ( f ) I E ( x ) , x E U fl D". By Theorem 3.3,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>x l ( k + 1) = -2 x , ( k ) + x 2 ( k )+ 1 1 x,(k + 1) = x , ( k ) -2 x , ( k ) + I 2 i.e.,The eigenvalues of T are -1 and -3.Using equations (3.5), we can find nE's as 0.05 I. m = 0, xE = Case I , when m = 0, is not an asymptotically stable equilibrium point.Case 11, when rn = 1, XEI = [ " : " ' I ,we have r E I = -1.533 &lt; 1, it is not asymptotically stable. For[ 0 . t 6 7 l ' xEz = [ rgl = -1.533 &lt; 1; xE3 = [3 rg3 = -1.533 &lt; 1; xE4 = rE4 = -1.8 &lt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-</head><label></label><figDesc>Table I provides the average CPU prime time for these runs. These data Our simulation results verified that a l , . * , a6 are stored as asymptotically stable equilibrium points for the synthesized LSSM system ( S , )</figDesc><table><row><cell>-</cell></row><row><cell>0.0537</cell></row><row><cell>0.2627</cell></row><row><cell>-0.9313</cell></row><row><cell>0.8239</cell></row><row><cell>0.0537</cell></row><row><cell>0.3104</cell></row><row><cell>0.8358</cell></row><row><cell>-0.2448</cell></row><row><cell>0.2627</cell></row><row><cell>-0.2149 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISONS OF AVERAGE CONVERGENCE TIMES FROM NOISY INPUT DATA FOR THE NEURAL NETWORKS IN</head><label>I</label><figDesc><ref type="bibr" target="#b6">[8]</ref> AND 191 AND FOR (S,) (EXAMPLE 2.3)</figDesc><table><row><cell>~~</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Index Model</cell><cell>from [8] Model</cell><cell>Model ( 7 = 10) from 19]</cell><cell>Model from [9] ( 7 = 100)</cell><cell>System (S,) 7, = 1.5 r2 = 0.5</cell><cell>System (S,) r , = 0.95 r , = 1.5</cell></row><row><cell>Average Time (min)</cell><cell>0.016039</cell><cell>1.805213</cell><cell>3.125679</cell><cell>0.003314</cell><cell>0.0077 17</cell></row><row><cell>Time Index</cell><cell>5</cell><cell>545</cell><cell>943</cell><cell>1</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I1 ESTIMATES OF BASINS OF ATTRACTION FOR THE NEURAL NETWORK MODEL I</head><label>I1</label><figDesc></figDesc><table /><note><p><p><p>N</p><ref type="bibr" target="#b6">[8]</ref> </p>(EXAMPLE 2.4) Hamming Distance</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I11 ESTIMATES OF BASINS OF ATTRACTION FOR THE NEURAL NETWORK</head><label>I11</label><figDesc></figDesc><table /><note><p><p><p>MODEL I N</p><ref type="bibr" target="#b7">[9]</ref> </p>(EXAMPLE 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">CONVERGENCE RATES FOR SYSTEM (S,) (EXAMPLE 2.5)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Zero Mean Gaussian Noise</cell><cell></cell><cell>Binary Noise</cell><cell></cell></row><row><cell>Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="3">Standard Deviation (SD)</cell><cell cols="3">Hamming Distance (HD)</cell></row><row><cell>Rate in Percentage</cell><cell>0.71</cell><cell>0.85</cell><cell>1 .oo</cell><cell>6</cell><cell>10</cell><cell>13</cell></row><row><cell>Converge to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Key Pattern</cell><cell>100.0</cell><cell>92.6</cell><cell>79.6</cell><cell>92.6</cell><cell>90.7</cell><cell>88.9</cell></row><row><cell>Converge to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spurious State</cell><cell>0.0</cell><cell>7.4</cell><cell>20.4</cell><cell>1.4</cell><cell>9.3</cell><cell>1 1 . 1</cell></row><row><cell>Not Convergent</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row></table><note><p><p><p>may outnumber the order of the network. (For a specific example of a neural network of dimension n = 81 which stores 151 vectors as equilibrium points, refer to</p><ref type="bibr" target="#b6">[8]</ref></p>. ) Secondly, in a network designed by the eigenstructure method, all of the desired patterns are guaranteed to be stored as asymptotically stable equilibrium points.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>T</head><label></label><figDesc>Thus xg2, x E 3 , and xt4 are not asymptotically stable either. Case 111, when rn = 2 , Syst., vol. 36, pp. 1405-1422, NOV. 1989. pp. 773-778. Dame, IN. Currently, he is Frank M. Freimann Professor of Engineering and Dean of the College of Engineering at the University of Notre Dame. He is author and coauthor of three texts and several other publications. Dr. Michel received the 1978 Best Transactions Paper Award of the IEEE Control Systems Society (with R. D. Rasmussen), the 1984 Guillemin-Cauer Prize Paper Award from the IEEE Circuits and Systems Society (with R. K. Miller and B. H. Nam), and an IEEE Centennial Medal. He is a former Associate Editor and a former Editor of the IEEE TRANSACTIONS ON CIRCUITS A N D SYSTEMS and a former Associate Editor of the IEEE TRANSACTIONS ON AUTOMATIC CONTROL. He was the Program Chairman of the 1985 IEEE Conference on Decision and Control. He was President of the IEEE Circuits and Systems So- ciety in 1989. He was co-chairman of the 1990 IEEE International Symposium on Circuits and Systems. He is an Associate Editor for the IEEE TRANSACTIONS ON NEURAL NETWORKS and an Associate Editor at Large for the IEEE TRANSACTIONS ON AUTOMATIC CONTROL. Si received the B.S. and M.S. degrees in electrical and computer engineering in 1985 and 1988, respectively, from Tsinghua University, Beijing, People's Republic of China. Since 1988, she has been a Research Assistant with the Department of Electrical and Computer Engineering at the University of Notre Dame, Notre Dame, IN, where she is working toward her Ph.D. degree. N. Michel (S'55-M'59-SM'79-F'82) received the Ph.D. degree in electncal engineering from Marquette University and the D.Sc. degree in applied mathematics from the Technical University of Graz, Austna. He has seven years of industrial expenence. From 1968 to 1984, he was at Iowa State University, Ames, IA. From 1984 to 1988, he was Frank M. Friemann Professor of Engineering and Chairman of the Department of Electrical Engineering, University of Notre Dame, Notre Gune Yen received the M.S. degree in electncal engineering from Marquette University, Milwaukee, WI, in 1988. At present, he is working toward the Ph.D. degree in electrical engineering at the University of Notre Dame, Notre Dame, IN.</figDesc><table><row><cell>Jie Anthony</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Corresponding, rcl = -0.9 &lt; 1, rE2 = -1.1 &lt; 1, rc3 = -4.1 &lt; 1, rE4 = -3.1 &lt; 1. Thus xt x E 2 , xc3, and xc4 are not asymptotically stable equilibrium points.</p><p>In simulations of this system, none of the initial conditions within the unit square converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX IV SYNTHESIS</head><p>In the present section, we address a synthesis procedure for system (S ) which is in the spirit of some of the work in 171,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>181.</head><p>Synthesis problem: Given m vectors in B", say a l , . . . , a,,,, how can we properly choose a pair { T, I } such that the resulting synthesized system ( S ) has the properties enumerated below?</p><p>* , a, are asymptotically stable equilibrium points of system (S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) a I , *</head><p>2) The system has no oscillatory solutions.</p><p>3) The total number of spurious asymptotically stable equilibrium points (i.e., asymptotically stable equilibrium points of (S ) contained in B"-{ aI, . . . , CY, } ) is as small as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>The domain of attraction of each a, is as large as possible.</p><p>In view of the analysis results developed in the previous section, we may approach the preceding problem in the following manner.</p><p>Synthesis strategy: Given m vectors in B", say a l , * . . , a,,,, findann x n m a t r i x T = [ T , , ] a n d a v e c t o r I = ( Z , , . . . ,I,,)' such that:</p><p>1) T satisfies Assumption 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>x,, = a,, where 71x,, = Tal + I , and rl &gt; 1.</p><p>(4.1 )</p><p>3) T has repeated eigenvalues equal to r , and -T ~, I 72 1 &lt; 1, and r2 should be as large as possible.</p><p>Remark 4.1: In the following, we give the rationale for the above synthesis strategy. With Assumption 3.1 and by Remark 3.3, we may assume that Assumption 3.2 is also true for the synthesized system ( S ). Then all of the results in the previous section are applicable. We have:</p><p>1) By Theorem 3.3, the synthesized system ( S ) will have no oscillatory solutions.</p><p>2) By Theorem 3.1, for 1 I i I m , x,, = a, is an asymptotically stable equilibrium point of the synthesized system ( S ).</p><p>3) As shown in Lemma 4.2 below, the larger r2 is ( -1 &lt; 72 &lt; 1 ), the fewer spurious asymptotically stable equilibrium points for system (S ) will exist. In general, this will result in larger domains of attraction for the remaining desired asymptotically stable equilibrium points.</p><p>Suppose that we are given m vectors in B", say aI, . . . , a,n.</p><p>Let L = Span ( a , -a,, . . . , a,-, -a,,,) (4.2a) and La = Aspan ( a ~, . . . , a,).</p><p>Then L is the linear subspace of R" generated by the ( m -1 ) vectors a ,am, , a,,!-, -a,,,, L, is the affine subspace of R " generated by the vectors a I, . . . , a,, and Lo = L + a,,,.</p><p>and let T, = r,T+ -r 2 T -, and I, = rla, -T,a,,, (4.4)</p><p>where 7 , , r2 E R are parameters. For this class of synthesized LSSM systems, (S,)</p><p>x ( k + 1) = s a t ( T , x ( k ) + I T ) , k = 0, 1, 2, . . .</p><p>we have the following result. 2) For any 7 , and r2, T, is symmetric.</p><p>3) For any r , , r2 and for any a E L,, T,a + I, = ?,a. In particular, for each a , , i = 1, * * * , m , T,a, + I, = r l a , .</p><p>Proof: 1) The proof of Part 1 can be found in linear algebra text books, e.g., <ref type="bibr">[lo]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Since ( T + ) ' = ( ( u + ) ( u + ) ~) ~ = ( u + ) ( u + ) '</head><p>= T + , it follows that T + is symmetric. Also Tis symmetric, and</p><p>Remark 4.2: For synthesized system ( S7), ( T , -E,#) is nonsingular. Simulation results suggest that for proper choices of  <ref type="figure">-+ R , E ( x ) = -x r ( T,  -E , )</ref> x -2x'fT. In fact, it can be proved as in <ref type="bibr" target="#b5">[7]</ref> that each a E La n B" is a global minimum of the energy function E.</p><p>In the following, we address spurious asymptotically stable equilibrium points of ( S T ) . Lemma 4.2: For a given 7 , &gt; 1, if -1 &lt; r2' &lt; r2.. &lt; 1 and if a E B" is an asymptotically stable equilibrium point of system (S,,,), then a is also an asymptotically stable equilibrium point of system (&amp;).</p><p>Proof: For a given 7 , &gt; 1, take h, = (r2.. -T ~~) / ( 7 , + r 2 ' ) and = ( r l + 7 2 . ) / ( ~1 + r 2 " ) . Then X I , X2 &gt; 0, and XI + X2 = 1, and T,,a</p><p>If a is an asymptotically stable equilibrium point of system + 71(Y,,, = h,[?lT+(aa,,,) + 71T-(aa,) + ?' l (Y,,,] f rem 3.1, a is an asymptotically stable equilibrium point of systems ( S, . ).</p><p>We conclude our discussion with the synthesis procedures given below.</p><p>Synthesis Procedure 4.1: Suppose we are given m vectors a), . * * , a, in B " , which are to be stored as asymptotically stable equilibrium points for an n dimensional system ( S ). We proceed as follows.   Then all conclusions will remain unchanged. In particular, each -a,, i = 1, . . . , in, will also be an asymptotically stable equilibrium point of the synthesized system ( S , ) and the number of elements in L, n B" will approximately be doubled. By Theorem 3.1 and Theorem 3.2, synthesis procedure 4.1 can be generalized to store data given in A = { x E D": x, = 1, -1 orO, 1 I i I n } . W e h a v e Synthesis Procedure 4.2: Suppose that we are given m vectors cyI, . . . , a, in A, which are to be stored as asymptotically stable equilibrium points for an n dimensional system (S ). We proceed as follows.</p><p>--r 45 1) For a, a' E { a I , , a,}, a' # a, according to Theorem 3.2, the two vectors a and a' can not be stored as asymptoticallystableequilibrium points of system (S,), if either a E Then all vectors in L, n A, where L, = Aspan ( al, . . . , a m ) , including a l r . . . , a,,,, will be stored as equilibrium points in the LSSM system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(cr') or</head><p>x ( k + 1 ) = sat(T,x(k) + Z,), k = 0 , 1, 2, * . ( S , )</p><p>6) For an a E { a), . . . , a, }, if 01 E B " , a is an asymptotically stable equilibrium point of ( S T ) . If a B " , whether a is an asymptotically stable equilibrium point of (S,) or not can be determined using Theorem 3.1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neurons with graded response have collective computational properties like those of two-state neurons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . J . Hopfield</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="1984-05">May 1984. 1985</date>
		</imprint>
	</monogr>
	<note>Biol. Cybernet.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple &apos;neural&apos; optimization networks: An AID converter, signal decision circuit, and a linear programming circuit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information storage and retrieval in spin-glass like neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Personnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">L359-L365</title>
		<imprint>
			<date type="published" when="1985-04">Apr. 1985</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collective computational properties of neural networks: New learning mechanisms</title>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4217" to="4228" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Qualitative analysis and synthesis of a class of neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Porod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Sysr</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="976" to="986" />
			<date type="published" when="1988-08">Aug. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Qualitative analysis and synthesis of a class of neural networks: Variable structure systems with infinite gains</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Sysr</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="713" to="731" />
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Analysis and synthesis of a class of neural networks: Linear systems operating on a closed hypercube</title>
		<imprint>
			<publisher>IEEE Trans. Circ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis and synthesis techniques for discrete time neural network models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1989 IEEE Conf. Decision Control</title>
		<meeting>1989 IEEE Conf. Decision Control<address><addrLine>Tampa, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Herget</surname></persName>
		</author>
		<title level="m">Mathematical Foundations in Engineering and Science</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</editor>
		<meeting><address><addrLine>Englewood Cliffs, NJ; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1981">1981. 1982</date>
		</imprint>
	</monogr>
	<note>Ordinary Diflerential Equations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information capacity of the Hopfield model</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><surname>Jacques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associative memories via artificial neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Sysr. Mag</title>
		<imprint>
			<biblScope unit="volume">IO</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="17" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A synthesis procedure for Hopfield&apos;s continuous time associative memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1990-07">July 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to Matrix Compurarions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Narl. Acad. Sri. U.S.A</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
