<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressed Data Cubes for OLAP Aggregate Query Approximation on Continuous Dimensions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jayavel</forename><surname>Shanmugasundaram</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Usama</forename><surname>Fayyad</surname></persName>
							<email>fayyad@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
							<email>bradley@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compressed Data Cubes for OLAP Aggregate Query Approximation on Continuous Dimensions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C210DBFD980DB4566F6851ED8F8EC9F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>OLAP</term>
					<term>data cubes</term>
					<term>clustering</term>
					<term>density estimation</term>
					<term>approximate query answering</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficiently answering decision support queries is an important problem. Most of the work in this direction has been in the context of the data cube. Queries are efficiently answered by pre-computing large parts of the cube. Besides having large space requirements, such pre-computation requires that the hierarchy along each dimension be fixed (hence dimensions are categorical or prediscretized). Queries that take advantage of pre-computation can thus only drill-down or roll-up along this fixed hierarchy. Another disadvantage of existing pre-computation techniques is that the target measure, along with the aggregation function of interest, is fixed for each cube. Queries over more than one target measure or using different aggregation functions, would require pre-computing larger data cubes. In this paper, we propose a new compressed representation of the data cube that (a) drastically reduces storage requirements, (b) does not require the discretization hierarchy along each query dimension to be fixed beforehand and (c) treats each dimension as a potential target measure and supports multiple aggregation functions without additional storage costs. The tradeoff is approximate, yet relatively accurate, answers to queries. We outline mechanisms to reduce the error in the approximation. Our performance evaluation indicates that our compression technique effectively addresses the limitations of existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There has been much work on answering multi-dimensional aggregate queries efficiently, for example the data cube operator <ref type="bibr" target="#b13">[13]</ref>. OLAP systems perform queries fast by pre-computing all or part of the data cube <ref type="bibr" target="#b15">[15]</ref>. Such pre-computation takes a large amount of space and is possible only when the query hierarchy along each dimension of interest is fixed or discretized beforehand. For example, a retailer may group stores by city, then region, then state or province and finally by country. All future queries issued by the retailer can group by only the specified categories, if the benefits of pre-computation are to be realized.</p><p>While this works well when the query categories are fixed beforehand, there are many cases where it is too restrictive. Consider an employee database in which each employee has attributes -age, salary and vested amount in a pension plan. A user may want to issue a query that provides a cross-tabulation of the average ages of employees having pension plans in the range 100K-200K, 200K-400K, 400K-1000K and having salaries in the range 50K-100K, 100K-200K, and 200K-300K. For traditional approaches, the problem is that the ranges specified by the user can be arbitrary. In other words, the query hierarchy is dynamic and not pre-discretized along each dimension.</p><p>Existing approaches are also limited in that the aggregation dimension must be fixed beforehand. Thus, if a cube is materialized for age as the aggregate dimension, then it can only be used for aggregate queries on age. It cannot be used to query the average salary of people in a particular age group. Efficient processing of such queries requires the materialization of more (partial) data cubes, resulting in higher space overhead. Since cube size explodes as dimensionality increases, even a single (partially) pre-computed data cube has significant storage requirements <ref type="bibr" target="#b23">[23]</ref>. Hence cubes must be limited to only a few dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions of this Paper</head><p>We propose a new cube compression technique that is based on modeling the statistical structure of the data. By estimating the probability density of the data, we can build an extremely compact data representation that supports aggregate queries. This reduction has the following desirable properties:</p><p>1. The ability to efficiently answer ad hoc queries over continuous dimensions without pre-discretizing each such dimension.</p><p>2. The ability to store a compact, portable representation of the database that is smaller than the original database by orders of magnitude, and much smaller than the corresponding cube. This enables ad hoc query support on small or mobile clients (for example, laptops) without the need to be connected to the main OLAP server.</p><p>3. The ability to build cubes on many dimensions without incurring a large storage penalty.</p><p>4. The ability to treat aggregate dimensions interchangeably while using several aggregate measures without paying an added storage penalty.</p><p>The tradeoff is accuracy: approximate, rather than exact results. This is appropriate for most exploratory data analysis applications and cube design exercises. For example, an administrator may wish to explore many possible cubes before committing to the expensive task of materializing an actual cube. Also, in reporting applications, it may be desirable to provide off-line support with approximate results while the report is being created and edited. Actual values can be populated prior to publishing the final report by synchronizing with the database server and performing the expensive queries. A bound on the error of this scheme is straightforward and we present a technique to approximately minimize it.</p><p>This work exploits the probability density distribution of the data and our approach to estimating this density is based on statistically clustering the records in the database. Clustering provides a mixture model density estimate, which is compact yet capable of supporting aggregate queries (count, sum, average, etc.) We show that it is possible to support such queries while utilizing a very small memory requirement. The primary emphasis is on studying the behavior of our scheme at extreme levels of compression. We then describe how more memory can be utilized.</p><p>We consider continuous-valued dimensions because the issues of dynamic query hierarchy and interchangeability of the aggregation variable are most relevant in this situation. We believe, however, that the same approximation technique can be used for data having discrete-valued dimensions (see Section 6). This paper introduces the notion that statistical models of the data can be used to dramatically reduce the size of data cubes while supporting approximate aggregate queries. It is not our intent to replace cubes, nor do we claim that the method used to estimate the probability density of the data is the best one possible: we aim at a proof of concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>As mentioned above, much work has been done on providing exact answers to queries over the data cube <ref type="bibr">[15][27]</ref>. All such approaches share the disadvantages of excessive space overhead, prediscretizing dimensions and treating dimension and target measures non-uniformly. More recently, work has been done on approximating data cubes through various forms of compression such as wavelets <ref type="bibr" target="#b25">[25]</ref> and multivariate polynomials <ref type="bibr" target="#b3">[3]</ref>. A related approach is that of multi-dimensional histograms <ref type="bibr" target="#b20">[20]</ref>. Beyer et. al. <ref type="bibr" target="#b4">[4]</ref> consider the problem of pre-computing a sparse data cube for queries accessing more than a certain number of data records using association rule mining techniques. All these approaches reduce the space overhead but still have the stringent restrictions of prediscretizing dimensions and are tailored to a particular target dimension and aggregate function. To the best of our knowledge, there is no general technique proposed that solves the range of practical problems addressed in this paper.</p><p>Work on efficient computational schemes for exact query support includes <ref type="bibr" target="#b9">[9]</ref>[18] <ref type="bibr" target="#b27">[27]</ref>[14] <ref type="bibr" target="#b15">[15]</ref>[19] <ref type="bibr" target="#b1">[1]</ref>. In <ref type="bibr" target="#b9">[9]</ref>, a scheme is proposed in which portions of previous queries are cached and reused. The work of <ref type="bibr" target="#b27">[27]</ref> is focused on optimizing multiple related dimensional queries. In <ref type="bibr" target="#b18">[18]</ref>, an alternative storage and index organization scheme is proposed based upon a collection of packed and compressed R-trees. In <ref type="bibr" target="#b15">[15]</ref>, the focus is on determining which cells to pre-compute in a cube in order to reduce size, yet maintain ability to infer the missing cube values using logical or algebraic constraints. This approach is restricted to the traditional OLAP framework where all dimensions are discrete.</p><p>In contrast, this work targets a dramatic reduction in storage while supporting approximate query results over continuous-valued data attributes. As such, it addresses a major gap in actual OLAP systems today. It is also a direct application of scalable clustering techniques in Data Mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ANSWERING AGGREGATE QUERIES USING DENSITY DISTRIBUTIONS</head><p>Multi-dimensional data records can be viewed as points in a multidimensional space. For example, the records of the schema (age, salary) could be viewed as points in a two-dimensional space, with the dimensions of age and salary. Figure <ref type="figure">1</ref> shows some data conforming to the above example schema. Figure <ref type="figure">2</ref> shows its representation as points in a two dimensional space. The average age for data with a 1 ≤ a ≤ a 2 and s 1 ≤ s ≤ s 2 can be computed as the ratio of the above two quantities (computing the average of salary is symmetric). There are several advantages to executing the query using the density function Pr(a,s) rather than the actual data:</p><formula xml:id="formula_0">AGE</formula><p>• If the density function is compact, significant storage is saved as the actual data is not used to answer the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>If the integration of the density function is efficient, then executing queries in this fashion is efficient and presents an alternative to pre-computation without the disadvantage of prediscretizing query hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The same density function is used to answer many different aggregate queries (i.e. any dimension can also be a measure in OLAP terminology), without paying an added cost, leading to a further savings in space over traditional pre-computation approaches.</p><p>Thus a density-based approach addresses the limitations of existing pre-computation techniques.</p><p>There are two crucial properties that the density function must satisfy in order to realize the benefits: (a) the density function must be compact; and (b) integration of the density function must be efficient. The challenge is to derive such a density function from large volumes of data.</p><p>Techniques for estimating densities include: histogram-based methods, Parzen windows, and kernel-density estimates <ref type="bibr">[21][24]</ref>. Kernel density estimators place a kernel (typically a bump-shaped function; e.g. a Gaussian) atop each data point. Assuming that the kernel has the appropriate width, the sum of the kernel contributions at any point can be shown to converge to the value of the true density function as the number of data points tends to infinity <ref type="bibr" target="#b24">[24]</ref>. Kernel-based methods are essentially nearest-neighbor-type algorithms: assuming far-off points have negligible contribution to the sum, one only has to find the nearest neighbors and sum over their kernel contributions to obtain the density estimate at the point. Kernel-based density estimates are extremely difficult to compute in high dimensions <ref type="bibr">[21][24]</ref>. In addition, they require the presence of the data set, and hence are not effective for compression. We employ clustering as our basic density estimation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DENSITY ESTIMATION USING CLUSTERING</head><p>In this paper we choose to use clustering-derived mixture models as our efficient and scalable approach to density estimation. The clustering problem has been formulated in various ways in statistics <ref type="bibr" target="#b2">[2]</ref>[16] <ref type="bibr" target="#b24">[24]</ref>[21], pattern recognition <ref type="bibr" target="#b5">[5]</ref>[10] <ref type="bibr" target="#b12">[12]</ref>, optimization <ref type="bibr">[6][22]</ref>, and machine learning literature <ref type="bibr" target="#b11">[11]</ref>. The fundamental problem is that of grouping together (clustering) data items that are similar to each other. Data is generally not uniformly distributed and some combinations of attribute values are more likely than others.</p><p>Clustering can be viewed as identifying the dense regions of the probability density of the data source. An efficient representation of the probability density function is the mixture model: a model consisting of several components (e.g. a model consisting of the sum of 3 Gaussians). Each component generates a set of data records (a "cluster"). The data set is then a mixture of clusters and the problem is to identify the data points constituting a cluster and to infer the properties of the distribution governing each cluster.</p><p>The mixture model probability density function has the form:</p><p>The coefficients W (mixture weights) are the fraction of the database represented by the corresponding clusters, and k is the number of clusters. We focus on models whose components (clusters) are represented by multivariate Gaussians, though in principle any representation that can be integrated numerically can be used (see Section 3.2). This choice of Gaussians is motivated by the fact that any distribution can be approximated accurately with a mixture model containing a sufficient number of components <ref type="bibr">[24][21]</ref>. Further, clustering is ideal for the following reasons:</p><p>• Clusters represented by multivariate Gaussians are compact to represent and easily integrable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Efficient methods for clustering large volumes of data have been developed <ref type="bibr" target="#b7">[7]</ref>[8] <ref type="bibr" target="#b26">[26]</ref>.</p><p>Next we discuss how queries can be efficiently evaluated. The following discussion does not depend on a particular clustering method. The only requirement is that the clusters be represented in a compact form that can be efficiently integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation of Clusters</head><p>Each cluster in a mixture model is represented by a multivariate Gaussian probability density function:</p><p>where x = (x 1 ,x 2 ,x 3 ,x 4 ,…,x n ) is a n-dimensional column vector corresponding to a data point in the selected n-dimensional data space, µ is the n-dimensional column vector whose elements are the means (averages) of the data belonging to the cluster. Σ is an n × n covariance matrix, with determinant |Σ| and matrix inverse Σ -1 .</p><p>For n-dimensional data, a cluster is represented by 1 + n + [n(n+1)]/2 values: the number of data points in the cluster (1), the mean vector µ (n), and symmetric covariance matrix Σ (n(n+1)/2).</p><p>If the covariance matrix is diagonal, then its representation requires only n values, and the resulting cluster is characterized by 2n+1 values. For a mixture model with k clusters, the total representation cost is k(1+2n). We assume the diagonal covariance structure in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answering Aggregate Queries using Cluster Representations</head><p>We consider three types of aggregate queries: count, sum and average. We first describe how range selection queries are answered, before outlining how multiple intervals can be specified along a each dimension.</p><p>Count Queries: Suppose a query requests the number of items (count) in specified ranges on variables x i1 through x im , (m &lt; n, i.e. a subset of the n dimensions), the ranges being from a r to b r for dimension x ir . Let the unspecified dimensions be x j1 through x jn-m .</p><p>In this case, count = ∑</p><formula xml:id="formula_1">= K l l Num 1 ) ( , where . ... ... ) | ,..., , Pr( ... ... ) ( 1 1 2 1 1 1 m jn j i im bm am n b a l dx dx dx dx l x x x N l Num - ∞ ∞ - ∞ ∞ - ∫ ∫ ∫ ∫ ⋅ =</formula><p>Assuming a diagonal covariance matrix, Pr(x) can be expressed as a product of univariate Gaussians. Hence, the relation for Num(l) becomes:</p><formula xml:id="formula_2">∑ = = k 1 . ) | ( Pr W ) ( Pr ¡ ¡ l x x         - ∑ -       - - = ) ( ) ( 2 1 2 / 1 | | ) 2 ( 1 ) Pr( x x x T e n π ∫ ∫ ∫ ∫ ∫ ⋅ ⋅ ⋅ ⋅ = - ∞ ∞ - - ∞ ∞ - ∞ ∞ - 1 1 1 1 1 1 2 2 1 1 . ) | Pr( ... ) | Pr( ) | Pr( ... ) | Pr( ) | Pr( ) ( b a i i b a im im m jn m jn j j j j l dx l x dx l x x d l x x d l x x d l x N l Num</formula><p>The integrals from ∞ to ∞ + for dimensions not involved in the range queries evaluate to one. The remaining terms are univariate integrals over the dimensions x i1 to x im .</p><p>Sum Queries. This query specifies ranges in dimensions x i1 through x im and requests for the sum of data items present in the range over a specific dimension x S . The range specified on variable </p><formula xml:id="formula_3">x</formula><formula xml:id="formula_4">) ( 1 1 2 1 1 1 m jn j i im bm am n S b a l dx dx dx dx l x x x x N l Sum - ∞ ∞ - ∞ ∞ - ∫ ∫ ∫ ∫ ⋅ =</formula><p>The integral decomposes and can be evaluated as before.</p><p>Average Queries. This query can be computed as the ratio of the result of the sum query for dimension x s in the specified ranges and the result of the count query in the specified ranges.</p><p>Multiple Interval Queries. In the discussion above, we assumed that only one range selection is specified along each dimension. Disjunctive queries are easily transformed to sums over disjoint ranges. Another common type of query is a "cross-tabulation", where multiple ranges are specified along each dimension. Thus, a query may require the number of data points for every combination of ages in the ranges 10-20, 20-30 and 30-40 and salaries in the range 50K-60K, 60K-90K and 90K-120K. Rather than evaluating all combinations (in this case, there are nine) as separate queries, integrals corresponding to sub-queries may be cached and re-used. Thus, in the current example, the integral for the age ranges 10-20, 20-30 and 30-40 would be performed exactly once for the entire query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE BASIC ALGORITHM</head><p>The first algorithm we investigate takes as input the memory budget and attempts to fit the compressed data representation into it. Since we are interested in studying extremes of compression, we focus on building small models of large databases as proof of concept.</p><p>Consider the representation of data as a set of clusters. At one extreme, one can assign an individual cluster to each data point. In this case there is no compression and answers to queries are exact. This approach is tantamount to holding the entire database in memory and clearly does not scale. As the memory bound is reduced, one needs to select "good clusters" to accurately represent substantial subsets of the data. Assuming each cluster represents the data it displaces accurately, the accuracy lost due to dropping data records is (hopefully) minimized.</p><p>Recall that the storage requirement for the mixture model with k diagonal Gaussians is k(2n+1) values as opposed to Nn values to represent N data records. We assume that clustering is performed independently (e.g. scalable EM clustering <ref type="bibr">[8]</ref>) and that we are given a mixture model consisting of a set of k' clusters. If the memory budget only allows k &lt; k' clusters, the k clusters with maximum data representation are selected. If k &gt; k' either: the dataset is re-clustered with k clusters, or if re-clustering is not a viable option, the remaining memory buffer is filled with data points fitting the current model least (i.e. the buffer is filled with outliers). An outlier is a data point that gets assigned the lowest maximum density by the k clusters.</p><p>When answering a query, integration is done over the model, and then the outliers are checked to see if any contribute to the answer (i.e. the aggregation is performed over models plus data). Answers obtained over actual cached records are by definition exact.</p><p>We are concerned with evaluating the concept of supporting count, sum and average queries over a compressed data model. We focus on the case when k' &gt; k. Performance is evaluated in Sections 4.3 -4.5. In Section 5 we introduce an algorithm that grows additional clusters in the case when k' &lt; k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Real World Data Sets for Empirical Evaluation</head><p>The four data sets in Table <ref type="table" target="#tab_1">1</ref> are used in evaluating performance. Since OLAP type queries are usually performed over lowdimensional data sets, we select 3 and 5 dimensional projections. This allows us to study the effect of varying k, and concentrate on using small memory buffers. High-dimensional datasets typically require many more clusters to be modeled accurately<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b5">[5]</ref>.</p><p>For each data set we construct a statistical model using the EM algorithm <ref type="bibr">[8]</ref>. Model construction details are not relevant. Cluster models used in the evaluations in this section were computed for fixed values of k and were initialized randomly on the range of the data. These models were not tuned to specifically support the query application.</p><p>The performance measures were:</p><p>•    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Methodology</head><p>Five hundred uniform random queries were generated for each dataset. The queries were equally divided among those accessing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Time/Compression Ratios</head><p>Table <ref type="table" target="#tab_2">2</ref> summarizes the compression ratios for cluster models with k = 100 clusters and evaluation times for queries over the full SQL database and over the cluster model. For datasets in 3 dimensions, 5.6 kB of memory was required to store the mixture model summarizing the dataset. For datasets in 5 dimensions, 8.8 kB was required to store the mixture model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measuring Accuracy</head><p>We report accuracy results for the datasets listed above using cluster models with k = 100. Average relative errors of Cluster and Sample query results with respect to the SQL result over the full dataset are summarized in Figure <ref type="figure" target="#fig_3">3</ref>. An improvement of as much as 87% is observed on the Investor dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Increasing k</head><p>Figures <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure" target="#fig_5">7</ref>summarize the effect of varying the number of clusters k on the query error. Average percent error with respect to the SQL result on the full database is plotted for cluster models with the number of clusters k = 10, 50, 100. For comparison, results over random samples of equivalent size are included (Sample). These results indicate that indeed, on average, as the cluster model complexity is increased (via increasing k), the resulting mixture model more accurately approximates the data density 2 . The cluster models for all values of k tested yielded superior query results in comparison to the sampling approach. The following table summarizes the memory requirement (in kilobytes (kB)) needed to store the mixture models for each dataset used in this evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Model Memory Requirement </p><formula xml:id="formula_5">k = 10 k = 50 k =100 Census3 Astro3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>The results presented were over real-world data sets. We have experimented with synthetic data sets, and obtained very good results as expected. We note that in general, we do expect our method to perform poorly when the actual query result is small. For small query results, the distinction between zero and a small number is not meaningful. In a practical application, large error bars would be associated with estimates of small counts.</p><p>It is difficult to measure an error for queries in which SQL returns 0.0. In this case, we measured the percentage of queries that Cluster returns 0.1 or less. These ranged from 24% (Census3) to 2 Small variations to this trend are sometimes present, like in Figure <ref type="figure" target="#fig_5">7</ref> (k = 50 and k = 100), because of the random choice of initial seed values that affect the quality of the clusters.  It is important to note, however, that the cluster models used in the evaluations in this section were computed for fixed values of k, and were initialized randomly on the range of the data. These cluster models were not tuned to specifically support our query application. This issue is addressed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MODEL GENERATION TO SUPPORT QUERYING</head><p>In the previous section, we outlined an approach to approximately answer multi-dimensional aggregate queries given a probabilistic model of the data. The accuracy of the answer depends critically on the fit of this model to the dataset. We define a notion of accuracy appropriate in our context, and then outline a technique that approximately satisfies this definition and also handles incremental updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A Definition of Accuracy</head><p>The notion of accuracy that we consider is defined for the result of an aggregate query. There are several components to the accuracy metric. We describe and motivate each in turn. This parameter specifies the number of "unusual" data points that are to be identified and explicitly stored.</p><p>Suppose an approximate model was defined with the following values for the parameters: d = 10%, c = 90%, s = 1000, o = 50. This model then answers all queries that aggregate over more than 1000 data points within 10% accuracy 90% of the time. In addition, queries whose selection conditions specify aggregates over any subset of the 50 most unusual points will return exact results.</p><p>Note that conditions a), b) and c) progressively weaken the accuracy requirement. If d were set to 0%, c to 100% and s to 0, then the accuracy metric would not allow for any approximation. The factor d) explicitly models a common decision support requirement. Increasing factor d) increases the accuracy of the model because more data points are explicitly stored.</p><p>In the next section, we outline a technique to build a cluster model that approximately satisfies these requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Constructing Cluster Models Satisfying Accuracy Requirements</head><p>The goal in this section is not to propose a new clustering technique. Rather, our focus is to exploit existing clustering strategies and to adapt them for our purposes. We base our discussion on the Expectation-Maximization (EM) clustering algorithm <ref type="bibr">[8]</ref>.</p><p>The idea is to iteratively refine the cluster models until the accuracy requirements are satisfied. This is done by identifying data regions in which the current cluster model violates the accuracy requirements. New clusters are grown in these regions. The high-level algorithm for the technique is as follows.</p><p>The initial model can be chosen based on some standard clustering mechanism <ref type="bibr" target="#b7">[7]</ref>[8] <ref type="bibr" target="#b26">[26]</ref>. The challenge lies in determining when a cluster model is not sufficiently accurate and then to grow new clusters so that the new model is more accurate. We address these two issues next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Determining and Improving the Accuracy of a Cluster Model</head><p>The problem is identifying whether the cluster model satisfies the accuracy parameters. This is achieved by dividing the multidimensional space into (possibly overlapping) tiles. The number of data points belonging to a tile is set approximately equal to the support for the aggregate result (the accuracy parameter s). The motivation behind this approach is that the tiles serve as surrogates for possible range queries over the data. Data points in the tiles correspond to those selected by an aggregate range query. By evaluating the accuracy criterion on the individual tiles, an estimate of the accuracy of the model on queries selecting more than s data points can be calculated. We next address the issue of placement of the tiles.</p><p>One of the key roles of the tiles, in addition to determining the accuracy of the model, is to identify the location where new clusters are to be grown. If a cluster does not sufficiently summarize the data belonging to it (according to the specified values of d, c and s (Section 5.1)), the cluster needs to be split. cluster is divided so that the area under the Gaussian is the same for each tile. Intuitively, each tile then should encompass the same number of data points, if the data follows the Gaussian distribution of the cluster.</p><p>As an illustrative example, consider a one-dimensional Gaussian cluster which summarizes 3s data points. The tiling procedure would result in three tiles, each having area under the Gaussian of 1/3 (i.e. s points). One such tile will have range from ∞ to t such that the area under the Gaussian on this range is 1/3. Similarly, the two other tiles have ranges [t,u] (t &lt; u) and [u, ∞ + ) such that the area under the Gaussian on these ranges is 1/3, respectively. The above three ranges correspond to tiles for the one-dimensional Gaussian given the correctness requirements. This is done in multiple dimensions (assuming low dimensionality) independently -justified by the diagonal covariance structure.</p><p>We now determine whether each tile satisfies the accuracy requirement. The data records are scanned and partitioned first by cluster membership, then by tile membership within the cluster. Let N be the total number of data points in the dataset and let N(l) be the number of data points with membership in cluster l. Let N T (l) be the number of data points contained within tile T of cluster l defining a region in the n-dimensional data space. Let Pr(x | l ) denote the Gaussian density function for cluster l. The error for tile T is then defined as the difference between the model-predicted data density in the tile and the actual data density in the tile: , where ⋅ applied to a set indicates the number of elements in the set. Hence cluster l satisfies the accuracy requirement if c% of its tiles satisfy the requirement. The clusters that do not satisfy the accuracy requirement are split into smaller clusters, each of which will hopefully better satisfy the accuracy requirements.</p><p>For each cluster that does not satisfy the accuracy requirement, the tile having the maximum positive error (or the tile having the minimum negative error if a tile with positive error does not exist) is determined. A positive error over tile T occurs when T contains more data than the model predicts (i.e.</p><formula xml:id="formula_6">∫ &gt; T T d ) l | ( N ) l ( N x x Pr</formula><p>). Similarly, a negative error occurs when the tile contains less data than model predicts (i.e.</p><formula xml:id="formula_7">∫ &lt; T T d ) l | ( N ) l ( N x x Pr</formula><p>). The model approximates the actual data distribution worst on this tile. We place a new cluster at this location. Such new cluster locations are determined for each offending cluster and the clustering algorithm is run again, with new cluster initialization points in addition to the original clusters. The RefineModelAccuracy (RMA) algorithm implementing this process is summarized below and is executed repeatedly until the accuracy requirement is satisfied. The specified number of outliers o are then found in a single data scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Incremental Updates</head><p>Updates can be handled efficiently within the RMA framework. The tiling procedure is used to determine whether the updates violate the accuracy requirement of the cluster model. If the accuracy requirement is satisfied, then no change is required to the cluster representation. On the other hand, if the accuracy requirement is not met, clusters are grown in only those regions where the discrepancy exists as in the regular RMA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Evaluation</head><p>Our goal in this evaluation is not to exhaustively test the algorithm, but to demonstrate that refinement is possible and results in improved models of the data. Cluster models were computed via algorithm RMA with the following fixed parameter settings: deviation d = 10%, confidence c = 90%, support s = N/3125 for datasets with N records and 5 dimensions and s = N/1000 for datasets with 3 dimensions, in all cases the number of outliers o = N/1000. These conservative values for s were chosen so that tiling the 5 dimensional datasets results in approximately 5 partitions per dimension and 10 partitions per dimension for the 3 dimensional datasets. The experimental setup was the same as in Section 4.2.</p><p>Average error percentages with respect to the true SQL result for cluster models (Cluster) and random samples (Sample) are given in Figure <ref type="figure">8</ref> for Astro5, Investor and Cover5 datasets. The results are averaged over queries in which the full SQL result is greater than 1% of total data set. Resulting cluster models had the following number of clusters: k = 219 for Astro5, k = 91 for Investor and k = 318 for Cover5. The memory requirement to store the mixture model for Astro5 was 19.3 kB and requirements for Investor and Cover5 were 5.1 kB and 28.0 kB, respectively. These cluster models provide superior results over sampling and improved accuracy by as much as 93% (Investor) over the sampling result with a corresponding compression ratio of 3935. In comparison to cluster models used in Section 4.4, models produced by RMA improved query results by as much as 76% (Astro5). RMA models improved Investor results by 6% and Cover5 results by 30% over those used in Section 4.4. Average SQL result for the Astro5, Investor and Cover5 datasets were 58265, 71559 and 37337, respectively.</p><p>A more exhaustive evaluation of RMA studying its sensitivity to parameter settings and performance over other large databases is not presented here due to space limitations. However, the goal of this paper is to demonstrate that the concept is feasible, and that reasonable accuracies can be obtained with 3 orders of magnitude reduction in space over the size of the data. We have purposefully limited our exposition to studying the performance of small model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>This work demonstrates that significant compression ratios are attainable while supporting OLAP-type aggregate queries over large databases when the statistical structure of the data is exploited. In addition, the method presented supports ad hoc querying over continuous dimensions, allows each dimension to serve as a measure and supports various aggregate functions. In general, OLAP systems do not support continuous dimensions without pre-discretization -hence our scheme fills an important niche in the OLAP space. Also notable is the fact that our approach performs significantly better than a sampling-based approach -this is not surprising but serves as additional evidence supporting the merit of the technique introduced here.</p><p>The tradeoff when using this compression scheme is a sacrifice in accuracy. Empirically, however, accuracy is not exceedingly compromised. Moreover, the Gaussian mixture model readily gives rise to error bounds on the approximate query result. A system utilizing this technique is capable of judging uncertainty associated with a prediction and this information can be used to enhance usability. In particular, inaccuracy occurs when the query result is small and the system can suppress or highlight such results to the user.</p><p>We introduced an algorithm dedicated to the task of computing cluster models specifically supporting OLAP-type queries, rather than using general models obtained by classical EM clustering. Initial computational evaluations of this algorithm were provided.</p><p>We plan to follow many threads as future work. An obvious extension is to generalize to discrete dimensions using discrete clustering techniques. Note that the theory makes no assumptions regarding the type of data attributes or the density distribution. The only requirement is that the distribution be efficiently integrable and have a compact representation. For discrete attributes, the multinomial distribution naturally fits in the context of EM clustering and satisfies our distribution requirements.</p><p>We also plan to investigate statistical methods that can provide tighter error bounds. For instance, the statistical density model can be used to assess the uncertainty regarding the expected result. The variance can be derived for the predicted quantity over the query region. This allows the system or the user to make tradeoff decisions between accuracy and the utility of having an approximate result quickly. Finally, studying special clustering methods tailored to the task of answering queries is an intriguing problem. The presentation of the RMA algorithm was primarily targeted at demonstrating this possibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ir is [a r , b r ]. Using the above notation, let the unspecified dimensions be represented as x j1 through x jn-m . Again, Sum =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>C lu s te r ( 3 D im ) S a m p le ( 3 D im ) C lu s te r ( 5 D im ) S a m p le( 5 D im )    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average % Error for Cluster and Sample.Figure 4: Varying k on Census dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 3: Average % Error for Cluster and Sample.Figure 4: Varying k on Census dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Varying k on Investor dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>a)</head><label></label><figDesc>Deviation from the Actual Value (d): This requirement is the percentage difference between the approximate query result and the actual query result. b) Confidence (c): This requirement specifies the percentage of time that the approximate query result satisfies the deviation requirement. c) Support for the Aggregate Result (s): This accuracy requirement is used to model the nature of decision support queries. Most queries deal with aggregates over large amounts of data, not with selections of individual data points. This parameter establishes a threshold on the number of data points such that only queries that select more than s data points need to satisfy requirements a) and b). d) Number of Outliers (o): One important goal of decision support queries lies in detecting information that is unusual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4 . 5 .</head><label>45</label><figDesc>Tiles are placed according to the current clustering model. Each C := Initial Cluster Model While (C is not sufficiently accurate) do Grow new clusters in C where it is not sufficiently accurate End while C is the required cluster model Algorithm RefineModelAccuracy (RMA) (Input: Model C, Support s, Deviation d, Confidence c, Data D) Output Model C' 1. Split each cluster in the model C into tiles each of size approximately s 2. Scan the data D and classify each data point first by the cluster membership and then by the tile membership in the cluster it belongs to. Also, update the aggregate value for each tile 3. Set Tile Set T = ∅ For each cluster in C do If the aggregate value for c% of the tiles in the cluster is within d% of the expected value s, THEN the cluster is accurate. ELSE add the tile having maximum positive (minimum negative) error to the tile set T If the tile set T = ∅ , THEN cluster model C satisfies the accuracy requirement. Exit. ELSE a. Rerun the clustering algorithm with the mid-points of the tiles in T and the means of clusters in old model C as the new cluster initialization. b. Repeat steps 1 -5 for new model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A</head><label></label><figDesc>l) = { T 1 , T 2 ,…T h } be the set of tiles for cluster l. Let Γ S (l) be the subset of Γ(l) satisfying the accuracy requirement: cluster l is said to satisfy the accuracy requirement if:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Figure 1: Sample Data Figure 2: 2-D View of Data</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">SALARY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell cols="2">50000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>28</cell><cell cols="2">55000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell cols="2">58000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">50 100000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">55 130000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">57 120000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">Aggregate queries over multi-dimensional data sets specify a region</cell></row><row><cell cols="16">in the multi-dimensional space and an aggregation function of</cell></row><row><cell cols="16">interest. For example, an aggregate query may require the number of data records in the region 10 ≤ age ≤ 40, and 30K ≤ salary ≤</cell></row><row><cell cols="16">80K. The aggregate function in the above example is "count" -the</cell></row><row><cell cols="16">number of data records in a particular region. Common aggregate</cell></row><row><cell cols="16">functions include sum (the sum of values, in a particular dimension,</cell></row><row><cell cols="16">of data records in a region) and average (the ratio of the sum and</cell></row><row><cell cols="7">count aggregate functions).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">A key observation is that if the data probability density distribution</cell></row><row><cell cols="16">is known, multi-dimensional aggregate queries can be answered</cell></row><row><cell cols="16">without accessing the data. As a concrete example, assume that the</cell></row><row><cell cols="16">data density function for the two dimensional case with points</cell></row><row><cell cols="16">having age a and salary s is given by Pr(a,s). Then, assuming that</cell></row><row><cell cols="16">Pr(a,s) is integrable, one can compute the aggregate query count, for a 1 ≤ a ≤ a 2 and s 1 ≤ s ≤ s 2 as:</cell></row><row><cell></cell><cell></cell><cell cols="2">N</cell><cell>⋅</cell><cell cols="3">1 s ∫ ∫ 1 a 2 s a 2</cell><cell cols="2">Pr(</cell><cell>a</cell><cell>,</cell><cell cols="2">s</cell><cell cols="2">)</cell><cell>⋅</cell><cell>da</cell><cell>⋅</cell><cell>ds</cell></row><row><cell cols="16">where N is the total number of records in the data set. Similarly, the sum of the ages for data with a 1 ≤ a ≤ a 2 and s 1 ≤ s ≤ s 2 can be</cell></row><row><cell cols="16">computed as (computing the sum of salary is symmetric):</cell></row><row><cell></cell><cell>N</cell><cell>⋅</cell><cell cols="3">s s 2 ∫ ∫ a 2 a 1 1</cell><cell>a</cell><cell cols="2">⋅</cell><cell cols="2">Pr(</cell><cell cols="2">a</cell><cell cols="2">,</cell><cell>s</cell><cell>)</cell><cell>⋅</cell><cell>da</cell><cell>⋅</cell><cell>ds</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Experimental Real Data Sets</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>size of the model used compared to data size</cell></row><row><cell></cell><cell>(compression ratio),</cell></row><row><cell>•</cell><cell>accuracy of the model as compared to performing the</cell></row><row><cell></cell><cell>queries over the actual SQL database,</cell></row><row><cell>•</cell><cell>and model query response time as compared to time taken</cell></row><row><cell></cell><cell>by a DBMS.</cell></row><row><cell cols="2">For the DBMS, all experiments were conducted using Microsoft</cell></row><row><cell cols="2">SQL Server version 7.0 running on a Pentium II 300 MHz</cell></row><row><cell cols="2">workstation with Microsoft Windows NT 4.0 Server OS.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Evaluation time and compression ratios.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>75% (Cover3). Results presented in Sections 4.3 and 4.4 are averaged over queries in which the full SQL result is greater than 1% of total data set. The following table summarizes average size of the SQL result for the databases used in the evaluation.</figDesc><table><row><cell>Dataset</cell><cell>Ave. SQL Result Size</cell></row><row><cell>Census3</cell><cell>12739</cell></row><row><cell>Census5</cell><cell>14034</cell></row><row><cell>Astro3</cell><cell>51155</cell></row><row><cell>Astro5</cell><cell>58265</cell></row><row><cell>Investor</cell><cell>71599</cell></row><row><cell>Cover3</cell><cell>30982</cell></row><row><cell>Cover5</cell><cell>37337</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Fortunately, experiments indicate that scalable EM degrades more gracefully as dimensionality increases than other variants of EM targeted at massive databases[8].</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We gratefully acknowledge Jeff Bernhardt and Ilya Vinarsky for much assistance with implementation and other aspects of this work. This work was conducted while J. Shanmugasundaram was on a research internship at Microsoft Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Astro5</head><p>Investor Cover5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ave % Error</head><note type="other">Cluster Sample</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Computation of Multidimensional Aggregates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22 nd Int. VLDB Conf., Mumbai (Bombay)</title>
		<meeting>22 nd Int. VLDB Conf., Mumbai (Bombay)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-based Gaussian and non-Gaussian Clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Banfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="803" to="821" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Space-Efficient way to support Approximate Multidimensional Databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sullivan</surname></persName>
		</author>
		<idno>ISSE-TR-98-03</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>George Mason University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bottom-Up Computation of Sparse and Iceberg CUBEs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Conf., Philadelphia</title>
		<meeting>ACM SIGMOD Conf., Philadelphia</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering via Concave Minimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scaling Clustering Algorithms to Large Databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4 th Intl. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>4 th Intl. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling EM (Expectation-Maximization) Clustering to Large Databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
		<idno>MSR-TR-98-35</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Microsoft Research Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caching Multidimensional Queries Using Chunks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Seattle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge Acquisition via Incremental Conceptual Clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="139" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data Cube: A Relational Aggregation Operator Generalizing Group-by, Cross-Tab, and Sub Totals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Layman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venkatrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="53" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Index Selection for OLAP</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Harinarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. On Data Engineering</title>
		<meeting>Intl. Conf. On Data Engineering<address><addrLine>Birmingham, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-04">April 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implementing Data Cubes Efficiently</title>
		<author>
			<persName><forename type="first">V</forename><surname>Harinarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Conf</title>
		<meeting>ACM SIGMOD Conf<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<title level="m">Finding Groups in Data</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd International Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 2nd International Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Alternative Storage Organization for ROLAP Aggregate Views Based on Cubetrees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roussopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Seattle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maintenance of Data Cubes and Summary Tables in a Warehouse</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mumick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mumick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Tuscon</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selectivity Estimation Without the Attribute Value Independence Assumption</title>
		<author>
			<persName><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23 rd VLDB Conf</title>
		<meeting>23 rd VLDB Conf<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Multivariate Density Estimation, Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">K-Means-Type Algorithms: A Generalized Convergence Theorem and Characterization of Local Optimality</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22 nd Int. VLDB Conf., Mumbai (Bombay)</title>
		<meeting>22 nd Int. VLDB Conf., Mumbai (Bombay)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Density Estimation for Statistics and Data Analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data Cube Approximation and Histograms via Wavelets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7 th Intl. Conf. Information and Knowledge Management (CIKM&apos;98)</title>
		<meeting>7 th Intl. Conf. Information and Knowledge Management (CIKM&apos;98)<address><addrLine>Washington D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BIRCH: An Efficient Data Clustering Method for Very Large Databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Montreal</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Array-Based Algorithm for Simultaneous Multidimensional Aggregates</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Tucson</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous Optimization and Evaluation of Multiple Dimensional Queries</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><surname>Shukla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD Conf</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Seattle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
