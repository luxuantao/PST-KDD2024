<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Complete Recipe for Stochastic Gradient MCMC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi-An</forename><surname>Ma</surname></persName>
							<email>yianma@u</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
							<email>tqchen@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
							<email>ebfox@stat.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Complete Recipe for Stochastic Gradient MCMC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">49BFEF4E557693B1741D25FF2950DDF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is nontrivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers-including stochastic gradient versions-based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Markov chain Monte Carlo (MCMC) has become a defacto tool for Bayesian posterior inference. However, these methods notoriously mix slowly in complex, high-dimensional models and scale poorly to large datasets. The past decades have seen a rise in MCMC methods that provide more efficient exploration of the posterior, such as Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> and its Reimann manifold variant <ref type="bibr" target="#b9">[10]</ref>. This class of samplers is based on defining a potential energy function in terms of the target posterior distribution and then devising various continuous dynamics to explore the energy landscape, enabling proposals of distant states. The gain in efficiency of exploration often comes at the cost of a significant computational burden in large datasets.</p><p>Recently, stochastic gradient variants of such continuous-dynamic samplers have proven quite useful in scaling the methods to large datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. At each iteration, these samplers use data subsamples-or minibatches-rather than the full dataset. Stochastic gradient Langevin dynamics (SGLD) <ref type="bibr" target="#b16">[17]</ref> innovated in this area by connecting stochastic optimization with a first-order Langevin dynamic MCMC technique, showing that adding the "right amount" of noise to stochastic gradient ascent iterates leads to samples from the target posterior as the step size is annealed. Stochastic gradient Hamiltonian Monte Carlo (SGHMC) <ref type="bibr" target="#b5">[6]</ref> builds on this idea, but importantly incorporates the efficient exploration provided by the HMC momentum term. A key insight in that paper was that the naïve stochastic gradient variant of HMC actually leads to an incorrect stationary distribution (also see <ref type="bibr" target="#b3">[4]</ref>); instead a modification to the dynamics underlying HMC is needed to account for the stochastic gradient noise. Variants of both SGLD and SGHMC with further modifications to improve efficiency have also recently been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>In the plethora of past MCMC methods that explicitly leverage continuous dynamics-including HMC, Riemann manifold HMC, and the stochastic gradient methods-the focus has been on showing that the intricate dynamics leave the target posterior distribution invariant. Innovating in this arena requires constructing novel dynamics and simultaneously ensuring that the target distribution is the stationary distribution. This can be quite challenging, and often requires significant physical and geometrical intuition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. A natural question, then, is whether there exists a general recipe for devising such continuous-dynamic MCMC methods that naturally lead to invariance of the target distribution. In this paper, we answer this question to the affirmative. Furthermore, and quite importantly, our proposed recipe is complete. That is, any continuous Markov process (with no jumps) with the desired invariant distribution can be cast within our framework, including HMC, Riemann manifold HMC, SGLD, SGHMC, their recent variants, and any future developments in this area. That is, our method provides a unifying framework of past algorithms, as well as a practical tool for devising new samplers and testing the correctness of proposed samplers.</p><p>The recipe involves defining a (stochastic) system parameterized by two matrices: a positive semidefinite diffusion matrix, D(z), and a skew-symmetric curl matrix, Q(z), where z = (θ, r) with θ our model parameters of interest and r a set of auxiliary variables. The dynamics are then written explicitly in terms of the target stationary distribution and these two matrices. By varying the choices of D(z) and Q(z), we explore the space of MCMC methods that maintain the correct invariant distribution. We constructively prove the completeness of this framework by converting a general continuous Markov process into the proposed dynamic structure.</p><p>For any given D(z), Q(z), and target distribution, we provide practical algorithms for implementing either full-data or minibatch-based variants of the sampler. In Sec. 3.1, we cast many previous continuous-dynamic samplers in our framework, finding their D(z) and Q(z). We then show how these existing D(z) and Q(z) building blocks can be used to devise new samplers; we leave the question of exploring the space of D(z) and Q(z) well-suited to the structure of the target distribution as an interesting direction for future research. In Sec. 3.2 we demonstrate our ability to construct new and relevant samplers by proposing stochastic gradient Riemann Hamiltonian Monte Carlo, the existence of which was previously only speculated. We demonstrate the utility of this sampler on synthetic data and in a streaming Wikipedia analysis using latent Dirichlet allocation <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Complete Stochastic Gradient MCMC Framework</head><p>We start with the standard MCMC goal of drawing samples from a target distribution, which we take to be the posterior p(θ|S) of model parameters θ ∈ R d given an observed dataset S. Throughout, we assume i.i.d. data x ∼ p(x|θ). We write p(θ|S) ∝ exp(-U (θ)), with potential function U (θ) = -x∈S log p(x|θ) -log p(θ). Algorithms like HMC <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> further augment the space of interest with auxiliary variables r and sample from p(z|S) ∝ exp(-H(z)), with Hamiltonian</p><formula xml:id="formula_0">H(z) = H(θ, r) = U (θ) + g(θ, r), such that exp(-g(θ, r))dr = constant.<label>(1)</label></formula><p>Marginalizing the auxiliary variables gives us the desired distribution on θ. In this paper, we generically consider z as the samples we seek to draw; z could represent θ itself, or an augmented state space in which case we simply discard the auxiliary variables to perform the desired marginalization.</p><p>As in HMC, the idea is to translate the task of sampling from the posterior distribution to simulating from a continuous dynamical system which is used to define a Markov transition kernel. That is, over any interval h, the differential equation defines a mapping from the state at time t to the state at time t + h. One can then discuss the evolution of the distribution p(z, t) under the dynamics, as characterized by the Fokker-Planck equation for stochastic dynamics <ref type="bibr" target="#b13">[14]</ref> or the Liouville equation for deterministic dynamics <ref type="bibr" target="#b19">[20]</ref>. This evolution can be used to analyze the invariant distribution of the dynamics, p s (z). When considering deterministic dynamics, as in HMC, a jump process must be added to ensure ergodicity. If the resulting stationary distribution is equal to the target posterior, then simulating from the process can be equated with drawing samples from the posterior.</p><p>If the stationary distribution is not the target distribution, a Metropolis-Hastings (MH) correction can often be applied. Unfortunately, such correction steps require a costly computation on the entire dataset. Even if one can compute the MH correction, if the dynamics do not nearly lead to the correct stationary distribution, then the rejection rate can be high even for short simulation periods h. Furthermore, for many stochastic gradient MCMC samplers, computing the probability of the reverse path is infeasible, obviating the use of MH. As such, a focus in the literature is on defining dynamics with the right target distribution, especially in large-data scenarios where MH corrections are computationally burdensome or infeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Devising SDEs with a Specified Target Stationary Distribution</head><p>Generically, all continuous Markov processes that one might consider for sampling can be written as a stochastic differential equation (SDE) of the form:</p><formula xml:id="formula_1">dz = f (z)dt + 2D(z)dW(t),<label>(2)</label></formula><p>where f (z) denotes the deterministic drift and often relates to the gradient of H(z), W(t) is a ddimensional Wiener process, and D(z) is a positive semidefinite diffusion matrix. Clearly, however, not all choices of f (z) and D(z) yield the stationary distribution p s (z) ∝ exp(-H(z)).</p><p>When D(z) = 0, as in HMC, the dynamics of Eq. ( <ref type="formula" target="#formula_1">2</ref>) become deterministic. Our exposition focuses on SDEs, but our analysis applies to deterministic dynamics as well. In this case, our frameworkusing the Liouville equation in place of Fokker-Planck-ensures that the deterministic dynamics leave the target distribution invariant. For ergodicity, a jump process must be added, which is not considered in our recipe, but tends to be straightforward (e.g., momentum resampling in HMC).</p><p>To devise a recipe for constructing SDEs with the correct stationary distribution, we propose writing f (z) directly in terms of the target distribution:</p><formula xml:id="formula_2">f (z) = -D(z) + Q(z) ∇H(z) + Γ(z), Γ i (z) = d j=1 ∂ ∂z j D ij (z) + Q ij (z) .<label>(3)</label></formula><p>Here, Q(z) is a skew-symmetric curl matrix representing the deterministic traversing effects seen in HMC procedures. In contrast, the diffusion matrix D(z) determines the strength of the Wienerprocess-driven diffusion. Matrices D(z) and Q(z) can be adjusted to attain faster convergence to the posterior distribution. A more detailed discussion on the interpretation of D(z) and Q(z) and the influence of specific choices of these matrices is provided in the Supplement.</p><p>Importantly, as we show in Theorem 1, sampling the stochastic dynamics of Eq. ( <ref type="formula" target="#formula_1">2</ref>) (according to Itô integral) with f (z) as in Eq. ( <ref type="formula" target="#formula_2">3</ref>) leads to the desired posterior distribution as the stationary distribution: p s (z) ∝ exp(-H(z)). That is, for any choice of positive semidefinite D(z) and skewsymmetric Q(z) parameterizing f (z), we know that simulating from Eq. (2) will provide samples from p(θ | S) (discarding any sampled auxiliary variables r) assuming the process is ergodic.</p><formula xml:id="formula_3">Theorem 1. p s (z) ∝ exp(-H(z)) is a stationary distribution of the dynamics of Eq. (2) if f (z) is restricted to the form of Eq. (3), with D(z) positive semidefinite and Q(z) skew-symmetric. If D(z)</formula><p>is positive definite, or if ergodicity can be shown, then the stationary distribution is unique.</p><p>Proof. The equivalence of p s (z) and the target p(z | S) ∝ exp(-H(z)) can be shown using the Fokker-Planck description of the probability density evolution under the dynamics of Eq. ( <ref type="formula" target="#formula_1">2</ref>) :</p><formula xml:id="formula_4">∂ t p(z, t) = - i ∂ ∂z i f i (z)p(z, t) + i,j ∂ 2 ∂z i ∂z j D ij (z)p(z, t) .<label>(4)</label></formula><p>Eq. ( <ref type="formula" target="#formula_4">4</ref>) can be further transformed into a more compact form <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>:</p><formula xml:id="formula_5">∂ t p(z, t) =∇ T • [D(z) + Q(z)] [p(z, t)∇H(z) + ∇p(z, t)] .<label>(5)</label></formula><p>We can verify that p(z | S) is invariant under Eq. ( <ref type="formula" target="#formula_5">5</ref>) by calculating e -H(z) ∇H(z) + ∇e -H(z) = 0. If the process is ergodic, this invariant distribution is unique. The equivalence of the compact form was originally proved in <ref type="bibr" target="#b15">[16]</ref>; we include a detailed proof in the Supplement for completeness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Completeness of the Framework</head><p>An important question is what portion of samplers defined by continuous Markov processes with the target invariant distribution can we define by iterating over all possible D(z) and Q(z)? In Theorem 2, we show that for any continuous Markov process with the desired stationary distribution, p s (z), there exists an SDE as in Eq. ( <ref type="formula" target="#formula_1">2</ref>) with f (z) defined as in Eq. ( <ref type="formula" target="#formula_2">3</ref>). We know from the Chapman-Kolmogorov equation <ref type="bibr" target="#b8">[9]</ref> that any continuous Markov process with stationary distribution p s (z) can be written as in Eq. ( <ref type="formula" target="#formula_1">2</ref>), which gives us the diffusion matrix D(z). Theorem 2 then constructively defines the curl matrix Q(z). This result implies that our recipe is complete. That is, we cover all possible continuous Markov process samplers in our framework. See Fig. <ref type="figure" target="#fig_0">1</ref>. Theorem 2. For the SDE of Eq. (2), suppose its stationary probability density function p s (z) uniquely exists, and that f i (z)p s (z) -</p><formula xml:id="formula_6">d j=1 ∂ ∂θ j D ij (z)p s (z)</formula><p>is integrable with respect to the Lebesgue measure, then there exists a skew-symmetric Q(z) such that Eq. (3) holds.</p><p>The integrability condition is usually satisfied when the probability density function uniquely exists.</p><p>A constructive proof for the existence of Q(z) is provided in the Supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Practical Algorithm</head><p>In practice, simulation relies on an -discretization of the SDE, leading to a full-data update rule</p><formula xml:id="formula_7">z t+1 ← z t -t D(z t ) + Q(z t ) ∇H(z t ) + Γ(z t ) + N (0, 2 t D(z t )).<label>(6)</label></formula><p>Calculating the gradient of H(z) involves evaluating the gradient of U (θ). For a stochastic gradient method, the assumption is that U (θ) is too computationally intensive to compute as it relies on a sum over all data points (see Sec. 2). Instead, such stochastic gradient algorithms examine independently sampled data subsets S ⊂ S and the corresponding potential for these data:</p><formula xml:id="formula_8">U (θ) = - |S| | S| x∈ S log p(x|θ) -log p(θ); S ⊂ S.<label>(7)</label></formula><p>The specific form of Eq. ( <ref type="formula" target="#formula_8">7</ref>) implies that U (θ) is an unbiased estimator of U (θ). As such, a gradient computed based on U (θ)-called a stochastic gradient <ref type="bibr" target="#b14">[15]</ref>-is a noisy, but unbiased estimator of the full-data gradient. The key question in many of the existing stochastic gradient MCMC algorithms is whether the noise injected by the stochastic gradient adversely affects the stationary distribution of the modified dynamics (using ∇ U (θ) in place of ∇U (θ)). One way to analyze the impact of the stochastic gradient is to make use of the central limit theorem and assume</p><formula xml:id="formula_9">∇ U (θ) = ∇U (θ) + N (0, V(θ)),<label>(8)</label></formula><p>resulting in a noisy Hamiltonian gradient ∇ H(z) = ∇H(z) + [N (0, V(θ)), 0] T . Simply plugging in ∇ H(z) in place of ∇H(z) in Eq. ( <ref type="formula" target="#formula_7">6</ref>) results in dynamics with an additional noise term (D(z t ) + Q(z t ) [N (0, V(θ)), 0] T . To counteract this, assume we have an estimate Bt of the variance of this additional noise satisfying 2D(z t )t Bt 0 (i.e., positive semidefinite). With small , this is always true since the stochastic gradient noise scales down faster than the added noise. Then, we can attempt to account for the stochastic gradient noise by simulating</p><formula xml:id="formula_10">z t+1 ← z t -t D(z t ) + Q(z t ) ∇ H(z t ) + Γ(z t ) + N (0, t (2D(z t ) -t Bt )).<label>(9)</label></formula><p>This provides our stochastic gradient-or minibatchvariant of the sampler. In Eq. ( <ref type="formula" target="#formula_10">9</ref>), the noise introduced by the stochastic gradient is multiplied by t (and the compensation by 2 t ), implying that the discrepancy between these dynamics and those of Eq. ( <ref type="formula" target="#formula_7">6</ref>) approaches zero as t goes to zero. As such, in this infinitesimal step size limit, since Eq. ( <ref type="formula" target="#formula_7">6</ref>) yields the correct invariant distribution, so does Eq. ( <ref type="formula" target="#formula_10">9</ref>). This avoids the need for a costly or potentially intractable MH correction. However, having to decrease t to zero comes at the cost of increasingly small updates. We can also use a finite, small step size in practice, resulting in a biased (but faster) sampler. A similar bias-speed tradeoff was used in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref> to construct MH samplers, in addition to being used in SGLD and SGHMC.</p><p>3 Applying the Theory to Construct Samplers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Casting Previous MCMC Algorithms within the Proposed Framework</head><p>We explicitly state how some recently developed MCMC methods fall within the proposed framework based on specific choices of D(z), Q(z) and H(z) in Eq. ( <ref type="formula" target="#formula_1">2</ref>) and (3). For the stochastic gradient methods, we show how our framework can be used to "reinvent" the samplers by guiding their construction and avoiding potential mistakes or inefficiencies caused by naïve implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hamiltonian Monte Carlo (HMC)</head><p>The key ingredient in HMC <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> is Hamiltonian dynamics, which simulate the physical motion of an object with position θ, momentum r, and mass M on an frictionless surface as follows (typically, a leapfrog simulation is used instead):</p><formula xml:id="formula_11">θ t+1 ← θ t + t M -1 r t r t+1 ← r t -t ∇U (θ t ).<label>(10)</label></formula><p>Eq. ( <ref type="formula" target="#formula_11">10</ref>) is a special case of the proposed framework with z = (θ, r), H(θ, r)</p><formula xml:id="formula_12">= U (θ) + 1 2 r T M -1 r, Q(θ, r) = 0 -I I 0 and D(θ, r) = 0.</formula><p>Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) As discussed in <ref type="bibr" target="#b5">[6]</ref>, simply replacing ∇U (θ) by the stochastic gradient ∇ U (θ) in Eq. ( <ref type="formula" target="#formula_11">10</ref>) results in the following updates:</p><p>Naive :</p><formula xml:id="formula_13">θ t+1 ← θ t + t M -1 r t r t+1 ← r t -t ∇ U (θ t ) ≈ r t -t ∇U (θ t ) + N (0, 2 t V(θ t )),<label>(11)</label></formula><p>where the ≈ arises from the approximation of Eq. ( <ref type="formula" target="#formula_9">8</ref>). Careful study shows that Eq. ( <ref type="formula" target="#formula_13">11</ref>) cannot be rewritten into our proposed framework, which hints that such a naïve stochastic gradient version of HMC is not correct. Interestingly, the authors of <ref type="bibr" target="#b5">[6]</ref> proved that this naïve version indeed does not have the correct stationary distribution. In our framework, we see that the noise term N (0, 2 t D(z)) is paired with a D(z)∇H(z) term, hinting that such a term should be added to Eq. ( <ref type="formula" target="#formula_13">11</ref>). Here,</p><formula xml:id="formula_14">D(θ, r) = 0 0 0 V(θ)</formula><p>, which means we need to add D(z)∇H(z) = V(θ)∇ r H(θ, r) = V(θ)M -1 r. Interestingly, this is the correction strategy proposed in <ref type="bibr" target="#b5">[6]</ref>, but through a physical interpretation of the dynamics. In particular, the term V(θ)M -1 r (or, generically, CM -1 r where C V(θ)) has an interpretation as friction and leads to second order Langevin dynamics:</p><formula xml:id="formula_15">θ t+1 ← θ t + t M -1 r t r t+1 ← r t -t ∇ U (θ t ) -t CM -1 r t + N (0, t (2C -t Bt )). (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>Here, Bt is an estimate of V(θ t ). This method now fits into our framework with H(θ, r) and Q(θ, r)</p><p>as in HMC, but with D(θ, r) = 0 0 0 C . This example shows how our theory can be used to identify invalid samplers and provide guidance on how to effortlessly correct the mistakes; this is crucial when physical intuition is not available. Once the proposed sampler is cast in our framework with a specific D(z) and Q(z), there is no need for sampler-specific proofs, such as those of <ref type="bibr" target="#b5">[6]</ref>.</p><p>Stochastic Gradient Langevin Dynamics (SGLD) SGLD <ref type="bibr" target="#b16">[17]</ref> proposes to use the following first order (no momentum) Langevin dynamics to generate samples</p><formula xml:id="formula_17">θ t+1 ← θ t -t D∇ U (θ t ) + N (0, 2 t D). (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>This algorithm corresponds to taking z = θ with H(θ) = U (θ), D(θ) = D, Q(θ) = 0, and Bt = 0. As motivated by Eq. ( <ref type="formula" target="#formula_10">9</ref>) of our framework, the variance of the stochastic gradient can be subtracted from the sampler injected noise to make the finite stepsize simulation more accurate. This variant of SGLD leads to the stochastic gradient Fisher scoring algorithm <ref type="bibr" target="#b0">[1]</ref>.</p><p>Stochastic Gradient Riemannian Langevin Dynamics (SGRLD) SGLD can be generalized to use an adaptive diffusion matrix D(θ). Specifically, it is interesting to take D(θ) = G -1 (θ), where G(θ) is the Fisher information metric. The sampler dynamics are given by</p><formula xml:id="formula_19">θ t+1 ← θ t -t [G(θ t ) -1 ∇ U (θ t ) + Γ(θ t )] + N (0, 2 t G(θ t ) -1 ). (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>Taking D(θ) = G(θ) -1 , Q(θ) = 0, and Bt = 0, this SGRLD <ref type="bibr" target="#b12">[13]</ref> method falls into our frame-</p><formula xml:id="formula_21">work with correction term Γ i (θ) = j ∂D ij (θ) ∂θ j</formula><p>. It is interesting to note that in earlier literature <ref type="bibr" target="#b9">[10]</ref>,</p><formula xml:id="formula_22">Γ i (θ) was taken to be 2 |G(θ)| -1/2 j ∂ ∂θ j G -1 ij (θ)|G(θ)| 1/2</formula><p>. More recently, it was found that this correction term corresponds to the distribution function with respect to a non-Lebesgue measure <ref type="bibr" target="#b17">[18]</ref>; for the Lebesgue measure, the revised Γ i (θ) was as determined by our framework <ref type="bibr" target="#b17">[18]</ref>. Again, we have an example of our theory providing guidance in devising correct samplers.</p><p>Stochastic Gradient Nosé-Hoover Thermostat (SGNHT) Finally, the SGNHT <ref type="bibr" target="#b6">[7]</ref> method incorporates ideas from thermodynamics to further increase adaptivity by augmenting the SGHMC system with an additional scalar auxiliary variable, ξ. The algorithm uses the following dynamics:</p><formula xml:id="formula_23">       θ t+1 ← θ t + t r t r t+1 ← r t -t ∇ U (θ t ) -t ξ t r t + N (0, t (2A -t Bt )) ξ t+1 ← ξ t + t 1 d r T t r t -1 .<label>(15)</label></formula><p>We can take z = (θ, r</p><formula xml:id="formula_24">= U (θ) + 1 2 r T r + 1 2d (ξ -A) 2 , D(θ, r, ξ) = 0 0 0 0 A • I 0 0 0 0 , and Q(θ, r, ξ) = 0 -I 0 I 0 r/d 0 -r T /d 0<label>, ξ), H(θ, r, ξ)</label></formula><p>to place these dynamics within our framework.</p><p>Summary In our framework, SGLD and SGRLD take Q(z) = 0 and instead stress the design of the diffusion matrix D(z), with SGLD using a constant D(z) and SGRLD an adaptive, θ-dependent diffusion matrix to better account for the geometry of the space being explored. On the other hand, HMC takes D(z) = 0 and focuses on the curl matrix Q(z). SGHMC combines SGLD with HMC through non-zero D(θ) and Q(θ) matrices. SGNHT then extends SGHMC by taking Q(z) to be state dependent. The relationships between these methods are depicted in the Supplement, which likewise contains a discussion of the tradeoffs between these two matrices. In short, D(z) can guide escaping from local modes while Q(z) can enable rapid traversing of low-probability regions, especially when state adaptation is incorporated. We readily see that most of the product space D(z) × Q(z), defining the space of all possible samplers, has yet to be filled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Gradient Riemann Hamiltonian Monte Carlo</head><p>In Sec. 3.1, we have shown how our framework unifies existing samplers. In this section, we now use our framework to guide the development of a new sampler. While SGHMC <ref type="bibr" target="#b5">[6]</ref> inherits the momentum term of HMC, making it easier to traverse the space of parameters, the underlying geometry of the target distribution is still not utilized. Such information can usually be represented by the Fisher information metric <ref type="bibr" target="#b9">[10]</ref>, denoted as G(θ), which can be used to precondition the dynamics. For our proposed system, we consider H(θ, r) = U (θ) + 1 2 r T r, as in HMC/SGHMC methods, and modify the D(θ, r) and Q(θ, r) of SGHMC to account for the geometry as follows:</p><formula xml:id="formula_25">D(θ, r) = 0 0 0 G(θ) -1 ; Q(θ, r) = 0 -G(θ) -1/2 G(θ) -1/2 0 .</formula><p>We refer to this algorithm as stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC).</p><p>Our theory holds for any positive definite G(θ), yielding a generalized SGRHMC (gSGRHMC) algorithm, which can be helpful when the Fisher information metric is hard to compute.</p><p>A naïve implementation of a state-dependent SGHMC algorithm might simply (i) precondition the HMC update, (ii) replace ∇U (θ) by ∇ U (θ), and (iii) add a state-dependent friction term on the order of the diffusion matrix to counterbalance the noise as in SGHMC, resulting in:</p><p>Naive :</p><formula xml:id="formula_26">θt+1 ← θt + tG(θt) -1/2 rt rt+1 ← rt -tG(θt) -1/2 ∇ θ U (θt) -tG(θt) -1 rt + N (0, t(2G(θt) -1 -t Bt)). (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>Algorithm 1: Generalized Stochastic Gradient Riemann Hamiltonian Monte Carlo initialize (θ 0 , r 0 ) for t = 0, 1, 2 • • • do optionally, periodically resample momentum r as r (t) ∼ N (0, I)  (two peaks), we compare the KL divergence of methods: SGLD, SGHMC, the naïve SGRHMC of Eq. ( <ref type="formula" target="#formula_26">16</ref>), and the gSGRHMC of Eq. ( <ref type="formula" target="#formula_29">17</ref> However, as we show in Sec. 4.1, samples from these dynamics do not converge to the desired distribution. Indeed, this system cannot be written within our framework. Instead, we can simply follow our framework and, as indicated by Eq. ( <ref type="formula" target="#formula_10">9</ref>), consider the following update rule:</p><formula xml:id="formula_28">θ t+1 ← θ t + t G(θ t ) -1/2 r t , Σ t ← t (2G(θ t ) -1 -t Bt ) r t+1 ← r t -t G(θ t ) -1/2 ∇ θ U (θ t ) + t ∇ θ (G(θ t ) -1/2 ) -t G(θ t ) -1 r t + N 0, Σ t</formula><formula xml:id="formula_29">θt+1 ← θt + tG(θt) -1/2 rt rt+1 ← rt -t[G(θ) -1/2 ∇ θ U (θt) + ∇ θ G(θt) -1/2 -G(θt) -1 rt] + N (0, t(2G(θt) -1 -t Bt)),<label>(17)</label></formula><p>which includes a correction term ∇ θ G(θ) -1/2 , with i-th component</p><formula xml:id="formula_30">j ∂ ∂θ j G(θ) -1/2 ij .</formula><p>The practical implementation of gSGRHMC is outlined in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In Sec. 4.1, we show that gSGRHMC can excel at rapidly exploring distributions with complex landscapes. We then apply SGRHMC to sampling in a latent Dirichlet allocation (LDA) model on a large Wikipedia dataset in Sec. 4.2. The Supplement contains details on the specific samplers considered and the parameter settings used in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Experiments</head><p>In this section we aim to empirically (i) validate the correctness of our recipe and (ii) assess the effectiveness of gSGRHMC. In Fig. <ref type="figure" target="#fig_2">2</ref>(left), we consider two univariate distributions (shown in the Supplement) and compare SGLD, SGHMC, the naïve state-adaptive SGHMC of Eq. ( <ref type="formula" target="#formula_26">16</ref>), and our proposed gSGRHMC of Eq. <ref type="bibr" target="#b16">(17)</ref>. See the Supplement for the form of G(θ). As expected, the naïve implementation does not converge to the target distribution. In contrast, the gSGRHMC algorithm obtained via our recipe indeed has the correct invariant distribution and efficiently explores the distributions. In the second experiment, we sample a bivariate distribution with strong correlation. The results are shown in Fig. <ref type="figure" target="#fig_2">2</ref>(right). The comparison between SGLD, SGHMC, and our gSGRHMC method shows that both a state-dependent preconditioner and Hamiltonian dynamics help to make the sampler more efficient than either element on its own. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Online Latent Dirichlet Allocation</head><p>We also applied SGRHMC (with G(θ) = diag(θ) -1 , the Fisher information metric) to an online latent Dirichlet allocation (LDA) <ref type="bibr" target="#b4">[5]</ref> analysis of topics present in Wikipedia entries. In LDA, each topic is associated with a distribution over words, with β kw the probability of word w under topic k. Each document is comprised of a mixture of topics, with π The goal of our analysis here is inference of the corpus-wide topic distributions β k . Since the Wikipedia dataset is large and continually growing with new articles, it is not practical to carry out this task over the whole dataset. Instead, we scrape the corpus from Wikipedia in a streaming manner and sample parameters based on minibatches of data. Following the approach in <ref type="bibr" target="#b12">[13]</ref>, we first analytically marginalize the document distributions π (d) and, to resolve the boundary issue posed by the Dirichlet posterior of β k defined on the probability simplex, use an expanded mean parameterization shown in Figure <ref type="figure" target="#fig_4">3</ref>(upper left). Under this parameterization, we then compute ∇ log p(θ|x) and, in our implementation, use boundary reflection to ensure the positivity of parameters θ kw . The necessary expectation over word-specific topic indicators z (d) j is approximated using Gibbs sampling separately on each document, as in <ref type="bibr" target="#b12">[13]</ref>. The Supplement contains further details.</p><p>For all the methods, we report results of three random runs. When sampling distributions with mass concentrated over small regions, as in this application, it is important to incorporate geometric information via a Riemannian sampler <ref type="bibr" target="#b12">[13]</ref>. The results in Fig. <ref type="figure" target="#fig_4">3</ref>(right) indeed demonstrate the importance of Riemannian variants of the stochastic gradient samplers. However, there also appears to be some benefits gained from the incorporation of the HMC term for both the Riemmannian and non-Reimannian samplers. The average runtime for the different methods are similar (see Fig. <ref type="figure" target="#fig_4">3</ref>(lower left)) since the main computational bottleneck is the gradient evaluation. Overall, this application serves as an important example of where our newly proposed sampler can have impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a general recipe for devising MCMC samplers based on continuous Markov processes. Our framework constructs an SDE specified by two matrices, a positive semidefinite D(z) and a skew-symmetric Q(z). We prove that for any D(z) and Q(z), we can devise a continuous Markov process with a specified stationary distribution. We also prove that for any continuous Markov process with the target stationary distribution, there exists a D(z) and Q(z) that cast the process in our framework. Our recipe is particularly useful in the more challenging case of devising stochastic gradient MCMC samplers. We demonstrate the utility of our recipe in "reinventing" previous stochastic gradient MCMC samplers, and in proposing our SGRHMC method. The efficiency and scalability of the SGRHMC method was shown on simulated data and a streaming Wikipedia analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The red space represents the set of all continuous Markov processes. A point in the black space represents a continuous Markov process defined by Eqs. (2)-(3) based on a specific choice of D(z), Q(z). By Theorem 1, each such point has stationary distribution p s (z) = p(z | S). The blue space represents all continuous Markov processes with p s (z) = p(z | S). Theorem 2 states that these blue and black spaces are equivalent (there is no gap, and any point in the blue space has a corresponding D(z), Q(z) in our framework).</figDesc><graphic coords="4,156.02,90.14,54.61,54.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: For two simulated 1D distributions defined by U (θ) = θ 2 /2 (one peak) and U (θ) = θ 4 -2θ 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) relative to the true distribution in each scenario (left and right bars labeled by 1 and 2). Right: For a correlated 2D distribution with U (θ1, θ2) = θ 4 1 /10 + (4 • (θ2 + 1.2) -θ 2 1 ) 2 /2, we see that our gSGRHMC most rapidly explores the space relative to SGHMC and SGLD. Contour plots of the distribution along with paths of the first 10 sampled points are shown for each method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Upper Left: Expanded mean parameterization of the LDA model. Lower Left: Average runtime per 100 Wikipedia entries for all methods. Right: Perplexity versus number of Wikipedia entries processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>k</head><label></label><figDesc>the probability of topic k in document d. Documents are generated by first selecting a topic z (d) j ∼ π (d) for the jth word and then drawing the specific word from the topic as x (d) j ∼ β z (d) j . Typically, π (d) and β k are given Dirichlet priors.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by ONR Grant N00014-10-1-0746, NSF CAREER Award IIS-1350133, and the TerraSwarm Research Center sponsored by MARCO and DARPA. We also thank Mr. Lei Wu for helping with the proof of Theorem 2 and Professors Ping Ao and Hong Qian for many discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian posterior sampling via stochastic gradient Fisher scoring</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML&apos;12)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed stochastic gradient MCMC</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahbaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 31st International Conference on Machine Learning (ICML&apos;14)</title>
		<meeting>eeding of 31st International Conference on Machine Learning (ICML&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards scaling up Markov chain Monte Carlo: An adaptive subsampling approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;14)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data subsampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning (ICML&apos;15)</title>
		<meeting>the 31th International Conference on Machine Learning (ICML&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic gradient Hamiltonian Monte Carlo</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 31st International Conference on Machine Learning (ICML&apos;14)</title>
		<meeting>eeding of 31st International Conference on Machine Learning (ICML&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian sampling using stochastic gradient thermostats</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Skeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS&apos;14)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">Introduction to Probability Theory and its Applications</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Riemann manifold Langevin and Hamiltonian Monte Carlo methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calderhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="214" />
			<date type="published" when="2011">03 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Austerity in MCMC land: Cutting the Metropolis-Hastings budget</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;14)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="113" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic gradient Riemannian Langevin dynamics on the probability simplex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS&apos;13)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Fokker-Planck Equation: Methods of Solutions and Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Risken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation of a new interpretation of stochastic differential equations to Itô process</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient Langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML&apos;11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Langevin diffusions and the Metropolis-adjusted Langevin algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xifara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sherlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Existence and construction of dynamical potential in nonequilibrium processes without detailed balance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and General</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page">8593</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zwanzig</surname></persName>
		</author>
		<title level="m">Nonequilibrium Statistical Mechanics</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
