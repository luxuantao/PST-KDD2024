<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Coupled Dictionary Hashing for Fast Cross-Media Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qitian@cs.utsa.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of ITEE</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">SIGIR&apos;14</orgName>
								<address>
									<addrLine>July 6-11, Gold Coast</addrLine>
									<postCode>2014</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Coupled Dictionary Hashing for Fast Cross-Media Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEA80F165468BCF630987AA8C477F665</idno>
					<idno type="DOI">10.1145/2600428.2609563</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Information Search and Retrieval Coupled dictionary learning</term>
					<term>Cross-media retrieval</term>
					<term>Hashing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-media hashing, which conducts cross-media retrieval by embedding data from different modalities into a common low-dimensional Hamming space, has attracted intensive attention in recent years. The existing cross-media hashing approaches only aim at learning hash functions to preserve the intra-modality and inter-modality correlations, but do not directly capture the underlying semantic information of the multi-modal data. We propose a discriminative coupled dictionary hashing (DCDH) method in this paper. In DCDH, the coupled dictionary for each modality is learned with side information (e.g., categories). As a result, the coupled dictionaries not only preserve the intra-similarity and inter-correlation among multi-modal data, but also contain dictionary atoms that are semantically discriminative (i.e., the data from the same category is reconstructed by the similar dictionary atoms). To perform fast cross-media retrieval, we learn hash functions which map data from the dictionary space to a low-dimensional Hamming space. Besides, we conjecture that a balanced representation is crucial in cross-media retrieval. We introduce multi-view features on the relatively "weak" modalities into DCDH and extend it to multi-view DCDH (MV-DCDH) in order to enhance their representation capability. The experiments on two real-world data sets show that our DCDH and MV-DCDH outperform the state-of-the-art methods significantly on cross-media retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the rapid development of Internet and social network, it has attracted increasing attention to study the correlations among multi-modal data. For example, an uploaded image on the Flickr web site is always tagged with some related descriptions or labels; a microblog may consist of a short text and correlative images. The relevant data from different modalities may have semantic correlations. Therefore, it is desirable to support cross-media retrieval across the data of different modalities, e.g., the retrieval of semantically-related textual documents in response to a query image and vice versa. Due to the large-scale nature of the existing multimedia data over the Internet, efficient retrieval of cross-media is particularly important.</p><p>An effective way to speed up the similarity search is the hashing-based method, which makes a tradeoff between accuracy and efficiency by approximate nearest neighbor search. The principle of hashing method is to map the high dimensional data into compact hash codes and generate the same or similar hash codes for similar data.</p><p>The motivation of hashing is to solve the approximate nearest neighbor (ANN) search problem. However, in the cross-media retrieval, the NN cannot be directly obtained as the data may come from different modalities. Therefore, most of the existing hashing approaches are not applicable to cross-media retrieval and cross-media hashing method should be specifically studied.</p><p>Generally speaking, the existing hashing approaches can be classified into three categories:</p><p>• uni-modal hashing: uni-modal hashing utilizes only a single type of feature (homogeneous feature) from uni-modal data as input, aiming at learning hash functions to project the homogeneous feature to compact hash codes.</p><p>• multi-view hashing: multi-view hashing utilizes multiple types of features (heterogeneous features) from uni-modal data as input, and learns hash functions to project the heterogenous features to hash codes.</p><p>• cross-media hashing: cross-media hashing utilizes data from multi-modalities (e.g, images and texts) as input, and preserves the intra-modality similarity and inter-modality correlation to learn hash functions. Thus, the correlation of the data from different modalities is measurable.</p><p>Most of the existing hashing approaches are uni-modal hashing. One of the well-known uni-modal hashing method is Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b2">[2]</ref>, which uses random projections to obtain the hash functions. However, due to the limitation of random projection, LSH usually needs a quite long hash code and hundreds of hash tables to guarantee good retrieval performance. To make the hash codes compact, several learning based approaches are proposed. <ref type="bibr">Weiss et al.</ref> proposed Spectral Hashing (SH) <ref type="bibr" target="#b23">[23]</ref> which utilizes the distribution of training data and uses eigenfunction to obtain the hash functions. Compared with LSH, SH achieves better performance since the hash functions capture the manifold structure of the data. Since then, many extensions of SH have been proposed <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>However, in the real world applications, we can extract heterogenous features from the data and some multi-view hashing approaches are therefore leveraged to boost the retrieval performance <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b17">17]</ref>. The principal idea of them is to learn the hash functions while preserving the local structures of each individual feature and globally considering the consistency of multi-view features.</p><p>Cross-media hashing is a new research area and there has been only limited research efforts focusing on it so far <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b25">25]</ref>. Most of the existing cross-media hashing approaches share the common idea of learning different hash functions individually for each modality and map the data from different modalities to a shared low-dimensional Hamming space. However, such a binary embedding strategy often results in poor indexing performance for the shared embedding space is not semantically discriminative, which is significantly important for cross-media retrieval.</p><p>In this paper, we propose a cross-media hashing framework titled Discriminative Coupled Dictionary Hashing (D-CDH). Firstly, data from different modalities along with their classes or categories are jointly utilized to learn the both discriminative and coupled dictionaries. The discriminative capability indicates that data from same category will have similar sparse representation (i.e., sparse codes), and the coupling means not only intra-modality similarity but also inter-modality correlation will be preserved. As a result, DCDH assigns an explicit semantic meaning (i.e., topic) to each dictionary atom in multi-modal dictionaries and thus makes the sparse representation for the multi-modal data interpretable. Secondly, the obtained sparse codes for the data over their corresponding dictionary are exploited to learn the hash functions and further transform the sparse codes to compact binary hash codes.</p><p>Furthermore, we find that the representation capability of the dictionaries from different modalities varies and an "unbalanced" representation may adversely influence the performance of cross-media hashing. To address this problem, we additionally incorporate multi-view features into DCDH to enhance the representation capability of the dictionaries from the relatively "weak" modalities. This extended version of DCDH is named Multi-View DCDH (MV-DCDH).</p><p>The main contributions of this paper are three-fold:</p><p>• We propose a two-stage cross-media hashing framework consisting of the learning of discriminative coupled dictionaries and hash functions, respectively. The learned discriminative coupled dictionaries in the first stage have both discriminative and similarity-preserving capability.</p><p>• The discriminative coupled dictionary learning is formulated as an optimization problem of submodular function and an approximation solution can be efficiently obtained using a greedy algorithm.</p><p>• Multi-view and multi-modal data are jointly considered in the MV-DCDH framework. The multi-view features is incorporated to strengthen the representation capability for the dictionary from a relatively "weak" modality and lead to a balanced cross-media representation. This enhancement improves the crossmedia retrieval performance of DCDH significantly.</p><p>The rest of the paper is organized as follows: In Section 2, we review the related work of dictionary learning and cross-media hashing approaches. In Section 3, we give out the detailed explaination of our DCDH and its multi-view extension MV-DCDH. The complexity of DCDH is analyzed in Section 4. Experimental results and comparisons on two real-world data sets are demonstrated in Section 5. Finally, the conclusions are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dictionary Learning</head><p>Beyond the traditional dictionary learning approaches <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b1">1]</ref>, coupled or semi-coupled dictionary learning approaches <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b22">22]</ref> attempt to learn dictionaries for multi-modal data by minimizing the reconstruction error of each dictionary and preserving the pairwise correspondence across different modalities. However, these approaches are unsupervised so that the class or category information is not exploited and can not significantly boost the performance of learned coupled dictionaries. Zhuang et al. proposed a supervised semi-coupled dictionary learning approach which introduces the category side information into multi-modal dictionary learning via a ℓ2,1-norm regularization term.</p><p>Our proposed DCDH bears some resemblance to submodular dictionary learning (SDL) <ref type="bibr" target="#b7">[7]</ref> that takes advantage of the submodularity to learn dictionary efficiently. We extend the idea from uni-modal data into multi-modal data in order to learn discriminative coupled dictionaries .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-media Hashing</head><p>Cross-media retrieval is a hot research focus in recent years <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b31">31]</ref>. With the rapid advance of hashing, some cross-media hashing approaches have been proposed <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>The problem of cross-media hashing was first proposed by Bronstein et al. in CMSSH <ref type="bibr" target="#b3">[3]</ref>. Specifically, given two modalities of data sets, CMSSH learns two groups of hash functions to ensure that if two data points (with different modalities) are relevant, their corresponding hash codes are similar and otherwise dissimilar. However, CMSSH only preserves the inter-modality correlation but ignores the intra-modality similarity. Kumar et al. extended Spectral Hashing <ref type="bibr" target="#b23">[23]</ref> from the traditional uni-modal setting to the multi-modal scenario and proposed CVH <ref type="bibr" target="#b9">[9]</ref>. CVH attempts to generate the hash codes by minimizing the distance of hash codes for the similar data and maximizing the distance for the dissimilar data. The inter-view and intra-view similarities are both preserved in CVH. LCMH <ref type="bibr" target="#b30">[30]</ref> adopts a "two-stage" strategy to learn the cross-media hash functions: First, the data within each modality are low-rank represented using the anchor graph <ref type="bibr" target="#b11">[11]</ref>. Then, hash functions for each modality are learned to project the data from each anchor graph space into a shared Hamming space. MLBE employs a probabilistic generative model to encode the intra-similarity and inter-similarity of data across multiple modalities. According to the estimation of maximum a posteriori, the binary latent factors can be obtained and then be taken as the hash codes in MLBE. However, the hash codes generated by MLBE do not require the independency between different hash bits, and may obtain highly redundant hash bits.</p><p>Wu et al. introduced dictionary learning into cross-media hashing <ref type="bibr" target="#b25">[25]</ref>. By the joint modeling of the intra-modality similarity and inter-modality correlation among multi-modal data with a hypergraph, the coupled dictionaries with a hypergraph laplacian regularizer are learned in an iterative manner. The learned dictionary for each modality is then adopted as the hash functions, and the sparse code of each data point over its corresponding dictionary is regarded as the hash code to perform cross-media retrieval.</p><p>Inspired by the effectiveness of using the coupled dictionary space to represent the data points from different modalities, we additionally emphasize discrimination when learning coupled dictionaries in order to make the shared dictionary space interpretable. Furthermore, unlike <ref type="bibr" target="#b25">[25]</ref> that directly exploits the sparse codes as the hash codes, we further learn hash functions to map the sparse codes to binary hash codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE OVERVIEW OF DCDH</head><p>In this section, we introduce the detail of DCDH. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the algorithmic flowchart of our DCDH. For the sake of illustrative simplicity, we assume that only two kinds of data (e.g., images and texts) are available in Figure <ref type="figure" target="#fig_0">1</ref>. The proposed DCDH mainly consists of the following two stages:</p><p>1. Discriminative coupled dictionary learning: In Figure <ref type="figure" target="#fig_0">1</ref>, the clusters of multi-modal data can be learned by a submodular function with the help of inter-modality correlation, intra-modality similarity as well as the supervised side information (e.g.,category labels). Given a cluster, its centroid is taken as a dictionary atom of their corresponding dictionary. That is to say, the dictionary atom from one modality is coupled with the corresponding dictionary atoms from the other modalities in the same cluster.</p><p>2. Unified hash functions learning: Based on the learned coupled dictionaries, the data points from different modalities can be represented as sparse codes in a unified dictionary space. Afterwards, utilizing the sparse property, hash functions which project the sparse codes into compact binary hash codes can be learned efficiently.</p><p>The notations used in this paper are listed in Table <ref type="table" target="#tab_0">1</ref>. the dimensionality of each modality</p><formula xml:id="formula_0">X 1 ,...,X M data set X m = [x m 1 , x m 2 , ..., x m Nm ] ∈ R p m ×N D 1 ,...,D M dictionary D m = [d m 1 , d m 2 , ..., d m K ] ∈ R p m ×K Z 1 ,...,Z M sparse codes Z m ∈ R K×N of X m w.r.t. D m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unified Graph Representation of Labeled Multi-modal Data</head><p>To well model the intra-modality similarity and the intermodality correlation of M data sets, we resort to the unified graph G(V, E, w) similar to <ref type="bibr" target="#b19">[19]</ref>. The vertex set V denotes the data from all the data sets, and the edge set E models the pairwise intra-modality similarity or the inter-modality correlation between data points. The weight of an edge is measured by some similarity functions which we will discuss in the following.</p><p>To model the intra-modality similarity within the same modality, we adopt the local similarity metric with a Gaussian kernel. The intra-modality similarity w m i,j of two data points x m i and x m j from modality m is defined as:</p><formula xml:id="formula_1">w m i,j =    e - |x m i -x m j | 2 2σ 2 , if x m i ∈ N k (x m j ) or x m i ∈ N k (x m j ) 0 otherwise<label>(1)</label></formula><p>where NK(x) represents the set of k-nearest neighbours of</p><formula xml:id="formula_2">x and σ = 1 N i,j |x m i -x m j | 2</formula><p>is the expectation over all the pairwise distance in X m .</p><p>It takes O(N 2 p m ) time to compute an intra-modality similarity matrix. When N is large, we can use some approximated methods such as the anchor graph structure to construct this similarity matrix efficiently <ref type="bibr" target="#b11">[11]</ref>. In this paper, we simply use the exact k-NN graph in Eq. <ref type="bibr" target="#b1">(1)</ref>.</p><p>To model the inter-modality correlation of the data from two modalities (we name them as modality a and b, a = b and a, b ∈ {1, 2, ..., M }), the similarity function w a,b i,j for two data x a i and x b j is defined as:</p><formula xml:id="formula_3">w a,b i,j = 1, if x a i has known correlation with x b j 0, otherwise<label>(2)</label></formula><p>Moreover, to better understand the semantics of data, we additionally exploit the category information. Let C be the category-labels set indicating the category label of each data point (i.e., each vertex in G), the final category-labeled unified graph is denoted as G(V, E, w, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminative Coupled Dictionary Learning</head><p>Given the category-labeled graph G(V, E, w, C), we attempt to jointly learn the discriminative coupled dictionaries D 1 ,...,D M for the data from each modality. The coupling of the dictionaries indicates that these dictionaries have the same number of atoms (i.e, K) and the dictionary atoms from M modalities have a one-to-one correspondence Without loss of generality, we assume that there are two modalities of data (represented as squares and circles). The data points with the same stripe have the same category label (e.g., 'sports', 'biology', 'history'). Given multi-modal data, the submodular dictionary learning is utilized to obtain discriminative coupled dictionaries. The "discriminative" capability is reflected by the fact that each dictionary atom is assigned a dominant category label to enhance its interpretability (i.e., category 'sport' for G1 and 'history' for G2). The "coupling" means that each dictionary atom in one modality has its counterpart dictionary atom in another modality. The coupled dictionary atoms are combined to characterize the multi-modal data. Each data point from a given modality can be sparsely represented as a sparse code using its corresponding dictionary. Finally, hash functions is learned to transform the sparse codes to binary hash codes.</p><p>(paired dictionary atoms), since the paired dictionary atoms have their different intrinsic power to characterize the multimodal data. Moreover, the paired dictionary atoms are discriminative in terms of semantics (i.e., category ) and is consistent with only one category label. That is to say, the data from different modalities are semantically aligned in a shared coupled dictionary space.</p><p>Inspired by the efficiency and effectiveness of submodular dictionary learning approach <ref type="bibr" target="#b7">[7]</ref>, we formulate our discriminative coupled dictionary learning as a graph partition problem on G(V, E, w, C). Learning coupled dictionaries with size K is equal to partitioning the category-labeled graph G into K subgraphs which can be further regarded as a problem of selecting a subset A of the edge set E (i.e., A ⊆ E) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">10]</ref>. We can formulate an objective function with respect to A and maximize it to obtain the optimal partitions. Our objective function has the property of submodularity and thus can be approximately optimized with an efficient greedy algorithm.</p><p>Our objective function consists of three parts which corresponds to the following requirements: 1) each subgraph should be compact so that the obtained dictionaries have a good representative capability; 2) each subgraph is encouraged to be discriminative so that the sparse representation of the data over learned dictionary (i.e. using the centroid to represent subgraphs), from the same category to be similar; 3) to avoid the over-fitting on the subgraphs' size (some subgraphs may be extremely large while the others are so tiny), the size of each subgraph is in balance (nearly equal).</p><p>Compact Function: The entropy rate of the random walk over the graph G is exploited to obtain the compact subgraphs. The entropy rate measures the uncertainty of a stochastic process S = {St|t ∈ T } where T is an index set. For a discrete random process, the entropy rate is defined as an asymptotic measure as: H(S) = limt→∞H(St|St-1, ..., S1), which is the conditional entropy of the last random variable given the past. In the case of a stationary 1st-order Markov chain, the entropy rate is:</p><formula xml:id="formula_4">H(S) = limt→∞H(St|St-1) = limt→∞H(S2|S1) = H(S2|S1).</formula><p>We define the random walk model on graph G as S = {St|t ∈ T }. The transition probability from the vertex vi to the vertex vj is defined as pi,j = P r(St+1 = vj |St = vi) = wi,j /wi where wi = k:e i,k ∈E w i,k is the sum of incident weights of the vertex vi, and the stationary distribution is defined as:</p><formula xml:id="formula_5">µ = (µ1, µ2, ..., µ |V | ) T = ( w1 w all , w2 w all , ..., w |V | w all ) T<label>(3)</label></formula><p>where w all = |V | i=1 wi is the sum of incident weights of all vertices. The entropy rate of the random walk is defined as:</p><formula xml:id="formula_6">H(S) = H(S2|S1) = i µiH(S2|S1 = vi) = -i µi j Pi,jlogPi,j<label>(4)</label></formula><p>Leaving µ in Eq.(3) intact, the set functions for the transition probability Pi,j : 2 E → R w.r.t. A are defined as:</p><formula xml:id="formula_7">Pi,j (A) =        w i,j w i , if i = j and ei,j ∈ A 0, if i = j and ei,j / ∈ A 1 - j:e i,j ∈A w i,j w i , if i = j<label>(5)</label></formula><p>Consequently, the compact function with respect to A can be defined as the entropy rate of the random walk on G:</p><formula xml:id="formula_8">H(A) = - i µi j Pi,j (A)logPi,j (A)<label>(6)</label></formula><p>Given the entropies of the transition probabilities, maximizing the entropy rate in Eq.( <ref type="formula" target="#formula_8">6</ref>) encourages the edges with large weights (small distance) to be selected <ref type="bibr" target="#b7">[7]</ref>. Hence the compact function H(A) can generate compact subgraphs.</p><p>Discriminative Function: To encourage the discrimination of subgraphs which further guarantees the sparse representation of the data from the same category to be similar, a discriminative function on G is proposed <ref type="bibr" target="#b7">[7]</ref>.</p><p>Let A be the selected edge set, NA be the number of subgraphs with respect to A, the partition of graph G with selected edge set A is GA = {G1, ..., GN A } where each Gi is a subgraph. We construct a count matrix N = [N 1 , ..., N N A ] ∈ R c×N A for the count of each category label of the data assigned to each subgraph and c is the number of the categories of the multi-modal data set. Each</p><formula xml:id="formula_9">N i = [N i 1 , ..., N i c ] T ∈ R c where N i</formula><p>c is the number of data points from the c-th category assigned to i-th subgraph. It is worth noting that the size of the count matrix N is dynamic since NA changes when new edges are added to the selected edge set A.</p><p>The purity for each subgraph Gi is defined as: P(Gi) = 1 C i maxyN i y where y ∈ {1, 2, ..., c} is the category label, Ci = c y=1 N i y is the count for data points of all categories assigned to subgraph Gi. The overall purity of GA is:</p><formula xml:id="formula_10">P(GA) = N A i=1 Ci C total P(Gi) = N A i=1 1 C total maxyN i y (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where C total = i Ci = |V | is the sum of the count of all subgraphs. The discriminative function is defined as:</p><formula xml:id="formula_12">D(A) = P(GA) -NA = N A i=1 1 C total maxyN i y -NA<label>(8)</label></formula><p>D(A) measures the discriminative capability of the subgraphs. Maximizing D(A) encourages each subgraph to have a consistent category label, i.e., the data within each subgraph are expected to have the same category label.</p><p>Balancing Function: If we only use the compact and discriminative functions, there may exist some extreme cases where the majority of data belong to one subgraph and the other data are sporadically dispersed. This makes the learned dictionary suffer from over-fitting. Therefore, a balancing function is used to regularize the subgraphs of similar sizes.</p><p>Denote pA as the distribution of the subgraph membership, pA is formulated as:</p><formula xml:id="formula_13">pA(i) = |Gi| i |Gi| , i = {1, 2, ..., NA}<label>(9)</label></formula><p>The balancing function is defined using the entropy maximum theory:</p><formula xml:id="formula_14">B(A) = - i pA(i)log(pA(i)) -NA<label>(10)</label></formula><p>The aforementioned three functions are proved to be monotonically increasing and submodular with respect to A [10] <ref type="bibr" target="#b7">[7]</ref>. Furthermore, It has been proved that the linear combination of submodular function is still submodular <ref type="bibr" target="#b14">[14]</ref>. Therefore, we define an overall function F = H(A) + λD(A) + γB(A) which is also monotonically increasing and submodular. The optimal solution of F(A) is achieved by maximizing the objective function with best A as:</p><formula xml:id="formula_15">max A H(A) + λD(A) + γB(A) s.t.</formula><p>A ⊆ E and NA ≥ K</p><p>where λ and γ control the contribution of the three terms.</p><p>Follow the settings of <ref type="bibr" target="#b7">[7]</ref>, we set λ = maxe i,j H(e i,j )-H(∅) maxe i,j D(e i,j )-D(∅) λ ′ and γ = maxe i,j H(e i,j )-H(∅) maxe i,j B(e i,j )-B(∅) γ ′ , where λ ′ and γ ′ are predefined parameters. NA ≥ K is a constraint on the number of subgraphs which enforces exactly K subgraphs since the objective function is monotonically increasing.</p><p>Directly maximizing Eq.( <ref type="formula" target="#formula_16">11</ref>) is a NP-hard problem. However, since F(A) is a submodular function, we can obtain an approximate solution by a simple greedy algorithm, which gives a 1  2 -approximation lower bound on the optimality of the solution <ref type="bibr" target="#b14">[14]</ref>. When the optimal K subgraphs are obtained, we simply use the center of the data within each subgraph as the corresponding dictionary atom. Since each subgraph consists of the data from M modalities respectively, M coupled dictionary atoms are obtained. The coupled dictionaries of the common size K are generated based on all the subgraphs. The overall algorithm of the discriminative coupled dictionary learning is summarized in Algorithm 1.</p><p>Note that the weights of the intra-modality edges w m i,j ∈ (0, 1] are not larger than the inter-modality edges w a,b i,j = 1, and the two vertices connected by the inter-modality edges are within the same category. Therefore, adding an intermodality edge satisfies the discriminative function and the compact function at the same time and each inter-modality edge has a high probability to be selected out at the early iterations in the step 4 of Algorithm 1. This observation is valuable which ensures that within each subgraph, there is at least one data point for every modality and the trivial zero-value dictionary atoms is avoided. for m = 1 to M do 9:</p><p>V m i = {vj |vj ∈ Gi and vj from modality m} 10:</p><formula xml:id="formula_17">D m ← D m ∪ { 1 |V m i | j:v j ∈V m i vj } 11:</formula><p>end for 12: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unified Hash Function Learning</head><p>As the coupled dictionary for each modality is learned, the data from each modality can be encoded as sparse codes using its corresponding learned dictionary.</p><p>For the data points in data set X m , their K-dimensional sparse codes Z m can be efficiently computed using the dictionary D m as follows:</p><formula xml:id="formula_18">min Z m X m -D m Z m 2 F + β Z m 1 s.t. Z m ≥ 0<label>(12)</label></formula><p>The non-negative constraint on Z m is needed for the following hash functions learning step. Eq.( <ref type="formula" target="#formula_18">12</ref>) is a simple non-negative LASSO problem <ref type="bibr" target="#b18">[18]</ref>, and we use the efficient LARS <ref type="bibr" target="#b4">[4]</ref> solver to solve this problem. Moreover, despite of different choices of β, the sparsity (maximum number of the zero-elements) of Z can be well controlled by LARS. This is helpful since we expect the sparsity of each sparse code to be equivalent. The sparsity is set to 0.9 (i.e., 90% elements of a sparse code are 0) throughout the paper. By solving the Eq.( <ref type="formula" target="#formula_18">12</ref>) for the data set of each modality, the sparse codes Z<ref type="foot" target="#foot_0">1</ref> , ..., Z M are correspondingly generated. Denote Z = [Z 1 , ..., Z M ] ∈ R K×M N as the joint sparse codes for all M data sets , we intend to further learn hash functions which linearly projects each sparse code zi ∈ Z onto Ldimensional compact binary hash codes (L &lt; K).</p><p>The commonly used hash function learning strategy is based on graph-laplacian <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b27">27]</ref>, etc. The hash functions are learned by solving an eigenvalue-decomposition problem of a laplacian matrix which takes O(N 3 ) time. However, it is infeasible to learn hash functions when N is large. Therefore, we adopt the hash function learning strategy based on the sparse characteristic of sparse codes.</p><p>Since Z is non-negative, we can use Z (each column has been ℓ1 normalized) to construct an approximate adjacency matrix Ŵ = Z T Λ -1 Z ∈ R N×N where Λ = diag(Z1) ∈ R K×K <ref type="bibr" target="#b11">[11]</ref>. The approximate adjacency matrix Ŵ is: 1) nonnegative and sparse; 2) low-rank (the rank is at most K); 3) double stochastic, i.e., has unit row and column sum. Afterwards, the laplacian matrix is formulated as L = I -Ŵ where I is the identity matrix.</p><p>The optimal hash functions can be acquired as the L eigenvectors with smallest eigenvalues of the approximated laplacian matrix L (removing the trivial eigenvector corresponds to eigenvalue 0), which is equal to L eigenvectors with largest eigenvalues of Ŵ . Due to the low-rank property of Ŵ , a smaller matrix Q = Λ -1/2 ZZ T Λ -1/2 ∈ R K×K is substituted for eigenvalue-decomposition problem on L. By solving the eigen-system of Q, L largest eigenvectoreigenvalue pairs{(v k , σ k )} L k=1 where 1 &gt; σ1 ≥ ...σL &gt; 0 are obtained. Denote V = [v1, v2, ..., vL] ∈ R K×L and Σ = diag(σ1, σ2, ..., σL) ∈ R L×L , the hash functions are defined as follows:</p><formula xml:id="formula_19">h(z) = sign(P T z)<label>(13)</label></formula><p>where</p><formula xml:id="formula_20">P = √ M N Λ -1/2 V Σ -1/2 ∈</formula><p>R K×L is the normalized projection matrix, z ∈ R K is a sparse code and sign(•) is the binary function.</p><p>When given a new data point, its hash code can be consequently generated using a two-stage mechanism: for example, given a data point x m from modality m, it is first nonlinearly transformed to a sparse code z m using its corresponding dictionary D m similar with Eq.( <ref type="formula" target="#formula_18">12</ref>). After that, with the learned projection matrix P , z m is linearly transformed to a L-dimensional compact binary hash code using the learned hash functions in Eq.( <ref type="formula" target="#formula_19">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-View Enhancement</head><p>It is natural that the representation capability of the dictionaries for different modalities varies widely. For example, the representing capability of the dictionary for a text modality is much stronger than the one for an image modality. This "unbalanced" representation may lead to an unsatisfying cross-media retrieval performance. Therefore, we incorporate the multi-view representation into our coupled dictionary learning to enhance the representing capability of the relatively "weak" modalities.</p><p>Without loss of generality, assuming we have two modalities a and b, for modality a, we have a single-view feature X a ; for modality b, we extract multi-view (e.g., 2 views) features X b 1 and X b 2 . The construction of category-labeled unified graph G(V, E, w, C) is similar with the aforementioned methods. The size of vertex set does not change since each vertex represents one data point. The edge set E is expanded as some relations between the vertices in V are added with the introduction of multi-view features.</p><p>The multi-view discriminative coupled dictionary learning method is similar to the DCDH in Algorithm (1) except for the generation of coupled dictionaries. For the single-view modality a, its corresponding dictionary D a is learned; for the multi-view modality b, two dictionaries D b 1 and D b 2 are learned, respectively.</p><p>For modality a, we use the dictionary D a to generate sparse codes Z a for the data set X a by Eq.( <ref type="formula" target="#formula_18">12</ref>). For modality b, we use the dictionaries D b 1 and D b 2 to jointly learn the sparse codes Z b as follows:</p><formula xml:id="formula_21">min Z b i=1,2 X b i -D b i Z b 2 F + β Z b 1 ⇔ X b -D b Z b 2 F + β Z b 1 s.t. Z b ≥ 0 (14)</formula><p>where Z b is the sparse codes over the multi-view dictionaries,</p><formula xml:id="formula_22">X b = [X b 1 ; X b 2 ] ∈ R (p 1 +p 2 )×N , D b = [D b 1 ; D b 2 ] ∈ R (p 1 +p 2 )×K .</formula><p>Here, p 1 , p 2 denote the dimensionality of the two views. The optimization of Eq.( <ref type="formula">14</ref>) is similar to Eq.( <ref type="formula" target="#formula_18">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">COMPLEXITY ANALYSIS</head><p>Our DCDH approach consists of an off-line stage to learn the discriminative coupled dictionaries and unified hash functions; an on-line stage to encode an out-of-sample data point into a binary hash code. We detail the time complexity for each part respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Off-line training</head><p>Discriminative coupled dictionary learning: Leaving out the time for constructing the graph G(V, E, w, C) which takes O( M m=1 N 2 p m ) time, the complexity of discriminative coupled dictionary learning can be implemented efficiently. Using a well designed heap structure, the ideal time complexity is O(|V |log|V |) (i.e., O(M N logM N )) <ref type="bibr" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified hash functions learning:</head><p>To learn the unified hash functions V , we first learn the sparse codes of Z 1 , ..., Z M using Eq. <ref type="bibr" target="#b12">(12)</ref>. which can be solved in O( M m=1 N Kp m ) time using the LARS algorithm <ref type="bibr" target="#b4">[4]</ref>. However, the generation of each sparse code is independent which can be solved in O(p m K) time, some parallel implementation can be adopted to solve the problem efficiently 1 . After the sparse codes for all training data are obtained, an eigensystem of a small matrix Q ∈ R K×K is solved in O(K 3 ) time to obtain the projection matrix W and corresponding hash functions. Therefore, the overall unified hash functions learning step can be very efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">On-line hash encoding</head><p>The on-line hash encoding step should be fast enough to support the cross-media retrieval over the large scale data set. The time for encoding a new data point is two-fold:  Sparse Coding: Given a new data point x m from modality m, its sparse code z m is obtained similar as Eq. <ref type="bibr" target="#b12">(12)</ref>. Therefore, the time complexity is O(p m K).</p><p>Binary Embedding: The linear transformation from a sparse code z m to a binary hash code is achieved by the hash functions in Eq.( <ref type="formula" target="#formula_19">13</ref>) which takes O(KL) time.</p><p>An intuitive comparison of DCDH and other state-of-theart cross-media hashing methods on off-line training and online testing time are demonstrated in Figure <ref type="figure" target="#fig_3">2</ref>. We can see that our DCDH and MV-DCDH require the least time in the training stage and is also very efficient in the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In our experiments, we evaluate the performance of our D-CDH. We first introduce the data set, evaluation criteria and the parameter setting we used in the experiments. Then, we compare our DCDH with other state-of-the-art methods and analyze the results. Finally, we further investigate the learned coupled dictionary space to explain why our DCDH and MV-DCDH achieve the superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We use two real-world data sets "Wikipedia-Picture of the Day"(abbreviated as Wiki-Potd) 2 and NUS-WIDE 3 . Both data sets are bi-modal containing images and texts.</p><p>The Wiki-Potd data set consists of 2866 Wikipedia documents. Each document contains one text-image pair. All documents are labeled by one of 10 semantic categories. For the image modality, we extract 1000-D Bag of visual words (BoVW) and 512-D GIST descriptors for each image. For the text modality, we calculate the frequency of all words in the data set and select the most representative words to quantize all texts into 5,000-D Bag-of-Words (BoW).</p><p>The NUS-WIDE data set contains 269,648 labeled images and is manually annotated with 81 categories. Each image with its annotated tags in NUS-WIDE can be taken as a pair of image-text data. To guarantee that each category has abundant training samples, we select those pairs that belong to one of the 10 largest categories (e.g., 'sky', 'buildings', 'person') with each pair exclusively belonging to one of the 10 categories (discrimination on concepts are required when learning coupled dictionaries.). For the image modality, over three types of visual features are extracted for each image 2 http://www.svcl.ucsd.edu/projects/crossmodal/ 3 http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm (i.e., 500-D BoVW, 255-D Color Moments, 128-D Wavelet Texture). For the text modality, the corresponding labels of each image are represented by a 1,000-D BoW.</p><p>The details of the two data sets are shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>To evaluate the performance of the cross-media retrieval results, we adopt the mean average precision (MAP) and mean average top-R precision (MAP@R) defined in <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Methods</head><p>We perform three types of retrieval schemes in the experiments : 1) Image-query-Texts: use image queries to retrieve relevant texts. 2) Text-query-Images: use text queries to retrieve relevant images. 3) Image-query-Images: use image queries to retrieve relevant images. For the first two retrieval schemes, we compare with the state-of-the-art crossmedia hashing methods CMSSH <ref type="bibr" target="#b3">[3]</ref>, CVH <ref type="bibr" target="#b9">[9]</ref>, MLBE <ref type="bibr" target="#b28">[28]</ref>, LCMH <ref type="bibr" target="#b30">[30]</ref>; for the third retrieval scheme, we additionally compare with some uni-modal hashing approaches: SH <ref type="bibr" target="#b23">[23]</ref>, KLSH <ref type="bibr">[8]</ref>, AGH <ref type="bibr" target="#b11">[11]</ref> and a multi-view hashing approach MFH <ref type="bibr" target="#b17">[17]</ref>. The reason why we don't compare DCDH with the approaches in <ref type="bibr" target="#b31">[31]</ref> and <ref type="bibr" target="#b25">[25]</ref> is that they can not generate compact and binary hash codes, thus their performance can not be fairly evaluated under the same settings.</p><p>Our DCDH method and its multi-view enhancement are denoted as DCDH and MV-DCDH, respectively.</p><p>For the Image-query-Texts and Text-query-Images retrieval schemes, the performances of CMSSH, CVH, MLBE, L-CMH, DCDH, MV-DCDH are compared. Except for our MV-DCDH which induces multi-view features, the remaining methods take the BoVW descriptors for the image modality and BoW for text modality; for MV-DCDH, the multiview features are utilized for image modality.</p><p>For the Image-query-Images retrieval scheme, we compare with all the counterparts aforementioned. It is notable that for MFH, the multi-view features are exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameters Sensitivity</head><p>There are four parameters in DCDH: the k-NN of the intra-modality; λ ′ , γ ′ in Eq.( <ref type="formula" target="#formula_16">11</ref>) when learning the coupled dictionaries; the size of the coupled dictionaries K.</p><p>Following the prior settings in <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">10]</ref>, λ ′ and γ ′ are set to 10 and 1 respectively throughout the experiments.</p><p>We fix the code length L = 24 and evaluate the average MAP variations (the average MAP scores of Imagequery-Texts and Text-query-Images) in terms of K and k-NN on the validation set. The tested combinations are K = {50, 100, 200, 300, 400, 500} and k-NN = {5, 10, 20, 30, 50}, The optimal combination on NUS-WIDE is k-NN = 5, K = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Comparisons</head><p>We compare our DCDH and its extension MV-DCDH with the three following methods: CMSSH <ref type="bibr" target="#b3">[3]</ref>, CVH <ref type="bibr" target="#b9">[9]</ref> and MLBE <ref type="bibr" target="#b28">[28]</ref>. We evaluate the cross-media retrieval performance with code length varying from 8 to 40 and report results in terms of MAP in Table <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>, respectively. Moreover, we report the Image-query-Images retrieval performance in terms of MAP@50 in Figure <ref type="figure" target="#fig_5">3</ref>. The reason why we choose MAP@50 rather than MAP for the Image-query-Images task is that the differences of MAP scores, especially for the counterparts, are not statically significant, which makes it difficult to illustrate them explicitly in a line graph.</p><p>The MAP scores on Wiki-Potd data set is generally lower than that on NUS-WIDE even if the Wiki-Potd data set contains rich textual information. This may be explained as that the categories of Wiki-Potd data set (e.g., 'art', 'biology') is too general, so the feature vector can not precisely capture its corresponding semantic meaning.  It can be noted that DCDH significantly outperforms the counterparts over different code lengths: 2% ∼ 7% on Wiki-Potd data set and 2% ∼ 5% on NUS-WIDE, respectively. The improvement is due to the effectiveness of the sparse representation over the discriminative coupled dictionaries. All the counterparts simply project the data from different modalities into a shared Hamming space using the learned hash functions. However, the meaning of the shared Hamming space is ambiguous and cannot well clarify the semantic information of the data. By contrast, our DCDH exploits the category information when learning the coupled dictionaries, thus making the coupled dictionary space semantic interpretable. By utilizing the distribution of the sparse codes, the manifold structure of the dictionary space is also preserved in the embedding Hamming space. Therefore, the binary hash codes represent the semantic information of the data and lead to superior cross-media retrieval performance. Moreover, we find that the incorporation of the multi-view features on the image modality, i.e., MV-DCDH, produces a significant improvement over DCDH on both of the Imagequery-Texts and the Text-query-Images tasks. This observation is explained as that exploiting multiple features over the "weak" modality (image modality) gives better understanding of the semantics of the images. This observation also verifies our hypothesis that a balanced representation is important in cross-media retrieval.</p><p>Although our main goal is cross-media retrieval, we can readily use the hash functions we learned to perform unimodal retrieval. That is to say, all the cross-media hashing approaches can be adapted to uni-modal hashing. We conduct the task of Image-query-Images and report the performance of the aforementioned cross-media hashing approaches. Besides, we add some state-of-the-art uni-modal hashing and multi-view hashing approaches to perform fair experimental comparison. The results are shown in Figure <ref type="figure" target="#fig_5">3</ref>.</p><p>From Figure <ref type="figure" target="#fig_5">3</ref>, we find that all the cross-media hashing approaches except CMSSH achieve reasonable performance. This is due to the fact that the learned hash functions for image modality can borrow strength from text modality. The observation for the poor performance of CMSSH in this task may be explained as the lack of intra-modality preservation when learning hash functions in CMSSH.</p><p>In addition, since MV-DCDH induces multi-features, we also add MFH <ref type="bibr" target="#b17">[17]</ref> into comparison which also exploits multifeatures when learning hash functions. The results show that MFH outperforms the other uni-modal hashing approaches and most of the cross-media hashing approaches, which demonstrates the effectiveness of multi-views in image understanding. Our MV-DCDH slightly outperforms </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The Discriminative Capability of the Coupled Dictionary Space</head><p>The results over both the data sets above outline the superior performance of DCDH over the other cross-media hashing approaches. This superiority mainly owes to the aptitude of the discriminative capability of the coupled dictionary space in DCDH. To verify our hypothesis, we investigate the coupled dictionary space. We choose the Wiki-Potd data set since it has rich textual information and is convenient for illustrations. To show the effect of discriminative capability of the coupled dictionary space, we design a non-discriminative version of DCDH by simply setting λ ′ = 0. The rest settings are same as the ones used aforementioned (K = 300 and k-NN = 20).</p><p>Denote the learned coupled dictionaries for image and text modalities as D x amd D y , respectively. Z x and Z y are the sparse codes of the test data set in two modalities by the corresponding dictionaries. To measure the discrimination of the sparse codes, we define a metric called Discriminative Degree (abbreviated as DD) as follows:</p><formula xml:id="formula_23">DD(Z) = 1 N N i=1 P (zi) (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where N is the number of the testing samples. P (zi) = 1 if at least one of the selected dictionary atoms of the sparse code zi indicates the true category label of the i-th data point and 0 otherwise. Moreover, DD can also be used to measure the coupling degree of the sparse codes from different modalities. Denote the dot product of Z x and Z y as Z xy and DD(Z xy ) reflects the one-to-one correspondence of the paired dictionary atoms .</p><p>Figure <ref type="figure" target="#fig_7">4</ref> shows the comparison results. The "Random" method indicates that the dictionaries are randomly generated; The "Non-discriminative" method is the aforementioned DCDH with λ ′ = 0. The "Discriminative" and "MV-Discriminative" methods correspond to our DCDH and MV-DCDH. From the results, we get four observations: 1) the sparse codes of the text modality is more semantically discriminative than the ones from the image modality (even when we do not impose the discriminative constraints); 2) The introduction of side information improves the discriminative capability especially for the image modality (without the category side information, the DD score of the image modality is almost equal to the random method); 3) The  coupling degree is significantly improved when imposing the discriminative constraint. This is mainly due to the better representation of the image modality; 4) Exploiting multiview features further improves the performance on both the understanding of the image modality and the coupling degree.</p><p>To give an intuitive illustration of the learned dictionary, we give an insight into the textual dictionary D y (the image dictionary D x is learned from the BoVW, which is difficult to illustrate). Since each dictionary atom d y k is obtained by clustering a portion of BoW features, the values of the elements in d y k measure the occurrence frequency of the words. Naturally, we can use the elements with largest values in d y k to represent the topic words of this dictionary atom d y k . Moreover, each d y k is learned with a discriminative constraint and has a dominated category label, we demonstrate the relation between the category label and topic words of the dictionary atoms in Table <ref type="table" target="#tab_4">5</ref> (topic words correspond to the 5 largest elements of the selected dictionary atoms). From the results, we can find that the topic words for each dictionary atom indeed reflect some certain semantic information and are consistent with its belonging category. Besides, two dictionary atoms belong to the same category have an explicit disparity in semantics. e.g., the two atoms from the category "Biology" describe different topics of "Biology": the first topic is about dinosaurs and the second one is about whale killing. This observation can be explained as the collaborative effect of the compact function and discriminative function. The discriminative function encourages the topics to be classified into the correct categories and the compact function encourages each topic to reflect an individual aspect of its corresponding category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we propose a discriminative coupled dictionary hashing (DCDH) approach for fast cross-media retrieval. Our DCDH is two-stage in that we first learn coupled dictionary for each modality discriminatively with the side information of category labels, so that the data from different modalities are represented as the sparse codes in a shared semantically discriminative dictionary space. Afterwards, the sparse codes are mapped to binary hash codes by the learned unified hash functions to support fast crossmedia retrieval. Extensive experiments on two real-world data sets demonstrate the superior performance of DCDH over the existing state-of-the-art hashing approaches.</p><p>Moreover, we conjecture that a balanced cross-media representation benefits the cross-media retrieval performance.</p><p>Therefore, we extend DCDH to MV-DCDH which introduces multi-view features on the relatively "weak" modalities (i.e., the image modality in our experiments) to obtain a balanced representation. The experimental results verify the effectiveness of MV-DCDH.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The algorithmic flowchart of DCDH. Without loss of generality, we assume that there are two modalities of data (represented as squares and circles). The data points with the same stripe have the same category label (e.g., 'sports', 'biology', 'history'). Given multi-modal data, the submodular dictionary learning is utilized to obtain discriminative coupled dictionaries. The "discriminative" capability is reflected by the fact that each dictionary atom is assigned a dominant category label to enhance its interpretability (i.e., category 'sport' for G1 and 'history' for G2). The "coupling" means that each dictionary atom in one modality has its counterpart dictionary atom in another modality. The coupled dictionary atoms are combined to characterize the multi-modal data. Each data point from a given modality can be sparsely represented as a sparse code using its corresponding dictionary. Finally, hash functions is learned to transform the sparse codes to binary hash codes.</figDesc><graphic coords="4,59.69,57.89,99.52,161.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Discriminative Coupled Dictionary LearningInput: data sets X 1 ,...,X M , λ ′ , γ ′ , K Output: The learned coupled dictionaries D 1 ,...,D M1: Construct unified graph with labeled multi-modal data G(V, E, w, C) for the data sets X 1 ,...,X M 2: Initialization: A ← ∅, D 1 , ..., D M ← ∅ 3: while NA &gt; K do 4:e * = argmax e∈E F(A ∪ {e}) -F(A)5:A ← A ∪ {e * } 6: end while # generation of coupled dictionaries. 7: for each subgraph Gi in G do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The time cost of the training and testing stages for DCDH, MV-DCDH and other crossmedia hashing approaches. The experiments are conducted on Wiki-Potd data set with dictionary size K = 100 and MV-DCDH uses two views of features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The performance comparison of Imagequery-Images on two data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The discriminative capability comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>: Notations used in this paper</cell></row><row><cell>Symbols</cell><cell>Explanation</cell></row><row><cell>M</cell><cell>the number of modalities</cell></row><row><cell>m K</cell><cell>m ∈ {1, 2, ...M } is one of the M modalities the common size of the coupled dictionaries</cell></row><row><cell>L</cell><cell>the length of the hash codes</cell></row><row><cell>N</cell><cell>the common size of each data set</cell></row><row><cell>p 1 ,...,p M</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The details of the data sets used in the experiments Partitions are ordered by query/database set respectively. The query set are random sampled from the database set.</figDesc><table><row><cell>Data Set</cell><cell>NUS-WIDE</cell><cell>Wiki-Potd</cell></row><row><cell>Image modality</cell><cell>BoVW(500-D)</cell><cell>BoVW(1000-D)</cell></row><row><cell></cell><cell>CM(255-D)</cell><cell>GIST(512-D)</cell></row><row><cell></cell><cell>Wavelet(128-D)</cell><cell>-</cell></row><row><cell>Text modality</cell><cell>BoW(1000-D)</cell><cell>BoW(5000-D)</cell></row><row><cell>Data set size</cell><cell>60641</cell><cell>2866</cell></row><row><cell>Training set size</cell><cell>3000</cell><cell>1000</cell></row><row><cell>Validation set size*</cell><cell>2000 /10000</cell><cell>866/866</cell></row><row><cell>Testing set size*</cell><cell>2000 / 47641</cell><cell>866/1000</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The MAP performance comparison on the NUS-WIDE data set with code length L varying from 8 to 40. The items in bold are the two best results, and the results with asterisk are the best.</figDesc><table><row><cell>Task</cell><cell>Methods</cell><cell cols="3">Hash code length L = 8 L = 24 L = 40</cell></row><row><cell></cell><cell>CVH</cell><cell>0.3144</cell><cell>0.3139</cell><cell>0.3140</cell></row><row><cell>Image</cell><cell>CMSSH</cell><cell>0.3233</cell><cell>0.3131</cell><cell>0.3140</cell></row><row><cell>query</cell><cell>MLBE</cell><cell>0.3142</cell><cell>0.3138</cell><cell>0.3119</cell></row><row><cell>Texts</cell><cell>LCMH</cell><cell>0.3163</cell><cell>0.3117</cell><cell>0.3144</cell></row><row><cell></cell><cell>DCDH</cell><cell>0.3608</cell><cell>0.3573</cell><cell>0.3568</cell></row><row><cell></cell><cell cols="4">MV-DCDH 0.3645* 0.3627* 0.3608*</cell></row><row><cell>Task</cell><cell>Methods</cell><cell cols="3">Hash code length L = 8 L = 24 L = 40</cell></row><row><cell></cell><cell>CVH</cell><cell>0.3158</cell><cell>0.3152</cell><cell>0.3144</cell></row><row><cell>Text</cell><cell>CMSSH</cell><cell>0.3373</cell><cell>0.3309</cell><cell>0.3287</cell></row><row><cell>query</cell><cell>MLBE</cell><cell>0.3133</cell><cell>0.3156</cell><cell>0.3167</cell></row><row><cell>Images</cell><cell>LCMH</cell><cell>0.3142</cell><cell>0.3124</cell><cell>0.3133</cell></row><row><cell></cell><cell>DCDH</cell><cell>0.3452</cell><cell>0.3559</cell><cell>0.3554</cell></row><row><cell></cell><cell cols="4">MV-DCDH 0.3640* 0.3629* 0.3604*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The MAP performance comparison on the Wiki-Potd data set.</figDesc><table><row><cell>Task</cell><cell>Methods</cell><cell cols="3">Hash code length L = 8 L = 24 L = 40</cell></row><row><cell></cell><cell>CVH</cell><cell>0.1490</cell><cell>0.1407</cell><cell>0.1381</cell></row><row><cell>Image</cell><cell>CMSSH</cell><cell>0.1448</cell><cell>0.1407</cell><cell>0.1431</cell></row><row><cell>query</cell><cell>MLBE</cell><cell>0.1445</cell><cell>0.1359</cell><cell>0.1371</cell></row><row><cell>Texts</cell><cell>LCMH</cell><cell>0.1267</cell><cell>0.1273</cell><cell>0.1258</cell></row><row><cell></cell><cell>DCDH</cell><cell>0.1821</cell><cell>0.1985</cell><cell>0.1934</cell></row><row><cell></cell><cell cols="4">MV-DCDH 0.2017* 0.2010* 0.1997*</cell></row><row><cell>Task</cell><cell>Methods</cell><cell cols="3">Hash code length L = 8 L = 24 L = 40</cell></row><row><cell></cell><cell>CVH</cell><cell>0.1435</cell><cell>0.1361</cell><cell>0.1351</cell></row><row><cell>Text</cell><cell>CMSSH</cell><cell>0.1412</cell><cell>0.1364</cell><cell>0.1380</cell></row><row><cell>query</cell><cell>MLBE</cell><cell>0.1449</cell><cell>0.1344</cell><cell>0.1363</cell></row><row><cell>Images</cell><cell>LCMH</cell><cell>0.1260</cell><cell>0.1237</cell><cell>0.1235</cell></row><row><cell></cell><cell>DCDH</cell><cell>0.1606</cell><cell>0.1648</cell><cell>0.1620</cell></row><row><cell></cell><cell cols="4">MV-DCDH 0.1745* 0.1734* 0.1716*</cell></row><row><cell cols="5">100 and k-NN = 20, K = 300 on Wiki-Potd. These settings</cell></row><row><cell cols="4">are adopted in the following experiments.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Topic words and corresponding category lab els of some selected dictionary atoms on the Wiki-Potd data set.</figDesc><table><row><cell>Categories</cell><cell>Topic Words</cell></row><row><cell>Biology</cell><cell>Skull Dinosaurs Bone Fossils Prey Whales Killer Meat Oil Atlantic</cell></row><row><cell>Sport</cell><cell>Goals Hockey Players Montreal NHL Football Yard Bowl Champion Quarter</cell></row><row><cell>Warfare</cell><cell>Natives Crops Australian Infantry Landing Soviet, Moscow Marshall Defense Battle</cell></row><row><cell>Media</cell><cell>Theater Broadway Movie Actor Disc</cell></row><row><cell>Geography</cell><cell>Creek Tree Parks Ridge Forest</cell></row><row><cell cols="2">the MFH and achieves the overall best performance in the</cell></row><row><cell cols="2">Image-query-Images task.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://spams-devel.gforge.inria.fr/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENT</head><p>This work is supported in part by National Basic Research Program of China (2012CB316400), NSFC (No.61128007), 863 program (2012AA012505), the Fundamental Research Funds for the Central Universities and Chinese Knowledge Center of Engineering Science and Technology (CKCEST) and Program for New Century Excellent Talents in University. Dr.Qi Tian is also supported by ARO grant W911NF-12-1-0057, Faculty Research Award by NEC Laboratories of America, and 2012 UTSA START-R Research Award respectively. Dr. Jiebo is also supported by Google Faculty Research Awards.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data fusion through cross-modality metric learning using similarity-sensitive hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3594" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">Least angle regression. The Annals of Sstatistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image transformation based on learning dictionaries across image spaces</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Submodular dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3418" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2130" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hash functions for cross-view similarity search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1360" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entropy rate superpixel segmentation</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2097" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spline regression hashing for fast image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A low rank structural large margin method for cross-modal ranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions ֒ a li</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparing apples to oranges: a scalable solution with heterogeneous hashing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple feature hashing for real-time large scale near-duplicate video retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A general framework for manifold alignment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for scalable image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3424" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic hashing using tags and topic modeling</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2216" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing. In NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse multi modal hashing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="427" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Composite hashing with multiple information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-taught hashing for fast similarity search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A probabilistic model for multimodal hash function learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-regularized hashing for multimodal data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1385" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linear cross-modal hashing for efficient multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised coupled dictionary learning with group structures for multi-modal retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining semantic correlation of heterogeneous multimedia data for cross-media retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
