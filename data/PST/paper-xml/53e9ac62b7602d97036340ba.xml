<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composite Multimedia and Active Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Simon</forename><surname>Gibbs</surname></persName>
							<email>simon@cui.unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Centre Universitaire d&apos;Informatique</orgName>
								<orgName type="institution">Université de Genève</orgName>
								<address>
									<addrLine>12 rue du Lac</addrLine>
									<postCode>1207</postCode>
									<settlement>Geneva</settlement>
									<country key="CH">SWITZERLAND</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Composite Multimedia and Active Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC7563F95DA81FE8A5652171F7CB05E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An object-oriented framework for composite multimedia is described. In analogy to constructing complex graphics entities from graphics primitives and geometric transformations, composite multimedia is constructed from multimedia primitives and temporal transformations. Active objects based on real-time processes are proposed as multimedia primitives. Their combination to form composite multimedia and the requisite temporal transformations are illustrated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The improving capabilities of multimedia workstations, combined with the development of multimedia recording formats (for example, CD-I and DVI, see <ref type="bibr" target="#b13">[14]</ref> for a recent description) is likely to increase the demand for applications involving multimedia. However the development of multimedia applications is presently hampered by two key problems: First, multimedia involves concepts from audio recording, video production, animation, and music -concepts that are novel to many programmers. Second, multimedia operations often involve special hardware, leading to lack of portability and longer development times.</p><p>One way of alleviating these two problems is by providing programmers with a high-level programming "framework" <ref type="bibr" target="#b3">[4]</ref> for dealing with multimedia. The framework should:</p><p>• be based on a simple conceptual model of multimedia functionality, yet one which is general enough to capture the variety of multimedia, including sound, video, music, and animated sequences,</p><p>• be easy to use and not require expertise in multimedia technology, yet be open and extensible so that more experienced programmers are not constrained,</p><p>• encapsulate hardware dependencies,</p><p>• allow complex multimedia effects, for example the synchronization of an audio and video signal, or the juxtaposition of two video signals.</p><p>The following describes such a framework and introduces constructs, based on active objects, for multimedia programming. In the next section we discuss just what is multimedia and motivate our model and the use of active objects. Section 3 discusses multimedia primitives and the basic operations they support. Combination of multimedia primitives to form composite multimedia is discussed in Section 4, examples of programming with composite multimedia are also given. Finally we review related work and discuss future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Objects and Multimedia</head><p>The term "multimedia" often appears in descriptions of systems -there are multimedia documents, multimedia interfaces, and multimedia applications. As To appear in Proc. <ref type="bibr">OOPSLA'91.</ref> long as the document, interface, or application deals somehow with multimedia information, the description is viewed as justified. But the question arises, then, as to just what constitutes multimedia information.</p><p>One possible view of multimedia information is based on multimedia data types. For example, a multimedia data type CDaudio could consist of all possible audio signals as encoded on a compact disc, i.e., CDaudio values would be sequences of the form sample i where the data format used by the samples would be specified by the CD recording format. As with other data types, one could then define operations for CDaudio, for example operations dealing with compression, filtering, or concatenation.</p><p>An interesting aspect of the data type view is the notion of multimedia data being based on sequences. This appears to hold for multimedia in general (see Table <ref type="table" target="#tab_0">1</ref>). Perhaps this is not surprising since a characteristic of multimedia information is its temporal nature, and a sequence of samples is one of the basic representations of a time varying signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This suggests the following definition for multimedia data values:</head><p>Definition: A multimedia value, v, of data type D, is a (finite) sequence d i , where the encoding and interpretation of the d i are governed by D. In particular D determines how the presentation of v (the physical realization of v, within some medi- The above definition is somewhat vague about the meaning of r D . Generally r D should be viewed as a measure of the "flow" of values of type D. In many cases this rate can be associated with the number of sequence values presented per unit time. For example, CD audio requires some 44 thousand samples per second and NTSC video involves about 30 frames per second. In other cases r D may indicate an ideal presentation rate -presentation may then take place at a different or varying rate, perhaps with loss of quality. Also there may be data types where the d i contain explicit timing information. In such cases, although the interval from one d i to the next will vary, there is likely to be a maximum data rate which can then be associated with r D .</p><p>The above definition associates each multimedia value with a lower-level sequence, there are, however, cases where the sequence may not be known a priori (i.e., before presentation) or may not be directly accessible by software:</p><p>• Presentation may involve synthesis. For example, a pure audio tone might be generated from a sine function rather than from a stored sequence of audio samples.</p><p>• We may want to associate a multimedia value with an on-going activity (for instance, a recording session). Here the sequence is not known until the activity has completed.</p><p>• Many computer-controllable video products, including VCRs, video disc players, and cameras, have analog inputs or outputs. It is important to support such devices within the framework, so we would like to allow multimedia values which exist only as analog signals. However these signals are not directly accessible by software (i.e., they must be digitized before being processed).</p><p>The last case can be accounted for by allowing analog signals as multimedia data types, with the understanding that the underlying d i are unavailable within a program. Values of these types are software handles;</p><p>they are used for referring to the inputs and outputs of hardware devices which in turn manipulate the analog signal.</p><p>The first two cases involve processes (the sine generator and the recording session) that produce multimedia values. These processes are likely to have their own internal state (e.g., the frequency of the sine generator) and their own operations (e.g., change the frequency, increase the recording volume). To allow programmers to conveniently handle such cases we need a construct which combines the notion of data, behavior and process.</p><p>An appropriate programming language construct appears to be the active object <ref type="bibr">[5][17]</ref>. Active objects, like ordinary or passive objects, have state (instance variables) and behavior (methods). In addition, an active object may spontaneously perform actions, even if no messages have been sent to the object. There are many ways in which concurrency can be added to an object-oriented programming language in order to support active objects <ref type="bibr" target="#b12">[13]</ref>. Here we assume a simple model where objects are multi-threaded (i.e., simultaneous invocations of an object's methods do not block). We also assume that synchronization of threads within an object is the responsibility of the programmer.</p><p>Using active objects, we can then define multimedia objects as follows:</p><p>Definition: A multimedia object is an active object which produces and/or consumes multimedia values (of specified types) at their associated data rates.</p><p>There are a number of observations we can make from this definition. First, multimedia objects are subject to real-time constraints in that they have to produce or consume data at particular rates. Second, each multimedia object can be viewed as a collection of ports. A port has a (multimedia) data type and is used either for input or output. A multimedia object's ports (the number, their data type and direction) would be determined by the class of the object. Third, we can divide multimedia objects into three categories: sources, sinks, and filters. A source produces multimedia values (i.e., has only output ports), a sink consumes val-ues (has only input ports), and a filter both produces and consumes (has both input and output ports).</p><p>It will be convenient to use a graphical notation for multimedia objects. We will represent such objects by circles (see Figure <ref type="figure" target="#fig_0">1</ref>) with attached boxes for their ports (extruding for output ports and intruding for input ports).</p><p>We next give some examples of classes of multimedia objects. For each we list the class name, port specification (whether input or output and the associated data type), and give a brief description of what the class's instances do. In general, these classes are parameterized by the data types of their input and output ports. In the following port specifications we make use of generic multimedia data types, such as Audio, for sequences of sounds, Video, for analog video signals, and Multimedia, for any multimedia data type. In practice, before instantiating these classes, it would be necessary to bind the ports to specific data types, e.g., CDaudio or NTSC.</p><p>AudioIn (output port: Audio) Reads an audio input device (for example, an analog to digital converter) and produces an Audio value.</p><p>AudioOut (input port: Audio) Writes values from its input port to an audio output device (e.g., a digital to analog converter).</p><p>AudioFileRead (output port: Audio) Produces an Audio value by reading samples stored in a file.</p><p>AudioMix (input ports: Audio, Audio; output port: Audio) Combines (e.g., by averaging or summing) two Audio values to produce a third.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VCR (input port: Video)</head><p>Records a Video value on a video cassette. source filter sink</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multimedia Primitives</head><p>Multimedia objects belonging to classes such as those listed above are examples of what we will call multimedia primitives. In general a multimedia primitive is a multimedia object which cannot be decomposed into other multimedia objects. Is this section we look at the operations supported by these primitives. In the next section we will look at composite multimedia, i.e., objects where such a decomposition is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimedia Hierarchies</head><p>Multimedia primitives make use of two hierarchiesa supertype/subtype hierarchy of data types and a class inheritance hierarchy. Figure <ref type="figure" target="#fig_1">2</ref> shows some possible multimedia data types. (It would also be possible to implement multimedia values as passive objects. However, we prefer to retain the data type terminology since it stresses that the internal representation of a multimedia value is not likely to be a matter for the implementor to decide, rather it is likely to be fixed by standard or convention.)</p><p>Figure <ref type="figure" target="#fig_1">2</ref> is not intended to be complete but merely to give some indication of the richness of the hierarchy. The division into five high-level subtypes: Duplicates its input on two output ports.</p><p>Perform (input port: Music, output port: Audio) A Perform object takes from its input port a sequence of musical events (for instance, a MIDI 1 stream) and synthesizes an Audio value.</p><p>Render (input port: Scene; output port: Image) A Render object graphically renders a sequence of scene descriptions (e.g., PEX 2 requests) producing a sequence of images.</p><p>FrameBuffer (input port: Image; output port: Video) A FrameBuffer object converts an image into a video signal.</p><p>1. MIDI, or Musical Instrument Digital Interface, is a standard for communicating with musical devices.</p><p>2. PEX is an extension to the X Window System protocol that supports 3d graphics. base-types) is also somewhat arbitrary and incomplete. Other data types could be added if needed. Some possibilities are static text, static graphics or static images (all sequences of length 1); moving text (e.g., as in a film's credits); or data from specialized acquisition devices (e.g., volume data produced by a CAT scanner). However, the five base-types give a good range and are common enough to be supported on many platforms.</p><p>Similarly the multimedia class hierarchy in Figure <ref type="figure" target="#fig_3">3</ref> is also incomplete. Since many of these classes correspond to hardware devices there may be different specializations for different models and manufacturers (illustrated here by the two subclasses of VideoDisc). Another source of variety is the range of filter objects. For example, one product 1 supports over 100 digital video effects; in our framework each of these would correspond to a subclass of DVE. Finally, many of the classes in Figure <ref type="figure" target="#fig_3">3</ref> are parameterized by the multimedia data types of their input and output ports.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Operations on Multimedia Primitives</head><p>Multimedia object classes inherit methods from Acti-veObject and MultimediaObject. A partial specification of ActiveObject, written in C++, is: The methods of ActiveObject provide basic activity control for a multimedia object. The two subclasses of ActiveObject, DeviceObject and ProcessObject, differ on how these methods are implemented but do not add additional methods.</p><p>The class MultimediaObject introduces a more interesting set of methods. These methods make use of two temporal coordinate systems: world time and object time. The origin and units of world time are set by the application. The origin would normally be set to coincide with the beginning of multimedia activity. World time would run while the activity is in progress, and be stopped or resumed as the activity is stopped or resumed.</p><p>Object time is relative to a multimedia object. In particular, each object can specify the origin of object time with respect to world time and the units used for measuring object time. (Normally these units relate to the data rates of the object's ports.) Furthermore, each object can specify the orientation of object time, i.e., whether it flows forward (increases as world time increases) or backwards (decreases as world time increases).</p><p>A partial specification for MultimediaObject is: The methods of MultimediaObject are divided into three groups, those dealing with temporal coordinates, with composition, and with synchronization. The first group contains utilities for converting between the two temporal coordinate systems, these methods are quite simple and will not be discussed further. The composition and synchronization methods are more complex and will be introduced via examples in the following sections.</p><formula xml:id="formula_0">class MultimediaObject { protected: // current</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Transformations</head><p>The Translate, Scale and Invert methods of Multime-diaObject are called temporal transformations. Informally, these methods do the following:</p><p>Translate -translates the object in world time. Scale -scales the duration of the object. Invert -flips the orientation of object time between "forward" and "reverse."</p><p>To illustrate these operations, suppose we have a Vid-eoDisc object named source#1 which produces a (recorded) Video value on its output port. Convenient units for world and object times could be seconds and frame numbers. If we send the following messages to It should be clear that there are many practical limitations on the above methods. In particular:</p><p>1. It is not possible to apply Scale or Invert (or Cue) to a "live" source (e.g., a video camera or a microphone).</p><p>2. There may be hardware or software limitations on Scale. For instance, video players have a small set, rather than a continuous range, of playback speeds. When Scale is im-plemented in software, by skipping or interpolating sequence values, again only certain scale factors may be supported.</p><p>3. There are limits on the resolution of Translate (and Cue). For example, a video player cannot be cued between frames, an Audio-FileRead object can only be cued to sample boundaries.</p><p>In general, temporal transformations affect the order and timing of the output of a multimedia object. For an object with no output ports, a sink, these methods can be treated as no-ops.</p><p>The effect of temporal transformations can be visualized using timeline diagrams of output ports. Figure <ref type="figure" target="#fig_4">4</ref> shows one such diagram for an output port which at time The effect on an object's output ports of applying the temporal transformations to the object are illustrated in Figure <ref type="figure" target="#fig_5">5</ref>. Using the timeline triples we can express the temporal transformations more concisely:</p><formula xml:id="formula_1">Translate(&lt;T B , T L , S&gt;, delta) = &lt;T B + delta, T L , S&gt; Scale(&lt;T B , T L , S&gt;, s) = &lt;T B *s, T L *s, S'&gt; Invert(&lt;T B , T L , S&gt;) = &lt;T B , T L , S''&gt;</formula><p>The difficulty in implementing temporal transformations is in producing the new sequences S' and S'' required by Scale and Invert. Now for some cases there may be considerable leeway in generating these sequences. For instance, faithfully inverting an Audio sequence is likely to produce meaningless noise. For audio it may be more useful to generate a new sequence, perhaps one whose pitch corresponds to the current scale factor. There are cases though where Scale and Invert produce meaningful information. For instance, we may want to present an image sequence speeded up or in reverse. As a specific example, consider the animation of a bouncing ball. Using multimedia objects, the animation would require a Scene source object, i.e, an object which generates scene descriptions containing the ball. Suppose the object calculates the position of the ball by reading displacement information from a file, and then places an updated scene description on its output port. The basic loop for this object could look something like:</p><p>while(not EOF) { // read displacement from file ball.position = ball.position + displacement; // put scene description on the output port } One problem here is that to produce the i th ball position it is necessary to know the (i-1) th position. This makes the Cue operation difficult to implement (but still possible, by scanning the file up to the cue point). The real problem is that if we Invert or Scale this object it will not produce animations of the ball moving backwards or at a scaled velocity.</p><p>In general, for a ProcessObject to support temporal transformations, the values produced (on its output ports) must be a function of object time. Invocation of this function can be viewed as sampling the object. Note that the sampling rate, which is essentially the data rate of the object's output ports, is constant in world time, but, as measured in object time, varies as the object is scaled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Example: "FadeOver" a Filter Object</head><p>Let us look at a multimedia object which does perform correctly when temporally transformed. The following is a partial implementation of FadeOver, a filter which gradually switches its output from one input port to another: The Process method for FadeOver allows for both increasing and decreasing object time, it will function correctly if the object is inverted. Also the method is sensitive to scaling and will vary the object's duration (as measured in world time) appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Composite Multimedia</head><p>One reason for the complexity of multimedia information is that it often consists of a richly structured collection of lower-level multimedia components. There are several ways in which this structure may be organized. For instance, multimedia documents make use of a hierarchical component structure, while hypermedia involves a network structure. While these two forms of organization are certainly useful, they tend to neglect possible temporal relationships between components. A third method of organizing multimedia information, and one that focuses on these relationships, is what we will call temporal composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporal Composition</head><p>The motivation for temporal composition comes from the need to model situations where a number of multimedia components are simultaneously presented. Television and films are two obvious examples, each containing both audible and visual components.</p><p>Within our object-oriented framework, we will model composite multimedia through the use of special objects defined as follows:</p><p>Definition: A composite multimedia object is a multimedia object containing a collection of component multimedia objects and a specification of their temporal and configurational relationships.</p><p>The two groups of relationships specified by a composite multimedia object are used for different purposes. In particular:</p><p>• temporal relationships -indicate the synchronization and temporal sequencing of components.</p><p>• configurational relationships -indicate the connections between the input and output ports of components.</p><p>As an example, suppose we want to construct a multimedia composite, c 1 , which, when presented, behaves as follows:</p><p>Starting at time t 0 an audio object The temporal relationships of a composite object are easily depicted with a composite timeline diagram. Such a diagram contains one timeline for each output port within the composite. The diagram for the composite c 1 is shown in Figure <ref type="figure">6</ref>.</p><p>In constructing a composite timeline, the Translate transformation is used to adjust the positioning of the components; Scale and Invert may also be applied to alter presentation timing. In general, each component's "placement" within the composite is specified by its translation offset, scale, and orientation. This is analogous to how complex graphics objects can be constructed by applying geometric transformations to groups of simpler objects (see Table <ref type="table" target="#tab_8">2</ref>). In graphics, the sequence of transformations that have been ap- Since composites are multimedia objects, it is possible to apply temporal transformations to composites as well as to their components. This allows the construction of hierarchical composites. As in graphics modelling, the transformation from object to world coordinates is then calculated by traversing the hierarchy from the node representing the object to the root or "world" node.</p><p>A final transformation, the presentation transformation, analogous to the viewing transformation in graphics, provides the mapping from world time to presentation time -the actual time of the presentation (i.e., the hour of the day, the minute of the hour, etc.).</p><p>By adjusting this transformation it is possible to dynamically control the presentation, allowing implementation of user-level commands such as "fast forward" or "reverse."</p><p>The timeline representation shows the concurrency within a composite due to the superimposition of a number of multimedia channels (each horizontal bar in the timeline diagram), it also identifies transition points, i.e., times where sources start or stop. For composite c 1 , there are four channels and four transition points: t 0 , t 1 , t 2 and t 3 .</p><p>Transition points divide world time into a number of intervals. A composite's configurational relationships specify, for each such interval, the connections between input and output ports of its components. This information can be depicted with a component network, where nodes correspond to components of a multimedia composite and edges to port connections. The component network for c 1 during the interval [t 1 , t 2 ] is shown in Figure <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Class "CompositeMultimedia-Object"</head><p>The When a composite object is activated (i.e., it has been sent the Start message) it has two major responsibilities: connecting and disconnecting components when transition points are reached, and synchronization of the composite's components.</p><p>Regarding port connection, we have not been very specific in describing what ports are and how they may be connected. Connecting ports in our framework is complicated by the fact that there are two ba-  sic categories of multimedia objects: those that are hardware-based (in which case the object is a software handle used to manipulate some external device) and those that are software-based. This means that many communication mechanisms are possible. A particular communication mechanism is abstracted by a connector class. Subclasses of MultimediaObject specify, for each port, the connector classes which may be used when connecting the port. The framework presently contains four connector classes:</p><p>SharedMemoryConnector, BufferedConnector, Dele-gateConnector, and CableConnector.</p><p>A connection is represented by a connector object being attached to two ports. Ports themselves are instances of the Port class. Each Port object has an underlying multimedia data type, a direction, is located on some multimedia object, and may have an attached connector. Methods are provided to read and write data from the port's connector, flush data from the connector, determine the amount of data the connector can accept, and determine how much data is currently queued in the connector. In general these methods simply invoke corresponding methods in the connector.</p><p>Port objects are used to enforce valid component networks. For two ports to be connected, they must have the same data type, compatible directions, and allow attachment by connectors of the same class.</p><p>When a transition point is reached and a composite object must establish a connection between two ports, its actions will depend on the class of the connector to be used. For CableConnectors the composite cannot actually plug in the cable (unless a computer-controllable switch is available) and so would leave this connection to the user. For the other connector classes, however, the composite can establish the connection.</p><p>The second major responsibility of composite objects is synchronization of their components. To be more specific, we define synchronization as follows:</p><p>Definition: Two multimedia objects, m 1 and m 2 , are synchronized to a tolerance ∆ t iff:</p><formula xml:id="formula_2">Abs(m 1 .ObjectToWorld(m 1 .now) -m 2 .ObjectToWorld(m 2 .now)) &lt; ∆t</formula><p>When a transition point is reached, a composite starts and stops the appropriate components. However once started these components may fall out of synchronization for two reasons. First, a component may not be able to keep up with the rate at which data is provided on its input ports. Second, hardware delays and slight variations in performance may cause objects to drift out of sync (this is often referred to as "jitter", see <ref type="bibr" target="#b11">[12]</ref> for instance).</p><p>A composite, c, maintains synchronization by attempting to assure:</p><formula xml:id="formula_3">Abs(c.CurrentWorldTime() -c i .CurrentWorldTime()) &lt; c i .SyncTolerance()</formula><p>for each activated component c i . However, because of the varying nature of components, composites must be flexible and support a variety of synchronization techniques. In the framework, each component has a synchronization mode attribute. Depending on the value of this attribute, which can be queried by the method SyncMode, the composite adopts different approaches to synchronization. Presently there are four synchronization modes: NO_SYNC, DEMAND_-SYNC, TEST_SYNC, and INTERRUPT_SYNC. The actions a composite takes for a component of each mode can be summarized as follows (see <ref type="bibr" target="#b6">[7]</ref> for further details):</p><p>NO_SYNC: the composite ignores the component as far as synchronization is concerned.</p><p>DEMAND_SYNC: the composite initially queries the component's SyncInterval method which returns a world time value (the inverse of this val-ue is the sync rate). When the component is activated, the composite periodically invokes the component's Sync method passing the component the composite's value of current world time.</p><p>TEST_SYNC: the composite initially queries the component's SyncInterval and SyncTolerance methods. When the component is activated, the composite periodically queries the component's CurrentWorldTime method and issues a Jump if the component is out of sync.</p><p>INTERRUPT_SYNC: the component takes it upon itself to periodically "interrupt" the composite by querying its CurrentWorldTime method. If the component is out of sync with the composite it must take some action. Typically this involves issuing a Jump and/or calling the composite's OutOfSync method.</p><p>As an example, consider an audio version of the Unix "talk" program. This could be based on two pairs of composite objects (c 11 thru c 22 ) as shown in Figure <ref type="figure">8</ref>.</p><p>The problem is to maintain synchronization between AudioIn on one host and AudioOut on the other, that is, we want the delay from when one user speaks until heard by the second user to be less than some threshold. If we assume, as is likely, that AudioIn and Au-dioOut are essentially pieces of hardware filling and emptying a DMA buffer at some constant rate, then it will be up to AudioToNet and AudioFromNet to maintain synchronization. Suppose that the maximum delay we are prepared to tolerate is 0.25 seconds. Sup- Here note that if AudioFromNet receives data which is too old, the data is thrown away (i.e., not written to the output port) and an OutofSync message is generated. This somewhat drastic since discarding data may produce a perceptible click in the final output. However this can be avoided by writing to the output port a small amount of data which smooths the transition over the skipped packet. Also note that packets received by AudioFromNet that are ahead of world time (i.e., delta &lt; 0) are not discarded but passed on to the output port (AudioFromNet will block if the connector attached to this port cannot accept any more data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Another Example</head><p>As a final and more elaborate example, consider combining interactive 3d graphics with video imagery. Suppose that presentation takes place on a headmounted stereo display and that the user is wearing an input device which measures head position and orientation (there are commercial user-interface devices which combine both the stereo display and the position/orientation sensor into a single helmet-like package). The problem is to generate 3d imagery which responds to changes in the user's visual perspective and combine this imagery with an appropriately transformed video signal. The video is provided by a source, such as a VCR or video disc and overlaid on a "video surface" appearing in the 3d imagery. For simplicity we assume that the video surface is rectangular (in world coordinates) and of the same aspect ratio as the video signal. However, because of the perspective transformation, the surface may appear skewed. Also the surface may be fully or partially hidden by other surfaces.</p><p>The combined video and graphics can be represented by a composite object. This object is not interesting from a timeline perspective since all its components start and stop together. However, the component network is rather complex (see Figure <ref type="figure" target="#fig_8">9</ref>) and contains fourteen objects. They are: head -produces a sequence of head position/orientation values.</p><p>model -maintains a 3d scene model, alters the viewing perspective according to deduced eye positions and sends scene descriptions for the left and right eye to its output ports.</p><p>rdrL, rdrR -two Render objects. Each accept a scene description, generate an image, and determine the display coordinates of the video surface.</p><p>fbufL, fbufR -two FrameBuffer objects.</p><p>video -the source of the video signal.</p><p>tee -duplicates its input on two output ports.</p><p>persL, persR -two DVE objects used to apply a perspective transformation to an input video signal, the transformation is specified by the second input port.</p><p>overL, overR -two DVE objects. Each overlays two input video signals to produce a third (some clipping may be done using a technique known as "chroma-keying").</p><p>eyeL, eyeR -the final display devices.</p><p>Using current technology, most of these objects would be hardware-based, the exceptions being model, rdrL and rdrR. This does not mean, though, that our software structure is redundant. By encapsulating the hardware within subclasses of DeviceObject we gain much flexibility. For example, if we wanted to ignore the video, or ignore stereopsis, we could just form other, simpler, composites. We also gain flexibility from being able to substitute objects. For instance, one might replace the head object by an object which outputs recorded information -this could be useful for debugging or demos. Also, since the composite would then have no live sources, we could apply temporal transformations and reverse or change the speed of presentation.</p><p>Furthermore, our framework allows us to address synchronization. In this example there are two synchronization problems. First, the two eyes must be kept synchronized. Second, the display must not lag behind head movements. Eye synchronization could be handled by DEMAND_SYNCing rdrL and rdrR, and having their Sync method switch double buffers (assuming that rendering is done using double buffering).</p><p>Head synchronization could be achieved by the model object reducing the data rate from head (perhaps 50 events per second) to the sync rate of Render objects (perhaps 10 frames per second). Finally, the frame update rate for Render objects (which is the same as their sync rate) can be dynamically adjusted and so provide graceful degradation of animation continuity should model complexity increase or computing resources decrease <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Research in multimedia is taking place among many different groups -for instance, user interface designers, database implementors, communication protocol engineers, and workstation builders. There seem to be an abundance of ideas and approaches being discussed, yet few unifying concepts. In this section we describe a few of the ideas and approaches that we feel are closely related to our framework.</p><p>The role of component networks was motivated by experience with an audio programming package in- tended for a prototype multimedia workstation <ref type="bibr">[8][10]</ref>. This workstation contained a number of audio devices which could be connected with software processes to form "audio circuits." ACE, the "audio circuit editor" allowed interactive creation, modification and activation of audio circuits. A screen image from ACE is shown in Figure <ref type="figure" target="#fig_2">10</ref>, the right bank is a collection of icons representing different audio devices and processes, the main window shows a circuit which produces an audio data stream from an ADC (analog to digital converter) then tees the stream, one branch going to a DAC (digital to analog converter), the other through a compressor and then to disk. The restriction to audio was a major limitation of ACE. In addition, the underlying audio package used to implement ACE was not object-oriented and could not be readily extended to other devices or other media.</p><p>An object-oriented framework for handling digital audio and music is provided by the NeXT Sound Kit and Music Kit <ref type="bibr" target="#b10">[11]</ref>. The Sound Kit has two classes: Sound and SoundView. The first is used for actual sound objects and has methods for loading and storing sounds, Multimedia presentations, we have seen, are constructed by applying temporal transformations to mul-timedia primitives. How these transformations are specified will depend upon the context. Many multimedia authoring systems, for instance, use a timeline representation where temporal transformations can be interactively specified by moving and stretching the timelines. Another approach to constructing complex multimedia presentations is to specify concurrency and synchronization using a "temporal scripting language" <ref type="bibr">[3][6]</ref>. For instance, TEMPO <ref type="bibr" target="#b2">[3]</ref> provides a set of temporal operators which are used to combine activities -one activity, perhaps the presentation of an audio sequence, can be specified as occurring in parallel, or before, or offset from, a second, visual, activity. A powerful feature of TEMPO is that, provided activities are implemented using a notion of virtual time (similar to object time), special effects such as acceleration can be achieved by "filters" which "modulate" virtual time. However, TEMPO does not have a construct similar to component networks so it does not handle activities which interact or communicate with each other.</p><p>There has been interest in multimedia within the database community for a number of years, particularly in the storage and retrieval of multimedia documents. Woelk et al. <ref type="bibr" target="#b17">[18]</ref> describe extensions to an object-oriented database for handling multimedia input and output. Woelk's model shows the flexibility of treating multimedia as objects but does not treat the problems of composition and synchronization.</p><p>More recently a number of authors have proposed multimedia object models which directly address synchronization <ref type="bibr" target="#b8">[9]</ref>[12] <ref type="bibr" target="#b14">[15]</ref>. These models take into account device and communication channel characteristics and storage organization. However, the models are used for description and analysis rather than programming -they have yet to be specified within an object-oriented programming language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented an object-oriented framework for multimedia based on a few simple concepts such as ports, sinks and sources. Our hope is that such a framework will make configuring multimedia applications as easy as plugging together modular stereo components.</p><p>The framework we have described is not an abstract model, but one that is intended to be implemented and made available to programmers. This, however, is a major task and requires an elaborate hardware platform to fully exercise the model. The platform we have chosen is the NeXTdimension <ref type="bibr" target="#b1">[2]</ref> which, in addition to audio processing capabilities, has image compression hardware that allows a video signal to be compressed and stored in real-time. Also NeXTs run the Mach kernel <ref type="bibr" target="#b0">[1]</ref>; Mach supports lightweight processes and fast IPC, these features simplify the implementation of active objects and ports. At present we are using C++ to test various parts of the framework, many of the base classes have now been implemented. We are also collecting code for various useful objects -these include a DVE which overlays RGB video with composite (NTSC) video, a 6-degree of freedom trackball, a video disc player, a CD player, and objects for audio recording and playback on a workstation.</p><p>In parallel with the framework implementation, we are implementing a trial application which will be used to test the feasibility of complex composites such as shown in Figure <ref type="figure" target="#fig_8">9</ref>. We have gathered hardware needed to combine interactive 3d graphics with video. This hardware, which includes a high-performance graphics workstation and a write-once video disc system, is being used to construct a "virtual museum" where a 3d scene of a museum interior is combined with video clips of museum artifacts <ref type="bibr" target="#b15">[16]</ref>.</p><p>Implementation of the framework and trial application is proving an interesting exercise in object-oriented programming. The conceptual model of sources, sinks, ports and connections maps directly into object classes. The implementation makes heavy use of object-oriented techniques such as inheritance and delayed binding. Active objects are essential to the framework, but we have found a fairly simple form of concurrency sufficient.</p><p>Future work may focus on two problems. First it would be interesting to develop multimedia objects corresponding to audio and video production equipment -this would also serve as a rigorous test of the framework. Secondly, the conceptual model provides composite multimedia objects but not composite multimedia values. It may be possible to integrate com-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Multimedia Objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Some Multimedia Data Types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>The Video Toaster by NewTek.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3Some Multimedia Object Classes. Concrete multimedia object classes, i.e., those which can be instantiated, inherit from one of DeviceObject or ProcessObject. (Not all such inheritance relationships are shown however.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 A</head><label>4</label><figDesc>Figure 4 A Timeline Diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Temporal Transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 A</head><label>7</label><figDesc>Figure7A Component Network (for the interval [t 1 ,t 2 ] of Figure6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9</head><label>9</label><figDesc>Figure 9 Stereoscopic 3d Graphics with Video Overlays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Multimedia Values as Temporal Sequences. some time interval) can be obtained from the d i . Presentation of v takes place at a rate r D , the data rate of D.</figDesc><table><row><cell>Multimedia Data Type</cell><cell>Sequence Of</cell></row><row><cell>sound</cell><cell>audio samples</cell></row><row><cell>video</cell><cell>video frames</cell></row><row><cell>animation</cell><cell>raster frames</cell></row><row><cell>music</cell><cell>musical events</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>T B (called the base time) starts to produce the sequence S = d 1 , d 2 , ..., d n . It is useful to denote a timeline by a triple &lt;T B , T L , S&gt; where, if D is the data type of the d i , then T L = nr D .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>audio 1 , and a video object, video 1 , are presented. At time t 1 , a fade starts from video 1 to a second video object, video 2 . The transition is completed at time t 2 and at time t 3 both audio 1 and video 2 are stopped. Here audio 1 , video 1 and video 2 are source objects. To complete the composite, three additional objects are needed: audioOut, an audio sink; videoOut, a video sink; and dve 1 , a DVE filter for performing the fade from video 1 to video 2 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2</head><label>2</label><figDesc>Graphics versus Multimedia.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>plied to an object is captured by the object's transfor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mation matrix. Similarly, the sequence of temporal</cell></row><row><cell></cell><cell></cell><cell></cell><cell>transformations used to place a component within a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>composite can be described by a 2x2 transformation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>matrix (if time is expressed in homogeneous coordi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nates).</cell></row><row><cell>video 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>dve 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>video 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>audio 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t 0</cell><cell>t 1</cell><cell>t 2</cell><cell>t 3</cell></row><row><cell>time</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 6 A Composite Timeline.</cell><cell></cell></row><row><cell>Graphics</cell><cell></cell><cell>Multimedia</cell><cell></cell></row><row><cell cols="2">2d or 3d space</cell><cell>1d time</cell><cell></cell></row><row><cell>translation</cell><cell></cell><cell>translation</cell><cell></cell></row><row><cell>scaling</cell><cell></cell><cell>scaling</cell><cell></cell></row><row><cell>rotation</cell><cell></cell><cell>inversion</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>AudioToNet attaches its current world time to each "audio packet" received from AudioIn. Then a possible implementation for the relevant methods of AudioFromNet, using INTER-RUPT_SYNC, is:</figDesc><table><row><cell cols="3">pose also that worldInterval AudioFromNet::SyncTolerance()</cell><cell></cell><cell></cell><cell></cell></row><row><cell>{</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>return 0.25;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">void AudioFromNet::Process()</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>{</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>while(TRUE)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">now = Sample(now);</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">objectTime AudioFromNet::Sample(objectTime t)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>{</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>audioPacket</cell><cell>packet;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>worldInterval</cell><cell>delta;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">// read a packet from a network socket</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">delta = parent.CurrentWorldTime()</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">-packet.timeStamp;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">if(delta &lt; SyncTolerance())</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">out.Write(packet, sizeof(audioPacket));</cell><cell></cell><cell></cell><cell></cell></row><row><cell>else</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">parent.OutOfSync(this, delta);</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">return WorldToObject(packet.timeStamp);</cell><cell></cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Host 1</cell><cell></cell><cell>Host 2</cell><cell></cell><cell></cell></row><row><cell>c 11</cell><cell>Audio In</cell><cell>Audio ToNet</cell><cell>Audio FromNet</cell><cell>Audio Out</cell><cell>c 21</cell></row><row><cell>c 12</cell><cell>Audio Out</cell><cell>Audio FromNet</cell><cell>Audio ToNet</cell><cell>Audio In</cell><cell>c 22</cell></row><row><cell></cell><cell></cell><cell cols="2">Figure 8 Audio Talk.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Sound objects are fairly simple and limited in functionality. The Music Kit is more interesting, it is based on the classes Performer, Instrument and Note. Performer and Instrument objects can be connected to form "music networks." A Conductor object coordinates a music network by scheduling the sending of Note objects from Performers to Instruments. The notion of music networks is much like what we have called component networks, also some of the functionality of Conductor objects resembles that of composite objects. However, the Music Kit is specifically for handling music, it shares few concepts with the Sound Kit; it would have to be extended to incorporate multimedia in general, yet it is not clear which parts of its design are specific to music and which parts could be generalized.</figDesc><table><row><cell>START</cell><cell>STOP</cell><cell>PAUSE</cell><cell>EXIT</cell></row><row><cell></cell><cell cols="3">Figure 10 ACE -An Audio Circuit Editor.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>record and playback, editing, and manipulation. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell>second is for displaying sound data. Sound objects</cell></row><row><cell></cell><cell></cell><cell></cell><cell>exist in many encodings, but this need not concern an</cell></row><row><cell></cell><cell></cell><cell></cell><cell>application developer who merely wants to incorpo-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rate sound effects.</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>posite values, in particular, hypermedia, within the model and then investigate the "loading" and "playing" of hypermedia by composite multimedia objects.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A New Kernel Foundation for UNIX Development</title>
		<author>
			<persName><forename type="first">M</forename><surname>Acetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tevanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Mach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Summer 1986 USENIX Conf</title>
		<meeting>Summer 1986 USENIX Conf</meeting>
		<imprint>
			<date type="published" when="1986-07">July 1986</date>
			<biblScope unit="page" from="93" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NeXTstep: Putting JPEG to Multiple Uses</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cockroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hourvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. of the ACM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="1991-04">April 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal Scripting using TEMPO</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nierstrasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsichritzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Active Object Environments</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Centre Universitaire d&apos;Informatique, Université de Genève</orgName>
		</respStmt>
	</monogr>
	<note>(ed. D. Tsichritzis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Design Reuse and Frameworks in the Smalltalk-80 System</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Deutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Software Reusability</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Biggerstaff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Perlis</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="1989">1989</date>
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active Objects: Realities and Possibilities</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object-Oriented Concepts, Applications, and Databases</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Lochovsky</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Temporal Scripting Language for Object-Oriented Animation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsichritzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics&apos;87</title>
		<meeting>Eurographics&apos;87<address><addrLine>North-Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Object-Oriented Framework for Multimedia Composition and Synchronisation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsichritzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia -Principles, Systems and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio Programming: A Framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Konstantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Centre Universitaire d&apos;Informatique, Université de Genève</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synchronization and Storage Models for Multimedia Objects</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Sel. Areas in Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="427" />
			<date type="published" when="1990-04">April 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Design Issues of an Intelligent Workstation for the Office</title>
		<author>
			<persName><forename type="first">N</forename><surname>Naffah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>NCC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">System Reference Manual: Concepts</title>
		<idno>NeXT 1.0</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>NeXT Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-Time Multimedia Communication Systems Architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Sel. Areas in Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="1990-04">April 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concurrency Issues in Object-Oriented Programming Languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Papathomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Object Oriented Development</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Universitaire d&apos;Informatique, Université de Genève</orgName>
		</respStmt>
	</monogr>
	<note>(ed. D. Tsichritzis) Centre</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compuvision or Teleputer?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Press</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1990-09">Sept. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synchronization Properties in Multimedia Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steinmetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Sel. Areas in Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="1990-04">April 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Virtual Museums and Virtual Realities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsichritzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Hypermedia and Interactivity in Museums</title>
		<meeting>of the International Conference on Hypermedia and Interactivity in Museums</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concepts and Paradigms of Object-Oriented Programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OOPS Messenger</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="87" />
			<date type="published" when="1990-08">Aug. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Object-Oriented Approach to Multimedia Databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Woelk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="311" to="325" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
