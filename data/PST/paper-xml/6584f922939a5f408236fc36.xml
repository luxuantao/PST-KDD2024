<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time is Encoded in the Weights of Finetuned Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-12-20">20 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kai</forename><surname>Nylund</surname></persName>
							<email>knylund@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Time is Encoded in the Weights of Finetuned Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-20">20 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2312.13401v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal variation is a fundamental characteristic of language. As we show in ?3, it manifests in language model development as temporal misalignment, where deviations in train and test data lead to large performance degradation across different time periods <ref type="bibr" target="#b14">(Luu et al., 2022;</ref><ref type="bibr">Lazaridou et al., 2021, inter alia)</ref>. This necessitates adaptation techniques for customizing models to specific time periods as needed. Designing such techniques is difficult, however, due to the multitude of time scales and the possibility that data from a target time period might be unavailable.</p><p>Recent work has shown that the behavior of neural networks can be edited through closed-form interpolation between parameters of finetuned models <ref type="bibr" target="#b8">(Ilharco et al., 2023;</ref><ref type="bibr" target="#b15">Ortiz-Jim?nez et al., 2023;</ref><ref type="bibr" target="#b12">Li et al., 2022;</ref><ref type="bibr">Wortsman et al., 2021, inter alia)</ref>. In this work, we demonstrate that weight-space interpolation can also be used to cheaply edit language model behavior over time. To this end, we introduce time vectors ( ?4), an extension of task Figure <ref type="figure">1</ref>: We present time vectors, a simple tool to customize language models to new time periods. Time vectors (? i ) specify a direction in weight space that improves performance on text from a time period i. They are computed by subtracting the pretrained weights (? pre ; left panel) from those finetuned to a target time period (? i ). We can customize model behavior to new time periods (e.g., intervening months or years) by interpolating between time vectors and adding the result to the pretrained model (middle panel). We can also generalize to a future time period j with analogy arithmetic (right panel). This involves combining a task-specific time vector with analogous time vectors derived from finetuned language models (? LM j ).</p><p>vectors <ref type="bibr" target="#b8">(Ilharco et al., 2023)</ref>. We finetune a pretrained language model on text from a single time period, and then subtract the pretrained weights. This vector represents a direction of movement in weight space that improves performance on text from the target time period. We analyze the structure of time vectors with temporally organized datasets for language modeling, classification, and summarization ( ?2). Our results consistently suggest that time vectors are intuitively organized on a manifold; years or months that are closer together in time yield time vectors that are also closer together in weight space. Similarly, we show that temporal degradation in yearly and monthly settings is strongly correlated with the angles between time vectors ( ?4.2).</p><p>We use this structure of time vectors to induce Figure <ref type="figure">2</ref>: Model performance degrades linearly year-to-year. We evaluate language model perplexity (WMT), ROUGE-L (news summarization), and macro F1 (political affiliation classification). Each cell indicates the monthly performance of T5-3B finetuned and evaluated on a single year from that task. We report the percentage difference from the average performance for each year, and find linear degradation as finetuning and evaluation years become more misaligned regardless of task. We display similar trends for T5-small and medium, as well as for other domains and tasks, in ?A.1. We measure the linearity of these degradations in Appendix Table <ref type="table" target="#tab_5">4</ref>.</p><p>models that generalize better to data from new time periods. By interpolating between two time vectors, we discover vectors that, when applied to the pretrained model, improve performance on intervening months or years ( ?4.3). The structure can also be used to generalize task-specific models across time periods with analogous time vectors specialized to unlabeled data ( ?4.4).</p><p>Our results show that temporal variation is to some extent encoded in the weight space of finetuned models, and that weight interpolation can help customize language models to new time periods. We publicly release our code, data, and over 500 models finetuned on specific time periods. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data and Finetuning</head><p>In this section, we describe our datasets and finetuning techniques, which serve as the basis for all subsequent experiments. We finetune language models on multiple time-stratified datasets, which we use to analyze temporal misalignment and build time vectors. Then, we explore different ways of interpolating between time vectors to generalize to new times. See ?4.3-4.5 for more details on interpolation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>Language Modeling We create two new timespecific language modeling datasets from unlabeled text in news and Twitter domains. For these To understand the level of contamination in our datasets, we measure the overlap between yearly train and test splits in both tasks using a Bloom filter. 3 We find that less than two percent and 0.1 percent of examples in the Twitter and WMT LM test sets, respectively, contain contaminated n-grams. Downstream Tasks For downstream tasks, we draw from <ref type="bibr" target="#b14">Luu et al. (2022)</ref>. We measure each model's performance on the test set in ROUGE-L for NewsSum and macro F1 for PoliAff. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Finetuning</head><p>To compare the same weight space across tasks, we use pretrained T5 <ref type="bibr" target="#b16">(Raffel et al., 2023)</ref> checkpoints for all our experiments. We finetune T5small, T5-large, and T5-3b on each of our timestratified datsets. For language modeling, we use the "LM adaptation" objective <ref type="bibr" target="#b11">(Lester et al., 2021)</ref>.</p><p>To reduce the computational burden, we finetune T5-large and T5-3B with Low-Rank Adaptation (LoRA; <ref type="bibr" target="#b7">Hu et al., 2021)</ref> and default hyperparameters (q and v attention target modules, r = 8, ? = 32, dropout = 0.1). When creating time vectors, we merge LoRA weights back into the base model before subtracting the pretrained model.</p><p>Across all settings, we use a batch size of 2 with 8 gradient accumulation steps. We finetune for a single epoch on LM splits and three epochs on downstream task splits. Our learning rates across all tasks are 8 ? 10 -4 for T5-small and T5-large, and 2 ? 10 -4 for T5-3b. We finetuned models concurrently with a single GPU each; we used 8 2080ti, 4 Titan, and 8 A40 GPUs. In experiments for sections ?4.4 and ?4.5, we ran evaluations in parallel using available Titan, A40, and A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revealing Temporal Misalignment at Multiple Time Scales</head><p>We begin with an analysis of temporal misalignment using the new set of models and tasks that we consider in this work ( ?2). These findings set the stage for our creation of time vectors in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Yearly Degradation is Linear</head><p>Previous work on temporal misalignment shows that models degrade over time on a yearly basis. Each cell indicates the monthly performance of T5-small finetuned and evaluated on a single month of the WMT dataset. We report the percentage difference in test perplexity from the average on the evaluation month over all finetuned T5-small models (darker is better). The diagonal indicates that each model does best on its finetuning month. Models also do relatively better on the same month in other years, visible as the stripes radiating out from the diagonal every 12 months.</p><p>To confirm these results, we finetune T5-small, T5-large, and T5-3b on each yearly split from every dataset. We then evaluate each of these yearfinetuned models on every other time split of the test data. We display heatmaps of temporal misalignment at a yearly scale in Figure <ref type="figure">2</ref>. We report percent perplexity change from the average on each year to avoid inherent year performance differences. Consistent with past work <ref type="bibr">(Lazaridou et al., 2021;</ref><ref type="bibr" target="#b14">Luu et al., 2022;</ref><ref type="bibr" target="#b13">Longpre et al., 2023)</ref>, we observe linear patterns of degradation in each task for all model sizes (see Table <ref type="table" target="#tab_5">4</ref> in the Appendix for more details). Like <ref type="bibr" target="#b14">Luu et al. (2022)</ref> show, some tasks, like political affiliation classification, exhibit clearer degradation than others. We quantify these variations in ?A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monthly Degradation is Seasonal</head><p>Next, we turn to month-by-month temporal misalignment, which, to the best of our knowledge, is unexplored. We train T5-small on each WMT LM month split from 2012-2016, resulting in 58 month-finetuned models. We then test every 2012-2016 month model on each month test split for a total of 3,364 evaluations. As seen in Figure <ref type="figure" target="#fig_1">3</ref>, finetuning and evaluating models on specific months in the WMT dataset re- veals non-linear patterns in temporal misalignment, which correspond to the cycle of months in each year. This pattern is captured by the stripes that occur parallel to the diagonal every 12 months, which indicate that the model for a particular month tends to do better on the same month in other years. We quantify these differences in perplexity in appendix Figure <ref type="figure">12</ref>. We also report degradation patterns in online training settings in ?A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary</head><p>We measure temporal misalignment across a variety of domains, tasks and time scales. While performance decays linearly on a yearly scale, we discover seasonal trends in month-to-month misalignment. Next, we analyze how these phenomena relate to the weights of time-specific models, and then use that relationship to present techniques for adapting LMs to new times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Temporal Adaptation with Time Vectors</head><p>The collection of year and month-finetuned models from ?3 presents a new source of data to study temporal misalignment: model weights. In this section, we analyze these weights through the lens of time vectors, formed by taking the difference of a model finetuned on a specific time and the pretrained model. First, we show that the weights of two time vectors become less similar as the times they were finetuned on become more misaligned ( ?4.2). Then, we attempt to use the reverse relation- ship to update models to unseen times: reducing misalignment on intervening ( ?4.3), future ( ?4.4), and multiple time periods ( ?4.5) by interpolating time vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Background and Definition</head><p>Task vectors <ref type="bibr" target="#b8">(Ilharco et al., 2023)</ref> are the difference of the weights of a pretrained model from the weights of the same model after finetuning on a task. Adding and subtracting task vectors from finetuned models is a simple and effective way to improve performance on other settings, or reduce unwanted behavior without further training. Like word embeddings, if there are tasks with the analogous relationship "A is to B as C is to D," then task vectors can be used to improve performance on D with the approximation D ? C + (B -A). Time vectors are an extension of task vectors to the time domain. Given the weights of the pretrained model, ? pre and those of the model finetuned on data from only a single time period t, ? t , a time vector ? t = ? t -? pre . Like their task-based counterparts, we add back the pretrained weights at inference time and evaluate ? pre + ? t <ref type="bibr" target="#b8">(Ilharco et al., 2023)</ref>. We call time vectors from models finetuned on individual years and months "year-vectors" and "month-vectors."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlation of Time Vector Similarity and Temporal Degradation</head><p>We visualize time vectors with a UMAP in Figure <ref type="figure" target="#fig_2">4</ref>, which suggests that time vectors closer together in weight space are also closer together in time.</p><p>To verify this hypothesis, we measure the cosine similarity between model weights from each pair of time vectors trained on different time periods (visualized in ?A.1). We find that this similarity metric and perfor-mance (Figure <ref type="figure" target="#fig_10">11</ref>) decay similarly over time. Table <ref type="table" target="#tab_1">1</ref> shows that the correlation between cosine similarity and relative performance change on different years is highest in WMT language modeling. Correlations are generally similar across T5 sizes, with a higher score for T5-small in the WMT LM setting than T5-large and T5-3b, and no absolute values less than 0.6. This relationship also extends to the monthly scale. Seasonal stripes are visible in the cosine similarities between each pair of monthly WMT time vectors (visualized in Appendix Figure <ref type="figure" target="#fig_8">9</ref>). The monthly performance degradation from the mean (Figure <ref type="figure" target="#fig_1">3</ref>) and cosine similarity matrices (Figure <ref type="figure" target="#fig_8">9</ref>) have a negative correlation (Pearson r = -0.667; p &lt; 10 -16 ). We analyze cosine similarities to single-year time vectors throughout online training in Appendix ?A.5.</p><p>These results indicate that time vectors are organized in way that is predictive of their performance on corresponding time periods. Next, we explore how we can use this structure to improve performance on new time periods by interpolating between time vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalizing to Intervening Time Periods</head><p>Archiving issues or a low sampling rate can lead to gaps in datasets between the oldest and newest examples. Without data, we expect models to perform worse on these "gap" times due to temporal misalignment. In this section, we find that we can generalize better to these intervening time periods by interpolating between models finetuned on the oldest and newest times.</p><p>Method For two time vectors ? j , ? k , we compute their interpolation ?</p><formula xml:id="formula_0">? ? j + (1 -?) ? ? k with ? ? [0, 1].</formula><p>In this section, we interpolate between the earliest year time vector ? 0 and latest year time vector ? n and evaluate on times t 0 , ..., t n for each ? ? [0.1, 0.2, ..., 1.0].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows that interpolating between start and end-year finetuned models improves performance on intervening years in both WMT LM and PoliAff tasks. Improvement is generally greatest on the exact middle years (2014 for WMT LM, 2017 for PoliAff) and decreases on years closer to start and end times. Patterns of improvement also vary depending on setting, with flatter changes in performance near ? = 1.0, 0.0 in PoliAff compared to <ref type="bibr">WMT</ref>  to the difference in performance between evaluation years. Table <ref type="table">2</ref> quantifies these changes, showing that interpolation closes the gap on intervening years between temporally aligned and misaligned models. Improvements are particularly large for PoliAff, nearly eight macro-F1 points just by averaging the start and end-year time vectors.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows that these results extend to the monthly scale for WMT LM; we can interpolate between time vectors finetuned on January and December in a year to improve performance on the months between them. The best interpolations for each month follow an intuitive pattern, with a higher percentage of the January model leading to better performance on earlier months and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalizing to the Future</head><p>The creation of labeled datasets lags behind corpera of raw text, which can be scraped automatically. As a result, language models that rely on supervision for finetuning are quickly outdated. Updating these models can be expensive, involving extra finetuning and creating labeled datasets from more recent examples. In this section, we present a new technique for updating task models finetuned on a source time period j to a target time period k with only unlabeled data from j, using task analogies <ref type="bibr" target="#b8">(Ilharco et al., 2023)</ref>.</p><p>Method Given language models with weights ? LM j , ? LM k finetuned on unlabeled text from times j, k, and a task-specific model with weights ? j finetuned on labeled data from time j, we perform the  We interpolate between January and December month vectors and evaluate on all other months within the same finetuning year. Like at the yearly scale, early months do better with a higher percentage of the January model and vice versa while middle months do best with a 50% split between the models. The stars in the upper plots correspond to the best performing interpolations for each evaluation month; these optimums are mirrored in the lower line plots.</p><p>following arithmetic on the vectors:</p><formula xml:id="formula_1">? j = ? j -? pre ? LM j = ? LM j -? pre ? LM k = ? LM k -? pre ? k ? ? 1 ? ? j + (? 2 ? ? LM k -? 3 ? ? LM j ) ? k = ? k + ? pre</formula><p>We evaluate our estimated ? k on each target time t k , sweeping over all combinations of ? 1 ? [0.6, 0.8, . . . Results Task analogies improve performance on future years in both PoliAff and NewsSum tasks. Figure <ref type="figure" target="#fig_6">7</ref> shows that improvement compared to finetuning on the start year increases as the target and start years become more misaligned. Model size also affects performance, with T5-large and T5-3b showing greater improvements. In PoliAff, T5- small has no improvement over the baseline and T5-large task analogies perform worse than the baseline on 2016 and 2017 before improving on 2019 and 2020. Strangely, we find that only scaling ? 1 can also improve performance on future years. We report these ? ablations and our results on two other classification tasks in Appendix ?A.6. We observe mostly similar results on these tasks, although there are task-specific inconsistencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Generalizing to Multiple Time Periods</head><p>Because interpolations prove useful for generalizing to intervening and future time periods, we next test if we can build models that perform well on multiple time periods by interpolating between all time vectors for a task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We approach this problem with the model soup technique <ref type="bibr">(Wortsman et al., 2022)</ref>. One of the key practical advantages of soups is that constituent time-specific models can be trained independently (on smaller compute budgets) and combined at any time. Furthermore, the multi-year model does not need to be retrained to include new time periods; new time periods can be incorporated by merely growing the soup with additional finetuned models.</p><p>We attempt to create a multi-year model by following the recipe outlined by <ref type="bibr">Wortsman et al. (2022)</ref>. They introduce two soup variants: the uniform soup and greedy soup. The uniform soup applies a uniform weight among all constituent models in the interpolation, while the greedy soup is an iterative procedure that only includes models in the soup that improves validation performance. We assess both variants here.</p><p>Our "uniform time soup" is ? pre + 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|T |</head><p>t?T ? t where T is the set of all years for a given task. For our "greedy time soup," we implement a similar algorithm to <ref type="bibr">Wortsman et al. (2022)</ref> which samples time vectors (with replacement) from each year in order of decreasing performance and adds them to the average model soup if they improve performance.</p><p>To evaluate our ability to build models that generalize to multiple time periods, we measure the average performance across all evaluation years for each task. We compare our model soups against two baselines: 1) a model trained on all shuffled available data at once and 2) the best-performing model finetuned on only a single year of data. The all-year model is the most compute-intensive approach.</p><p>Results Overwhelmingly, time soups perform worse than the model finetuned on all shuffled available data. For WMT LM and NewsSum, the uniform time soup performs worse than even the best single year model, despite having access to five times the amount of finetuning data. The greedy time soup only improves over the best single-year model on PoliAff with a single macro F1 point gain. These findings suggest that a model which generalizes to multiple time periods does not lie in a region of weight space bounded by models finetuned on single years of data. Future work may explore more sophisticated methods of merging which to induce better performing multi-year models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Summary</head><p>We propose methods for updating models to intervening, future, and multiple time periods using time vector arithmetic. We find that interpolating between two time vectors improves performance on unseen intervening times at both yearly and monthly scales. Similarly, we can improve performance on the future with unlabeled data from target times using time vector analogies. Building a multi-year model with a "soup" of time vectors, We compare multiple ways of building multi-year models; T5-small models finetuned to individual years or all years, and "time soups" created by averaging together all year time vectors for a task.</p><p>however, does not approach the performance of a model finetuned on all times at once. These results suggest that task arithmetic can be a simple way to update models to new times, but it does not help to improve genearlization across the board within a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Temporal Misalignment The phenomenon of temporal misalignment in language models has gained attention in the last three years. Moving from semantic drift to model misalignment, Rijhwani and Preo?iuc-Pietro (2020) demonstrate the effect of temporal drift on named entity recognition using timestamped tweets. <ref type="bibr">Lazaridou et al. (2021)</ref> extend these analyses to language modeling on News and Science domains and show that increasing model size does not help mitigate temporal misalignment. <ref type="bibr" target="#b14">Luu et al. (2022)</ref> compare temporal misalignment across a variety of downstream tasks, finding that degradation varies greatly over both domain and task. Using the same suite of tasks, Longpre et al. ( <ref type="formula">2023</ref>) report similar degradation over time in pretraining regardless of model size.</p><p>Updating LMs Recent attempts at updating language models to new time periods have used a range of techniques. <ref type="bibr" target="#b14">Luu et al. (2022)</ref> find limited improvement with continued pretraining <ref type="bibr" target="#b18">(R?ttger and Pierrehumbert, 2021)</ref> on target times. Similar to the sequential updating setting, however, <ref type="bibr">Lazaridou et al. (2021)</ref> show that dynamic evaluation <ref type="bibr" target="#b5">(Gururangan et al., 2020)</ref> can improve language modeling performance on new times, but results in forgetting the past. More recent techniques have been proposed for keeping models up to date in the QA domain by adding flags with the year for each example <ref type="bibr" target="#b2">(Dhingra et al., 2022)</ref> or by discarding outdated facts <ref type="bibr">(Zhang and Choi, 2023)</ref>. Unlike these methods, we consider the problem of updating models to new time periods without data in the target time and without additional finetuning.</p><p>Semantic Drift Although changes in the full weight spaces of models over time have not been previously explored, semantic changes in word embeddings over time are well-documented <ref type="bibr" target="#b6">(Hamilton et al., 2016)</ref>. Temporal misalignment <ref type="bibr" target="#b0">(Bamler and Mandt, 2017;</ref><ref type="bibr" target="#b3">Gonen et al., 2021)</ref> and word analogies over time <ref type="bibr" target="#b19">(Szymanski, 2017)</ref> have also been studied in embeddings. Our work extends these analyses to the full set of language model parameters.</p><p>Interpolation Our work draws heavily on recent techniques for editing models directly with interpolation and task analogies. Time vectors are an application of task vectors <ref type="bibr" target="#b8">(Ilharco et al., 2023)</ref> to the time domain, our interpolation experiments are inspired by previous work on patching models for multiple tasks <ref type="bibr" target="#b9">(Ilharco et al., 2022)</ref>, and our time soups are an application of models soups (averaging multiple models trained with different initializations) <ref type="bibr">(Wortsman et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We connect studies of temporal misalignment and weight arithmetic with time vectors, formed by finetuning a model on a specific time period and then subtracting its pretrained weights. We show that the weights of time vectors are more similar if their corresponding times are closer and vice versa. These similarities are highly correlated to temporal misalignment at both yearly and monthly scales (which exhibit seasonal patterns). Leveraging this temporal structure in weight space, we induce new models that perform better on intervening years by interpolating between adjacent time vectors. Similarly, we use task analogies to improve downstream performance on future time periods using only unlabeled data from those times. These results show that task arithmetic can be a simple tool for combating temporal misalignment.</p><p>Michael JQ Zhang and Eunsol Choi. 2023. Mitigating temporal misalignment by discarding outdated facts. arXiv preprint arXiv:2305.14824.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Yearly Misalignment with Other Tasks and T5 Sizes</p><p>In this section, we report raw performance degradation over time on four downstream and three language modeling tasks with three sizes of T5.</p><p>We evaluate on all tasks in the main paper plus Newsroom Source Classification (NewsCls) and AI Venue Classification (AIC) from <ref type="bibr" target="#b14">Luu et al. (2022)</ref>.</p><p>We also create a third science domain language modeling task from abstracts in the Kaggle arXiv dataset<ref type="foot" target="#foot_0">4</ref> . For each group of three years from 2006-2008 to 2018-2020 we randomly sample 26-38M and 2.6-3.9M BPE tokens (150MB and 15MB of text) of arXiv paper abstracts for train and test splits respectively. Figures 8 and 11 are yearly degradation heatmaps for each model size and task. These results show that normalizing performance by the average on each evaluation time helps account for variations in test splits. ArXiv language modeling and NewsSum, for example, have large differences in performance on evaluation years regardless of finetuning year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Task Variations in Linear Yearly Degradation</head><p>Like <ref type="bibr" target="#b14">Luu et al. (2022)</ref>, we find differences across domain and task in the rate and linearity of year-toyear decay. TD scores measure the average rate of performance degredation for each year of misalignment between train and test time periods <ref type="bibr" target="#b14">(Luu et al., 2022)</ref>. We find the rate of decay using a linear least squares regression and average rates for each task over all evaluations. Table <ref type="table" target="#tab_5">4</ref> shows TD scores <ref type="bibr" target="#b14">(Luu et al., 2022)</ref> for all tasks and T5-sizes. We also compare TD scores calculated from raw performance to TD scores calculated from performance normalized by the average on each evaluation year. In general, percent performance difference from the mean on an evaluation year decays more linearly than raw performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Yearly and Monthly Cosine Similarities</head><p>In this section, we report cosine similarity between each pair of yearly and monthly time vectors. Fig-</p><p>ure <ref type="figure" target="#fig_9">10</ref> shows cosine similarity between every pair of year vectors for each T5-size and task. Figure <ref type="figure" target="#fig_8">9</ref> shows cosine similarity between each pair of T5small monthly WMT LM time vectors. Similar to performance, year-to-year degradation in cosine similarity between task vectors appears to be linear regardless of setting. Like figure <ref type="figure" target="#fig_1">3</ref>, we observe seasonal "stripes" every 12 months from the diagonal 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Temporal Degradation in Online Settings</head><p>Our work so far illustrates temporal misalignment on static time splits. However, in practice, we usually deploy language models in online settings, meaning that they are continually updated with the latest data, and we do not have access to data from all training years simultaneously.</p><p>To show how temporal misalignment manifests in these settings, we first sort all the training data from the PoliAff and WMT tasks by month, and finetune T5-small on each task separately. We display the performance of the LM on every year throughout training in Figure <ref type="figure" target="#fig_1">13</ref>. As expected, for PoliAff, we see that the performance of models on a particular year peak at the final month of that year, and then gradually degrade as the model continues training.</p><p>For language modeling on WMT data, performance consistently improves during training, regardless of the evaluation year. However, perplexity reduces more slowly in earlier years as we continue training. These results suggest that temporal misalignment may manifest differently in online settings based on the training setup and task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Online Cosine Similarities</head><p>We study the relationship between performance degradation and cosine similarity during online training. Recall that in the online setting, we perform a single finetuning run on the Poliaff and WMT tasks (after ordering their training data by month), and measure performance on each year throughout training. To study how time vectors move throughout space in this setting, we measure the cosine similarity between the time vector of the model trained up to month m and each yearly time vector for the PoliAff and WMT tasks.</p><p>We find that the cosine similarity to each time vector decreases as the online model is updated past the first 12 months of data. This means that online models' peak similarity to earlier years tends to be higher than those to later years since the they    make up a smaller part of its total finetuning set.</p><p>Like our experiments with soups of time vectors in section ?4.5, this indicates that models trained on multiple years of data lay outside a region defined by single-year models.</p><p>To account for these decreases, we normalize the similarity to each year time vector by its average after updating on all months in Figure <ref type="figure" target="#fig_1">13</ref>. Our results reveal that the vector for our online model is relatively most similar to each year vector after finetuning on the months in that year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Time Vector Analogy Ablations</head><p>In this section, we ablate our time vector analogy experiment to determine the effects of only adding the LM vector from the target time, and only scaling the weights of the initial time vector. For</p><formula xml:id="formula_2">? k ? ? 1 ? ? j + (? 2 ? ? LM k -? 3 ? ? LM j )</formula><p>, we define our "task addition" ablation for ? 3 = 0, ? 1 , ? 2 ? = 0, and our "scaling only" ablation for ? 1 ? = 0, ? 2 , ? 3 = 0</p><p>We report the best results after sweeping over the same ? ranges from ?4.4 with the added constraints in figure <ref type="figure" target="#fig_11">15</ref>. While task analogies generally perform best across tasks and T5-sizes (especially as ? j and ? k become more misaligned), we find that ablating ? LM k and ? LM j can still improve over the base ? j model. Surprisingly, only scaling ? j also improves over the initial model on many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Temporal Misalignment Affects Some</head><p>Parameters More than Others</p><p>In this section, we explore whether we can reduce temporal misalignment by swapping parameter weights from a model trained on a misaligned year with those of the model trained on the target year. For example, we substitute the QKV attention layers from a model finetuned on 2015 PoliAff with those finetuned on 2020 PoliAff and evaluate on 2020 data. In table 5 we evaluate the start-year finetuned models for each task on the end times (e.g. start = 2012 for WMT LM, end = 2016) with various parameter weights swapped with the end-year finetuned model.</p><p>From these experiments, we find that we can improve performance on a target time by swapping out weights with a time vector finetuned on that time. Surprisingly, swapping embeddings with the target time vector makes very little difference, except in language modeling tasks, and swapping all non-embedding weights with a target time almost reaches the performance the target time-specific models for downstream tasks. Swapping only feed-forward or attention layers also improves performance on the target time, suggesting temporal misalignment is somewhat isolated to those model regions in downstream tasks.   Table <ref type="table">5</ref>: We can improve performance on a target time by swapping out weights with a time vector finetuned on that time. T5-small start-year finetuned model performance on the end-year split for each task (e.g. finetuning on 2015 for PoliAff and evaluating on 2020). We compare the baseline start-year model (none swapped) to versions with various parameter weights from the target-year model, and the target-year model itself (all swapped).</p><p>Figure <ref type="figure">12</ref>: Seasonality makes a small, but noticeable impact on monthly misalignment. Distribution of perplexity change from the mean for aligned finetuning and evaluation months (left, mean=-4.36), seasonal "stripes" (middle, mean=0.04), and all finetuning and evaluation combinations which share neither the same month nor year (right, mean=0.77).</p><p>Figure <ref type="figure" target="#fig_1">13</ref>: In online settings, language model performance degrades on earlier time periods. We show macro F1 and perplexity on each year split of PoliAff and WMT LM respectively after sequentially finetuning T5-small on each new month of task data. PoliAff performance over all years plateaus after finetuning on months up to 2018.</p><p>WMT performance continues to improve with more data, but perplexity decrease slows on earlier years. Starred points are where performance on a year is best relative to the average performance on all years.</p><p>Figure <ref type="figure" target="#fig_2">14</ref>: Cosine similarity between an online time vector and a year vector peaks relative to other years after updating on data for that year. We show cosine similarity between each monthly checkpoint of online T5-small time vectors and yearly vectors for PoliAff and WMT LM. To account for overall decreases in similarity as online time vectors are updated, we normalize similarities to each year vector by the mean similarity to that year over all checkpoints. We star the point for each year vector where its cosine similarity to the online model is largest relative to the average on all years. ), ? 1 , ? 2 , ? 3 ? = 0, we define "task addition" to be only adding the language modeling vector (i.e ? 1 , ? 2 ? = 0, ? 3 = 0), and "scaling only" to be only scaling the base ? j model (i.e ? 1 ? = 0, ? 2 , ? 3 = 0). We sweep over the same alpha combinations as in ?4.4 and report the best results for each target year, task, and T5-size.</p><p>Figure <ref type="figure" target="#fig_4">16</ref>: Year-to-year, T5-small feed forward layers change the most across all tasks and domains, and attention changes more in the language modeling setting. For our T5-large and T5-3b models trained with LoRA, the V attention layers change more than the Q layers, with most of the changes (regardless of model size) concentrated in the last layers. Like our param swapping experiment, this suggests that some parameters play a larger role in temporal misalignment than others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>NewsSum: We use Luu et al. (2022) postprocessing of Grusky et al. (2018) news summarization task. To align with out WMT dataset, we do not bin adjacent years together, creating uniformly sized splits for each year from 2012 to 2016. ? PoliAff: We use the Political Affiliation task from Luu et al. (2022), with uniformly sized datasets for each year from 2015 to 2020.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Monthly temporal degradation has seasonal patterns. Each cell indicates the monthly performance of T5-small finetuned and evaluated on a single month of the WMT dataset. We report the percentage difference in test perplexity from the average on the evaluation month over all finetuned T5-small models (darker is better). The diagonal indicates that each model does best on its finetuning month. Models also do relatively better on the same month in other years, visible as the stripes radiating out from the diagonal every 12 months.</figDesc><graphic url="image-3.png" coords="3,306.14,70.87,218.27,163.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time vectors are organized in a manifold that reflects temporal variation. Each point is a UMAP projection of the last feedforward layer of a T5small time vector finetuned on single month of WMT. Points and edges between adjacent months are colored by year. Distances between the weights of time vectors correlate with temporal misalignment ( ?4.2).</figDesc><graphic url="image-4.png" coords="4,70.87,70.87,218.27,150.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Interpolating between two year vectors improves performance on the years between them. These performance improvements follow an intuitive structure, e.g. when interpolating between 2012 and 2016, the best result on 2013 occurs with a higher percentage of 2012 and vice versa for 2015. Improvement from interpolation varies across settings.</figDesc><graphic url="image-5.png" coords="6,70.87,70.87,453.54,142.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Interpolating between two month vectors improves performance on the months between them.We interpolate between January and December month vectors and evaluate on all other months within the same finetuning year. Like at the yearly scale, early months do better with a higher percentage of the January model and vice versa while middle months do best with a 50% split between the models. The stars in the upper plots correspond to the best performing interpolations for each evaluation month; these optimums are mirrored in the lower line plots.</figDesc><graphic url="image-6.png" coords="6,70.87,283.48,453.54,225.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2.2], ? 2 , ? 3 ? [0.1, . . . 0.6] and reporting the best result compared to the original model ? j . In this section, we update a 2012 News-Sum model to 2013-2016, and a 2015 PoliAff model to 2016-2020 using WMT LM and Twitter LM time vectors respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Task analogies can offset downstream temporal misalignment without labeled data from the target time. We report the performance of NewsSum and PoliAff T5 models updated using WMT LM and Twitter LM vectors for each target evaluation time. We report the percent improvement of the best updated model over 2012 NewsSum and 2015 PoliAff models on each target time for all model sizes.</figDesc><graphic url="image-7.png" coords="7,70.87,70.86,218.27,218.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Yearly downstream performance degradation on four tasks and three T5 sizes.</figDesc><graphic url="image-8.png" coords="11,70.87,127.00,453.55,307.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Cosine similarity between monthly time vectors also exhibits seasonality. We observe similar "stripes" every 12 months when measuring the cosine similarity between each pair of T5-small WMT month vectors. The correlation between this heatmap (including the diagonal) and figure 3 is -0.667 with p &lt; 1 ? 10 -16 .</figDesc><graphic url="image-9.png" coords="12,70.87,70.87,218.27,155.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Cosine similarities between all pairs of year time vectors for all tasks and model sizes.</figDesc><graphic url="image-10.png" coords="13,70.87,105.80,453.52,194.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Yearly language modeling perplexity decay on three tasks and three T5 sizes.</figDesc><graphic url="image-11.png" coords="13,127.56,399.83,340.15,314.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Time vector analogy ablations for three sizes of T5. Given the time vector analogy ?k ? ? 1 ? ? j + (? 2 ? ? LM k -? 3 ? ? LM j ), ? 1 , ? 2 , ? 3 ? = 0, we define "task addition" to be only adding the language modeling vector (i.e ? 1 , ? 2 ? = 0, ? 3 = 0), and "scaling only" to be only scaling the base ? j model (i.e ? 1 ? = 0, ? 2 , ? 3 = 0). We sweep over the same alpha combinations as in ?4.4 and report the best results for each target year, task, and T5-size.</figDesc><graphic url="image-15.png" coords="16,70.87,88.98,453.55,336.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="2,70.87,70.87,453.55,156.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-13.png" coords="15,116.22,125.40,362.84,161.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-14.png" coords="15,116.22,473.48,362.84,161.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The</figDesc><table><row><cell>Pearson r</cell></row></table><note><p>similarity between time vectors correlates with temporal degradation. Pearson correlation between cosine similarity of yearly time vectors and % degradation from the mean performance of all yearly models on each evaluation time period. All p-values are &lt; 8 ? 10 -4 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>LM, and minimal improvements in NewsSum across ?s compared</figDesc><table><row><cell></cell><cell cols="3">Perplexity (?) Rouge (?) F1 (?)</cell></row><row><cell>Method</cell><cell>WMT LM</cell><cell cols="2">NewsSum PoliAff</cell></row><row><cell>Start-year finetuned (? 0 )</cell><cell>13.92</cell><cell>38.56</cell><cell>0.6886</cell></row><row><cell>End-year finetuned (? n )</cell><cell>13.84</cell><cell>35.09</cell><cell>0.6967</cell></row><row><cell>1 2 (? 0 + ? n )</cell><cell>13.77</cell><cell>38.86</cell><cell>0.7765</cell></row><row><cell>Best interpolations</cell><cell>13.75</cell><cell>40.11</cell><cell>0.7941</cell></row><row><cell>Eval-year finetuned (? i )</cell><cell>13.65</cell><cell>42.36</cell><cell>0.8341</cell></row><row><cell cols="4">Table 2: Interpolation between start and end-year</cell></row><row><cell cols="4">finetuned models reduces temporal misalignment</cell></row><row><cell cols="4">on intervening years. T5-3b average performance on</cell></row><row><cell cols="4">each year between start and end (non-inclusive). "Best</cell></row><row><cell cols="4">interpolations" use the best performing ? values for</cell></row><row><cell>each year.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Interpolation does not enable generalization to multiple time periods simultaneously. Here, we measure the average performance of models on all years.</figDesc><table><row><cell></cell><cell cols="3">Perplexity (?) Rouge (?) F1 (?)</cell></row><row><cell>Method</cell><cell>WMT LM</cell><cell cols="2">NewsSum PoliAff</cell></row><row><cell>Best single-year model</cell><cell>34.45</cell><cell>38.95</cell><cell>0.7101</cell></row><row><cell>Uniform time soup</cell><cell>34.70</cell><cell>33.05</cell><cell>0.6078</cell></row><row><cell>Greedy time soup</cell><cell>34.45</cell><cell>38.95</cell><cell>0.7202</cell></row><row><cell>Training on all years</cell><cell>29.17</cell><cell>40.07</cell><cell>0.7853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>TD scores for all tasks and T5 sizes for raw performance and performance divide by the average on each eval. year. Variance explained by the TD score linear fit in parentheses. TD scores calculated with normalized performance decay have generally higher R 2 scores, except on Twitter LM and PoliAff, and are easier to compare.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Swapped Params WMT LM NewsSum NewsCls Twitter LM PoliAff ArXiv LM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AIC</cell></row><row><cell>None</cell><cell>35.72</cell><cell>35.11</cell><cell>0.7232</cell><cell>6.69</cell><cell>0.5903</cell><cell>18.18</cell><cell>0.8224</cell></row><row><cell>Feed Forward</cell><cell>35.31</cell><cell>35.17</cell><cell>0.8162</cell><cell>13.25</cell><cell>0.6174</cell><cell>18.21</cell><cell>0.8500</cell></row><row><cell>Attention</cell><cell>36.23</cell><cell>34.49</cell><cell>0.7986</cell><cell>14.95</cell><cell>0.6095</cell><cell>19.24</cell><cell>0.8644</cell></row><row><cell>Embeddings</cell><cell>36.13</cell><cell>34.30</cell><cell>0.7232</cell><cell>16.65</cell><cell>0.5902</cell><cell>19.29</cell><cell>0.8192</cell></row><row><cell>Non-Embedding</cell><cell>34.57</cell><cell>37.24</cell><cell>0.8760</cell><cell>13.46</cell><cell>0.7991</cell><cell>17.37</cell><cell>0.8845</cell></row><row><cell>All</cell><cell>33.51</cell><cell>38.89</cell><cell>0.8759</cell><cell>5.79</cell><cell>0.7999</cell><cell>15.75</cell><cell>0.8845</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://www.kaggle.com/datasets/Cornell-University/arxiv/data</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic word embeddings</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="380" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">editors</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Time-aware language models as temporal knowledge bases</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00459</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="257" to="273" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Hila Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14330</idno>
		<title level="m">Simple, interpretable and stable method for detecting words with usage change across corpora</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11283</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.10964</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09096</idno>
		<title level="m">Diachronic word embeddings reveal statistical laws of semantic change</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Editing models with task arithmetic</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Patching open-vocabulary models by interpolating weights</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitzhak</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="29262" to="29277" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devang</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Branch-train-merge: Embarrassingly parallel training of expert language models</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13169</idno>
		<title level="m">A pretrainer&apos;s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Time waits for no one! analysis and challenges of temporal misalignment</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karishma</forename><surname>Mandyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5944" to="5958" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Task arithmetic in the tangent space: Improved editing of pre-trained models</title>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Ortiz-Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Favero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.12827</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>text transformer</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporally-informed analysis of named entity recognition</title>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7605" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>R?ttger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">B</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08116</idno>
		<title level="m">Temporal adaptation of bert and performance on downstream document classification: Insights from social media</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal word analogies: Identifying lexical replacement with diachronic word embeddings</title>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Szymanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
	<note>short papers</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><surname>Kornblith</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23965" to="23998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust fine-tuning of zero-shot models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
			<biblScope unit="page" from="7949" to="7961" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
