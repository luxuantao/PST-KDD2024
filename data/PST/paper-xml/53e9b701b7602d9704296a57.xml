<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Representative and Discriminant Models for Object Category Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimodal Interactive Systems</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimodal Interactive Systems</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>caputo@nada.kth.se</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision and Active Perception Laboratory</orgName>
								<orgName type="institution">KTH Stockholm</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimodal Interactive Systems</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<region>TU</region>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Representative and Discriminant Models for Object Category Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D01D85607B2E4F41D90C9CD9C6B2811</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Category detection is a lively area of research. While categorization algorithms tend to agree in using local descriptors, they differ in the choice of the classifier, with some using generative models and others discriminative approaches. This paper presents a method for object category detection which integrates a generative model with a discriminative classifier. For each object category, we generate an appearance codebook, which becomes a common vocabulary for the generative and discriminative methods. Given a query image, the generative part of the algorithm finds a set of hypotheses and estimates their support in location and scale. Then, the discriminative part verifies each hypothesis on the same codebook activations. The new algorithm exploits the strengths of both original methods, minimizing their weaknesses. Experiments on several databases show that our new approach performs better than its building blocks taken separately. Moreover, experiments on two challenging multi-scale databases show that our new algorithm outperforms previously reported results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years object categorization has regained interest and impressive results have been reported on various databases. Interestingly, while there seems to be a common consensus on the use of local features, there is much more variety on the classification methods, where the range goes from probabilistic models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> to discriminative approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Generative models are quite appealing for various reasons in the context of object categorization. For example those models can be learned incrementally <ref type="bibr" target="#b18">[19]</ref>, they can deal with missing data in a principled way, they allow for modular construction of composed solutions to complex problems and therefore lend themselves to hierarchical clas-sifier design. Also, prior knowledge can be easily taken into account <ref type="bibr" target="#b11">[12]</ref>. In practice generative models show considerable robustness with respect to partial occlusion and viewpoint changes and can tolerate significant intra-class variation of object appearance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. However, the price for this robustness typically is that they tend to produce a significant number of false positives. This is particularly true for object classes which share a high visual similarity such as horses and cows.</p><p>Discriminative methods enable the construction of flexible decision boundaries, resulting in classification performances often superior to those obtained by purely probabilistic or generative models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. This allows for example to explicitly learn the discriminant features of one particular class vs. background <ref type="bibr" target="#b24">[25]</ref> or between multiple classes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref>. Object categorization algorithms which use discriminative methods combined with global and/or local representations have been shown to perform well in the presence of clutter, viewpoint changes, partial occlusion and scale variations. Also, recent work has shown the suitability of discriminative methods for recognition of large numbers of categories <ref type="bibr" target="#b19">[20]</ref>.</p><p>While so far the object recognition community has chosen one of these two modeling approaches, there has been an increasing interest in the machine learning community in developing algorithms which combine the advantages of discriminative methods with those of probabilistic generative models <ref type="bibr" target="#b8">[9]</ref>; a similar strategy has proven to be beneficial for image parsing into regions and objects <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this paper we integrate two different types of approaches into a single common framework to fully exploit their strengths while minimizing their weaknesses. More specifically, we combine the Implicit Shape Model (ISM, <ref type="bibr" target="#b10">[11]</ref>) based on a codebook representation (which can be seen as a non-parametric probabilistic model of the appearance of object categories) with an SVM using Local Kernels (LK, <ref type="bibr" target="#b25">[26]</ref>), which has proven effective for object categorization <ref type="bibr" target="#b14">[15]</ref>. The idea to use a generative model inside a kernel function has been proposed before <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21]</ref>, and it has been applied to visual recognition tasks like object identification <ref type="bibr" target="#b22">[23]</ref>.</p><p>The first main contribution of this paper is a new approach which tightly integrates a probabilistic with a discriminant approach into a single categorization framework. This tight integration is made possible by a unified data representation used by both approaches. The new integrated approach is beneficial with respect to ISM, since the new approach preserves the generalization capabilities of ISM but increases its accuracy in rejecting false positives. Since ISM effectively acts as a pre-filter to the discriminant part of the algorithm the integration is also beneficial with respect to LK by using the discriminant power only where it is needed, namely on visually similar appearances of object classes.</p><p>The second main contribution are experimental results which show the superiority of the new integrated approach with respect to ISM and LK both in terms of detection performance and of a significant reduction of false positives on challenging databases. The new approach also outperforms state-of-the-art object categorization methods on challenging multi-scale data sets.</p><p>The third main contribution is that the integrated approach improves over and extends the original LK approach <ref type="bibr" target="#b25">[26]</ref> in various respects: the new approach is scale invariant, enables localization of the object in the scene, and allows cross-instance learning of object category models.</p><p>The rest of the paper is organized as follows: after a brief review of the ISM and LK algorithms (Sec. 2), we introduce the new approach, describing in detail how it integrates ISM and LK and discussing its advantages with respect to the two previous methods (Sec. 3). Section 4 reports experiments benchmarking our new method with its building blocks, on several databases of increasing difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Approaches</head><p>Our approach is motivated by two recent advances in object detection and discrimination. Object Detection with Implicit Shape Models. Implicit Shape Models (ISMs) <ref type="bibr" target="#b10">[11]</ref> are unique in that they address object category detection and top-down segmentation at the same time. They proceed by first collecting the evidence from local features in a probabilistic Hough voting procedure to determine possible object locations and scales. For each such hypothesis, they then go back to the image to determine on a per-pixel level where its support came from, thus effectively segmenting the object from the background. The segmentation information can then in turn be used to improve the accuracy of the detection and resolve ambiguities between overlapping hypotheses <ref type="bibr" target="#b10">[11]</ref>. As a result of this iterative process, ISMs have been shown to yield good object detection results and considerable robustness to partial occlusion.</p><p>The ISM approach provides a flexible representation of the target category. Since each image patch votes for the object center independently of the other patches, the resulting model can interpolate between local parts seen on different training objects. As a result, it can adapt well to novel objects of the target category and typically achieves high recall. However, as a price for this flexibility, it cannot reject false positives as accurately as a discriminative model. SVM Classification with Local Kernels. Most current object category detection systems are based on local features in order to reduce the influence of intra-class variations, noise, and occlusion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. Support Vector Machines (SVMs), on the other hand, have shown impressive learning and recognition performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8]</ref>. As the SVMs' machinery requires the computation of scalar products on the feature vectors, <ref type="bibr" target="#b25">[26]</ref> introduced a local kernel which formulates the feature matching step as part of the kernel itself. Despite the claim in <ref type="bibr" target="#b25">[26]</ref>, this family of kernels is not a Mercer kernel <ref type="bibr" target="#b3">[4]</ref>. Still, it can be shown that it statistically approximates a Mercer kernel in a way that makes it a suitable kernel for visual applications. On the basis of this finding, and of its reported effectiveness for object categorization <ref type="bibr" target="#b14">[15]</ref>, we will use this family of kernels in this paper.</p><p>Given two local feature lists L h and L k , these local kernels are defined as <ref type="bibr" target="#b25">[26]</ref> </p><formula xml:id="formula_0">K(L h , L k ) = 1 n h n h j h =1 max j k =1,...,n k K l (L j h h , L j k k ) ,<label>(1)</label></formula><p>where the local feature similarity kernel K l consists of an appearance part K a and a position constraint</p><formula xml:id="formula_1">K p K l (L a h , L b k ) = K a (L a h , L b k ) K p (pos(L a h ), pos(L b k )).<label>(2)</label></formula><p>Various options have been given for the selection of K a and K p <ref type="bibr" target="#b25">[26]</ref>, including the following choice</p><formula xml:id="formula_2">K a = exp -γ 1 - x -µ x |y -µ y ||x -µ x ||||y -µ y || (3)</formula><p>As shown by <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref>, Local SVMs can discriminate well between different object categories. However, they contain no localization component and require accurate initialization in position and scale. In the literature, the standard solution to this problem is to perform an exhaustive search over all possible object positions and scales <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref>. However, this exhaustive search imposes severe constraints, both on the detector's computational complexity and on its discriminance, since a large number of potential false positives need to be excluded. In this paper, we present a different solution to this problem by integrating Local SVMs with the ISM approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Integrated Approach</head><p>The main contribution of this paper is to integrate both approaches into a consistent framework (visualized in Fig. <ref type="figure" target="#fig_0">1</ref>). Applied to a novel test image (Fig. <ref type="figure" target="#fig_0">1(a)</ref>), the representative ISM is first used to find a set of promising hypotheses (Fig. <ref type="figure" target="#fig_0">1(b)</ref>) and estimate their support in both location and scale (Fig. <ref type="figure" target="#fig_0">1(c)</ref>). For each of those hypotheses, the more exact discriminative model is then applied in order to verify them and filter out false positives (Fig. <ref type="figure" target="#fig_0">1(d))</ref>. By using the same internal representation, namely the appearance codebooks, those two approaches are tightly integrated. The ISM uses these appearance codebooks to generate hypotheses which are visually consistent and which follow a weak spatial model. The discriminative model on the other hand uses the same appearance codebooks to find visually discriminant information for object classes and also to add a stronger spatial model effectively extracting discriminant spatial codebook distributions. We thus combine the capabilities of both models in an advantageous manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generation of an Appearance Codebook</head><p>As a common representation, we generate a categoryspecific appearance codebook, as described in <ref type="bibr" target="#b10">[11]</ref>. For this, we apply a scale-invariant DoG interest point operator <ref type="bibr" target="#b12">[13]</ref> to all training images and extract image patches with a radius of 3σ of the detected scale. All extracted patches are then rescaled to a uniform size (in our case 25 × 25 pixels) and grouped using an agglomerative clustering scheme. The resulting clusters form a compact representation of local object structure. In the following, we keep only the cluster centers C = ( c 1 , . . . , c R ) as codebook entries.</p><p>For each codebook entry, we then learn its spatial occurrence distribution on the object category. For this, we perform a second iteration over all training images, again extracting patches around interest points, and record for each c i all locations where it can be matched to the extracted patches (where patch similarity is measured by normalized correlation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial Hypothesis Generation</head><p>In order to generate initial hypotheses about possible object locations and scales, we use a scale-invariant version of the ISM approach from <ref type="bibr" target="#b10">[11]</ref>. The approach starts by applying the same patch extraction procedure as before, and the local information from sampled patches is collected in a probabilistic Hough voting procedure. Each patch is matched to the codebook, and matching codebook entries cast votes for possible object positions and scales according to their learned spatial probability distribution. This is formalized as follows. Let e be an image patch observed at location . Each matching codebook entry c i generates probabilistic votes for different object categories o n and locations λ = (λ x , λ y , λ s ) according to the following marginalization:</p><formula xml:id="formula_3">P (o n , λ| e, ) = i P (o n , λ| c i , )p( c i | e)<label>(4)</label></formula><p>where p( c i | e) denotes the probability that e matches to c i , and P (o n , λ| c i , ) describes the stored spatial probability distribution for the object center relative to an occurrence of that codebook entry. For describing the matching probability, we make the assumption that an image patch can be approximated by the mean of the closest-matching codebook entries</p><formula xml:id="formula_4">C * e = { c * i |sim( c * i , e) ≥ θ}, thus setting p( c * i | e) = 1</formula><p>|C * e | . Object hypotheses are found as maxima in the 3D voting space using Mean-Shift Mode Estimation <ref type="bibr" target="#b4">[5]</ref> with a scale-adaptive balloon density estimator <ref type="bibr" target="#b5">[6]</ref> and a uniform ellipsoidal kernel K:</p><formula xml:id="formula_5">p(o n , λ) = 1 nh(λ) d k j p(o n , λ j | e k , k )K( λ -λ j h(λ) )</formula><p>Once a hypothesis has been found, the contributing votes are backprojected to determine which local features and codebook activations supported it in the image (Fig. <ref type="figure" target="#fig_0">1(c)</ref>). The original ISM approach additionally computes a full topdown segmentation of the object, which has been shown to improve the results considerably. In our approach, we also apply this segmentation loop to improve the quality of hypotheses. However, as this part is of minor relevance to the understanding of our integrated approach, we refer the reader to <ref type="bibr" target="#b10">[11]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Representation in Codebook Coordinates</head><p>The result of the ISM stage is a set of object hypotheses h = (o n , λ), together with their support in the image (Fig. <ref type="figure" target="#fig_0">1(c)</ref>). This support consists of a list of local features that contributed to the hypothesis and their corresponding codebook activations. In order to interpret this information in the SVM framework, we first have to adapt the kernel formulation to our codebook representation.</p><p>The key idea is that the scalar product x, y used in the SVM Kernel can be expressed in terms of a codebook matching problem. For this, we project both x and y into the affine space spanned by the codebook entries c i as basis vectors. With x = i a i c i and y = j b j c j the scalar product can be written as</p><formula xml:id="formula_6">x, y = i j a i c i , c j b j . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>This formulation has two advantages. One is its computational efficiency -both the intra-codebook similarity matrix c i , c j and the support vector coefficients b j can be precomputed. Only the image-feature coefficients a i need to be calculated during recognition. The second advantage is that the data is now expressed in a common format and partial results can be reused by both stages.</p><p>Remains the problem how to select the coefficients a i and b j . The smallest reconstruction error would be obtained by a least-squares solution, but this solution is typically not sparse. In order to arrive at a sparse representation, we again consider only the closest-matching codebook entries</p><formula xml:id="formula_8">C * x = { c * i |sim( c * i , x</formula><p>) ≥ θ} and approximate the vectors x and y by the mean of those "activated" codebooks. Thus, with</p><formula xml:id="formula_9">n = |C * x |, m = |C * y |, we arrive at x, y ≈ µ x , µ y = &lt; 1 n n i=1 c * i , 1 m m j=1 c * j &gt; (6) = n i=1 m j=1 1 n c * i , c * j 1 m . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>This approximation is justified under the assumption that the codebook entries sufficiently cover the relevant "object" region of the appearance space. We have verified the validity of this assumption in a series of control experiments. The results indicate that the difference in reconstruction error between the least-squares solution and our sparse approximation is only modest and subsumes to an average error of approximately one gray level per pixel on a reconstructed patch. As a result, we get a problem-specific representation which expresses the data in a common vocabulary Each support vector specifies a configuration of local features, corresponding to a particular training example. When evaluating a hypothesis, the kernel K first searches for the k best feature correspondences, considering both appearance and relative position, and uses them to judge the quality of the match.</p><p>and is used throughout both stages of our approach. In particular, this representation allows us to reuse the results of the initial codebook matching stage for the SVM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SVM Verification with Local Kernels</head><p>Let X = {(x 1 , λ 1 ), . . . , (x N , λ N )} be a set of local features (with appearance and relative location) supporting hypothesis h, and A = {A 1 , . . . , A N } , A i = (a 1 , . . . , a R ) be their corresponding codebook activations. The ISM procedure guarantees that each feature in the supporting set is consistent in appearance and location with at least one training example. However, as only local consistency is enforced, this reference example may be a different one for each feature. In the next step, we therefore want to verify that the global feature configuration is also consistent.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> visualizes the chosen verification procedure. In the remainder of this section, we will define the Local Kernel in a way that each support vector corresponds to a distinct configuration of local features. When evaluating a hypothesis, it is successively compared to each support vector. For each such match, correspondences are established between visually similar features occurring in the same relative locations (with a small tolerance σ), and the quality of the resulting global configuration fit is measured. Thus, the kernel enforces strong spatial constraints to verify the hypothesis. This is done as follows.</p><p>Let Y = {(y 1 , λ 1 ), . . . , (y M , λ M )} be the features observed on a training image with corresponding codebook activations B = {B 1 , . . . , B M } , B j = (b 1 , . . . , b R ). In order to compare the feature configurations of X and Y , we first try to find a set of correspondences between their features. For each pair of features ( x, λ x ) and ( y, λ y ), the quality of a match is expressed by the local similarity kernel K l :</p><formula xml:id="formula_11">K l (( x, λ x ), ( y, λ y )) = K a ( x, y) K p (λ x , λ y ),<label>(8)</label></formula><p>where K a is measuring the appearance similarity and K p is imposing a position constraint in the manner of a penalty function. For K a and K p we stick to the choices made in <ref type="bibr" target="#b25">[26]</ref>, but replace the correlation coefficient by the approximation from eq. ( <ref type="formula" target="#formula_6">5</ref>):</p><formula xml:id="formula_12">K a ( x, y) = exp(-γ (1 -x, y ))<label>(9)</label></formula><p>≈ exp(-γ(1 -</p><formula xml:id="formula_13">i j a i c i , c j b j )) K p (λ x , λ y ) = exp - (λ x -λ y ) 2 2σ 2 . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>In order to allow for some flexibility in the part arrangement, we do not enforce complete correspondence, but only match a subset of the features by searching for the k best correspondences. This is done using a greedy selection strategy on the feature similarity matrix</p><formula xml:id="formula_15">K l (X, Y ). Let Φ ∈ π N 1 , Ψ ∈ π M</formula><p>1 be permutations of the local features to reflect this greedy assignment. According to <ref type="bibr" target="#b25">[26]</ref>, the corresponding Local Kernel is then defined as</p><formula xml:id="formula_16">K(X, Y ) = (11) 1 k max Φ,Ψ k j=1 K l ( x Φ(j) , λ x,Φ(j) ), ( y Ψ(j) , λ y,Ψ(j) ) .</formula><p>Note that the resulting kernel does not need to consider the original features anymore, but only operates on the codebook activations passed from the previous stage. It thus requires very little computation and imposes only a small overhead on the total execution time. In all experiments presented in this paper, we set k to 50 and determine the remaining parameters using cross-validation on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>It is important to emphasize that through the integration, the SVM stage is solving a simpler problem than the previous LK approach. Not only is it initialized with an estimate of the object position and scale, but it directly obtains also the supporting image features as input. It can thus optimize its decision surface on the failure cases of the ISM stage and learn a stronger discriminative model. In addition, the discriminative model makes it possible to achieve a better separation from background constellations, whose complex distributions are notoriously hard to express in a probabilistic framework.</p><p>The matching to a common codebook enables both stages to make use of "across-instance" learning which is essential when dealing with limited training set sizes. In addition, the Local Kernel stage complements the ISM's weak spatial model with stronger spatial constraints.</p><p>As a side benefit, the output confidence of the SVM stage (i.e. the distance to the hyperplane) becomes comparable for different object categories. This is the case because the Local Kernel bases its computation on a fixed number of k correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show that our new approach benefits from the integrated representative and discriminative representation (in the following termed IRD). We present our results in three steps. First, we compare our new approach to the original ISM and LK approaches. Section 4.2 then reports results on a multi-category detection/discrimination task. Finally, we evaluate our approach on two difficult data sets containing large scale changes and partial occlusion. Data. In order to evaluate our approach, we apply it to a test set containing objects of four categories, namely cars, cows, horses, and motorbikes. The pairs cars/motobikes and cows/horses were especially chosen to measure crosscategory confusions, since they share similar visual features. The data is mostly taken from the PASCAL database collection <ref type="bibr" target="#b0">[1]</ref>. For cars we use the UIUC single-scale test set; for motorbikes the CalTech motorbike set (with the same training/test split as in <ref type="bibr" target="#b6">[7]</ref>); and for cows the TUD cow database (supplemented with 556 test images) . For the background set, we use 450 CalTech background images. The horse images are taken from the Weizmann horse database <ref type="bibr" target="#b2">[3]</ref> and split into 79 training and 164 test images. This is the first time detection results are reported on this database, as it was previously only used for segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Original Approaches</head><p>We start the experimental part with a comparison of our new IRD approach with the approaches it originates from -namely the Local Kernels and the ISM. To provide a fair comparison with the LK approach, which is not designed to be scale invariant or perform a detection task in the first place, we report results of object present/absent experiments. The test is performed on images of each category vs. 450 novel background images.</p><p>Tabel 1 summarizes the equal error rate (EER) performances for this experiment. As can be seen from the table, the integrated approach achieves superior performance compared to its building blocks. to localize image regions in which an instance of the category of interest is present. For evaluating the car detections, we use exactly the same acceptance criterion and evaluation software as in <ref type="bibr" target="#b1">[2]</ref>. However, as this criterion is only welldefined for fixed-size bounding boxes (and thus not directly applicable to the cow and horse categories), we apply an extended criterion for the other three categories. We inscribe an ellipse in the ground truth bounding box and measure the distance d r between the bounding box centers relative to the ellipse's radius at the corresponding angle. A hypothesis is accepted if d r ≤ 0.5 and the ground truth and hypothesis bounding boxes cover one another by at least 50%. In accordance to <ref type="bibr" target="#b1">[2]</ref>, only one hypothesis per object is accepted as correct -any additional hypothesis on the same object is counted as false positive. Detection Results. Figure <ref type="figure">3</ref> shows the results of this evaluation in the form of Recall-Precision curves (RPCs). To vary the strictness of the local kernel SVM in our new IRD approach without retraining, we used the distance to the decision boundary as a confidence measure. Although the ISM by itself performs already quite well on all four categories, our new IRD approach improves the EER performance for cars from 87.6% to 88.6%, for cows from 92.5% to 93.2% and for motorbikes from 80.0% to 84.0%. For horses, the performance stays at the same level. Besides the gain in EER performance, cars, cows and motorbikes profit from the added discriminance in terms of increased precision of the final detector, which shifts the precisionrecall curves to the left. Especially the relatively rigid car and motorbike categories profit from the stronger structured constraint of the local kernel. Figure <ref type="figure" target="#fig_3">4</ref> displays some example detections of our approach which illustrate the generalization capabilities over large intra-category variations, including different articulations, and its robustness to partial occlusion. Discrimination Results. Given these detectors operating at their equal error rate, we now investigate the produced confusions. Table <ref type="table">2</ref> displays the false detections each of the detectors produces per image on all four object categories. The left number reports the false positives detected by the ISM. It can be seen that the ISM performs well for the car model, but still produces a relatively large number of false positives for the other categories. The larger number of con-fusions on the car images can be explained by the fact that those images are about twice as large as the other images. The right number reports the false positives detected by our new IRD approach processing all detectors in parallel and acting as a single unified detector. Ambiguous detections are eliminated using the local SVM output as a confidence measure. We can observe a drastic reduction of false positives down to (or even below) the 0.1 level for almost all combinations. In particular, these results show that in our new IRD approach the SVM output is well suited as a confidence measure for comparing hypotheses across categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Category Discrimination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discriminant Category Detection</head><p>In this section, we evaluate our approach on two more challenging databases that include large scale changes and significant partial occlusion. We use the UIUC multi-scale cars and the TUD motorbikes, which are also both part of the PASCAL collection <ref type="bibr" target="#b0">[1]</ref>. For the multi-scale cars, we again use the acceptance criterion from <ref type="bibr" target="#b1">[2]</ref>; for the motorbikes we use the criterion described in Section 4.2 for the reasons given there. Figure <ref type="figure" target="#fig_4">5</ref> shows the result of this evaluation. The black line corresponds to the performance reported by <ref type="bibr" target="#b1">[2]</ref>, with an EER performance of about 45%. In contrast, our IRD approach achieves 87.8% EER -an improvement of over 40%! Interestingly, our method obtains up to 64% recall before generating any false positives. On the motorbike test set, our approach achieves an EER performance of 81%. Compared to ISM, there is a consistent improvement in precision. The difficulty of the task is illustrated by Figure <ref type="figure" target="#fig_5">6</ref>, which shows example detections of our IRD approach documenting the performance under occlusion, extreme illumination conditions and large scale changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Conclusions</head><p>Summarizing our approach, we integrated a representative object detection method with a discriminative verification stage for the generated hypotheses. Both stages operate on a common codebook representation. They share and reuse the same information from sampled image patches, but interpret it in different ways. The ISM hypothesis generation stage searches for agglomerations of image patches that are locally consistent with a common object center. Treating each sampled patch independently, it can interpolate between different training examples and adapt to novel objects and changed articulations. The SVM verification stage, on the other hand, enforces stronger spatial constraints and verifies the global feature configuration. At the same time, its discriminative capabilities obviate the need for a dedicated background model, which is difficult to estimate reliably in a probabilistical framework. Finally, the tight integration with the output of the  ISM stage removes the influences of translation and scale changes, which greatly simplifies the discrimination problem.</p><p>Together, both stages manage to reduce the number of false positives and cross-category confusions significantly and perform considerably better than either stage alone. In our experiments, we have shown this improvement both for a four-class detection/discrimination task and for object detection on two challenging data sets containing large scale changes and partial occlusion. In particular these last results show a considerable improvement on the state-of-the--art with over 40% difference in EER performance for the UIUC multi-scale car database.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Stages of the integrated approach. (a) original image; (b) hypotheses detected by the representative ISM; (c) input to the SVM stage; (d) verified hypotheses.</figDesc><graphic coords="3,59.52,31.14,121.18,89.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A look inside the LK verification stage.Each support vector specifies a configuration of local features, corresponding to a particular training example. When evaluating a hypothesis, the kernel K first searches for the k best feature correspondences, considering both appearance and relative position, and uses them to judge the quality of the match.</figDesc><graphic coords="4,320.76,22.23,226.70,127.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Table 2 :</head><label>32</label><figDesc>Figure 3: Recall-Precision curves for the car, cow, horse and motorbike model on a detection task.</figDesc><graphic coords="7,395.16,255.86,74.20,91.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example detections on the car, cow, horse, and motorbike test sets.</figDesc><graphic coords="7,395.16,348.65,74.20,65.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Precision-Recall curves for the difficult multi-scale databases of cars and motorbikes. Our new IRD approach clearly outperforms the state-of-the-art on the UIUC multi-scale car database.</figDesc><graphic coords="8,428.76,221.31,121.20,86.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Example detections of our new IRD approach on the difficult multi-scale UIUC car database and the multiscale TUD motorbike test set.</figDesc><graphic coords="8,61.92,309.22,121.20,90.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Equal error rate performances achieved by the Local Kernel (LK), Implicit Shape Model (ISM), and our integrated approach (IRD) on present/absent tasks.</figDesc><table><row><cell>Detection Task and Evaluation. Our main experiments</cell></row><row><cell>are performed on a detection task, where the detector has</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work has been funded, in part, by the EU project CoSy (IST-2002-004250).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.pascal-network.org/challenges/VOC" />
		<title level="m">The PASCAL Object Recognition Database Collection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect objects in images via a sparse, part-based representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-specific, top-down segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2353</biblScope>
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-mercer kernels for svm object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC&apos;04</title>
		<meeting><address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distribution free decomposition of multivariate data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The variable bandwidth mean shift and data-driven scale selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In ICCV&apos;01</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Component-based face detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="657" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;98</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Probability product kernels. JMLR&apos;04</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="819" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;04 Workshop on Stat. Learn. in Comp. Vis</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cue integration through discriminative accumulation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A trainable system for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Support vector machines for 3d object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A statistical method of 3d object detection applied to faces and cars</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;00</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted and robust incremental method for subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Skočaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1494" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sharing features: Efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new discriminative kernel from probabilistic models</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2397" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The kullback-leibler kernel as a framework for discriminant and localized representations for visual recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="430" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognition with local features: the kernel recipe</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
