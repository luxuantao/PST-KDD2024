<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A survey on region based image fusion methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bikash</forename><surname>Meher</surname></persName>
							<email>bikashmeher1981@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics &amp; Telecommunication Engineering</orgName>
								<orgName type="institution">VSS University of Technology</orgName>
								<address>
									<settlement>Burla</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
							<email>agrawals_72@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics &amp; Telecommunication Engineering</orgName>
								<orgName type="institution">VSS University of Technology</orgName>
								<address>
									<settlement>Burla</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rutuparna</forename><surname>Panda</surname></persName>
							<email>r_ppanda@yahoo.co.in</email>
							<affiliation key="aff1">
								<orgName type="institution">Machine Intelligence Research Labs</orgName>
								<address>
									<settlement>Washington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ajith</forename><surname>Abraham</surname></persName>
							<email>ajith.abraham@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics &amp; Telecommunication Engineering</orgName>
								<orgName type="institution">VSS University of Technology</orgName>
								<address>
									<settlement>Burla</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A survey on region based image fusion methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C0C0FFF7D25953CDAF4ABBBD7BB6471B</idno>
					<idno type="DOI">10.1016/j.inffus.2018.07.010</idno>
					<note type="submission">Received date: 12 April 2017 Revised date: 25 July 2018 Accepted date: 30 July 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Fusion Image fusion</term>
					<term>Region based fusion</term>
					<term>segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image fusion has been emerging as an important area of research. It has attracted many applications such as surveillance, photography, medical diagnosis, etc. Image fusion techniques are developed at three levels: pixel, feature and decision. Region based image fusion is one of the methods of feature level. It possesses certain advantagesless sensitive to noise, more robust and avoids misregistration. This paper presents a review of region based fusion approaches. A first hand classification of region based fusion methods is carried out. A comprehensive list of objective fusion evaluation metrics is highlighted to compare the existing methods. A detailed analysis is carried out and results are presented in tabular form. This may attract researchers to further explore the research in this direction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p> A comprehensive comparison among the existing methods is highlighted.</p><p> A detailed analysis with encouraging results is presented.</p><p> This survey may attract researchers to explore the domain of region based fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image fusion is the method of merging information from many images of the same scene taken from various sensors, different positions or different time. The fused image retains all the complementary and redundant information of the input images that are very useful for human visual perception and image processing task. The objective of image fusion is to fuse the details of the important information extracted from the two or multiple images.</p><p>In order to meet these objectives, the fused result should meet the following requirements: <ref type="bibr">(a)</ref> the fused image should retain the most complementary and significant information of the input images, (b) the fusion technique should not generate any synthetic information which may divert the human observer or the advance image processing application, (c) it must avoid imperfect states, for instance, misregistration and noise <ref type="bibr" target="#b0">[1]</ref>. It is observed from the literature that image fusion approaches are classified into two types, spatial based and transform based.</p><p>In spatial based methods, the pixels of the images to be fused are combined in a linear or nonlinear manner. The fused image is expressed mathematically as</p><formula xml:id="formula_0">  1 2 1 1<label>2 2</label></formula><p>, ,..., As discussed above, the fused image will retain both the complementary and the redundant information from the input images.</p><p>On the basis of levels of abstraction, image fusion algorithms are applied at three stages: pixel, feature and decision level. In pixel level, images are combined directly using individual pixels to form the fusion decision. A comprehensive survey on pixel level image fusion is found in <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. The feature level image fusion is achieved by region based fusion scheme <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. In region based image fusion, initially an image is partitioned into a set of regions. The different features of these regions are extracted. The appropriate features from all the source images are merged to get the fused image. They are less responsive to noise and more robust. Further, the feature information is used for considering more intelligent semantic fusion rules. A first hand survey on region based image fusion is presented in Section 2. Decision level image fusion techniques are based on the outputs of initial object detection and classification task <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Usually, a preliminary decision from the feature based image fusion serves as the input to the decision level fusion. A generic classification of image fusion methods is shown in Fig. <ref type="figure" target="#fig_10">2</ref>.  <ref type="bibr" target="#b13">[14]</ref> (c) Surveillance <ref type="bibr" target="#b14">[15]</ref> (d) Remote sensing <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Fusion Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel level</head><note type="other">Feature Level Decision level</note><p>A survey on different image fusion methods is done by many researchers. A classification of image fusion methods based on multi-scale decomposition technique is described in <ref type="bibr" target="#b16">[17]</ref>. A review of image fusion techniques in remote sensing is presented in <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Solanky and Katiyar <ref type="bibr" target="#b17">[18]</ref> focussed on pixel based fusion methods in remote sensing.</p><p>Ghassemian <ref type="bibr" target="#b18">[19]</ref> has carried out the survey on pixel, feature, and decision level with more emphasis on pixel level method. Vivone et al. <ref type="bibr" target="#b19">[20]</ref> presented a comparison among different remote sensing image fusion algorithms, particularly focussing on multiresolution analysis and component substitution methods. Wang et al. <ref type="bibr" target="#b20">[21]</ref> presented a review of image fusion methods based on pulse coupled neural network (PCNN). An overview of multimodal medical image fusion is described in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The authors in <ref type="bibr" target="#b21">[22]</ref>  The remainder of the manuscript has been structured as follows. Section 2 explains the different region based image fusion methods. Section 3 presents the performance evaluation parameters. An elaborate discussion on a comparison of various methods is presented in Section 4. Finally, Section 5 draws the conclusion giving a brief summary and critique of the findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Region based image fusion methods</head><p>It is observed from the literature that the feature level image fusion technique can be further classified into machine learning, region based and similarity matching to content based. In the machine learning method, the features are extracted and a suitable classifier is used for fusion. In region based method, the input images are divided into different regions using some segmentation techniques. The features are extracted from the regions and suitable fusion rules are used to get the fused image. The similarity matching to content based image retrieval technique uses the visual contents of an image, for instance, shape, texture, colour, and spatial layout to denote the index. The relevant indices are combined to get the fused image. A first hand classification of region based image fusion methods is proposed here. It is classified as shown in Fig. <ref type="figure" target="#fig_4">4</ref>.  based approach aims at the separation of the significant foreground object from the background leading to perceptually coherent regions. The advantages and disadvantages of these three approaches are mentioned later in this section.</p><p>A generic block diagram of the region based image fusion scheme is presented in Fig  Based on the vast information available in the literature, the various region based image fusion procedures are categorized into three classes as shown in Table <ref type="table" target="#tab_2">1</ref>. In the region partition based algorithms, the first step is to partition the input images into distinct regions by employing standard segmentation techniques. By considering the characteristics of the regions, fusion rules are employed to get the fused image. In general, the different approaches of multiresolution, ICA based transform, optimization etc. are incorporated with the segmentation algorithms. Some flow diagrams of the existing methods are shown. The flow diagram of region partition algorithm based method explained in <ref type="bibr" target="#b48">[49]</ref> is depicted in Fig. <ref type="figure" target="#fig_6">6</ref>. The region based image fusion technique was firstly suggested by Zhang and Blum <ref type="bibr" target="#b23">[24]</ref>. The authors in their work combined synergistically the pixel and feature based fusion.</p><p>The wavelet transform of the source images is merged to produce the fused image. The authors identified edges and region of interest (ROI) as the important features for guiding the fusion process. It is to be noted that this approach involves division of a series of images at discrete resolutions. This problem was addressed by Piella <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. The author proposed a</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T 10</formula><p>general framework for pixel and region based image fusion utilizing multiresolution approach. The segmentation stage is improved by performing a single segmentation from all the source images in a multiresolution fashion. The images are fused following the additive or weighted combination fusion rule. However, the author has not optimized the performance and investigated the effect of different parameters on the fusion process. Further, the regions are combined based on simple region property like average activity. The DWT lacks shift invariance and directionality property.</p><p>To overcome these problems, Lewis et al. reviewed a lot of pixel level fusion algorithms (using averaging, pyramids, DWT, DT-CWT). The authors compared their findings with a new region based technique <ref type="bibr" target="#b27">[28]</ref>. The authors used DT-CWT for segmentation of the features of the input images to develop a region map. The properties of every region are determined, and the fusion is performed region by region utilizing region based approach in the wavelet domain. However, the authors pointed towards the improvement of higher level region based fusion rules. The quality of the fused image may deteriorate due to the inverse DT-CWT transform. To improve the fusion results, Zaveri and Zaveri <ref type="bibr" target="#b28">[29]</ref> used highboost filter concept with DWT for the fusion. Their proposed technique overcomes the shift variance problem; as inverse wavelet transform is not needed in the algorithm. The highboost filter is used to get accurate segmented image. The segmented image is utilized for obtaining the regions from the input images. The extracted regions are fused using the fusion rule i.e. mean max standard deviation and spatial frequency. However, they concluded that, to enhance the robustness of the method, complex fusion rules may be developed.</p><p>Many researchers have also used the NSCT approach to image fusion. A region based image fusion procedure for infrared (IR) and visible image using NSCT is suggested by Guo et al. <ref type="bibr" target="#b29">[30]</ref>. The features of the target region are used to segment the IR image. The input images are divided using NSCT. The target region and the background regions are merged using different fusion rules. The inverse NSCT is used to get the fused image. However, the fusion process involved has a high computational complexity. Zheng et al. used NSCT and fuzzy logic for the fusion of IR and visible image <ref type="bibr" target="#b30">[31]</ref>. Firstly, the required input images are segmented using the live wire segmentation method followed by decomposition using NSCT transform. The fuzzy fusion method is used in the low frequency domain and region based rule is used in high frequency domain. The inverse NSCT is applied to get the fused image.</p><p>To effectively analyse the geometric structure of remote images with computationally efficient approach , a SIST and regional selection algorithm is suggested by Luo et al. <ref type="bibr" target="#b31">[32]</ref> for remote sensing image fusion application. The feature vectors from MS and PAN images are</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T 11</formula><p>divided into regions using fuzzy c-means (FCM) clustering. Based on the regional similarity, an adaptive multi-strategy fusion rule of high frequency band is suggested. Finally, the fused image is found by taking the inverse SIST and inverse entropy component analysis.</p><p>The local features of an image are usually not considered for fusion. To resolve this, Wang et al. suggested a fusion procedure based on the DWFT and regional characteristics <ref type="bibr" target="#b32">[33]</ref>. The transform coefficients are found from the two input images using the DWFT.</p><p>Taking the mean of the transform coefficients, an average image is obtained which represents the approximate features of the input images. The average image is segmented based on the region features to get the region coordinates. The coefficients and the region coordinates are mapped. The fused image is produced by combining the coefficients of each region using suitable fusion rules. However, it needs a relatively long time for processing due to the image segmentation process involved.</p><p>Researchers have extensively used ICA for image fusion <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. Mitianoudis and Stathaki <ref type="bibr" target="#b33">[34]</ref> used ICA for region based image fusion. The authors investigated the effectiveness of a transform utilizing ICA and topographic ICA bases in image fusion. The fused image in ICA domain is obtained by utilizing new pixel and region based fusion rules.</p><p>The suggested method demonstrated better performance as compared to conventional wavelet approaches at the cost of slightly more computational load. Contrary to the suggested framework, Cvejic et al. used ICA bases for region based multimodal image fusion <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The authors used different training subsets to identify the most significant regions in the source images. Subsequently, they combined the ICA coefficients utilizing the fusion metrics for enhanced results. In <ref type="bibr" target="#b34">[35]</ref>, a combined approach is used to obtain the most significant regions from the source images. The fused image is obtained by merging the ICA coefficients from the obtained regions. The authors used the Piella metric to enhance the quality of results. The performance improved with an increase in the computational complexity.</p><p>These proposed methods exhibit the following drawbacks: (i) the approaches cannot be easily extended to multiple sensor fusion application, (ii) there is no theoretical justification for the presence of the global optimum of the objective function derived from the Piella index. To overcome this, in <ref type="bibr" target="#b36">[37]</ref>, the authors used optimization of the Piella index for multiple input sensors. To get the optimal solution, the authors revisited the previously proposed work in <ref type="bibr" target="#b33">[34]</ref> and further suggested a method to find the optimum intensity range through optimization of a fusion index. The suggested method improves the original ICAbased framework and produces a better fused image. In <ref type="bibr" target="#b37">[38]</ref>, the authors extended their work to a more advanced region based fusion approach. A group of fusion rules using textural info is presented. The suggested method enhances the performance than the max-abs fusion rule in</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T 12</formula><p>case of multifocus images. On the contrary, it was not so good for multimodal image fusion.</p><p>The reason may be the various modality images have different texture properties. In <ref type="bibr" target="#b38">[39]</ref>,</p><p>Omar et al. used the combination of Chebyshev polynomial (CP) and ICA depending on regional information of source images. The proposed method used segmentation technique to identify features such as texture, edge, etc. The fused image is found using distinct fusion rules as per the selected regions. The advantage of this method is that it offers an autonomous denoising property, combining the benefits of both CP and ICA. Nirmala et al. proposed a region based multimodal (visible and Infrared ) image fusion scheme using ICA and SVM <ref type="bibr" target="#b39">[40]</ref>. The source images are jointly segmented in the spatial domain. The significant features of every region are calculated. The ICA coefficients of the particular regions are combined to form the fused ICA representation. As the ICA bases are computed and SVM is trained, the suggested technique may seem to multiply the computational load.</p><p>In recent studies, many optimization methods have been utilized by researchers for image fusion. Neural networks (NN) have been extensively employed for image fusion <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>. Hsu et al. suggested a multi-sensor image fusion method using ANN, which merges the features of the feature and pixel level fusion scheme <ref type="bibr" target="#b42">[43]</ref>. The basic concept is to segment only the far infrared image. The information from each segmented region is added to the visual image. The different fused parameters are determined according to the different regions. In <ref type="bibr" target="#b45">[46]</ref>, PCNN has been utilized for the fusion of multi-sensor images. The procedure begins with the segmentation of source images using PCNN and the output is used to direct the fusion process. The suggested method outperforms the pixel based methods in terms of blurring effect, sensitivity to noise and misregistration. Saeedi and Faez <ref type="bibr" target="#b46">[47]</ref> proposed a fusion of visible and IR image using fuzzy logic and PSO. The high frequency wavelet coefficient of IR and visible images is fused using fuzzy based approach. The PSO is suggested for the low frequency fusion rule. The low frequency and high frequency parts of the wavelet coefficient are fused. The authors have not considered noise in their work. It is evident that for noisy images, the segmentation process will result in over segmentation with inaccurate regions. Aslantas et al. described a new method for thermal and visible images <ref type="bibr" target="#b47">[48]</ref>. Instead of using a single weighting factor, the authors have used multiple weighting factors for distinct regions to get the fused image. The weighting factors are optimized using DE. A new image fusion metricsum of correlation difference is formed to assess the performance of the fused images during the optimization procedure. Some other methods have also been proposed by many researchers. Zheng and Qin used BEMD technique for region based image fusion <ref type="bibr" target="#b48">[49]</ref>. The source images are partitioned into several intrinsic mode functions and a residual image. The process of fusion is carried</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T 13</formula><p>out as per the segmentation of the source images, which produces a combined BEMD representation. The fused image is found by using the inverse BEMD. Because of the finite length of wavelet function, DWT induces energy leaking. This problem does not occur in BEMD because it is considered as an adaptive highpass filter. Li and Yang <ref type="bibr" target="#b49">[50]</ref> proposed a novel method for multifocus images utilizing region segmentation and spatial frequency. The normalized cut method is utilized to segment the intermediate fused image. The two input images are segmented as per the segmenting results of the intermediary fused image. The fused image is obtained by merging the segmented regions using spatial frequency. The advantage of the suggested technique is that it does not use the multiresolution approach, as few information of the input image may be missed while performing the inverse multiresolution operation. The limitation of the proposed method is that the computational time is more as compared to the wavelet based approaches, as the segmentation procedure takes more time.</p><p>In most of the papers, a common segmentation technique for different input images taken from distinct sensors have not been used for region based image fusion. These approaches are restricted to the particular input images only. Luo et al. proposed the method of region partition strategy in which the segmentation is performed on the similar features of input images (irrespective of the kind of input images) <ref type="bibr" target="#b50">[51]</ref>. The complementary and redundant correlations of the source images are distinguished by using fusion methods. On the basis of the similar components, the small homogeneous regions are merged. The final region map is obtained by comparing the resemblance between the images. The method has several advantages like the generality of application, superior visual perception and simple realization without parameter setting.</p><p>Chen and Qin <ref type="bibr" target="#b51">[52]</ref> used CS theory in the region based fusion framework. The authors considered both compression capabilities of sensors and the intelligent understanding of the image features for fusion. In dynamic scene, it is very hard to accurately determine whether a pixel or region is blurry or not by utilizing only the focus information only. Besides, another limitation of the pixel based method is that the fusion results obtained is not accurate when the image patterns become complex. In contrast to this, Li et al. proposed image matting fusion technique for the fusion of multifocus images in dynamic scenes <ref type="bibr" target="#b52">[53]</ref>. The algorithm uses morphological filters to get rough segmentation results followed by image matting to obtain the accurate focussed regions. The fused image is found by merging the focussed regions. Chen et al. <ref type="bibr" target="#b53">[54]</ref> used the SR method for the fusion of multifocus images. The suggested technique merges the advantages of regional and sparse representation based fusion to obtain the fused image. The fusion of high resolution images using mean shift</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 14 segmentation method is described in <ref type="bibr" target="#b54">[55]</ref>. The authors used SSIM for the measurement of regional similarity.</p><p>These methods need a precise selection of segmentation technique. The choice of segmentation technique is crucial to obtain a fused image. In these approaches, the regions are chosen and fused based on some regional characteristics. The significance of the statistical characteristics of the regions has not been considered, which is utilized to enhance the precision of the decision process in image fusion applications.</p><p>Most of the region based image fusion procedures suggested by different researchers have not applied the estimation theory approach rigorously. In the statistical and estimation based algorithms, first the input images are partitioned into regions by using some sophisticated region segmentation algorithms. A joint region map is developed by analysing the region map of each source image to produce the fused image. A statistical image formation model is developed for every region in the joint region map. The estimation procedure is utilized in combination with the model to build an iterative fusion process to determine the model constraints and to generate the fused image. A typical flow diagram of statistical and estimation based algorithm explained in <ref type="bibr" target="#b57">[58]</ref> is shown in Fig. <ref type="figure" target="#fig_7">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sharma et al. [56] suggested a Bayesian fusion scheme inspired by estimation theory.</head><p>A statistical signal processing method to image fusion is suggested by Yang and Blum in <ref type="bibr" target="#b56">[57]</ref>. Many researchers have proposed image fusion using the EM algorithms <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. In <ref type="bibr" target="#b57">[58]</ref>, the authors used the EM algorithm, which utilizes the features of regions for fusion in an optimal way. Each region is built by a statistical image formation model. The region-level EM algorithm is developed by using the EM fusion procedure in combination with the model to produce the fused image. Zhang and Ge <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref> proposed image fusion method employing energy estimation approach. To partition the source images into regions on the basis of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>15 homogeneity, piecewise smooth Mumford and Shah Model is used. To speed up convergence, a level set based optimization procedure is combined with it. The fusion quality is determined using an energy model. The methods proposed in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58]</ref>, fail to consider the importance of the statistical features of regions, which is utilized to enhance the precision of the decision process in image fusion applications. In some of the research articles, statistical model for fusion and the segmentation of images are integrated. Wan et al. integrated the multi-scale image segmentation technique and statistical feature extraction in the suggested framework <ref type="bibr" target="#b62">[63]</ref>. A region map of input images is obtained using DT-CWT and statistical region combining algorithm. The source images are divided into significant regions that contain salient information using symmetric alpha stable distribution. By employing the BαS technique, region features are modelled. The fused image is obtained by applying a segmented driven approach in the complex wavelet domain.</p><p>In most of the practical applications, the dimensions of the training data set are too large which accounts for a large computation time in statistical image fusion. This constraint requires data reduction. This is achieved by choosing a suitable subset of the prime training data set without compromising the image fusion precision appreciably. Zribi <ref type="bibr" target="#b63">[64]</ref> proposed a non-parametric and region based image fusion method following the principle of bootstrap sampling. This brings down the dependence effect of pixels in true image and also reduces the fusion time. The image sensors are expressed as a real scene degraded by distortion in the statistical image formation model. The authors used a non-parametric EM procedure to determine the model constraints and the fused image. However, the proposed method utilized only two source images for fusion.</p><p>These methods characterized the segmented regions using the statistical and estimation approaches. The quality of image segmentation is vital for determining accurate segmented regions. The ROI and boundary detection of focused regions is not accurately obtained with the methods discussed above. Many researchers have also used saliency map based algorithms for region based image fusion. In general, salient object detection is an image/background segmentation problem and aims at the separation of the significant foreground object from the background. It is marginally different from the conventional segmentation procedure in which image is segmented into the perceptually coherent regions.</p><p>To select an optimal region extraction method, researchers have compared many saliency analytical methods <ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>. Fig. <ref type="figure" target="#fig_8">8</ref> shows the flow diagram of focus region detection and saliency map based algorithm described in <ref type="bibr" target="#b75">[76]</ref>. The problem of the spatial domain based methods is that they may generate artefacts or imprecise results at the focused border areas, as the boundary of focused areas cannot be estimated correctly. This problem is solved by the use of multi-scale transform method.</p><p>However, the problem with this method is to choose the proper fusion rule. Additionally, some information of the input image may be lost while implementing the inverse multi-scale transform. So as to avoid these problems, the authors proposed methods combining the benefits of spatial region based and transform domain <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref> In these approaches, the regions are selected and fused based on certain regional characteristics. However, the importance of the statistical characteristics of the regions are not considered, which is utilized to enhance the precision of the decision process in image fusion applications. The statistical and estimation approach uses the statistical characteristics to improve the precision of the decision process. It is needed to choose an efficient image segmentation technique for determining accurate segmented regions. However, the ROI and boundary detection of focused regions is not accurately obtained with these approaches. The focus region detection and saliency map approaches uses the ROI and boundary detection to accurately obtain the fused image. The selection of segmentation technique is not vital in these approaches.</p><p>However, these approaches are complex and application specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Performance evaluation</head><p>There are two methods in which the quality of the fused image is computed i.e., qualitative or subjective and quantitative or objective. Qualitative analysis means visual analysis in which the fused image is matched with the source images by a group of observers.</p><p>The analysis of the fused image considers different optical parameters like spatial details, the size of the object, colour etc. The qualitative analysis is typically accurate if it is done correctly. However, these methods are inconvenient, expensive and consumes more time. It is a very difficult task in most of the image fusion applications due to lack of availability of a ground truth image that is perfectly fused. So, another technique to calculate the fusion performance is the quantitative or objective evaluation. But again, it remains an issue as how to measure the performances of the fused image objectively. In this survey, the quantitative fusion assessment metrics are classified in two groups based on the presence or absence of a reference fused image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Objective fusion evaluation metrics with reference image</head><p>The reference image is the ideal fused image that is taken as a ground truth image for validating the image fusion algorithm. The ground truth image may be available or manually constructed. A list of commonly used objective fusion evaluation metrics with reference image is illustrated in Table <ref type="table">2</ref>. The quality metrics employed to compute the fusion performance are peak signal to noise ratio (PSNR), root mean square error (RMSE), mutual information (MI), structural similarity index measure (SSIM), correlation coefficient (CC) and universal quality index (UQI). The symbols used in the expressions carry the same meaning as depicted in the corresponding reference papers mentioned in table 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Objective fusion evaluation metrics with reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sl.</head><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality metric</head><p>Description Formula Reference 1 PSNR It is calculated as the ratio of number of intensity levels in the image to the related pixels in the ideal and the fused image. A higher PSNR value indicates superior fusion. ,,</p><formula xml:id="formula_5">      10 2 2 1 1 20 log 1 , , M N r f i j PSNR L I i j I i j MN                  [79]</formula><formula xml:id="formula_6">MN ij RMSE MN R i j F i j      [80] A C C E P T E D M A N U S C R I P T 20 3 MI</formula><p>The similarity of image intensity between ideal and fused image is measured using mutual information. MI should be high for a better fusion performance. </p><formula xml:id="formula_7">C C SSIM x y C C               [82]</formula><p>5 CC It is used to compute the spectral feature similarity between the reference and the fused image. The value of CC should be high i.e. close to 1.</p><formula xml:id="formula_8">        , , ,<label>2 2 , , , , ˆˆ.</label></formula><p>ˆî</p><formula xml:id="formula_9">j i j ij i j i j i j i j X x X x CC X x X x                           [83] 6 UQI</formula><p>The UQI is used to calculate how much amount of relevant data is transformed from ideal image into the fused image. The value of this metric ranges between -1 to 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>   </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective fusion evaluation metrics without reference image</head><p>In very rare case, the ground truth image is available. Hence, it is highly desirable to assess the quality of the fused image without taking the ideal image. The meaning of quality metric with the reference image is that either the ground truth image or the ideal image is available. The meaning of quality metric without the reference image is that either the ground truth image or the ideal image is not available. In such cases the quality metric is computed using the source or input images and the output or fused image. A commonly used list of the objective fusion evaluation metrics without ideal/reference image is illustrated in Table <ref type="table">3</ref>.</p><p>The quality metrics used to evaluate the fusion performance are standard deviation (SD), entropy (H), cross entropy (CE), spatial frequency (SF), fusion mutual information (FMI), sum of the correlation of difference (SCD), Piella metric and Petrovic metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Objective fusion evaluation metrics without reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sl.</head><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Formula References </p><formula xml:id="formula_10">A C C E P T E D M A N U S C</formula><formula xml:id="formula_11">              0 0 0 1 , , 1 , wW Q W ab w Q a f w w Q b f w                        0 ,, , 1 , W wW Q a b f Cw w Q a f w w Q b f w              ,, , , , , E WW Q a b f Q a b f Q a b f       [89] 8 Petrovic metric (Q AB/F )</formula><p>It provides the matching between the edges transmitted in the fusion procedure. The dynamic range of  The quantitative assessment for the existing methods is provided in form of tables.</p><formula xml:id="formula_12">Q AB/F is [0, 1].               / 1 1 1 1 , ,<label>, , , , AB F</label></formula><p>The different tables are prepared after considering different references using similar test conditions, same application and using the same source images. The symbol '-' in all the tables represent unavailability of the particular information. The particular performance metric is not evaluated due to non-availability of the fused image. A comparison of different methods for multifocus image fusion application is illustrated in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Comparison of different methods for Multifocus images (Reference: Clock Image). Therefore, these values may be computed for the above mentioned methods for a fair comparison. A comparison of different methods for medical image fusion application is depicted in Table <ref type="table">6</ref>.</p><formula xml:id="formula_13">Method MI Q AB/F H SF Q 0 Q w Q E RSSF [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Comparison of different methods for Medical images (Reference: CT and MRI).   <ref type="bibr" target="#b71">[72]</ref>, (d) CS <ref type="bibr" target="#b51">[52]</ref>, (e) shearlet and GBVS <ref type="bibr" target="#b74">[75]</ref>, (f) NSCT and focused area detection <ref type="bibr" target="#b72">[73]</ref>, (g) LSWT <ref type="bibr" target="#b69">[70]</ref>, (h) SR <ref type="bibr" target="#b53">[54]</ref>, (i) BEMD <ref type="bibr" target="#b48">[49]</ref>, (j) BEMD <ref type="bibr" target="#b50">[51]</ref>, (k) DWFT <ref type="bibr" target="#b32">[33]</ref>,(l) RSSF <ref type="bibr" target="#b28">[29]</ref>, (m) RF-SSIM <ref type="bibr" target="#b50">[51]</ref>, (n) RSSF <ref type="bibr" target="#b50">[51]</ref>, (o) RSSF <ref type="bibr" target="#b53">[54]</ref>, (p) DWT and highboost filter <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_14">M A N U S C R I P T 24 Method MI Q AB/F Q 0 Q w Q E H RF-SSIM [</formula><p>As shown in Fig. <ref type="figure" target="#fig_13">9</ref>, the input images are represented in   The ICA based methods perform better in case of the multimodality image fusion applications. The region partition algorithms are used in medical image fusion applications producing better fusion results. It is observed that the saliency map method is an emerging technique that can be used in many applications. The quality assessment metrics Q AB/F and MI is mostly preferred in all the applications. However, the other metrics can also be </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A state-of-the-art survey on region based image fusion is presented. A first hand classification of region based fusion methods is fostered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>is the forward transformation operator and 1 TFig. 1 .</head><label>11</label><figDesc>Fig. 1. Image fusion example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. A generic classification of image fusion methods.Most of the fusion applications need analysis of multiple images of the same scene for improved results. For instance, in the medical imaging applications, computer tomography (CT), magnetic resonance (MR) and positron emission tomography (PET) images are fused</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>have used different image reconstruction and decomposition techniques, namely multiresolution based, sparse representation based and salient feature based. They have experimented with different fusion rules and compared the results using different image quality assessment parameters. Most of these methods describe the image fusion techniques based on pixels. However, it has several limitations: (i) blurring effects, (ii) high sensitivity to noise and (iii) misregistration. These limitations may be reduced by deploying the region based image fusion techniques. These techniques have the capability to utilize intelligent fusion rules. As far as our knowledge is concerned, a survey on region based image fusion methods is not available in the literature. This has motivated us to carry out a survey on different region based image fusion methods. In this context, we present a survey on image fusion methods based on the regions. Mainly, the paper focusses on different approaches and ideas of the existing region based fusion practices. The results of the different techniques are summarized. An elaborate discussion is presented at the end, comparing the various fusion methods. This may clearly set a path for more investigations into region based fusion techniques. This may open doors to new unsolved problems in the domain of image fusion.A C C E P T E D M A N U S C R I P T 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Classification of region based image fusion methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A schematic block representation of region based image fusion method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Flow diagram of region based BEMD fusion scheme [49].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Flow diagram of region based EM fusion scheme<ref type="bibr" target="#b57">[58]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Flow diagram of region based image fusion scheme of NSCT based method [76].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>. Chai et al. suggested a multifocus image fusion technique utilizing the focus region detection and multiresolution approach<ref type="bibr" target="#b69">[70]</ref>. The authors have used the focus region detection and LSWT to combine the benefits of spatial and transform domain based fusion approaches. The idea of local visibility (LV) in LSWT domain is used for the fusion of the lowpass subband coefficients. The sum modified Laplacian inspired local visual contrast rule is applied for the fusion of the highpass subband coefficients. However, the fused image consists of several imprecise results at the boundary of the focused region. To overcome this, the authors proposed a multifocus image fusion method based on NSCT and focus region detection<ref type="bibr" target="#b70">[71]</ref>.The image visibility rule in NSCT domain is used for the fusion of lowpass subband coefficients. The local area spatial frequency rule is employed for the fusion of highpass A C C E P T E D M A N U S C R I P T 18 produce a primary fused image. At last, the final fused image is found by combining the primary fused image with the object region. Again in [77], Meng et al. presented the region based image fusion technique to merge IR and visible image by employing the saliency map and interest point. A saliency map is built using a saliency detection process for IR image. Further, it is explored to identify interest points. To get the salient region, a convex hull of the salient interest point is computed. The first saliency map is developed by merging the convex hull of the salient interest points. Finally, the various fusion rules are employed for object region and background. In [78], Han et al. proposed a saliency aware fusion procedure for multimodal image fusion (IR and visible image) to enrich the visualization of the visible image. The process is used for saliency recognition followed by a bias fusion. The information of these two sources is combined using the Markov random fields. The following fusion phase is used to bias the end result favouring the visible image, excepting when a region has distinct IR saliency. The fused image represents both salient foreground from the IR image and background as provided by the visible image. The fused image obtained with these methods has significant improvement compared to other methods, because it avoids the traditional segmentation process. The transform used in these methods may consume more time. Still, the different methods have their own importance. The further improvement in the fusion quality may require a suitable selection of the regional features. The region partition based algorithms are simpler than the other two approachesstatistical and estimation based, focus region detection and saliency map based. The region partition based algorithms need a precise selection of segmentation technique. The choice of segmentation technique is crucial to obtain a fused image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 RMSE</head><label>2</label><figDesc>It estimates the quality of the fused image by relating the ideal and the fused image. The value of RMSE should be lower i.e. close to zero for a good fused image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>inconvenient and time consuming. The fused images for different image fusion applications are depicted below. The experimental results obtained for clock images carried out by different researchers are shown in Fig. 9. Experimental results for clock images, (a) right focus, (b) Left focus, (c) QWT and normalized cut</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) and (b). In (a), the left clock is out of focus and in (b) the right clock is out of focus. The fused images of different methods are illustrated in fig (c)-(p). It is observed that almost all the images look similar. Therefore, the subjective evaluation, in general, is treated as an ineffective tool for comparison. However, it is seen that the visual quality of the images in (c), (e), (f) and (g) is better as compared to other methods. The image in (c) and (f) contain sharp edges at the upper portion of the right clock. The fused image obtained with these methods have retained more relevant information of the source images. In (i) and (j), the edge at the top of the right clock is not sharp i.e. fractional edge information is lost. Further, the images in (d) and (k) have blurry clock hands. The experimental results obtained for IR and visible image fusion application carried out by different researchers are shown in Fig. 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 . 26 AsFig. 11 .</head><label>102611</label><figDesc>Fig. 10. Experimental results for IR and visible image fusion application, (a) Visible image, (b) IR image, (c) FPSO<ref type="bibr" target="#b46">[47]</ref>, (d) ICA-SVM<ref type="bibr" target="#b39">[40]</ref>, (e) BαS<ref type="bibr" target="#b62">[63]</ref>, (f) Region based ICA<ref type="bibr" target="#b35">[36]</ref>, (g) Region based ICA<ref type="bibr" target="#b39">[40]</ref>, (h) DT-CWT<ref type="bibr" target="#b39">[40]</ref>, (i) DT CWT<ref type="bibr" target="#b62">[63]</ref>, (j) DT-CWT<ref type="bibr" target="#b46">[47]</ref>,(k)DT-CWT<ref type="bibr" target="#b35">[36]</ref> </figDesc><graphic coords="26,97.48,620.20,84.60,70.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>In this paper, we have presented a survey on region based image fusion methods. The region based image fusion algorithms are classified, for the first time, into three classes: region partition based, statistical and estimation based, and focus region detection and A C C E P T E D M A N U S C R I P T 27 saliency map based. A comparison of different methods in terms of various metrics for different applications is done. Based on this comparison, an idea about the various image fusion methods is developed for different applications. The focus region detection and saliency map based algorithm is mostly suitable for the multifocus image fusion applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>computed for a fair comparison. The problems existing in different methods is discussed. The survey carried out in this paper may help the researchers in further research in the domain of region based image fusion. A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Classification of various region based image fusion algorithms.</figDesc><table><row><cell>Algorithm</cell><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Image fusion is very important and beneficial for various image processing steps such as object extraction, identification and computer vision. It is a tedious job due to misregistration, distortion and other artefacts. Recently, in many applications like photography<ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref>, medical diagnosis<ref type="bibr" target="#b93">[94]</ref><ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref>, surveillance<ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref>, remote sensing[100-  102]  etc., the region based image fusion has been widely used. Different methods are proposed in the literature for region based image fusion, as discussed in section 2. A comparison among various methods is quite a difficult task, as they use different modalities, databases and performance indices. The assessment of quality of fused image is carried out in two ways i.e. qualitative and quantitative. In this paper, a comparison of various methods is carried out for different applications using quantitative and qualitative analysis. This comparison would benefit the researchers to apply different methods in the various applications.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>It is seen that most of the methods perform well for the multifocus fusion example in terms of MI. However, the methods listed under the focus region detection group performs better as compared to other methods. A similar trend is observed for the parameter Q AB/F . For instance, QWT and normalized cut, NSCT and focus area detection and Surface area based methods give a value of 8.9971, 8.6580, and 8.8280 respectively for MI. The reason may be (i) presence of superiorities such as multiresolution, multidirection and shift-invariance, (ii) use of the detected focus area as a fusion decision map to drive the fusion process, (iii) preventing artefact and imprecise results at the boundary of the focus region. Further, few researchers have computed the remaining metrics. So, these metrics may be calculated for the given methods for a fair comparison. A comparison of various methods for IR and visible image fusion application is illustrated in Table5. Comparison of different methods for IR and visible images (Reference: UN Camp Image).It is observed that the ICA based methods outperform other methods in the IR and visible image fusion application. For instance, the ICA-SVM method gives a value of 0.61 for Q AB/F , 7.1700 for MI, 7.1800 for H, 31.4100 for SD, 0.9500 for the Piella metric and 0.7118 for Q w which is better as compared to other methods. The reason may be the ICA based methods perform segmentation and uses numerous statistical properties of the regions to make intelligent decisions. The researchers have not computed most of the metric values.</figDesc><table><row><cell>29]</cell><cell></cell><cell cols="4">6.9279 0.7119 8.7813 10.3350 0.7138 0.8312 0.6618</cell></row><row><cell>BEMD [49]</cell><cell></cell><cell cols="4">5.9016 0.6250 7.4562 8.8864 0.6170 0.7690 0.6500</cell></row><row><cell>RF-SSIM [51]</cell><cell></cell><cell cols="4">7.0572 0.4285 7.4262 9.1006 0.7608 0.8516 0.6652</cell></row><row><cell>BEMD [51]</cell><cell></cell><cell cols="4">6.1665 0.4830 7.3462 9.1925 0.7133 0.8342 0.6632</cell></row><row><cell>RSSF [51]</cell><cell></cell><cell cols="4">8.6973 0.7032 7.3691 8.9858 0.7628 0.8212 0.6516</cell></row><row><cell cols="2">DWT and highboost</cell><cell cols="4">7.7344 0.7018 8.8066 10.0048 0.7608 0.8119 0.6521</cell></row><row><cell>filter [29]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SR [54]</cell><cell></cell><cell cols="4">5.6106 0.7533 7.0657 8.4682 0.6911 0.7814 0.5918</cell></row><row><cell>CS [52]</cell><cell></cell><cell cols="4">4.7918 0.4261 7.4123 8.5618 0.6124 0.7764 0.4939</cell></row><row><cell>LSWT [70]</cell><cell></cell><cell cols="4">8.5518 0.7246 7.1549 8.0456 0.6818 0.7631 0.5825</cell></row><row><cell>QWT</cell><cell>and</cell><cell cols="4">8.9971 0.7443 7.3419 8.3981 0.7632 0.8428 0.6649</cell></row><row><cell cols="2">normalized cut [72]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NSCT and focus</cell><cell cols="4">8.6580 0.7502 7.2906 8.4729 0.7529 0.8341 0.5823</cell></row><row><cell cols="2">area detection [73]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Surface area based</cell><cell>8.8280 0.7400 -</cell><cell>13.6500 -</cell><cell>-</cell><cell>-</cell></row><row><cell>[74]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Shearlet and GBVS</cell><cell cols="4">7.8397 0.7168 7.4337 8.7078 0.6812 0.7355 0.5862</cell></row><row><cell>[75]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The results indicated in the table shows that the region partition based algorithms perform better than the other methods. For example, the RF-SSIM method gives a value of 5.6274 for MI, 0.8928 for Q 0 , 0.8027 for Q w , 0.6283 for Q E and 6.8614 for H which is better than the other methods. The reason may be the methods merge the homogenous regions based on SSIM. The statistical and estimation based algorithms also show promising results but using other metrics. So researchers may compute these metrics for a fair comparison.</figDesc><table><row><cell>51]</cell><cell>5.6274</cell><cell>0.6259</cell><cell>0.8928</cell><cell>0.8027</cell><cell>0.6283</cell><cell>6.8614</cell></row><row><cell>RSSF [51]</cell><cell>3.4326</cell><cell>0.5017</cell><cell>0.4372</cell><cell>0.4034</cell><cell>0.4128</cell><cell>5.4359</cell></row><row><cell>BEMD [51]</cell><cell>2.1028</cell><cell>0.4604</cell><cell>0.5293</cell><cell>0.7145</cell><cell>0.5612</cell><cell>6.8879</cell></row><row><cell>CS [52]</cell><cell>2.8958</cell><cell>0.5195</cell><cell>0.5634</cell><cell>0.7231</cell><cell>0.6275</cell><cell>6.3807</cell></row><row><cell>BαS [63]</cell><cell>-</cell><cell>0.6808</cell><cell>-</cell><cell>0.7541</cell><cell>-</cell><cell>-</cell></row><row><cell>DT-CWT [63]</cell><cell>-</cell><cell>0.6339</cell><cell>-</cell><cell>0.6722</cell><cell>-</cell><cell>-</cell></row></table><note><p>The subjective or qualitative evaluation reflects the visual perception of the image, which varies from viewer to viewer. It is evaluated properly by the experts who have long experience in the field. However, this type of evaluation is not always preferred, as it is</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 17 subband coefficients. The limitation of the methods is that the post-processing phase utilizing morphological procedure is not robust. In order to avoid this problem, <ref type="bibr">Liu et al.</ref> suggested a multifocus image fusion utilizing QWT <ref type="bibr" target="#b71">[72]</ref>. Initially, the authors used the local variance of the phases to identify the focus or defocus for each pixel. They segmented the focus detection result using normalized cut to eliminate the detected errors. Finally, the fused image is found utilizing the spatial frequency as fusion weight along the edge of the focus region. However, the method may create false information and irregular phenomenon at the boundaries of the focused areas, as the boundary cannot be estimated precisely. To perfectly determine the boundaries of the focused region, Yang et al. suggested a novel hybrid multifocus image fusion method based on NSCT and focus area detection <ref type="bibr" target="#b72">[73]</ref>. The authors modified their work in the sense that they used the log Gabor filter for high frequency subband coefficients. The limitation of the method is that the NSCT procedure is consuming more time. Nejati et al. proposed a new focus measure depending on the surface area of the region surrounded by juncture points for multifocus image fusion <ref type="bibr" target="#b73">[74]</ref>. The objective of this metric is to differentiate focus regions from blurred regions. The juncture points of the source images are computed and segmented utilizing these juncture points. The surface area of every region is used as a quantity to get the focused regions. An initial selection map for the fusion is obtained using this measure which is refined using morphological operations.</p><p>The most challenges task in region based image fusion is the proper image segmentation. In recent years, numerous state-of-the-art saliency region detection approaches have been suggested. Still, there are some shortcomings with them. The most noticeable among them is the salient region detection approaches which may focus the object region as well as some of the background region. Detection of the visual salient region has been an ongoing research process for a long time. The saliency map is an emerging technique to identify the salient region, which can overcome the above problem. Zhang et al. suggested a multifocus image fusion technique based on focus region extraction. The authors used saliency analysis <ref type="bibr" target="#b74">[75]</ref>. The GBVS procedure is utilized to identify the focused region in the input image. Subsequently, watershed and morphological techniques are employed to find the bounded area of saliency map and eliminate the pseudo focus area. In the final step, the focused area is fused straight and the residual area are fused using shearlet transform.</p><p>In <ref type="bibr" target="#b75">[76]</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image fusion: Advances in the state of the art</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goshtasby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="118" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An integrated framework for the spatio-temporalspectral fusion of remote sensing images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7135" to="7148" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pansharpening with a guided filter based on three-layer decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1068</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthesis of multispectral images to high spatial resolution: A critical review of fusion methods based on remote sensing physics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1301" to="1312" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pixel-level image fusion: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="100" to="112" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Review of pixel-level image fusion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Shanghai Jiaotong Univ</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="6" to="12" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion based on sparse feature matrix decomposition and morphological filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Commun</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving hyperspectral image classification by combining spectral, texture, and shape features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mirzapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghassemian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1070" to="1096" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DT-CWT: Feature level image fusion based on dual-tree complex wavelet transform</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Maruthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Information Communication and Embedded Systems (ICICES)</title>
		<meeting>IEEE International Conference on Information Communication and Embedded Systems (ICICES)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decision Based Fusion for Pansharpening of Remote Sensing Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murtaza Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bienvenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decision-level fusion of infrared and visible images for face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yunfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yixin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dongmei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Control and Decision Conference (CCDC)</title>
		<meeting>Control and Decision Conference (CCDC)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2411" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decision Fusion for the Classification of Urban Remote Sensing Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Atli</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2828" to="2838" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">From Multi-Scale Decomposition to Non-Multi-Scale Decomposition Methods: A Comprehensive Survey of Image Fusion Techniques and Its Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Access</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16040" to="16067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fusion technique for grey-scale visible light and infrared images based on non-subsampled contourlet transform and intensity-hue-saturation transform</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET signal processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="80" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusion method for remote sensing image based on fuzzy integral</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electrical and Computer Engineering</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A categorization of multiscale-decomposition-based image fusion schemes with a performance study for a digital camera application</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1315" to="1326" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pixel-level image fusion techniques in remote sensing: a A</title>
		<author>
			<persName><forename type="first">V</forename><surname>Solanky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Katiyar ; C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T 29 review, Spat. Inf. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="475" to="483" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A review of remote sensing image fusion methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ghassemian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="75" to="89" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Critical Comparison Among Pansharpening Algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vivone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Licciardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Restaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2565" to="2586" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Review of Image Fusion Based on Pulse-Coupled Neural Network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Comput. Methods Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="659" to="671" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An overview of multi-modal medical image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical image fusion: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Dasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Region-based image fusion scheme for concealed weapon detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference on Information Sciences and Systems</title>
		<meeting>the 31st Annual Conference on Information Sciences and Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Region-based wavelet fusion of ultrasonic, radiographic and shearographic non-destructive testing images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Shark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th World Conference on Non-Destructive Testing</title>
		<meeting>the 15th World Conference on Non-Destructive Testing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A region-based multiresolution image fusion algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Information Fusion</title>
		<meeting>IEEE International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1557" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A general framework for multiresolution image fusion: from pixels to regions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information fusion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="280" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pixel-and region-based image fusion with complex wavelets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J O Õ</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
	<note>information fusion</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A novel region based multimodality image fusion method</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pattern Recognition Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Region based fusion of infrared and visible images using nonsubsampled contourlet transform,Chin</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt.Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="338" to="341" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Region-based Image Fusion Algorithm for Detecting Trees in Forests</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Cybernetics &amp; Systemics Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="540" to="545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel algorithm of remote sensing image fusion based on shift-invariant Shearlet transform and regional selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEU -International Journal of Electronics and Communications</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">New region-based image fusion scheme using the discrete wavelet frame transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Intelligent Control and Automation</title>
		<meeting>IEEE International Conference on Intelligent Control and Automation</meeting>
		<imprint>
			<publisher>WCICA</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3066" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pixel-based and region-based image fusion schemes using ICA bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive region-based multimodal image fusion using ICA bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Information Fusion</title>
		<meeting>IEEE International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Region-based multimodal image fusion using ICA bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="743" to="751" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal contrast correction for ICA-based fusion of multimodal images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE sensors journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2016" to="2026" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Region-based ICA image fusion using textural information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Antonopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Digit. Signal Process</title>
		<meeting>IEEE International Conference on Digit. Signal Process</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region-based image fusion using a combinatory Chebyshev-ICA method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving independent component analysis using support vector machines for multimodal image fusion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Nirmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaidehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1117" to="1132" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BFO-ICA based multi focus image fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Swarm Intelligence (SIS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="194" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A novel ICA domain multimodal image fusion algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Of International Society for Optics and Photonics on Multisensor, Multisource Information Fusion: Architectures, Algorithms, and Applications</title>
		<meeting>Of International Society for Optics and Photonics on Multisensor, Multisource Information Fusion: Architectures, Algorithms, and Applications</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6242</biblScope>
			<biblScope unit="page">62420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Region-based Image Fusion with Artificial Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Inf.Math.Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="264" to="267" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multifocus image fusion using artificial neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="985" to="997" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An approach for multi-focus image fusion using neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pagidimarry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligent Systems and Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="732" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A region-based image fusion scheme using pulse-coupled neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1948" to="1956" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using fuzzy logic and population-based optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1041" to="1054" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">New optimised region-based multi-scale image fusion method for thermal and visible images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aslantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Toprak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="289" to="299" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Region-based image fusion method using bidimensional empirical mode decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13008" to="013008" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multifocus image fusion using region segmentation and spatial frequency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Visual Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="979" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A regional image fusion based on similarity characteristics, Signal Processing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1268" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Region-based image-fusion framework for compressive imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics</title>
		<imprint>
			<date type="published" when="2014-10-29">2014 Oct 29. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image matting for fusion of multi-focus images in dynamic scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="162" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Regional multifocus image fusion using sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5182" to="5197" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A region-based technique for fusion of high resolution images using mean shift segmentation, The International Archives of the Photogrammetry</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Li Shuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part B</title>
		<imprint>
			<biblScope unit="volume">XXXVII</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic image sensor fusion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="824" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A statistical signal processing approach to image fusion for concealed weapon detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A region-based image fusion method using the expectationmaximization algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Information Sciences and Systems</title>
		<meeting>IEEE Conference on Information Sciences and Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">EM image fusion algorithm based on statistical signal processing</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2nd International Congress on Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image fusion using the expectation-maximization algorithm and a hidden Markov model in</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Vehicular Technology</title>
		<meeting>IEEE Conference on Vehicular Technology</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4563" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Region-based image fusion using energy estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing</title>
		<meeting>IEEE International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Adaptive region-based image fusion using energy evaluation model for fusion decision, Signal, Image and Video Processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Segmentation-driven image fusion based on alpha-stable modeling of wavelet coefficients</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="624" to="633" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Non-parametric and region-based image fusion with Bootstrap sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zribi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Comput. Vis. Pattern Recognition</title>
		<meeting>IEEE Conference on Comput. Vis. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion scheme using focused region detection and multiresolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="4376" to="4389" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion based on nonsubsampled contourlet transform and focused regions detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik (Stuttg)</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Region level based multifocus image fusion using quaternion wavelet and normalized cut</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion based on NSCT and focused area detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2824" to="2838" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Surface area-based focus criterion for multi-focus image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nejati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Roosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Najarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="284" to="295" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multifocus image fusion algorithm based on focused region extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="733" to="748" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Image fusion based on object region detection and Non-Subsampled Contourlet Transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electrical Engineering</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Image fusion with saliency map and interest points</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Discrete Cosine Transform-based Image Fusion</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P S</forename><surname>Naidu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Def. Sci. J</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Quality evaluation of multiresolution remote sensing images fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Zoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UPB Sci Bull Series C</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="38" to="52" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Quality assessment of image fusion based on image content and structural similarity</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE 2nd International Conference on Information Engineering and Computer Science (ICIECS)</title>
		<meeting>IEEE 2nd International Conference on Information Engineering and Computer Science (ICIECS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A sparse image fusion algorithm with application to pansharpening</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2827" to="2836" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letter</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A multi-focus image fusion method based on laplacian pyramid</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2559" to="2566" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A non-reference image fusion metric based on mutual information of image features</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B A</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghagolzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seyedarabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="756" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Combination of images with diverse focuses using the spatial frequency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A new image quality metric for image fusion: The sum of the correlations of differences</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aslantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEU-International Journal of Electronics and Communications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1890" to="1896" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A new quality metric for image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heijmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Objective evaluation of signal-level image fusion performance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xydeas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="87003" to="87008" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Fusion of multi-focus images using differential evolution algorithm, Expert System</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aslantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="8861" to="8870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A novel multi-focus image fusion approach based on image decomposition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="102" to="116" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with dense SIFT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="155" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Medical image fusion using multi-level local extrema</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="38" to="48" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Multi-modal medical image fusion using the inter-scale andintra-scale dependencies between image shift-invariant shearlet coefficients</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A novel approach for multimodal medical image fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="7425" to="7435" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Perceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with Gaussian and bilateral filters</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via gradient transfer and total variation minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visual images using bacterial foraging strategy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WSEAS Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Image fusion techniques for remote sensing applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Morabito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Remote sensing image fusion via wavelet transform and sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="158" to="173" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Study of Remote Sensing Image Fusion and Its Application in Image Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wenbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tingjun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="1141" to="1146" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>The International Archives of the Photogrammetry</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
