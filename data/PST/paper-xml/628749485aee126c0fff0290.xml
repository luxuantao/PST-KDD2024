<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts</title>
				<funder ref="#_n2XUqGE">
					<orgName type="full">U.S. NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>imyiyang@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
							<email>aabbasi@nd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to finetune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models' understanding abilities, as shown using the GLUE benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (PLMs), such as masked language models (MLMs), have achieved remarkable success in many natural language processing (NLP) tasks <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Liu et al., 2019;</ref><ref type="bibr" target="#b23">Lan et al., 2020;</ref><ref type="bibr" target="#b5">Brown et al.)</ref>. Unfortunately, pretrained language models, which are trained on large human-written corpora, also inherit human-like biases and undesired social stereotypes <ref type="bibr" target="#b6">(Caliskan et al., 2017;</ref><ref type="bibr" target="#b4">Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Blodgett et al., 2020)</ref>. For example, in the fill-inthe-blank task, BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> substitutes <ref type="bibr">[MASK]</ref> in the sentence "The man/woman had a job as [MASK]" with "manager/receptionist" respectively, reflecting occupational gender bias.</p><p>The human-like biases and stereotypes encoded in PLMs are worrisome as they can be propagated or even amplified in downstream NLP tasks such as sentiment classification <ref type="bibr" target="#b21">(Kiritchenko and Mohammad, 2018)</ref>, co-reference resolution <ref type="bibr" target="#b48">(Zhao et al., 2019;</ref><ref type="bibr" target="#b37">Rudinger et al., 2018)</ref>, clinical text classification <ref type="bibr" target="#b47">(Zhang et al., 2020)</ref> and psychometric analysis <ref type="bibr" target="#b0">(Abbasi et al., 2021;</ref><ref type="bibr" target="#b1">Ahmad et al., 2020)</ref>.</p><p>However, although it is important to mitigate biases in PLMs, debiasing masked language models such as BERT is still challenging, because the biases encoded in the contextualized models are hard to identify. To address this challenge, previous efforts seek to use additional corpora to retrieve the contextualized embeddings or locate the biases and then debias accordingly. For example, <ref type="bibr" target="#b25">Liang et al. (2020)</ref>; <ref type="bibr" target="#b20">Kaneko and Bollegala (2021)</ref>; <ref type="bibr" target="#b13">Garimella et al. (2021)</ref> use external corpora to locate sentences containing the demographic-specific words (e.g., man and women) or stereotype words (e.g., manager and receptionist) and then use different debiasing losses to mitigate the biases.</p><p>Using external corpora to debias PLMs heavily relies on the quality of the corpora. Empirical results show that different corpora have various effects on the debiasing results: some external corpora do mitigate the bias, while others introduce new biases to the PLMs <ref type="bibr" target="#b13">(Garimella et al., 2021;</ref><ref type="bibr" target="#b25">Liang et al., 2020)</ref>. This is because the corpora used for debiasing may not have enough coverage of the biases encoded in the PLMs. Nevertheless, our understanding of how to quantitatively assess the level of biases in a corpus remains limited <ref type="bibr" target="#b3">(Blodgett et al., 2020)</ref>.</p><p>Mitigating biases in PLMs without external corpora is an open research gap. Recent work in language model prompting shows that through cloze-style prompts, one can probe and analyze the knowledge <ref type="bibr" target="#b34">(Petroni et al., 2019)</ref>, biases <ref type="bibr" target="#b29">(May et al., 2019)</ref> or toxic content <ref type="bibr" target="#b32">(Ousidhoum et al., 2021)</ref> in PLMs. Motivated by this, instead of refer- In the first stage, our approach searches for the biased prompts such that the cloze-style completions (i.e., masked token prediction) have the highest disagreement in generating stereotype words. In the second stage, the language model is fine-tuned by minimizing the disagreement between the distributions of the cloze-style completions.</p><p>ring to any external corpus, we directly use clozestyle prompts to probe and identify the biases in PLMs. But what are the biases in a PLM? Our idea is motivated by the assumption that a fair NLP system should produce scores that are independent to the choice of identities mentioned in the text <ref type="bibr" target="#b35">(Prabhakaran et al., 2019)</ref>. In our context, we propose automatically searching for "discriminative" prompts such that the cloze-style completions have the highest disagreement in generating stereotype words (e.g., manager/receptionist) with respect to demographic words (e.g., man/woman). The automatic biased prompt search also minimizes human effort.</p><p>After we obtain the biased prompts, we probe the biased content with such prompts and then correct the model bias. We propose an equalizing loss to align the distributions between the [MASK] tokens predictions, conditioned on the corresponding demographic words. In other words, while the automatically crafted biased prompts maximize the disagreement between the predicted [MASK] token distributions, the equalizing loss minimizes such disagreement. Combining the automatic prompts generation and the distribution alignment fine-tuning, our novel method, Auto-Debias can debias the PLMs without using any external corpus. Auto-Debias is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>In the experiments, we evaluate the performance of Auto-Debias in mitigating gender and racial biases in three popular masked language models: BERT, ALBERT, and RoBERTa. Moreover, to alleviate the concern that model debias-ing may worsen a model's performance on natural language understanding (NLU) tasks <ref type="bibr" target="#b30">(Meade et al., 2021)</ref>, we also evaluate the debiased models on GLUE tasks. The results show that our proposed Auto-Debias approach can effectively mitigate the biases while maintaining the capability of language models. We have released the Auto-Debias implementation, debiased models, and evaluation scripts at https://github. com/Irenehere/Auto-Debias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>As NLP models are prevalent in real-world applications, a burgeoning body of literature has investigated human-like biases in NLP models. Bias in NLP systems can stem from training data <ref type="bibr" target="#b10">(Dixon et al., 2018)</ref>, pre-trained word embeddings or can be amplified by the machine learning models. Most existing work focuses on the bias in pre-trained word embeddings due to their universal nature <ref type="bibr" target="#b8">(Dawkins, 2021)</ref>. Prior work has found that traditional static word embeddings contain humanlike biases and stereotypes <ref type="bibr" target="#b6">(Caliskan et al., 2017;</ref><ref type="bibr" target="#b4">Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b12">Garg et al., 2018;</ref><ref type="bibr" target="#b28">Manzini et al., 2019;</ref><ref type="bibr" target="#b14">Gonen and Goldberg, 2019)</ref>. Debiasing strategies to mitigate static word embeddings have been proposed accordingly <ref type="bibr" target="#b4">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b49">Zhao et al., 2018;</ref><ref type="bibr" target="#b19">Kaneko and Bollegala, 2019;</ref><ref type="bibr" target="#b36">Ravfogel et al., 2020)</ref>.</p><p>Contextualized embeddings such as BERT have been replacing the traditional static word embeddings. Researchers have also reported similar human-like biases and stereotypes in contextual embedding PLMs <ref type="bibr" target="#b29">(May et al., 2019;</ref><ref type="bibr" target="#b22">Kurita et al., 2019;</ref><ref type="bibr" target="#b41">Tan and Celis, 2019;</ref><ref type="bibr" target="#b18">Hutchinson et al., 2020;</ref><ref type="bibr" target="#b16">Guo and Caliskan, 2021;</ref><ref type="bibr" target="#b45">Wolfe and Caliskan, 2021)</ref> or in the text generation tasks <ref type="bibr" target="#b38">(Schick et al., 2021;</ref><ref type="bibr" target="#b39">Sheng et al., 2019)</ref>. Compared to static word embeddings, mitigating the biases in contextualized PLMs is more challenging since the representation of a word usually depends on the word's context. <ref type="bibr" target="#b13">Garimella et al. (2021)</ref> propose to augment the pretraining corpus with demographicbalanced sentences. <ref type="bibr" target="#b25">Liang et al. (2020)</ref>; <ref type="bibr" target="#b7">Cheng et al. (2021)</ref> suggest removing the demographicdirection from sentence representations in a posthoc fashion. However, augmenting the pretraining corpus is costly and post-hoc debiasing does not mitigate the intrinsic biases encoded in PLMs. Therefore, recent work has proposed to fine-tune the PLMs to mitigate biases by designing different debiasing objectives <ref type="bibr" target="#b20">(Kaneko and Bollegala, 2021;</ref><ref type="bibr" target="#b13">Garimella et al., 2021;</ref><ref type="bibr" target="#b24">Lauscher et al., 2021)</ref>. They rely on external corpora, and the debiasing results based on these external corpora vary significantly <ref type="bibr" target="#b13">(Garimella et al., 2021)</ref>. Moreover, <ref type="bibr" target="#b13">Garimella et al. (2021)</ref> find that existing debiasing methods are generally ineffective: first, they do not generalize well beyond gender bias; second, they tend to worsen a model's language modeling ability and its performance on NLU tasks. In this work, we propose a debiasing method that does not necessitate referring to any external corpus. Our debiased models are evaluated on both gender and racial biases, and we also evaluate their performance on NLU tasks.</p><p>3 Auto-Debias: Probing and Debiasing using Prompts</p><p>We propose Auto-Debias, a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, we automatically craft the biased prompts, such that the cloze-style completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, after we obtain the biased prompts, we debias the language model by a distribution alignment loss, with the motivation that the prompt completion results should be independent to the choice of different demographic-specific words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>Let M be a Masked Language Model (MLM), and V be its vocabulary. The language model pre-trained with human-generated corpus contains social bias towards certain demographic groups. To mitigate the bias, we have two types of words: target concepts which are the paired tokens related to demographic groups (e.g., he/she, man/woman), and attribute words which are the stereotype tokens with respect to the target concepts (e.g., manager, receptionist). We denote the target concepts as a set of m-tuples of words</p><formula xml:id="formula_0">C = {(c (1) 1 , c (1) 2 , .., c<label>(1)</label></formula><p>m ), (c</p><formula xml:id="formula_1">(2) 1 , c<label>(2)</label></formula><p>2 , .., c</p><p>(2) m ), ...}. For example, in the two-gender debiasing task, the target concepts are {(he,she), (man,woman),...}. In the three-religion debiasing task, the target concepts are {(judaism,christianity,islam), (jew, christian,muslim), ...}. We omit the superscript of C if without ambiguity. We denote the set of attribute words as W.</p><p>An MLM can be probed by cloze-style prompts. Formally, a prompt x prompt ? V * is a sequence of words with one masked token [MASK] and one placeholder token. We use x prompt (c) to denote the prompt with which the placeholder is filled with a target concept c. For example, given x prompt = "[placeholder] has a job as [MASK]", we can fill in the placeholder with the target concept "she" and obtain</p><p>x prompt (she) = she has a job as <ref type="bibr">[MASK]</ref>.</p><p>Given a prompt and a target concept x prompt (c) as the input of M, we can obtain the predicted [MASK] token probability as</p><formula xml:id="formula_2">p([MASK] = v|M, x prompt (c)) = exp(M [MASK] (v|x prompt (c))) v ?V exp(M [MASK] (v |x prompt (c)))<label>(1)</label></formula><p>where v ? V. Prior literature has used this [MASK] token completion task to assess MLM bias <ref type="bibr" target="#b29">(May et al., 2019)</ref>. To mitigate the bias in an M, we hope that the output distribution predicting a [MASK] should be conditionally independent on the choice of any target concept in the m-tuple (c 1 , c 2 , ..., c m ). Therefore, for different c i ? (c 1 , c 2 , ..., c m ), our goal to debias M is to make the conditional distributions p([MASK] = v|M, x prompt (c i )) as similar as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finding Biased Prompts</head><p>The first stage of our approach is to generate prompts that can effectively probe the bias from M, so that we can remove such bias in the second stage. One straightforward way to design such </p><formula xml:id="formula_3">P gen ? top-K x?Pcan {JSD(p([MASK]|x prompt (c i ), M), i ? {1, 2, ..m})}; 5 // where x prompt (c i ) = c i ? x ? [MASK]</formula><p>and we only consider the probability of the attribute words W in the [MASK] position 6 P ? P ? P gen ;</p><formula xml:id="formula_4">7 P can ? {x ? v|?x ? P gen , ?v ? V } 8 end</formula><p>prompts is by manual generation. For example, "A [placeholder] has a job as [MASK]" is such biased prompts as it generates different mask token probabilities conditioned on the placeholder word being man or woman. However, handcrafting such biased prompts at scale is costly and the models are highly sensitive to the crafted prompts.</p><p>To address the problem, we propose biased prompt search, as described in Algorithm 1, a variant of the beam search algorithm, to search for the most discriminative, or in other words, the most biased prompts with respect to different demographic groups. Our motivation is to search for the prompts that have the highest disagreement in generating attribute words W in the [MASK] position. We use Jensen-Shannon divergence (JSD), which is a symmetric and smooth Kullback-Leibler divergence (KLD), to measure the agreement between distributions. In the case of the two-gender debiasing (male/female) task, JSD measures the agreement between the two distributions.</p><p>The JSD among distributions p 1 , p 2 , ..p m is defined as</p><formula xml:id="formula_5">JSD(p 1 , p 2 , ..., p m ) = 1 m i KLD(p i || p 1 + p 2 + ... + p m m ),<label>(2)</label></formula><p>where the Kullback-Leibler divergence(KLD) between two distributions p i , p j is computed as v) ). Algorithm 1 describes our algorithm for searching biased prompts. The algorithm finds the sequence of tokens x from the search space to craft prompts, which is firstly the candidate vocabulary space 1 , and then, after the first iteration, the con-1 We could use the entire V as the search space, but it catenation of searched sequences and candidate vocabulary. Specifically, during each iteration, for each candidate x in the search space, we construct the prompt as</p><formula xml:id="formula_6">KLD(p i ||p j ) = v?V p i (v)log( p i (v) p j (</formula><formula xml:id="formula_7">x prompt (c i ) = c i ? x ? [MASK],</formula><p>where ? is the string concatenation, for c i in an mtuple (c 1 , c 2 , ..., c m ). Given the prompt x prompt (c i ), M predicts the [MASK] token distribution over attribute words W (e.g. manager, receptionist,...):</p><formula xml:id="formula_8">p([MASK] = v|M, x prompt (c i )), v ? W.</formula><p>Next, we compute the JSD score between p([MASK] = v|M, x prompt (c i )) for each c i ? (c 1 , c 2 , ..., c m ), and select the prompts with high scores -indicating large disagreement between the [MASK] predictions for the given target concepts. The algorithm finds the top K prompts x prompt from the search space in each iteration step, and the procedure repeats until the prompt length reaching the pre-defined threshold. We merge all the generated prompts as the final biased prompts set P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-tuning MLM with Prompts</head><p>After we obtain the biased prompts, we fine-tune M to correct the biases. Specifically, given an mtuple of target words (c 1 , c 2 , ..., c m ) and a biased prompt x prompt , we expect M to be unbiased in the sense that p([MASK] = v|M, x prompt (c i )) = p([MASK] = v|M, x prompt (c j )) for any c i , c j ? (c 1 , c 2 , ..., c m ). This equalizing objective is motivated by the assumption that a fair NLP system should produce scores that are independent to the choice of the target concepts in our context, mencontains punctuations, word pieces and meaningless words. Therefore, instead of using the vocabulary V, we use the 5,000 highest frequency words in Wikipedia as the search space. https://github.com/IlyaSemenov/ wikipedia-word-frequency tioned in the text <ref type="bibr" target="#b35">(Prabhakaran et al., 2019)</ref>.</p><p>Therefore, given a prompt x prompt , our equalizing loss aims to minimize the disagreement between the predicted [MASK] token distributions. Specifically, it is defined as the Jensen-Shannon divergence (JSD) between the predicted [MASK] token distributions:</p><formula xml:id="formula_9">loss(x prompt ) = k JSD(p (k) c 1 , p (k) c 2 , .., p (k) cm ) (3)</formula><p>where p</p><p>(k)</p><formula xml:id="formula_10">c i = p([MASK] = v|M, x prompt (c (k) i ))</formula><p>, for v in a certain stereotyped word list. And the total loss is the average over all the prompts in the prompt set P. Discussion: Another perspective for Auto-Debias is that the debiasing method resembles adversarial training <ref type="bibr" target="#b15">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b33">Papernot et al., 2017)</ref>. In the first step, Auto-Debias searches for the biased prompts by maximizing disagreement between the masked language model (MLM) completions. In the second step, Auto-Debias leverages the biased prompts to fine-tune the MLM, by minimizing disagreement between the MLM completions. Taken together, Auto-Debias corrects the biases encoded in the MLM without relying on any external corpus. Overcoming the need to manually specify biased prompts would also make the entire debiasing pipeline more objective.</p><p>Recent research has adopted the adversarial training idea to remove biases from sensitive features, representations and classification models <ref type="bibr" target="#b46">(Zhang et al., 2018;</ref><ref type="bibr" target="#b11">Elazar and Goldberg, 2018;</ref><ref type="bibr" target="#b2">Beutel et al., 2017;</ref><ref type="bibr" target="#b17">Han et al., 2021)</ref>. Our work differs from this line of research in two ways. First, our work aims to mitigate biases in the PLMs. Second, the crafted biased prompts are not adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Debiasing Performance</head><p>We evaluate the performance of Auto-Debias in mitigating biases in masked language models. Debiasing strategy benchmarks. We consider the following debiasing benchmarks. Based on which stage the debiasing technique applies to, the benchmarks can be grouped into three categories.</p><p>? Pretraining: CDA is a data augmentation method that creates a gender-balanced dataset for language model pretraining <ref type="bibr" target="#b51">(Zmigrod et al., 2019)</ref>. Dropout is a debiasing method by increasing the dropout parameters in the PLMs <ref type="bibr" target="#b43">(Webster et al., 2020)</ref>;</p><p>? Post-hoc: Sent-Debias is a post-processing debias work that removing the estimated gender-direction from the sentence representations <ref type="bibr" target="#b25">(Liang et al., 2020)</ref>. FairFil uses a contrastive learning approach to correct the biases in the sentence representations <ref type="bibr" target="#b7">(Cheng et al., 2021)</ref>;</p><p>? Fine-tuning: Context-Debias proposes to debias PLM by a loss function that encourages the stereotype words and gender-specific words to be orthogonal <ref type="bibr" target="#b20">(Kaneko and Bollegala, 2021)</ref>. DebiasBERT proposes to use the equalizing loss to equalize the associations of gender-specific words <ref type="bibr" target="#b13">(Garimella et al., 2021)</ref>.</p><p>Both works essentially fine-tune the parameters in PLMs.</p><p>Our proposed Auto-Debias approach belongs to the fine-tuning category. It does not require any external corpus compared to the previous fine-tuning debiasing approaches. Pretrained Models. In the experiments, we consider three popular masked language models: BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b23">(Lan et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b26">(Liu et al., 2019)</ref>. We implement BERT, ALBERT, and RoBERTa using the Huggingface Transformers library <ref type="bibr" target="#b44">(Wolf et al., 2020)</ref>.  word's context-independent embeddings, which allows measuring the association between two demographic-specific words (e.g., man and woman) and stereotypes words (e.g., career and family). An ideally unbiased model should exhibit no difference between the demographic-specific words and their similarity to the stereotype words. We report the effect size in the SEAT evaluation. Effect size with an absolute value closer to 0 indicates lower biases. In the experiment, following prior work <ref type="bibr" target="#b25">(Liang et al., 2020;</ref><ref type="bibr" target="#b20">Kaneko and Bollegala, 2021)</ref>, we use SEAT 6, 6b, 7, 7b, 8, and 8b for measuring gender bias. Also, we use SEAT 3, 3b, 4, 5, and 5b for measuring racial bias. The SEAT test details, including the bias types and demographic/stereotype word associations, are presented in Appendix A. Experiment Setting. In our prompt searching algorithm 1, we set the maximum biased prompt length P L as five and beam search width K as 100. In total, we automatically generate 500 biased prompts for debiasing each model. In the gender debias experiments, we use BERT-base-uncased, RoBERTa-base, and ALBERT-large-v2. In the racial debiasing experiments, we use BERT-baseuncased and ALBERT-base-v2. We use different ALBERT models in the two experiments to allow a fair comparison with existing benchmarks. We do not debias RoBERTa-base in the race experiment because it has a pretty fair score in the SEAT metric. All Auto-Debias models are trained for 1 epoch with AdamW <ref type="bibr" target="#b27">(Loshchilov and Hutter, 2019)</ref> optimizer and 1e -5 learning rate. All models are trained on a single instance of NVIDIA RTX 3090 GPU card. For gender and race experiments, we run Auto-Debias separately on each base model five times and report the average score for the evaluation metrics<ref type="foot" target="#foot_2">4</ref> .</p><formula xml:id="formula_11">Bias</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mitigating gender bias</head><p>SEAT. We report gender debiasing results in Table <ref type="table" target="#tab_1">1</ref>, leading to several findings. First, our proposed Auto-Debias approach can meaningfully mitigate gender bias on the three tested masked language models BERT, ALBERT, and RoBERTa, in terms of the SEAT metric performance. For example, the average SEAT score of the original BERT, AL-BERT, and RoBERTa is 0.35, 0.28, and 0.67, respectively. Auto-Debias can substantially reduce the score to 0.14, 0.18, and 0.20. Second, Auto-Debias is more effective in mitigating gender biases compared to the existing state-of-the-art bench-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Length</head><p>Generated Prompts 1 substitute, premier, united, became, liberal, major, acting, professional, technical, against, political 2 united domestic, substitute foreign, acting field, eventual united, professional domestic, athletic and 3 professional domestic real, bulgarian domestic assisted, former united free, united former inside 4 eventual united reading and, former united choice for, professional domestic central victoria 5 united former feature right and, former united choice for new, eventual united reading and marks. BERT is the most studied model in prior work, so we include the state-of-the-art debiasing numbers reported in existing benchmark papers. We can see that Auto-Debias achieves the lowest average SEAT score in all three pretrained model experiments. For example, in SEAT-6 and SEAT-6b, where we examine the association between male/female names/terms and career/family terms, Auto-Debias achieves SEAT scores that are close to 0, indicating the debiased model can almost eliminate the gender bias in the career/family direction. Third, we observe that Auto-Debias, while achieving the lowest average SEAT score, is also relatively stable on SEAT score across different tasks. Conversely, benchmark debiasing approaches have high variance across tasks, which is consistent with recent empirical findings <ref type="bibr" target="#b30">(Meade et al., 2021)</ref>. This indicates that Auto-Debias is a more stable and generalizable in terms of its debiasing performance. CrowS-Pairs. In addition to the word association test, we also evaluate debiasing performance using the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs) <ref type="bibr" target="#b31">(Nangia et al., 2020)</ref>. This dataset contains a set of sentence pairs that are intended to be minimally distant, semantically speaking, except that one sentence in each pair is considered to be more indicative of stereotyping than the other. The CrowS-Pairs benchmark metric measures the percentage of sentence pairs in which the language model assigns a higher likelihood to the sentence deemed to be more stereotyping. An ideal model is expected to achieve a score of 50%.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the debiasing performance on CrowS-Pairs (gender subset) for BERT, ALBERT, and RoBERTa. The original model's stereotype scores are also presented in the table for direct reference. Note that a score closer to 50 is preferred, as it implies that the model assigns equal probability to male and female sentences. In the BERT and RoBERTa models, Auto-Debias reduces the language models' bias and assigns more equal likelihood to the sentences in both gender groups. Interestingly, in ALBERT, for the sentences in the dataset that demonstrate stereotypes (Stereo), Auto-Debias even over-corrects the stereotypes: it slightly prefers the historically disadvantaged groups. Overall, Auto-Debias can reduce the biases in all three models.</p><p>Biased prompts. We present some examples of the generated biased prompts in Table <ref type="table" target="#tab_3">3</ref>. Although the biased prompts from Auto-Debias are not grammatical, which is expected in the case of automatically generated prompts <ref type="bibr" target="#b40">(Shin et al., 2020;</ref><ref type="bibr" target="#b50">Zhong et al., 2021)</ref>, they do contain stereotype related tokens such as professional, political, and liberal. Also, the automated biased generation can minimize human effort and may scale well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mitigating racial bias</head><p>Mitigating non-gender biases is a challenging task in debiasing research. <ref type="bibr" target="#b30">Meade et al. (2021)</ref> empirically show that some of the debiasing techniques considered in our benchmarks generalize poorly in racial debiasing. One of the challenges could be the ambiguity of words (white, black) in different contexts. Therefore, the counterfactual dataaugmentation approach or the fine-tuning approach relying on external corpora may be less effective.</p><p>In this experiment, we evaluate Auto-Debias's performance in mitigating racial biases in the PLMs and evaluate the performance using <ref type="bibr">SEAT 3,</ref><ref type="bibr">3b,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">and 5b tests.</ref>  We can see from Table <ref type="table" target="#tab_4">4</ref> that Auto-Debias can meaningfully mitigate the racial biases in terms of the SEAT metric. Note that the racial SEAT test examines any association difference between European-American/African American names/terms and the stereotype words (pleasant vs. unpleasant). For example, on BERT, Auto-Debias considerably mitigates the racial bias in 4 out of 5 SEAT sub-tests, and the overall score is reduced from 0.23 to 0.18. On ALBERT, Auto-Debias also significantly mitigates the bias in all subsets.</p><p>5 Does Auto-Debias affect downstream NLP tasks? <ref type="bibr" target="#b30">Meade et al. (2021)</ref> find that the previous debiasing techniques often come at a price of worsened performance in downstream NLP tasks, which implies that prior work might over-debias. Our work instead directly probes the bias encoded in PLM, alleviating the concern of over-debias.</p><p>In this section, we evaluate the gender debiased BERT/ALBERT/RoBERTa on the General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b42">(Wang et al., 2019)</ref>, to examine the capabilities of the language models. The results are reported in Table <ref type="table" target="#tab_5">5</ref>. The racial-debiased PLM models achieve similar GLUE scores. Auto-Debias performs on par with the base models on most natural language understanding tasks. There is only one exception: CoLA dataset. CoLA evaluates linguistic acceptability, judging whether a sentence is grammatically correct. Our method adjusts the distribution of words using prompts, which may affect the grammatical knowledge contained in PLMs. But overall speaking, Auto-Debias does not adversely affect the downstream performance. Taking the results together, we see that Auto-Debias can alleviate the bias concerns while also maintaining language modeling capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Prompts have been an effective tool in probing the internal knowledge relations of language models <ref type="bibr" target="#b34">(Petroni et al., 2019)</ref>, and they can also reflect the stereotypes encompassed in PLMs <ref type="bibr" target="#b32">(Ousidhoum et al., 2021;</ref><ref type="bibr" target="#b39">Sheng et al., 2019)</ref>. Ideally, when prompted with different demographic targets and potential stereotype words, a fair language model's generated predictions should be equally likely. Our method shows that, from the other direction, imposing fairness constraints on the prompting results can effectively promote the fairness of a language model.</p><p>We also observe a trade-off between efficiency and equity: tuning with more training steps, more prompts and more target words leads to a fairer model (which can even make the SEAT score very close to 0), however, it comes at the price of harming the language modeling ability. Over-tuning may harm the internal language patterns. It is important to strike a balance between efficiency and equity with appropriate fine-tuning.</p><p>Also, in order not to break the desirable connections between targets and attributes, carefully selecting the target words and stereotyped attribute words is crucial. However, acquiring such word lists is difficult and depends on the downstream applications. Some prior work establishes word lists based on theories, concepts, and methods from psychology and other social science literature <ref type="bibr" target="#b20">(Kaneko and Bollegala, 2021;</ref><ref type="bibr" target="#b28">Manzini et al., 2019)</ref>. However, such stereotyped word lists are usually lim-ited, are often contextualized, and offer limited coverage. Moreover, word lists about other protected groups, such as the groups related to education, literacy, or income, or even intersectional biases <ref type="bibr" target="#b0">(Abbasi et al., 2021)</ref>, are still missing. One promising method to acquire such word lists is to probe related words from a pre-trained language model, for example, "the man/woman has a job as [MASK]" yields job titles that reflect the stereotypes. We leave such probing-based stereotype word-list generation as an important and open future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we propose Auto-Debias, a framework and method for automatically mitigating the biases and stereotypes encoded in PLMs. Compared to previous efforts that rely on external corpora to obtain context-dependent word embeddings, our approach automatically searches for biased prompts in the PLMs. Therefore, our approach is effective, efficient, and is perhaps also more objective than prior methods that rely heavily on manually crafted lists of stereotype words. Experimental results on standard benchmarks show that Auto-Debias reduces gender and race biases more effectively than prior efforts. Moreover, the debiased models also maintain good language modeling capability. Bias in NLP systems can stem from different aspects such as training data, pretrained embeddings, or through amplification when finetuning the machine learning models. We believe this work contributes to the emerging literature that sheds light on practical and effective debiasing techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The Auto-Debias framework. In the first stage, our approach searches for the biased prompts such that the cloze-style completions (i.e., masked token prediction) have the highest disagreement in generating stereotype words. In the second stage, the language model is fine-tuned by minimizing the disagreement between the distributions of the cloze-style completions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Biased Prompt Search input :Language model M, candidate vocabulary V , target words C, stereotype words W, prompt length P L, beam width K. output :Generated Biased Prompts P 1 P ? {}; 2 Candidate prompts P can ? V ; 3 for l ? 1 to P L do 4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Gender debiasing results of SEAT on BERT, ALBERT and RoBERTa. Absolute values closer to 0 are better. Auto-Debias achieves better debiasing performance. The results of Sent-Debias, Context-Debias, FairFil are from the original papers. CDA, Dropout are reproduced from the released model<ref type="bibr" target="#b43">(Webster et al., 2020)</ref>. "-" means the value is not reported in the original paper.</figDesc><table><row><cell>Word List. Debiasing approaches leverage</cell></row><row><cell>existing hand-curated target concepts and stereo-</cell></row><row><cell>type word lists to identify and mitigate biases in</cell></row><row><cell>the PLMs. Those word lists are often developed</cell></row><row><cell>based on concepts or methods from psychology or</cell></row><row><cell>other social science literature, to reflect cultural</cell></row><row><cell>and cognitive biases. In our experiments, we aim</cell></row><row><cell>to mitigate gender or racial biases. Following prior</cell></row><row><cell>debiasing approaches, we obtain the gender con-</cell></row><row><cell>cept/stereotype word lists used in (Kaneko and Bol-</cell></row><row><cell>legala, 2021) 2 and racial concept/stereotype word</cell></row><row><cell>lists used in (Manzini et al., 2019) 3 .</cell></row><row><cell>Evaluating Biases: SEAT. Sentence Embedding</cell></row><row><cell>Association Test (SEAT) (May et al., 2019) is</cell></row><row><cell>a common metric used to assess the biases in</cell></row><row><cell>the PLM embeddings. It extends the standard</cell></row><row><cell>static word embedding association test (WEAT)</cell></row><row><cell>(Caliskan et al., 2017) to contextualized word em-</cell></row><row><cell>beddings. SEAT leverages simple templates such</cell></row><row><cell>as "This is a[n] &lt;word&gt;" to obtain individual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Gender debiasing performance on CrowS-Pairs. An ideally debiased model should achieve a score of 50%. Auto-Debias mitigates the overall bias on all three models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Examples of prompts generated by Biased Prompt Search (BERT model, for gender).</figDesc><table><row><cell></cell><cell cols="3">SEAT-3 SEAT-3b SEAT-4</cell></row><row><cell>BERT</cell><cell>-0.10</cell><cell>0.37</cell><cell>0.21</cell></row><row><cell>+Auto-Debias</cell><cell>0.25</cell><cell>0.19</cell><cell>0.12</cell></row><row><cell>ALBERT</cell><cell>0.60</cell><cell>0.29</cell><cell>0.53</cell></row><row><cell>+Auto-Debias</cell><cell>0.10</cell><cell>0.12</cell><cell>0.19</cell></row><row><cell></cell><cell cols="2">SEAT-5 SEAT-5b</cell><cell>avg.</cell></row><row><cell>BERT</cell><cell>0.16</cell><cell>0.34</cell><cell>0.23</cell></row><row><cell>+Auto-Debias</cell><cell>0.15</cell><cell>0.17</cell><cell>0.18</cell></row><row><cell>ALBERT</cell><cell>0.40</cell><cell>0.46</cell><cell>0.46</cell></row><row><cell>+Auto-Debias</cell><cell>0.26</cell><cell>0.19</cell><cell>0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mitigating racial biases in BERT and AL-BERT. RoBERTa is excluded because it barely exhibits racial bias in terms of the SEAT metric.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Table4reports the SEAT score on GLUE test results on the original and the gender-debiased PLMs. Auto-Debias can mitigate the bias while also maintaining the language modeling capability.the original and debiased BERT and ALBERT. The RoBERTa model is excluded because it barely exhibits racial biases in the SEAT test with an average score of 0.05. We do not include other debiasing benchmarks in Table4because most benchmark papers do not focus on racial debiasing. Thus, we focus on comparing the Auto-Debias performance against the original models.</figDesc><table><row><cell></cell><cell cols="5">CoLA SST-2 MRPC STS-B QQP</cell><cell>MNLI</cell><cell cols="2">QNLI RTE WNLI</cell></row><row><cell>BERT</cell><cell>0.53</cell><cell>0.92</cell><cell>0.88</cell><cell>0.87</cell><cell cols="3">0.90 0.84/0.85 0.92</cell><cell>0.58</cell><cell>0.55</cell></row><row><cell>+Auto-Debias</cell><cell>0.52</cell><cell>0.92</cell><cell>0.89</cell><cell>0.88</cell><cell cols="3">0.91 0.84/0.85 0.91</cell><cell>0.60</cell><cell>0.56</cell></row><row><cell>ALBERT</cell><cell>0.59</cell><cell>0.92</cell><cell>0.91</cell><cell>0.91</cell><cell cols="3">0.91 0.88/0.87 0.92</cell><cell>0.74</cell><cell>0.55</cell></row><row><cell>+Auto-Debias</cell><cell>0.58</cell><cell>0.94</cell><cell>0.91</cell><cell>0.90</cell><cell cols="3">0.91 0.87/0.87 0.92</cell><cell>0.75</cell><cell>0.47</cell></row><row><cell>RoBERTa</cell><cell>0.52</cell><cell>0.94</cell><cell>0.89</cell><cell>0.88</cell><cell cols="3">0.91 0.88/0.87 0.93</cell><cell>0.61</cell><cell>0.56</cell></row><row><cell>+Auto-Debias</cell><cell>0.46</cell><cell>0.94</cell><cell>0.89</cell><cell>0.87</cell><cell cols="3">0.91 0.88/0.87 0.93</cell><cell>0.61</cell><cell>0.56</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/kanekomasahiro/ context-debias/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/TManzini/ DebiasMulticlassWordEmbedding/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The SEAT score is based on the average of absolute value.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was funded in part through <rs type="funder">U.S. NSF</rs> grant <rs type="grantNumber">IIS-2039915</rs> and an <rs type="grantName">Oracle for Research grant entitled "NLP for the Greater Good</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_n2XUqGE">
					<idno type="grant-number">IIS-2039915</idno>
					<orgName type="grant-name">Oracle for Research grant entitled &quot;NLP for the Greater Good</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: SEAT Test Details</head><p>We present more information on the SEAT tests that are used in the experiments, in Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix: Target Word Lists</head><p>We provide details about the gender and racial word lists used in the debiasing experiments.</p><p>For gender, we use the target concept words and stereotype words listed in <ref type="bibr" target="#b20">(Kaneko and Bollegala, 2021)</ref>.</p><p>For race, we use the target concept words and stereotype words listed in <ref type="bibr" target="#b28">(Manzini et al., 2019)</ref>, with a slight modification on the target concept words. We present the racial concept word lists below:</p><p>African American: black, african, black, africa, africa, africa, black people, african people, black people, the africa European American: caucasian, caucasian, white, america, america, europe, caucasian people, caucasian people, white people, the america  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias type</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing a psychometric testbed for fair natural language processing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dobolyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Netemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendall</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3748" to="3758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A deep learning architecture for psychometric natural language processing</title>
		<author>
			<persName><forename type="first">Faizan</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Dobolyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Netemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsinchun</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00075</idno>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fairfil: Contrastive neural debiasing method for pretrained text encoders</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijing</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marked attribute bias in natural language inference</title>
		<author>
			<persName><forename type="first">Hillary</forename><surname>Dawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4214" to="4226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring and mitigating unintended bias in text classification</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial removal of demographic attributes from text data</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word embeddings quantify 100 years of gender and ethnic stereotypes</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Londa</forename><surname>Schiebinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3635" to="E3644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">He is very intelligent, she is very beautiful? on mitigating social biases in language modelling and generation</title>
		<author>
			<persName><forename type="first">Aparna</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhash</forename><surname>Amarnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Pramod Yalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anandhavelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Vasan Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4534" to="4545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="609" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="122" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupling adversarial training for fair nlp</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="471" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social biases in nlp models as barriers for persons with disabilities</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Denuyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5491" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gender-preserving debiasing for pre-trained word embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1641" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Debiasing pre-trained contextualised embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>of the 16th European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="43" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07337</idno>
		<title level="m">Measuring bias in contextualized word representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</editor>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2020. April 26-30, 2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ALBERT: A lite BERT for self-supervised learning of language representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>L?ken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03646</idno>
		<title level="m">Sustainable modular debiasing of language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards debiasing sentence representations</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">Mengze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the</title>
		<meeting>the 58th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On measuring social biases in sentence encoders</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Rudinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An empirical survey of the effectiveness of debiasing techniques for pre-trained language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elinor</forename><surname>Poole-Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08527</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crows-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Probing toxic content in large pre-trained language models</title>
		<author>
			<persName><forename type="first">Djouhra</forename><surname>Nedjma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Ousidhoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit</forename><forename type="middle">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
		<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perturbation sensitivity analysis to detect unintended model biases</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5740" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Null it out: Guarding protected attributes by iterative nullspace projection</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7237" to="7256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahana</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00453</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Elisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celis</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01485</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06032</idno>
		<title level="m">Measuring and reducing gendered correlations in pre-trained models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Low frequency names exhibit bias and overfitting in contextualizing language models</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hu Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hurtful words: quantifying biases in clinical contextual word embeddings</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning gender-neutral word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Factual probing is [MASK]: learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
