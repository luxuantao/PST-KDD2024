<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
							<email>zhouhang@link</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<email>yuliu@ee</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>zwliu@ie</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding talking faces visually is of great importance to machine perception and communication. Humans can not only guess the semantic meaning of words by observing lip movement but also imagine the scenario when a specific subject talks (i.e. face generation). Recent advances have focused on automatic lip reading, which surpasses humanlevel performance in certain domains. Here, we explore generating a video of arbitrary-subject speaking, which perfectly syncs with a specific speech where the speech information can be represented by either a clip of audio or video. We refer this problem as arbitrary-subject talking face generation, as shown in Fig. <ref type="figure">1</ref>.</p><p>However, generating identity-preserving talking faces that clearly conveys certain speech information is a challenging task, since the continuous deformation of the face region relates to both intrinsic subject traits <ref type="bibr" target="#b14">(Liu et al. 2015)</ref> and extrinsic speech vibrations. Previous efforts in this direction are mainly from computer graphics <ref type="bibr" target="#b25">(Xie and Liu 2007;</ref><ref type="bibr"></ref> Figure <ref type="figure">1</ref>: Problem description. Given a single face image of a target person, this work aims to generate the talking video based on the given speech information that is represented by either a clip of video or an audio. <ref type="bibr" target="#b24">Wang et al. 2010;</ref><ref type="bibr" target="#b11">Fan et al. 2015;</ref><ref type="bibr" target="#b21">Suwajanakorn, Seitz, and Kemelmacher-Shlizerman 2017;</ref><ref type="bibr" target="#b22">Thies et al. 2016)</ref>. Researchers construct specific 3D face model for a chosen subject and the talking faces are animated by manipulating 3D meshes of the face model. However, these approaches strongly rely on the 3D face model and are hard to scale up to arbitrary identities. More recent attempts <ref type="bibr" target="#b9">(Chung, Jamaludin, and Zisserman 2017)</ref> leverage the power of deep generative model and learn to generate talking faces from scratch. Though the resulting models can be applied to an arbitrary subject, the generated face sequences are sometimes blurry and not temporally meaningful. One important reason is that the subject-related and speech-related information are coupled together such that the talking faces are difficult to learn in a purely data-driven manner.</p><p>To address the aforementioned problems, we integrate the identity-related and speech-related information by learning disentangled audio-visual representation, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. We aim to disentangle a talking face sequence into two complementary representations, one containing identity information while the other containing speech information. However, directly separating these two parts is not a trivial task because the variations of face deformation can be extremely large considering the diversity of potential subjects and speeches.</p><p>The key idea here is using audio-visual speech recognition <ref type="bibr" target="#b6">(Chung and Zisserman 2016b;</ref><ref type="bibr">2017)</ref>   ing) as a probe task for associating audio-visual representations, and then employing adversarial learning to disentangle the subject-related and speech-related information inside them. Specifically, we first learn a joint audio-visual space where talking face sequence and its corresponding audio are embedded together. It is achieved by enforcing the lip reading result obtained from talking faces aligns with the speech recognition result obtained from audio. Next, we further utilize lip reading task to disentangle subject-related and speech-related information through adversarial learning <ref type="bibr" target="#b18">(Liu et al. 2018b</ref>). Notably, we enforce one of the representations extracted from talking faces to fool the lip reading system, in the sense that it only contains subject-related information, but not speech-related information. Overall, with the aid of associative-and-adversarial training, we can jointly embed audio-visual inputs and disentangle subject and speechrelated information of talking faces. The contributions of this work can be summarized as follows. (1) A joint audio-visual representation is learned through audio-visual speech discrimination by associating several supervisions. Experiments show that the jointembedding improves the baseline of lip reading result on LRW dataset <ref type="bibr" target="#b5">(Chung and Zisserman 2016a)</ref>. (2) Thanks to the discriminative nature of our joint representation, we disentangle the person-identity and speech information through adversarial learning for better talking face generation. (3) By unifying audio-visual speech recognition and audio-visual synchronizing, we achieve arbitrary-identity talking face generation from either video or audio speech as inputs in an end-to-end framework, which synthesizes high-quality and temporally-accurate talking faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generating Talking Faces. The work of synthesizing lip motion from either audio <ref type="bibr" target="#b25">(Xie and Liu 2007;</ref><ref type="bibr" target="#b24">Wang et al. 2010;</ref><ref type="bibr" target="#b11">Fan et al. 2015;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b21">Suwajanakorn, Seitz, and Kemelmacher-Shlizerman 2017;</ref><ref type="bibr" target="#b9">Chung, Jamaludin, and Zisserman 2017)</ref> or generating moving faces from videos <ref type="bibr" target="#b22">(Thies et al. 2016;</ref><ref type="bibr" target="#b16">Liu et al. 2017b</ref>; Wiles, Koepke, and Zisserman 2018) has long been a task of concern in both the community of computer vision and graphics. However, most synthesis works from audio require a large amount of video footage of the target person for training, modeling, or sampling. They could not transfer the speech information to an arbitrary photo in the wild.</p><p>Chung, Jamaludin, and Zisserman (2017) use a setting that is different from the traditional ones. They try to directly generate the whole face image with different lip motions in an image-to-image translation manner based on audios.</p><p>But their method base on data-driven training using an autoencoder, which leads to blurry results and lacks continuity. More recently, <ref type="bibr" target="#b21">Song et al. (2018)</ref> propose to use conditional RNN adversarial network, and <ref type="bibr" target="#b3">Chen et al. (2018)</ref> propose to use correlation loss and three-stream GAN.</p><p>(Wiles, Koepke, and Zisserman 2018) use flow to generate high precision arbitrary-identity talking face based on videos and claim to be able to produce videos based on audios, but with no results shown. However, as a common problem, without specific disentangling face and lip motion information, they all cannot generate high-quality results. Learning Audio-Visual Representation. The task of audiovisual speech recognition is a recognition problem uses either one or both video and audio as inputs. Using visual information only for recognition is also referred to as Lip Reading. A review of traditional methods for tackling this task has been made in <ref type="bibr" target="#b26">Zhou et al. (2014)</ref> thoroughly. In recent years, this field develop quickly with the usage of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for end-to-end word-level (Chung and Zisserman 2016a; Stafylakis and Tzimiropoulos 2017), sentencelevel <ref type="bibr" target="#b0">(Assael et al. 2016;</ref><ref type="bibr">Chung et al. 2017)</ref>, and multiview (Chung and Zisserman 2017) lip reading. In the meantime, the exploration of this topic has been greatly pushed forward by the build-up of large-scale word-level lip reading dataset <ref type="bibr" target="#b5">(Chung and Zisserman 2016a)</ref>, and the large sentence-level multi-view dataset <ref type="bibr">(Chung and Zisserman 2017)</ref>.</p><p>For the correspondence between human faces and audio clips, a number of works have been proposed to solve the problem of the audio-video synchronization between mouth motion and speech <ref type="bibr" target="#b19">(McAllister et al. 1997</ref>; Chung and Zisserman 2016b). Particularly, SyncNet (Chung and Zisserman 2016b; 2017) used two stream CNNs to sync audio mfcc with 5 consecutive frames. In <ref type="bibr">Chung and Zisserman (2017)</ref>, they further fixed the sync image feature as the pretraining for lip reading, but the two tasks are still separate from each other. Recently, works from (Nagrani, Albanie, and Zisserman 2018b; 2018a) also attempt to learn the association between a human face and voice for identity recognition instead of semantic level synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We propose Disentangled Audio-Visual System (DAVS), an end-to-end trainable network for talking face genera- tion by learning disentangled audio-visual representations, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>We leverage both talking video S v and its corresponding audio S a as training inputs. For learning the disentangled audio-visual representations between Person-ID space (pid) and the Word-ID space (wid), there are three encoder networks involved:</p><p>• Video to Word-ID space encoder (E v w ): E v w learns to embed the video frame s v into a visual representation f v w which only contains speech-related information. It is achieved by learning a joint embedding space which associates video and audio that correspond to the same word.</p><p>• Audio to Word-ID space encoder (E a w ): E a w learns to embed the speech s a into an audio representation f a w , which resides in the shared space with f v w as introduced above. • Video to Person-ID space encoder (E v p ): E v p learns to embed the video frame s v into a representation f v p which only contains subject-related information. It is achieved by the adversarial training process, forcing our target representation f v p to fool the speech recognition system. The whole idea of our pipeline is to first learn the discriminative audio-visual joint space wid, then disentangle it from the pid space. Finally to combine features from the two spaces to get generation results. Specifically, for learning the wid space, we employ three supervisions: the supervision of Word-ID labels with shared classifier C w for associating audio and visual signals with semantic meanings; contrastive loss L C for pulling paired video and audio samples closer; and an adversarial training supervision on audio and video features to make them indistinguishable. As for the pid space, Person-ID labels from extra labeled face data are used. For disentangling wid and pid spaces, adversarial training is employed. As for generation, we introduce L 1 -norm reconstruction loss L L1 and temporal GAN loss L GAN for sharpness and continuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Joint Audio-Visual Representation</head><p>We learn a joint audio-visual space that associates representations from both sources. We constrain the extracted audio representation to be close to its corresponding visual representation, forcing the embedded features to share a same distribution and restricting</p><formula xml:id="formula_0">f a w f v w , so that G(f v p , f v w ) G(f v p , f a w</formula><p>) can be achieved. While requiring information of person facial identity flows from the pid space, the other space of wid would have to be person-ID invariant. The task of audio-visual speech recognition benefits us in achieving the shared latent space assumption and creating a discriminative space through mapping videos and audios to word labels. The implementation of learning the space is shown in Fig <ref type="figure" target="#fig_3">4 (a)</ref>. Then with the discriminative embedding, we can take the advantage of adversarial training for thoroughly information disentangling as described in Sec. 3.2 Sharing Classifier. After the embedded features are extracted from the wid encoders E a w , E v w to get</p><formula xml:id="formula_1">F v w = [f v w(1) , • • • , f v w(n) ] and F a w = [f a w(1) , • • • , f a w(n) ]</formula><p>, normally they would be fed into different classifiers for visual and audio speech recognition. Here we share the classifier for both the modalities to enforce them to share their distributions. As a classifier's weight w j tend to fall into the center of the clustering of the features belonging to the j'th class, through sharing the weights, the features between both modalities are pulled towards the centroid of the class <ref type="bibr" target="#b17">(Liu et al. 2018a</ref>). The supervision is denoted as L w . Contrastive Loss. As the problem of mapping audio and visual together is very similar to feature mapping <ref type="bibr" target="#b4">(Chopra, Hadsell, and LeCun 2005)</ref>, retrieval and particularly the same as lip sync (Chung and Zisserman 2016b), we adopted the contrastive loss which aims at bringing closer paired data while dispelling unpaired as a baseline. During training, for a batch of N audio-video samples, the mth and nth sample are drawn with labels l m=n = 1 while the others l m =n = 0. The distance metric used to measure the dis- tance between F a w(m) and F v w(n) here is the euclidean norm</p><formula xml:id="formula_2">E v w E v w E v w E v w E v w v w F E a w E a w E a w E a w E a</formula><formula xml:id="formula_3">d mn = F v w(m) − F a w(n) 2 .</formula><p>The objective can be written as:</p><formula xml:id="formula_4">L C = N,N n=1,m=1 (l mn d mn + (1 − l mn ) max(1 − d mn , 0))<label>(1)</label></formula><p>During our implementation, all features F v w , F a w used in this loss are normalized first. Domain Adversarial Training. To further push the face and audio features to be in the same distribution, we apply a domain adversarial training. An extra two-class domain classifier is appended for distinguishing the source of the feature. The audio and face encoders are then trained to prevent the classifier from success. This is mostly a simple version of the adversarial training described in section 3.2. We refer to the objective of this method as L D adv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Training for Latent Space Disentangling</head><p>In this section, we describe how we disentangle the subjectrelated and speech-related information in the joint embedding space using adversarial training. Specifically, we would like the Person-ID feature f v p to be free of Word-ID information. The discriminator could be formed to be a classifier C w p to map the collection of</p><formula xml:id="formula_5">F v p = [f v p (1) , • • • , f v p (n) ]</formula><p>to the N w Word-ID classes. The objective function for training the classifier is the same as softmax cross-entropy loss. However, the parameter updating is only performed on C w p , where p w j is the one-hot label of the identity classes:</p><formula xml:id="formula_6">L w dis p (C w p |E v p ) = − Nw j=1 p w j log(softmax(C w p (F v p )) j ). (2)</formula><p>Then we update the encoder while fixing the classifier. The way to ensure that the features have lost all information about speech information is that it produces the same prediction for all classes after being sent into C w p . One way to form this limitation is to assign the probabilities of each wordlabel to be 1 Nw in softmax cross-entropy loss. The problem of this loss is that it would still backward gradient for updating parameters even if it reaches the minimum, so we propose to implement the loss using Euclidean distance:</p><formula xml:id="formula_7">L w p (E v p |C w p ) = Nw j=1 softmax(C w p (F v p )) j − 1 N w 2 2 . (3)</formula><p>The dual feature f v w should also be free of pid information accordingly, so the loss for encoding pid information from each f v w using classifier C p w and loss for wid encoder E v w to dispel pid information can be formed as follows:</p><formula xml:id="formula_8">L p dis w (C p w |E v w ) = − Np j=1 p p j log(softmax(C p w (f v w )) j ),<label>(4)</label></formula><formula xml:id="formula_9">L p w (E v w |C p w ) = Np j=1 softmax(C p w (f v w )) j − 1 N p 2 2 . (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>N p is the number of person identities in the training set for embedding pid space. We summarize the adversarial training procedure for classifier C w p and encoder E v p as Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference: Arbitrary-Subject Talking Face Generation</head><p>In this section, we describe how we generate arbitrarysubject talking faces using the disentangled representations learned above. Combining pid feature f v p with either of the video wid feature f v w or audio wid feature f a w , our system can generate a frame using the decoder G. The newly generated frame can be expressed as G(f v p , f v w ), G(f v p , f a w ). Here we take synthesizing talking faces from audio wid information as example. The generation results can be expressed as G</p><formula xml:id="formula_11">(f v p (k) , F a w ) = {G(f v p (k) , f a w(1) ), • • • , G(f v p (k) , f a w(n) )}, where f v p (k)</formula><p>is the pid feature of the random kth frame, which acts Full input audio:</p><p>Input video:</p><p>Guidance input:</p><p>Generation from audio:</p><p>Generation from video:</p><p>Guidance input: as identity guidance. Our overall loss function consists of a L 1 reconstruction loss and a temporal GAN loss, where a discriminator D seq takes the generated sequence G(f v p (k) , F a w ) as input. These two terms can be formulated as follows:</p><formula xml:id="formula_12">L L1 = S v − G(f v p (k) , F a w ) 1 ,<label>(6)</label></formula><formula xml:id="formula_13">L GAN = E S v [log D seq (S v )] + E F v p ,F a w [log(1 − D seq (G(f v p (k) , F a w )]<label>(7)</label></formula><p>The overall reconstruction loss can be written as L Re , α is a hyper-parameter that leverages the two losses.</p><formula xml:id="formula_14">L Re = L GAN + αL L1 . (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>The same procedure can be applied to generation from video information by substituting F a w with F v w . As the reconstruction from audio and video can perform at the same time during training, we use L Re to denote the overall reconstruction loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. Our model is trained and evaluated on the LRW dataset <ref type="bibr" target="#b5">(Chung and Zisserman 2016a)</ref>, which is currently the largest word-level lip reading dataset with 1-of-500 diverse word labels. For each class, there are more than 800 training samples and 50 validation/test samples. Each sample is a one-second video with the target word spoken. Besides, the identity-preserving module of the network is trained on a subset of the MS-Celeb-1M dataset <ref type="bibr" target="#b12">(Guo et al. 2016)</ref>. All the talking faces in the videos are detected and aligned using RSA algorithm <ref type="bibr" target="#b15">(Liu et al. 2017a)</ref>, and then resized to 256 × 256. For the audio stream, we follow the implementation in <ref type="bibr" target="#b6">(Chung and Zisserman 2016b)</ref> to extract the mf cc features at the sampling rate of 100Hz. Then we match each image with a mf cc audio input with the size of 12 * 20.</p><p>Network Architecture. We adopted a modified VGG-M <ref type="bibr" target="#b2">(Chatfield et al. 2014)</ref> as the backbone for encoder E v p , and for encoder E v w , we modified a simple version of FAN <ref type="bibr" target="#b1">(Bulat and Tzimiropoulos 2017)</ref>. The encoder E a w has a similar structure as that used in <ref type="bibr" target="#b6">Chung and Zisserman (2016b)</ref>. Meanwhile, our decoder contains 10 convolution layers with 6 bilinear upsampling layers to obtain a fullresolution output image. All the latent representations are set to be 256-dimensional.</p><p>Implementation Details. We implemented DAVS using Pytorch. The batch size is set to be 18 with 1e-4 learning rate and trained on 6 Titan X GPUs. It takes about 4 epochs for the audio-visual speech recognition and person-identity recognition to converge and another 5 epochs for further tuning the generator. The whole training process takes about a week. Due to the alignment of the training set, the directly generated results may suffer from a scale changing problem, so we apply the subspace video stabilization <ref type="bibr" target="#b14">(Liu et al. 2011)</ref> for smoothness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results of Arbitrary-Subject Talking Face Generation</head><p>At test time, the input identity guidance s v p to E v p is any person's face image and only one of the source for speech information S v w , S a w is needed to generate a sequence of images. Quantitative Results. To verify the effectiveness of our GAN loss for improving image quality, we evaluate the PSNR and SSIM <ref type="bibr" target="#b23">(Wang et al. 2004</ref>) score on the test set of LRW based on reconstruction. We compare the results with and without the GAN loss in Table <ref type="table" target="#tab_0">1</ref>. We can see that both the scores are improved by changing L L1 to L Re . Qualitative Results. Video results can be found on our project page<ref type="foot" target="#foot_0">1</ref> . Here we show image results in Fig 6 <ref type="figure">.</ref> The input guidance photos are celebrities chosen randomly from the Internet. Our model is capable of generating talking faces based on both audios or videos. The focus of our work is to improve audio guided generation results by using joint audio-visual embedding, so we compare our work with Chung, Jamaludin, and Zisserman (2017) at Fig 7 <ref type="figure">.</ref> It can be clearly seen that our results outperform theirs from both the perspective of identity preserving and image quality. User Study. We also conduct user study to investigate the visual quality of our generated results comparing with a fair reproduction of Chung, Jamaludin, and Zisserman (2017) with our network structure. They are evaluated w.r.t two different criteria: whether participants could regard the generated talking faces as realistic (true or false), and how much percent of the time steps the generated talking faces temporally sync with the corresponding audio. We generate videos with the identity guidance to be 10 different celebrity photos. As for speech content information, we use clips from the test set of LRW dataset and selections from the Voxceleb dataset <ref type="bibr">(Nagrani, Chung, and Zisserman 2017)</ref>, which is not used for training. There are overall 10 participants involved, and the results are average over persons and video time steps. The ground-truth is not included in the user study.  Different subjects may behave different lip motion given the same audio clip and it is not desirable for the ground-truth to interfere the participants' perception. When conducting the user study for lip sync evaluation, we asked the participants to only focus on whether the lip motion and given audio are temporally synchronized. Their ratings indicate that our generation results outperform the baseline by synchronizing rate and the extent of realistic, according to Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Audio-Visual Representation</head><p>In order to inspect the quality of our embedded audio-visual representation, we evaluate the discriminative power and the closeness of our co-embedded features.</p><p>Word-level Audio-Visual Speech Recognition. We report audio-visual speech recognition accuracy on the test set of LRW dataset. Containing the task of visual recognition (lip reading) and audio recognition (speech recognition).</p><p>Our model structure for lip reading is similar to the Multiple-Towers method which reaches the highest lip reading results in <ref type="bibr" target="#b5">Chung and Zisserman (2016a)</ref>, so we consider it as a baseline. The difference is that the concatenation of features is performed at the spacial size of 1 × 1 in our setting. This would not be a reasonable choice for this task alone for the spatial information in images would be lost across time. However, as shown in Table <ref type="table" target="#tab_2">3</ref>, our results adding the contrastive loss alone outperforms the baseline. With the help of sharing classifier and domain adversarial training, the results improve a large margin. Audio-Video Retrieval. To evaluate the closeness between the audio and face features, we borrow protocols used in the retrieval community. The retrieval experiments are conducted on the test set of LRW with 25000 samples, which means that given a test target video (audio), we try to find the closest audio (video) based on the distance of wid features F v w , F a w among all the test samples. Here we report the R@1, R@10 and M ed R measurements which is the same as <ref type="bibr" target="#b10">Faghri et al. (2017)</ref>. As we can see in Table <ref type="table" target="#tab_2">3</ref>, with all supervisions, the highest results can be achieved. Qualitative Results. Figure <ref type="figure" target="#fig_9">8</ref> shows the sequence generation quality from audio with different supervisions provided above. We can observe from the figure that given the same clip of audio, the duration of the mouth opening and to what extent it is opened is affected by different supervisions. Sharing the classifier apparently lengthens the time and strength of the mouth opening to make the image closer to the ground truth. Combining with the adversarial training makes the image quality improves. Note that it is not a one-to-one mapping between audio and lip motion; different subjects may behave different lip motion given the same audio clip so the final results may not perform the same as the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Identity-Speech Disentanglement</head><p>To validate our adversarial training is able to disentangle speech information from person-ID branch, we use person-ID encoder on every frame of a video and concatenate them to get F v p = {f v p(1) , ..., f v p(n )}. Then we train an SVM to map training samples to their wid labels and test the results, which implies that we attempt to find the wid information left in the pid encoder. The whole procedure is repeated before and after the feature disentanglement. Before the disentanglement, 27.8% of the test set can be assigned to the right class, but only 9.7% left after, indicating that considerable speech content information within the encoder E v p is gone. We then highlight the merits of adversarial disentanglement from two aspects, identity preserving and lip sync quality. For identity preserving, we use OpenFace's squared L2 similarity score as an indicator and compare the identity distance between the generated faces and the original ones (lower indicates more similar). For lip sync quality, we detect 20 landmarks using dlib library <ref type="bibr" target="#b13">(King 2009)</ref> around the lips to characterize its deviation from ground truth, measured by the averaged L2-norm (lower is better). Then we conduct retrieval experiments between all generated results and source videos based on extracted F v wid features. Experiments are also conducted on a direct replication of every video clip, to prove that the retrieval results are affected by lip motion rather than appearance features. From Table <ref type="table" target="#tab_3">4</ref>, we  can observe that adversarial disentanglement indeed helps improves lip sync quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel framework called Disentangled Audio-Visual System (DAVS), which generates high quality talking face videos using disentangled audio-visual representation. Specifically, we first learn a joint audiovisual embedding space wid with discriminative speech information by leveraging the word-ID labels. Then we disentangled the wid space from the person-ID pid space through adversarial learning. Compared to prior works, DAVS has several appealing properties: (1) A joint audio-visual representation is learned through audio-visual speech discrimination by associating several supervisions. The disentangled audio-visual representation significantly improves lip reading performance;</p><p>(2) Audio-visual speech recognition and audio-visual synchronizing are unified in an end-to-end framework;</p><p>(3) Most importantly, arbitrary-subject talking face generation with high-quality and temporal accuracy can be achieved by our framework; both audio and video speech information can be employed as input guidance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i.e. recognizing words from talking face sequence and audios, aka lip read-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We propose to guide the information flow by using labels to ensure the spaces contain discriminative semantic information dispelling from each other. With the assumption that Word-ID space is shared between visual and audio information, our model can reconstruct faces base on either video or audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of our framework. E v p is the encoder that encodes Person-ID information from visual source to the pid space, E v w and E a w are the Word-ID encoders that extract speech content information to wid space from video and audio. Decoder G takes any combination of features in pid and wid space to generate faces. D seq is a discriminator used for GAN loss. The adversarial training part contains two extra classifiers C w p and C p w . The details of embedding the wid space and adversarial training are shown in Fig 4 and 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of embedding the audio-visual shared space wid. The encoded featuresF v w = [f v w(1) , • • • , f v w(n) ] and F a w = [f a w(1) , • • • , f a w(n)] are constrained by contrastive loss L c , classification loss L w and domain adversarial training L D adv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Procedure of adversarial training for dispelling wid information from pid space. The training for classifier C w p is illustrated on the left and encoder E v p on the right. weights are updated on solid lines but not on the dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative results. The guidance input image is on the left . The upper half is the generation from video and lower half is the generation from audio information.</figDesc><graphic url="image-84.png" coords="5,123.15,177.12,428.66,109.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Qualitative results comparing with Chung et al. The mouth shapes are arbitrary.</figDesc><graphic url="image-92.png" coords="6,403.78,59.59,73.38,115.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Full</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Qualitative results for different types of supervisions. The left indicates different supervisions. All the generations are audio-based.</figDesc><graphic url="image-100.png" coords="7,400.60,315.05,153.04,138.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PSNR and SSIM scores for generation from audio and video wid information with and without GAN loss.</figDesc><table><row><cell>Approach \ Score</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Audio (L L1 )</cell><cell>25.4</cell><cell>0.859</cell></row><row><cell>Video (L L1 )</cell><cell>25.7</cell><cell>0.865</cell></row><row><cell>Audio (L Re )</cell><cell>26.7</cell><cell>0.883</cell></row><row><cell>Video (L Re )</cell><cell>26.8</cell><cell>0.884</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>User study of our generation results and reproduced baseline. The results are averaged over person and time.</figDesc><table><row><cell>Method \ Rate</cell><cell cols="2">Realistic Lip-Audio Sync</cell></row><row><cell>Reproduced Baseline</cell><cell>44.1%</cell><cell>58.0%</cell></row><row><cell>Ours (Generation from Audio)</cell><cell>51.5%</cell><cell>72.3%</cell></row><row><cell>Ours (Generation from Video)</cell><cell>87.8%</cell><cell>88.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Audio-Visual Speech Recognition and 1:25000 audio-video retrieval results with different supervisions. The first column is the supervisions, we use L C to represent contrastive loss, SC for sharing classifier, L D adv for the adversarial training.</figDesc><table><row><cell></cell><cell cols="3">Audio-Visual Speech Recognition</cell><cell cols="3">Video to Audio Retrieval</cell><cell cols="3">Audio to Video Retrieval</cell></row><row><cell>Approach</cell><cell cols="9">Visual acc. Audio acc. Combine acc. R@1 R@10 M ed R R@1 R@10 M ed R</cell></row><row><cell>(Chung and Zisserman 2016a)</cell><cell>61.1%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (LC )</cell><cell>61.8%</cell><cell>81.7</cell><cell>90.8%</cell><cell>29.3</cell><cell>56.3</cell><cell>6.0</cell><cell>29.8</cell><cell>56.3</cell><cell>6.0</cell></row><row><cell>Ours (LC + SC)</cell><cell>65.6%</cell><cell>91.6%</cell><cell>94.9%</cell><cell>38.8</cell><cell>66.4</cell><cell>3.0</cell><cell>44.5</cell><cell>70.9</cell><cell>2.0</cell></row><row><cell>Ours (LC + L D adv ) Ours (LC + SC + L D adv )</cell><cell>63.5% 67.5%</cell><cell>88.1% 91.8%</cell><cell>93.7% 95.2%</cell><cell>39.3 64.2</cell><cell>67.9 84.7</cell><cell>3.0 1.0</cell><cell>42.2 67.7</cell><cell>69.2 85.8</cell><cell>2.0 1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on disentangle mechanism. Landmark L2 ID Squared L2 Retrieval R@1 Landmark L2 ID Squared L2</figDesc><table><row><cell></cell><cell cols="2">Audio Generation to Source</cell><cell></cell><cell></cell><cell>Video Generation to Source</cell><cell></cell></row><row><cell cols="2">Experiment Retrieval R@1 Direct Replication 2.5</cell><cell>4.27</cell><cell>-</cell><cell>2.5</cell><cell>4.31</cell><cell>-</cell></row><row><cell>Without Disentanglement</cell><cell>53.8</cell><cell>3.94</cell><cell>0.212</cell><cell>90.8</cell><cell>3.60</cell><cell>0.194</cell></row><row><cell>With Disentanglement</cell><cell>60.5</cell><cell>3.48</cell><cell>0.188</cell><cell>95.3</cell><cell>2.85</cell><cell>0.174</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://liuziwei7.github.io/projects/TalkingFace</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yu Xiong for helpful discussions and his assistance with our video. This work is supported by Sense-Time Group Limited, the General Research Fund sponsored by the Research Grants Council of Hong Kong and the Hong Kong Innovation and Technology Support Program (No.ITS/121/15FX).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading in profile</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A deep bidirectional lstm approach for video-realistic talking head</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP. Fan</title>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<publisher>Multimedia Tools and Applications</publisher>
		</imprint>
	</monogr>
	<note>Photoreal talking head with deep bidirectional lstm</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><surname>Tog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2011">2011. 2015</date>
		</imprint>
	</monogr>
	<note>Subspace video stabilization</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent scale approximation for object detection in cnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive centroid projection for semi-supervised largescale recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring disentangled feature representation beyond face identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lip synchronization of speech</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Bitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio-Visual Speech Processing: Computational &amp; Cognitive Science Approaches</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing voices and hearing faces: Cross-modal biometric matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
				<editor>
			<persName><surname>Cvpr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zisserman</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018a. 2018b. 2017</date>
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04786</idno>
		<imprint>
			<date type="published" when="2017">2018. 2017. 2017</date>
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Synthesizing obama: learning lip sync from audio</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<title level="m">Image quality assessment: from error visibility to structural similarity</title>
				<imprint>
			<publisher>TIP</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation by using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2010">2010. 2018</date>
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Realistic mouth-synching for speech-driven talking face using articulatory modelling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
