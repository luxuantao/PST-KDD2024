<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Cross Networks with Vertex Infomax Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maosen</forename><surname>Li</surname></persName>
							<email>maosen_li@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siheng</forename><surname>Chen</surname></persName>
							<email>schen@merl.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>ya_zhang@sjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
							<email>ivor.tsang@uts.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Cross Networks with Vertex Infomax Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel graph cross network (GXN) to achieve comprehensive feature learning from multiple scales of a graph. Based on trainable hierarchical representations of a graph, GXN enables the interchange of intermediate features across scales to promote information flow. Two key ingredients of GXN include a novel vertex infomax pooling (VIPool), which creates multiscale graphs in a trainable manner, and a novel feature-crossing layer, enabling feature interchange across scales. The proposed VIPool selects the most informative subset of vertices based on the neural estimation of mutual information between vertex features and neighborhood features. The intuition behind is that a vertex is informative when it can maximally reflect its neighboring information. The proposed feature-crossing layer fuses intermediate features between two scales for mutual enhancement by improving information flow and enriching multiscale features at hidden layers. The cross shape of feature-crossing layer distinguishes GXN from many other multiscale architectures. Experimental results show that the proposed GXN improves the classification accuracy by 1.96% and 1.15% on average for graph classification and vertex classification, respectively. Based on the same network, the proposed VIPool consistently outperforms other graph-pooling methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there are explosive interests in studying graph neural networks (GNNs) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, which expand deep learning techniques to ubiquitous non-Euclidean graph data, such as social networks <ref type="bibr" target="#b48">[49]</ref>, bioinformatic networks <ref type="bibr" target="#b14">[15]</ref> and human activities <ref type="bibr" target="#b32">[33]</ref>. Achieving good performances on graph-related tasks, such as vertex classification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref> and graph classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50]</ref>, GNNs learn patterns from both graph structures and vertex information with feature extraction in spectral domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref> or vertex domain <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref>. Nevertheless, most GNN-based methods learn features of graphs with fixed scales, which might underestimate either local or global information. To address this issue, multiscale feature learning on graphs enables capturing more comprehensive graph features for downstream tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Multiscale feature learning on graphs is a natural generalization from multiresolution analysis of images, whose related techniques, such as wavelets and pyramid representations, have been well studied in both theory and practice <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">54]</ref>. However, this generalization is technically nontrivial. While hierarchical representations and pixel-to-pixel associations across scales are straightforward for images with regular lattices, the highly irregular structures of graphs cause challenges in producing graphs at various scales <ref type="bibr" target="#b7">[8]</ref> and aggregating features across scales.</p><p>To generate multiscale graphs, graph pooling methods are essential to compress large graphs into smaller ones. Conventional graph pooling methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>  (a) Encoder-decoder <ref type="bibr" target="#b5">[6]</ref>.</p><p>(b) Graph U-net <ref type="bibr" target="#b19">[20]</ref>. (c) Readout <ref type="bibr" target="#b30">[31]</ref>.</p><p>(d) GXN (ours). designed rules. Recently, some data-driven pooling methods are proposed, which automatically merge a fine-scale vertex subset to a coarsened vertex <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50]</ref>. The coarsened graph, however, might not have direct vertex-to-vertex association with the original scale. Some other graph pooling methods adaptively select vertices based on their importance over the entire graph <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>; however, they fail to consider local information.</p><p>To aggregate features across multiple scales, existing attempts build encoder-decoder architecture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12]</ref> to learn graph features from the latent spaces, which might underestimate fine-scale information. Some other works gather the multiscale features in parallel and merge them as the final representation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref>, which might limit information flow across scales.</p><p>In this work, we design a new graph neural network to achieve multiscale feature learning on graphs, and our technical contributions are two-folds: a novel graph pooling operation to preserve informative vertices and a novel model architecture to exploit rich multiscale information.</p><p>A novel graph pooling operation: Vertex infomax pooling (VIPool). We propose a novel graph pooling operation by selecting and preserving those vertices that can maximally express their corresponding neighborhoods. The criterion of vertex-selection is based on the neural estimation of mutual information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref> between vertex and neighborhood features, thus we call the proposed pooling mechanism vertex infomax pooling (VIPool). Based on VIPool, we can implement graph pooling and unpooling to coarsen and refine multiple scales of a graph. Compared to the vertex-grouping-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50]</ref>, the proposed VIPool provides the direct vertex-vertex association across scales and makes the coarsened graph structure and information fusion easier to achieve. Compared to other vertex-selection-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>, VIPool considers both local and global information on graphs by learning both vertex representation and graph structures.</p><p>A novel model architecture: Graph cross network (GXN). We propose a new model with a novel architecture called graph cross network (GXN) to achieve feature learning on multiscale graphs. Employing the trainable VIPool, our model creates multiscale graphs in data-driven manners. To learn features from all parallel scales, our model is built with a pyramid structure. To further promote information flow, we propose novel intermediate feature-crossing layers to interchange features across scales in each network layer. The intuition of feature-crossing is that the it improves information flow and exploits richer multiscale information in multiple network layers rather than only combine them in the last layer. Similar crossing structures have been explored for analyzing images <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref>, but we cannot directly use those structures for irregular graphs. The proposed feature-crossing layer handles irregular graphs by providing the direct vertex-vertex associations across multiple graph scales and network layers; see typical multiscale architectures in Figure <ref type="figure" target="#fig_1">1</ref>, where GXN is well distinguished because intermediate feature interchanging across scales forms a crossing shape.</p><p>Remark: In each individual scale, graph U-net <ref type="bibr" target="#b19">[20]</ref> simply uses skip connections while GXN uses multiple graph propagation layers to extract features. The proposed feature-crossing layer is used to fuse intermediate features and cannot be directly applied to graph U-net.</p><p>To test our methods, we conduct extensive experiments on several standard datasets for both graph classification and vertex classification. Compared to state-of-the-art methods for these two tasks, GXN improves the average classification accuracies by 1.96% and 1.15%, respectively. Meanwhile, based on the same model architecture, our VIPool consistently outperforms previous graph pooling methods; and more intermediate connection leads to a better performance. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Multiscale graph neural networks with graph pooling. To comprehensively learn the multiscale graph representations, various multiscale network structures have been explored. Hierarchical encoder-decoder structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12]</ref> learn graph features just from much coarse scales. LancozsNet <ref type="bibr" target="#b34">[35]</ref> designs various graph filters on the multiscale graphs. Graph U-net <ref type="bibr" target="#b19">[20]</ref> and readout functions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref> design pyramid structures with skip-connections and combines features from all scales in the last layer. Compared to previous works, the proposed GXN has two main differences. 1) Besides the common late fusion of features, GXN uses intermediate fusion across scales, where the features at various scales in each network layer are fused to embed richer multiscale information. 2) GXN extracts hierarchical multiscale features through a deep network, previous Graph U-net <ref type="bibr" target="#b19">[20]</ref> extracts features only once in each scale and then uses skip-connections to fuse feature across scales.</p><p>To compress a graph into multiple coarser scales, various methods of graph pooling are proposed.</p><p>Early graph pooling methods are usually designed based on graph sampling theory <ref type="bibr" target="#b7">[8]</ref> or graph coarsening <ref type="bibr" target="#b41">[42]</ref>. With the study of deep learning, some works down-scale graphs in data-driven manner. The graph-coarsening-based pooling methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> cluster vertices and merge each cluster to a coarsened vertex; however, there is not vertex-to-vertex association to preserve the original vertex information. The vertex-selection-based pooling methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> preserve selected vertices based on their importance, but tend to loss the original graph structures.</p><p>Compared to previous works, the proposed VIPool in GXN is trained given an explicit optimization for vertex selection, and the pooled graph effectively abstracts the original graph structure.</p><p>Mutual information estimation and maximization. Given two variables, to estimate their mutual information whose extact value is hard to compute, some models are constructed based on the parameterization of neural networks. <ref type="bibr" target="#b1">[2]</ref> leverages trainable networks to depict a lower bound of mutual information, which could be optimized toward a precise mutual information estimation. <ref type="bibr" target="#b25">[26]</ref> maximizes the pixel-image mutual information to promote to capture the most informative image patterns via self-supervision. <ref type="bibr" target="#b47">[48]</ref> maximizes the mutual information between a graph and each single vertex, where the representative vertex features are obtained. Similarly, <ref type="bibr" target="#b43">[44]</ref> applies the mutual information maximization on graph classification. Compared to these mutual-information-based studies, the proposed VIPool, which also leverages mutual information maximization on graphs, aims to obtain an optimization for vertex selection by finding the vertices that maximally represent their local neighborhood. We also note that, in VIPool, the data distribution is defined on a single graph, while previous works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b43">44]</ref> assume to train on the distribution of multiple graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vertex Infomax Pooling</head><p>Before introducing the overall model, we first propose a new graph pooling method to create multiple scales of a graph. In this graph pooling, we select and preserve a ratio of vertices and connect them based on the original graph structure. Since downscaling graphs would lose information, it is critical to preserve as much information as possible in the pooled graph, which could maximally represent the original graphs. To this end, we propose a novel vertex infomax pooling (VIPool), preserving the vertices that carry high mutual information with their surrounding neighborhoods by mutual information estimation and maximization. The preserved vertices well represent local subgraphs, and they also abstract the overall graph structure based on a vertex selection criterion.</p><p>Mathematically, let G(V, A) be a graph with a set of vertices</p><formula xml:id="formula_0">V = {v 1 , . . . , v N } whose features are X = [x 1 • • • x N ] ∈ R N ×d</formula><p>, and an adjacency matrix A ∈ {0, 1} N ×N . We aim to select a subset Ω ⊂ V that contains |Ω| = K vertices. Considering a criterion function C(•) to quantify the information of a vertex subset, we find the most informative subset through solving the problem,</p><formula xml:id="formula_1">max Ω⊂V C (Ω) , subject to |Ω| = K.<label>(1)</label></formula><p>We design C(Ω) based on the mutual information between vertices and their corresponding neighborhoods, reflecting the vertices' abilities to express neighborhoods. In the following, we first introduce the computation of vertex-neighborhood mutual information, leading to the definition of C(•); we next select a vertex set by solving (1); we finally pool a fine graph based on the selected vertices.</p><p>Mutual information neural estimation. In a graph G(V, A), for any selected vertex v in Ω ⊂ V, we define v's neighborhood as N v , which is the subgraph containing the vertices in V whose geodesic distances to v are no greater than a threshold R according to the original G(V, A), i.e.</p><formula xml:id="formula_2">N v = G({u} d(u,v)≤R , A {u},{u}</formula><p>). Let a random variable v be the feature of a randomly picked vertex in Ω, the distribution of v is</p><formula xml:id="formula_3">P v = P (v = x v )</formula><p>, where x v is the outcome feature value when we pick vertex v. Similarly, let a random variable n be the neighborhood feature associated with a randomly picked vertex in Ω, the distribution of n is P n = P (n = y Nu ), where y Nu is the outcome feature value when we pick vertex u's neighborhood. The mutual information between selected vertices and neighborhoods is the KL-divergence between the joint distribution P v,n = P (v = x v , n = y Nv ) and the product of marginal distributions P v ⊗ P n :</p><formula xml:id="formula_4">I (Ω) (v, n) = D KL (P v,n ||P v ⊗ P n ) (a) ≥ sup T ∈T E xv,y Nv ∼Pv,n [T (x v , y Nv )] − E xv∼Pv,y Nu ∼Pn e T (xv,y Nu )−1 ,</formula><p>where (a) follows from f -divergence representation based on KL divergence <ref type="bibr" target="#b1">[2]</ref>; T ∈ T is an arbitrary function that maps features of a pair of vertex and neighborhood to a real value, here reflecting the dependency of two features. To achieve more flexibility and convenience in optimization, fdivergence representation based on a non-KL divergence can be adopted <ref type="bibr" target="#b37">[38]</ref>, which still measures the vertex-neighborhood dependency. Here we consider a GAN-like divergence.</p><formula xml:id="formula_5">I (Ω) GAN (v, n) ≥ sup T ∈T E Pv,n [log σ (T (x v , y Nv ))] + E Pv,Pn [log (1 − σ (T (x v , y Nu )))] ,</formula><p>where σ(•) is the sigmoid function. In practice, we cannot go over the entire functional space T to evaluate the exact value of</p><formula xml:id="formula_6">I (Ω)</formula><p>GAN . Instead, we parameterize T (•, •) by a neural network T w (•, •), where the subscript w denotes the trainable parameters. Through optimizing over w, we obtain a neural estimation of the GAN-based mutual information, denoted as</p><formula xml:id="formula_7">I (Ω)</formula><p>GAN . We can define our vertex selection criterion function to be this neural estimation; that is,</p><formula xml:id="formula_8">C (Ω) = I (Ω) GAN = max w 1 |Ω| v∈Ω log σ (T w (x v , y Nv )))+ 1 |Ω| 2 (v,u)∈Ω log 1−σ (T w (x v , y Nu ))) .</formula><p>In C(Ω), the first term reflects the affinities between vertices and their own neighborhoods; and the second term reflects the differences between vertices and arbitrary neighborhoods. Notably, a higher C score indicates that vertices maximally reflect their own neighborhoods and meanwhile minimally reflect arbitrary neighborhoods. To specify T w , we consider T w (x v , y Nu ) = S w (E w (x v ), P w (y Nu )), where the subscript w indicates the associated functions are trainable<ref type="foot" target="#foot_1">2</ref> , E w (•) and P w (•) are embedding functions of vertices and neighborhoods, respectively, and S w (•, •) is an affinity function to quantify the affinity between vertices and neighborhoods; see an illustration in Figure <ref type="figure" target="#fig_2">2</ref>. We implement E w (•) and S w (•, •) by multi-layer perceptrons (MLPs), and implement P w (•) by aggregating vertex features and neighborhood connectivities in y Nu ; that is</p><formula xml:id="formula_9">P w (y Nu ) = 1 R R r=0 ν∈Nu ( D −1/2 A D −1/2 ) r ν,u W (r) E w (x ν ), ∀u ∈ Ω,<label>(2)</label></formula><p>where A = A + I ∈ {0, 1} N ×N denotes the self-connected graph adjacency matrix and D is the degree matrix of A; W (r) is the trainable weight associated with the rth hop of neighbors; P w (•).</p><p>The detailed derivation is presented in Appendix A.</p><p>When we maximize C(Ω) by training E w (•), P w (•) and S w (•, •), we estimate the mutual information between vertices in Ω and their neighborhoods. This is similar to deep graph infomax (DGI) <ref type="bibr" target="#b47">[48]</ref>, which estimates the mutual information between any vertex feature and a global graph embedding.</p><p>Both DGI and the proposed VIPool apply the techniques of mutual information neural estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> to the graph domain; however, there are two major differences. First, DGI aims to train a graph embedding function while VIPool aims to evaluate the importance of a vertex via its affinity to its neighborhood. Second, DGI considers the relationship between a vertex and an entire graph while VIPool learns the dependency between a vertex and a neighborhood. By varying the neighbor-hop R of N u in Eq. ( <ref type="formula" target="#formula_9">2</ref>), VIPool is able to tradeoff local and global information.</p><p>Solutions for vertex selection. To solve the vertex selection problem (1), we consider the submodularity of mutual information <ref type="bibr" target="#b8">[9]</ref> and employ a greedy algorithm: we select the first vertex with maximum C(Ω) with |Ω| = 1; and we next add a new vertex sequentially by maximizing C(Ω) greedily; however, it is computationally expensive to evaluate C(Ω) for two reasons: (i) for any vertex set Ω, we need to solve an individual optimization problem; and (ii) the second term of C(Ω) includes all the pairwise interactions involved with quadratic computational cost. To address issue (i), we set the vertex set to all the vertices in the graph, maximize</p><formula xml:id="formula_10">I (V)</formula><p>GAN to train E w (•), P w (•) and S w (•, •). We then fix those three functions and evaluate I (Ω) GAN . To address issue (ii), we perform negative sampling, approximating the second term <ref type="bibr" target="#b35">[36]</ref>, where we sample negative neighborhoods N u from the entire graph, whose number equals the number of positive vertex samples; that is, |Ω|.</p><p>Graph pooling and unpooling. After solving problem. (1), we obtain Ω that contains K unique vertices selected from V. To implement graph pooling, we further consider the distinct importance of different vertices in Ω, we compute an affinity score for each vertex based on its ability to describe its neighborhood. For vertex v with feature x v and neighborhood feature y Nv , the affinity score is</p><formula xml:id="formula_11">a v = σ (S w (E w (x v ), P w (y Nv ))) ∈ [0, 1], ∀v ∈ Ω.<label>(3)</label></formula><p>Eq. ( <ref type="formula" target="#formula_11">3</ref>) considers the affinity only between a vertex and its own neighborhood, showing the degree of vertex-neighborhood information dependency. We collect a v for ∀v ∈ Ω to form an affinity vector a ∈ [0, 1] K . For graph data pooling, the pooled vertex feature X Ω = X(id, :) (a1 ) ∈ R K×d , where id denotes selected vertices's indices that are originally in V, 1 is an all-one vector, and denotes the element-wise multiplication. With the affinity vector a, we assign an importance to each vertex and provide a path for back-propagation to flow gradients. As for graph structure pooling, we calculate A Ω = Pool A (A), and we consider three approaches to implement Pool A (•):</p><p>• Edge removal, i.e. A Ω = A(id, id). This is simple, but loses significant structural information;</p><p>• Kron reduction <ref type="bibr" target="#b15">[16]</ref>, which is the Schur complement of the graph Laplacian matrix and preserves the graph spectral properties, but it is computationally expensive due to the matrix inversion;</p><p>• Cluster-connection, i.e. A Ω = SAS with S = softmax(A(id, :)) ∈ [0, 1] K×N . Each row of S represents the neighborhood of a selected vertex and the softmax function is applied for normalization. The intuition is to merge the neighboring information to the selected vertices <ref type="bibr" target="#b49">[50]</ref>.</p><p>Cluster-connection is our default implementation of the graph structure pooling. Figure <ref type="figure" target="#fig_2">2</ref> illustrates the overall process of vertex selection and graph pooling process.</p><p>To implement graph unpooling, inspired by <ref type="bibr" target="#b19">[20]</ref>, we design an inverse process against graph pooling. We initialize a zero matrix for the unpooled graph data, X = O ∈ {0} N ×d ; and then, fill it by fetching the vertex features according to the original indices of retrained vertices; that is, X (id, :) = X Ω . We then interpolate it through a graph propagation layer (implemented by graph convolution <ref type="bibr" target="#b29">[30]</ref>) to propagate information from the vertices in Ω to the padded ones via the original graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph Cross Network</head><p>In this section, we propose the architecture of our graph cross network (GXN) for multiscale graph feature learning; see an exemplar model with 3 scales and 4 feature-crossing layers in Figure <ref type="figure" target="#fig_3">3</ref>. The graph pooling/unpooling operations apply VIPool proposed in Section 3 and the graph propagation layers adopt the graph convolution layers <ref type="bibr" target="#b29">[30]</ref>. A key ingredient of GXN is that we design featurecrossing layers to enhance multiscale information fusion. The entire GXN includes three stages: multiscale graphs generation, multiscale features extraction and multiscale readout.</p><p>Multiscale graphs generation. Given an input graph G(V, A) with vertex features, X ∈ R N ×d , we aim to create graph representations at multiple scales. We first employ a graph propagation layer on the input graph to initially embed the finest scale of graph as G 0 (V 0 , A 0 ) with V 0 = V, A 0 = A and vertex representations X 0 , where the graph propagation layer is implemented by a graph convolution  layer <ref type="bibr" target="#b29">[30]</ref>. We then recursively apply VIPool for S times to obtain a series of coarser scales of graph G 1 (V 1 , A 1 ), . . . , G S (V S , A S ) and corresponding vertex features X 1 , . . . , X S from G 0 and X 0 , respectively, where</p><formula xml:id="formula_12">|V s | &gt; |V s | for ∀ 1 ≤ s &lt; s ≤ S.</formula><p>Multiscale features extraction. Given multiscale graphs, we build a graph neural network at each scale to extract features. Each network consists of a sequence of graph propagation layers. To further enhance the information flow across scales, we propose feature-crossing layers between two consecutive scales at various network layers, which allows multiscale features to communicate and merge in the intermediate layers. Mathematically, at scale s and the any network layer, let the feature matrix of graph G s be X h,s , the feature of graph G s−1 after pooling be X (p) h,s , and the feature of graph G s+1 after unpooling be X (up) h,s , the obtained vertex feature X h,s is formulated as</p><formula xml:id="formula_13">X h,s = X h,s + X (p) h,s + X (up) h,s , 0 &lt; s &lt; S.</formula><p>For s = 0 or S, X h,s is not fused by features from finer or coarser scales; see Figure <ref type="figure" target="#fig_3">3</ref>. The graph pooling/unpooling here uses the same vertices as those obtained in multiscale graph generation to associate the vertices at different layers, but the affinity score a in each feature-crossing layer is trained independently to reflect the vertex importance at different levels. Note that the vertex-to-vertex association across scales is important here for feature-crossing and VIPool nicely fits it.</p><p>Multiscale readout. After multiscale feature extraction, we combine deep features at all the scales together to obtain the final representation. To align features at different scales, we adopt a sequence of graph unpooling operations implemented in the VIPool to transform all features to the original scale; see Figure <ref type="figure" target="#fig_3">3</ref>. We finally leverage a readout graph propagation layer to further embed the fused multiscale features and generate the readout graph representation for various downstream tasks. In this work, we consider both graph classification and vertex classification.</p><p>Model Training. To train GXN, we consider the training loss with two terms: a graph-pooling loss</p><formula xml:id="formula_14">L pool = − I (V)</formula><p>GAN and a task-driven loss L task . For graph classification, the task-driven loss is the cross-entropy loss between the predicted and ground-truth graph labels, L task = −y log(ŷ), where y and ŷ are ground-truth label and predicted label of a graph; for vertex classification, it is the crossentropy loss between the predictions and ground-truth vertex labels, L task = − v∈V L y v log(ŷ v ), where y v and ŷv are ground-truth and predicted vertex labels, and V L contains labeled vertices. We finally define the overall loss as L = L task + αL pool , where the hyper-parameter α linearly decays per epoch from 2 to 0 during training, balancing a final task and vertex pooling<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experiment Setup</head><p>Datasets. To test our GXN, we conduct extensive experiments for graph classification and vertex classification on several datasets. For graph classification, we use social network datasets: IMDB-B, IMDB-M and COLLAB <ref type="bibr" target="#b48">[49]</ref>, and bioinformatic datasets: D&amp;D <ref type="bibr" target="#b14">[15]</ref>, PROTEINS <ref type="bibr" target="#b17">[18]</ref>, and ENZYMES <ref type="bibr" target="#b3">[4]</ref>. Table <ref type="table">1</ref> shows the dataset information. Note that no vertex feature is provided in three social network datasets, and we use one-hot vectors to encode the vertex degrees as vertex features, explicitly utilizing some structural information. We use the same dataset separation as in <ref type="bibr" target="#b19">[20]</ref>, perform 10-fold cross-validation, and show the average accuracy for evaluation. For vertex Table <ref type="table">1</ref>: Graph classification accuracies (%) of different methods on different datasets. GXN (gPool) and GXN (SAGPool) denote that we apply previous pooling operations, gPool <ref type="bibr" target="#b19">[20]</ref> and SAGPool <ref type="bibr" target="#b30">[31]</ref> in our GXN framework, respectively. Various fashions of feature-crossing are presented, including fusion of coarse-to-fine (↑), fine-to-coarse (↓), no feature-crossing (noCross), and feature-crossing at early, late and all layers of networks. Model configuration. We implement GXN with PyTorch 1.0 on one GTX-1080Ti GPU. For graph classification, we consider three scales, which preserve 50% to 100% vertices from the original scales, respectively. For both input and readout layers, we use 1-layer GCNs; for multiscale feature extraction, we use 2 GCN layers followed by ReLUs at each scale and feature-crossing layers between any two consecutive scales at any layers. After the readout layers, we unify the embeddings of various graphs to the same dimension by using the same SortPool in DGCNN <ref type="bibr" target="#b52">[53]</ref>, AttPool <ref type="bibr" target="#b26">[27]</ref> and Graph U-Net <ref type="bibr" target="#b19">[20]</ref>. In the VIPool, we use a 2-layer MLP and R-layer GCN (R = 1 or 2) as E w (•) and P w (•), and use a linear layer as S w (•, •). The hidden dimensions are 32. To improve the efficiency of solving problem (1), we modify C(Ω) by preserving only the first term. In this way, we effectively reduce the computational costs to sovle (1) from O(|V | 2 ) to O(|V |), and each vertex contributes the vertex set independently. The optimal solution is top-K vertices. We compare the outcomes of C(Ω) by the greedy algorithm and top-k method in Figure <ref type="figure" target="#fig_6">5</ref>. For vertex classification, we use similar architecture as in graph classification, while the hidden feature are 128-dimension. We directly use the readout layer for vertex classification. In the loss function L, α decays from 2 to 0 during training, where the VIPool needs fast convergence for vertex selection; and the model gradually focuses more on tasks based on the effective VIPool. We use Adam optimizer <ref type="bibr" target="#b13">[14]</ref> and the learining rates range from 0.0001 to 0.001 for different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison</head><p>Graph classification. We compare the proposed GXN to representative GNN-based methods, including PatchySAN <ref type="bibr" target="#b36">[37]</ref>, ECC <ref type="bibr" target="#b42">[43]</ref>, Set2Set <ref type="bibr" target="#b20">[21]</ref>, DGCNN <ref type="bibr" target="#b52">[53]</ref>, DiffPool <ref type="bibr" target="#b49">[50]</ref>, Graph U-Net <ref type="bibr" target="#b19">[20]</ref>, SAGPool <ref type="bibr" target="#b30">[31]</ref>, AttPool <ref type="bibr" target="#b26">[27]</ref>, and StructPool <ref type="bibr" target="#b50">[51]</ref>, where most of them performed multiscale graph feature learning. Additionally, we design several variants of GXN: 1) to test the superiority of VIPool, we apply gPool <ref type="bibr" target="#b19">[20]</ref>, SAGPool <ref type="bibr" target="#b30">[31]</ref> and AttPool <ref type="bibr" target="#b26">[27]</ref> in the same architecture of GXN, denoted as 'GXN (gPool)', 'GXN (SAGPool)' and 'GXN (AttPool)', respectively; 2) we investigate different feature-crossing mechanism, including various crossing directions and crossing positions. Table <ref type="table">1</ref> compares the accuracies of various methods for graph classification. We see that our model outperforms the state-of-the-art methods on 5 out of 6 datasets, achieving an improvement by 1.96% on average accuracies. Besides, VIPool and more feature-crossing lead to better performance. We also show the qualitative results of vertex selection of different graph pooling methods in Appendix. Vertex classification. We compare GXN to state-of-the-art methods: DeepWalk <ref type="bibr" target="#b38">[39]</ref>, GCN <ref type="bibr" target="#b29">[30]</ref>, GraphSAGE <ref type="bibr" target="#b22">[23]</ref>, FastGCN <ref type="bibr" target="#b6">[7]</ref>, ASGCN <ref type="bibr" target="#b27">[28]</ref>, and Graph U-Net <ref type="bibr" target="#b19">[20]</ref> for vertex classification. We reproduce these methods for both full-supervised and semi-supervised learning based on their official codes. Table <ref type="table">8</ref> compares the vertex classification accuracies of various methods. Considering both full-supervised and semi-supervised settings, we see that our model achieves higher average accuracy by 1.15%. We also test a degraded GXN without any feature-crossing layer, and we see that the feature-crossing layers improves the accuracies by 1.10% on average; see more results in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Analysis</head><p>We further conduct detailed analysis about the GXN architecture and VIPool.</p><p>GXN architectures. We test the architecture with various graph scales and feature-crossing layers.</p><p>Base on the dataset of IMDB-B for graph classification, we vary the number of graph scales from 1 to 5 and the number of feature-crossing layers from 1 to 3. We present the vertex classification results in Table <ref type="table" target="#tab_3">3</ref>. We see that the architecture with 3 graph scales and 2 feature-crossing layers leads to the best performance. Compared to use only 1 graph scale, using 2 graph scales significantly improve the graph classification accuracy by 2.53% on average, indicating the importance of multiscale representations. When we use more than 3 scales, the classification results tend to be stable, indicating the redundant scales. To keep the model efficiency and effectiveness, we adopt 3 scales of graphs. As for the number of feature-crossing layers, only using 1 feature-crossing layer do not provide sufficient information for graph classification; while using more than 2 feature-crossing layers tends to damage model performance due to the higher model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hops of neighborhood in VIPool.</head><p>To validate the effects of different ranges of neighborhood information in VIPool, we vary the neighbor-hops R in P w (•) from 1 to 5 and perform graph classification on D&amp;D and IMDB-B. When R increases, we push a vertex to represent a bigger neighborhood with more global information. Figure <ref type="figure" target="#fig_4">4</ref> shows the graph classification accuracies with various R on the two datasets. We see that, for D&amp;D, which include graphs with relatively larger sizes (see Table <ref type="table">1</ref>, line 3), when R = 2, the model achieves the best performance, reflecting that vertices could express their neighborhoods within R = 2; while for IMDB-B with smaller graphs, vertices tend to express their 1-hop neighbors better. This reflects that VIPool achieves a flexible trade-off between local and global information through varying R to adapt to various graphs.</p><p>Approximation of C function. In VIPool, to optimize problem (1) more efficiently, we substitute the original C(Ω) by only preserving the positive term C + (Ω) = v∈Ω log σ(S w (h v , h Nv )) and maximize C + (Ω) by selecting 'Top-K', which also obtains the optimal solution. To see the performance gap between the original and the accelerated versions, we compare the exact value of C(Ω)      with the selected Ω by optimizing C(Ω) with greedy algorithm and optimizing C + (Ω) with 'Top-K' method, respectively, on different types of datasets: bioinformatic, social and citation networks. We vary the ratio of the vertex selection among the global graph from 0.1 to 0.9. Figure <ref type="figure" target="#fig_6">5</ref> compares the C(Ω) as a function of selection ratio with two algorithms on 3 datasets, and the vertical dash lines denotes the boundaries where the value gaps equal to 10%. We see that, when we select small percentages (e.g. &lt; 60%) of vertices, the C value obtained by the greedy algorithm is much higher than 'Top-K' method; when we select more vertices, there are very small gaps between the two optimization algorithms, indicating two similar solutions of vertex selection. In GXN, we set the selection ratio above 60% in each graph pooling. More results about the model performances varying with ratios of vertices selection are presented in Appendix.</p><formula xml:id="formula_15">#</formula><p>Graph structure pooling. In VIPool, we consider three implementations for graph structure pooling Pool A (•): edge-removal, Kron reduction and cluster-connection. We test these three operations for semi-supervised vertex classification on Cora and graph classification on IMDB-B, and we show the classification accuracies and time costs of the three graph structure pooling operations (denoted as 'Edge-Remove', 'Kron-Reduce' and 'Clus-Connect', respectively) in Tables <ref type="table" target="#tab_5">4 and 5</ref>. We see that Kron reduction or cluster-connection tend to provide the best accuracies on different datasets, but Kron reduction is significantly more expensive than the other two methods due to the matrix inversion.</p><p>On the other hand, cluster-connection provides a better tradeoff between effectiveness and efficiency and we thus consider cluster-connection as our default choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper proposes a novel model graph cross network (GXN), where we construct parallel networks for feature learning at multiple scales of a graph and design novel feature-crossing layers to fuse intermediate features across multiple scales. To downsample graphs into various scales, we propose vertex infomax pooling (VIPool), selecting those vertices that maximally describe their neighborhood information. Based on the selected vertices, we coarsen graph structures and the corresponding graph data. VIPool is optimized based on the neural estimation of the mutual information between vertices and neighborhoods. Extensive experiments show that (i) GXN outperforms most state-of-the-art methods on graph classification and vertex classification; (ii) VIPool outperforms the other pooling methods; and (iii) more intermediate fusion across scales leads to better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact of Our Work</head><p>In this work, we aim to propose a method for multiscale feature learning on graphs, achieving two basic but challenging tasks: graph classification and vertex classification. This work has the following potential possible impacts to the society and the research community.</p><p>This work could be effectively used in many practical and important scenarios such as drug molecular analysis, social network mining, biometrics, human action recognition and motion prediction, etc., making our daily life more convenient and efficient. Due to the ubiquitous graph data, in most cases, we can try to construct multiscale graphs to comprehensively obtain rich detailed, abstract, and even global feature representations, and effectively improve downstream tasks.</p><p>Our network structure can not only solve problem of feature learning with multiple graph scales, but also can be applied to the pattern learning of heterogeneous graphs, or other cross-modal or crossview machine learning scenarios. This is of great significance for improving the ability of pattern recognition, feature transfer, and knowledge distillation to improve the computational efficiency.</p><p>At the same time, this work may have some negative consequences. For example, in social networks, it is uncomfortable even dangerous to use the models based on this work to over-mine the behavior of users, because the user's personal privacy and information security are crucial; companies should avoid mining too much users' personal information when building social platforms, keeping a safe internet environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Mutual Information Neural Estimation for Vertex Selection</head><p>An individual vertex is fully identified through its feature, which works as the vertex attribute. Given an vertex set V that contains all the vertices on the graph and a vertex subset Ω ⊂ V which contains the selected vertices, we let a random variable v to represent the vertex feature when we randomly pick a vertex from Ω. Then we define the probability distribution of v as</p><formula xml:id="formula_16">P v = P (v = x v ), ∀v ∈ Ω</formula><p>where x v is the feature value when we pick vertex v.</p><p>The neighborhood of any vertex u ∈ Ω is defined as N u , which is the subgraph containing the vertices in V whose geodesic distances to the central vertex u are no greater than a threshold R, i.e.</p><formula xml:id="formula_17">N v = G({u} d(u,v)≤R , A {u},{u}</formula><p>). Let a random variable n be the neighborhood features when we randomly pick a central vertex from Ω, then we define the probability distribution of n as</p><formula xml:id="formula_18">P n = P (n = y Nu ), ∀u ∈ Ω</formula><p>where y Nu = [A Nu,Nu , {x ν } ν∈Nu ] denotes the neighborhood feature value when we pick vertex u's neighborhood, including both the internal connectivity information and contained vertex features in the neighborhood N u .</p><p>Therefore, we define the joint distribution of the random variables of vertex features and neighborhood features, which is formulated as</p><formula xml:id="formula_19">P v,n = P (v = x v , n = y Nv ), ∀v ∈ Ω</formula><p>where the joint distribution reflects probability that we randomly pick the corresponding vertex feature and neighborhood feature of the same vertex v together.</p><p>The mutual information between the vertex features and the neighborhood features is defined as the KL-divergence between the joint distribution P v,n and the product of the marginal distributions of two random variable, P v ⊗ P n ; that is</p><formula xml:id="formula_20">I (Ω) (v, n) = D KL (P v,n ||P v ⊗ P n ) ,</formula><p>This mutual information measures of the mutual dependency between vertices and neighborhoods in the selected vertex subset Ω. The KL divergence admits the f -representation <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_21">D KL (P v,n ||P v ⊗ P n ) ≥ sup T ∈T E xv,y Nv ∼Pv,n [T (x v , y Nv )] − E xv∼Pv,y Nu ∼Pn e T (xv,y Nu )−1 , (<label>4</label></formula><formula xml:id="formula_22">)</formula><p>where T is an arbitrary class of functions that maps a pair of vertex features and neighborhood features to a real value, and here we use T (•, •) to compute the dependency of two features. It could be a tight lower-bound of mutual information if we search any possible function T ∈ T .</p><p>Note that the main target here is to propose a vertex-selection criterion based on quantifying the dependency between vertices and neighborhood. Therefore instead of computing the exact mutual information based on KL divergence, we can use non-KL divergences to achieve favourable flexibility and convenience in optimization. Both non-KL and KL divergences can be formulated based on the same f -representation framework. Here we start from the general f -divergence between the joint distribution and the product of marginal distributions of vertices and neighborhoods.</p><formula xml:id="formula_23">D f (P v,n ||P v ⊗ P n ) = P v P n f P v,n P v P n dx v dy Nv</formula><p>where f (•) is a convex and lower-semicontinuous divergence function. when f (x) = x log x, the f -divergence is specificed as KL divergence. The function f (•) has a convex conjugate function f * (•), i.e. f * (t) = sup x∈dom f {xt − f (x)}, where dom f is the definition domain of f (•). Note that the two functions f (•) and f * (•) is dual to each other. According to the Fenchel conjugate <ref type="bibr" target="#b24">[25]</ref>, the f -divergence can be modified as</p><formula xml:id="formula_24">D f (P v,n ||P v ⊗ P n ) = P x P n sup t∈dom f * t P x,n P v P n − f * (t) ≥ sup T ∈T E Pv,n [T (x v , y Nv )] − E Pv,Pn [f * (T (x v , y Nu ))]</formula><p>where T denotes any functions that map vertex and neighborhood features to a scalar, and the function T (•, •) works as a variational representation of t. We further use an activation function a : R → dom f * to constrain the function value; that is T (•, •) → a(T (•, •)). Therefore, we have since the a(T (•, •)) is also in T and its value is in dom f * , the optimal solution satisfies the equation. Suppose that the divergence function is f (x) = x log x, the conjugate divergence function is f * (t) = exp(t − 1) and the activation function is a(x) = x, we can obtain the f -representation of KL divergence; see Eq. ( <ref type="formula" target="#formula_21">4</ref>). Note that the form of activation function is not unique, and we aim to find a proper one that helps to derivation and computation.</p><p>Here, we consider another form of divergence based on f -representation; that is, GAN-like divergence, where we have specific form of divergence function f (x) = x log x − (x + 1) log(x + 1) and conjugate divergence function f * (t) = − log(1 − exp(t)) <ref type="bibr" target="#b37">[38]</ref>. We let the activation be a(•) = − log(1 + exp(•)). The GAN-like divergence is formulated as where σ(•) is the sigmoid function that maps a real value into the range of (0, 1). Eventually, the GAN-like divergence converts the f -divergence to a binary cross entropy, which is similar to the objective function to train the discriminator in GAN <ref type="bibr" target="#b21">[22]</ref>. where E w (•) is modeled by a Multi-layer perceptron (MLP), P w (•) is modeled by a R-hop graph convolution layer and S w (•, •) is also modeled by an MLP. In P w (•), A = A + I is the self-connected graph adjacency matrix and D is the degree matrix of A; W (r) ∈ R d×d is the trainable weight matrix associated with the rth hop of neighborhood. The neighborhood embedding function P w (•) aggregates neighborhood information with in a geodesic distance threshold R. Note that P w (•) separately use neighborhood features y Nu in form of connectivity information and vertex features.</p><formula xml:id="formula_25">D GAN (P v,n ||P v ⊗ P n ) ≥ sup T ∈T E Pv,</formula><p>In this way, the GAN-like-divergence-based mutual information between graph vertices and neighborhoods can be represented with the parameterized GAN-like divergence, which is a variational divergence and works as a lower bound of of the theorical GAN-like-divergence-based mutual information; that is,  </p><formula xml:id="formula_26">I (Ω) GAN (v, n) = D GAN (P v,n ||P v ⊗ P n ) ≥ I</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>leverage graph sampling theory and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.01804v1 [cs.LG] 5 Oct 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architectures of multiscale graph neural networks. Our architecture adopts intermediate fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Vertex infomax pooling (VIPool). We evaluate the importance of a vertex based on how much it can reflect its own neighborhood and how much it can discriminate from an arbitrary neighborhood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GXN architecture. We show an exemplar model with 3 scales and 4 feature-crossing layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graph classification accuracies (%) with neighbor-hops R from 1 to 5 on D&amp;D and IMDB-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of C values on different types of graph datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>D</head><label></label><figDesc>f (P v,n ||P v ⊗ P n ) ≥ sup T ∈T E Pv,n [a(T (x v , y Nv ))] − E Pv,Pn [f * (a(T (x v , y Nu )))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>n [a(T (x v , y Nv ))] − E Pv,Pn [f * (a(T (x v , y Nu )))] = sup T ∈T E Pv,n [− log(1 + exp(−T (x v , y Nv )))] + E Pv,Pn log(1 − exp(− log(1 + e T (xv,y Nu ) ))) = sup T ∈T E Pv,n log 1 1 + e −T (xv,y Nv ) + E Pv,Pn log(1 − 1 1 + e −T (xv,y Nu ) ) = sup T ∈T E Pv,n log σ (T (x v , y Nv )) + E Pv,Pn log (1 − σ (T (x v , y Nu )))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>To determine the form of the function T (•, •), we parameterized T (•, •) by trainable neural networks rather than design it manually. The parameterized function is denoted as T w (•, •), where w generally denotes the parameterization. In this work, T w (•, •) is constructed with three trainable functions: 1) A vertex embedding function E w (•); 2) A neighborhood embedding function P w (•); and 3) a vertex-neighborhood affinity function C w (•, •); which are formulated asT w (x v , y Nu ) = S w (E w (x v ), P w (y Nu )) = S w E w (x v ), 1 R R r=0 ν∈Nu ( D −1/2 A D −1/2 ) r ν,u W (r) E w (x ν ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>E</head><label></label><figDesc>Pv,n log σ (T w (x v , y Nv )) + E Pv,Pn log (1 − σ (T w (x v , y Nu ))) = max w 1 |Ω| v∈Ω log σ(T w (x v , y Nv )) + 1 |Ω| 2 (v,u)∈Ω log(1 − σ(T w (x v , y Nu )))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Vertex selection by using different pooling algorithms on three spatial mesh graph (better viewed on a color screen).</figDesc><graphic url="image-9.png" coords="19,284.74,545.59,83.43,83.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Vertex classification accuracies (%) of different methods, where 'full-sup.' and 'semi-sup.' denote the scenarios of full-supervised and semi-supervised vertex classification, respectively.</figDesc><table><row><cell>Dataset</cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell># Vertices (Classes)</cell><cell cols="2">2708 (7)</cell><cell cols="2">3327 (6)</cell><cell cols="2">19717 (3)</cell></row><row><cell>Supervision</cell><cell>full-sup.</cell><cell>semi-sup.</cell><cell>full-sup.</cell><cell>semi-sup.</cell><cell>full-sup.</cell><cell>semi-sup.</cell></row><row><cell>DeepWalk [39] ChebNet [11] GCN [30] GAT [47] FastGCN [7] ASGCN [28] Graph U-Net [20]</cell><cell>78.4 ± 1.7 86.4 ± 0.5 86.6 ± 0.4 87.8 ± 0.7 85.0 ± 0.8 87.4 ± 0.3 -</cell><cell>67.2 ± 2.0 81.2 ± 0.5 81.5 ± 0.5 83.0 ± 0.7 80.8 ± 1.0 -84.4</cell><cell>68.5 ± 1.8 78.9 ± 0.4 79.3 ± 0.5 80.2 ± 0.6 77.6 ± 0.8 79.6 ± 0.2 -</cell><cell>43.2 ± 1.6 69.8 ± 0.5 70.3 ± 0.5 73.5 ± 0.7 69.4 ± 0.8 -73.2</cell><cell>79.8 ± 1.1 88.7 ± 0.3 90.2 ± 0.3 90.6 ± 0.4 88.0 ± 0.6 90.6 ± 0.3 -</cell><cell>65.3 ± 1.1 74.4 ± 0.4 79.0 ± 0.3 79.0 ± 0.3 78.5 ± 0.7 -79.6</cell></row><row><cell>GXN GXN (noCross)</cell><cell>88.9 ± 0.4 87.3 ± 0.4</cell><cell>85.1 ± 0.6 83.2 ± 0.5</cell><cell>80.9 ± 0.4 79.5 ± 0.4</cell><cell>74.8 ± 0.4 73.7 ± 0.3</cell><cell>91.8 ± 0.3 91.1 ± 0.2</cell><cell>80.2 ± 0.3 79.6 ± 0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Graph</figDesc><table><row><cell># scales</cell><cell>Accuracy 1 2 3 4</cell><cell>feature-cross layers 1 2 3 73.20 74.10 73.80 76.00 76.50 76.20 76.70 77.20 77.20 76.80 77.30 77.10</cell><cell>Accuracy</cell><cell>0.75 0.80 0.85 0.90</cell><cell></cell><cell></cell><cell>D&amp;D IMDB-B</cell></row><row><cell></cell><cell>5</cell><cell>76.80 77.30 77.00</cell><cell></cell><cell>R=1</cell><cell>R=2</cell><cell>R=3</cell><cell>R=4</cell><cell>R=5</cell></row></table><note>classification accuracies (%) with various scales and feature-crossing layers on IMDB-B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Vertex classification accuracies and time costs per epoch with different PoolA(•) on Cora.</figDesc><table><row><cell>PoolA(•)</cell><cell cols="2">Accuracy (%) Time (s)</cell></row><row><cell>Edge-Remove</cell><cell>84.6</cell><cell>0.44</cell></row><row><cell>Kron-Reduce</cell><cell>85.3</cell><cell>8.36</cell></row><row><cell>Clus-Connect</cell><cell>85.1</cell><cell>1.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Graph classification accuracies and time costs per epoch with different PoolA(•) on IMDB-B.</figDesc><table><row><cell>PoolA(•)</cell><cell cols="2">Accuracy Time (s)</cell></row><row><cell>Edge-Remove</cell><cell>77.20</cell><cell>1.97</cell></row><row><cell>Kron-Reduce</cell><cell>77.50</cell><cell>13.44</cell></row><row><cell>Clus-Connect</cell><cell>77.60</cell><cell>3.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The code could be downloaded at https://github.com/limaosen0/GXN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The trainable parameters in Ew(•), Pw(•), and Sw(•, •) are not weight-shared.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">VIPool is trained through both L pool and L task , which makes graph pooling adapt to a specific task.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since we consider the dependency between vertices and neighborhoods within a specific vertex set, the possible outcomes of the joint distribution and the two marginal distributions are countable. We thus use the summation to aggregate all the possible cases. To maximize</p><p>, and S w (•, •), we can maximally approximate the mutual information between individual vertex and neighborhood for vertex selection in our VIPool. Note that the value of I (Ω) GAN (v, n) is not very close to the exact KL-divergence-based mutual information, but it has the consistency to I (Ω) (v, n) to reflect the pair of vertex-neighborhood with high or low mutual information, leading to effective vertex selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Detailed Information of Experimental Graph Datasets</head><p>Here we show more details about the graph datasets used in our experiments of both graph classification and vertex classification. We first show the six datasets for graph classification in Table <ref type="table">6</ref>. We see that, we show the numbers of graphs, graph classes, vertices, numbers of graphs in training/test datasets and feature dimensions of all the six datasets. Note that, three social network datasets, IMDB-B, IMDB-M and COLLAB do not provide specific vertex features, where the vertex dimension is denoted as 1 and the maximum vertex degrees are shown in addition. In our experiments, we use one-hot vectors to encode the vertex degrees in these three datasets as their vertex features which explicitly contains the structure information.</p><p>We then show the details of three citation network datasets used in the experiments of vertex classification in Table <ref type="table">7</ref>. We see that, we present the numbers of vertices, edges, vertex classes and feature dimensions of the three datasets, as well as we show the separations of training/validation/test sets, where '# Train Vertices (full-sup.)' denotes the number of training vertices for full-supervised vertex classification and '# Train Vertices (semi-sup.)' denotes the number of training vertices for semi-supervised vertex classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. More GXN Variants for Vertex Classification</head><p>Here we show more results of vertex classification of more variants of the proposed GXN associated with different pooling methods; that is, we test different pooling methods with the same GXN model framework, where the pooling methods include gPool <ref type="bibr" target="#b19">[20]</ref>, SAGPool <ref type="bibr" target="#b30">[31]</ref> and AttPool <ref type="bibr" target="#b26">[27]</ref>. The full-supervised and semi-supervised vertex classification accuracies of different algorithms on three  citation networks are shown in Table <ref type="table">8</ref>. We see that, comprared to the previous pooling methods, the proposed GXN which uses VIPool could provide higher average classification accuracies for both full-supervised and semi-supervised vertex classification. Different GXN variants with different pooling methods tend to consistently outperform most state-of-the-art models for vertex classification, reflecting the effectiveness of the proposed GXN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Different Ratios of Vertex Selection</head><p>Here we investigate the effects of the different ratios of vertex selection and preservation in the proposed VIPool operation. We conduct experiments on the tasks of graph classification. In the proposed GXN model, we vary the ratios of vertex selection in two coarsened scales of graphs from 0.9 to 0.1, where the ratio of scale 1 is larger than the ratio scale 2. We employ greedy algorithm to maximize C(Ω) or top-K method to maximize C + (Ω) for vertex selection. We note that, for any scale that preserve more than 70% vertices of the original graph, we do not use greedy algorithm on C(Ω) due to the much high time costs, where the greedy algorithm, specifically, spends more than 30 times of running time than top-K method. Additionally, we present the classification accuracy of a naive baseline; that is, we use the only the original scale for graph classification, which reflects the lowest bound of the model performance. Given the different ratios of vertex selection, we perform tasks of graph convolution on IMDB-B dataset, where the classification accuracies are presented in Table <ref type="table">9</ref>. We see that if we preserve more vertices in the coarsened scales of graphs, the entire model tends to obtain a higher graph classification accuracy, where we use not only greedy algorithm but also top-K method to optimize C(Ω) and C + (Ω), respectively, for vertex selection; in contrast, the  classification accuracies decrease with the removal of more and more vertices. The reason is that a much coarser graph cannot provides as much information as a coarsened graph which still has a large proportion of vertices. Another phenomenon is that, as the number of removed nodes increases, using greedy algorithm to select vertices tends to achieve increasingly higher classification accuracy than using top-K method, indicating that, greedy algorithm obtain more informative vertices when we select a few ones. Note that, when the ratio of scale 1 and ratio of scale 2 are respectively 0.8 and 0.5, we run the GXN model with the default configurations in our main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Use A Few Selected Vertices for Semi-supervised Vertex Classification Training</head><p>Here we consider active-sample-based semi-supervised classification, where we are allowed to select a few vertices and obtain their corresponding labels as supervision to train a classifier for vertex classification. In other words, we actively select training data in a semi-supervised classification task. Intuitively, since a graph structure is highly irregular, selecting a few informative vertices would potentially significantly improve the overall classification accuracy. Here we compare the proposed VIPool to random sampling. Note that for this task, we cannot compare with other graph pooling methods. The reason is that previous pooling pooling methods need a subsequent task to provide an explicit supervision; however, the vertex selection here should be blind to the final classification labels. The proposed VIPool is rooted in mutual information neural estimation and can be trained in either an unsupervised or supervised setting.</p><p>Specifically, given a graph, such as a citation network, Cora or Citeseer, we aim to show the classification accuracy as a function of the number of selected vertices. For example, there are 7 classes in Cora, we can select 7, 14, 21, 28 and 35 vertices (1, 2, 3, 4 and 5 times of 7) and use their ground-truth labels as supervision for semi-supervised vertex classification. As for Citeseer, there are 6 classes and we can select 6, 12, 18, 24 and 30 vertices. During evaluation, we test the performances on all the unselected vertices. We compare two method for vertex selection and classification: 1) the proposed VIPool method, where we use greedy algorithm to optimize C(Ω) for vertex selection; and 2) Random Sampling, where we randomly select each vertex with the same probability on the whole graph. We conduct semi-supervised vertex classification on the datasets of Cora and Citeseer. Figure <ref type="figure">6</ref> shows the the classification accuracies varying with the numbers of selected vertices for two vertex selection methods. We see that, when we select only a few vertices, such as fewer than 3 times of the number of vertex classes (i.e. 21 for Cora and 18 for Citeseer), the proposed VIPool method could select much more informative vertices than randomly sampling the same number of vertices, leading to over 10% higher vertex classification accuracies. If we select more vertices by using the two vertex selection methods, the classification results corresponding to the two methods become closer to each other, indicating that a large number of selected vertices tend to potentially provide sufficient information to represent the rich patterns of the graphs and we could obtain more similar classification results than only selecting a few vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F. Illustration of Vertex Selection</head><p>To show the pooling effects of different pooling algorithms, we conduct a toy experiments to reconstruct three spatial mesh graphs with an encoder-decoder model. The encoder employs different pooling methods to squeeze the original graph into a few vertices (10 vertices) and the decoder attempt to reconstruct the original graphs based on the pooled vertex features and graph structures.</p><p>To train the encoder-decoder model, we use an L2-norm loss to measure the distances between the vertex coordinates of reconstructed graphs and ground-truth graphs.</p><p>The three mesh graphs have vertex features as the 2D Euclidean coordinates and the specific vertex distributions are that 1) 88 vertices uniformly distribute in a circle region; 2) 503 vertices distribute in a hollow square region, where the vertices densely distribute around the center and sparsely distribute near the margins; 3) 310 vertices distribute in a circle, where the vertices densely distribute near the center and sparsely distribute around. The specific topologies are shown in the first row of Figure <ref type="figure">7</ref>.</p><p>We compare the proposed VIPool operation with several baseline methods: random sampling, gPool <ref type="bibr" target="#b19">[20]</ref>, SAGPool <ref type="bibr" target="#b30">[31]</ref> and AttPool <ref type="bibr" target="#b26">[27]</ref>. The selected vertices are colored blue and illustrated in Figure <ref type="figure">7</ref>. We see that, VIPool can abstract the original graphs more properly, where the preserved vertices distribute dispersely in both dense and sparse regions to cover the overall graphs. As for the baselines, we see that, 1) random sampling tends to select more vertices in dense regions, since each vertex is sampled with equal probability and the dense regions include more vertices and chances for vertex selection; 2) gPool and SAGpool calculate the importance weight for each vertices mainly based on vertex information itself without topological constraints, thus the selected vertices tends to distributed concentrated in local regions. 3) AttPool considers to model the local attentions and select more representative vertices, thus it can abstract graph structures to some extent, but the vertex distributions still slightly collapse the dense region.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2644615</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</title>
				<imprint>
			<date type="published" when="2017-12">December 2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Processing directed acyclic graphs with recursive neural networks</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Cristina</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.963781</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks (TNN)</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1464" to="1470" />
			<date type="published" when="2001-02">February 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bti1007</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005-03">March 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Sampling theory</title>
		<author>
			<persName><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Kovačević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="6510" to="6523" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential information maximization: When is greedy near-optimal?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Hamed</forename><surname>Yuxin Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory, COLT 2015</title>
				<meeting>The 28th Conference on Learning Theory, COLT 2015<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">July 3-6, 2015. 2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="338" to="363" />
		</imprint>
		<respStmt>
			<orgName>JMLR.org</orgName>
		</respStmt>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><surname>Kulis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2007.1115</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</title>
		<idno type="ISSN">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0022-2836(03)00628-4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology (JMB)</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kron reduction of graphs with applications to electrical networks</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Dörfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I Regul. Pap</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName><forename type="first">Aasa</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marleen</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
				<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning topological representation for networks via hierarchical sampling</title>
		<author>
			<persName><forename type="first">Guoji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengbin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
		<idno>CoRR, abs/1902.06684</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2014-12">December 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fundamentals of convex analysis</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Hiriart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Urruty</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Lemarechal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attpool: Towards hierarchical feature representation in graph convolutional networks via attention mechanism</title>
		<author>
			<persName><forename type="first">Jingjia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<publisher>Workshop</publisher>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mile: A multi-level framework for scalable graph embedding</title>
		<author>
			<persName><forename type="first">Jiongqian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Gurukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<idno>CoRR, abs/1802.09612</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
				<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkovl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno>CoRR, abs/1403.6652</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</title>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention (MICCAI)</title>
				<imprint>
			<date type="published" when="2015-10">October 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Advanced coarsening schemes for graph partitioning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal of Experimental Algorithmics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">X-cnn: Cross-modal convolutional neural networks for sparse datasets</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence (SSCI)</title>
				<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A structural smoothing framework for robust graph comparison</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
				<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structpool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
