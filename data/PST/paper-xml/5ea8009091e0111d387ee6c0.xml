<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-25">25 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
							<email>zichaoy@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-25">25 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.12239v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents MixText, a semisupervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-ofthe-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https: //github.com/GT-SALT/MixText.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the era of deep learning, research has achieved extremely good performance in most supervised learning settings <ref type="bibr" target="#b20">(LeCun et al., 2015;</ref><ref type="bibr" target="#b42">Yang et al., 2016)</ref>. However, when there is only limited labeled data, supervised deep learning models often suffer from over-fitting <ref type="bibr" target="#b37">(Xie et al., 2019)</ref>. This strong dependence on labeled data largely prevents neural network models from being applied to new settings or real-world situations due to the need of large amount of time, money, and expertise to obtain enough labeled data. As a result, semi-supervised learning has received much attention to utilize both labeled and unlabeled data for different learning tasks, as unlabeled data is always much easier and cheaper to collect <ref type="bibr" target="#b4">(Chawla and Karakoulas, 2011)</ref>.</p><p>This work takes a closer look at semi-supervised text classification, one of the most fundamental tasks in language technology communities. Prior research on semi-supervised text classification can Figure <ref type="figure">1</ref>: TMix takes in two text samples x and x with labels y and y , mixes their hidden states h and h at layer m with weight λ into h, and then continues forward passing to predict the mixed labels ỹ.</p><p>be categorized into several classes: (1) utilizing variational auto encoders (VAEs) to reconstruct the sentences and predicting sentence labels with latent variables learned from reconstruction such as <ref type="bibr" target="#b7">(Chen et al., 2018;</ref><ref type="bibr" target="#b41">Yang et al., 2017;</ref><ref type="bibr" target="#b13">Gururangan et al., 2019)</ref>; (2) encouraging models to output confident predictions on unlabeled data for selftraining like <ref type="bibr" target="#b21">(Lee, 2013;</ref><ref type="bibr" target="#b12">Grandvalet and Bengio, 2004;</ref><ref type="bibr" target="#b25">Meng et al., 2018)</ref>; (3) performing consistency training after adding adversarial noise <ref type="bibr" target="#b27">(Miyato et al., 2019</ref><ref type="bibr" target="#b26">(Miyato et al., , 2017) )</ref> or data augmentations <ref type="bibr" target="#b37">(Xie et al., 2019)</ref>; (4) large scale pretraining with unlabeld data, then finetuning with labeled data <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. Despite the huge success of those models, most prior work utilized labeled and unlabeled data separately in a way that no supervision can transit from labeled to unlabeled data or from unlabeled to labeled data. As a result, most semi-supervised models can easily still overfit on the very limited labeled data, despite unlabeled data is abundant.</p><p>To overcome the limitations, in this work, we introduce a new data augmentation method, called TMix (Section 3), inspired by the recent success of Mixup <ref type="bibr" target="#b13">(Gururangan et al., 2019;</ref><ref type="bibr" target="#b1">Berthelot et al., 2019)</ref> on image classifications. TMix, as shown in Figure <ref type="figure">1</ref>, takes in two text instances, and interpolates them in their corresponding hidden space. Since the combination is continuous, TMix has the potential to create infinite mount of new augmented data samples, thus can drastically avoid overfitting. Based on TMix, we then introduce a new semi-supervised learning method for text classification called MixText (Section 4) to explicitly model the relationships between labeled and unlabeled samples, thus overcoming the limitations of previous semi-supervised models stated above. In a nutshell, MixText first guesses low-entropy labels for unlabeled data, then uses TMix to interpolate the label and unlabeled data. MixText can facilitate mining implicit relations between sentences by encouraging models to behave linearly in-between training examples, and utilize information from unlabeled sentences while learning on labeled sentences. In the meanwhile, MixText exploits several semi-supervised learning techniques to further utilize unlabeled data including selftarget-prediction <ref type="bibr" target="#b17">(Laine and Aila, 2016)</ref>, entropy minimization <ref type="bibr" target="#b12">(Grandvalet and Bengio, 2004)</ref>, and consistency regularization <ref type="bibr" target="#b1">(Berthelot et al., 2019;</ref><ref type="bibr" target="#b37">Xie et al., 2019)</ref> after back translations.</p><p>To demonstrate the effectiveness of our method, we conducted experiments (Section 5) on four benchmark text classification datasets and compared our method with previous state-of-the-art semi-supervised method, including those built upon models pre-trained with large amount of unlabeled data, in terms of accuracy on test sets. We further performed ablation studies to demonstrate each component's influence on models' final performance. Results show that our MixText method significantly outperforms baselines especially when the given labeled training data is extremely limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-training and Fine-tuning Framework</head><p>The pre-training and fine-tuning framework has achieved huge success on NLP applications in recent years, and has been applied to a variety of NLP tasks <ref type="bibr" target="#b31">(Radford et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019;</ref><ref type="bibr" target="#b0">Akbik et al., 2019)</ref>. <ref type="bibr" target="#b14">Howard and Ruder (2018)</ref> proposed to pre-train a language model on a large general-domain corpus and fine-tune it on the target task using some novel techniques like discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing. In this manner, such pretrained models show excellent performance even with small amounts of labeled data. Pre-training methods are often designed with different objectives such as language modeling <ref type="bibr" target="#b30">(Peters et al., 2018;</ref><ref type="bibr" target="#b14">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b40">Yang et al., 2019b)</ref> and masked language modeling <ref type="bibr" target="#b9">(Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Lample and Conneau, 2019)</ref>. Their performances are also improved with training larger models on more data <ref type="bibr" target="#b40">(Yang et al., 2019b;</ref><ref type="bibr" target="#b22">Liu et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Learning on Text Data</head><p>Semi-supervised learning has received much attention in the NLP community <ref type="bibr" target="#b13">(Gururangan et al., 2019;</ref><ref type="bibr" target="#b8">Clark et al., 2018;</ref><ref type="bibr" target="#b39">Yang et al., 2015)</ref>, as unlabeled data is often plentiful compared to labeled data. For instance, <ref type="bibr" target="#b13">Gururangan et al. (2019)</ref>; <ref type="bibr" target="#b7">Chen et al. (2018)</ref>; <ref type="bibr" target="#b41">Yang et al. (2017)</ref> leveraged variational auto encoders (VAEs) in a form of sequenceto-sequence modeling on text classification and sequential labeling. <ref type="bibr" target="#b26">Miyato et al. (2017)</ref> utilized adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings. <ref type="bibr" target="#b38">Yang et al. (2019a)</ref> took advantage of hierarchy structures to utilize supervision from higher level labels to lower level labels. <ref type="bibr" target="#b37">Xie et al. (2019)</ref> exploited consistency regularization on unlabeled data after back translations and tf-idf word replacements. <ref type="bibr" target="#b8">Clark et al. (2018)</ref> proposed crossveiw training for unlabeled data, where they used an auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) and match the predictions of the full model seeing the whole input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interpolation-based Regularizers</head><p>Interpolation-based regularizers (e.g., Mixup) have been recently proposed for supervised learning <ref type="bibr" target="#b44">(Zhang et al., 2017;</ref><ref type="bibr" target="#b34">Verma et al., 2019a)</ref> and semisupervised learning <ref type="bibr" target="#b1">(Berthelot et al., 2019;</ref><ref type="bibr" target="#b35">Verma et al., 2019b)</ref> for image-format data by overlaying two input images and combining image labels as virtual training data and have achieved state-ofthe-art performances across a variety of tasks like image classification and network architectures. Different variants of mixing methods have also been designed such as performing interpolations in the input space <ref type="bibr" target="#b44">(Zhang et al., 2017)</ref>, combining interpolations and cutoff <ref type="bibr" target="#b43">(Yun et al., 2019)</ref>, and doing interpolations in the hidden space representations <ref type="bibr">(Verma et al., 2019a,c)</ref>. However, such interpolation techniques have not been explored in the NLP field because most input space in text is discrete, i.e., one-hot vectors instead of continues RGB values in images, and text is generally more complex in structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data Augmentations for Text</head><p>When labeled data is limited, data augmentation has been a useful technique to increase the amount of training data. For instance, in computer vision, images are shifted, zoomed in/out, rotated, flipped, distorted, or shaded with a hue <ref type="bibr" target="#b29">(Perez and Wang, 2017)</ref> for training data augmentation. But it is relatively challenging to augment text data because of its complex syntactic and semantic structures. Recently, <ref type="bibr" target="#b37">Wei and Zou (2019)</ref> utilized synonym replacement, random insertion, random swap and random deletion for text data augmentation. Similarly, <ref type="bibr" target="#b16">Kumar et al. (2019)</ref> proposed a new paraphrasing formulation in terms of monotone submodular function maximization to obtain highly diverse paraphrases, and Xie et al. ( <ref type="formula">2019</ref>) and <ref type="bibr" target="#b6">Chen et al. (2020)</ref> applied back translations <ref type="bibr" target="#b32">(Sennrich et al., 2015)</ref> and word replacement to generate paraphrases on unlabeled data for consistency training. Other work which also investigates noise and its incorporation into semi-supervised named entity classification <ref type="bibr" target="#b18">(Lakshmi Narayan et al., 2019;</ref><ref type="bibr" target="#b28">Nagesh and Surdeanu, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TMix</head><p>In this section, we extend Mixup-a data augmentation method originally proposed by <ref type="bibr" target="#b44">(Zhang et al., 2017)</ref> for images-to text modeling. The main idea of Mixup is very simple: given two labeled data points (x i , y i ) and (x j , y j ), where x can be an image and y is the one-hot representation of the label, the algorithm creates virtual training samples by linear interpolations:</p><formula xml:id="formula_0">x = mix(x i , x j ) =λx i + (1 − λ)x j , (1) ỹ = mix(y i , y j ) =λy i + (1 − λ)y j , (2)</formula><p>where λ ∈ [0, 1]. The new virtual training samples are used to train a neural network model. Mixup can be interpreted in different ways. On one hand, Mixup can be viewed a data augmentation approach which creates new data samples based on the original training set. On the other hand, it enforces a regularization on the model to behave linearly among the training data. Mixup was demonstrated to work well on continuous image data <ref type="bibr" target="#b44">(Zhang et al., 2017)</ref>. However, extending it to text seems challenging since it is infeasible to compute the interpolation of discrete tokens.</p><p>To this end, we propose a novel method to overcome this challenge -interpolation in textual hidden space. Given a sentence, we often use a multilayer model like BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> to encode the sentences to get the semantic representations, based on which final predictions are made. Some prior work <ref type="bibr" target="#b2">(Bowman et al., 2016)</ref> has shown that decoding from an interpolation of two hidden vectors generates a new sentence with mixed meaning of two original sentences. Motivated by this, we propose to apply interpolations within hidden space as a data augment method for text. For an encoder with L layers, we choose to mixup the hidden representation at the m-th layer, m ∈ [0, L].</p><p>As demonstrated in Figure <ref type="figure">1</ref>, we first compute the hidden representations of two text samples separately in the bottom layers. Then we mix up the hidden representations at layer m, and feed the interpolated hidden representations to the upper layers. Mathematically, denote the l-th layer in the encoder network as g l (.; θ), hence the hidden representation of the l-th layer can be computed as h l = g l (h l−1 ; θ). For two text samples x i and x j , define the 0-th layer as the embedding layer, i.e., h i 0 = W E x i , h j 0 = W E x j , then the hidden representations of the two samples from the lower layers are:</p><formula xml:id="formula_1">h i l =g l (h i l−1 ; θ), l ∈ [1, m], h j l =g l (h j l−1 ; θ), l ∈ [1, m].</formula><p>The mixup at the m-th layer and continuing forward passing to upper layers are defined as:</p><formula xml:id="formula_2">hm = λh i m + (1 − λ)h j m , hl = g l ( hl−1 ; θ), l ∈ [m + 1, L].</formula><p>We call the above method TMix and define the new mixup operation as the whole process to get hL : TMix(x i , x j ; g(.; θ), λ, m) = hL .</p><p>By using an encoder model g(.; θ), TMix interpolates textual semantic hidden representations as a type of data augmentation. In contrast with Mixup defined in the data space in Equation 1, TMix depends on an encoder function, hence defines a much broader scope for computing interpolations. For ease of notation, we drop the explicit dependence on g(.; θ), λ and m in notations and denote it simply as TMix(x i , x j ) in the following sections.</p><p>In our experiments, we sample the mix parameter λ from a Beta distribution for every batch to perform the interpolation :</p><formula xml:id="formula_3">λ ∼ Beta(α, α), λ = max(λ, 1 − λ),</formula><p>in which α is the hyper-parameter to control the distribution of λ. In TMix, we mix the labels in the same way as Equation 2 and then use the pairs ( hL , ỹ) as inputs for downstream applications.</p><p>Instead of performing mixup at random input layers like <ref type="bibr" target="#b34">Verma et al. (2019a)</ref>, choosing which layer of the hidden representations to mixup is an interesting question to investigate. In our experiments, we use 12-layer BERT-base <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> as our encoder model. Recent work <ref type="bibr" target="#b15">(Jawahar et al., 2019)</ref> has studied what BERT learned at different layers. Specifically, the authors found {3,4,5,6,7,9,12} layers have the most representation power in BERT and each layer captures different types of information ranging from surface, syntactic to semantic level representation of text. For instance, the 9-th layer has predictive power in semantic tasks like checking random swapping of coordinated clausal conjuncts, while the 3-rd layer performs best in surface tasks like predicting sentence length.</p><p>Building on those findings, we choose the layers that contain both syntactic and semantic information as our mixing layers, namely M = {7, 9, 12}. For every batch, we randomly sample m, the layer to mixup representations, from the set M computing the interpolation. We also performed ablation study in Section 5.5 to show how TMix's performance changes with different choice of mix layer sets.</p><p>Text classification Note that TMix provides a general approach to augment text data, hence can be applied to any downstream tasks. In this paper, we focus on text classification and leave other applications as potential future work. In text classification, we minimize the KL-divergence between the mixed labels and the probability from the classifier as the supervision loss:</p><p>L TMix = KL(mix(y i , y j )||p(TMix(x i , x j ); φ) where p(.; φ) is a classifier on top of the encoder model. In our experiments, we implement the classifier as a two-layer MLP, which takes the mixed representation TMix(x i , x j ) as input and returns a probability vector. We jointly optimize over the encoder parameters θ and the classifier parameters φ to train the whole model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-supervised MixText</head><p>In this section, we demonstrate how to utilize the TMix to help semi-supervised learning. Given a limited labeled text set X l = {x l 1 , ..., x l n }, with their labels Y l = {y l 1 , ..., y l n } and a large unlabeled set X u = {x u 1 , ..., x u m }, where n and m are the number of data points in each set. y l i ∈ {0, 1} C is a one-hot vector and C is the number of classes. Our goal is to learn a classifier that efficiently utilizes both labeled data and unlabeled data.</p><p>We propose a new text semi-supervised learning framework called MixText<ref type="foot" target="#foot_0">1</ref> . The core idea behind our framework is to leverage TMix both on labeled and unlabeled data for semi-supervised learning. To fulfill this goal, we come up a label guessing method to generate labels for the unlabeled data in the training process. With the guessed labels, we can treat the unlabeled data as additional labeled data and perform TMix for training. Moreover, we combine TMix with additional data augmentation techniques to generate large amount of augmented data, which is a key component that makes our algorithm work well in setting with extremely limited supervision. Finally, we introduce an entropy minimization loss that encourages the model to assign sharp probabilities on unlabeled data samples, which further helps to boost performance when the number of classes C is large. The overall architecture is shown in Figure <ref type="figure" target="#fig_0">2</ref>. We will explain each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Augmentation</head><p>Back translations <ref type="bibr" target="#b11">(Edunov et al., 2018)</ref> is a common data augmentation technique and can generate diverse paraphrases while preserving the semantics of the original sentences. We utilize back translations to paraphrase the unlabeled data. For each x u i in the unlabeled text set X u , we generate K augmentations x a i,k = augment k (x u i ), k ∈ [1, K] by back translations with different intermediate languages. For example, we can translate original sentences from English to German and then translate them back to get the paraphrases. In the augmented text generation, we employ random sampling with a tunable temperature instead of beam search to ensure the diversity. The augmentations are then used for generating labels for the unlabeled data, which we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Label Guessing</head><p>For an unlabeled data sample x u i and its K augmentations x a i,k , we generate the label for them using weighted average of the predicted results from the current model:</p><formula xml:id="formula_4">y u i = 1 w ori + k w k (w ori p(x u i ) + K k=1 w k p(x a i,k ))).</formula><p>Note that y u i is a probability vector. We expect the model to predict consistent labels for different augmentations. Hence, to enforce the constraint, we use the weighted average of all predictions, rather than the prediction of any single data sample, as the generated label. Moreover, by explicitly introducing the weight w ori and w k , we can control the contributions of different quality of augmentations to the generated labels. Our label guessing method improves over <ref type="bibr" target="#b33">(Tarvainen and Valpola, 2017)</ref> which utilizes teacher and student models to predict labels for unlabeled data, and UDA (Xie et al., 2019) that just uses p(x u i ) as generated labels.</p><p>To avoid the weighted average being too uniform, we utilize a sharpening function over predicted labels. Given a temperature hyper-parameter T :</p><formula xml:id="formula_5">Sharpen(y u i , T ) = (y u i ) 1 T ||(y u i ) 1 T || 1 ,</formula><p>where ||.|| 1 is l 1 -norm of the vector. When T → 0, the generated label becomes a one-hot vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TMix on Labeled and Unlabeled Data</head><p>After getting the labels for unlabeled data, we merge the labeled text X l , unlabeled text X u and unlabeled augmentation text X a = {x a i,k } together to form a super set X = X l ∪ X u ∪ X a . The corresponding labels are Y = Y l ∪ Y u ∪ Y a , where Y a = {y a i,k } and we define y a i,k = y u i , i.e., the all augmented samples share the same generated label as the original unlabeled sample.</p><p>In training, we randomly sample two data points x, x ∈ X, then we compute TMix(x, x ), mix(y, y ) and use the KL-divergence as the loss:</p><formula xml:id="formula_6">L TMix = E x,x ∈X KL(mix(y, y )||p(TMix(x, x )).</formula><p>Since x, x are randomly sampled from X, we interpolate text from many different categories: mixup among among labeled data, mixup of labeled and unlabeled data and mixup of unlabeled data. Based on the categories of the samples, the loss can be divided into two types: Supervised loss When x ∈ X l , the majority information we are actually using is from the labeled data, hence training the model with supervised loss.</p><p>Consistency loss When the samples are from unlabeled or augmentation set, i.e., x ∈ X u ∪ X a , most information coming from unlabeled data, the KL-divergence is a type of consistency loss, constraining augmented samples to have the same labels with the original data sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Entropy Minimization</head><p>To encourage the model to produce confident labels on unlabeled data, we propose to minimize the entropy of prediction probability on unlabeled data as a self-training loss:</p><formula xml:id="formula_7">L margin = E x∈Xu max(0, γ − ||y u || 2 2 ),</formula><p>where γ is the margin hyper-parameter. We minimize the entropy of the probability vector if it is larger than γ.</p><p>Combining the two losses, we get the overall objective function of MixText:</p><formula xml:id="formula_8">L MixText = L TMix + γ m L margin .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Pre-processing</head><p>We performed experiment with four English text classification benchmark datasets: AG News <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref>, BPpedia <ref type="bibr" target="#b24">(Mendes et al., 2012)</ref>, Yahoo! Answers <ref type="bibr" target="#b3">(Chang et al., 2008)</ref> and IMDB <ref type="bibr" target="#b23">(Maas et al., 2011)</ref>. We used the original test set as our test set and randomly sampled from the training set to form the training unlabeled set and development set. The dataset statistics and split information are presented in Table <ref type="table" target="#tab_0">1</ref>.</p><p>For unlabeled data, we selected German and Russian as intermediate languages for back translations using FairSeq<ref type="foot" target="#foot_1">2</ref> , and the random sampling temperature was 0.9. Here is an example, for a news from AG News dataset: "Oil prices rallied to a record high above $55 a barrel on Friday on rising fears of a winter fuel supply crunch and robust economic growth in China, the world's number two user", the augment texts through German and Russian are: "Oil prices surged to a record high above $55 a barrel on Friday on growing fears of a winter slump and robust economic growth in world No.2 China" and "Oil prices soared to record highs above $55 per barrel on Friday amid growing fears over a winter reduction in U.S. oil inventories and robust economic growth in China, the world's second-biggest oil consumer".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>To test the effectiveness of our method, we compared it with several recent models:</p><p>• VAMPIRE <ref type="bibr" target="#b13">(Gururangan et al., 2019)</ref>: VAriational Methods for Pretraining In Resourcelimited Environments(VAMPIRE) pretrained a unigram document model as a variational autoencoder on in-domain, unlabeled data and used its internal states as features in a downstream classifier.</p><p>• BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>: We used the pretrained BERT-based-uncased model<ref type="foot" target="#foot_2">3</ref> and finetuned it for the classification. In details, we used average pooling over the output of BERT encoder and the same two-layer MLP as used in MixText to predict the labels.</p><p>• UDA (Xie et al., 2019): Since we do not have access to TPU and need to use smaller amount of unlabeled data, we implemented Unsupervised Data Augmentation(UDA) using pytorch by ourselves. Specifically, we used the same BERT-based-uncased model, unlabeled augment data and batch size as our MixText, used original unlabeled data to predict the labels with the same softmax sharpen temperature as our MixText and computed consistency loss between augmented unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Settings</head><p>We used BERT-based-uncased tokenizer to tokenize the text, bert-based-uncased model as our text encoder, and used average pooling over the output of the encoder, a two-layer MLP with a 128 hidden size and tanh as its activation function to predict the labels. The max sentence length is set as 256. We remained the first 256 tokens for sentences that exceed the limit. The learning rate is 1e-5 for BERT encoder, 1e-3 for MLP. For α in the beta distribution, generally, when labeled data is fewer than 100 per class, α is set as 2 or 16, as larger α is more likely to generate λ around 0.5, thus creating "newer" data as data augmentations; when labeled data is more than 200 per class, α is set to 0.2 or 0.4, as smaller α is more likely to generate λ around 0.1, thus creating "similar" data as adding noise regularization.</p><p>For TMix, we only utilize the labeled dataset as the settings in Bert baseline, and set the batch size as 8. In MixText, we utilize both labeled data and unlabeled data for training using the same settings as in UDA. We set K = 2, i.e., for each unlabeled data we perform two augmentations, specifically German and Russian. The batch size is 4 for labeled data and 8 for unlabeled data. 0.5 is used as a starting point to tune temperature T . In our experiments, we set 0.3 for AG News, 0.5 for DBpedia and Yahoo! Answer, and 1 for IMDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We evaluated our baselines and proposed methods using accuracy with 5000 unlabeled data and with different amount of labeled data per class ranging from 10 to 10000 (5000 for IMDB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Varying the Number of Labeled Data</head><p>The results on different text classification datasets are shown in Table <ref type="table" target="#tab_1">2</ref> and Figure <ref type="figure" target="#fig_1">3</ref>. All transformer based models (BERT, TMix, UDA and MixText) showed better performance compared to VAMPIRE since larger models were adopted. TMix outperformed BERT, especially when labeled data was limited like 10 per class. For instance, model accuracy improved from 69.5% to 74.1% on AG News with 10 labeled data, demonstrating the effective-ness of TMix. When unlabeled data was introduced in UDA, it outperformed TMix such as from 58.6% to 63.2% on Yahoo! with 10 labeled data, because more data was used and consistency regularization loss was added. Our proposed MixText consistently demonstrated the best performances when compared to different baseline models across four datasets, as MixText not only incorporated unlabeled data and utilized implicit relations between both labeled data and unlabeled data via TMix, but also had better label guessing on unlabeled data through weighted average among augmented and original sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Varying the Number of Unlabeled Data</head><p>We also conducted experiments to test our model performances with 10 labeled data and different amount of unlabeled data (from 0 to 10000) on AG News and Yahoo! Answer, shown in Figure <ref type="figure" target="#fig_2">4</ref>. With more unlabeled data, the accuracy became much higher on both AG News and Yahoo! Answer, which further validated the effectiveness of the usage of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Loss on Development Set</head><p>To explore whether our methods can avoid overfitting when given limited labeled data, we plotted  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>We performed ablation studies to show the effectiveness of each component in MixText.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Different Mix Layer Set in TMix</head><p>We explored different mixup layer set M for TMix and the results are shown in Table <ref type="table" target="#tab_2">3</ref>. Based on <ref type="bibr" target="#b15">(Jawahar et al., 2019)</ref>, the {3,4,5,6,7,9,12} are the most informative layers in BERT based model and each of them captures different types of informa- tion (e.g., surface, syntactic, or semantic). We chose to mixup using different subsets of those layers to see which subsets gave the optimal performance. When no mixup is performed, our model accuracy was 69.5%. If we just mixup at the input and lower layers ({0, 1, 2}), there seemed no performance increase. When doing mixup using different layer sets (e.g., {3,4}, or {6,7,9}), we found large differences in terms of model performances: {3,4} that mainly contains surface information like sentence length does not help text classification a lot, thus showing weaker performance. The 6th layer captures depth of the syntactic tree which also does not help much in classifications. Our model achieved the best performance at {7, 9, 12}; this layer subset contains most of syntactic and semantic information such as the sequence of top level constituents in the syntax tree, the object number in main clause, sensitivity to word order, and the sensitivity to random replacement of a noun/verb. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Remove Different Parts from MixText</head><p>We also measured the performance of MixText by stripping each component each time and displayed the results in Table <ref type="table" target="#tab_3">4</ref>. We observed the performance drops after removing each part, suggesting that all components in MixText contribute to the final performance. The model performance decreased most significantly after removing unlabeled data which is as expected. Comparing to weighted average prediction for unlabeled data, the decrease from removing TMix was larger, indicating that TMix has the largest impact other than unlabeled data, which also proved the effectiveness of our proposed Text Mixup, an interpolation-based regularization and augmentation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To alleviate the dependencies of supervised models on labeled data, this work presented a simple but effective semi-supervised learning method, Mix-Text, for text classification, in which we also introduced TMix, an interpolation-based augmentation and regularization technique. Through experiments on four benchmark text classification datasets, we demonstrated the effectiveness of our proposed TMix technique and the Mixup model, which have better testing accuracy and more stable loss trend, compared with current pre-training and fine-tuning models and other state-of-the-art semi-supervised learning methods. For future direction, we plan to explore the effectiveness of MixText in other NLP tasks such as sequential labeling tasks and other real-world scenarios with limited labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall Architecture of MixText. MixText takes in labeled data and unlabeled data, conducts augmentations and predicts labels for unlabeled data, performs TMix over labeled and unlabeled data, and computes supervised loss, consistency loss and entropy minimization term.</figDesc><graphic url="image-2.png" coords="5,72.00,62.81,453.54,167.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance (test accuracy (%)) on AG News, DBpedia, Yahoo! Answer and IMDB with 5000 unlabeled data and varying number of labeled data per class for each model.</figDesc><graphic url="image-3.png" coords="8,72.00,62.81,458.37,136.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance (test accuracy (%)) on AG News (y axis on the right) and Yahoo! Answer (y axis on the left) with 10 labeled data and varying number of unlabeled data per class for MixText.</figDesc><graphic url="image-4.png" coords="8,72.00,253.40,218.26,138.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Loss on development set on IMDB and Yahoo! Answer in each epoch while training with 200 labeled data and 5000 unlabeled data per class.</figDesc><graphic url="image-5.png" coords="8,307.28,253.40,218.26,152.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics and dataset split. The number of unlabeled data, dev data and test data in the table means the number of data per class.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">Label Type</cell><cell cols="3">Classes Unlabeled Dev</cell><cell>Test</cell></row><row><cell></cell><cell>AG News</cell><cell cols="2">News Topic</cell><cell>4</cell><cell>5000</cell><cell cols="2">2000 1900</cell></row><row><cell></cell><cell>DBpedia</cell><cell cols="2">Wikipeida Topic</cell><cell>14</cell><cell>5000</cell><cell cols="2">2000 5000</cell></row><row><cell cols="2">Yahoo! Answer</cell><cell></cell><cell>QA Topic</cell><cell>10</cell><cell>5000</cell><cell cols="2">5000 6000</cell></row><row><cell></cell><cell>IMDB</cell><cell cols="2">Review Sentiment</cell><cell>2</cell><cell>5000</cell><cell cols="2">2000 12500</cell></row><row><cell>Datset</cell><cell>Model</cell><cell>10</cell><cell cols="2">200 2500 Dataset</cell><cell>Model</cell><cell cols="2">10</cell><cell>200 2500</cell></row><row><cell></cell><cell>VAMPIRE</cell><cell>-</cell><cell>83.9 86.2</cell><cell></cell><cell cols="2">VAMPIRE</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AG News</cell><cell>BERT TMix*</cell><cell cols="2">69.5 87.5 90.8 74.1 88.1 91.0</cell><cell>DBpedia</cell><cell>BERT TMix*</cell><cell cols="2">95.2 98.5 99.0 96.8 98.7 99.0</cell></row><row><cell></cell><cell>UDA</cell><cell cols="2">84.4 88.3 91.2</cell><cell></cell><cell>UDA</cell><cell cols="2">97.8 98.8 99.1</cell></row><row><cell></cell><cell cols="3">MixText* 88.4 89.2 91.5</cell><cell></cell><cell cols="3">MixText* 98.5 98.9 99.2</cell></row><row><cell></cell><cell>VAMPIRE</cell><cell>-</cell><cell>59.9 70.2</cell><cell></cell><cell cols="2">VAMPIRE</cell><cell>-</cell><cell>82.2 85.8</cell></row><row><cell>Yahoo!</cell><cell>BERT TMix*</cell><cell cols="2">56.2 69.3 73.2 58.6 69.8 73.5</cell><cell>IMDB</cell><cell>BERT TMix*</cell><cell cols="2">67.5 86.9 89.8 69.3 87.4 90.3</cell></row><row><cell></cell><cell>UDA</cell><cell cols="2">63.2 70.2 73.6</cell><cell></cell><cell>UDA</cell><cell cols="2">78.2 89.1 90.8</cell></row><row><cell></cell><cell cols="3">MixText* 67.6 71.3 74.1</cell><cell></cell><cell cols="3">MixText* 78.7 89.4 91.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance (test accuracy(%)) comparison with baselines. The results are averaged after three runs to show the significance<ref type="bibr" target="#b10">(Dror et al., 2018)</ref>, each run takes around 5 hours. Models are trained with 10, 200, 2500 labeled data per class. VAMPIRE, Bert, and TMix do not use unlabeled data during training while UDA and MixText utilize unlabeled data. * means our models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance (test accuracy (%)) on AG News with 10 labeled data per class with different mixup layers set for TMix. ∅ means no mixup.</figDesc><table><row><cell cols="2">Mixup Layers Set Accuracy(%)</cell></row><row><cell>∅</cell><cell>69.5</cell></row><row><cell>{0,1,2}</cell><cell>69.3</cell></row><row><cell>{3,4}</cell><cell>70.4</cell></row><row><cell>{6,7,9}</cell><cell>71.9</cell></row><row><cell>{7,9,12}</cell><cell>74.1</cell></row><row><cell>{6,7,9,12}</cell><cell>72.2</cell></row><row><cell>{3,4,6,7,9,12}</cell><cell>71.6</cell></row><row><cell>Model</cell><cell>Accuracy(%)</cell></row><row><cell>MixText</cell><cell>67.6</cell></row><row><cell>-weighted average</cell><cell>67.1</cell></row><row><cell>-TMix</cell><cell>63.5</cell></row><row><cell>-unlabeled data</cell><cell>58.6</cell></row><row><cell>-all</cell><cell>56.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance (test accuracy (%)) on Yahoo! Answer with 10 labeled data and 5000 unlabeled data per class after removing different parts of MixText.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that MixText is a semi-supervised learning framework while TMix is a data augmentation approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/pytorch/fairseq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://pypi.org/project/ pytorch-transformers/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their helpful comments, and Chao Zhang for his early feedback. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research. DY is supported in part by a grant from Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semisupervised learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>CoRR, abs/1905.02249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11">2016. 2016. August 11-12, 2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Importance of semantic representation: Dataless classification</title>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
				<meeting>the 23rd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="830" to="835" />
		</imprint>
	</monogr>
	<note>AAAI&apos;08</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data: An empirical study across techniques and domains</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigoris</forename><forename type="middle">I</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><surname>Karakoulas</surname></persName>
		</author>
		<idno>CoRR, abs/1109.2047</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating structured commonsense knowledge in story completion</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
				<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="6244" to="6251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised Models via Data Augmentation for Classifying Interactive Affective Responses</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop On Affective Content Analysis, The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational sequential labelers for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08370</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">and Roi Reichart</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
	<note>Australia</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno>CoRR, abs/1808.09381</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems, NIPS&apos;04</title>
				<meeting>the 17th International Conference on Neural Information Processing Systems, NIPS&apos;04<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Variational pretraining for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CoRR, abs/1906.02242</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3609" to="3619" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>CoRR, abs/1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploration of noise strategies in semisupervised named entity classification</title>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Lakshmi Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</title>
				<meeting>the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno>CoRR, abs/1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio, and Geoffrey Hinton</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 Workshop : Challenges in Representation Learning (WREPL)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dbpedia for nlp: A multilingual cross-domain knowledge base</title>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
				<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly-supervised neural text classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An exploration of three lightly-supervised representation learning approaches for named entity classification</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2312" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1712.04621</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06709</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Weightaveraged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno>CoRR, abs/1703.01780</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="3635" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.11715</idno>
		<imprint>
			<date type="published" when="2019">2019c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">EDA: easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou ; Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
	</analytic>
	<monogr>
		<title level="m">Unsupervised data augmentation for consistency training</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lets make your request more persuasive: Modeling persuasive strategies via semi-supervised neural nets on crowdfunding platforms</title>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3620" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised role identification in teamwork interactions</title>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miaomiao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1671" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno>CoRR, abs/1702.08139</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
				<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<idno>CoRR, abs/1905.04899</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>CoRR, abs/1710.09412</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1509.01626</idno>
		<title level="m">Character-level convolutional networks for text classification</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
