<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deception Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Wu</surname></persName>
							<email>zhewu@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bharat</forename><surname>Singh</surname></persName>
							<email>bharat@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dartmouth College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deception Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a system for covert automated deception detection using information available in a video. We study the importance of different modalities like vision, audio and text for this task. On the vision side, our system uses classifiers trained on low level video features which predict human microexpressions. We show that predictions of high-level microexpressions can be used as features for deception prediction. Surprisingly, IDT (Improved Dense Trajectory) features which have been widely used for action recognition, are also very good at predicting deception in videos. We fuse the score of classifiers trained on IDT features and high-level microexpressions to improve performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio domain also provide a significant boost in performance, while information from transcripts is not very beneficial for our system. Using various classifiers, our automated system obtains an AUC of 0.877 (10-fold cross-validation) when evaluated on subjects which were not part of the training set. Even though state-ofthe-art methods use human annotations of micro-expressions for deception detection, our fully automated approach outperforms them by 5%. When combined with human annotations of micro-expressions, our AUC improves to 0.922. We also present results of a user-study to analyze how well do average humans perform on this task, what modalities they use for deception detection and how they perform if only one modality is accessible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deception is common in our daily lives. Some lies are harmless, while others may have severe consequences and can become an existential threat to society. For example, lying in a court may affect justice and let a guilty defendant go free. Therefore, accurate detection of a deception in a high stakes situation is crucial for personal and public safety.</p><p>The ability of humans to detect deception is very limited. In <ref type="bibr" target="#b0">(Bond Jr and DePaulo 2006)</ref>, it was reported that the average accuracy of detecting lies without special aids is 54%, which is only slightly better than chance. To detect deception more accurately, physiological methods have been developed. However, physiological methods such as the Polygraph, or more recent functional Magnetic Resonance Imaging (fMRI) based methods are not always correlated with Figure <ref type="figure">1</ref>: Micro-expression: Eyebrows Raising. The left image and rightmost image in the image sequence are the same. However, it is much easier to detect the micro-expression in the image sequence when compared to the static image, as shown by the two green dotted lines. deception <ref type="bibr" target="#b11">(Farah et al. 2014)</ref>. Additionally, the cost of the equipment and the overt nature of the method make the utility of these devices limited for real-life deception detection.</p><p>Another line of work attempts to find behavioral cues for deception detection <ref type="bibr" target="#b6">(DePaulo et al. 2003)</ref>. These cues are faint behavioral residues, which are difficult for untrained people to detect. For example, according to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</ref>, facial micro-expressions reflect emotions that subjects might want to hide. However, because of the variablity across different subjects, these microexpressions are extremely difficult to detect using computer vision, especially in unconstrained settings.</p><p>It is usually much easier for humans to detect a subtle facial expression from videos than from static images <ref type="bibr" target="#b13">(Grill-Spector et al. 1998)</ref>. For example, Fig. <ref type="figure">1</ref> shows a comparison of static and dynamic representations of a simple micro micro-expression: Eyebrows Raise. Given only the left static image, people have a difficult time detecting that the eyebrows are raising. In contrast, we can clearly see from the right image sequence that the eyebrows are raising, even though the last image of the image flow is exactly the left static image.</p><p>Motivated by these observations, we propose to use motion dynamics for recognizing facial micro-expressions. This coincides with the psychological insights from <ref type="bibr" target="#b7">(Duran et al. 2013)</ref>, in which the authors suggest focusing on dynamic motion signatures which are indicative of deception. To accomplish this, we design a two-level feature representation for capturing dynamic motion signatures. For the low-level feature representation, we use dense trajectories which represent motion and motion changes. For the highlevel representation, we train facial micro-expression detectors using low level features, and use their confidence score as high-level features. Experiments on 104 court room trial videos demonstrate the effectiveness and the complementary nature of our low-level and high-level features.</p><p>Deception is a complex human behavior where subjects try to inhibit their deceptive evidence, from facial expressions to gestures, from the way they talk to what they say. Thus, a reliable deception detection method should integrate information from more than one modality. Taking motivation from prior work <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal, Tabibu, and Bajpai 2016)</ref>, we also include features from other modalities, specifically audio and text. These additional modalities improve AUC (Area under the precisionrecall curve) of our automated system by 5% to 0.877. When using ground truth facial micro-expression annotations, the system obtains 0.922 AUC which is 9% better than the previous state-of-the-art. We also conduct user studies to analyze how well do average humans perform on this task, what modalities they use and how they perform if only one modality is accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Physiological measures have been considered to be useful in deception detection for a long time. Polygraph measures physiological indices such as blood pressure, heart rate, skin conductivity of the person under interrogation, but their reliability is questionable. Thermal imaging can record the thermal patterns <ref type="bibr" target="#b21">(Pavlidis, Eberhardt, and Levine 2002)</ref> and measure the blood flow of the body <ref type="bibr" target="#b1">(Buddharaju et al. 2005)</ref>, but the technique requires expensive thermal cameras. Brain-based detectors, such as functional MRI, have recently been proposed to detect deception by scanning the brain and finding areas that are correlated with deception. Although it achieves high accuracy <ref type="bibr">(Kozel et al. 2005;</ref><ref type="bibr" target="#b17">Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>, important questions related to the working mechanism, reliability and the experimental setting are still open research problems <ref type="bibr" target="#b17">(Langleben and Moriarty 2013;</ref><ref type="bibr" target="#b11">Farah et al. 2014)</ref>. Furthermore, the above mentioned physiological measure based methods are overt and could be disrupted by the subject's counter preparation and behavior <ref type="bibr" target="#b12">(Ganis et al. 2011</ref>).</p><p>Among the covert systems, computer vision based methods play an important role. Early works <ref type="bibr" target="#b19">(Lu et al. 2005;</ref><ref type="bibr" target="#b26">Tsechpenakis et al. 2005</ref>) used blob analysis to track head and hand movements, which were used to classify human behavior in videos in three different behavioral states. However, these methods used person specific sample images for training blob detectors, and since the database was small, the methods were prone to overfitting and did not generalize to new subjects. Based on Ekman's psychology research <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009</ref>) that some facial behaviors are involuntary and might serve as an evidence for deceit detection, several automatic vision-based systems were <ref type="bibr">developed. Zhang et al. (Zhang et al. 2007</ref>) tried to detect the differences between simulated facial expressions and involuntary facial expressions by identifying Deceit Indicators, which were defined by a group of specific Facial Action Units <ref type="bibr" target="#b8">(Ekman and Friesen 1977)</ref>. However, this method requires people to manually label facial landmarks and input major components of FAU, thus is not fully automated. Besides, this method was only tested with static images, so essential motion patterns in facial expressions were not captured. <ref type="bibr" target="#b20">(Michael et al. 2010)</ref> proposed a new feature called motion profiles to extend the head and hand blob analysis with facial micro-expressions. Although fully automatic, this method relies heavily on the performance of facial landmarks localization, and the experimental setting is very constrained. For unconstrained videos, the facial landmark localization method may be unreliable.</p><p>These early works were mainly in low-stake situations. Recently, researchers focused more on high stake deception detection, so that experiments are closer to real life settings. In <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015)</ref>, a new dataset containing reallife trial videos was introduced. In this work, a multi-modal approach is presented for the high-stake deception detection problem, but the method requires manual labelling of human micro-expressions. Furthermore, in the dataset, the number of videos for different trials varies a lot, biasing the results towards longer trials. In <ref type="bibr" target="#b25">(Su and Levine 2016)</ref>, the authors also collected a video database of high-stakes situations, and manually designed a variety of features for different facial parts. Again, the manually designed features require the facial landmarks to be accurately detected. Also, the method segmented each video into multiple temporal volumes and assumes all the temporal volume labels to be the same when learning the classifier. This could be incorrect for deception cases, because the deception evidence could be buried anywhere in the video.</p><p>Deceptive behavior is very subtle and varies across different people. Thus, detecting these subtle micro motion patterns, e.g. micro facial expression, itself is a challenging problem. In addition, Duran et al. <ref type="bibr" target="#b7">(Duran et al. 2013)</ref> suggested that research should focus more on the movement dynamics and behavior structure. Motivated by this, we directly describe behavioral dynamics without detecting facial landmarks, then use behavior dynamics to learn micro-expressions and deceptive behavior. We also include simple verbal and audio features, as other multi-modal approaches <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015;</ref><ref type="bibr" target="#b16">Jaiswal, Tabibu, and Bajpai 2016)</ref>, into the overall covert automated system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Our automated deception detection framework consists of 3 steps: multi-modal feature extraction, feature encoding and classification. The framework is shown in Figure <ref type="figure">.</ref> 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Modal Feature Extraction</head><p>Motion Features Our input source are videos, where a person is making truthful or deceptive statements. The video acquisition conditions are unconstrained, so the subject's face may not always be viewed frontally or centered. Here, we employ IDT (Improved Dense Trajectory) <ref type="bibr" target="#b26">(Wang et al. 2016</ref>) features due to their excellent performance in action recognition, especially in unconstrained settings. IDT compute local feature correspondences in consecutive frames and estimate the camera motion using RANSAC. After cancelling the effect of camera motion, the method samples feature points densely at multiple spatial scales, then tracks them through a limited number of frames to prevent drifting. Within the space-time volume around the trajectory, the method computes HOG (histogram of oriented gradients), HOF (histogram of optical flow) <ref type="bibr" target="#b18">(Laptev et al. 2008)</ref>, MBH (motion bountary histogram) <ref type="bibr" target="#b4">(Dalal, Triggs, and Schmid 2006)</ref> and trajectory descriptors. We found that the MBH descriptor works better than other descriptors (like HOG/HOF) for our task, because MBH computes optical flow derivatives and captures derivatives of motion rather than first order motion information. Since we want to detect micro-expressions, the descriptor should represent changes in motion rather than constant motion, which is captured in MBH.</p><p>Audio Features We use MFCC (Mel-frequency Cepstral Coefficients) <ref type="bibr" target="#b5">(Davis and Mermelstein 1980)</ref> features as our audio features. MFCC has been widely used for ASR (Automatic Speech Recognition) tasks for over 30 years. We use the following MFCC extraction procedure: first estimate the periodogram of the power spectrum for each short frame, then warp to a Mel frequency scale, and finally compute the DCT of the log-Mel-spectrum. Then for each video, we have a series of MFCC features corresponding to short intervals. After MFCC features are extracted, we use GMM (Gaussian Mixture Model) to build an audio feature dictionary for all training videos. We treat all the audio features equally and use our feature encoding method to encode the whole sequence, similar to <ref type="bibr" target="#b2">(Campbell, Sturim, and Reynolds 2006)</ref>. This is because we are not interested in speech content (spoken words), but in hidden cues of deception in the audio domain.</p><p>Transcript Features For every video, we use Glove(Global Vectors for Word Representation) (Pen-nington, Socher, and Manning 2014) for encoding the entire set of words in the video transcripts to one fixed-length vector. Glove is an unsupervised learning algorithm for representing words using vectors. It is trained using word co-occurrence statistics. As a result, the word vector representations capture meaningful semantic structure. Compared to other text-based deception detection methods <ref type="bibr" target="#b24">(Porter and Brinke 2010)</ref>, Glove is more widely applicable in unconstrained environments.</p><p>We use the pre-trained Wikipedia 2014+ Gigaword5 corpus which contains 6 billion tokens in total. Each word is embedded in a 300 dimensional vector space. Again, we use GMM to learn a vocabulary for the word vectors and employ a Fisher Vector encoding, described below, to aggregate all the word vectors into a fixed-length representation for the entire transcript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Encoding</head><p>Since the number of features is different for each video, we employ a Fisher Vector encoding to aggregate a variable number of features to a fixed-length vector. Fisher Vectors were first introduced in <ref type="bibr" target="#b15">(Jaakkola and Haussler 1999)</ref> to combine the advantages of generative and discriminative models, and are widely used in other computer vision tasks, such as image classification (Perronnin and Dance 2007), action recognition (Wang et al. 2016) and video retrieval <ref type="bibr" target="#b14">(Han et al. 2017)</ref>.</p><p>Fisher Vector encoding first builds a K-component GMM model (Î¼ i , Ï i , w i : i = 1, 2, ..., K) from training data, where Î¼ i , Ï i , w i are the mean, diagonal covariance, and mixture weights for the i th component, respectively. Given a bag of features {x 1 , x 2 , ..., x T }, its Fisher Vector is computed as: </p><formula xml:id="formula_0">G Î¼i = 1 T â w i T t=1 Î³ t (i) x t â Î¼ i Ï i (1)</formula><formula xml:id="formula_1">G Ïi = 1 T â 2w i T t=1 Î³ t (i) (x t â Î¼ i ) 2 Ï 2 i â 1 (2)</formula><p>where, Î³ t (i) is the posterior probability. Then all the G Î¼i and G Ïi are stacked to form the 2DK-dimension Fisher Vector, where D is the dimensionality of local feature x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Micro-Expression Prediction</head><p>The multi-modal features discussed above are low-level features. Here, we introduce the high-level features used to represent facial micro-expressions. According to <ref type="bibr" target="#b9">(Ekman et al. 1969;</ref><ref type="bibr" target="#b10">Ekman 2009)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deception Detection</head><p>The Fisher vector encoding of the low level features and the video level score vector are then used to train four binary deception classifiers. Three of those classifiers are based on the visual, auditory and text channels for which GMM's were constructed, and the fourth uses the pooled score vectors for the micro-expression detectors. Denote the prediction score of the multi-modal Fisher Vector and the high-level microexpression feature as {S mi }, i â [1, 3] and S high . The final </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We evaluate our automated deception detection approach on a real-life deception detection database <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015)</ref>. This database consists of 121 court room trial video clips. Videos in this trial database are unconstrained videos from the web. Thus, we need to handle differences in the viewing angle of the person, variation in video quality and background noise, as shown in Figure <ref type="figure">.</ref> 4.</p><p>We use a subset of 104 videos from the trial database of 121 videos, including 50 truthful videos and 54 deceptive videos. The pruned videos have either significant scene change or human editing. In the experiments shown below, we do not report the results, as described in <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015)</ref>. Instead, we re-implement the method (referred to Ground Truth micro-expressions) on our training and test splits to avoid over fitting to identities rather than deception clues.</p><p>The dataset contains only 58 identities, which is less than the number of videos and often the same identity is ei- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro-Expression Prediction</head><p>We first analyze the performance our micro-expression prediction module. We sample frames for each video clip using a frame rate of 15 fps. The motion features are Improved Dense Trajectories which are represented with a Fisher Vector encoding. The micro-expression detectors are trained using a linear kernel SVM using LibSVM <ref type="bibr" target="#b3">(Chang and Lin 2011)</ref>. The results are shown in Table <ref type="table">.</ref> 1, and we report AUC (Area under the precision-recall curve). We will show in the following experiments that even though an AUC of 0.6511 is not high for detecting micro-expressions, the high-level features representing the probability of micro-expressions still provide good performance on the final deception detection task. We believe deep learning based approaches could perform better at predicting micro-expressions; however, with the limited amount of training data available in this dataset, it is problematic to train such techniques. We did experiment with off-the-shelf CNN features for classifying microexpressions, but their performance was significantly worse than IDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deception Detection</head><p>We now evaluate our automated deception detection system. The results, measured by AUC, are shown in Table <ref type="table">.</ref> 2. The first 4 rows are results from one modality, while the last 4 rows are after late fusion of multi-modal features. Each column corresponds to one type of classifier. We can see the highest AUC (0.8773) is after late fusion, which uses all modality features and a linear SVM classifier. This performance is much better than using Ground Truth microexpression features (0.8325).</p><p>Performance of Different Classifiers SVM and Random Forest perform better compared to other classifiers like Naive Bayes and Logistic Regression because they are discriminative. One interesting finding is that different classifiers are good at utilizing different feature modalities. For example, we observe that linear SVM works best on IDT features, Random Forest works best on high-level microexpression features and Kernel SVM performs best on MFCC features. However, when we aggregate multi-modal features using late fusion, the performance of different classifiers converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of Different Modalities</head><p>We observe that IDT features obtain an AUC of 0.7731. Although the proposed high-level micro-expression features could not accurately predict micro-expressions, they help in improving deception detection. MFCC features obtain the highest AUC using a single modality, showing the importance of audio features in the deception detection task. The transcript feature obtains the lowest performance, mainly because the pretrained word vector representation does not capture the underlying complicated verbal deception cues. Nevertheless, the entire system still benefits from the transcript features after late fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Late Fusion</head><p>Although the performance of different modalities are different for each classifier, the overall performance improves when we combine different modalities. Combining the scores of the classifier trained on IDT features with the classifier trained on micro-expression predictions helps us obtain an AUC of 0.8347, which is the performance of visual modality. Thus, even though the microexpression detectors are trained using IDT features, the lowlevel and high-level classifiers are complementary. Other modalities like text and audio improve performance by 4% for the overall system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deception Detection with Ground Truth Micro-Expressions</head><p>Since the high-level feature is the prediction score of trained micro-expression detectors, one interesting question is how the performance will be affected if we use the Ground Truth micro-expression features, as in <ref type="bibr" target="#b23">(PÃ©rez-Rosas et al. 2015)</ref>. In the following experiment, we use the GT microexpression feature as the baseline, and test how the performance changes with other feature modalities. than high-level micro-expression features (which is the confidence score of the micro-expression classifier). With the addition of IDT features, our vision system improves by more than 5% (0.8988 AUC). This proves the effectiveness of motion-based features. After late fusion with results of transcript and MFCC features, the performance of the overall system is 0.9221 AUC, which is better than the proposed fully automated system. This suggests that developing more accurate methods for detecting micro-expressions is a potential direction for improving deception detection in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Micro-Expressions</head><p>We investigate the effectiveness of each individual microexpression.</p><p>For each micro-expression, we test the performance by using the high-level micro-expression score feature, with low level motion features and other modalities, shown in Figure <ref type="figure">.</ref> 5. The performance of using all micro-expressions is shown for comparison. We use linear SVM as the classifier in this study, as it was the best individual classifier. From Figure <ref type="figure">.</ref> 5, we observe that "Eyebrows Raise" is more effective than other micro-expressions in both predicted microexpressions and ground truth micro-expressions. "Head Side Turn" is also helpful when using predicted microexpressions, see Figure <ref type="figure">.</ref> 5a. This is different from results obtained from ground truth micro-expressions. On the other hand, "Frown" works better using ground truth feature than predicted feature, possibly because the "Frown" detector is not accurate enough, as also suggested in Table <ref type="table">.</ref> 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Study</head><p>To test human performance on this task, we perform user studies using AMT (Amazon Mechanical Turk). First, we ask 10 different people to watch each video and decide if they consider the subject in the video truthful or not. Each annotator is assigned 5 videos of 5 different identities to ensure no identity specific bias is used for deceit prediction. We also record if image, audio or transcripts were helpful for their decision. Note that here the decision is made using all modalities. The percentage of votes per video is used as the score for deception. The AUC for human prediction is 0.8102. This shows that this dataset is relatively easier than previous studies where predictions of people on this task were almost chance. Nevertheless, even on this dataset, it is not obvious if the subject is deceptive or not.</p><p>When making the decision, 67.4% of the time users rely on visual cues, 61.3% of the time on audio, and 70.7% of the time on transcripts, as shown in Figure <ref type="figure">.</ref> 6. Note that for each video, people can select multiple modalities as helpful. From this data, we notice that people tend to make decisions based on spoken content, as this is a semantic-level feature. Only half of the people think that audio helps them making decision, while in our system, audio features are very effective. Therefore, we conducted another user study where we only show one modality at a time to each individual user because when multiple sources of information are available simultaneously, it is not easy to tell which source was helpful for making the final decision.</p><p>To test human performance on each modality, we ask 5 people to watch each video without sound, 5 people to listen to the audio and 5 people to read the transcripts of the video. Therefore in each study, subjects have access to one modality only. Note that the same person is not shown any other modality from the same video again after being shown one modality. The results shown in Figure <ref type="figure">.</ref> 7 are of our system using linear SVM as classifier along with human performance. We can see that with only visual modality, there is a huge performance gap between human performance and our system. This shows that although humans lack the ability of predicting deceptive behavior with visual cues alone, our  computer vision based system is significantly better. On the other hand, with only audio, human performance is as good as when all modalities are accessible. But when only transcripts of videos are provided, the performance drops significantly both for humans and our system. This suggests that audio information plays an essential role for humans for predicting deceptive behavior, while transcripts are not that beneficial. With all modalities, our automated system is about 7% better compared to an average person, while our system with Ground Truth micro-expressions is about 11% better.</p><p>For future work, we believe better models for representing audio would be a promising direction for improving performance of our system. In addition, designing hybrid humancomputer systems might be very important for developing a robust and accurate deception detection system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>A system for covert automatic deception detection using multi-modal information in a video was presented. We demonstrated that deception can be predicted independent of the identity of the person. Our vision system, which uses both high-level and low level visual features, is sig- Figure <ref type="figure">7</ref>: Human performance in deception detection using different modalities is compared with our automated system and our system with Ground Truth micro-expressions. nificantly better at predicting deception compared to humans. When complementary information from audio and transcripts is provided, deception prediction can be further improved. These claims are true over a variety of classifiers verifying the robustness of our system. To understand how humans predict deception using individual modalities, results of a user study were also presented. As part of future work, we believe collecting more data for this task would be fruitful as more powerful deep learning techniques can be employed. Predicting deception in a multi-agent setting using information available in video would be a promising future direction as the system would need to understand the conversation between identities over time and then arrive at a logical conclusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our automated deception detection framework.</figDesc><graphic url="image-2.png" coords="3,78.72,53.50,280.03,201.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Five most predictive micro-expressions, from left to right: Frowning, Eyebrows raising, Lip corners up, Lips protruded and Head Side Turn.</figDesc><graphic url="image-9.png" coords="4,319.50,174.19,238.54,150.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, facial micro-expressions play an important role in predicting deceptive behavior. To investigate their effectiveness, (PÃ©rez-Rosas et al. 2015) manually annotated facial expressions and use binary features derived from the ground truth annotations to predict deception. They showed the five most predictive micro-expressions are: Frowning, Eyebrows raising, Lip corners up, Lips protruded and Head Side Turn. Samples of these facial microexpressions are shown in Figure. 3.We use the low-level visual features to train microexpression detectors, and then use the predicted scores of the micro-expression detectors as high-level features for predicting deception. We divide each video in the database into short fixed-duration video clips and annotate these clips with micro-expression labels. Formally, given a training video set V = {v 1 , v 2 , ..., v N }, by dividing each video into clips, we obtain a training set C = {v j i }. The annotation set is L = {l j i }, i â [1, N] denotes the video id, the superscript j â [1, n i ] denotes the clip id , n i is the number of clips for video i and the duration of v j i is a constant (4 seconds in our implementation). The dimension of l j i is the number of micro-expressions. Then we train a set of micro-expression classifiers using the clips C, and apply the classifiers on test video clips C to generate the predicted score L = { lj i }. These score vectors are pooled by averaging them over all clips in a video to produce a video score vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Video Samples in the Real-life Trial Deception Database depicting the variation in human face size, pose and illumination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Analysis of predicted micro-expressions and Ground truth micro-expressions. Each group depicts the performance with only that micro-expression. The performance of using all micro-expressions is shown in the rightmost group.</figDesc><graphic url="image-11.png" coords="7,59.61,205.75,225.67,135.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The importance of modalities for humans in making decisions.</figDesc><graphic url="image-12.png" coords="7,331.11,53.68,215.29,138.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Table. 3 shows the results, measured by AUC. Note that we re-ran this study because we do not use the same identity in the training and test splits. From Table. 2, we observe that the performance of GT-MicroExpression (PÃ©rez-Rosas et al. 2015) alone is better Deception Detection results using different feature and classifier combinations. First 4 rows are results of independent features. Last 4 rows are late fusion results of multi-modal features.</figDesc><table><row><cell>Features</cell><cell cols="2">L-SVM K-SVM</cell><cell>NB</cell><cell>DT</cell><cell>RF</cell><cell>LR</cell><cell>Adaboost</cell></row><row><cell>IDT</cell><cell>0.7731</cell><cell cols="5">0.6374 0.5984 0.5895 0.5567 0.6425</cell><cell>0.6591</cell></row><row><cell>MicroExpression</cell><cell>0.7502</cell><cell cols="5">0.7540 0.7629 0.7269 0.8064 0.7398</cell><cell>0.7507</cell></row><row><cell>Transcript</cell><cell>0.6457</cell><cell cols="5">0.4667 0.6625 0.5251 0.6172 0.5643</cell><cell>0.6416</cell></row><row><cell>MFCC</cell><cell>0.7694</cell><cell cols="5">0.8171 0.6726 0.4369 0.7393 0.6683</cell><cell>0.6900</cell></row><row><cell>IDT+MicroExpression</cell><cell>0.8347</cell><cell cols="5">0.7540 0.7629 0.7687 0.8184 0.7419</cell><cell>0.7507</cell></row><row><cell cols="2">IDT+MicroExpression+Transcripts 0.8347</cell><cell cols="5">0.7540 0.7776 0.7777 0.8184 0.7419</cell><cell>0.7507</cell></row><row><cell>IDT+MicroExpression+MFCC</cell><cell>0.8596</cell><cell cols="5">0.8233 0.7629 0.7687 0.8477 0.7894</cell><cell>0.7899</cell></row><row><cell>All Modalities</cell><cell>0.8773</cell><cell cols="5">0.8233 0.7776 0.7777 0.8477 0.7894</cell><cell>0.7899</cell></row><row><cell>Features</cell><cell cols="2">L-SVM K-SVM</cell><cell>NB</cell><cell>DT</cell><cell>RF</cell><cell>LR</cell><cell>Adaboost</cell></row><row><cell>GTMicroExpression</cell><cell>0.7964</cell><cell cols="5">0.8102 0.8325 0.7731 0.8151 0.8275</cell><cell>0.8270</cell></row><row><cell>GTMicroExpression+IDT</cell><cell>0.8456</cell><cell cols="5">0.8137 0.8468 0.7834 0.8205 0.8988</cell><cell>0.8270</cell></row><row><cell cols="2">GTMicroExpression+IDT+Transcript 0.8594</cell><cell cols="5">0.8137 0.8923 0.8074 0.8205 0.8988</cell><cell>0.8270</cell></row><row><cell>GTMicroExpression+IDT+MFCC</cell><cell>0.8969</cell><cell cols="5">0.9002 0.8668 0.7834 0.8319 0.9221</cell><cell>0.8320</cell></row><row><cell>GTMicroExpression+All Modalities</cell><cell>0.9065</cell><cell cols="5">0.9002 0.8905 0.8074 0.8731 0.9221</cell><cell>0.8321</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Deception Detection results with Ground Truth micro-expression features and other feature modalities.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegement</head><p>The research was funded by ARO Grant W911NF1610342.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accuracy of deception judgments</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Bond</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and social psychology Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="214" to="234" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic thermal monitoring system (athemos) for deception detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buddharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dowdall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsiamyrtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shastri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1179</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines using gmm supervectors for speaker verification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Sturim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="308" to="311" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the movement dynamics of deception</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pan-cultural elements in facial displays of emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Sorenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">3875</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<title level="m">Telling lies: Clues to deceit in the marketplace, politics, and marriage</title>
				<imprint>
			<publisher>WW Norton &amp; Company</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>revised edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Functional mri-based lie detection: scientific and societal challenges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lying in the scanner: covert countermeasures disrupt deception detection by functional magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ganis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kievit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Schendan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="319" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cue-invariant activation in object-related areas of the human occipital lobe</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grill-Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Itzchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vrfp: On-the-fly video retrieval using web images and fast fisher vector products</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The truth and nothing but the truth: Multimodal analysis for deception detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabibu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops (ICDMW), 2016 IEEE 16th International Conference on</title>
				<editor>
			<persName><surname>Ieee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Kozel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Mu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Grenesko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Laken</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>George</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2005">2016. 2005</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
	<note>Detecting deception using functional magnetic resonance imaging</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using brain imaging for lie detection: Where science, law, and policy collide</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Langleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Moriarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology, Public Policy, and Law</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blob analysis of the head and hands: A method for deception detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System Sciences, 2005. HICSS&apos;05. Proceedings of the 38th Annual Hawaii International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion profiles for deception detection using visual cues</title>
		<author>
			<persName><forename type="first">N</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dilsizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="462" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human behaviour: Seeing through the face of deception</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">6867</biblScope>
			<biblScope unit="page" from="35" to="35" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">V</forename><surname>PÃ©rez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
				<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2015. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The truth about lies: What works in detecting high-stakes deception? Legal and criminological</title>
		<author>
			<persName><forename type="first">S</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Does lie to me lie to you? an evaluation of facial clues to high-stakes deception</title>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time automatic deceit detection from involuntary facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Slowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2016. 2007. 2007</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
