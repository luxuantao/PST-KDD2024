<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modified cuckoo search algorithm with rough sets for feature selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Abd</surname></persName>
						</author>
						<author>
							<persName><forename type="first">El</forename><surname>Aziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">Zagazig University</orgName>
								<address>
									<settlement>Zagazig</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Aboul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ella</forename><surname>Hassanien</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computers Information</orgName>
								<orgName type="institution">Cairo University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ella</forename><surname>Aboul</surname></persName>
						</author>
						<author>
							<persName><surname>Hassanien</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Neural Comput &amp; Applic</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modified cuckoo search algorithm with rough sets for feature selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FEF0CA8B55ECBC4ECB10C6184AA69D8</idno>
					<idno type="DOI">10.1007/s00521-016-2473-7</idno>
					<note type="submission">Received: 19 December 2015 / Accepted: 6 July 2016 Ó The Natural Computing Applications Forum 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hamming distance</term>
					<term>Modified cuckoo search (MCS)</term>
					<term>Feature selection</term>
					<term>Rough sets</term>
					<term>Le ´vy flight</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a modified cuckoo search algorithm with rough sets is presented to deal with high dimensionality data through feature selection. The modified cuckoo search algorithm imitates the obligate brood parasitic behavior of some cuckoo species in combination with the Le ´vy flight behavior of some birds. The modified cuckoo search uses the rough sets theory to build the fitness function that takes the number of features in reduct set and the classification quality into account. The proposed algorithm is tested and validated benchmark on several benchmark datasets drawn from the UCI repository and using different evaluation criteria as well as a further analysis is carried out by means of the Analysis of Variance test. In addition, the proposed algorithm is experimentally compared with the existing algorithms on discrete datasets. Finally, two learning algorithms, namely K-nearest neighbors and support vector machines are used to evaluate the performance of the proposed approach. The results show that the proposed algorithm can significantly improve the classification performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For a given dataset, a feature is a measurable property of the problem under observation. Over the past years, the domain of features in a machine learning or a pattern recognition applications have been expanded from tens to hundreds of variables or features used in such applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. Hence, the use of reduction or selection techniques is essential to reduce the large number of feature in the problem. Feature selection (FS) is a process of selecting a subset of features from a larger set of features, which leads to the reduction of the dimensionality of features space for a successful classification task <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. Feature selection provides a way for identifying the important features and removing irrelevant or redundant features from dataset. Feature selection helps in understanding data, reducing computation requirement, reduces the effect of curse of dimensionality and improve the predictor performance <ref type="bibr" target="#b5">[6]</ref>.</p><p>Formerly, an exhaustive search for the optimal or near to optimal solution in an enormous search space may be impracticable. Many researches seek to model the feature selection as a optimization problem. One of the most used methods to solve the feature selection problems are evolutionary and swarm intelligence methods. Swarm intelligence is a computational intelligence-based approach which is made up of a population of artificial agents and inspired by the social behavior of animals (fish, birds, fireflies, etc.) in the real world. Example of such methods is ant colony optimization <ref type="bibr" target="#b26">[27]</ref>, bat algorithm <ref type="bibr" target="#b30">[31]</ref> and particle swarm optimization (PSO) <ref type="bibr" target="#b14">[15]</ref>. Also, genetic algorithms (GA) were the first evolutionary-based algorithm introduced in the literature and were developed based on the natural process of evolution through reproduction <ref type="bibr" target="#b47">[48]</ref>.</p><p>Most of these algorithms are combined with rough set (RS) to improve the performance of FS such as: In <ref type="bibr" target="#b40">[41]</ref>,</p><p>Wroblewski introduced a combination of GA and RS to select the relevant features. However, this algorithm is time consuming and does not consider the intensification ability. Bjorvand in <ref type="bibr" target="#b3">[4]</ref> solved the problem of Wroblewski algorithm, in which the speed and the performance of FS were improved. Also, this algorithm contains the features core in all candidates to choose the best initial population for the GA and updates the dynamic mutation rate to reduce the time computation. Zhu et al. <ref type="bibr" target="#b47">[48]</ref> introduced a combination of local search and GA, where the results showed that this algorithm is better than GA alone. In <ref type="bibr" target="#b32">[33]</ref>, a hybrid GA with RS for feature subset selection, this algorithm is similar to the Bjorvand algorithm in the following: (1) It used the local search algorithm to fine-tune parameters. <ref type="bibr" target="#b1">(2)</ref> The core is used to generate the initial population. Also, Elshazly et al. <ref type="bibr" target="#b8">[9]</ref> introduced a hybrid system based on rough set and GA for classification of medical datasets of different sizes.</p><p>Another swarm algorithm is PSO, in which there are many variations of PSO that are combined with RS such as: In <ref type="bibr" target="#b16">[17]</ref> Javani et al. presented PSO for FS, by removing features with a low weight. However, these features may be useful due to feature interaction and removing these feature may cause that the performance is reduced. Wang et al. <ref type="bibr" target="#b20">[21]</ref> redefined the velocity in BPSO as the number of elements that should be changed in the position. The results showed that this algorithm is less computational than GA. Inbaria et al. <ref type="bibr" target="#b14">[15]</ref> proposed a hybrid model that combines the RS and PSO. This algorithm depends on quick reduct algorithm and relative reduct algorithm. However, the fitness function was considered only by a correctness of FS without an optimal subset of features. Also, the relevant features are selected by using the unsupervised PSO-based relative reduct to assess the fetal heart rate <ref type="bibr" target="#b2">[3]</ref>. Chuang et al. <ref type="bibr" target="#b6">[7]</ref> provided a modification to PSO algorithm by reinitializing the worst particles if global best values have not been improved after number of iterations, where the classification performance of cancer-related gene expression datasets is improved. In <ref type="bibr" target="#b21">[22]</ref>, Long et al. applied a combination between the discrete firefly algorithm (FA) <ref type="bibr" target="#b41">[42]</ref> and RS. This algorithm outperforms PSO and GA in terms of accuracy and running time.</p><p>The ant colony optimization (ACO) is another metaheuristic algorithm that can be combined with RS to find minimal reducts <ref type="bibr" target="#b34">[35]</ref>. The artificial bee colony algorithm with a RS was proposed for FS by using different medical datasets in the area of dermatology <ref type="bibr" target="#b35">[36]</ref>, whereas in <ref type="bibr" target="#b31">[32]</ref>, the same algorithm is introduced using neural networks for FS.</p><p>The harmony search (HS) algorithm converges faster than the gradient descent algorithms, and it is a global optimal solution <ref type="bibr" target="#b1">[2]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, Inbarani et al. improved harmony search (IHS) is combined with rough set. This algorithm depends on the quick reduct and relative reduct algorithm and is used for feature selection. Also, the same algorithm is used for feature selection of protein sequence classification <ref type="bibr" target="#b15">[16]</ref>. Recently, Yang and Deb <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> proposed a new metaheuristic algorithm for continuous optimization namely cuckoo Search (CS), which is based on the fascinating reproduction strategy of cuckoo birds. Cuckoo has been successfully applied to a large number of applications such as solving reliability-redundancy allocation <ref type="bibr" target="#b17">[18]</ref> and mobile robot navigation <ref type="bibr" target="#b23">[24]</ref> problems.</p><p>Following <ref type="bibr" target="#b42">[43]</ref>, there are similarities between CS and other metaheuristic algorithms: (1) CS algorithm is a population-based algorithm, at the same time it explores some type of elitism like that of HS. <ref type="bibr" target="#b1">(2)</ref> The randomization is more efficient as the step size which is heavy-tailed with any possible large step size. <ref type="bibr" target="#b2">(3)</ref> The number of parameters to be tuned is less than GA and PSO, and thus, it is potentially more generic to adapt to a wider class of optimization problems. The results showed that CS has fast convergence and global optima achievement <ref type="bibr" target="#b42">[43]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, the binary CS (BCS) for FS was proposed. However, the main drawbacks of CS algorithm are that it needs more time to compute the fitness function, also its convergence rate is slow.</p><p>From the previous, the metaheuristic algorithms such as ACO and PSO have been combined with rough sets for feature selection and their performance were proved. However, there are some drawbacks in these algorithms such as slow convergence, time consuming and the space complexity is high and premature convergence. To overcome these drawbacks, in this paper, we propose a new feature selection algorithm, which combines the modified CS (MCS) <ref type="bibr" target="#b37">[38]</ref> and rough set theory. The position of each cuckoo in the proposed algorithm is converted to a binary vector, where 1's corresponds to the selected feature and 0's otherwise. The selected features are passed to rough set to compute the dependency degree; then, the fitness function is computed. Based on the fitness function value, each cuckoo updates its position until the stopping condition is satisfied. The MCS outperforms other swarm algorithms in terms of accuracy and running time. Therefore, the proposed algorithm is used to find reduct, and to make the classification performance is the same as to use all features. Through reducing the number of learning parameters, the optimal reduct set is less time-consuming and the rate of convergence becomes very fast.</p><p>The rest of this paper is organized as follows: We give the preliminaries in Sect. 2, where a brief overview on the rough set theory and cuckoo search is introduced. In Sect. 3, the modified cuckoo search algorithm is introduced. The proposed algorithm is illustrated in Sect. 4. Section 5 presents the experimental results and the analysis illustrating properties of the proposed algorithm. Section 6 concludes with some recommendations for future work.</p><p>This section briefly provides an introduction for rough set theory and cuckoo search algorithm. A more comprehensive review can be found in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rough sets</head><p>Rough set theory (RS) is a new computational intelligent technique used to deal with uncertainty <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. The uncertainty can be used for evaluation of the importance of attributes and extraction of rules from databases. The main difference between RS and other computational techniques is that the RS deals with data itself only and not need any other information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Consider an information system defined as S ¼ hU; A; V; f i where U and A are two finite, non-empty sets, U ¼ fx 1 ; . . .; x n g is the universe of primitive objects and A represents the family of features <ref type="bibr" target="#b27">[28]</ref>. V ¼ S a2A V a , where V a is the value set of feature a. f : U Â A ! V is an information function which assigns values from domains of feature to objects such as 8a 2 A; x 2 U and f ða; xÞ 2 V a .</p><p>The indiscernibility relation of B &amp; A, denoted by IND(B) , is an equivalence relation defined as:</p><formula xml:id="formula_0">IND B ð Þ ¼ x; y ð Þ2 U Â U : 8a 2 B; f a; x ð Þ¼f a; y ð Þ f g<label>ð1Þ</label></formula><p>If ðx; yÞ 2 INDðBÞ, then x and y are said to be indiscernible with respect to B. The family of all equivalence classes of INDðBÞ is denoted by U=INDðBÞ (also called U/B). Each element in U/B is a set of indiscernible objects with respect to B. It can be calculated as follows:</p><formula xml:id="formula_1">U=INDðBÞ ¼ b a 2 B : U=IND a f g ð Þ f g<label>ð2Þ</label></formula><p>Where</p><formula xml:id="formula_2">AbB ¼ X \ Y : 8X 2 A; 8Y 2 B; X \ Y 6 ¼ ; n o<label>ð3Þ</label></formula><p>If ðx; yÞ 2 U=INDðBÞ, then x and y are indiscernible by features from B. The equivalence classes of the B-indiscernibility relation are denoted as ½x B . Let X U, the lower and upper approximations of X are defined as:</p><formula xml:id="formula_3">BðXÞ ¼ ½x B j ½x B X È É<label>ð4Þ</label></formula><formula xml:id="formula_4">BðXÞ ¼ ½x B j ½x B \ X 6 ¼ ; n o<label>ð5Þ</label></formula><p>The feature set A ¼ C S D, where C and D are called condition and decision features, respectively <ref type="bibr" target="#b27">[28]</ref>. Then, the positive region can be defined as:</p><formula xml:id="formula_5">POS C D ð Þ ¼ [ B X ð Þ; x 2 U=D<label>ð6Þ</label></formula><p>The positive region contains all objects of U that can be classified into classes of U/D using the information in features C. Rough set reducts can be found by using the degree of dependency <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_6">c C D ð Þ ¼ jPOS C D ð Þj=jUj ð<label>7Þ</label></formula><p>The dependency function calculates the approximating power of a feature set. It can be used as a feature significance measure. The reduction of features is achieved by comparing equivalence relations generated by sets of features. Irrelevant features are removed so that the reduced set provides the same predictive capability of the decision feature as the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cuckoo search</head><p>In the following subsection, we will give an overview of the main concepts and the structure of the cuckoo search algorithm as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The behavior of cuckoo breading and Le ´vy flights</head><p>Cuckoo search algorithm is a swarm intelligence algorithm inspired from the reproduction strategy of the cuckoo birds. The cuckoo birds lay their eggs in a communal nest, and they may remove other's eggs to increase the probability of hatching their own eggs <ref type="bibr" target="#b28">[29]</ref>. This method of laying eggs in other's nests is called obligate brood parasitism. Some host bird can discover the eggs are not its own, throw these eggs away or abandons its nest and build a new nest in a new place. Some kind of cuckoo birds can mimic the color and the pattern of the eggs of a few host bird in order to reduce the probability of discovering the intruding eggs. The cuckoos laid their eggs in a nest where the host bird just laid its own eggs, since the cuckoo eggs are hatching earlier than the host bird eggs. Once the eggs are hatched, the cuckoo chick's starts to propel the host eggs out of the nest in order to increase its share of food provided by its host bird.</p><p>Recent studies show that the behavior of many animals when searching for foods has the same characteristics of Le ´vy flights (also defined as a random walk) <ref type="bibr" target="#b4">[5]</ref>. The steplengths in Le ´vy flights are distributed based on a heavytailed probability distribution, and the distance from the origin of the Le ´vy flight after a large number of steps tends to a stable distribution.</p><p>The steps of Le ´vy flights that introduced a random walk are estimated from a Le ´vy distribution:</p><formula xml:id="formula_7">Levy $ u ¼ t Àk 0\k\3<label>ð8Þ</label></formula><p>The Le ´vy flight is Le ´vy distribution with infinite variance and mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cuckoo search algorithm</head><p>In this section, we present in details the main steps of the cuckoo search algorithm as shown in Algorithm 1.</p><p>The standard cuckoo search Algorithm 1 starts with the initial values of population size N, probability p a 2 ½0; 1, maximum number of iterations Max itr and the initial iteration counter t is setting. Then, the initial population N is generated randomly and each solution x i in the population is evaluated by calculating its fitness function f ðx i Þ (as in 7 or 13). It is followed by several repeated steps until the termination criterion satisfied. These steps are: (1) A new solution is generated randomly using a Le ´vy flight by the following form.</p><formula xml:id="formula_8">x tþ1 i ¼ x t i þ a È L evyðkÞ;<label>ð9Þ</label></formula><p>Where È denotes entry-wise multiplication, a is the step size, a [ 0, and Le ´vy ðkÞ is the Le ´vy distribution. ( <ref type="formula" target="#formula_1">2</ref>) The new solution is replaced with a random selected solution if the fitness function of the new solution is better than the fitness function of the selected random solution, (3) A fraction ð1 À p a Þ of the solutions selected randomly, abandoned and replaced by new solutions generated by using local random walks as follows.</p><p>x</p><formula xml:id="formula_9">tþ1 i ¼ x t i þ d x t j À x t k ;<label>ð10Þ</label></formula><p>Where x t j and x t k are two different solutions selected randomly and d 2 ½0; 1 is a uniform random number. (4) The solutions are ranked according to their fitness values, the best solution is assigned and the iteration counter increases.</p><p>The operation is repeated until the termination criteria are satisfied. The algorithm ends by producing the best found solution so far.</p><p>The use of Le ´vy flights as the search method means that the CS can simultaneously find all optima in a design space and the method has been shown to perform well in comparison with PSO. However, CS has some drawbacks such as slow convergence rate, and it needs expensive time due to the large number of fitness function evaluations to find the minimum value. Walton et al. <ref type="bibr" target="#b37">[38]</ref> avoids these limitations by introducing modified cuckoo search (MCS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modified cuckoo search-based RS (MCSRS)</head><p>In this section, the MCS algorithm is introduced where there are two modifications <ref type="bibr" target="#b37">[38]</ref>. Following <ref type="bibr" target="#b37">[38]</ref>, the first modification is that the value of a was made to decrease with increasing the number of generations (such as the inertia constant in PSO). The initial value of a ¼ B ¼ 1 was selected and a new value</p><formula xml:id="formula_10">a ¼ B= ffiffiffi ffi G p</formula><p>, where G is the generation number and B is the boundary. Unlike in the CS, where a is constant and typically the value a ¼ 1 <ref type="bibr" target="#b44">[45]</ref>. The solutions (eggs) are sorted by order fitness f x i ð Þ and a part of the best, eggs are considered as the elite eggs. Secondly, the crossover between cuckoos is performed as <ref type="bibr" target="#b37">[38]</ref>: <ref type="bibr" target="#b0">(1)</ref> For all the top elite eggs x i , randomly picks x j from elite egg. <ref type="bibr" target="#b1">(2)</ref> The new solution (egg) x k is generated using x i and x j , by computing the distance between them. There are two ways to compute the distance: the first one is midpoint, where if</p><formula xml:id="formula_11">f ðx i Þ ¼ f ðx j Þ, then distance dx ¼ jx i À x i j=2.</formula><p>The second way used the inverse of the golden ratio u ¼ ð1 þ ffiffi ffi 5 p Þ=2, where dx ¼ jx i À x i j=u. If x i ¼ x j (i.e, the same egg was picked twice from the randomly picked nest, with a ¼ B=G 2 . 3). From the set of all nests, a random nest l is selected and the new solution (egg) x k is compared with the solution x l in the random nest.</p><formula xml:id="formula_12">If the f ðx k Þ is better than f ðx l Þ, then x l ¼ x k , otherwise it is discarded.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The proposed algorithm</head><p>The hybrid algorithm introduced in this paper for feature selection is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The fitness function is evaluated in parallel environment in order to decrease the number of evaluations. We will show the details of the MCSRS algorithm below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation of position</head><p>The positions in the traditional MCS are represented by continuous-valued positions. However, in the binary MCS for feature selection, the positions are represented by logical vectors, in which the features that correspond to 1's are selected and that correspond to 0's are not selected The MCSRS algorithm starts by selecting a random position for each nest. If the population is generated, then the position of each nest is converted into a binary vector of length n (the total number of features) as:</p><formula xml:id="formula_13">S x j i t ð Þ À Á ¼ 1 1 þ e Àx j i t ð Þ<label>ð11Þ</label></formula><formula xml:id="formula_14">x j i t þ 1 ð Þ¼ 1 if S x j i t ð Þ À Á [ r 0 otherwise &amp;<label>ð12Þ</label></formula><p>where r $ Nð0; 1Þ and x j t denotes a new solution at time step t. To compute the distance between x j t and x i t the hamming distance is used, since the two solutions are binary bit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The fitness function</head><p>Depending on the concept of RS, the fitness function is defined based on two factors: the number of features in the reduct set and the classification quality as:</p><formula xml:id="formula_15">FðRÞ ¼ g Â c C D ð Þ þ n Â 1 À R j j C j j<label>ð13Þ</label></formula><p>where c C D ð Þ is defined in <ref type="bibr" target="#b6">(7)</ref> and R represents the number of selected features form C and C j j is the total number of features in the original set. n and g (n þ g ¼ 1; g 2 ½0; 1) are two parameters which represent the relative quality between the R j j and the classification performance. For each cuckoo, the fitness function in ( <ref type="formula" target="#formula_15">13</ref>) is computed and compared with the global best fitness F best . If the current value (F i ) better, then the global best fitness is replaced with it and its position (x i ) becomes the reduct set R ¼ x best (corresponding to the best feature subset). After that, the positions are updated. This process is carried out until the stopping criterion is met, usually a maximum number of iterations. We present the pseudo-code of the MCSRS algorithm in Algorithm 2, which is a RS feature selection process based on MCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and classifiers description</head><p>To investigate the performance of the proposed MCSRS algorithm for classification dataset in Table <ref type="table" target="#tab_1">1</ref>, we compare our algorithm with other six algorithms, namely PSORS <ref type="bibr" target="#b14">[15]</ref>, gravitational search algorithm-based RS (GRSARS) <ref type="bibr" target="#b13">[14]</ref>, FARS <ref type="bibr" target="#b21">[22]</ref>, IHSRS <ref type="bibr" target="#b14">[15]</ref>, RS <ref type="bibr" target="#b27">[28]</ref> and GARS <ref type="bibr" target="#b32">[33]</ref>. Fourfold cross-validation (CV) was used to split a whole dataset into labeled training set and unlabeled test set. All the algorithms are implemented in MATLAB language and run in windows environment with 64-bit support. To evaluate the performance of algorithms, two classifiers are used: support vector machine (SVM) <ref type="bibr" target="#b25">[26]</ref> and K-nearest neighbor rule (KNN) <ref type="bibr" target="#b0">[1]</ref>.</p><p>Figures <ref type="figure">2</ref> and<ref type="figure">3</ref> show the averages of accuracy on 7 datasets using SVM and KNN, respectively and the less accuracy along all datasets is RS algorithm. The GARS algorithm gives better performance compared with RS on all datasets (expect on Pima Indians and Hepatitis) . PSORS, GRSARS and FARS give nearly the same results for all datasets. From all 7 groups of results, it can be seen that MCSRS is better than IHSRS in general. Table <ref type="table" target="#tab_2">2</ref> shows a comparison of performance of our algorithm with other existing algorithms for FS in terms of overall average accuracies of SVM and KNN. The results are averaged over ten runs of the classifier for seven different training sets of a particular size. It can be observed from the table that the best results are shown in bold face. In particular, the MCSRS algorithm outperformed the IHSRS, PSORS, GRSARS and FARS on larger domains of datasets. It can be seen from the table that the MCSRS combination achieved the best accuracies (given in bold face) on 4 datasets (i.e., Wisconsin, Iris, Pima Indian and Lenses) , while PSORS, GRSARS and IHSRS are reported best results on dermatology, hepatitis and lung datasets, respectively. Moreover, for the purpose of illustration, Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref> show the box plots representing the percentage accuracy over 10 runs of the different algorithms over all dataset using KNN and SVM. It is evident from Figs. 4 and 5 that the MCSRS is situated at the upper side of the figure, which indicates that MCSRS results in higher accuracy scores than those produced by the other algorithms. FARS is found to be very close to MCSRS in this case.   In order to strengthen the conclusions drawn from the previous results, a further analysis was carried out by means of the analysis of variance (ANOVA) test. So far, we have examined the means of a set of measures using ten runs to mean of the accuracy showing that some differences can be found among them. We used the ANOVA test with pos thoc LSD test to further statistically compare all the classifiers over their mean accuracies on the 7 datasets. The null hypothesis of our ANOVA test is that all classifiers are equivalent in terms of error rate. The ANOVA gives a statistical test to reject the null hypothesis. Once it is rejected, the LSD test is used to find the difference among the classifiers. In the ANOVA test, if the p value (sig) is smaller than significance level, then we say that they are significantly different. The significance level was set to a ¼ 0:05 (95 % Confidence interval) in our experiment. Table <ref type="table" target="#tab_3">3</ref> shows the ANOVA parameters obtained for the mean accuracy measure. The p values of mean accuracy are considerably low. So, we reject the null hypothesis. Therefore, post hoc test is used in order to find the algorithm that has the largest effect, Table <ref type="table" target="#tab_4">4</ref>. We can conclude from this table that these results are consistent with Figs. <ref type="figure">2</ref> and<ref type="figure">3</ref>. Also, the four algorithms MCSRS, PSORS, GARS and RS have the largest effect where GARS and RS have negative effect, since the mean of accuracy is the smallest over all algorithms (but GARS is better than RS). However, the PSORS and MCSRS </p><p>Fig. <ref type="figure">4</ref> Box plots of accuracy of algorithms using SVM Fig. <ref type="figure">5</ref> Box plots of accuracy of the algorithms using KNN algorithms have positive effect where the value of the mean of accuracy is the value over all algorithms. The difference between the averages of FARS, IHSRS and GRSARS is not significant at 0.05 level. However, the algorithm that gives the higher accuracy is FARS, then IHSRS gives better results than the PSORS algorithm. We can conclude that the MCSRS algorithm could be correctly classified. From all previous analysis, we can verify that MCSRS algorithm is a much better measure of classification data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>Feature selection is important for real-world data. Since there are a lot of irrelevant and redundant features, the feature selection must be used as preprocess step. Rough set has been used as feature selection technique. However, most of the rough set algorithms are expensive computation and not suitable for big data. To solve this problem, the RS is combined with other metaheuristic algorithms. However, most of these algorithms depend on many parameters and initial values. In this paper, we proposed a modified cuckoo search algorithm with rough set. The rough set-based fitness function is defined based on two factors: the number of features in reduct set and the classification quality. Experiments conducted over several public datasets, and the results showed that the proposed algorithm significantly outperformed other algorithms in selecting lower number of features. Hence, removing irrelevant, redundant, or noisy features while maintaining the classification accuracy. The proposed algorithm is also proven to be more stable than other methods and is capable of producing more general feature subsets. For future work, further investigations are required to identify the behavior of the proposed algorithm in medical diagnosis and very high-dimensional datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Flowchart of proposed algorithm</figDesc><graphic coords="5,85.06,415.45,425.28,280.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 3</head><label>23</label><figDesc>Fig.2Averages of accuracy on datasets using SVM</figDesc><graphic coords="7,178.71,59.24,365.76,183.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,178.71,59.24,365.76,183.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,178.71,263.50,365.76,183.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Datasets descriptions</figDesc><table><row><cell>Dataset no.</cell><cell>Dataset</cell><cell>Samples</cell><cell>Genes</cell><cell>Classes</cell></row><row><cell>1</cell><cell>Breast cancer</cell><cell>10</cell><cell>699</cell><cell>2</cell></row><row><cell>2</cell><cell>Hepatitis</cell><cell>19</cell><cell>155</cell><cell>2</cell></row><row><cell>3</cell><cell>Dermatology</cell><cell>34</cell><cell>366</cell><cell>6</cell></row><row><cell>4</cell><cell>Iris</cell><cell>4</cell><cell>150</cell><cell>3</cell></row><row><cell>5</cell><cell>Pima Indians</cell><cell>8</cell><cell>768</cell><cell>2</cell></row><row><cell>6</cell><cell>Lenses</cell><cell>4</cell><cell>24</cell><cell>3</cell></row><row><cell>7</cell><cell>Lung cancer</cell><cell>56</cell><cell>32</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 Comparison</head><label>2</label><figDesc></figDesc><table><row><cell>of performance of our algorithm with other existing algorithms</cell><cell></cell><cell></cell><cell>RS</cell><cell>GA RS</cell><cell>RS</cell><cell>GRSA RS</cell><cell>MCS RS</cell><cell>IHS RS</cell><cell>FA RS</cell></row><row><cell>using SVM and KNN</cell><cell>Wisconsin</cell><cell>KNN</cell><cell>80.41</cell><cell>87.08</cell><cell>87.40</cell><cell>94.96</cell><cell>97.36</cell><cell>95.03</cell><cell>94.83</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>85.29</cell><cell>87.09</cell><cell>90.80</cell><cell>94.38</cell><cell>96.09</cell><cell>94.22</cell><cell>93.65</cell></row><row><cell></cell><cell>Iris</cell><cell>KNN</cell><cell>79.80</cell><cell>87.70</cell><cell>91.05</cell><cell>92.91</cell><cell>97.01</cell><cell>94.23</cell><cell>96.30</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>80.33</cell><cell>87.83</cell><cell>89.86</cell><cell>96.03</cell><cell>96.56</cell><cell>95.12</cell><cell>96.08</cell></row><row><cell></cell><cell>Dermatology</cell><cell>KNN</cell><cell>77.26</cell><cell>78.50</cell><cell>86.27</cell><cell>81.41</cell><cell>84.41</cell><cell>83.63</cell><cell>85.72</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>83.18</cell><cell>85.20</cell><cell>93.74</cell><cell>90.79</cell><cell>90.79</cell><cell>90.22</cell><cell>89.78</cell></row><row><cell></cell><cell>Pima Indians</cell><cell>KNN</cell><cell>90.27</cell><cell>86.57</cell><cell>89.15</cell><cell>93.36</cell><cell>97.29</cell><cell>93.45</cell><cell>97.27</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>86.55</cell><cell>84.50</cell><cell>89.97</cell><cell>93.49</cell><cell>98.59</cell><cell>94.38</cell><cell>96.59</cell></row><row><cell></cell><cell>Hepatitis</cell><cell>KNN</cell><cell>87.27</cell><cell>84.47</cell><cell>95.78</cell><cell>97.23</cell><cell>97.19</cell><cell>96.69</cell><cell>95.26</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>87.87</cell><cell>82.24</cell><cell>92.88</cell><cell>94.10</cell><cell>93.29</cell><cell>90.10</cell><cell>93.86</cell></row><row><cell></cell><cell>Lung</cell><cell>KNN</cell><cell>86.85</cell><cell>90.71</cell><cell>89.57</cell><cell>91.85</cell><cell>94.85</cell><cell>97.14</cell><cell>96.30</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>86.14</cell><cell>89.42</cell><cell>90.71</cell><cell>90.67</cell><cell>92.02</cell><cell>92.57</cell><cell>89.14</cell></row><row><cell></cell><cell>Lenses</cell><cell>KNN</cell><cell>77.51</cell><cell>80.16</cell><cell>91.53</cell><cell>93.16</cell><cell>97.16</cell><cell>93.16</cell><cell>95.61</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>81.17</cell><cell>83.10</cell><cell>84.26</cell><cell>85.90</cell><cell>90.30</cell><cell>88.39</cell><cell>89.17</cell></row><row><cell></cell><cell>Average</cell><cell>KNN</cell><cell>82.76</cell><cell>85.03</cell><cell>90.10</cell><cell>92.12</cell><cell>95.04</cell><cell>93.31</cell><cell>94.38</cell></row><row><cell></cell><cell></cell><cell>SVM</cell><cell>84.35</cell><cell>85.62</cell><cell>90.57</cell><cell>91.97</cell><cell>93.94</cell><cell>91.86</cell><cell>92.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>ANOVA test Sum of squares df Mean square F Sig.</figDesc><table><row><cell>Between groups</cell><cell>952.43</cell><cell>6 158.73</cell><cell>7.93 .000</cell></row><row><cell>Within groups</cell><cell>839.75</cell><cell>42 19.99</cell><cell></cell></row><row><cell>Total</cell><cell>1792.18</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Neural Comput &amp; Applic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multiple comparisons by LSD test</cell><cell>Group</cell><cell>Data</cell><cell>GARS</cell><cell>PSORS</cell><cell>GRSARS</cell><cell>MCSRS</cell><cell>IHSRS</cell><cell>FARS</cell></row><row><cell></cell><cell>RS</cell><cell>Mean difference</cell><cell>-2.26</cell><cell>-7.33*</cell><cell>9.35*</cell><cell>-12.27*</cell><cell>-10.54*</cell><cell>11.61*</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell>.349</cell><cell>.004</cell><cell>.000</cell><cell>.000</cell><cell>.000</cell><cell>.000</cell></row><row><cell></cell><cell>GARS</cell><cell>Mean difference</cell><cell></cell><cell>-5.07*</cell><cell>-7.09*</cell><cell>-10.01*</cell><cell>-8.28*</cell><cell>-9.35*</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell></cell><cell>.040</cell><cell>.005</cell><cell>.000</cell><cell>.001</cell><cell>.000</cell></row><row><cell></cell><cell>PSORS</cell><cell>Mean difference</cell><cell></cell><cell></cell><cell>-2.02</cell><cell>-4.93*</cell><cell>-3.20</cell><cell>-4.27</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell></cell><cell></cell><cell>.402</cell><cell>.045</cell><cell>.187</cell><cell>.081</cell></row><row><cell></cell><cell>GRSARS</cell><cell>Mean difference</cell><cell></cell><cell></cell><cell></cell><cell>-2.91</cell><cell>-1.18</cell><cell>-2.25</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell></cell><cell></cell><cell></cell><cell>.230</cell><cell>.622</cell><cell>.350</cell></row><row><cell></cell><cell>MCSRS</cell><cell>Mean difference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.72</cell><cell>.65</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.475</cell><cell>.786</cell></row><row><cell></cell><cell>IHSRS</cell><cell>Mean difference</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1.07</cell></row><row><cell></cell><cell></cell><cell>p value</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.656</cell></row><row><cell></cell><cell cols="4">* The mean difference is significant at the 0.05 level</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Neural Comput &amp; Applic</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification of heart disease using K-nearest neighbor and genetic algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Akhil Jabbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Deekhatulu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Conf Comput Intell Model Tech Appl Proc Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">the variants of the harmony search algorithm: an overview</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="68" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PSORR: an unsupervised feature selection technique for fetal heart rate</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Banu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inbarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International conference on modelling, identification and control (ICMIC 2013)</title>
		<meeting><address><addrLine>Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-31">2013. 31 Aug, 1-2 Sept 2013</date>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical applications of genetic algorithms for efficient reduct computation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Bjorvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiss Tech Verl</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="601" to="606" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Le ´vy flights in Dobe Ju/hoansi foraging patterns</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Liebovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Glendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum Ecol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on feature selection methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Electr Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved binary PSO for feature selection using gene expression data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Chem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid system based on rough sets and genetic algorithms for medical data classifications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Elshazly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Elkorany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Fuzzy Syst Appl (IJFSA)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><forename type="middle">J</forename></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<meeting><address><addrLine>Waltham</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tolba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced machine learning technologies and applications: second international conference</title>
		<meeting><address><addrLine>Cairo, Egypt; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-11-30">2014. 2014. 28-30 Nov, 2014</date>
			<biblScope unit="page">488</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, communications in computer and information science</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Big data in complex systems: challenges and neural comput &amp; applic opportunities, studies in big data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Snasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kacprzyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Abawajy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature ranking in rough sets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Commun</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature Selection based on rough set and gravitational search algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hua-Qiang W, Zhan-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li-Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 20th international conference on industrial engineering and engineering management</title>
		<meeting>20th international conference on industrial engineering and engineering management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="409" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised feature selection based on PSO and rough sets for medical diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Inbarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jothi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel hybrid feature selection method based on rough set and improved harmony search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Inbarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagyamathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustering and feature selection via pso algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Javani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aghlmandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on artificial intelligence and signal processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hybrid cuckoo search and genetic algorithm for reliability-redundancy allocation problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kanagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ponnambalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Ind Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1115" to="1124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information gain and divergence-based feature selection for machine learning-based text categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Process Manag</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Computational methods of feature selection (Chapman &amp; Hall/CRC data mining and knowledge discovery series)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An improved particle swarm optimization for feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Bionic Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="200" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute reduction based on rough sets and the discrete firefly algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meesad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recent Adv Inf Commun Technol</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comparison of rough set methods and representative inductive learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fund Inf</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="203" to="219" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cuckoo search algorithm for the mobile robot navigation, in swarm, evolutionary, and memetic computing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Parhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="527" to="536" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Genetic programming for feature subset ranking in binary classification problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Neshatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on genetic programming</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>IEEE computer society conference on computer vision and pattern recognition<address><addrLine>San Juan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integration of graph clustering with ant colony optimization for feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="144" to="161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rough sets: theoretical aspects of reasoning about data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pawlak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Kluwer Academic Publishing</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Sorenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klitz</surname></persName>
		</author>
		<title level="m">The Cuckoos</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BCS: A Binary Cuckoo search algorithm for feature selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lam</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international symposium on circuits and systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2013-05">2013. May 2013</date>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page">468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A wrapper approach for feature selection based on Bat Algorithm and optimum-path forest</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lam</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Nakamura Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl Int J</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2250" to="2258" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hybrid approach for effective feature selection using neural networks and artificial bee colony optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokouhifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd International conference on machine vision (ICMV)</title>
		<imprint>
			<date type="published" when="2010-12">2010. Dec 2010</date>
			<biblScope unit="page" from="502" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A hybrid genetic algorithm for feature subset selection in rough set theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si-Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1373" to="1382" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Feature selection for data and pattern recognition</title>
		<author>
			<persName><forename type="first">U</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">584</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel rough set reduct algorithm for medical domain based on bee colony optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Suguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thanushkodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="49" to="54" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An independent rough set approach hybrid with artificial bee colony algorithm for dimensionality reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Suguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thanushkodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Appl Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="266" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rough set methods in feature selection and recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Swiniarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="833" to="849" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modified cuckoo search: a new gradient free optimization algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos Solitons Fractals</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="710" to="718" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature selection based on rough sets and particle swarm optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A novel rough set reduct algorithm to feature selection based on artificial fish swarm algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Swarm Intell</title>
		<imprint>
			<biblScope unit="volume">8795</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finding minimal reducts using genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wroblewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of second annual join conference on information sciences</title>
		<meeting>second annual join conference on information sciences<address><addrLine>Wrightsville Beach, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="186" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Firefly algorithms for multimodal optimization</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stoch Algorithms Found Appl</title>
		<imprint>
			<biblScope unit="volume">5792</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bat algorithm and cuckoo search: a tutorial, artificial intelligence</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol Comput Metaheur Stud Comput Intell</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="421" to="434" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cuckoo search via flvy flights</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deb</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NaBIC 2009-world congress on nature &amp; biologically inspired computing</title>
		<meeting>the NaBIC 2009-world congress on nature &amp; biologically inspired computing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="210" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Engineering optimization by cuckoo search</title>
		<author>
			<persName><forename type="first">X-S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deb</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Math Model Numer Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="330" to="430" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">novel feature selection approach for biomedical data classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yonghong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhiqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jianmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Biomed Inf</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Different metaheuristic strategies to solve the feature selection problem</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yusta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="525" to="534" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wrapper-filter feature selection algorithm using a memetic framework</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Syst Man Cybern Part B Cybern</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Neural Comput &amp; Applic</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
