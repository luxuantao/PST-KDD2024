<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Handling Data Skew in Parallel Joins in Shared-Nothing Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Xu</surname></persName>
							<email>yu.xu@teradata.com</email>
						</author>
						<author>
							<persName><forename type="first">Pekka</forename><surname>Kostamaa</surname></persName>
							<email>pekka.kostamaa@teradata.com</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Teradata San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Teradata El Segundo</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Xin Zhou Teradata El Segundo</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego San Diego</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Handling Data Skew in Parallel Joins in Shared-Nothing Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BE3749FE199D23DC4CC39FF6EBC89C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.4 [Information Systems]: DATABASE MANAGE-MENT-Systems data skew</term>
					<term>parallel joins</term>
					<term>shared nothing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Parallel processing continues to be important in large data warehouses. The processing requirements continue to expand in multiple dimensions. These include greater volumes, increasing number of concurrent users, more complex queries, and more applications which define complex logical, semantic, and physical data models. Shared nothing parallel database management systems [16] can scale up "horizontally" by adding more nodes. Most parallel algorithms, however, do not take into account data skew. Data skew occurs naturally in many applications. A query processing skewed data not only slows down its response time, but generates hot nodes, which become a bottleneck throttling the overall system performance. Motivated by real business problems, we propose a new join geography called PRPD (Partial Redistribution &amp; Partial Duplication) to improve the performance and scalability of parallel joins in the presence of data skew in a shared-nothing system. Our experimental results show that PRPD significantly speeds up query elapsed time in the presence of data skew. Our experience shows that eliminating system bottlenecks caused by data skew improves the throughput of the whole system which is important in parallel data warehouses that often run high concurrency workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Parallel processing continues to be important in large data warehouses. The processing requirements continue to expand in multiple dimensions. These include greater volume, increasing number of concurrent users, more complex queries, and more applications which define complex logical, semantic, and physical data models.</p><p>In a shared nothing architecture, multiple nodes communicate via high-speed interconnect network and each node has its own private memory and disk(s). In current systems, there are usually multiple virtual processors (collections of software processes) running on each node to take advantage of the multiple CPUs and disks available on each node for further parallelism. These virtual processors, responsible for doing the scans, joins, locking, transaction management, and other data management work, are called Parallel Units (PUs) in this paper.</p><p>Relations are usually horizontally partitioned across all PUs which allows the system to exploit the I/O bandwidth of multiple disks by reading and writing them in parallel. Hash partitioning is commonly used to partition relations across all PUs. Tuples of a relation are assigned to a PU by applying a hash function to their Partitioning Column. This Partitioning Column is one or more attributes from the relation, specified by the user or automatically chosen by the system.</p><p>As an example, Figure <ref type="figure" target="#fig_0">1</ref> shows the partitioning of two relations R(x, a) and S(y, b) on a three-PU system, assuming that the partitioning columns are R.x and S.y respectively, and that the hash function h is h(i) = i mod 3 + 1. The hash function h places any tuple with the value i in the partitioning column on the h(i)-th PU. For example, a tuple (x = 3, a = 1) of R is placed on the first PU since h(3) = 1. The fragment of R (or S) on the i-th PU is denoted as R i (or S i ).</p><p>Conventionally, in a shared nothing parallel system, there are two join geographies to evaluate R R.a=S.b 1 S. The first join geography is called the redistribution plan and the second is called the duplication plan. Both plans consist of two stages.</p><p>In the first stage of the redistribution plan, when neither R.a or S.b is the partitioning column, both R and S are redistributed based on the hash values of their join attributes so that matching rows are sent to the same PUs <ref type="foot" target="#foot_0">1</ref> . This redistribution in the first stage of the redistribution plan is called hash redistribution. For example, Figure <ref type="figure" target="#fig_1">2</ref> shows the result of the first stage after hash redistributing both R and S on R.a and S.b respectively. R i redis (or S i redis ) denotes the spool on the i-th PU that contains all rows of R (or S) hash redistributed to the i-th PU from all PUs. These include rows from R i (S i ). Obviously, if a relation's join attribute is its partitioning column, there is no need to hash redistribute that relation. When both relations' join attributes are their partitioning columns, the first stage of the redistribution is not needed.</p><p>In the first stage of the duplication plan, tuples of the smaller relation on each PU is duplicated (broadcast) to all PUs so that each PU has a complete copy of the smaller relation. As an example, Figure <ref type="figure">3</ref> shows the result of duplicating S in Figure <ref type="figure" target="#fig_0">1</ref> to every PU.</p><p>In the second stage of both plans, the join operation is performed on each PU in parallel. This can be done since the first stage has put all matching rows from the join relations on the same PUs.</p><p>Research has shown that the redistribution plan has nearlinear speed-up on shared nothing systems under even balancing conditions <ref type="bibr" target="#b6">[6]</ref>. However, if we use the redistribution</p><formula xml:id="formula_0">plan to evaluate R R.a=S.b</formula><p>1 S and if one relation (R) has many rows with the same value v in the join attribute (R.a), one PU will receive all these rows. This PU will become hot and can be the performance bottleneck in the whole system. The value v is called a skewed value of R in R.a. Any row of R containing a skewed value v is called a skewed row. Figure <ref type="figure" target="#fig_1">2</ref> shows that the number of rows of R the second PU receives is 4 times that of any other PU. Obviously the more skew in the data, the hotter the hot PU will become.</p><p>Adding more nodes to the system will not solve the skew problem because all skewed rows will still be sent to a single PU. This will only reduce the parallel efficiency, since adding more nodes will make each non-hot PU colder (having fewer rows) and make the hot PU comparatively even hotter. The type of data skew demonstrated in Figure <ref type="figure" target="#fig_1">2</ref> is categorized as redistribution skew in <ref type="bibr" target="#b17">[17]</ref> and is what we study in this paper.</p><p>Redistribution skew could be the result of a poorly designed hash function. However, theoretical research on hashing offers a class of good universal hash functions <ref type="bibr" target="#b4">[4]</ref> that perform well with a high probability. The more fundamental problem comes from naturally occurring skewed values in the join attributes.</p><p>We have seen redistribution skew in many types of industrial applications. For example, in the travel booking industry, a big customer often makes a large number of reservations on behalf of all of its end users. In online e-commerce, a few professionals make millions of transactions a year while the vast majority of the other customers only do a few a year. In telecommunication, some phone numbers used for telemarketing make a huge number of phone calls while most customers make a few calls daily. In retail, some items sell far more than other items. Usually the relations in these  applications are almost evenly partitioned by their unique transaction ids. However, when the join attribute is a nonpartitioning column attribute (like customer id, phone number or item id), severe redistribution skew happens. This can have a disastrous effect on system performance. Out of spool space errors, which cause queries to abort (often after hours of operation in large data warehouses), can also happen in the presence of severe redistribution skew. This is because although disk continues to become larger and cheaper, parallel DBMSes still maintain spool space quotas for users on each PU for the purpose of workload management and concurrency control. Sometimes, a system can choose the duplication plan instead of the redistribution plan to alleviate the skew problem. This works in very limited cases when one join relation is fairly small. Though many algorithms have been proposed in the research community, to our best knowledge, no effective skew handling mechanism has been implemented by major parallel DBMS vendors, either because of their high implementation complexity or communication cost, or the significant changes required to the current systems.</p><p>We make the following contributions in the paper:</p><p>• We propose a practical and efficient join geography called Partial Redistribution &amp; Partial Duplication (PRPD) to handle data skew in parallel joins motivated by real business problems.</p><p>• The PRPD join geography does not require major changes to the current implementations of a shared-nothing architecture. • Our experiments show the efficiency of the proposed PRPD join geography.</p><p>The rest of the paper is organized as follows. In Section 2 we present the PRPD join geography. Section 3 discusses how the PRPD geography is applied to multiple joins. Section 4 shows the experimental results. Section 5 discusses related work. Section 6 concludes the paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PARTIAL REDISTRIBUTION PARTIAL DUPLICATION (PRPD)</head><p>In this section we present the PRPD join geography, starting with an observation and an intuition behind the PRPD approach. We notice that parallel system DBAs and application writers are well educated on the importance of choosing good partitioning columns to evenly partition their data for efficient parallel processing, usually through required certification or training. Parallel DBMSes provide tools to show how data is partitioned and, if necessary, to help users change the partitioning columns. In the absence of other good choices, tables can be defined using identity columns (which automatically generate unique values) as partitioning columns. Thus, in practice, large relations are most likely to be evenly partitioned.</p><p>Consider the join R R.a=S.b 1 S. Assume R is evenly partitioned, R.a is not the partitioning column of R, and R has many skewed rows in R.a. The key observation is that the skewed rows of R tend to be evenly partitioned on each PU. The intuition behind the PRPD join geography is to deal with the skewed rows and non-skewed rows of R differently. PRPD keeps the skewed rows of R locally on each PU (instead of hash redistributing them all to a single PU), and duplicates those matching rows from S to all PUs. PRPD uses the redistribution plan for other rows of R and S. In Section 2.1 we discuss the PRPD join geography. Section 2.2 compares PRPD with the redistribution plan and the duplication plan. Section 2.3 discusses how PRPD handles unusual cases where skewed rows are not evenly partitioned on all PUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRPD description</head><p>Assume the optimizer chooses the PRPD join geography to join R R.a=S.b 1 S. For simplicity, we will use the terms PRPD join geography and the PRPD plan interchangeably. We first assume that the system knows the set of skewed values L1 in R.a and the set of skewed values L2 in S.b and that neither R.a nor S.b is the partitioning column. We will show later how PRPD works when only one relation is skewed or when one relation's partitioning column is the join attribute.</p><p>There are three steps in the PRPD plan. Assume there are n PUs in the system.</p><p>• Step 1 -1) On each P Ui, scan R i once and split the rows into three sets. The first set, named R i 2-loc , contains all skewed rows of R i for any value in L1. R i 2-loc is kept locally. The second set, named R i 2-dup , contains every row of R i whose R.a value matches any value in L2 and is duplicated to all PUs. The third set, named R i 2-redis contains all other rows in R i and is hash redistributed on R.a.</p><formula xml:id="formula_1">Note that R i 2-loc , R i 2-dup and R i 2-redis disjointly partition R i on each PU.</formula><p>On each P Ui, three receiving spool files R i redis , R i dup , and R i loc are created. The first spool R i redis receives all rows of R redistributed to the i-th PU from any PU including P Ui itself. That is, R i redis contains any row from any R j 2-redis (1 ≤ j ≤ n) which is redistributed to the i-th PU by the system's hash function, h.</p><formula xml:id="formula_2">R i redis = {τ | h(τ.a) = i &amp; τ ∈ 1≤j≤n R j 2-redis }</formula><p>The second spool R i dup receives all rows of R duplicated to the i-th PU from any PU including</p><formula xml:id="formula_3">P Ui itself. That is, R i dup = 1≤j≤n R j 2-dup . No- tice that R i dup = R j dup = 1≤p≤n R p 2-dup for any i and j. The third spool R i loc receives all rows from R i 2-loc . Mathematically, R i loc = R i 2-loc .</formula><p>-2) Similarly, on each P Ui, scan S i once and split the rows into three sets. The first set, named S i 2-loc , contains all skewed rows of S i in any value in L2. S i 2-loc is kept locally. The second set, named S i 2-dup , contains every row of S i whose S.b value matches any value in L1 and is duplicated to all PUs. The third set, named S i 2-redis contains all other rows in S i and is hash redistributed on S.b. Note that S i 2-loc , S i 2-dup and S i 2-redis disjointly partition S i on each PU.</p><p>On each P Ui, three receiving spools S i redis , S i dup , and S i loc are also created, similarly to R i redis , R i dup , and R i loc . The definitions are shown below.</p><formula xml:id="formula_4">S i redis = {τ | h(τ.b) = i &amp; τ ∈ 1≤j≤n S j 2-redis } S i dup = 1≤j≤n S j 2-dup S i loc = S i 2-loc</formula><p>Note that in implementation, when a row of R or S is read, the system immediately determines whether to keep it locally, redistribute it or duplicate it based on the value in the join attribute.</p><p>The six sets</p><formula xml:id="formula_5">R i 2-redis , R i 2-dup , R i 2-loc , S i 2-redis , S i</formula><p>2-dup and S i 2-loc are not materialized and are used only for presentation purpose. Only the six receiving spools R i redis , R i dup , R i loc , S i redis , S i dup and S i loc are materialized and used in the join operations shown in the next step.</p><p>• Step 2 On each P Ui do the following three joins,</p><formula xml:id="formula_6">-1) R i redis a=b 1 S i redis . -2) R i loc a=b 1 S i dup . -3) R i dup a=b 1 S i loc . • Step 3</formula><p>The final result is the union of the three joins from</p><p>Step 2 on all PUs.</p><p>Notice that all sub-steps in each step can run in parallel and they usually do. For example, the sub-steps 1.1 and 1.2 in the first step can run in parallel, and so can the three joins in the second step.</p><p>In Step 1, the skewed values in R.a and S.b together determine how to split R and S into different spools. When a value v appears in both the skewed values in R.a and S.b, we can not duplicate them in both relations. Our solution dealing with overlapping skewed values is described below. When invoking the PRPD plan, the system always invokes the PRPD join plan with two non-overlapping sets L1 (of skewed values in R) and L2 (of skewed values in S). The systems chooses to include the skewed value v (in both relations) in only one of L1 and L2, instead of in both. By doing this, it is guaranteed that R i 2-loc , R i 2-dup and R i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-redis</head><p>disjointly partition R i on each PU, and S i 2-loc , S i 2-dup and S i 2-redis disjointly partition S i on each PU. The question is to decide which set (L1 or L2) should include v. Assume the number of skewed rows of R (whose R.a value is v) times the projected row size of R is bigger than the number of skewed rows of S (whose S.b value is v) times the projected row size of S, then the system chooses to include v in L1 to keep these skewed rows of R locally in R i loc , and those rows of S whose join attribute value is v are duplicated to S i dup . When only one relation is skewed, the first step produces two receiving spools for each relation instead of three. Assume only R is skewed. The first step will produce only two receiving spools R i loc and R i redis for R, and S i dup and S i redis for S. R i dup and S i loc are not created since there is no skewed value in S. Consequently, the second step will only need to perform the first two joins. In Step 2, we allow the optimizer to choose the best physical join method for each of the three joins. The optimizer may choose different join methods for each of the three joins.</p><p>When one relation's (assume it is S) join attribute is also its partitioning column, the PRPD approach works logically in the same way. The one difference is that each PU will keep locally those rows of S whose join attribute values are not skewed in R, rather than redistribute them to any other PUs since S.b is the partitioning column. Although the redistribution plan does not need to redistribute S, skewed rows in R will be still sent to a single PU, causing system bottleneck. In this case, PRPD can outperform the redistribution plan just as in the case where neither relation's join attribute is the partitioning column.</p><p>Overall, when R is skewed, the skewed rows of R are kept locally, the matching rows in S for these skewed rows are duplicated to all PUs, and all other rows from both relations are redistributed. Part of S is redistributed and part of S is duplicated. When S is also skewed, symmetric actions are taken. Thus we named the proposed approach PRPD (partial redistribution and partial duplication).</p><p>The basic idea of the PRPD plan is illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. The new operator Split reads each row and either keeps it locally, redistributes it or duplicates it according to its value in the join attribute and the set of skewed values in R and S. As an example, Figure <ref type="figure" target="#fig_4">5</ref>(a) shows how the PRPD plan is going to place each row of R and S at each PU for the example data shown in Figure <ref type="figure" target="#fig_0">1</ref>. Figure <ref type="figure" target="#fig_4">5(b)</ref> shows the data placement after applying the PRPD plan. Comparing Figure <ref type="figure" target="#fig_4">5</ref>(b) with Figure <ref type="figure" target="#fig_1">2</ref>, we can see the figures visually show that the processing at each PU in the PRPD plan is even and there is no hot PU.</p><p>Appendix A provides the correctness proof of the PRPD plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PRPD, redistribution plan and duplication plan comparison</head><p>The Partial Duplication part of the PRPD plan is not in the redistribution plan, therefore causes PRPD to use more total spool space than the redistribution plan. On the other hand, the Partial Redistribution part of the PRPD plan keeps the skewed rows locally and has less networking communication cost than the redistribution plan. A more important difference between the PRPD plan and the redistribution plan is that PRPD does not send all skewed rows to a single PU and does not cause a system bottleneck.</p><p>The Partial Duplication part of the PRPD plan may make PRPD use less total spool space than the duplication plan since not every row of a relation is duplicated. On the other ), the set of unique values in S.b, then PRPD will behave exactly the same way as the duplication plan since every row of R will be kept locally (because every value in R.a is a skewed value) and every row of S will be duplicated. If L1 is equal to Uniq(R.a), but is not a superset of Uniq(S.b), then PRPD will keep every row of R locally, duplicate some rows of S and hash redistribute some rows of S. In this case those hash redistributed rows of S will not match any rows of R. If L1 is a superset of Uniq(S.b) and a subset of Uniq(R.a), then PRPD will duplicate every row of S, keep some rows of R locally, and hash redistribute some rows of R. In this case those hash redistributed rows of R will not match any rows of S. Neither hash redistribution in the last two cases is necessary. However unless the optimizer knows in advance the relationships among L1, Uniq(R.a) and Uniq(S.b), the system has no way to eliminate it.</p><p>Assume the number of PUs in the system is n, x is the percentage of the skewed rows (skewness) in a relation R.</p><p>The number of rows of R in the hot PU after redistribution in the redistribution plan is roughly x|R|+ (1-x)|R| n while the number of rows of R on a non-hot PU is roughly (1-x)|R| n . Thus, the ratio of skewed rows of R on the hot PU over the number of rows of R on a non-hot PU is roughly 1 + nx 1-x . In the PRPD plan, after the first stage, the number of rows of R on each PU is roughly</p><formula xml:id="formula_7">x|R| n + (1-x)|R| n = |R| n .</formula><p>Thus, the ratio of the number of rows of R on the hot PU in the redistribution plan over the number of rows of R on any PU in the PPRD plan after the first stage is roughly 1+(n-1)x.</p><p>The ratio depends on the skewness of R and the size of the system (the number of PUs in the system), but does not depend on the size of the relation R itself. The ratio is nearly linear to the size of the system and the skewness of R.</p><p>The ratio is one important factor determining the speedup ratio that PRPD can achieve over the redistribution plan.</p><p>As an example, for a small system where n = 100 (4 nodes each running 25 PUs), the ratio is 2 for x = 1%, 3 for x = 2%. For a medium size system where n = 500 (20 nodes each running 25 PUs), the ratio is 6 for x = 1%, 11 for x = 2%. For a large system where n = 5000 (200 nodes each running 25 PUs), the ratio is 51 for x = 1% and 103 for x = 2%. We include in Section 4 experiments exploring the impact of the size of the system and data skewness on the speedup ratio of PRPD over the redistribution plan.</p><p>One important issue is how the optimizer chooses PRPD over other plans. As usual, a cost based optimizer will choose the PRPD plan over other plans only when it determines the cost of applying PRPD is smaller. We will leave out the discussion on how the optimizer computes and compares the costs of PRPD, redistribution plan and duplication plan, which is beyond the scope of this paper.</p><p>In addition to using statistics, the optimizer can also use sampling to find skewed values as sampling is a negligible cost in query response time (with or without data skew) as shown in <ref type="bibr">[7,</ref><ref type="bibr" target="#b8">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Handling unevenly partitioned skewed rows in PRPD</head><p>The PRPD join plan presented so far works on the assumption that skewed rows are roughly evenly partitioned on all PUs. This section discusses how the PRPD plan handles unusual cases where the skewed rows are only on one or a few PUs.</p><p>Consider the join R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R.a=S.b</head><p>1 S and R is skewed in its join attribute. Consider a case where all the skewed rows of R are on a small number of PUs, called skewed PUs in this section. Though the PRPD plan will not cause redistribution skew itself since all skewed rows are kept locally, these skewed PUs will become hot PUs since in addition to the skewed rows they already have, they will receive redistributed rows from other PUs. Consider the skewed PU P that contains the most number of skewed rows among all PUs. Assume R is evenly partitioned, the number of PUs is n and the skewness of R is x. With PRPD, P will keep all of its skewed rows and receive some non-skewed rows sent from other PUs. The number of rows on P after the first step in PRPD is roughly</p><formula xml:id="formula_8">min( |R| n , x|R|) + |R|(1-x) n</formula><p>where the function min(a, b) returns the smaller number from a and b. The PRPD plan is slightly modified to handle skewed PUs cases. Instead of keeping the skewed rows of R locally on each PU, we randomly redistribute them to all PUs so that skewed rows are evenly redistributed to all PUs. There is no change on handling S. Those rows of S that contain skewed values in R are still duplicated to every PU. With this change in PRPD, the skewed PU P will now have |R| n rows of R after the first step in the modified PRPD plan. The ratio of the number of rows on the skewed PU P using the original PRPD plan over the number of rows on the same PU P using the modified PRPD plan is min( |R| n , x|R|)</p><formula xml:id="formula_9">+ |R|(1-x) n / |R| n = min(1, nx) + (1 -x). The ratio is 1 when x = 0 or x = 1. When nx ≥ 1, the ratio is 2-x. Notice that nx = x|R| |R| n</formula><p>is the minimal number of PUs needed to contain all skewed rows in R , assuming R is evenly partitioned. The ratio shows that redistributing the skewed rows to all PUs in the case where skewed rows are on only one or a few PUs can partition the skewed relation in preparation for the joins in the second stage better than keeping all skewed rows locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPLYING PRPD IN MULTIPLE JOINS</head><p>In this section we discuss how the PRPD join geography is applied in multiple joins. Intuitively, the fact that not all rows in the join result from applying the PRPD plan are partitioned by the join attributes may be an issue in multiple joins.</p><p>In this section we show how the PRPD plan can be mixed with other conventional plans in multi-table join queries. We will not discuss join ordering and planning with the introduction of PRPD, which is beyond the scope of this paper. We focus on how the join result from applying the PRPD plan can be joined with other relations using different join geographies.</p><p>Consider a query that joins three relations R, S and Y and the join condition on R and S is R.a = S.b. Assume the optimizer chooses to first join R and S. Let X be the result of joining R and S. There are 6 possible cases in terms of join geography that may be used to join X and Y before the introduction of PRPD. For each of X and Y , we can keep it locally, redistribute it or duplicate it. Thus, there are 9 combinations for X and Y . However, when one relation is duplicated, the other relation does not need to be duplicated or redistributed. The 6 possible combinations are shown in Figure <ref type="figure" target="#fig_5">6</ref> where each line denotes one possible case described below.</p><p>• Case 1: duplicate X; keep Y locally. Now let us consider the impact of joining R and S using PRPD in the context of multi-table joins. Assume the optimizer has chosen PRPD to join R and S. The skew handling using PRPD has no impact in the first four cases, in the sense that the system can execute the specified join geography in each case as usual. In cases 5 and 6, we cannot simply keep X locally anymore when PRPD has been applied. Certainly we can redistribute X again on R.a or S.b to join X and Y. However we do not need to redistribute every row in X since some rows have been already redistributed to the right PUs in PRPD. We just need to redistribute the results from joining those rows that were kept locally in PRPD.</p><p>PRPD is modified to work in multi-table joins as follows. When the optimizer identifies that the join geography of joining X and Y is either case 5 or 6, it uses the following modified PRPD step with the changes highlighted.</p><p>Step 2</p><p>• 1) Join R i redis and S i redis .</p><p>• 2) Join R i loc and S i dup . Hash Redistribute the result on R.a</p><p>• 3) Join R i dup and S i loc . Hash Redistribute the result on R.a When considering the impact of PRPD in multiple joins, the optimizer takes into account of the cost of the extra redistribution step described above that is necessary for cases 5 and 6.</p><p>Note that these six cases are all based on traditional join geographies. PRPD introduces one more choice for the optimizer in joining X and Y in multi-table joins. With PRPD, another choice is to use the PRPD plan again to join X and Y . Indeed, this is a likely choice for the optimizer for cases 5 and 6.</p><p>Consider the case where R.a has skewed values and S is not skewed (called "single-skew" in <ref type="bibr" target="#b14">[14]</ref>, which is also the case most previous work focuses on). If the tuples in R with a skewed value v1 match some tuples in S and either R.a or S.b is the join attribute in the second join (as in case 5 or case 6), v1 tends to be a skewed value in X as well, as shown below. Assume the set of unique values that appear in both R.a and S.b is {v1, . . . , v k }. Let mi and ni be the number of tuples containing vi in R.a (in R) and in S.b (in S) respectively (1 ≤ i ≤ k). Assume v1 is a skewed value in R.a. The number of tuples in the result of joining R and S with the value v1 in the join attribute is: m1n1. The total number of tuples in the join result is: k i=1 mini. Thus, the percentage of tuples in the join result containing v1 in the join attribute is:</p><formula xml:id="formula_10">δ2 = m1n1 k i=1 mini</formula><p>Let δ1 be the skewness of v1 in R (the percentage of rows in R containing v1 in R.a) and nmax be the maximal value in </p><formula xml:id="formula_11">δ2 = m1n1 k i=1 mini ≥ m1n1 k i=1 minmax = n1 nmax m1 k i=1 mi ≥ n1 nmax m1 |R| = n1 nmax δ1</formula><p>The above equation shows that when v1 is a skewed value in R, v1 tends to be a skewed value in the join result as well, especially when S is not skewed and n1 is close to nmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>In this Section, we are mainly interested in comparing the performances of the PRPD plan and the redistribution plan because the redistribution plan is more widely used in large data warehousing systems than the duplication plan. Since duplicating rows is expensive in terms of both I/O and CPU cost on all PUs and the network bandwidth, the larger the number of PUs in the system and the larger the join relations, the less likely the duplication plan can outperform the redistribution plan or the PRPD plan. However, when one join relation is fairly small, the duplication plan can outperform the other two plans, and we have seen this in our experiments. In this Section, we only report the experiments comparing PRPD and the redistribution plan.</p><p>We use the TPC-H <ref type="bibr" target="#b1">[1]</ref> and the following query in our experiments. select * from CUSTOMER, SUPPLIER Q1 where C NATIONKEY = S NATIONKEY The schema of the Customer and Supplier relations are shown in Figure <ref type="figure">7</ref>.</p><p>Originally there are only 25 unique nations in the TPC-H database. We have increased the number of unique nations to 1000 to facilitate our skew experiments. To control data skewness in the Customer relation's join attribute (C NATIONKEY), we randomly choose a portion of the data, and change their C NATIONKEY values to one value. For example, in our experiments, if the skewness is 5% in the Customer relation's join attribute, this means that we have used the following SQL statement to change 5% of the rows of the Customer relation to have the same value (24) in the join attribute.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Query execution time</head><p>The test system we use for the experiments has 10 nodes and 80 PUs. Each node has four Pentium IV 3.6 GHz CPUs, 4 GB memory and 8 PUs. We generate 1 million rows for the Supplier relation, and 1 million rows for the Customer relation and we vary the data skewness in the Customer relation. The size of the query result is around 1 billion rows. Figure <ref type="figure">8</ref> shows the execution times using the PRPD plan and the redistribution plan. When the skewness increases, the execution time of the redistribution plan grows almost linearly because all skewed values are redistributed to one PU and that PU becomes the system bottleneck while the execution time of the PRPD plan essentially stays the same.</p><p>We consider the case where there are two skewed values and they cause two hot PUs in the redistribution plan (rows with different skewed values in the join attribute are hash redistributed to two different PUs). We report the results in Figure <ref type="figure">9</ref> where both skewed values have the same skewness shown on x-axis. Figures 9 shows the same performance relationship between the two plans as in Figure <ref type="figure">8</ref>. We also consider the case where both relations have skewed values. The results are similar to what are shown in Figures <ref type="figure">8</ref> and<ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speedup ratio with different number of PUs</head><p>In this experiment, we calculate the speedup ratio of PRPD over the redistribution plan on three systems, 24 PUs, 56 PUs and 72 PUs respectively. The result is shown in Figure <ref type="figure" target="#fig_8">10</ref>. As can been from Figure <ref type="figure" target="#fig_8">10</ref>, when the skewness increases, the speedup ratio of PRPD over the redistribu-  tion plan increases. Also the larger the system, the larger the speedup ratio is.</p><p>We have also done experiments on joining two relations where one relation's partitioning column is the join attribute and we have seen similar performance relationships shown in Figures 8, 9 and 10. We note here that our experimental results on using PRPD in multi-table joins also show significant query performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Extensive research has been done on handling data skew in parallel DBMSes. <ref type="bibr" target="#b17">[17]</ref> describes four types of skew : tuple placement skew (TPS), selectivity skew (SS), redistribution skew (RS) and join product skew (JPS). TPS can be avoided with a good hash function. In the theoretical literature, hash functions have been extensively studied, and several techniques have been proposed to design a well-performed hash function <ref type="bibr" target="#b4">[4]</ref>. Furthermore, in practice partitioning columns are carefully chosen by DBAs so that tuples of large relations are most likely evenly partitioned on all PUs (parallel units). Thus, the possibility of TPS is extremely low and nearly no work focuses on TPS. SS is caused by different selectivity of selection predicates and is query-dependent. SS becomes a problem only when it causes RS or JPS. Most prior work does not consider SS either.</p><p>As discussed in Section 1, RS occurs when PUs receive different numbers of tuples when they are redistributed in preparation for the actual joins. JPS occurs when the join selectivity at each node differs, causing imbalance in the number of tuples produced at each PU. RS and JPS are closely related problems and have been heavily studied. Previous algorithms can be roughly classified into the following three categories. The first category of algorithms models the skew problem as the conventional task scheduling problem. For parallel joins, one or both join relations are partitioned into smaller units (ranges <ref type="bibr">[7]</ref> or hash buckets <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">5]</ref>) which are distributed to all PUs. Joins are computed in parallel locally. Since the task scheduling problem is NPcomplete, various well-known heuristics including LPT <ref type="bibr" target="#b9">[9]</ref> and MULTIFIT <ref type="bibr" target="#b12">[12]</ref> are used in this type of algorithms <ref type="bibr">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20]</ref>. Based on different cost models, different algorithms in the first category have been proposed.</p><p>The first type of models uses the number of tuples on each PU as the estimation of join cost. The goal is to make every PU receive similar number of tuples of the two join relations so that RS does not occur. A few range-partitioning based algorithms in <ref type="bibr">[7]</ref> handle the RS problem. The algorithms first find the skewed relation by sampling both relations, and then compute a split vector (different algorithms use different strategies to compute the split vector) to split the join attribute values in the skewed relation to a few ranges such that the number of ranges is equal to the number of PUs in the system. Tuples whose join attribute values fall within one range are all mapped (redistributed) to the same PU. The goal of the split vector is to make each PU receive similar number of tuples. If one value spans more than one range (which means it is a skewed value), the subset-replication algorithm in <ref type="bibr">[7]</ref> replicates (duplicates) the tuples in the other join relation with this join attribute value to all PUs that this value maps to. Notice that in the subset-replication algorithm skewed rows are usually redistributed to a few PUs according to the split vector, not to all PUs.</p><p>The second type of algorithms uses more sophisticated models ( <ref type="bibr">[7,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">5]</ref>). The Virtual Processor Scheduling (VPS) algorithm in <ref type="bibr">[7]</ref> deals with the JPS problem. It uses |R| + |S| + |R 1 S| (R and S are the two join relations) to estimate the cost of join on each PU. In <ref type="bibr" target="#b2">[2]</ref>, two functions, output function and work function are used to estimate join cost. <ref type="bibr" target="#b5">[5]</ref> considers parallel systems whose PUs may not have identical configurations.</p><p>Unlike the first category of algorithms which try to estimate the execution time before the actual join computation, the core idea of the second category of algorithms is to handle skew dynamically. Work load at each PU is monitored at run time. If a PU is doing much more work than others, some work load from this hot PU will be migrated to other PUs. <ref type="bibr" target="#b15">[15]</ref> uses shared virtual memory to migrate workload from hot PUs to idle PUs. <ref type="bibr" target="#b10">[10]</ref> detects the processing rate of the join relations on each PU. If a PU's processing rate is slow, this PU is deemed as hot. Two algorithms are introduced: result redistribution and processing task migration. In result redistribution, results generated by hot PUs are not stored locally but sent to other PUs' disk. In processing task migration, part of unprocessed relations on a hot PU is migrated to other idle PUs. <ref type="bibr" target="#b21">[21]</ref> proposes two algorithms: the single-phase and the two-phase scheduling algorithms. The single-phase algorithm is similar to <ref type="bibr">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b11">11]</ref>. In the twophase scheduling algorithm, a central coordinator is used to maintain a task pool. When a PU becomes idle, it picks up one task from the central pool.</p><p>The third category of algorithms uses semi-joins to more accurately estimate the size of joins and to reduce the cost of network communication. <ref type="bibr" target="#b3">[3]</ref> proposes two methods. The first method consists of two stages. In the first stage, it computes the histogram of one join relation (R). Based on the histogram, redistribution plan is computed such that each PU receives similar number of R's tuples. In this stage, rows with skewed values are randomly sent to other PUs. In the second stage, the other relation (S) is semi-joined with the histogram of R. The result of the semi-join is duplicated to all PUs and joined with R. The second method in <ref type="bibr" target="#b3">[3]</ref> is a hybrid approach. It uses the first method only for skewed values and uses other conventional algorithms for non-skewed values.</p><p>Compared with previous algorithms, PRPD has following characteristics. Skewed values are kept locally rather than redistributed to all (or some) PUs. This saves redistribution cost and decreases execution time. Unlike most previous work, PRPD not only deals with cases where only one relation is skewed, but also cases where both relations are skewed. PRPD also solves the type of JPS problems that result from redistribution skew, though it is mainly designed to solve the redistribution skew problem that we have seen in various industrial applications discussed in Section 1. One big advantage of the PRPD algorithm is that it does not require major changes to the implementations of the sharednothing architecture since it does not require any new type of central coordination or communication among PUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>One of the important challenges in parallel DBMSes is to effectively handle data skew in joins. Based on our observations of data warehouses in practice at various industries, we notice that skewed rows tend to be evenly partitioned on all parallel units. Motivated by the redistribution skew problem that arises naturally in business applications in the various industries, we propose an approach called PRPD (partial redistribution &amp; partial duplication). To identify the skewed values, PRPD uses either collected or sampled statistics. Instead of redistributing these skewed rows as is done for the non-skewed rows, they are kept locally. The matching skewed value rows from the joined table are duplicated to each parallel unit to complete the join. The PRPD algorithm also handles the extreme cases where skewed rows are clustered in only one or a few parallel units by redistributing the skewed rows to all parallel units.</p><p>Eliminating skewed processing eliminates system bottlenecks created by more conventional algorithms. Our experimental results have demonstrated the effectiveness of the PRPD algorithm in improving query execution time. We have also shown that the PRPD algorithm can be used in multiple joins. In addition, we are looking at handling skew dynamically to avoid reliance on collected or sampled statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. PRPD PROOF OF CORRECTNESS</head><formula xml:id="formula_12">(R i redis 1 S i redis ∪ R i loc 1 S i dup ∪ R i dup 1 S i loc )<label>(1)</label></formula><p>This is because both R and S are split into three disjoint sets, such that</p><formula xml:id="formula_13">R redis ∪ R loc ∪ R dup = R, S redis ∪ S loc ∪ S dup = S, R redis ∩ R loc = ∅, R redis ∩ R dup = ∅, R loc ∩ R dup = ∅,</formula><p>S redis ∩ S loc = ∅, S redis ∩ S dup = ∅, S loc ∩ S dup = ∅.</p><p>Since these sets of R and S are split by the join columns, the following joins all produce the empty set: R redis 1 S loc = ∅, R redis 1 S dup = ∅, R loc 1 S redis = ∅, R loc 1 S loc = ∅, R dup 1 S redis = ∅, R dup 1 S dup = ∅. and 1≤i≤n 1≤j≤n (R i redis 1 S j redis ) = n i=1 (R i redis 1 S i redis ) since R i redis 1 S j redis = ∅ for any i = j due to the fact that any two rows hash partitioned on different PUs by their join attributes cannot match.</p><p>Equation <ref type="bibr" target="#b1">(1)</ref> shows that we only need to do three joins on each PU though there may be three spools for R and three spools for S on each PU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two relations R and S on 3 Parallel Units. The partitioning columns are R.x and S.y respectively. The hash function, h(i) = i mod 3 + 1, places a tuple with value i in the partitioning column on the h(i)-th PU.</figDesc><graphic coords="2,322.92,213.45,226.85,75.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The first stage of the redistribution plan for R a=b 1 S : data placement after hash redistributing R and S on the join attributes (R.a and S.b) based on the hash function h(i) = i mod 3 + 1. R i redis (or S i redis ) denotes the spool on the i-th PU that contains all rows of R (or S) hash redistributed to the i-th PU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 1 S</head><label>31</label><figDesc>Figure 3: The first stage of the duplication plan for R a=b 1 S : data placement after duplicating S to every PU. The spool S dup on each PU has a complete copy of S.</figDesc><graphic coords="3,60.00,53.85,226.70,140.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: the PRPD plan for R a=b 1 S : R i 2-redis and S i 2-redis are hash redistributed on R.a and S.b respectively; R i 2-dup and S i 2-dup are duplicated; R i 2-loc and S i 2-loc are kept locally.</figDesc><graphic coords="3,322.92,53.91,226.85,161.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The PRPD join geography for R a=b 1 S. Assume the hash function is h(i) = i mod 3 + 1, 1 is the skewed value in R and 2 is the skewed value in S. (a) illustration of each tuple's future placement. R i 2-redis and S i 2-redis are to be hash redistributed on R.a and S.b. R i 2-dup and S i 2-dup to be duplicated; R i 2-loc and S i 2-loc</figDesc><graphic coords="5,53.76,54.18,510.30,96.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Join geographies for X 1 Y</figDesc><graphic coords="7,60.00,53.82,226.70,57.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>update CUSTOMER setC NATIONKEY = 24 Q2 where random(1,100) ≤ 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Two relations CUST OMER and SUP P LIER from TPC-H. The partitioning columns are C CUST KEY and S SUP P KEY respectively.</figDesc><graphic coords="7,365.52,54.11,141.70,100.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Speedup ratio of the PRPD plan over the redistribution plan</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Case 2: keep X locally; duplicate Y . Case 6: keep both X and Y locally when Y is a base table and the join attribute of Y is Y 's partitioning column or when Y is an intermediate result and has been redistributed on its join attribute; and when the join attribute of X is R.a or S.b.</figDesc><table><row><cell>• Case 3: redistribute X on its join attribute that is not</cell></row><row><cell>R.a or S.b; redistribute Y .</cell></row></table><note><p><p><p>• Case 4: redistribute X on its join attribute that is not R.a or S.b; keep Y locally when Y is a base table and the join attribute of Y is Y 's partitioning column or when Y is an intermediate result and has been redistributed on its join attribute.</p>• Case 5: redistribute Y , keep X locally when the join attribute of X is R.a or S.b.</p>•</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Query execution time, two skewed val- ues with the same skewness shown on x-axis in the Customer relation causing two hot PUs in the redis- tribution plan</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>700</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time (S)</cell><cell>300 400 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>5%</cell><cell>10%</cell><cell>15%</cell><cell>20%</cell><cell>25%</cell><cell>30%</cell><cell>35%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Skewness</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PRPD</cell><cell cols="2">Redistribution</cell><cell></cell><cell></cell></row><row><cell cols="2">2.00 4.00 12.00 14.00 16.00 Figure 9: 0.00 6.00 8.00 10.00 Speedup Ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>10%</cell><cell></cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell></cell><cell>50%</cell><cell>60%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Skewness</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">72 PUs</cell><cell></cell><cell cols="2">56 PUs</cell><cell></cell><cell cols="2">24 PUs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This section proves the correctness of the PRPD plan.Let S = (R redis ∪ R loc ∪ R dup ) 1 (S redis ∪ S loc ∪ S dup ) = (R redis 1 S redis ) ∪ (R redis 1 S loc ) ∪ (R redis 1 S dup ) ∪ (R loc 1 S redis ) ∪ (R loc 1 (S loc ) ∪ (R loc 1 S dup ) ∪ (R dup 1 S redis ) ∪ (R dup 1 S ) ∪ (R dup 1 S dup ) = (R redis 1 S redis ) ∪ (R loc 1 S dup ) ∪ (R dup 1 S loc )</figDesc><table><row><cell cols="3">R redis = n i=1 R i redis , R loc = n i=1 R i loc , R dup = R i dup ∀1 ≤ i ≤ n, S dup = S i S redis = n i=1 S i redis , S loc = n i=1 S i loc , dup ∀1 ≤ i ≤ n.</cell></row><row><cell>We have</cell><cell></cell><cell></cell></row><row><cell>R 1 =</cell><cell>(R i redis 1 S j redis ) ∪</cell><cell>(R i loc 1 S dup )∪</cell></row><row><cell>1≤i≤n</cell><cell>1≤i≤n</cell><cell></cell></row><row><cell>1≤j≤n</cell><cell></cell><cell></cell></row><row><cell cols="2">(R dup 1 S i loc )</cell><cell></cell></row><row><cell>1≤i≤n</cell><cell></cell><cell></cell></row><row><cell>n</cell><cell></cell><cell></cell></row><row><cell>=</cell><cell></cell><cell></cell></row><row><cell>i=1</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The base relations are not changed, only copies of the projected rows are redistributed for the evaluation of this query.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.tpc.org" />
		<title level="m">TPC Benchmark H (decision support) standard specification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skew-insensitive parallel algorithms for relational join</title>
		<author>
			<persName><forename type="first">K</forename><surname>Alsabti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HIPC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frequency-adaptive join for shared nothing machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bamha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hains</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in computer research</title>
		<imprint>
			<biblScope unit="page" from="227" to="241" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal classes of hash functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictive dynamic load balancing of parallel hash-joins over heterogeneous processors in the presence of data skew</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PDIS</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel database systems: the future of high performance database systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="85" to="98" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical skew handling in parallel joins</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sampling from databases: a survey</title>
		<author>
			<persName><forename type="first">Doronȃrotem</forename><surname>Frankȃolken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bounds on multiprocessing timing anomalies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="429" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic join product skew handling for hash-joins in shared-nothing database systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DASFAA</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="246" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Handling data skew in multiprocessor database computers using partition tuning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="525" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An application of bin-packing to multiprocessor scheduling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G C</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the super database computer (sdc)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kitsuregawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ogawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effectiveness of parallel joins</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="424" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using shared virtual memory for parallel join processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shatdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The case for shared nothing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Database Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="9" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A taxonomy and performance model of data skew effects in parallel joins</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Jenevein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="537" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A parallel sort merge join algorithm for managing data skew</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An effective algorithm for parallelizing hash joins in the presence of data skew</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">New algorithms for parallelizing relational database joins in the presence of data skew</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="990" to="997" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handling data skew in parallel hash join computation using two-phase scheduling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Orlowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 1st International Conference on Algorithm and Architecture for Parallel Processing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
