<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biobjective Scheduling Algorithms for Execution Time-Reliability Trade-off in Heterogeneous Computing Systems *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Atakan</forename><surname>Do Gan</surname></persName>
							<email>atdogan@anadolu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Anadolu University</orgName>
								<address>
									<postCode>26470</postCode>
									<settlement>Eskişehir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Füsun</forename><surname>Özgüner</surname></persName>
							<email>ozguner@ece.osu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>2015 Neil Avenue Columbus</addrLine>
									<postCode>43210-1272</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Biobjective Scheduling Algorithms for Execution Time-Reliability Trade-off in Heterogeneous Computing Systems *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">022CB1005D643E3B538753BA8455A45E</idno>
					<idno type="DOI">10.1093/comjnl/bxh086</idno>
					<note type="submission">Received 10 April 2004; revised 12 November 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A heterogeneous computing (HC) system is composed of a suite of geographically distributed high-performance machines interconnected by a high-speed network, thereby providing high-speed execution of computationally intensive applications with diverse demands. In HC systems, however, there is a possibility of machine and network failures and this can have an adverse impact on applications running on the system. In order to decrease the impact of failures on an application, matching and scheduling algorithms must be devised which minimize not only the execution time but also the failure probability of the application. However, because of the conflicting requirements, it is not possible to minimize both at the same time. Thus, the goal of this paper is to develop matching and scheduling algorithms which account for both the execution time and the failure probability and can trade off execution time against the failure probability of the application. In order to attain these goals, a biobjective scheduling problem is first formulated and then two different algorithms, the biobjective dynamic level scheduling algorithm and the biobjective genetic algorithm, are developed. Unique to both algorithms is the expression used for computing the failure probability of an application with precedence constraints. The simulation results confirm that the proposed algorithms can be used for producing task assignments where the execution time is weighed against the failure probability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Heterogeneous computing (HC) systems are among the emerging platforms for executing computationally intensive applications with diverse computing needs. An HC system is formed by interconnecting a collection of geographically distributed dissimilar machines by a high-speed network. However, there are still many challenges involved in building a functional HC system, e.g. achieving high-speed communication over a wide-area network, developing a middleware that enables a number of users to access all the available resources and services of the system in a transparent and efficient way, matching and scheduling problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">2]</ref>. In particular, the matching and scheduling problem, which is the problem of minimizing the makespan of an application by scheduling its tasks to machines in the system, is an NPhard problem and one of the challenges that has been studied * A preliminary version of this paper was published in 2001 International Parallel and Distributed Processing Symposium (IPDPS'01). extensively in the literature. Many static, dynamic and even hybrid algorithms have been proposed to minimize the execution time of applications with precedence constraints running on an HC system, e.g. <ref type="bibr" target="#b8">[3,</ref><ref type="bibr" target="#b9">4,</ref><ref type="bibr" target="#b10">5,</ref><ref type="bibr">6]</ref>.</p><p>Although advances in computer and networking technologies have made it possible to apply heterogeneous computing at a global scale, machine and network failures are inevitable in any large network of machines. An experiment conducted in <ref type="bibr" target="#b12">[7]</ref> found that the estimated value of the mean time-tofailure of a machine connected to a local area network ranged from 4 to 33 days. In the literature several techniques have been developed to reduce the adverse effect of failures on applications executing on a distributed system. One approach is to employ a reliable scheduling algorithm in which the tasks of an application are assigned to machines in such a way that the failure probability of the application is minimized. The reliable scheduling technique has been pursued in allocating distributed programs, e.g. <ref type="bibr" target="#b13">[8,</ref><ref type="bibr">9,</ref><ref type="bibr">10]</ref>, undirected task graphs, e.g. <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b18">13]</ref> and directed acyclic task graphs, e.g. <ref type="bibr" target="#b19">[14,</ref><ref type="bibr" target="#b20">15]</ref> The Computer Journal Vol. 48 No. <ref type="bibr" target="#b8">3,</ref><ref type="bibr">2005</ref> to distributed systems. In addition, reliability has been considered for real-time systems, e.g. <ref type="bibr" target="#b21">[16,</ref><ref type="bibr" target="#b22">17]</ref>.</p><p>It was shown in <ref type="bibr" target="#b23">[18]</ref> that a scheduling algorithm that minimizes only the schedule length may lead to a high failure probability, and that a reliable scheduling algorithm that minimizes only the failure probability may yield a high schedule length for an application running on an HC system. This result implies that a scheduling algorithm must account for both the execution time and the failure probability of an application. In addition, there are usually conflicting requirements between minimizing the execution time and the failure probability of an application, and it may not be possible to simultaneously minimize both. Consequently, a scheduling algorithm must be capable of balancing the execution time and failure probability of the application, i.e. it should be able to produce task assignments whereby the execution time is decreased at the expense of higher failure probability or vice versa. Unfortunately, there are few algorithms in the literature that address the problem of minimizing both the execution time and the failure probability of applications, or the problem of trading execution time for failure probability.</p><p>In <ref type="bibr" target="#b21">[16]</ref> and similar studies which attempt to schedule application tasks with timing constraints, the primary goal is first to satisfy each task's timing constraint (deadline). Then, among the machines on which the task's deadline can be met, the task is scheduled to a machine where the failure probability of the application is minimized. Thus, in the framework of <ref type="bibr" target="#b21">[16]</ref> and similar studies, the execution time of the application is not of any concern. In addition, even though there are two objectives, they are not considered simultaneously during the scheduling. Finally, there is no trading, i.e. missing the deadline of a task is not traded off against a lower failure probability of the application.</p><p>The study in <ref type="bibr" target="#b22">[17]</ref> is unique in that scheduling decisions are based on maximizing an objective function that is a multiplication of two objective functions. These two objective functions are the probability that all tasks are completed before their respective deadlines and the reliability of the application. To optimize this composite objective function, an optimal algorithm is proposed in <ref type="bibr" target="#b22">[17]</ref>. The main difference between the current study and <ref type="bibr" target="#b22">[17]</ref> is the fact that minimizing execution time rather than meeting task deadlines is considered as one of the objectives here. Furthermore, the optimal algorithm of <ref type="bibr" target="#b22">[17]</ref> seems to be too slow to be used for problems of practical size.</p><p>For applications with precedence constraints executing on an HC system, <ref type="bibr" target="#b24">[19]</ref> is the first study to address the problem of minimizing both the execution time and the failure probability at the same time. In <ref type="bibr" target="#b24">[19]</ref>, the network topology of the HC system was assumed to be a tree. The method of <ref type="bibr" target="#b24">[19]</ref> was extended to general network topologies in <ref type="bibr" target="#b25">[20]</ref>, which is the most relevant study to our own.</p><p>In summary, the current study is motivated by the fact that there is a trade-off between minimizing execution time and minimizing the failure probability of an application, and a scheduling algorithm needs to be developed to trade off between these two objectives. Since there are two conflicting objectives, a biobjective scheduling problem is first formulated in Section 3, where the first objective is the schedule length and the second is the failure probability. In order to compute the schedule length and failure probability of an application with precedence constraints under a given task assignment, two mathematical models are derived in Sections 3.1 and 3.2. The model used to compute the failure probability is unique in the literature in that it is specifically formulated for HC systems with arbitrary network topology and that it is computationally efficient in estimating the failure probability. We should note here that the same two objectives are also considered in <ref type="bibr" target="#b25">[20]</ref>. However, <ref type="bibr" target="#b25">[20]</ref> does not provide a rigorous formulation of the biobjective scheduling problem as in Section 3. Furthermore, <ref type="bibr" target="#b25">[20]</ref> proposes only a method to estimate the reliability of the communication between two machines, whereas the mathematical model presented in this study estimates the reliability of an application with precedence constraints. Mathematical models similar to the one developed here can be found in the literature, e.g. <ref type="bibr" target="#b19">[14,</ref><ref type="bibr" target="#b21">16,</ref><ref type="bibr" target="#b22">17]</ref>. Studies <ref type="bibr" target="#b19">[14]</ref>, <ref type="bibr" target="#b21">[16]</ref> and <ref type="bibr" target="#b22">[17]</ref> actually use the model proposed in <ref type="bibr">[11]</ref> for computing the reliability of an application. However, the model of <ref type="bibr">[11]</ref> is specifically formulated for applications that can be modeled using undirected task graphs and is therefore not well suited to computing the failure probability of an application with precedence constraints. In addition, the model is applicable only to computing systems with tree network topology, although in <ref type="bibr" target="#b22">[17]</ref> this limitation is overcome by arbitrarily choosing one of the paths among many possible ones between two tasks assigned to different machines to establish the intertask communication.</p><p>In order to solve the biobjective scheduling problem, two matching and scheduling algorithms, the biobjective dynamic level scheduling algorithm and biobjective genetic algorithm, are developed in Sections 4 and 5 respectively. The first algorithm is obtained by modifying an existing static matching and scheduling algorithm, and the second one is a standard genetic algorithm tuned to solve the biobjective scheduling problem. The extensive simulation studies presented in Section 6 show that both algorithms can be used to trade off execution time against the failure probability of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODELS AND ASSUMPTIONS</head><p>The network topology of an HC system is modeled using a connected, undirected graph G = (M, N ), where M denotes the computation and communication machines and N denotes the communication links. Let m j ∈ M denote a machine, where 1 ≤ j ≤ p + q, and n k,l ∈ N denote a link between machines m k and m l . A machine (m j ) will refer to a computation machine if 1 ≤ j ≤ p and to a communication machine if p + 1 ≤ j ≤ p + q. Let R = M ∪ N denote the set of resources in an HC system; an element r i ∈ R refers to either a machine or a network link. The set R is introduced only for notational convenience. A simple path p s,t between machines m s and m t is defined as the set of resources that form a path from machine m s to m t in which</p><p>The Computer Journal Vol. 48 No. 3, 2005</p><p>A. Do gan and F. Özgüner a resource is not visited more than once. The resource set includes both the source and destination machines as well. In this model of the system, computation machines are interconnected by a network of communication machines (e.g. switches, bridges, routers) and links. In addition, the network has an arbitrary topology and can accommodate different networking technologies.</p><p>An application executing on the system is represented using a directed acyclic graph (DAG) T = (V , E), where V = {v 1 , v 2 , . . . , v n } is the set of tasks to be executed and the set E of directed edges denotes the data communication between pairs of tasks. Let t E i,j be the expected execution time of task v i on machine m j , 1 ≤ j ≤ p. It is assumed that the expected execution time t E i,j , 1 ≤ i ≤ n and 1 ≤ j ≤ p, is known. Techniques such as code profiling/analytic benchmarking <ref type="bibr" target="#b26">[21]</ref> and statistical prediction <ref type="bibr" target="#b27">[22]</ref> have been devised to estimate t E i,j . Let e k,l ∈ E indicate communication from task v k to task v l , where task v k (v l ) is said to be an immediate predecessor (successor) task of task v l (v k ). Associated with directed edge e k,l ∈ E is the volume of data in terms of bytes, which is denoted by d k,l , that will be transmitted from task v k to task v l upon completion of task v k .</p><p>Throughout the paper, it is assumed that all computation machines are dedicated, i.e. a task will run to completion without preemption on any computation machine in the system. Such a computing system is assumed to be controlled by a centralized scheduler that allocates system resources to applications with the goal of minimizing both the execution time and the failure probability of applications.</p><p>Regarding resource failures in the system, the following three assumptions are made. These assumptions are common to other studies, e.g. <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b18">13,</ref><ref type="bibr" target="#b19">14]</ref>, that deal with analyzing the reliability of computer systems. The failure of a resource in the system is assumed to follow a Poisson process and each resource r i ∈ R is accordingly associated with a constant failure rate λ r i . It should be noted that modeling the failure of a resource by a Poisson process may not always coincide with the actual failure dynamic of the resource. However, it is shown experimentally in <ref type="bibr" target="#b12">[7]</ref> that such an assumption can still result in reasonably useful mathematical models. For mathematical tractability, failures of resources are assumed to be statistically independent. In addition, once a resource has failed, it is assumed that it remains in the failed state for the remainder of the execution of the application.</p><p>Finally, let M : V → M denote a matching function, where M(i), 1 ≤ i ≤ n, defines the machine to which task v i is assigned. Note that only computation machines are considered for executing a given task. Thus, M(i) = m j implies that m j is a computation machine (1 ≤ j ≤ p). Let S j : V → {0, 1, . . . , n} be a scheduling function, where S j (i), 1 ≤ i ≤ n and 1 ≤ j ≤ p, denotes the execution order of task v i on machine m j (S j (i) = 0 indicates that task v i is not assigned to machine m j ). Consequently, a matching function M and a set of scheduling functions {S 1 , S 2 , . . . , S p } can represent a possible assignment of an application to machines in an HC system. Let X i = {M(i), S M(i) (i)} denote a possible matching and scheduling decision for task v i and π i denote all possible matching and scheduling decisions for task v i . Thus, X = X 1 × X 2 × • • • × X n defines a possible task assignment of tasks in set V to machines in set M and π = π 1 × π 2 × • • • × π n represents all possible task assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A BIOBJECTIVE SCHEDULING PROBLEM</head><p>As noted, in the context of this study, there are two objectives, namely minimizing execution time and minimizing failure probability, for which a Pareto-optimal <ref type="bibr" target="#b28">[23]</ref> solution will be sought. Thus, a biobjective scheduling problem is defined as</p><formula xml:id="formula_0">min X ∈π J 1 (X ) J 2 (X ) (1)</formula><p>where J 1 denotes the schedule length and J 2 denotes the failure probability of an application under task assignment X .</p><p>It should be emphasized that the biobjective scheduling problem formulation of Equation ( <ref type="formula">1</ref>) is new in the literature.</p><p>In the following two sections, both objective functions J 1 (X ) and J 2 (X ) are formally defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The first objective: minimize the schedule length</head><p>Suppose that task assignment X is given. Because of the precedence constraints, task v i cannot start running on machine M(i) unless all data items from its immediate predecessor tasks have been received by machine M(i). Let t D i,k (e k,i ∈ E) denote the time when task v i has received the data from task v k and</p><formula xml:id="formula_1">t D i,k = t F k , if M(k) = M(i) t F k + d k,i c M(k),M(i) , otherwise<label>(2)</label></formula><p>where t F k denotes the finish time of task v k and c s,t denotes the expected transmission time of sending one byte of data from machine m s to machine m t . The time when all data items of task v i have been received by machine M(i) is referred to as the data arrival time. Definition 1 formalizes the data arrival time of a task. Definition 1. The data arrival time of task v i , which is denoted by t D i , is defined to be</p><formula xml:id="formula_2">t D i = max e k,i ∈E t D i,k<label>(3)</label></formula><p>In order for the execution of a task to start on a machine, all data items for the task must have been received and the machine must be available. Thus, the start time of task v i , which is denoted by t S i , is defined to be</p><formula xml:id="formula_3">t S i = max t M i , t D i (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where t M i denotes the time when machine M(i) will be available to execute task v i (t M i = 0 if task v i is the first task to be executed on machine M(i)) and t M i equals the finish time of the (k -1)th task if it is the kth task. Finally, the finish time of task v i (t F i ) is defined to be</p><formula xml:id="formula_5">t F i = t S i + t E i,M(i)<label>(5)</label></formula><p>The Computer Journal Vol. 48 No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3, 2005</head><p>Biobjective Scheduling in HC Systems 303 Thus, the schedule length of the application under task assignment X is given by</p><formula xml:id="formula_6">J 1 (X ) = max v i ∈V t F i (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The second objective: minimize the failure probability</head><p>In this section, a new mathematical model to compute the reliability of an application with precedence constraints is presented. Before the presentation of this model some notation needs to be introduced. Let R j (T , X ), 1 ≤ j ≤ p, denote the reliability of computation machine m j , which is the probability that machine m j is functional for the execution of tasks assigned to it under task assignment X . In addition, let R j (T , X ), p + 1 ≤ j ≤ p + q, denote the reliability of communication machine m j and R k,l (T , X ) denote the reliability of link n k,l , where the reliability of a communication resource (machine or link) is the probability that the communication resource is functional for performing intertask communication during the execution of the application under task assignment X . Note that, since the failure of a resource is governed by a Poisson process, the reliability of resource r i at time t is e -λ r i t <ref type="bibr" target="#b29">[24]</ref>. Successful completion of the execution of the application requires that each computation machine be functional during the time that its assigned tasks are executing and that each communication resource that will be used in intertask communication be functional during the time that the intertask communication is taking place; it thus depends on the reliability of the resources to which the application is allocated. The probability that application T can run successfully on an HC system under task assignment X is denoted by R(T , X ), which also represents the reliability of the HC system when application T is allocated by X . Assuming that the failures of resources are statistically independent, R(T , X ) is defined to be</p><formula xml:id="formula_7">R(T , X ) = m j ∈R K R j (T , X ) • n k,l ∈R K R k,l (T , X ) = m j ∈R K e -λ j t A j • n k,l ∈R K e -λ k,l t A k,l = e (-COST(X )) (7)</formula><p>where</p><formula xml:id="formula_8">COST(X ) = m j ∈R K λ j t A j + n k,l ∈R K λ k,l t A k,l</formula><p>, and λ j and λ k,l are the failure rates of machine m j and link n k,l respectively. In addition, t A j , 1 ≤ j ≤ p, denotes the time at which computation machine m j will complete the execution of tasks under X and is defined to be</p><formula xml:id="formula_9">t A j = max S j (i)&gt;0 t F i (8)</formula><p>Variables t A j , p + 1 ≤ j ≤ p + q, and t A k,l denote the times at which communication machine m j and link n k,l will complete the intertask data communication under X respectively, and are defined to be</p><formula xml:id="formula_10">t A j = max e u,v ∈E I j (M(u), M(v))t D v,u and t A k,l = max e u,v ∈E I k,l (M(u), M(v))t D v,u<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">I j (s, t) =</formula><p>1, m j is on p s,t 0, otherwise and</p><formula xml:id="formula_12">I k,l (s, t) = 1, n k,l is on p s,t 0, otherwise</formula><p>Finally, R K is a resource set that is composed of computation and communication machines and links. That is, R K is a subset of resources used for executing the application under X and determined with respect to set K = {m j | m j = M(i) and v i ∈ V }, which is the set of computation machines to which at least one task of the application is allocated. If the network topology of an HC system can be modeled by a tree (i.e. there exists a unique simple path between any two machines) set R K is determined as follows. (i) Include all computation machines to which at least one task is assigned into R K . (ii) For all e k,l ∈ E, include all communication machines and links that form the unique simple path from machine m s to m t into R K provided tasks v k and v l are executing on machines m s and m t respectively. If the network topology is not a tree, computing R K is non-trivial; this is addressed in Section 3.2.1. As a result, the failure probability of the application under task assignment X is defined as</p><formula xml:id="formula_13">J 2 (X ) = 1 -R(T , X ) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">K-terminal reliability computation</head><p>In the previous section, it was noted that the computation of set R K is a difficult task for a system with arbitrary network topology. Basically, the complexity of this task comes from the fact that if there exists more than one simple path between two machines in the system, it is not clear which resources must be included into R K to represent the communication between the machines. In this section, a computationally simple method is presented to compute set R K . This method is based on the computation of the K-terminal reliability, which is formally defined below.</p><p>Definition 2. The K-terminal reliability is the probability that all machine pairs in a set K, which is a subset of computation machines in the system, can communicate.</p><p>For the computation of the K-terminal reliability, K-trees are used <ref type="bibr" target="#b30">[25]</ref>. Let K = {m 1 , m 2 , . . . , m k }, 2 ≤ k ≤ p, be any subset of M, where m j ∈ K is a computation machine. A K-tree is defined as follows. Definition 3. A K-tree of G = (M, N ) with respect to a set of computation machines K is a tree in which leaf vertices of the tree correspond to computation machines in set K and the others correspond to only communication machines. Based on K-trees, the K-terminal reliability is the probability that there exists at least one functional K-tree in the system. In the following, the K-terminal reliability is formally derived.</p><p>Let T i denote the ith K-tree of G, with respect to K, and T i be represented as a set of resources T i = {r i 1 , r i 2 , . . . , r i l i }, where l i is the number of resources in the ith K-tree. Let E be the event that at least one K-tree is failure-free and E i be the event that the ith K-tree is failure-free. Using the inclusion-exclusion principle, the K-terminal reliability can be computed as</p><formula xml:id="formula_14">P [E] = P [E 1 ∪ • • • ∪ E γ ] = γ i=1 P [E i ] - γ i=2 i-1 j =1 P [E i ∩ E j ] + γ i=3 i-1 j =2 j -1 k=1 P [E i ∩ E j ∩ E k ] -• • • + (-1) γ -1 P [E 1 ∩ • • • ∩ E γ ] (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>where γ is the number of different K-trees. In <ref type="bibr" target="#b23">[18]</ref>, we show that</p><formula xml:id="formula_16">P [E] = γ i=1 e -{ i } t - γ i=2 i-1 j =1 e -{ i ∪ j } t + γ i=3 i-1 j =2 j -1 k=1 e -{ i ∪ j ∪ k } t -• • • + (-1) γ -1 e -( { 1 ∪•••∪ γ })t<label>(12)</label></formula><p>where i = {λ i r 1 , . . . , λ i r l i } denotes the set of failure rates of the resources which form the ith K-tree and { } denotes the summation of the elements of . However, computing the exact K-terminal reliability for an arbitrary network using Equation ( <ref type="formula" target="#formula_16">12</ref>) is NP-hard <ref type="bibr" target="#b31">[26]</ref>. To simplify the computation of P [E], ēx can be replaced by its small-value approximation 1x. After the substitution, it is shown in <ref type="bibr" target="#b23">[18]</ref> that the K-terminal reliability expression ( <ref type="formula" target="#formula_16">12</ref>) simplifies to</p><formula xml:id="formula_17">P [E] = 1 - γ i=1 i t = 1 - r i ∈R K λ r i t (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where</p><formula xml:id="formula_19">R K = {r x | r x ∈ γ i=1 T i }, i.e. R K</formula><p>is a set of resources that are common to all K-trees. For example, suppose that a DAG with four tasks will run on an HC system with five machines, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, where task v 1 is assigned to machine m 2 , and so on. With respect to the task assignment in Figure <ref type="figure" target="#fig_0">1</ref>, K = {m 2 , m 3 , m 4 , m 5 } and there are only four distinct K-trees that span computation machines in set K. These are shown in Figure <ref type="figure">2</ref>. Given the K-trees in Figure <ref type="figure">2</ref></p><formula xml:id="formula_20">, R K = {m 2 , m 3 , m 4 , m 5 , m 7 , m 9 , m 11 , m 12 , n 2,7 , n 3,12 , n 4,11 , n 5,12 }.</formula><p>Note that the error due to the small-value approximation is less than r i ∈R K λ r i t.</p><p>Enumeration of all K-trees of an arbitrary network graph to compute set R K is computationally expensive even though it is possible to generate a K-tree of an undirected graph in O(|M||N |) <ref type="bibr" target="#b30">[25]</ref>. For the efficient computation of R K , a new method which avoids the enumeration of K-trees is developed in this paper. The new method is based on Theorem 1, whose proof can be found in <ref type="bibr" target="#b23">[18]</ref>. Before the presentation of this theorem, let p i s,t = {r i 1 , r i 2 , . . . , r i h i } denote the i-th simple path from machine m s to machine m t , where h i is the number of resources in the i-th simple path, and let γ s,t denote the number of different simple paths between machines m s and m t .</p><p>The Computer Journal Vol. 48 No. 3, 2005</p><formula xml:id="formula_21">FIGURE 2. K-trees of K = {m 2 , m 3 , m 4 , m 5 }.</formula><p>Theorem 1. The reliability of the communication between any machine pair (m s , m t ) in the system, which is denoted by R τ s,t (t), can be approximated as , m 4 }. The following theorem, whose proof can be found in <ref type="bibr" target="#b32">[27]</ref>, shows how to determine set R K without enumerating all K-trees.</p><formula xml:id="formula_22">R τ s,t (t) = 1 - r i ∈ Rγ s,</formula><formula xml:id="formula_23">Theorem 2. If R K = {r x | r x ∈ γ i=1 T i } and RK = {r x | r x ∈ |K| i=2 i-1 j =1 Rγ i,j i,j }, then R K ≡ RK . In addition, the time complexity of computing R K is O((|M|+|N |)|K| 2 ).</formula><p>With respect to Theorem 2, R K can be formed by including the resources in set Rγ s,t s,t for each possible machine pair (m s , m t ) in set K into R K . As a result, it is possible for the resources in Rγ s,t s,t to be included into R K even though there is no communication from machine m s to m t during the execution of the application. Thus, one may argue that some resources that will not be used for the execution of the application can appear in R K . However, according to Theorem 3, whose proof can be found in <ref type="bibr" target="#b32">[27]</ref>, all resources in R K will take part in the execution of the application.</p><formula xml:id="formula_24">Theorem 3. If RK = e k,l ∈E Īk,l (s, t)</formula><p>Rγ s,t s,t , where</p><formula xml:id="formula_25">Īk,l (s, t) = 1, m s = M(k) and m t = M(l) 0, otherwise. then R K ≡ RK . In addition, the time complexity of computing R K is O((|M| + |N |)|E|).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BIOBJECTIVE DYNAMIC LEVEL SCHEDULING ALGORITHM</head><p>The dynamic level scheduling (DLS) algorithm is a static list scheduling heuristic that has been developed to allocate a DAG-structured application to a set of heterogeneous machines to minimize the execution time of the application <ref type="bibr" target="#b8">[3]</ref>. At each scheduling step, the DLS algorithm chooses the next task to schedule and the machine on which that task is to be executed by finding the ready task and machine pair that has the highest dynamic level. The dynamic level of a task-machine pair, which is denoted by DL(v i , m j ), is defined to be</p><formula xml:id="formula_26">DL(v i , m j ) = SL(v i ) -max{t M i , t D i } + (v i , m j ) (15)</formula><p>The first term in Equation ( <ref type="formula">15</ref>) is called the static level of task v i and is defined to be</p><formula xml:id="formula_27">SL(v i ) = tE i + max e i,l ∈E {SL(v l )} (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>where tE i denotes the median execution time of task v i across all machines. The static level indicates the importance of a task in the precedence hierarchy by giving higher priority to tasks for which the time spent to complete the execution of the application is expected to be larger. The max term in Equation ( <ref type="formula">15</ref>) defines the time when task v i can begin execution on machine m j . A task-machine pair with an earlier starting time will have higher scheduling priority. The third term in Equation ( <ref type="formula">15</ref>) accounts for the machine speed differences and is defined to be</p><formula xml:id="formula_29">(v i , m j ) = tE i -t E i,j<label>(17)</label></formula><p>If machine m j runs task v i faster than the other machines in the network, (v i , m j ) will be positive, which increases the scheduling priority. While making matching and scheduling decisions, the DLS algorithm does not account for the reliability of the resources in a HC system.</p><p>In this study, the DLS algorithm is modified as shown in Figure <ref type="figure">3</ref> and the new algorithm will be referred to as the biobjective dynamic level scheduling (BDLS) algorithm. In the BDLS algorithm, when the dynamic level of a task-machine pair DL(v i , m j ) is computed, the corresponding incremental cost COST(v i , m j ) is also computed. The incremental cost of executing task v i on machine m j is defined to be</p><formula xml:id="formula_30">COST(v i , m j ) = COST( Xi ) -COST( Xi-1 ) = ln(R(T , Xi-1 )) -ln(R(T , Xi )) (18)</formula><p>where Xi denotes a partial task assignment in which tasks {v 1 , v 2 , . . . , v i } are assigned. Note that since R(T , Xi-1 ) &gt; R(T , Xi ) always holds, COST(v i , m j ) &gt; 0, i.e. making a new scheduling decision will always increase the failure probability of the application. Thus, the incremental cost must be kept low for each task in order to decrease the failure probability of the application.</p><p>According to Figure <ref type="figure">3</ref>, DL(v i , m j ) and COST(v i , m j ) are computed for each task-machine pair (v i , m j ), where v i is a ready task, and the results are stored in lists List 1 and List 2 respectively. Then, list List 1 is sorted in decreasing order and list List 2 is sorted in increasing order. Thus two ranks, Rank 1 i,j and Rank 2 i,j , are associated with each task-machine pair (v i , m j ), where Rank 1 i,j and Rank 2 i,j denote the orders of task-machine pair (v i , m j ) in List 1 and List 2 respectively. Thus, a higher ranked task-machine pair in either list implies a better choice with respect to the corresponding objective. In the DLS algorithm, the task-machine pair (v * i , m * j ) for which Rank 1 i,j = 1 is found, and task v * i is assigned to machine m * j . In the BDLS algorithm, however, the taskmachine pair (v * i , m * j ) for which Rank i,j is minimized is taken as the current best scheduling decision. Rank i,j is defined to be</p><formula xml:id="formula_31">Rank i,j = δ 1 Rank 1 i,j + δ 2 Rank 2 i,j<label>(19)</label></formula><p>where δ 1 and δ 2 are the weights that are introduced for trading off execution time for failure probability. Note that setting δ 2 = 0 reduces BDLS to DLS in that the goal of minimizing the failure probability is neglected, and setting δ 1 = 0 makes BDLS ignore the goal of minimizing the schedule length.</p><p>At this point, it is important to make a comparison between the BDLS algorithm and the RDLS algorithm proposed in <ref type="bibr" target="#b25">[20]</ref>, which is the only other algorithm in the literature that addresses the biobjective scheduling problem posed in this study. First, both the BDLS and RDLS algorithms are based on the DLS algorithm. Thus, the dynamic level expression of Equation ( <ref type="formula">15</ref>) is common to both.</p><p>In order to assign a rank to a scheduling decision represented by a task-machine pair (v i , m j ), the BDLS creates two lists as explained above. On the other hand, the RDLS creates only one list based on the following expression: DL(v i , m j ) -C(v i , m j ), where DL(v i , m j ) is from Equation <ref type="bibr" target="#b20">(15)</ref> and C(v i , m j ) is a term that reflects the impact of scheduling task v i on machine m j on the failure probability of the application. Note that the higher the value of C(v i , m j ), the higher the failure probability. An important disadvantage of the rank assignment mechanism of the RDLS is the fact that DL(v i , m j ) can dominate C(v i , m j ) or vice versa. This is because the ranges of values that DL(v i , m j ) and C(v i , m j ) can take are different. As a result, during the scheduling, one of the objectives can be easily overlooked by the RDLS. In order to alleviate this problem in the RDLS, both objective functions must be normalized and then combined. Note that the BDLS does not have this problem simply because a rank from the corresponding list is associated with each objective and then they are combined. On the other hand, the BDLS allows one to make one objective dominate the other simply by setting the weights appropriately.</p><p>Another important distinction of the BDLS comes from the definition of COST(v i , m j ) in Equation ( <ref type="formula">18</ref>) compared with C(v i , m j ) of the RDLS. COST(v i , m j ) is an estimate of the difference in the reliability of the application due to the scheduling decision (v i , m j ). In order to compute COST(v i , m j ), a unique method based on the K-terminal reliability estimation was developed in the previous section. C(v i , m j ), on the other hand, is expressed in time units as a function of the reliability of communication between a source machine where the owner of the application is located and machine m j . Thus, it is clear that COST(v i , m j ) provides much better information on the impact of a scheduling decision on the failure probability of the application than C(v i , m j ).</p><p>It should be noted that it was in the framework of this study that the idea of trading off execution time against failure probability was first brought up. In addition, it is also possible to modify the objective function of the RDLS, i.e. DL(v i , m j )-C(v i , m j ), to enable such trading. Apparently, however, the concern in <ref type="bibr" target="#b25">[20]</ref> was to produce compromise solutions by perturbing DL(v i , m j ) via C(v i , m j ) rather than trading execution time for failure probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">A BIOBJECTIVE GENETIC ALGORITHM</head><p>Many approaches to genetic algorithms (GAs) have been proposed in the literature. In this study, however, a standard GA <ref type="bibr" target="#b33">[28]</ref>  (1) selection, (2) crossover, (3) mutation, (4) evaluate the fitness of each chromosome, (iii.b) until the population has converged. Details of the steps for the implementation of the proposed biobjective genetic algorithm (BGA) will be discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Chromosome structure</head><p>A chromosome is treated as a data structure into which a solution of the biobjective scheduling problem will be encoded. In the BGA, a chromosome C i is composed of p scheduling lists, where C i denotes the i-th chromosome. A scheduling list L j i includes all tasks assigned to a particular machine and defines the order of execution of the tasks on the machine, where L j i is the j -th scheduling list of the i-th chromosome.</p><p>A set of chromosomes is referred to as a population. The population size denoted by N p is defined as the number of chromosomes in a population. In the BGA, the population size is kept fixed at N p through the generations of the population.</p><p>In a population of the BGA, there can be two kinds of chromosome, namely valid chromosomes, each of which represents a possible execution of the application, and invalid chromosomes, each of which corresponds to a schedule under which the execution of the application is impossible. As also pointed out in below, an invalid chromosome can be due to the initial population generation phase, the crossover phase or the mutation phase. Note that ascertaining the validity of a chromosome takes O(|V | + |E|) time by using the depth-first search algorithm <ref type="bibr" target="#b34">[29]</ref>. In a population of N p chromosomes, a small number of invalid chromosomes are allowed in the population, where the maximum number of invalid chromosomes allowed is denoted by N iv . The invalid chromosomes are differently treated from the valid ones only in the selection phase, which is explained in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Initial population generation</head><p>In the BGA, an initial population from which the search for a Pareto-optimal solution starts is randomly generated as follows. (i) The first chromosome is generated: (a) the DAG is topologically sorted <ref type="bibr" target="#b34">[29]</ref>; (b) each task is assigned to a randomly chosen machine in the order dictated by the topological sort. (ii) A new chromosome other than the first is generated: one of the previously generated chromosomes is randomly picked and mutated a random number of times (between one and the number of tasks) using the mutation operator, which will be defined below. (iii) If the newly generated chromosome is identical to any of the previously generated ones, it is discarded. The second and third steps are repeated until N p unique chromosomes are generated. Note that there is no guarantee that a newly generated chromosome will be a valid one. The initial population generation algorithm is inspired by <ref type="bibr" target="#b35">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fitness of a chromosome</head><p>In the area of evolutionary-based multiobjective optimization, determining the fitness values of chromosomes is one of the major research problems <ref type="bibr" target="#b28">[23]</ref>. Thus, in the literature, a number of fitness assignment techniques have been proposed, one of which is the sum of weighted global ratios (SWGR) <ref type="bibr" target="#b36">[31]</ref>. Because of its high performance and easy implementation, proven in <ref type="bibr" target="#b36">[31]</ref>, the SWGR is implemented as the fitness assignment technique in the BGA.</p><p>According to the SWGR, the fitness value of a valid chromosome C i , which is denoted by fitness i , is determined by aggregating two objective functions as follows:</p><formula xml:id="formula_32">fitness i = 2 j =1 δ j fit_val j i -min_fit j max_fit j -min_fit j (20)</formula><p>where fit_val j i is the fitness of C i with respect to the j th objective, max_fit j and min_fit j are the best and worst fitness values encountered for the j th objective respectively and δ j is the weight for the j th objective as defined before. Note that fit_val 1 i = J 1 (X ) and fit_val 2 i = J 2 (X ), where X is encoded in chromosome C i . In addition, the greater fitness i , the better the solution. Thus, if C i is an invalid chromosome, fitness i is set to -∞.</p><p>The BGA also implements elitism after the fitness evaluation process. Elitism is important because it ensures that the quality of the best solution found over generations is monotonically increasing. The details about incorporating elitism into a GA can be found in <ref type="bibr" target="#b23">[18,</ref><ref type="bibr" target="#b35">30,</ref><ref type="bibr" target="#b37">32]</ref> and thus will not be repeated here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Selection</head><p>The selection operator is used to choose chromosomes from the current population for a mating pool. In the literature, several selection techniques have been proposed. Of these the ranking selection, which is analytically shown to be a good choice in <ref type="bibr" target="#b38">[33]</ref>, is determined as the selection operator.</p><p>The BGA implements the ranking selection with roulette wheel sampling. The details of this implementation can be found in <ref type="bibr" target="#b23">[18,</ref><ref type="bibr" target="#b35">30]</ref> and thus will not be repeated here. Remember that there are both valid and invalid chromosomes in the population. The ranking selection with roulette wheel sampling is used for only the valid chromosomes. From a set of invalid chromosomes, at most N iv invalid chromosomes will be randomly selected for the mating pool, where each one has an equal chance of being selected. As a result, in the mating pool, there will be at least N p -N iv valid chromosomes selected from the valid chromosomes through the ranking selection with roulette wheel sampling and at most N iv invalid chromosomes randomly selected from the invalid chromosomes. Note that if the number of invalid chromosomes is less than N iv , valid chromosomes substitute for invalid ones. In addition, it is possible for a valid chromosome to appear more than once in the mating pool. After the selection is over, chromosomes in the mating pool will be subject to crossover and mutation operators to form the next generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Crossover</head><p>In the BGA, 1-point crossover <ref type="bibr" target="#b33">[28]</ref> is implemented as follows.</p><p>For each chromosome C i , it is possible to construct a list L i</p><p>The Computer Journal Vol. 48 No. 3, 2005</p><p>A. Do gan and F. Özgüner into which scheduling lists L j i are combined. The following definition will be used in the procedure that combines the scheduling lists. Definition 4. A valid position for a task in a list of tasks is defined to be a position such that if the task is placed at that position, there exist no predecessors (successors) of that task after (before) it in the list.</p><p>All predecessor and successor tasks of a task can be found by using the depth-first search algorithm <ref type="bibr" target="#b34">[29]</ref> before the search begins. Thus, it is possible to check the precedence relationship between any two tasks in constant time during the search.</p><p>The procedure for combining the scheduling lists of chromosome C i into list L i runs as follows. (i) Let L i be an empty list. (ii) For each non-empty scheduling list L j i , starting from the end of list L j i , each task in L j i is placed at a position in L i so that the execution order of tasks dictated by L j i is preserved and the position is the last available valid position. For at least one task of an invalid chromosome, finding a valid position in list L i that preserves the execution order of the task due to L j i is impossible. In such a case, the task is placed to preserve its execution order. If the task is the last task of L j i , it is put at the end of the list. An important property of this procedure is that it produces a topologically sorted list for a valid chromosome.</p><p>The crossover operation implemented as follows. (i) Chromosomes in the mating pool are randomly paired. (ii) A pair of non-crossed over chromosomes, e.g. C i and C j , is taken from the mating pool. (iii) The crossover operator is applied to the chosen pair with probability µ c , which is the probability of crossover and is experimentally determined, provided C i and C j are different. (iv) The procedure presented above is used to obtain lists L i and L j . (v) For the pair, a cut-off point, which divides lists L i and L j into top and bottom parts, is randomly generated. (vi) The machine to which task v k in the bottom part of L i (L j ) is assigned is changed to the machine to which task v k is assigned in list L j (L i ). (vii) Two new chromosomes are generated by assigning the tasks to the respective machines in the order dictated by lists L i and L j and these two new chromosomes are put back into the mating pool. Note that either or both of the newly generated chromosomes might be invalid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Mutation</head><p>The mutation operator is implemented as follows. (i) A nonmutated chromosome is randomly chosen from the mating pool. (ii) The mutation operator is applied to the chosen chromosome with probability µ m , which is the probability of mutation and is experimentally determined. (iii) A task and a machine on which the task will be scheduled are randomly determined. (iv) This task is randomly placed at one of the valid positions in the scheduling list of the chosen machine. After the mutation, the chromosome is put back into the mating pool. Note that the mutated chromosome might be invalid.</p><p>After the mutation operator is applied to all chromosomes in the mating pool, the chromosomes in the mating pool constitute the next generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In order to evaluate the BDLS algorithm and the BGA and compare their performance with RDLS algorithm <ref type="bibr" target="#b25">[20]</ref>, a simulation program that can be used to emulate the execution of randomly generated or real application task graphs on a simulated computing system was developed.</p><p>In the simulation program, a heterogeneous computing system is created based on two parameters, namely the number of computation machines p and the number of communication machines q. Associated with each computation machine is a FIFO queue that holds the tasks scheduled on each particular machine. (Recall that each machine is assumed to execute a task in its queue to completion without preemption.) In order to interconnect computation machines, a network of q communication machines is employed, where the network topology is randomly generated and each computation machine is randomly connected to a communication machine. This simulation model closely mimics a computing system in which a set of machines is interconnected by a switchedbased network. Other parameters of interest of the model are set as follows. The failure rates of machines and links are assumed to be uniformly distributed between 10 -3 and 10 -4 failures/h <ref type="bibr" target="#b12">[7]</ref>; the transmission rates of links are assumed to be uniformly distributed between 1 and 10 Mbits/s.</p><p>The simulation studies performed are grouped into three sets: (i) executing randomly generated task graphs with different number of tasks (between 20 and 100) on a computing system with p = 20 and q = 20, (ii) executing randomly generated task graphs with 50 tasks on a computing system with p ranging from 10 to 50 and q = 20 and (iii) executing a real application task graph on a computing system with p = 50 and q = 20, p = 100 and q = 40, p = 150 and q = 60, p = 200 and q = 80, and p = 250 and q = 100. For the randomly generated task graphs, the execution time of each task of the task graph is assumed to be uniformly distributed between 10 and 120 min, where the execution times of a given task are different on different machines. Furthermore, the volume of data to be transmitted among tasks is randomly generated such that the communication to computation ratio (CCR) is 1.0 or 10.0, where the average communication time between a task and its successor tasks is set to the average execution time of the task multiplied by the CCR. As far as the RDLS algorithm is concerned, it can assume one of the three cost functions developed in <ref type="bibr" target="#b25">[20]</ref> for computing C(v i , m j ). In this study, the RDLS with the first cost function is assumed since it leads to a middling performance compared with the RDLS algorithms with the second and third cost functions according to the simulation results in <ref type="bibr" target="#b25">[20]</ref>.</p><p>In the simulations, the execution time and failure probability of applications were measured for the different  The BGA was stopped if the quality of the elite chromosome did not improve over 30 generations.</p><p>The simulation results are shown in Figures <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_0">10</ref><ref type="figure" target="#fig_0">11</ref>. Specifically, Figures <ref type="figure">4</ref> and<ref type="figure">7</ref> present the results of the first set of simulation studies. According to Figures <ref type="figure">4</ref> and<ref type="figure">7</ref>, the failure probability of an application (random task graph) increases in proportion to the size of the application. This is due to the fact that when the size of an application increases, computation machines have to be failure-free for longer time periods for the execution of tasks and communication resources have to be failure-free for longer time periods for the transmission of intertask data items. Since the failure probability of a resource increases exponentially within the time interval for which the resource must remain failurefree, the failure probability of the application increases. As a result, a large, long running application will be more susceptible to failures, unless tasks of the application are A. Do gan and F. Özgüner  assigned to machines in such a way that the reliability of the resources committed for the execution of the application is accounted for.</p><p>The ability of the BDLS algorithm and the BGA to trade off execution time against reliability is clear in Figures <ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref><ref type="figure">8</ref><ref type="figure">9</ref><ref type="figure" target="#fig_0">10</ref><ref type="figure" target="#fig_0">11</ref>. These figures show that (i) the average schedule length is lowest when 1 = [1, 0] and highest when 5 = [0, 1]; (ii) while δ 1 decreases and δ 2 increases the average schedule length and reliability of an application increase and vice versa; (iii) the failure probability is largest when 1 = [1, 0] and lowest when 5 = [0, 1]. It should be noted that since there are no assigned capacity limits for computation machines, such as on processing time and memory capacity, both the BDLS algorithm and the BGA tend to schedule all the tasks of an application to a few of the most reliable machines for 5 = [0, 1]. As a result, the failure probability becomes lowest but the schedule length increases dramatically.</p><p>The results of the second simulation studies are shown in Figures <ref type="figure">8</ref> and<ref type="figure">9</ref>, where the random task graphs with CCR = 1.0 are assumed. In Figure <ref type="figure">8</ref>  for 5 , as expected, which also results in a decreased failure probability. In Figure <ref type="figure">9</ref>, however, the BGA does not show as much consistency as the BDLS algorithm. This may be attributed to the fact that the BGA is a stochastic search technique, whereas the BDLS algorithm is a deterministic one.</p><p>For the third set of simulation studies, a real application (CSTEM) task graph shown in Figure <ref type="figure" target="#fig_0">12</ref> with CCR = 1.0 is used. CSTEM (Coupled Structural-Thermal-Electromagnetic Analysis and Tailoring of Graded Composite Structures) <ref type="bibr" target="#b39">[34]</ref> is a finite element-based computer program. The numbers within the nodes of the graph in Figure <ref type="figure" target="#fig_0">12</ref> represent the approximate execution times, in seconds, on a Sun Microsystem Sparc 10 workstation <ref type="bibr" target="#b39">[34]</ref>. Since the execution time of each task depends on the size of the problem, the task execution times specified in the task  graph are changed to be used in the simulations as follows. The task with the smallest execution time (0.3 s) is assumed to take 10 min on average. Then, the average execution times (in seconds) of the other tasks are found by multiplying their execution times given in the task graph by (10 × 60)/0.3. Under these settings, the third set of simulation studies is shown in Figures <ref type="figure" target="#fig_0">10</ref> and<ref type="figure" target="#fig_0">11</ref>, which are similar to the results of the second set of simulation studies.</p><p>For all simulation results and 1 , the BGA produces dominated task assignments compared with the BDLS algorithm. Note that a task assignment is said to be dominated by another one if and only if it leads to both higher schedule length and higher failure probability. Except for the third set of simulation results, for 2 , the BGA once again is mostly dominated by the BDLS algorithm. For the third set of simulation results and 3 and 4 , the BGA is the nondominanted one. For the rest of the simulation studies and 2 , 3 , 4 and 5 , both the BDLS algorithm and the BGA are non-dominated between them.</p><p>As far as the performance of the RDLS algorithm is concerned, remember that the BDLS algorithm turns into the DLS algorithm for 1 . When the figures for the BDLS algorithm are studied, it is clear that the RDLS algorithm has not perturbed the DLS algorithm, i.e. the schedule length under the BDLS algorithm with 1 is close to that under the RDLS algorithm, for the first set of simulations with CCR = 1.0 and the second set of simulations. This is because the term DL(v i , m j ) mostly dominates the term C(v i , m j ) in the objective function of the RDLS algorithm for CCR = 1.0. When CCR = 10.0, C(v i , m j ) becomes comparable to DL(v i , m j ) and affects the scheduling decisions more. Thus, it is safe to say that the RDLS algorithm performs somewhere between the BDLS algorithm with 1 and the BDLS algorithm with 2 for CCR = 1.0 for the aforementioned simulation cases. This is also true for small values of CCR other than CCR = 1.0. For CCR = 10.0, the BDLS algorithm with 1 , 2 and 3 mostly dominates the RDLS algorithm, which generally dominates the BGA with 1 and 2 . For the third set, the RDLS algorithm dominates the BGA with 2 , 3 and 4 . In the rest of the simulation studies, the BDLS and RDLS algorithms and the BGA and the RDLS algorithms are non-dominated among themselves.</p><p>Our simulation program is run on a PC with the following specifications: Intel Pentium 4 processor with 1.6 GHz clock speed, 128 MBytes of RAM and Red Hat 8.0 Linux operating system. The running times of the BDLS algorithm and the BGA are measured as follows. For the first set of simulation studies, the BDLS algorithm's running times with an increasing number of tasks are 0.11, 0.92, 3.33, 11.79 and 21.44 sec; the BGA's running times are 2. <ref type="bibr">34, 7.35, 11.27, 17.</ref>06 and 26.44 sec. For the second set of simulation studies, the BDLS algorithm's running times with an increasing number of machines are 1.09, 2.12, 3.53, 3.93 and 5.05 sec; the BGA's running times are 7.35, 8.70, 9.03, 11.17 and 11.53 sec. For the simulation of the CSTEM executing on relatively large HC systems, the BDLS algorithm's running times are 0.09, 0.29, 0.50, 0.87 and 1.19 sec; the BGA's running times are 0.97, 1.87, 2.00, 2.80 and 2.90 sec. As expected, the running times of both algorithms increase with an increasing number of tasks and machines. Furthermore, the BDLS algorithm runs faster than the BGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, two algorithms are developed for matching and scheduling a DAG-structured application with the goal of minimizing execution time and failure probability of the application. Since it is not usually possible to acheive these two conflicting objectives at the same time, both algorithms are designed to trade off execution time against the failure probability of the application. The BDLS algorithm is obtained by modifying an existing scheduling algorithm, the DLS algorithm. In a similar manner, the steps taken to transform the DLS algorithm into the BDLS algorithm may be followed to have other existing scheduling algorithms account for the reliability of resources in making scheduling decisions. The BGA is inspired by the fact that GAs are very effective in producing Pareto-optimal solutions for multiobjective optimization problems. Thus, the BGA can be used not only for producing non-dominated task assignments but also as a benchmark algorithm against which the performance of new algorithms can be compared.</p><p>The simulation studies in the previous section showed that both the BDLS algorithm and the BGA are capable of trading execution time for the failure probability of applications. As a result, both algorithms can attain both objectives to some degree by producing compromise task assignments. Furthermore, having a compromise task assignment is important in that it may be required for a large, long running application to limit its execution time with relatively low failure probability.</p><p>According to the simulation results, there is no clear winner between the BDLS algorithm and the BGA. A disadvantage of the BGA is that it is slow compared with the BDLS. If the problem size is big, i.e. the numbers of machines and tasks are in the order of thousands, it will take a long time for the BGA to converge. On the other hand, even for such big problems, the BDLS algorithm can be used to produce solutions with a reasonable degree of quality in a relatively short time period. The solutions generated by the BDLS algorithm can be used by the BGA as initial solutions to come up with non-dominated task assignments meeting the desired trade-off.</p><p>Finally, a new mathematical model is proposed to compute the failure probability of a DAG-structured application executing on a HC system. As discussed, the proposed model provides a more accurate estimation of the failure probability than the models presented in the literature. In addition, unique to the model is that it is not restricted to tree network topologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1.An example allocation of a task graph on an HC system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>is implemented: (i) generate an initial population; (ii) evaluate the fitness of each chromosome; (iii.a) repeat The Computer Journal Vol. 48 No. 3, 2005</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>The 5 RDLSFIGURE 4 .</head><label>54</label><figDesc>FIGURE 4. Average schedule length and failure probability of applications with CCR = 1.0 under the BDLS algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 RDLSFIGURE 5 .</head><label>55</label><figDesc>FIGURE 5. Average schedule length and failure probability of applications with CCR = 1.0 under the BGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 RDLSFIGURE 6 .</head><label>56</label><figDesc>FIGURE 6. Average schedule length and failure probability of applications with CCR = 10.0 under the BDLS algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 RDLSFIGURE 7 .</head><label>57</label><figDesc>FIGURE 7. Average schedule length and failure probability of applications with CCR = 10.0 under the BGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4 ∆ 5 RDLSFIGURE 8 .</head><label>458</label><figDesc>FIGURE 8. Average schedule length and failure probability of applications with CCR = 1.0 under the BDLS algorithm with respect to an increasing number of machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 RDLSFIGURE 9 .</head><label>59</label><figDesc>FIGURE 9. Average schedule length and failure probability of applications with CCR = 1.0 under the BGA with respect to an increasing number of machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>5 RDLSFIGURE 11 .</head><label>511</label><figDesc>FIGURE 11. Average schedule length and failure probability of the CSTEM with CCR = 1.0 under the BGA with respect to an increasing number of machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For example, in order to compute the reliability of the communication between machines m 2 and m 4 in Figure1, only the reliability of resources in set R4 2,4 = {m 2 , n 2,7 , m 7 , m 9 , m 11 , n 11,4 , m 4 } needs to be considered. Note that each simple path between machines m 2 and m 4 includes the resources in R4 2,4 , where p 1 2,4 = {m 2 , n 2,7 , m 7 , n 7,8 , m 8 , n 8,9 , m 9 , n 9,11 , m 11 , n 11,4 , m 4 }, p 2 2,4 = {m 2 , n 2,7 , m 7 , n 7,6 , m 6 , n 6,9 , m 9 , n 9,11 , m 11 , n 11,4 , m 4 }, p 3 2,4 = {m 2 , n 2,7 , m 7 , n 7,8 , m 8 , n 8,9 , m 9 , n 9,10 , m 10 , n 10,12 , m 12 , n 12,11 , m 11 , n 11,4 , m 4 } and p 4 2,4 = {m 2 , n 2,7 , m 7 , n 7,6 , m 6 , n 6,9 , m 9 , n 9,10 , m 10 , n 10,12 , m 12 , n 12,11 , m 11 , n 11,4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>λ r i t</cell><cell>(14)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>t</cell></row><row><cell></cell><cell></cell><cell>s,t</cell><cell></cell></row><row><cell>where</cell><cell>Rγ s,t s,t = {r x | r x ∈</cell><cell cols="2">γ s,t i=1 p i s,t }, i.e.</cell><cell>Rγ s,t s,t is composed</cell></row><row><cell cols="5">of resources that are common to all simple paths between</cell></row><row><cell cols="3">machines m s and m t . In addition,</cell><cell cols="2">Rγ s,t s,t can be identified in</cell></row><row><cell cols="2">O(|M| + |N|) time.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Average schedule length and failure probability of the CSTEM with CCR = 1.0 under the BDLS algorithm with respect to an increasing number of machines.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">A. Do gan and F. Özgüner</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.4</cell><cell>5 x 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>∆ 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average schedule length</cell><cell>2.6 2.8 2.4</cell><cell></cell><cell></cell><cell></cell><cell>∆ 2 ∆ 3 ∆ 4 ∆ 5 RDLS</cell><cell>Average failure probability</cell><cell>0.4 0.5 0.6 0.3</cell><cell></cell><cell></cell><cell></cell><cell>∆ 1 ∆ 2 ∆ 3 ∆ 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell>∆ 5</cell></row><row><cell></cell><cell>2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Number of machines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Number of machines</cell><cell></cell></row><row><cell></cell><cell>2.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.1</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>RDLS</p>FIGURE 10.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The Computer Journal Vol. 48 No.<ref type="bibr" target="#b8">3,</ref> 2005    </figDesc><table><row><cell></cell><cell></cell><cell>Biobjective Scheduling in HC Systems</cell><cell>313</cell></row><row><cell></cell><cell>4.8</cell><cell></cell></row><row><cell>0.3</cell><cell cols="2">0.6</cell></row><row><cell></cell><cell>0.6</cell><cell></cell></row><row><cell>1.4</cell><cell>23</cell><cell>23</cell></row><row><cell>0.5</cell><cell>2.0</cell><cell></cell></row><row><cell>6.5</cell><cell>0.4</cell><cell></cell></row><row><cell>90</cell><cell>0.9</cell><cell></cell></row><row><cell>5.3</cell><cell>6.5</cell><cell></cell></row></table><note><p>FIGURE 12. The task graph of CSTEM.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>The Computer Journal Vol. 48 No.<ref type="bibr" target="#b8">3,</ref> 2005   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Find all ready tasks and put them on Ready list</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Compute COST(v i , m j ) and put on List</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sort List 1 in decreasing order of DL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sort List 2 in increasing order of COST</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">Find the task-machine pair (v * i , m * j ) which minimizes Rank i</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Assign task v * i on machine m * j and then</title>
		<imprint/>
	</monogr>
	<note>update Ready list</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Heterogeneous Computing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Eshagian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Artech House</publisher>
			<pubPlace>Norwood</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Globus: a metacomputing infrastructure toolkit</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Supercomput. Ap</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="115" to="128" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A compile-time scheduling heuristic for interconnecion-constraint heterogeneous processor architectures</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Sih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parall. Distr</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic, competitive scheduling of multiple DAGs in a distributed heterogeneous environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS/SPDP Workshop on Heterogeneous Computing</title>
		<meeting><address><addrLine>Orlando, FL; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998-03-30">1998. March 30</date>
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generational scheduling for dynamic task management in heterogeneous computing systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mirabile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="219" to="236" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dynamic matching and scheduling algorithm for heterogeneous computing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maheswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS/SPDP Workshop on Heterogeneous Computing</title>
		<meeting><address><addrLine>Orlando, FL; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998-03-30">1998. March 30</date>
			<biblScope unit="page" from="57" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experimental assessment of workstation failures and their impact on checkpointing systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Elwasif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. on Fault-Tolerant Computing</title>
		<meeting>Int. Symp. on Fault-Tolerant Computing<address><addrLine>Munich, Germany; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998-06-23">1998. June 23-25</date>
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On computer communication network reliability under program execution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Area Comm</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1393" to="1399" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A heuristic taskassignment algorithm to maximize reliability of a distributed system</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Reliab</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="408" to="415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The distributed program reliability analysis on a star topology: efficient algorithms and approximate solution</title>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICS Trans. Information and Systems</title>
		<imprint>
			<biblScope unit="page" from="1020" to="1029" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>E82-D</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Task allocation for maximizing reliability of distributed computer systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1156" to="1168" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Models &amp; algorithms for reliability-oriented task-allocation in redundant distributedcomputer systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Reliab</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Task allocation algorithms for maximizing reliability of distributed computing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kartik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S R</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="719" to="724" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Safety and reliability driven task allocation in distributed systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parall. Distr</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="238" to="251" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal and suboptimal reliable scheduling of precedence-constrained tasks in heterogeneous computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPP Workshop on Network Based Computing</title>
		<meeting>ICPP Workshop on Network Based Computing<address><addrLine>Toronto, Canada; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000-08-21">2000. August 21-24</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic, reliability-driven scheduling of parallel real-time jobs in heterogeneous systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Parallel Processing</title>
		<meeting>Int. Conf. Parallel essing<address><addrLine>Valencia, Spain; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001-09-03">2001. September 3-7</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Allocation of periodic task modules with precedence and deadline constraints in distributed real-time systems</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1338" to="1356" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Matching and Scheduling of Applications in Heterogeneous Computing Systems with Emphasis on High-Performance, Reliability, and QoS</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Columbus, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic Mapping and Scheduling Algorithms for a Multi-User Heterogeneous Computing Environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Iverson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Columbus, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matching and scheduling algorithms for minimizing execution time and failure probability of applications in heterogeneous computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parall. Distr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="308" to="323" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating execution time for parallel tasks in heterogeneous processing (HP) environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khokhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPPS Workshop on Heterogeneous Computing</title>
		<meeting>IPPS Workshop on Heterogeneous Computing<address><addrLine>Cancún, Mexico; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1994-04-26">1994. April 26</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical prediction of task execution times through analytic benchmarking for scheduling in a heterogeneous environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1374" to="1379" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comprehensive survey of evolutionary-based multiobjective optimization techniques</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="269" to="308" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Introduction to Reliability Engineering</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generation of K-trees of undirected graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Patvardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Pyara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Reliab</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="208" to="211" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational complexity of network reliability analysis: an overview</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Reliab., R</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="230" to="239" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Biobjective Scheduling Algorithms for Execution Time -Reliability Trade-off in Heterogeneous Computing Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
		<idno>2003- 001</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Eskişehir, Turkey</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Electronics Engineering, Anadolu University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An overview of genetic algorithms: part 1</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">fundamentals. U. Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="58" to="69" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Task matching and scheduling in heterogeneous computing environments using a geneticalgorithm-based approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distr. Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="8" to="22" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An Analysis of Multiobjective Optimization within Genetic Algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wakefield</surname></persName>
		</author>
		<idno>ENGPJB96</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Huddersfield, UK.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convergence analysis of canonical genetic algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networ</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comparative analysis of selection schemes used in genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="69" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parallelizing existing applications in a distributed heterogeneous environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özgüner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Follen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPPS Workshop on Heterogeneous Computing</title>
		<meeting>IPPS Workshop on Heterogeneous Computing<address><addrLine>Santa Barbara, CA; Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1995-04-24">1995. April 24</date>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
