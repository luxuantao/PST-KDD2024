<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeeperGCN: All You Need to Train Deeper GCNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-13">13 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<email>guohao.li@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Center</orgName>
								<address>
									<settlement>KAUST Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
							<email>chenxin.xiong@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Center</orgName>
								<address>
									<settlement>KAUST Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Center</orgName>
								<address>
									<settlement>KAUST Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Visual Computing Center</orgName>
								<address>
									<settlement>KAUST Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeeperGCN: All You Need to Train Deeper GCNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-13">13 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.07739v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on largescale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. * equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise of availability of non-Euclidean data has recently shed interest into the topic of Graph Convolutional Networks (GCNs). GCNs provide powerful deep learning architectures for unstructured data, like point clouds and graphs. GCNs have already proven to be valuable in several applications including predicting individual relations in social networks <ref type="bibr" target="#b42">[Tang and Liu, 2009]</ref>, modelling proteins for drug discovery <ref type="bibr">[Zitnik and</ref><ref type="bibr">Leskovec, 2017, Wale et al., 2008]</ref>, enhancing predictions of recommendation engines <ref type="bibr" target="#b35">[Monti et al., 2017b</ref><ref type="bibr" target="#b52">, Ying et al., 2018]</ref>, and efficiently segmenting large point clouds <ref type="bibr" target="#b45">[Wang et al., 2018</ref><ref type="bibr" target="#b29">, Li et al., 2019b]</ref>. Recent works have looked at frameworks to train deeper GCN architectures <ref type="bibr">[Li et al., 2019b,a]</ref>. These works demonstrate how increased depth leads to state-of-the-art performances on tasks like point cloud classification and segmentation, and protein interaction prediction. The power of these deep models will become more evident with the introduction of large scale graph datasets. Recently, the introduction of the Open Graph Benchmark (OGB) <ref type="bibr" target="#b19">[Hu et al., 2020]</ref> made available an abundant number of large scale graph datasets. OGB provides graph datasets for tasks like node classification, link prediction, and graph classification.</p><p>Graph convolutions in GCNs are based on the notion of message passing <ref type="bibr" target="#b12">[Gilmer et al., 2017]</ref>. At each GCN layer, node features are updated by passing information from its connected (neighbor) nodes. To compute a new node feature in message passing, we first combine information from the node and its neighbors through an aggregation function. We then update the node feature by passing the aggregated value through a differentiable function. Given the nature of graphs, aggregation functions must be permutation invariant. This invariance property guarantees the invariance/equivariance to isomorphic graphs <ref type="bibr" target="#b2">[Battaglia et al., 2018</ref><ref type="bibr" target="#b50">, Xu et al., 2019b]</ref>. Popular choices for aggregation functions include mean <ref type="bibr" target="#b23">[Kipf and Welling, 2016]</ref>, max <ref type="bibr" target="#b14">[Hamilton et al., 2017]</ref>, and sum <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>. Recent works suggest that different aggregations have different impact depending on the task. <ref type="bibr">For et al., 2018</ref><ref type="bibr" target="#b50">, Xu et al., 2019b]</ref>. Specifically, <ref type="bibr" target="#b14">Hamilton et al. [2017]</ref> examine mean, max, and LSTM aggregators, and they empirically find that max and LSTM achieve the best performance. Graph attention networks (GATs) <ref type="bibr" target="#b43">[Veličković et al., 2018]</ref> employ the attention mechanism <ref type="bibr" target="#b1">[Bahdanau et al., 2015]</ref> to obtain different and trainable weights for neighbor nodes by learning the attention between their feature vectors and that of the central node. Thus, the aggregator in GATs operates like a learnable weighted mean. Furthermore, <ref type="bibr" target="#b50">Xu et al. [2019b]</ref> propose a GCN architecture, denoted Graph Isomorphism Network (GIN), with a sum aggregation that is able to have as large discriminative power as the Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b47">[Weisfeiler and Lehman, 1968]</ref>.</p><p>Training Deep GCNs. Despite the rapid and fruitful progress of GCNs, most previous art employ shallow GCNs. Several works attempt different ways of training deeper GCNs <ref type="bibr" target="#b14">[Hamilton et al., 2017</ref><ref type="bibr" target="#b0">, Armeni et al., 2017</ref><ref type="bibr" target="#b40">, Rahimi et al., 2018</ref><ref type="bibr" target="#b49">, Xu et al., 2018]</ref>. All these works are however limited to 10 layers of depth before GCN performance would degrade. Inspired by the benefit of training deep CNN-based networks <ref type="bibr" target="#b15">[He et al., 2016a</ref><ref type="bibr" target="#b20">, Huang et al., 2017</ref><ref type="bibr" target="#b53">, Yu and Koltun, 2016]</ref>, DeepGCNs <ref type="bibr" target="#b29">[Li et al., 2019b]</ref> propose to train very deep GCNs (56 layers) by adapting residual/dense connections (ResGCN/DenseGCN) and dilated convolutions to GCNs. DeepGCN variants achieve state-of-the art results on S3DIS point cloud semantic segmentation <ref type="bibr" target="#b0">[Armeni et al., 2017]</ref> and the PPI dataset. A further obstacle to train deeper GCNs is over-smoothing, first pointed out by <ref type="bibr">Li et al. [2018]</ref>. Recent works focus on addressing this phenomenon <ref type="bibr" target="#b24">[Klicpera et al., 2019</ref><ref type="bibr" target="#b41">, Rong et al., 2020</ref><ref type="bibr" target="#b57">, Zhao and Akoglu, 2020]</ref>. <ref type="bibr" target="#b24">Klicpera et al. [2019]</ref> proposes a PageRank-based message passing mechanism, involving the root node in the loop. Alternatively, DropEdge <ref type="bibr" target="#b41">[Rong et al., 2020]</ref> proposes randomly removing edges from the graph, and PairNorm <ref type="bibr" target="#b57">[Zhao and Akoglu, 2020]</ref> develops a normalization layer. Their works are however still limited to small scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation Learning on Graphs</head><p>Graph Representation. A graph G is usually defined as a tuple of two sets G = (V, E), where V = { v 1 , v 2 , ..., v N } and E ⊆ V × V are the sets of vertices and edges, respectively. If an edge e ij = (v i , v j ) ∈ E, for an undirected graph, e ij is an edge connecting vertices v i and v j ; for a directed graph, e ij is an edge directed from v i to v j . Usually, a vertex v and an edge e in the graph are associated with vertex features h v ∈ R D and edge features h e ∈ R C respectively.<ref type="foot" target="#foot_0">2</ref> GCNs for Learning Graph Representation. We define a general graph representation learning operator as F, which takes as input a graph G and outputs a transformed graph G , i.e. G = F(G). The features or even the topology of the graph can be learned or updated after the transformation F. Typical graph representation learning operators usually learn latent features or representations for graphs such as DeepWalk <ref type="bibr" target="#b38">[Perozzi et al., 2014]</ref>, Planetoid <ref type="bibr" target="#b51">[Yang et al., 2016]</ref>, Node2Vec <ref type="bibr" target="#b13">[Grover and Leskovec, 2016]</ref>, Chebyshev graph CNN <ref type="bibr" target="#b8">[Defferrard et al., 2016]</ref>, GCN <ref type="bibr" target="#b23">[Kipf and Welling, 2016]</ref>, MPNN <ref type="bibr" target="#b12">[Gilmer et al., 2017]</ref>, GraphSage <ref type="bibr" target="#b14">[Hamilton et al., 2017]</ref>, GAT <ref type="bibr" target="#b43">[Veličković et al., 2018]</ref> and GIN <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>. In this work, we focus on the GCN family and its message passing framework <ref type="bibr" target="#b12">[Gilmer et al., 2017</ref><ref type="bibr" target="#b2">, Battaglia et al., 2018]</ref>. To be specific, message passing based on GCN operator F operating on vertex v ∈ V at the l-th layer is defined as follows:</p><formula xml:id="formula_0">m (l) vu = ρ (l) (h (l) v , h (l) u , h (l) evu ), u ∈ N (v)<label>(1)</label></formula><formula xml:id="formula_1">m (l) v = ζ (l) ({ m (l) vu | u ∈ N (v) }) (2) h (l+1) v = φ (l) (h (l) v , m (l) v ),<label>(3)</label></formula><p>where ρ (l) , ζ (l) , and φ (l) are all learnable or differentiable functions for message construction, message aggregation, and vertex update at the l-th layer, respectively. For simplicity, we only consider the case where vertex features are updated at each layer. It is straightforward to extend it to edge features. Message construction function ρ (l) is applied to vertex features h (l) v of v, its neighbor's features h  l) guarantees the invariance/equivariance to isomorphic graphs <ref type="bibr" target="#b2">[Battaglia et al., 2018]</ref>. ζ (l) can be a simply symmetric function such as mean <ref type="bibr" target="#b23">[Kipf and Welling, 2016]</ref>, max <ref type="bibr" target="#b14">[Hamilton et al., 2017]</ref>, or sum <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>. Vertex update function φ (l) combines the original vertex features h (l) v and the aggregated message m  <ref type="table" target="#tab_0">G 1 and G</ref> </p><formula xml:id="formula_2">2 = σ G 1 , F(G 2 ) = σ F(G 1 )</formula><p>, where denotes a permutation operator on graphs.</p><p>The invariance and equivariance properties on sets or GCNs/GNNs have been discussed in many recent works. <ref type="bibr" target="#b54">Zaheer et al. [2017]</ref> propose DeepSets based on permutation invariance and equivariance to deal with sets as inputs. <ref type="bibr" target="#b33">Maron et al. [2019b]</ref> show the universality of invariant GNNs to any continuous invariant function. <ref type="bibr" target="#b22">Keriven and Peyré [2019]</ref> further extend it to the equivariant case. <ref type="bibr" target="#b32">Maron et al. [2019a]</ref> compose networks by proposed invariant or equivariant linear layers and show their models are as powerful as any MPNN <ref type="bibr" target="#b12">[Gilmer et al., 2017]</ref>. In this work, we study permutation invariant functions of GCNs, which enjoy these proven properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalized Message Aggregation Functions</head><p>To embrace the nice properties of invariance and equivariance (Property 1), many works in the graph learning field tend to use simple permutation invariant functions like mean <ref type="bibr" target="#b23">[Kipf and Welling, 2016]</ref>, max <ref type="bibr" target="#b14">[Hamilton et al., 2017]</ref> and sum <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>. Inspired by the Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b47">[Weisfeiler and Lehman, 1968]</ref>, <ref type="bibr" target="#b50">Xu et al. [2019b]</ref> propose a theoretical framework and analyze the representational power of GCNs with mean, max and sum aggregators. Although mean and max aggregators are proven to be less powerful than the WL test in <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>, they are found to be effective on the tasks of node classification <ref type="bibr" target="#b23">[Kipf and</ref><ref type="bibr">Welling, 2016, Hamilton et al., 2017]</ref> and 3D point cloud processing <ref type="bibr" target="#b39">[Qi et al., 2017</ref><ref type="bibr" target="#b46">, Wang et al., 2019]</ref>. To further go beyond these simple aggregation functions and study their characteristics, we define generalized aggregation functions in the following. Definition 2 (Generalized Message Aggregation Functions). We define a generalized message aggregation function ζ x (•) as a function that is parameterized by a continuous variable x to produce a family of permutation invariant set functions, i.e. ∀x, ζ x (•) is permutation invariant to the order of messages in the set</p><formula xml:id="formula_3">{ m vu | u ∈ N (v) }.</formula><p>In order to cover the popular mean and max aggregations into the generalized space, we further define generalized mean-max aggregation for message aggregation. It is easy to include sum aggregation. For simplicity, we focus on mean and max. Definition 3 (Generalized Mean-Max Aggregation). If there exists a pair of x, say x 1 , x 2 , such that for any message set lim</p><formula xml:id="formula_4">x→x1 ζ x (•) = Mean(•) 3 and lim x→x2 ζ x (•) = Max(•), then ζ x (•) is a generalized mean-max aggregation function.</formula><p>The nice properties of generalized mean-max aggregation functions can be summarized as follows:</p><p>(1) they provide a large family of permutation invariant aggregation functions; (2) they are continuous and differentiable on x and are potentially learnable; (3) it is possible to interpolate between x 1 and x 2 to find a better aggregator than mean and max for a given task. To empirically validate these properties, we propose two families of generalized mean-max aggregation functions based on Definition 3, namely SoftMax aggregation and PowerMean aggregation. Proposition 4 (SoftMax Aggregation). Given any message set exp(βmvi) • m vu . Here β is a continuous variable called an inverse temperature. The SoftMax function with a temperature has been studied in many machine learning areas, e.g. Energy-Based Learning <ref type="bibr" target="#b26">[LeCun et al., 2006]</ref>, Knowledge Distillation <ref type="bibr" target="#b18">[Hinton et al., 2015]</ref> and Reinforcement Learning <ref type="bibr" target="#b10">[Gao and Pavel, 2017]</ref>. Here, for low inverse temperatures β, SoftMax_Agg β (•) behaves like a mean aggregation. For high inverse temperatures, it becomes close to a max aggregation. Formally, lim β→0 SoftMax_Agg β (•) = Mean(•) and lim β→∞ SoftMax_Agg β (•) = Max(•). It can be regarded as a weighted summation that depends on the inverse temperature β and the values of the elements themselves. The full proof of Proposition 4 is in the Appendix. Proposition 5 (PowerMean Aggregation). Given any message set</p><formula xml:id="formula_5">{ m vu | u ∈ N (v) }, m vu ∈ R D , SoftMax_Agg β (•) is a generalized mean-max aggregation function, where SoftMax_Agg β (•) = u∈N (v) exp(βmvu) i∈N (v)</formula><formula xml:id="formula_6">{ m vu | u ∈ N (v) }, m vu ∈ R D + , PowerMean_Agg p (•) is a generalized mean-max aggregation function, where PowerMean_Agg p (•) = ( 1 |N (v)| u∈N (v) m p vu ) 1/p .</formula><p>Here, p is a non-zero, continuous variable denoting the q-th power.</p><p>Quasi-arithmetic mean <ref type="bibr" target="#b25">[Kolmogorov and Castelnuovo, 1930]</ref> was proposed to unify the family of mean functions. Power mean is one member of Quasi-arithmetic mean. It is a generalized mean function that includes harmonic mean, geometric mean, arithmetic mean, and quadratic mean. The main difference between Proposition 4 and 5 is that Proposition 5 only holds when message features are all positive, i.e.</p><formula xml:id="formula_7">m vu ∈ R D + . PowerMean_Agg p=1 (•) = Mean(•) and lim p→∞ PowerMean_Agg p (•) = Max(•). PowerMean_Agg p (•)</formula><p>becomes the harmonic or the geometric mean aggregation when p = −1 or p → 0 respectively. See the Appendix for the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GENeralized Aggregation Networks (GEN)</head><p>Generalized Message Aggregator. Based on the Propositions above, we construct a simple message passing based GCN network that satisfies the conditions in Proposition 4 and 5. The key idea is to keep all the message features to be positive, so that generalized mean-max aggregation functions (SoftMax_Agg β (•) and PowerMean_Agg p (•)) can be applied. We define the message construction function ρ (l) as follows:</p><formula xml:id="formula_8">m (l) vu = ρ (l) (h (l) v , h (l) u , h (l) evu ) = ReLU(h (l) u + 1(h (l) evu ) • h (l) evu ) + , u ∈ N (v)<label>(4)</label></formula><p>where the ReLU(•) function is a rectified linear unit <ref type="bibr" target="#b36">[Nair and Hinton, 2010]</ref> that outputs values to be greater or equal to zero, 1(•) is an indicator function being 1 when edge features exist otherwise 0, is a small positive constant chosen as 10 −7 . As the conditions are satisfied, we can choose the message aggregation function ζ (l) (•) to be either SoftMax_Agg β (•) or PowerMean_Agg p (•).</p><p>Better Residual Connections. DeepGCNs <ref type="bibr" target="#b29">[Li et al., 2019b]</ref> show residual connections <ref type="bibr" target="#b15">[He et al., 2016a]</ref> to be quite helpful in training very deep GCN architectures. They simply build the residual GCN block with components following the ordering: GraphConv → Normalization → ReLU → Addition. However, one important aspect that has not been studied adequately is the effect of the ordering of components, which has been shown to be quite important by <ref type="bibr" target="#b16">He et al. [2016b]</ref>. As suggested by <ref type="bibr" target="#b16">He et al. [2016b]</ref>, the output range of the residual function should to be (−∞, +∞). Activation functions such as ReLU before addition may impede the representational power of deep models. Therefore, we propose a pre-activation variant of residual connections for GCNs, which follows the ordering: Normalization → ReLU → GraphConv → Addition. Empirically, we find that the pre-activation version performs better.</p><p>Message Normalization. In our experiments, we find normalization techniques play a crucial role in training deep GCNs. We apply normalization methods such as BatchNorm <ref type="bibr" target="#b21">[Ioffe and Szegedy, 2015]</ref> or LayerNorm <ref type="bibr" target="#b1">[Ba et al., 2016]</ref> to normalize vertex features. In addition to this, we also propose a message normalization (MsgNorm) layer, which can significantly boost the performance of networks with under-performing aggregation functions. The main idea of MsgNorm is to normalize the features of the aggregated message m (l) v ∈ R D by combining them with other features during the vertex update phase. Suppose we apply the MsgNorm to a simple vertex update function MLP(h</p><formula xml:id="formula_9">(l) v + m (l) v ).</formula><p>The vertex update function becomes as follows:</p><formula xml:id="formula_10">h (l+1) v = φ (l) (h (l) v , m (l) v ) = MLP(h (l) v + s • h (l) v 2 • m (l) v m (l) v 2 ) (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where MLP(•) is a multi-layer perceptron and s is a learnable scaling factor. The aggregated message m</p><formula xml:id="formula_12">(l)</formula><p>v is first normalized by its 2 norm and then scaled by the 2 norm of h</p><formula xml:id="formula_13">(l)</formula><p>v by a factor of s. In practice, we set the scaling factor s to be a learnable scalar with an initialized value of 1. Note that when s = m</p><formula xml:id="formula_14">(l) v 2 / h (l)</formula><p>v 2 , the vertex update function reduces to the original form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We propose GENeralized Aggregation Networks (GEN), which are comprised of generalized message aggregators, pre-activation residual connections, and message normalization layers. To evaluate the effectiveness of different components, we perform extensive experiments on the Open Graph Benchmark (OGB), which includes a diverse set of challenging and large-scale datasets. We first conduct a comprehensive ablation study on the task of node property prediction on the ogbn-proteins dataset. Then, we apply our GEN framework on another node property prediction dataset (ogbn-arxiv) and two graph property prediction datasets (ogbg-molhiv and ogbg-ppa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>PlainGCN. This baseline model stacks GCN layers with {3, 7, 14, 28, 56, 112} depth and without skip connections. Each GCN layer shares the same message passing operator as GEN except the aggregation function is replaced by Sum(•), Mean(•) or Max(•) aggregation. Layer normalization <ref type="bibr" target="#b1">[Ba et al., 2016]</ref> is used in every layer before the activation function ReLU.</p><p>ResGCN. Similar to <ref type="bibr" target="#b29">Li et al. [2019b]</ref>, we construct ResGCN by adding residual connections to PlainGCN following the ordering: GraphGonv → Normalization → ReLU → Addition.</p><p>ResGCN+. We get the pre-activation version of ResGCN by changing the order of residual connections to Normalization → ReLU → GraphGonv → Addition. We denote it as ResGCN+ to differentiate it from ResGCN.</p><p>ResGEN. The ResGEN models are designed using the message passing functions described in Section 4.2. The only difference between ResGEN and ResGCN+ is that generalized message aggregators (i.e. SoftMax_Agg β (•) or PowerMean_Agg p (•)) are used instead of Sum(•), Mean(•), or Max(•). Here we freeze the values of β to 10 n , where n ∈ {−3, −2, −1, 0, 1, 2, 3, 4} and p to {−1, 10 −3 , 1, 2, 3, 4, 5}.</p><p>DyResGEN. In contrast to ResGEN, DyResGEN learns parameters β and p dynamically for every layer at every gradient descent step. By learning β and p, we avoid the need to painstakingly searching for the best hyper-parameters. More importantly, DyResGEN can learn aggregation functions to be adaptive to the training process and the dataset. We also study DyResGEN with MsgNorm layers that learn the norm scaling factor s of aggregated messages.</p><p>Datasets. Traditional graph datasets have been shown to be limited and unable to provide a reliable evaluation and rigorous comparison among methods <ref type="bibr" target="#b19">[Hu et al., 2020]</ref> due to several reason including their small scale nature, non-negligible duplication or leakage rates, unrealistic data splits, etc. Consequently, we conduct our experiments on the recently released datasets of Open Graph Benchmark (OGB) <ref type="bibr" target="#b19">[Hu et al., 2020]</ref>, which overcome the main drawbacks of commonly used datasets and thus are much more realistic and challenging. OGB datasets cover a variety of real-world applications and span several important domains ranging from social and information networks to biological networks, molecular graphs, and knowledge graphs. They also span a variety of predictions tasks at the level of nodes, graphs, and links/edges. In this work, experiments are performed on two OGB datasets for node property prediction and two OGB datasets for graph property prediction. We introduce these four datasets briefly. More detailed information about OGB datasets can be found in <ref type="bibr" target="#b19">[Hu et al., 2020]</ref>.</p><p>Node Property Prediction. The two chosen datasets deal with protein-protein association networks (ogbn-proteins) and paper citation networks (ogbn-arxiv). ogbn-proteins is an undirected, weighted, and typed (according to species) graph containing 132, 534 nodes and 39, 561, 252 edges. All edges come with 8-dimensional features and each node has an 8-dimensional one-hot feature indicating which species the corresponding protein comes from. ogbn-arxiv consists of 169, 343 nodes and 1, 166, 243 directed edges. Each node is an arxiv paper represented by a 128-dimensional features and each directed edge indicates the citation direction. For ogbn-proteins, the prediction task is multi-label and ROC-AUC is used as the evaluation metric. For ogbn-arxiv, it is multi-class and evaluated using accuracy.</p><p>Graph Property Prediction. Here, we consider two datasets, one of which deals with molecular graphs (ogbg-molhiv) and the other is biological subgraphs (ogbg-ppa). ogbg-molhiv has 41, 127 subgraphs, while ogbg-ppa consists of 158, 100 subgraphs and it is much denser than ogbg-molhiv. The task of ogbg-molhiv is binary classification and that of ogbg-ppa is multi-class classification. The former is evaluated by the ROC-AUC metric and the latter is assessed by accuracy.</p><p>Implementation Details. We first ablate each proposed component on the ogbn-proteins dataset. We then evaluate our model on the other datasets and compare the performances with state-of-the-art (SOTA) methods. Since the ogbn-proteins dataset is very dense and comparably large, full-batch training is infeasible when considering very deep GCNs. We simply apply a random partition to generate batches for both mini-batch training and test. We set the number of partitions to be 10 for training and 5 for test, and we set the batch size to 1 subgraph. A hidden channel size of 64 is used for all the ablated models. An Adam optimizer with a learning rate of 0.01 is used to train models for 1000 epochs. We implement all our models based on PyTorch Geometric <ref type="bibr" target="#b9">[Fey and Lenssen, 2019]</ref> and run all our experiments on a single NVIDIA V100 32GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Effect of Residual Connections. Experiments in Table <ref type="table" target="#tab_0">1</ref> show that residual connections significantly improve the performance of deep GCN models. PlainGCN without skip connections does not gain any improvement from increasing depth. Prominent performance gains can be observed in ResGCN and ResGCN+, as models go deeper. Notably, ResGCN+ reaches 0.858 ROC-AUC with 112 layers, which is 0.7% higher than the counterpart in ResGCN. The average results show a consistent improvement of ResGCN+ for the three aggregators (Sum, Mean and Max). This validates the effectiveness of pre-activation residual connections. We also find that the Max aggregator performs best. . For PowerMean_Agg, we find that PowerMean_Agg reaches almost the same ROC-AUC as Mean when p = 1 (arithmetic mean). We also observe that all other orders of mean except p = 10 −3 (akin to geometric mean) achieve better performance than the arithmetic mean.</p><p>PowerMean_Agg with p = 5 reaches the best result. However, due to the numerical issues in PyTorch, we are not able to use larger p. These results empirically validate the discussion in Section 4.1 regarding Proposition 4 and 5.  <ref type="bibr" target="#b14">[Hamilton et al., 2017</ref><ref type="bibr" target="#b50">, Xu et al., 2019b]</ref>. Comparison with SOTA. We apply our GCN models to three other OGB datasets and compare all results with SOTA posted on OGB Learderboard at the time of this submission. The methods include GCN <ref type="bibr" target="#b23">[Kipf and Welling, 2016]</ref>, GraphSAGE <ref type="bibr" target="#b14">[Hamilton et al., 2017]</ref>, GIN <ref type="bibr" target="#b50">[Xu et al., 2019b]</ref>, GIN with virtual nodes, GaAN <ref type="bibr" target="#b56">[Zhang et al., 2018]</ref>, and GatedGCN <ref type="bibr" target="#b3">[Bresson and Laurent, 2018]</ref>. The provided results on each dataset are obtained by averaging the results from 10 independent runs. It is clear that our proposed GCN models outperform SOTA in all four datasets. In two of these datasets (ogbn-proteins and ogbg-ppa), the improvement is substantial. The implementation details and more intermediate experimental results can be found in the Appendix. Broader Impact. The proposed DeeperGCN models significantly outperform all the SOTA on the biological and chemical graph datasets of OGB. We believe that our models are able to benefit other scientific research to make contributions to accelerating the progress of discovering new drugs, improving the knowledge of the functions and 3D structures of proteins. Despite the good performance of our models, making GCNs go such deep does require more GPUs memory resources and consume more time. Training such deep models will potentially increase the energy consumption.</p><p>In the future, we will focus on optimizing the efficiency of DeeperGCN models.</p><p>ogbn-arxiv. We train a 28-layer ResGEN model with SoftMax_Agg β (•) aggregator where β is fixed as 0.1. Full batch training and test are applied. A batch normalization is used for each layer. The hidden channel size is 128. We apply a dropout with a rate of 0.5 for each layer. An Adam optimizer with a learning rate of 0.01 is used to train the model for 500 epochs.</p><p>ogbg-ppa. As mentioned, we initialize the node features via a Sum aggregation. We train a 28-layer ResGEN model with SoftMax_Agg β (•) aggregator where β is fixed as 0.01. We apply a layer normalization for each layer. The hidden channel size is set as 128. A dropout with a rate of 0.5 is used for each layer. We use an Adam optimizer with a learning rate of 0.01 to train the model for 200 epochs.</p><p>ogbg-molhiv. We train a 7-layer DyResGEN model with SoftMax_Agg β (•) aggregator where β is learnable. A batch normalization is used for each layer. We set the hidden channel size as 256. A dropout with a rate of 0.5 is used for each layer. An Adam optimizer with a learning rate of 0.01 are used to train the model for 300 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the corresponding edge features h evu to construct an individual message m (l) vu for each neighbor u ∈ N (v). Message aggregation function ζ (l) is commonly a permutation invariant set function that takes as input a countable unordered message set { m (l) vu | u ∈ N (v) }, where m (l) vu ∈ R D ; and outputs a reduced or aggregated message m (l) v ∈ R D . The permutation invariance of ζ (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Graph Isomorphic Equivariance). If a message aggregation function ζ is permutation invariant to the message set { m vu | u ∈ N (v) }, then the message passing based GCN operator F is equivariant to graph isomorphism, i.e. for any isomorphic graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on residual connections, model depth and aggregators on ogbn-proteins. In Table2 and Table 3, we examine SoftMax_Agg β (•) and PowerMean_Agg p (•) aggregators, respectively. As both of them are generalized mean-max aggregations, they can theoretically achieve a performance that is at least as good as Mean and Max aggregations through interpolation. For SoftMax_Agg, when β = 10 −3 , it performs similarly to Mean aggregation (0.814 vs. 0.811). As β increases to 10 2 , it achieves almost the same performance as Max aggregation. 112-layer ResGEN with SoftMax_Agg reaches the best ROC-AUC at 0.860 when β = 10 4</figDesc><table><row><cell></cell><cell></cell><cell>PlainGCN</cell><cell></cell><cell>ResGCN</cell><cell></cell><cell>ResGCN+</cell></row><row><cell cols="2">#layers Sum</cell><cell>Mean Max</cell><cell>Sum</cell><cell>Mean Max</cell><cell>Sum</cell><cell>Mean Max</cell></row><row><cell>3</cell><cell cols="6">0.824 0.793 0.834 0.824 0.786 0.824 0.830 0.792 0.829</cell></row><row><cell>7</cell><cell cols="6">0.811 0.796 0.823 0.831 0.803 0.843 0.841 0.813 0.845</cell></row><row><cell>14</cell><cell cols="6">0.821 0.802 0.824 0.843 0.808 0.850 0.840 0.813 0.848</cell></row><row><cell>28</cell><cell cols="6">0.819 0.794 0.825 0.837 0.807 0.847 0.845 0.819 0.855</cell></row><row><cell>56</cell><cell cols="6">0.824 0.808 0.825 0.841 0.813 0.851 0.843 0.810 0.853</cell></row><row><cell>112</cell><cell cols="6">0.823 0.810 0.824 0.840 0.805 0.851 0.853 0.820 0.858</cell></row><row><cell>avg.</cell><cell cols="6">0.820 0.801 0.826 0.836 0.804 0.844 0.842 0.811 0.848</cell></row><row><cell cols="4">Effect of Generalized Message Aggregators.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on ResGEN-SoftMax_Agg with different β values on ogbn-proteins.</figDesc><table><row><cell cols="2">#layers mean 10 −3 10 −2 10 −1 1</cell><cell>10</cell><cell>10 2</cell><cell>10 3</cell><cell>10 4</cell><cell>max</cell></row><row><cell>3</cell><cell cols="6">0.792 0.793 0.785 0.802 0.821 0.820 0.832 0.832 0.831 0.829</cell></row><row><cell>7</cell><cell cols="6">0.813 0.809 0.802 0.806 0.835 0.840 0.843 0.842 0.846 0.845</cell></row><row><cell>14</cell><cell cols="6">0.813 0.810 0.814 0.816 0.833 0.845 0.847 0.850 0.845 0.848</cell></row><row><cell>28</cell><cell cols="6">0.819 0.818 0.809 0.814 0.845 0.847 0.855 0.853 0.855 0.855</cell></row><row><cell>56</cell><cell cols="6">0.810 0.823 0.823 0.828 0.849 0.851 0.851 0.856 0.855 0.853</cell></row><row><cell>112</cell><cell cols="6">0.820 0.829 0.824 0.824 0.844 0.857 0.855 0.859 0.860 0.858</cell></row><row><cell>avg.</cell><cell cols="6">0.811 0.814 0.809 0.815 0.838 0.843 0.847 0.849 0.849 0.848</cell></row><row><cell cols="7">Learning Dynamic Aggregators. Many previous works have found that different aggregation</cell></row><row><cell cols="2">functions perform very differently on various datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on ResGEN-PowerMean_Agg with different p values on ogbn-proteins.Trying out every possible aggregator or searching hyper-parameters is computationally expensive. Therefore, we propose DyResGEN to explore the potential of learning dynamic aggregators by learning the parameters β, p, and even s within GEN. In Table4, we find that learning β or β&amp;s of SoftMax_Agg boosts the average performance from 0.838 to 0.850. Specifically, DyResGEN achieves 0.860 when β is learned. We also see a significant improvement in PowerMean_Agg. The dash in the table denotes missing values due to numerical issues. By default, the values of β, p, and s are initialized to 1 at the beginning of training.</figDesc><table><row><cell cols="2">#layers −1</cell><cell>10 −3 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>3</cell><cell cols="6">0.809 0.779 0.802 0.806 0.817 0.813 0.825</cell></row><row><cell>7</cell><cell cols="6">0.827 0.792 0.797 0.821 0.835 0.839 0.844</cell></row><row><cell>14</cell><cell cols="6">0.828 0.783 0.814 0.839 0.835 0.845 0.844</cell></row><row><cell>28</cell><cell cols="6">0.826 0.793 0.816 0.835 0.844 0.856 0.852</cell></row><row><cell>56</cell><cell cols="6">0.826 0.782 0.818 0.834 0.846 0.854 0.842</cell></row><row><cell>112</cell><cell cols="6">0.825 0.788 0.824 0.840 0.842 0.852 0.857</cell></row><row><cell>avg.</cell><cell cols="6">0.823 0.786 0.812 0.829 0.838 0.843 0.845</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on DyResGEN on ogbn-proteins. β, p and s denote the learned parameters.</figDesc><table><row><cell></cell><cell cols="2">SoftMax_Agg</cell><cell cols="2">PowerMean_Agg</cell></row><row><cell cols="2">#layers Fixed β</cell><cell>β&amp;s</cell><cell>Fixed p</cell><cell>p&amp;s</cell></row><row><cell>3</cell><cell cols="4">0.821 0.832 0.837 0.802 0.818 0.838</cell></row><row><cell>7</cell><cell cols="4">0.835 0.846 0.848 0.797 0.841 0.851</cell></row><row><cell>14</cell><cell cols="4">0.833 0.849 0.851 0.814 0.840 0.849</cell></row><row><cell>28</cell><cell cols="4">0.845 0.852 0.853 0.816 0.847 0.854</cell></row><row><cell>56</cell><cell cols="4">0.849 0.860 0.854 0.818 0.846 -</cell></row><row><cell>112</cell><cell cols="3">0.844 0.858 0.858 0.824 -</cell><cell>-</cell></row><row><cell>avg.</cell><cell cols="4">0.838 0.850 0.850 0.812 0.838 0.848</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons with SOTA. * denotes that virtual nodes are used.To enable training deeper GCNs, we first propose a differentiable generalized message aggregation function, which defines a family of permutation invariant functions. We believe the definition of such a generalized aggregation function provides a new view to the design of aggregation functions in GCNs. We further introduce a new variant of residual connections and message normalization layers. Empirically, we show the effectiveness of training our proposed deep GCN models, whereby we set a new SOTA on four datasets of the challenging Open Graph Benchmark.</figDesc><table><row><cell></cell><cell cols="2">GCN GraphSAGE GIN</cell><cell cols="4">GIN* GaAN GatedGCN Ours</cell></row><row><cell cols="2">ogbn-proteins 0.651 0.777</cell><cell>-</cell><cell>-</cell><cell>0.780</cell><cell>-</cell><cell>0.858</cell></row><row><cell>ogbn-arxiv</cell><cell>0.717 0.715</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.719</cell></row><row><cell>ogbg-ppa</cell><cell>0.684 -</cell><cell cols="3">0.689 0.704 -</cell><cell>-</cell><cell>0.771</cell></row><row><cell>ogbg-molhiv</cell><cell>0.761 -</cell><cell cols="3">0.756 0.771 -</cell><cell>0.777</cell><cell>0.786</cell></row><row><cell>6 Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In some cases, vertex features or edge features are absent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"> Mean(•)  denotes the arithmetic mean.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion on Generalized Message Aggregation Functions</head><p>The definition of generalized message aggregation functions help us to find a family of differentiable permutation invariant aggregators. In order to cover the Mean and Max aggregations into the function space, we propose two variants of generalized mean-max aggregation functions, i.e. SoftMax_Agg β (•) and PowerMean_Agg p (•). They can also be instantiated as a Min aggregator as β or p goes to −∞. We show an illustration of the proposed aggregation functions in Figure <ref type="figure">1</ref>. Although we do not generalize the proposed functions to the Sum aggregation in this work. It could be easily realized by introducing another control variable on the degree of vertices. For instance, we define the form of the function as N (v)  </p><p>N }). Suppose we have c elements that are equal to the maximum value m * . When β → ∞, we have:</p><p>N }). It is obvious that the conclusions above generalize to all the dimensions. Therefore, SoftMax_Agg β (•) is a generalized mean-max aggregation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof for Proposition 5</head><p>Proof. Suppose we have N = N (v) . We denote the message set as </p><p>N }). Assume we have c elements that are equal to the maximum value m * . When p → ∞, we have:</p><p>N }). The conclusions above hold for all the dimensions. Thus, PowerMean_Agg p (•) is a generalized meanmax aggregation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of DyResGEN</head><p>We provide more analysis and some interesting findings of DyResGEN in this section. The experimental results of DyResGEN in this section are obtained on ogbn-proteins dataset. We visualize the learning curves of learnable parameters β, p and s of 7-layer DyResGEN with SoftMax_Agg β (•) aggregator and PowerMean_Agg p (•) aggregator respectively. Note that MsgNorm layers with the norm scaling factor s are used. All learnable parameters are initialized as 1. Dropout with a rate of 0.1 is used for each layer to prevent over-fitting. The learning curves of learnable parameters of SoftMax_Agg β (•) are shown in Figure <ref type="figure">2</ref>. We observe that both β and s change dynamically during the training. The β parameters of some layers tend to be stable after 200 training epochs. Specially, the 6-th layer learns a β to be approximately 0 which behaves like a Mean aggregation. For the norm scaling factor s, we find that the s of the first layer converges to 0. It indicates that the network learns to aggregation less information from the neighborhood at the first GCN layer, which means a MLP layer could be sufficient for the first layer. Furthermore, we observe that the values of s at deeper layers (4, 5, 6, 7) are generally larger than the values at shallow layers (1, 2, 3). It illustrates that the network learns to scale the aggregated messages of the deeper layers with higher scaling factors. Similar phenomena of PowerMean_Agg p (•) aggregator are demonstrated in Figure <ref type="figure">3</ref>. The change of p shows a larger fluctuation than β. For the norm scaling factor s, the value at the first layer is also learned to be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Details on the Experiments</head><p>In this section, we provide more experimental details on the OGB datasets (ogbn-proteins, ogbnarxiv, ogbg-ppa and ogbg-molhiv). For a fair comparison with SOTA methods, we provide results on each dataset by averaging the results from 10 independent runs. We provide the details of the model configuration on each dataset. The evaluation results of those 10 independent runs and the corresponding mean and standard deviation are reported in Table <ref type="table">6</ref>. The mean and standard deviation are calculated from the raw results. We keep 3 and 4 decimal places for the mean and standard deviation respectively. All models are implemented based on PyTorch Geometric and all experiments are performed on a single NVIDIA V100 32GB.  ogbn-proteins. For both ogbn-proteins and ogbg-ppa, there is no node feature provided. We initialize the features of nodes through aggregating the features of their connected edges by a Sum aggregation, i.e. x i = j∈N (i) e i,j , where x i denotes the initialized node features and e i,j denotes the input edge features. We train a 112-layer DyResGEN with SoftMax_Agg β (•) aggregator. A hidden channel size of 64 is used. A layer normalization and a dropout with a rate of 0.1 are used for each layer. We train the model for 1000 epochs with an Adam optimizer with a learning rate of 0.01.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2016. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An experimental study of neural networks for variable graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the properties of the softmax function with application in game theory and reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00805</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7090" to="7099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sur la notion de la moyenne</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castelnuovo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">G. Bardi, tip. della R. Accad. dei Lincei</title>
		<imprint>
			<date type="published" when="1930">1930</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno>CoRR, abs/1910.06849</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence. AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09342</idno>
		<title level="m">On the universality of invariant networks</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
				<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Semi-supervised user geolocation via graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">04</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
