<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AUTOREGRESSIVE DIFFUSION MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-01">1 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
							<email>e.hoogeboom@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rianne</forename><surname>Van Den Berg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AUTOREGRESSIVE DIFFUSION MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-01">1 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.02037v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models <ref type="bibr" target="#b46">(Uria et al., 2014)</ref> and absorbing discrete diffusion <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep generative models have made great progress in modelling different sources of data, such as images, text and audio. These models have a wide variety of applications, such as denoising, inpainting, translating and representation learning. A popular type of likelihood-based models are Autoregressive Models (ARMs). ARMs model a high-dimensional joint distribution as a factorization of conditionals using the probability chain rule. Although very effective, ARMs require a pre-specified order in which to generate data, which may not be an obvious choice for some data modalities, for example images. Further, although the likelihood of ARMs can be retrieved with a single neural network call, sampling from a model requires the same number of network calls as the dimensionality of the data.</p><p>Recently, modern probabilistic diffusion models have introduced a new training paradigm: Instead of optimizing the entire likelihood of a datapoint, a component of the likelihood bound can be sampled and optimized instead. Works on diffusion on discrete spaces <ref type="bibr" target="#b41">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr"></ref> Figure <ref type="figure">1</ref>: Generation of Autoregressive Diffusion Models for the generation order σ = (3, 1, 2, 4). Filled circles in the first and third layers represent respectively the input and output variables, and the middle layer represents internal activations of the network. <ref type="bibr" target="#b18">Hoogeboom et al., 2021;</ref><ref type="bibr" target="#b2">Austin et al., 2021)</ref> describe a discrete destruction process for which the inverse generative process is learned with categorical distributions. However, the length of these processes may need to be large to attain good performance, which leads to a large number of network calls to sample from or evaluate the likelihood with discrete diffusion.</p><p>In this work we introduce Autoregressive Diffusion Models (ARDMs), a variant of autoregressive models that learns to generate in any order. ARDMs generalize order agnostic autoregressive models and discrete diffusion models. We show that ARDMs have several benefits: In contrast to standard ARMs, they impose no architectural constraints on the neural networks used to predict the distribution parameters. Further, ARDMs require significantly fewer steps than absorbing models to attain the same performance. In addition, using dynamic programming approaches developed for diffusion models, ARDMs can be parallelized to generate multiple tokens simultaneously without a substantial reduction in performance. Empirically we demonstrate that ARDMs perform similarly to or better than discrete diffusion models while being more efficient in modelling steps. The main contributions of this paper can be summarized as follows: 1) We introduce ARDMs, a variant of order-agnostic ARMs which include the ability to upscale variables. 2) We derive an equivalence between ARDMs and absorbing diffusion under a continuous time limit. 3) We show that ARDMs can have parallelized inference and generation processes, a property that among other things admits competitive lossless compression with a modest number of network calls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>ARMs factorize a multivariate distribution into a product of D univariate distributions using the probability chain rule. In this case the log-likelihood of such as model is given by:</p><formula xml:id="formula_0">log p(x) = D t=1 log p(xt|x&lt;t),<label>(1)</label></formula><p>where x &lt;t is shorthand for x 1 , x 2 , . . . , x t−1 . ARMs are trained by ensuring that the neural network has a triangular dependency structure, for instance implemented via causal masking. Although this allows parallelized computation of the likelihood for all conditional distributions at once, to sample from the model it requires D iterative sampling steps x 1 ∼ p(x 1 ), x 2 ∼ p(x 2 |x 1 ) towards x D ∼ p(x D |x 1 , x 2 , . . . , x D−1 ).</p><p>Order Agnostic ARMs Order Agnostic ARMs (OA-ARMs) <ref type="bibr" target="#b46">(Uria et al., 2014)</ref> generate variables with a random ordering σ ∈ S D , where S D represents a set of all permutations of the integers 1, . . . , D. The log-likelihood of this model is given by:</p><formula xml:id="formula_1">log p(x) ≥ E σ∼U (S D ) D t=1 log p(x σ(t) |x σ(&lt;t) ).<label>(2)</label></formula><p>This can be seen as a latent variable model and the log-likelihood is derived via Jensen's inequality: log p(x) = log E σ∼U (S D ) p(x|σ) ≥ E σ∼U (S D ) log p(x|σ).</p><p>Mind that hereafter we leave out σ in our notation to avoid clutter. One approach to train OA-ARMs is the procedure as described by <ref type="bibr" target="#b42">Yang et al. (2019)</ref> for XLNet. It takes a permutation equivariant network such as a Transformer that is causally masked. Then, inputs are permuted and outputs are permuted back according to a given order, which models the sequence in that specific order. However, such approaches typically suffer in likelihood score and cannot be combined with simple non-equivariant transformations such as convolutional layers.</p><p>Discrete Diffusion Discrete diffusion models define a destruction process on discrete data. An example is absorbing diffusion <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, for which each variable has a probability of decaying to an absorbing state. The opposite process to the destruction process is the learned generative process. This generative process models the distribution over variables that are currently absorbed, and generates these with a probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AUTOREGRESSIVE DIFFUSION MODELS</head><p>We introduce Autoregressive Diffusion Models (ARDMs). ARDMs generate variables in an arbitrary order, one by one. Further, ARDMs are able to upscale variables, such as the bit values of a Algorithm 1 Sampling from OA-ARDMs Input: Network f Output: Sample x Initialize x = 0 Sample σ ∼ U(S D ) for t in {1, . . . , D} do m ← (σ &lt; t) and n ← (σ = t)</p><formula xml:id="formula_2">x ∼ C(x|f (m x)) x ← (1 − n) x + n x</formula><p>Algorithm 2 Optimizing OA-ARDMs</p><formula xml:id="formula_3">Input: Datapoint x, Network f Output: ELBO L Sample t ∼ U(1, . . . , D) Sample σ ∼ U(S D ) Compute m ← (σ &lt; t) l ← (1 − m) log C(x|f (m x)) L t ← 1 D−t+1 sum(l) L ← D • L t</formula><p>pixel. Unlike standard ARMs, ARDMs are trained on a single step in the objective, as in modern diffusion models. In addition, both sampling and inference of ARDMs can be parallelized using dynamic programming with minimal degradation in log-likelihood.</p><p>Order Agnostic ARDMs The main difficulty of parameterizing an autoregressive model from an engineering perspective, is the need to enforce the triangular or causal dependence. Especially for 2D signals, this triangular dependence is difficult to enforce for arbitrary orders <ref type="bibr" target="#b20">(Jain et al., 2020)</ref> and tedious design is needed for multi-scale architectures <ref type="bibr" target="#b38">(Salimans et al., 2017)</ref>. To relax this requirement, we take inspiration from modern diffusion-based generative models. Using these insights, we derive an objective that is only optimized for a single step at a time. Starting at Equation 2, a different objective for an order agnostic ARM can be derived, by replacing the summation over t by an expectation that is appropriately re-weighted:</p><formula xml:id="formula_4">log p(x) ≥ E σ∼U (S D ) D t=1 log p(x σ(t) |x σ(&lt;t) ) = E σ∼U (S D ) D • E t∼U (1,...,D) log p(x σ(t) |x σ(&lt;t) ) = D • E t∼U (1,...,D) E σ∼U (S D ) 1 D − t + 1 k∈σ(≥t) log p(x k |x σ(&lt;t) )</formula><p>Compactly, we can write the expected lower bound as:</p><formula xml:id="formula_5">log p(x) ≥ E t∼U (1,...,D) [D • L t ], where L t = 1 D − t + 1 E σ∼U (S D ) k∈σ(≥t) log p(x k |x σ(&lt;t) ).</formula><p>Here the term L t represents the likelihood component for step t. Importantly, we do not need to optimize for all L t terms of a datapoint simultaneously. Instead, for each datapoint in a minibatch a single L t term is optimized where t is sampled from a uniform distribution. This objective was originally proposed by <ref type="bibr" target="#b46">Uria et al. (2014)</ref> to train order-agnostic ARMs. We will develop ARDMs starting from this perspective and refer to the special case of an order-agnostic ARM, as an order agnostic ARDM (OA-ARDM). Interestingly, each L t component can be seen as a BERT-like training objective <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, where exactly D − t + 1 tokens are masked and subsequently predicted. Therefore, an OA-ARDM is trained as a collection of D BERTs with loss terms L t , which contain the reweighting term</p><formula xml:id="formula_6">1 D−t+1 .</formula><p>Another insight is that this generative process is very similar to absorbing diffusion, where the model aims to generate absorbed (or masked) variables. In certain situations we might want to refer to loss terms instead of likelihood terms, so we define L t = −L t .</p><p>Figure <ref type="figure">2</ref>: ARDM training step. This step optimizes for step t = 2 for all possible permutations σ simultaneously which satisfy σ(1) = 3.</p><p>Parametrization We desire a parametrization for the model distribution log p(x k |x σ(&lt;t) ) for k ∈ σ(≥ t) for all σ and t. For each σ and t it is in principle allowed to have an entirely new neural network. However, this would be very inconvenient as the number of t grows as O(D) and the number of σ grows as O(D!). Instead, a single neural network is utilized and shared for different σ and t. This is implemented by masking variables at the input, and predicting those at the output. To be precise, we let x ∈ X = {1, 2, . . . , K} D represent discrete variables with K classes and a neural network f : X → R D×K that outputs probability vectors for each dimension. Conditioning is done via masking: For a given permutation array σ, we compute the elementwise comparison m = σ &lt; t which produces a Boolean mask. The mask is then used by predicting θ = f (m x), where denotes element-wise multiplication. For each location k ∈ σ(≥ t), the log probability vectors θ k are used. Letting C(x k |θ k ) denote a categorical distribution over x k with class probabilities θ k , we choose to model log(x k |x σ(&lt;t) ) = log C(x k |θ k ). The locations of these relevant indices k ∈ σ(≥ t) are retrieved by using the opposite mask 1 − m. The procedure to sample and optimize an ARDM with this parametrization are given in Algorithms 1 and 2. A training step is visualized in Figure <ref type="figure">2</ref>. Note that opposed to Figure <ref type="figure">1</ref> where only a single output was used per step, in the train step all dimensions that were masked are predicted simultaneously. Therefore, there are multiple variables to predict which ensures there is sufficient signal to optimize the model.</p><p>For clarity some of the implementation details have been left out in the explanation above. However, given that these are important for practical implementations, they are specified in the following. The input to the function f may be different depending on the data modality: For images and audio, the mask is applied to the input so that values are zero after feature normalization. The mask itself is also concatenated to the input as an input representation, which allows the model to identify whether a value is actually zero, or the value is in the absorbing state zero. For language the input representation is augmented and absorbed values are instead set to a new class K + 1, in which case there is no need to provide the mask itself as input to the model. More generally, we can represent the masked state as an absorbing state vector a which has the same shape as x but only contains a pre-specified value. The input to the network is then not the masked m x, but instead the combination m x + (1 − m) a. In addition, the network f may also take the time component t as input as is typically done in diffusion models <ref type="bibr" target="#b16">(Ho et al., 2020)</ref>. In summary the network takes some additional inputs as θ = f (i, m, t) where i = m x + (1 − m) a and the processing of x may be different depending on the type of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PARALLELIZED ARDMS</head><p>An important property of our parametrization is that the distribution over multiple variables is predicted at the same time. In this section, we will leverage this parameterization to allow parallel independent generation of variables. Essentially, we desire distributions over x σ(t+k) for positive k while conditioning only on x σ(&lt;t) . First we make an observation regarding a connection between predicting future variables and our likelihood terms: For k = 1, 2, . . . , D − t:</p><formula xml:id="formula_7">Eσ log p(x σ(t+k) |x σ(&lt;t) ) = Eσ log p(x σ(t) |x σ(&lt;t) ) = Lt,<label>(3)</label></formula><p>due to the uniform expectation over permutations. In other words, it does not matter which step t + k the model predicts, in expectation these all have the same associated likelihood. As a result, order agnostic generation of k tokens independently, starting from the t-th variable will result in a log-probability contribution of k • L t in a single step, whereas the traditional approach would take k steps at the cost of k i=1 L t+i . This knowledge is sufficient to construct a dynamic programming algorithm as described by <ref type="bibr" target="#b52">Watson et al. (2021)</ref> to compute how many parallel steps to take at which moment, given a budget. Since dynamic programming is typically described from a minimization perspective we define the loss component L t = −L t , which is measured in bits. In terms of loss, generating k variables at timestep t will cost k • L t bits. Further, we define the transition cost matrix L t,t+k = k • L t for positive integers k and L t+k,t = 0 otherwise. So L t,t+k exactly describes how much it costs to model the next k variables in parallel starting at the t-th position for all relevant t and k. Using this transition cost matrix, the dynamic programming algorithm can be utilized to find which steps should be parallelized. For instance, in the example in Figure <ref type="figure">3</ref> a hypothetical 20-step problem is given a budget of 5 steps. Typically, the algorithm will spend more steps on regions with large differences between L t components and fewer steps on regions where the L t components are Figure <ref type="figure">3</ref>: Loss components for Parallelized ARDMs using a budget of 5 steps for a problem of 20 steps. Left: individual loss component for every step. Right: parallelized policy extracted from the dynamic programming algorithm. Components of the same height are modelled simultaneously, so they are inferred and generated in parallel. approximately equal. Parallelizing an ARDM may incur some cost, as for a well-calibrated model:</p><formula xml:id="formula_8">Lt = Eσ log p(x σ(t+1) |x σ(&lt;t) ) ≤ Eσ log p(x σ(t+1) |x σ(&lt;t+1) ) = Lt+1,<label>(4)</label></formula><p>but can be traded off for faster generation because fewer steps are used. In other words, the loss components L t are monotonically decreasing over t and parallelizing a model incurs a cost, which the algorithm aims to minimize. Recall that this is under the assumption that model is well-calibrated, which is observed in practice. See Figure <ref type="figure">3</ref> for an example of a parallelized schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DEPTH UPSCALING ARDMS</head><p>Order agnostic ARDMs learn to generate variables in random order. As a result, decisions on very detailed information (such as the least significant bit in an image) are modelled relatively early in the generative process. Instead, we can structure the process into stages, where for each stage a refinement of the variable is generated. We refer to this process as upscaling. For example, instead of generating an entire 256-categorical variables at once, we can first generate the most significant bit, and then the subsequent bits in order of significance. To define the process, it is helpful to first imagine the opposite process to upscaling, which is the destructive process downscaling. Formally, we can define maps via transition matrices P (i) that define how a data variable downscales from its data value towards a common absorbing state. For simplicity assume single dimensional variables at this moment. Denote the absorbing state as a one-hot vector x (0) , where all values are zero except at a prespecified index a so that x (0) a = 1. From a diffusion perspective, upscaling is complementary to a downscaling destruction process where each variable decays by zeroing its least significant bit.</p><p>Let P (1) , . . . , P (S) define a sequence of downscaling maps so that for any categorical one-hot data variable x (S) ∈ {0, 1} K , it holds that P (1) • . . . • P (S) • x (S) = x (0) . In other words, any category K decays to the common absorbing state after S downscaling maps. We now define the upscaling generative process by learning the reverse of the downscaling map, specifically by modelling p(x (S) |x (S−1) ) • . . . • p(x (2) |x (1) )p(x (1) ). The transition matrices allow easy transitions between the different stage variable x (i) via the following rules: S) , where P (s) = P (s) • P (s+1) • . . . • P (S) .</p><formula xml:id="formula_9">x (s) = P (s+1) x (s+1) = P (s+1) x (</formula><p>The matrices P (i+1) are computed as cumulative matrix multiplications, and allow a transition directly from a datapoint x (S) to the corresponding downscaled variable x (i) . This is particularly useful during training, where the model will only be optimized for a single specific stage per datapoint. For implementations it is generally useful to define P (S+1) = I as an identity matrix so that the above equation also holds when s = S. To train Upscale ARDMs, we can extend Algorithm 2:</p><p>In addition to sampling a timestep t, a stage i ∼ U(1, . . . , S) to optimize is sampled. For this particular stage, the ARDM models p(x (s) |x (s−1) ) by sampling a permutation σ within the stage and a timestep t within the stage. Every term p(x (s) |x (s−1) ) represents a stage that is modelled with an order agnostic ARDM. This highlights an interesting property of ARDMs: Although sampling from a model may take up to D • S steps, the training complexity has not changed by modelling multiple stages. As a result, one can experiment with adding an arbitrary number of stages without an increase in computational complexity during training. Depth upscaling is reminiscent of the upscaling networks proposed in <ref type="bibr" target="#b24">(Kalchbrenner et al., 2018;</ref><ref type="bibr" target="#b33">Menick &amp; Kalchbrenner, 2019)</ref>, with the important differences that Upscale ARDMs model the variables order-agnostic and only utilize a single neural network to parametrize all stages. For a more detailed explanation that includes the dimensionality of the variables {x (s) } see Appendix A.</p><p>Bit Upscaling Depth upscaling is easiest demonstrated with an example, bit-upscaling. Consider the task of generating a standard image with pixel values {0, . . . , 255} so that an image with D dimensions can be represented by x (8) ∈ {0, 1} D×256 in onehot notation. Imagine a downscaling process defined by the function that removes the i least significant bits: lsb s (k) = k/2 s • 2 s , via this function we can define our transition matrices:</p><formula xml:id="formula_10">P (8+1−s) l,k = 1 if l = lsb s (k) and k ∈ Im(lsb s−1 ) otherwise P (8+1−s) l,k = 0,</formula><p>where {P (s) } are indexed starting at zero. Notice that in this case 8 stages to map every value to the absorbing state 0, because lsb 8 (k) = 0 for any k ∈ {0, . . . , 255}. See Figure <ref type="figure">4</ref> for a visualization of such matrices for a problem with less categories.</p><p>Figure <ref type="figure">4</ref>: Bit upscaling matrices for data with eight categories and hence three stages, meaning S = 3. Entries that are white represent zeros, coloured entries represent ones.</p><p>Depth upscaling is not confined to bits, and indeed a more general formulation is given by the downscaling map l = k/b s • b s , for a branching factor b. When b is set to 2, the bit upscaling transitions are retrieved as a special case. When b is set to higher values, then variables can be generated in fewer stages, S = log b (K) to be exact. This allows for a unique trade-off between the number of steps the model takes and the complexity that each modelling step inhibits. Other hand-crafted transitions are also imaginable, not excluding transitions that augment the space to new categories, but these are not considered in this paper.</p><p>Parametrization of the Upscaling Distributions Although it is now defined how a datapoint x (S) downscales to x (S−1) , . . . , x (1) and to its absorbing state x (0) , it is not immediately clear to parametrize the distributions p(x (s) |x (s−1) ). Two methods can be used to parametrize the distribution. The first is a direct parametrization. In the example of the bit-upscaling model above, one models the s-th significant bits given the (s − 1)-th significant bits. The direct parametrization is generally more computationally efficient, as it requires only distribution parameter outputs that are relevant for the current stage. This is especially useful when the number of classes is large (such as with audio, which has 2 16 classes). However, it can be somewhat tedious to figure out exactly which classes are relevant and should be modelled.</p><p>Alternatively we can use a data parametrization which is similar to the parametrization in <ref type="bibr" target="#b2">Austin et al. (2021)</ref>. An important difference with their work is that the downscaling matrices P (s) represent deterministic maps while theirs represent a stochastic process. For this parametrization, the network f outputs a probability vector θ that matches the shape of the data x (S) , which transformed and converted to the relevant probabilities in stage s via:</p><formula xml:id="formula_11">θ (s) = P (s) T x (s−1) P (s+1) θ x (s−1) T P (s) θ where p(x (s) |x (s−1) ) = C(x (s) |θ (s) ).</formula><p>The advantage of this parametrization is that one only has to define the transition matrices {P (s) }.</p><p>As a result, the appropriate probabilities can be automatically computed which is ideal for experimentation with new downscaling processes. The disadvantage may be that modelling full probability vectors for problems with high number of classes may be expensive and not even fit in memory.</p><p>Empirically in our experiments on image data we find that there is no meaningful performance difference between the two parametrizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Autoregressive Models Autoregressive Models (ARMs) factorize a joint distribution into a product of conditional distributions <ref type="bibr" target="#b3">(Bengio &amp; Bengio, 2000;</ref><ref type="bibr">Larochelle &amp; Murray, 2011)</ref>. Advances in deep learning have allowed tremendous progress on various modalities, such as images (van den <ref type="bibr" target="#b50">Oord et al., 2016b;</ref><ref type="bibr" target="#b8">Child et al., 2019</ref><ref type="bibr">, i.a.), audio (van den Oord et al., 2016a;</ref><ref type="bibr">Kalchbrenner et al., 2018, i.a.)</ref>, and text <ref type="bibr" target="#b4">(Bengio et al., 2003;</ref><ref type="bibr" target="#b14">Graves, 2013;</ref><ref type="bibr" target="#b32">Melis et al., 2018;</ref><ref type="bibr" target="#b35">Merity et al., 2018;</ref><ref type="bibr">Brown et al., 2020, i.a.)</ref>, where for the latter they are referred to as language models.</p><p>Although evaluating the likelihood of a datapoint is generally efficient with ARMs, sampling requires an iterative process with as many network calls as the dimensionality of the data. Parallelized ARM approaches often rely either on cutting many dependencies in the conditioning <ref type="bibr" target="#b37">(Reed et al., 2017)</ref> which tend to suffer in log-likelihood. Alternatively, ARMs can be solved using fixed-point iteration algorithms in fewer steps without sacrificing log-likelihood <ref type="bibr" target="#b53">(Wiggers &amp; Hoogeboom, 2020;</ref><ref type="bibr" target="#b43">Song et al., 2021)</ref>, but these methods typically still require a large number of steps to converge.</p><p>Order agnostic sequence modelling was introduced in <ref type="bibr" target="#b46">(Uria et al., 2014)</ref> and utilizes the same objective as AO-ARDMs to optimize the model, operating by masking and predicting variables. Different from their method, ARDMs have more choices in absorbing states, parallelization support and  depth upscaling techniques, in addition to modern advances to fit larger scale data. An alternative approach for order agnostic modelling is via causally masked permutation equivariant models such as Transformers <ref type="bibr" target="#b42">(Yang et al., 2019;</ref><ref type="bibr" target="#b1">Alcorn &amp; Nguyen, 2021)</ref>, but these have had limited success in likelihood-based tasks. In <ref type="bibr" target="#b13">(Ghazvininejad et al., 2019)</ref> a mask predict method is proposed, although it does not contain a likelihood analysis. In other work, mixtures of ARMs over certain orders are trained by overriding convolutional routines for masking <ref type="bibr" target="#b20">(Jain et al., 2020)</ref>. In a different context in <ref type="bibr" target="#b30">(Liu et al., 2018)</ref> graph edges connected to a node are modelled without order. However, the model is not entirely order agnostic because it models edges centered around focus nodes.</p><p>Diffusion Models Diffusion models learn to denoise a Gaussian base distribution into the distribution of the data via a chain of latent variables <ref type="bibr" target="#b42">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b41">Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b16">Ho et al., 2020)</ref>. Diffusion and score-matching methods have shown large improvements in image <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref> and audio sample quality <ref type="bibr" target="#b6">(Chen et al., 2020;</ref><ref type="bibr">Kong et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Order Agnostic Modelling To better understand how ARDMs compare to other order agnostic generative models, we study their performance on a character modelling task using the text8 dataset <ref type="bibr" target="#b31">(Mahoney, 2011)</ref>. ARDMs are compared to D3PMs that model the inverse absorbing diffusion process <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, and causally masked Transformers that are directly optimized on randomly permuted sequences as done in XLNet <ref type="bibr" target="#b42">(Yang et al., 2019)</ref>. The different methods all use the same underlying neural network architecture which is the Transformer used in <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, which has 12 layers, 786 hidden dimensions and 12 heads. For the OA-Transformer baseline the architecture is causally masked, and inputs are permuted to model the sequence in a specific order. In addition to the standard positional embeddings for the input, the embeddings for the output are also concatenated to the token embedding. This can be seen as an implicit method to condition on the permutation that is currently generated. The specific hyperparameters of the optimization procedure are specified in Appendix D and are the same as reported in <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, with the exception of a different learning rate schedule and further train steps.</p><p>Performance of these methods is presented in Table <ref type="table" target="#tab_0">1</ref>. Firstly, the OA-Transformer baseline does not perform very well compared to the other models. This result matches the behaviour that was found  <ref type="bibr">et al., 2009)</ref> where ARDMs also outperform D3PMs and degrade more gracefully under fewer steps. This comparison to D3PM is however less direct, as the underlying architectures differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lossless Compression</head><p>To validate that ARDMs can form a viable basis for practical neural network-based compressors, we study their performance when compressing CIFAR-10 images and comparing them to existing methods. Since ARDMs provide probabilities for a sequence of symbols, they can be directly used together with an off-the-shelf entropy coder for lossless compression.</p><p>In this experiment we use the range-based entropy coder rANS <ref type="bibr" target="#b11">(Duda, 2009)</ref>. To use ARDMs the order of the coding process needs to be fixed for all images. To avoid an unlucky sample, before coding we evaluate the log-likelihood of a few random permutations on the train set and pick the best performing one. Empirically, there is very little difference in performance (&lt; 0.02 bpd) between different permutations.</p><p>Several deep learning based lossless compression methods in literature rely on bits-back coding <ref type="bibr" target="#b44">(Townsend et al., 2019)</ref>, such as LBB <ref type="bibr" target="#b15">(Ho et al., 2019)</ref>, HiLLoC <ref type="bibr" target="#b45">(Townsend et al., 2020)</ref> and VDM <ref type="bibr" target="#b26">(Kingma et al., 2021)</ref>. Although bits-back coding methods can perform well on large datasets, they have a large overhead when used as per-image compressors. This is caused by the large number of initial bits that are required. Further, the dataset is often interlinked, meaning that if an image in the middle of the dataset needs to be accessed, it requires all images earlier in the bitstream to also be decompressed. Therefore per-image compression is important for practical applications, because it is desirable to be able to send a specific image without sending an entire dataset. On the other hand, direct compressors such as L3C <ref type="bibr" target="#b34">(Mentzer et al., 2019)</ref>, IDF <ref type="bibr" target="#b17">(Hoogeboom et al., 2019)</ref> and IDF++ (van den <ref type="bibr">Berg et al., 2021)</ref> do not incur an intial message overhead and their dataset performance translates directly to per-image compression. A more conventional codec is FLIF <ref type="bibr" target="#b40">(Sneyers &amp; Wuille, 2016)</ref>, which is a recent lossless compression codec with machine learning components that outperforms traditional codecs such as PNG.</p><p>Performance of ARDMs and related methods in literature is presented in Table <ref type="table" target="#tab_3">3</ref>. ARDMs significantly outperform all methods on compression per image, requiring only 2.71 bpd versus 3.26 for the next best performing model, IDF++. In addition, even compared to a setting where an entire dataset needs to be compressed, ARDMs perform competitively to VDM, which attain 2.72 bpd. Moreover, ARDMs degrade more gracefully when fewer steps are used to encode the data.</p><p>Note that the lossless compressor based on VDM was trained on non-augmented data, whereas the best-performing likelihood model of Kingma et al. ( <ref type="formula">2021</ref>) was trained with data augmentation. As a result, it is likely that their dataset compression results could be somewhat improved when trained on augmented CIFAR-10. Also, it is not a coincedence that HiLLoC and FLIF have the exact same compression per image performance. HiLLoC compresses the first few images using the FLIF format to fill the initial bitstream, and compresses the remaining images in the dataset with bits-back coding <ref type="bibr" target="#b45">(Townsend et al., 2020)</ref>. As a result, on a per-image compression benchmark the method is equivalent to FLIF.</p><p>Effects of Depth-Upscaling A natural question that might arise is how standard order-agnostic modelling performs compared to order agnostic bit-upscaling, and how bit-upscaling compares to   <ref type="bibr" target="#b51">(Warden, 2018)</ref>. For the audio data, the total number of categories is 2 16 , which is typically too large in terms of memory to model as a single softmax distribution. For that reason, the single stage OA-ARDM is trained using a discretized logistic distribution because it is computationally cheaper for a high number of categories. For the same reason, the Upscale ARDMs for audio can only be trained using the direct parametrization, whereas for images they are trained with the data parametrization.</p><p>For images, the best performing model has an upscaling factor of 4 with 2.64 bpd (see Table <ref type="table" target="#tab_5">5</ref>) and for audio the best performing model upscales by a factor of 2 or 4 with 6.29 bpd (see Table <ref type="table" target="#tab_4">4</ref>). The hypothesis is that as the upscale factor becomes smaller, the generative process generally becomes more structured and easier to model. However, although for audio this pattern is consistently observed, for images an upscale factor of 4 has better performance than an upscale factor of 2. It is possible that for certain data, at some point smaller upscale factors give diminishing returns for performance. We hypothesize that by prolonging the generative process, the model may get less gradient signal per optimization step, leading to the decreased performance of smaller upscale factors in some situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND CONCLUSION</head><p>Notwithstanding the good results in this paper, there are some limitations to ARDMs. 1) Even though ARDMs outperform all other order-agnostic approaches on text, there is still a gap to the performance of single-order autoregressive models. In preliminary experiments, upscale variants for language did not perform better than the order-agnostic versions. 2) In the current description, ARDMs model discrete variables. In principle one could also define absorbing processes for continuous distributions. 3) Finally, in this work we have focused on optimizing for log-likelihood, because it directly corresponds to coding length in lossless compression. However when optimizing for other objectives such as sample quality, different architectural choices may give better results.</p><p>In conclusion, we introduced ARDMs, a new class of models at the intersection of autoregressive models and discrete diffusion models. ARDMs perform competitively with existing generative models, and outperform competing approaches on per-image lossless compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY AND ETHICS STATEMENT</head><p>To ensure the work is as reproducible as possible, in this paper we have described in detail both the training algorithms and the sampling algorithms. The main ideas are presented in Section 3, and further clarifications that may be important for re-implementation are given in Appendix A.</p><p>The hyperparameter settings to run experiments are presented in Section 5 and further clarified in Appendix D. In addition, we plan to release the code that can be used to reproduce the experimental results in this paper.</p><p>In terms of ethics, we do not see immediate concerns for the models we introduce. However, deep generative models have a wide variety of applications such as representation learning, image inpainting, special effects in video, outlier detection and drug design. On the other hand, the generation of images, text and video may have negative downstream applications such as making false media seem realistic. Further, to the best of our knowledge no datasets were used that have known ethical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DETAILS ON PARALLELIZED ARDMS</head><p>This section discusses further details on Parallelized ARDMs, and provides a JAX version of the dynamic programming algorithm from <ref type="bibr" target="#b52">(Watson et al., 2021)</ref> that was written in NumPy. Since the algorithm scales with O(D 3 ) this implementation is important to scale to larger dimensional problems. To clarify, the upscale ARDMs can be seen as a S sequential OA-ARDMs that model p(x (s) |x (s−1) ), and when a parallel schedule is computed, it is computed for each stage separately.</p><p>It is also possible to run the dynamic programming algorithm for all S • D steps simultaneously, which could even choose to distribute steps unevenly over stages, but that is not done in this paper.</p><p>Recall that to run the algorithm a matrix L is needed which gives the cost of travelling from one generation step to another. It is constructed so that L t,t+k = k • L t for positive k and 0 otherwise, which represents the cost of generating k variables in parallel where L t is the loss component.</p><p>In practice this is implemented via a cumulative sum of a triangular mask. This part is relatively computationally cheap. The most expensive part of the algorithm is the loop which has computational complexity O(D 3 ). This is the most important extension of the NumPy version and reduces runtime from 5 minutes to about 2 seconds for D = 3072, which would be very impractical to run for our audio experiments where D = 16000, which now take less than half a minute to run. Through JAX this loop is XLAcompiled with the scan operation, limiting overhead when running the algorithm. The inner algorithm logic is then called via the function below. It first builds the loss transition matrix L which is referred to as nelbos and then calls the inner loop. As an output it gives the cost and dimension matrices that can be used to 1) find an optimal path and 2) describe how expensive such paths are. As can be seen in Figure <ref type="figure">6</ref>, the running average of the loss components {L t } might be somewhat noisy, which can negatively influence the algorithm. As a straightforward method to reduce variance of the values {L t }, they are sorted before they are given to the algorithm. This is uniquely possible for ARDMs, as we expect L t to be monotonically decreasing over t (see also Equation <ref type="formula" target="#formula_8">4</ref>). For Upscale ARDMs that have multiple stages, the loss components are seperately sorted per stage. The final part of this algorithm is used to retrieve the path that needs to be taken to attain a certain cost. This algorithm takes as input a budget and the cost &amp; dimension matrices, and returns the corresponding path to traverse.</p><p>def g e t _ o p t i m a l _ p a t h _ w i t h _ b u d g e t ( budget : int , costs : np . ndarray , dimensions : np . ndarray ): num_timesteps = len ( costs ) -1 t = num_timesteps path = np . zeros ( budget , dtype = np . int32 ) cost = costs <ref type="bibr">[ budget , num_timesteps ]</ref> for k in reversed ( range (1 , budget +1)):</p><formula xml:id="formula_12">t = dimensions [k , t ] path [k -1] = t return path , cost</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 RELATION TO OTHER LIKELIHOOD-BASED GENERATIVE MODELS</head><p>In this section we show how ARDMs perform compared to existing likelihood based generative models in literature. These results are presented in Table <ref type="table" target="#tab_6">6</ref>. The best performing model is the Variational Diffusion Model (VDM) <ref type="bibr" target="#b26">(Kingma et al., 2021)</ref>. ARDMs perform competitively with a best score of 2.64 bpd, and are the best performing model among discrete diffusion approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ADDITIONAL AUDIO EXPERIMENTS</head><p>In Table <ref type="table" target="#tab_7">7</ref> we present additional experimental results from our best Upscale ARDM model for the SC09 dataset (branching factor 4), in which we consider smaller computational budgets. Recall that dimensionality D = 16000 for SC09 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 LOSS COMPONENTS OVER TIME</head><p>Since the training algorithm estimates the NLL by sampling a step t for each input in the batch, we can collect and keep track of the loss components {L t } and plot them as a function of t (see Figure <ref type="figure">6</ref>). These are collected by updating an exponential moving average during training, and are used in the dynamic programming routine. As expected by Equation <ref type="formula" target="#formula_8">4</ref>, the components L t are monotonically decreasing over the step t within a stage. The height is re-normalized so that the average height represents the total bits per dimension. As a result, in the upscale model the value divided by number of stages S represents the actual uncertainty of generating that token.</p><p>The loss plot of the upscale model allows for interesting observations: For instance, After an initially high uncertainty (≈ 8/S = 2 bits) the most significant bits become increasingly easier to model very fast (&lt; 2/S = 0.5 bits). In contrast, for each stage that follows, the average height of that stages increases. This indicates that the model is more uncertain for the less significant bits. This can be a combination of two things: The model may have more difficulty in modelling these less significant bits (i.e. high KL between data and model distribution), and the data distribution may be more uncertain in those regions (i.e. high entropy of the data distribution).</p><p>Figure <ref type="figure">6</ref>: Loss components over model step on CIFAR-10. The height is normalized so that the average represents the total bits per dimension. Left: loss terms for the OA-ARDM. Right: loss terms for the ARDM-Upscale 4, which comprises four stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 SAMPLES FROM ARDMS</head><p>Language Sampling from ARDMs can be visualized at different steps t to highlight the generative process. Recall that for models trained on language, the absorbing state augments the space, meaning that an additional index that is added for the absorbing state. We visualize this token by the underscore character '_'. The process at four selected steps in the process are presented in Figure <ref type="figure" target="#fig_5">7</ref>, where the last sentence represents the resulting sample.  Images The generative processes for images are visualized in Figure <ref type="figure" target="#fig_7">8</ref>. In constrast with the language model, here the absorbing state takes on a specific value in the domain of the image itself.</p><p>In the case of OA-ARDMs, the absorbing state is 128 so that it is 0 when normalized in the network architecture. In constrast, the absorbing state of the Upscale ARDM is 0 because it is defined by zeroing least significant bits until everything is zero. The right-most grid represents the resulting samples from the model. The generative processes are very different: whereas the upscale ARDM first generates a coarses version of the images with fewer bits, the order agnostic ARDM generates each value at once.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EQUIVALENCE OF AO-ARDMS AND ABSORBING DIFFUSION IN CONTINUOUS TIME</head><p>In this section we examine the connection between absorbing diffusion and AO-ARDMs more closely. We start with a description of the independent process of absorbing diffusion as used in <ref type="bibr" target="#b2">(Austin et al., 2021)</ref> and highlight potential complications of this process. Then, we will show that AO-ARDMs are equivalent to a continuous-time version of absorbing diffusion models.</p><p>The Independent Absorbing Process from Austin et al.</p><p>In absorbing diffusion as described by <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, each dimension can independently get absorbed with some small probability for each time step. Specifically, letting a vector x(t) represent a Markov process as a collection of random variables index by integers t, where x(0) is the data distribution. Each dimension x i (t) as an equal and independent chance of getting absorbed according to rate γ(t) at index t to the absorbing state a i . Define the cumulative chance of retaining the state as α(t)</p><formula xml:id="formula_13">= t τ =1 (1 − γ(τ ))</formula><p>. This allows the direct expression for the distribution over x i (t) as categorical on data and the absorbing state {x i (0), a i } with probabilities {α(t), 1 − α(t)}. Typically, the decay rate γ is chosen so that α(T ) = 0 for some large integer T . For example in <ref type="bibr" target="#b2">(Austin et al., 2021)</ref> it is set T = 1000 for most experiments. We refer to the absorbing process from <ref type="bibr" target="#b2">(Austin et al., 2021)</ref> as an independent absorbing process, due to its independent absorbing probabilities between dimensions.</p><p>The reverse of this absorbing process is the generative process. As described above, the chance of a dimension absorbing is independent. As a result when T is small, it is inevitable that multiple dimensions decay at once. This has a direct consequence for the generative process, which is parametrized to model dimensions independently. The generative process will have to model the variables of these multiple absorbed dimensions as an independent factorized distribution, which causes a loss in modelling performance. This problem can be overcome by setting T to a larger value. Indeed, when T is larger the chance of multiple dimensions decaying at once decreases. During training, T can be set arbitrarily high without incurring costs. However, to sample or evaluate the likelihood of a specific datapoint, the computational cost scales directly with T so it is desired to keep T as low as possible.</p><p>As an example, consider the experiment from <ref type="bibr" target="#b2">(Austin et al., 2021)</ref> where text sequences of length 256 are modelled using a 1000 timestep absorbing diffusion model. When sampling from this model, at least 744 of the neural network forward passes do nothing to the latent variable and are needless compute. When T is reduced, performance degrades. In addition, it can be difficult to determine beforehand how high a T should be sufficient, and it depends on the data and the decay rate.</p><p>ARDMs model the reverse of a Fixed Absorbing Process Our ARDMs can be viewed as learning the generative process of a slightly modified absorbing process. Instead of independent absorbing costs, exactly one dimension decays at a time step until all dimensions have been absorbed. Since only one variable decays at a time, we refer to this process as a fixed absorbing process. This ensures that T = D exactly, where D is the dimensionality of the data.</p><p>An equivalent way to describe this process is by sampling a permutation of the indices 1, . . . , D and decaying in that order towards the absorbing state. The corresponding generative process is then modelling the variables exact opposite order of the permutation: an AO-ARDM. As a result the generative process with a fixed absorbing process process only requires at most D steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARDMs are equivalent to Continuous Time Absorbing Models</head><p>The absorbing diffusion model from <ref type="bibr" target="#b2">(Austin et al., 2021</ref>) can be relaxed to continuous time. Define a continuous-time discrete-space absorbing-state Markov process as a collection of Markov random variables {x(t)} in dimension D, parameterized by t ∈ R + . Starting in state x(0), each of the elements x i (t) of the state vector independently decays towards an absorbing state a at rate γ(t), such that at time t the distribution on the vector elements is categorical on {x i (0), a i }, with probabilities {α(t), 1 − α(t)}, with α(t) = exp(− t 0 γ(s)ds). This last equivalence is obtained via the first order logarithmic Taylor expansion log(1 − x) ≈ −x which holds for the small γ(s)ds.</p><p>An equivalent way of describing this stochastic process is as a finite set of D random transition times {τ i } for i ∈ {1, . . . , N } describing the time where the element x i has transitioned into the absorbing state. Specifically, we say that τ i is the latest time for which x i is non-yet absorbed, so x i (t) = a i for t &gt; τ i . From this perspective, x is only changing at the transition times τ i and remains the same at other times. Then, the reverse process to model {x(t)} for all t is equivalent to only modelling the finite dimensional {x i (τ i ), τ i }. In other words, to model the reverse process, we only need to model the transition times {τ i } and the variable right before it was absorbed.</p><p>To show an equivalence between the continuous time absorbing process and ARDMs, we will show that we can model the reverse process given by {x i (τ i ), τ i } by sampling the transition times independently, and by using an AO-ARDM for the transitions in x.</p><p>The distributions {τ i } are already known, they are given by the distribution with the cumulative distribution function 1 − α(t). That leaves the modelling of {x i (τ i )} to model the reverse process. An important question that now arises is whether the transition times {τ i } provide any additional information in modelling the variables {x i (τ i )}. Apart from knowing that the variable will be un-absorbed, the answer is no. This is because the values are distributed as the data distribution so x i (0)|x(t) ∼ x i (τ i )|x(t) and the actual continuous value of τ i does not matter, specifically {x i (0)} ⊥ {τ i }|x(t).</p><p>As a consequence, the model for the reverse process does not need to be conditioned on the precise values {τ i } and can instead be solely conditioned on x i (τ i+1 ) to model x i (τ i ) for all dimensions i. Recall that this process is equivalent to the generative process of our AO-ARDM: Each new timestep, a new dimension of the variable is modelled. The order in which the variables are modelled depends on the decay times {τ i }, and since these are all identically distributed, the order is uniform over all possible permutations.</p><p>We claim that we can write the VLB as follows:</p><formula xml:id="formula_14">log p(x(0)) ≥ E q(x(&gt;0)|x(0)) log p(x(&gt; 0)) + log p(x(0)|x(&gt; 0)) − log q(x(&gt; 0)|x(0)) = E q(τ1,...,τ D ) i log p(x(τ i )|x(τ i+1 )) − KL(q(τ 1 , . . . , τ D )|p(τ 1 , . . . , τ D )) = E σ∼U (S d ) i log p(x σ(i) |x σ(&lt;i) ),</formula><p>where x(&gt; 0) denotes all values of x(t) for t &gt; 0. The first equivalence follows from the above described equivalent representation of the continuous process. And indeed when the transition times and the values of x at the transition times are given, the remaining variables of the continuous process can be reconstructed so it can be ensured that: KL(q(x|x(τ 1 ), . . . , x(τ D ), τ 1 , . . . , τ D )|p(x|x(τ 1 ), . . . , x(τ D ), τ 1 , . . . , τ D )) = 0.</p><p>In addition, recall that any transition variable is distributed according to any chosen cumulative distribution α(t). Therefore, we can simply set our generative process to the same distribution, which ensures that: KL(q(τ 1 , . . . , τ D )|p(τ 1 , . . . , τ D )) = 0.</p><p>At this point we observe that the sampling times only determine the order in which the dimensions x are modelled. In fact when modelling x(τ i )|x(τ i+1 ) only one dimension dimension changes from τ i+1 to τ i . Since all {τ i } are independently and equally distributed, the distribution over the order of {τ i } is uniform. A subtle detail is that the reverse of the order of τ i describes the generative order since we model timestep τ i given τ i+1 . Nevertheless, since the distribution over orders is uniform, the distribution over reverse orders is also uniform. Therefore:</p><formula xml:id="formula_15">E q(τ1,...,τ D ) i log p(x(τ i )|x(τ i+1 )) = E σ∼U (S d ) i log p(x σ(i) |x σ(&lt;i) ),</formula><p>where the latter equation contains the same omitted notation as in the main paper: The model is aware which dimensions are conditioned on and which are not. In practical terms, this means that x σ(&lt;i) should be viewed as a masked vector and not as an order-less set. For the curious reader, technically the ability to move from order-less to the structured masked vector is enabled by conditioning on σ. In summary, modelling the generative process of a continuous absorbing jump process is equivalent to an AO-ARDM. This is beneficial, as viewing the model as an AO-ARDM gives a simple bound to the number of required steps and allows an easier perspective on the model and its implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAILS</head><p>In this section further details are given on the experimental setup.</p><p>Images For CIFAR10 <ref type="bibr" target="#b29">(Krizhevsky et al., 2009)</ref> we train the model using a fixed number of steps using the typical splits and evaluate the test log-likelihood after 3000 epochs of training. The results that are reported with standard deviations results are based on runs with three different initial seeds.</p><p>The other results are based on single-run experiments. The runs take approximately 2 weeks to complete training on 8 TPUv4 devices, although good performance (≈ 2.8 bits per dimension) is already achieved after a couple of days.</p><p>As a base architecture for the OA-ARDM and the Upscale ARDMs, the exact same U-Net architecture as in <ref type="bibr" target="#b26">(Kingma et al., 2021)</ref> are used. This architecture has 32 ResBlocks at resolution 32 × 32, a single middle attention layer and then another 32 ResBlocks at resolution 32 × 32. Throughout the network feature maps have 256 channels. This architecture is typical for NLL optimization and lossless compression, which typically require many high-resolution feature maps <ref type="bibr" target="#b34">(Mentzer et al., 2019)</ref>. The models are trained for 3000 epochs with Adam using a learning rate of 0.0001 and beta parameters (0.9 / 0.999). The input processing uses a combination of the floating point representation and an embedding layer. The integer-valued input is put through a simple normalization by dividing by the total number of classes, and subtracting 0.5. To this normalized input the mask m is then concatenated. In the case of upscale ARDMs, the current stage in one-hot representation is also converted to a mask with S channels, and is also part of m. So in that case m has S + 1 channels. Then a 3 × 3 convolutional layer maps the concatenated inputs to 3/4 of the channels. In addition, the integers are also fed through an embedding layer to 1/4 of the channels. These two outputs are then combined which produces the feature maps with 256 channels. This is given as an input to the U-Net architecture as described above. Following <ref type="bibr" target="#b2">Austin et al. (2021)</ref> we include the Cross-Entropy (CE) objective L CE = E t∼U (1,...,D) D(D − t + 1)L t (i.e. the unnormalized likelihood components), with a small factor of 0.001. However, in an experiment without the L CE loss included, no substantial differences in performance were found for our ARDMs. Since the likelihood of the dataset is estimated with the ARDM objective, the results are computed over multiple dataset passes to reduce the stochastic effects from sampling t and σ. For evaluation the exponential moving average of the parameters is used with a momentum coefficient of 0.9999. The models are trained with a batch size of 128. The gradient is clipped at 100.</p><p>Language For the text8 dataset (Mahoney, 2011)<ref type="foot" target="#foot_0">1</ref> we train using the typical 90•10 6 /5•10 6 /5•10 6 splits in characters. Because the text8 dataset is a long string of characters, there is predictive information between segments when chunked. For this reason there is a big difference between model performance in literature in the reported scores on the text8 benchmark. Some methods consider a larger context before the current sequence, which greatly improves the information available to the model and gives better log-likelihoods. The runs take approximately a week to complete on 4 TPUv4 devices.</p><p>Since we are interested in the pure modelling capacity of models, we follow <ref type="bibr" target="#b18">(Hoogeboom et al., 2021;</ref><ref type="bibr" target="#b2">Austin et al., 2021)</ref> and consider chunked text segments without any additional context. However, since the text8 splits are not evenly divisible by 256, we slightly adjust the chunk size to 250 characters, to avoid dropping the last batch. We validated empirically with a baseline Transformer that this small change does not meaningfully change the performance with the 256 version. For reference, a baseline 12 layer Transformer attains 1.35 bpc on this problem.</p><p>As a base architecture, we use a 12 layer Transformer as used in <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>. It has 768 dimensions, 12 heads, 3072 MLP dimensions. For the ARDM architectures we followed <ref type="bibr" target="#b2">(Austin et al., 2021)</ref> and used a batch size of 512 with no dropout. For standard language model baseline, since we observed overfitting the batch size was lowered and dropout of 0.1 was added. The models are trained for 3 • 10 6 training steps. ARDMs are optimized with Adam with a learning rate of 0.0005 which has a linear warm-up for the first 5000 steps. The additional L CE loss was included with a factor 0.0001. The gradient is clipped at 0.25. For evaluation the exponential moving average of the parameters is used with a momentum coefficient of 0.995. All models use a sinusoidal positional embedding. However, the OA-Transformer based on the XLNet approach <ref type="bibr" target="#b42">(Yang et al., 2019)</ref> requires both the input and target positional embeddings to infer which permutation currently needs to be generated. In <ref type="bibr" target="#b1">(Alcorn &amp; Nguyen, 2021)</ref>, this is handled by interleaving input and target nodes in the sequence. A downside to this approach is that is increases the sequence length by two, which increases the quadratic computational complexity of the attention layers by four. In contrast, we concatenate the input and target embeddings, which does not alter the sequence length.</p><p>Audio For audio experiments we used a subset of the SC09 dataset <ref type="bibr" target="#b51">(Warden, 2018)</ref> obtained by filtering out all non-digit commands from the dataset without changing the structure of the train/validation/test splits. The resulting dataset contains 31158/3643/4107 training/validation/test audio clips that are 1 second long and sampled at 16 kHz. In a few rare cases when the audio clips were shorter than 1 second, we right-padded them with zeros; and all considered models were trained and evaluated on padded data. A Tensorflow Datasets TFD version of this dataset is provided with the open-source code. Training takes approximately 4 days.</p><p>For both, the AO-ARDM as well as the Upscale ARDM experiments, we closely followed the Dif-fWave setup <ref type="bibr">(Kong et al., 2021)</ref> in terms of the network architecture and size, bit adapted the input embedding layer to take input masks into account. Specifically, we used a non-causal WaveNet (van den Oord et al., 2016a) architecture with 36 blocks, 256 channels and a dilation cycle of 11 (i.e. maximum dilation of 2048); input embedding were obtained by concatenating 1) 64-channel embeddings of integer input values; with 2) 192-channel mask and continuous input value embeddings output by a width 3 convolution; the shared time embedding was obtained by mapping the standard 256-channel sine and cosine representation through two dense layers with 1024 features and Swish nonlinearities <ref type="bibr" target="#b12">(Elfwing et al., 2018)</ref>.</p><p>Audio AO-ARDM and Upscale ARDM models were trained using the Adam optimizer (Kingma &amp; Ba, 2014) with beta paramters 0.9 / 0.999 for 10 6 steps with a batch size of 256 and a linear learning rate warm-up over the first 15000 steps followed by a constant learning rate of 10 −4 . During training we tracked an exponential moving average (EMA) of the model parameters using a momentum of 0.995, and employed the EMA parameters during evaluation. As in the case of image ARDMs, the models were optimized using a combination of the ELBO and CE objectives -the latter taken with a tiny weight of 10 −4 . No further regularisation was used.</p><p>Due to the large output space (2 16 classes) audio AO-ARDM modelled the output distribution using a mixture of discretized logistics (DMoL) with 30 components, although we experimentally found the number of components to not make a big difference. To aid with training, the DMoL was initialized as an approximately uniform distribution with different mixtures responsible for the different parts of this distribution; and gradients with an L 2 norm larger than 1000 were re-normalized. Owing to the smaller per-stage output space, we were able to utilize the categorical softmax parameterization for the Upscale ARDMs. Empirically we observed this model class to demonstrate a more stable training behaviour (in a addition to significantly improved likelihoods), which we (partially) attribute to the choice of parametrization.</p><p>For our autoregressive single-order baseline we sought to deviate from the above AO-ARDM setup as little as possible, and used a causal version of the WaveNet architecture above. However, we observed that the single-order baseline overfits quickly on the training data. To overcome this, we found it necessary to use weight decay (0.01), smaller batch size (64) and fewer channels (128) for the baseline model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of x through the generative process for an ARDM Upscale 4 model.by<ref type="bibr" target="#b42">Yang et al. (2019)</ref>, who observed underfitting behaviour and limited the task complexity by only predicting a subset of the permuted tokens. Further, as expected the performance of our OA-ARDM with 1.43 bpc is very close to the performance of D3PM-absorbing at 1000 steps with 1.45 bpc. This is expected, since OA-ARDMs are equivalent to the continuous time limit of D3PM-absorbing models. For sequences containing only 250 dimensions, the D3PM schedule with 1000 steps starts to approximate the jump process where generally only a single variable is absorbed at a time. The important takeaway from this comparison is that OA-ARDMs perform similar to large-steps D3PM absorbing models while only requiring a quarter of the steps. When the D3PM model is forced to take 256 steps which is comparable to our OA-ARDM model, then its performance degrades further towards 1.47 bpd. In addition, a Parallelized ARDM with only 20 steps has a performance of 1.51 bpd over a similar D3PM which has 1.56 bpd. This pattern translates to CIFAR-10 (<ref type="bibr" target="#b29">Krizhevsky et al., 2009)</ref> where ARDMs also outperform D3PMs and degrade more gracefully under fewer steps. This comparison to D3PM is however less direct, as the underlying architectures differ.</figDesc><graphic url="image-1.png" coords="8,110.77,78.76,51.38,51.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>import jax from jax import numpy as jnp import numpy as np def get_nelbo_matrix ( loss_components : np . ndarray ): num_timesteps = len ( loss_components ) # Creates multiplicative mask . E . g . if num_timesteps = 3 . triu ( np . ones (( num_timesteps , num_timesteps ))) triu = np . cumsum ( triu [:: -1] , axis = )[:: -1] # Compute nelbos [s , t ] which contains -logp ( x_s | x_t ) nelbos_ = loss_components [: , None ] * triu # Pad last row / first column . nelbos = np . zeros (( num_timesteps + 1 , num_timesteps + 1)) nelbos [: -1 , 1:] = nelbos_ return nelbos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>@</head><label></label><figDesc>jax . jit def i n n e r _ c o s t _ a n d _ d i m e n s i o n _ l o o p ( nelbos : jnp . ndarray , first_cost : jnp . ndarray ): """ Inner jax -loop that computes the cost and dimension matrices . """ num_timesteps = first_cost . shape [ ] -1 def compute_next_cost ( prev_cost : jnp . ndarray , _ : jnp . ndarray ): bpds = prev_cost [: , None ] + nelbos new_dimension = jnp . argmin ( bpds , axis = ) new_cost = jnp . min ( bpds , axis = ) return new_cost , ( new_cost , new_dimension ) _ , ( costs , dimensions ) = jax . lax . scan ( compute_next_cost , init = first_cost , xs = jnp . arange (1 , num_timesteps +1)) return costs , dimensions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>def g e t _ c o s t _ a n d _ d i m e n s i o n _ m a t r i c e s ( loss_components : np . ndarray ):""" Compute cost and assignment matrices , in JAX . """ num_timesteps = len ( loss_components ) # First row of the costs matrix . first_cost = np . full (( num_timesteps + 1 ,) , np . inf ) first_cost [ ] = first_cost = jnp . array ( first_cost ) # First row of the dimensions matrix . The first row just contains -1 # and is never used , but this way it aligns with the cost matrix . first_dimension = jnp . full (( num_timesteps + 1) , -1 , dtype = np . int32 ) # nelbos [s , t ] is going to contain the value logp ( x_s | x_t ) nelbos = jnp . array ( get_nelbo_matrix ( loss_components )) costs , dimensions = i n n e r _ c o s t _ a n d _ d i m e n s i o n _ l o o p ( nelbos , first_cost ) # Concatenate first rows to the matrices . costs = jnp . concatenate ([ first_cost [ None , :] , costs ] , axis = ) dimensions = jnp . concatenate ([ first_dimension [ None , :] , dimensions ] , axis = ) costs = np . array ( costs ) dimensions = np . array ( dimensions ) return costs , dimensions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>__n_ex_eri__ce______hort s_rike____b_r__wheth_r i _ ___se i__is__el__er__e fo__g__a_ls_h__e _ot__t__l_y u_s_c_e__pr______ i_je__ __erational w_a po_s_in_t___g_me_ca_ u__i_d__i__a_ m_d_l u__e___s___d____fy__g to r__li_e s_rategy for autom__ics o_ i__the c_n ex_erie_ce_fo_ short s_rike_bombers_wheth_r i n r_use it_is deli_erate for ga_auls ha_e _ot_ntially u_s_cke__pro_f___ i_ject __erational w_a po_s in t_e_game_car us_i_divid_a_ mod_l urre___s___de__ifyi_g to realize strategy for automatics or it the can experience for short strike bombers whether i n reuse it is deliberate for gasauls have potentially unsucked proof or inject operational wea pons in the game car us individual model urrealise identifying ____________s__________________________________________________________________________________ _________________________________________________ _________________t______________________t____ _________________________d__________________________________ _e_s__e___ows___ _____r___c__________e_______f_____b__s__el_i__ ____d__________s___c________he_ i____ __ ___ ___a____ma____i___e b____r_______bl_ __________d______t_n_a___ i_land__omp___t__n_ ______o_g_____e___o_ a___d__t____e____m____t__________ __m__ le_s_re_shows___ t____ro__ct__n_o__t_e _ta___f_____by_sa_el_i_e _o_rd______ro__s __c_ _en__ he_ ita_e __ ___ ___a__y_mai__fis_ e b__u_r_______bl_ e_ __r_a__d_n_a__tin_a__s i_land_comp__it_on_ __d_g_orge__c_e___on a___d_ltum _eak__mongst_e__e___a_ _om__ leisure shows__t t_e _rot_ction o__the sta__ fal_s by sa_ellite _oard_a_d troo_s __ce tenu_ he_ itage of t__ _ata__y main_fish e b__uer__ ____bl_ el _erpa _d_n_a_ tin_a__s island compo_it_on _nd george_ clea_son a__ diltum peak amongsthe ge___a_ _omm_ leisure shows it the protection of the stamp falls by satellite board and troops lace tenua he ritage of the catalay main fish e b fuerta e robla el serpa eden at tingalas island composition and georges clearson and diltum peak amongsthe general commu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two generative processes of an OA-ARDM trained on text8. The resulting sample from the model is at the bottom of each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Generative process of an Upscale 4 ARDM. This model was trained without data augmentation and with dropout, which explains that all images are generated upright. The performance of this model is approximately 2.71 bpd whereas the same model trained with data augmentation has 2.64 bpd.(b) Generative process of an OA-ARDM, starting at the absorbing state a. This model was trained with data augmentation, which is known to somewhat degrade sample quality and naturally sometimes samples rotated or reflected images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of the generative process for x, ending with the resulting samples at the right-most grid.</figDesc><graphic url="image-14.png" coords="19,115.76,319.29,77.57,77.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Order Agnostic model performance (in bpc) on the text8 dataset. The OA-Transformer learns arbitrary orders by permuting inputs and outputs as described in XLNet. A Transformer learning only a single order achieves 1.35 bpc.</figDesc><table><row><cell>Model</cell><cell>Steps</cell><cell>NLL</cell></row><row><cell>OA-Transformer</cell><cell>250</cell><cell>1.64</cell></row><row><cell>D3PM-uniform</cell><cell>1000</cell><cell>1.61 ±0.020</cell></row><row><cell>D3PM-absorbing</cell><cell>1000</cell><cell>1.45 ±0.020</cell></row><row><cell>D3PM-absorbing</cell><cell>256</cell><cell>1.47</cell></row><row><cell>OA-ARDM (ours)</cell><cell>250</cell><cell>1.43 ±0.001</cell></row><row><cell>D3PM-absorbing</cell><cell>20</cell><cell>1.56 ±0.040</cell></row><row><cell>Parallelized OA-ARDM (ours)</cell><cell>20</cell><cell>1.51 ±0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Order Agnostic modelling performance (in bpd) on the CIFAR-10 dataset. The upscaling model generates groups of four most significant categories, equivalent to 2 bits at a time.</figDesc><table><row><cell>Model</cell><cell>Steps</cell><cell>NLL</cell></row><row><cell>ARDM-OA</cell><cell>3072</cell><cell>2.69 ± 0.005</cell></row><row><cell>Parallel ARDM-OA</cell><cell>50</cell><cell>2.74</cell></row><row><cell>ARDM-Upscale 4</cell><cell cols="2">4 × 3072 2.64 ± 0.002</cell></row><row><cell>Parallel ARDM-Upscale 4</cell><cell>4 × 50</cell><cell>2.68</cell></row><row><cell>D3PM Absorbing</cell><cell>1000</cell><cell>4.40</cell></row><row><cell>D3PM Gaussian</cell><cell>1000</cell><cell>3.44 ± 0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CIFAR-10 lossless compression performance (in bpd).</figDesc><table><row><cell>Model</cell><cell cols="3">Steps Compression per image Dataset compression</cell></row><row><cell>VDM (Kingma et al., 2021)</cell><cell>1000</cell><cell>≥ 8</cell><cell>2.72</cell></row><row><cell>VDM (Kingma et al., 2021)</cell><cell>500</cell><cell>≥ 8</cell><cell>2.72</cell></row><row><cell>OA-ARDM (ours)</cell><cell>500</cell><cell>2.73</cell><cell>2.73</cell></row><row><cell>ARDM-Upscale 4 (ours)</cell><cell>500</cell><cell>2.71</cell><cell>2.71</cell></row><row><cell>VDM (Kingma et al., 2021)</cell><cell>100</cell><cell>≥ 8</cell><cell>2.91</cell></row><row><cell>OA-ARDM (ours)</cell><cell>100</cell><cell>2.75</cell><cell>2.75</cell></row><row><cell>ARDM-Upscale 4 (ours)</cell><cell>100</cell><cell>2.76</cell><cell>2.76</cell></row><row><cell>LBB (Ho et al., 2019)</cell><cell></cell><cell>≥ 8</cell><cell>3.12</cell></row><row><cell>IDF (Hoogeboom et al., 2019)</cell><cell></cell><cell>3.34</cell><cell>3.34</cell></row><row><cell>IDF++ (van den Berg et al., 2021)</cell><cell></cell><cell>3.26</cell><cell>3.26</cell></row><row><cell>HiLLoC (Townsend et al., 2020)</cell><cell></cell><cell>4.19</cell><cell>3.56</cell></row><row><cell>FLIF (Sneyers &amp; Wuille, 2016)</cell><cell></cell><cell>4.19</cell><cell>4.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Audio (SC09) depth upscaling test set performance (in bpd). A WaveNet baseline learning only a single order achieves 7.77 bpd.</figDesc><table><row><cell>Model</cell><cell>Steps</cell><cell>Performance</cell></row><row><cell>OA-ARDM</cell><cell>D = 16000</cell><cell>7.93</cell></row><row><cell>ARDM Upscale 256</cell><cell>2 × D</cell><cell>6.36</cell></row><row><cell>ARDM Upscale 16</cell><cell>4 × D</cell><cell>6.30</cell></row><row><cell>ARDM Upscale 4</cell><cell>8 × D</cell><cell>6.29</cell></row><row><cell>ARDM Upscale 2</cell><cell>16 × D</cell><cell>6.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Image (CIFAR-10) depth upscaling performance (in bpd).</figDesc><table><row><cell>Model</cell><cell>Steps</cell><cell>Performance</cell></row><row><cell>OA-ARDM</cell><cell>D = 3072</cell><cell>2.69</cell></row><row><cell>ARDM Upscale 16</cell><cell>2 × D</cell><cell>2.67</cell></row><row><cell>ARDM Upscale 4</cell><cell>4 × D</cell><cell>2.64</cell></row><row><cell>ARDM Upscale 2</cell><cell>8 × D</cell><cell>2.67</cell></row></table><note>the upscaling with larger values. Due to the constant training complexity of ARDMs, one can easily train models that have generative processes of arbitrary length. To test this, we train ARDMs on image data from CIFAR-10 and audio data from SC09</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>CIFAR-10 generative modelling.</figDesc><table><row><cell>Model</cell><cell>Type</cell><cell>NLL</cell></row><row><cell>ARDM-AO (ours)</cell><cell cols="2">Discrete Diffusion ∪ ARM 2.69</cell></row><row><cell>ARDM-Upscale 4 (ours)</cell><cell cols="2">Discrete Diffusion ∪ ARM 2.64</cell></row><row><cell>D3PM Gaussian (Austin et al., 2021)</cell><cell>Discrete Diffusion</cell><cell>3.44</cell></row><row><cell>DDPM (Ho et al., 2020)</cell><cell>Diffusion</cell><cell>3.69</cell></row><row><cell>Improved DDPM (Nichol &amp; Dhariwal, 2021)</cell><cell>Diffusion</cell><cell>2.94</cell></row><row><cell>VDM (Kingma et al., 2021)</cell><cell>Diffusion</cell><cell>2.49</cell></row><row><cell>PixelCNN++ (Salimans et al., 2017)</cell><cell>ARM</cell><cell>2.92</cell></row><row><cell>SPN (Menick &amp; Kalchbrenner, 2019)</cell><cell>ARM</cell><cell>2.90</cell></row><row><cell>Sparse Transformer (Jun et al., 2020)</cell><cell>ARM</cell><cell>2.52</cell></row><row><cell>NVAE (Vahdat &amp; Kautz, 2020)</cell><cell>VAE</cell><cell>2.91</cell></row><row><cell>Very Deep VAE (Child, 2021)</cell><cell>VAE</cell><cell>2.87</cell></row><row><cell>CR-VAE (Sinha &amp; Dieng, 2021)</cell><cell>VAE</cell><cell>2.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Audio (SC09) depth upscaling test set performance (in bpd) for various computational budgets.</figDesc><table><row><cell>Model</cell><cell>Steps</cell><cell>Performance</cell></row><row><cell cols="2">ARDM Upscale 4 8 × 16000</cell><cell>6.29</cell></row><row><cell cols="2">ARDM Upscale 4 8 × 1000</cell><cell>6.30</cell></row><row><cell></cell><cell>8 × 500</cell><cell>6.30</cell></row><row><cell></cell><cell>8 × 100</cell><cell>6.32</cell></row><row><cell></cell><cell>8 × 50</cell><cell>6.32</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://mattmahoney.net/dc/text8.zip</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FURTHER DETAILS OF AUTOREGRESSIVE DIFFUSION</head><p>Next to given descriptions, the implementation has been open-sourced at https://github.com/ google-research/google-research/tree/master/autoregressive_diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DEPTH UPSCALING</head><p>This section explains further details that are important to optimize and sample from Depth Upscaling ARDMs, which are summarized in Algorithm 3 and 4. Recall that for depthupscaling models, the variables are modelled in stages x (1) , . . . , x (S) and the model learns p(x (S) |x (S−1) ), . . . , p(x (1) |x (0) ). Here x (0) is a constant absorbing state and x (S) represents the data. The transition matrices {P (s) } describe the destructive maps which end up in the absorbing state. They form the destructive counterpart of the generative process.</p><p>Instead of optimizing for all stages simultaneously, we sample a stage uniformly s ∼ U(1, . . . , S) and optimize for that stage. Here the cumulative matrix products P (s) allow us to directly transition to a specific stage, since x (s) = P (s+1) x (S) . To be precise, for a single dimension i the variable</p><p>i is represented as a onehot vector and then transformed using the matrix multiplication x</p><p>i . For multiple dimensions this matrix multiplication is applied individually, meaning that P</p><p>2 , . . . , P s+1) x s) . For a optimization step, a stage s ∼ U(1, . . . , S) and a step t ∼ U(1, . . . , D) are sampled, in addition to a permutation σ ∼ U(S D ). Then using the cumulative matrices, from a datapoint x = x (S) the variables x (s) and x (s−1) are computed. As before, the mask m = σ &lt; t gives the locations of the variables that are conditioned on. For those locations the values in x (s) may already be accessed. For the opposite locations 1 − m, instead the values from x (s−1) are accessed. This leads to the expression for the input i = m x (s) +(1−m) x (s−1) . The target of the network will be to predict a distribution for x (s) at the locations at 1 − m. The network will take in the computed input i together with variables to clarify in which stage of the generative process the model is, m, s and t. In case of the data parametrization, the probabilities θ are appropriately normalized and reweighted to θ (s) using transitions {P (s) }. Then, the log probabilities log C(x (s) |θ (s) ) are computed elementwise over dimensions and subsequently masked with 1 − m. These quantities are then summed and reweighted to get a stochastic estimate for the ELBO.</p><p>For the sampling, the model traverses through each stage, and for each stage through every dimension in a different order. In each step the network together with the transition matrices produces a probability vector θ (s) from which elementwise samples are taken x ∼ C(x (s) |θ (s) ), but only the values at locations n ← (σ = t) are filled in, corresponding to the current generation step. By traversing through all steps and stages, the variable x (S) is generated.</p><p>Algorithm 3 Sampling from Upscale-ARDMs  (s+1) x and x (s−1) ← P (s) x</p><p>Compute m ← σ &lt; t i ← m x (s) + (1 − m) x (s−1) θ ← f (i, m, s, t) θ (s) ∝ P (s) T x (s−1) P (s+1) θ l t ← (1 − m) log C(x (s) |θ (s) ) L ← D D−t+1 sum(l t )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow Datasets, a collection of ready-to-use datasets</title>
		<ptr target="https://www.tensorflow.org/datasets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The DEformer: An order-agnostic distribution estimating transformer</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
		<idno>CoRR, abs/2107.03006</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Taking on the curse of dimensionality in joint distributions using neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<title level="m">Estimating gradients for waveform generation</title>
				<meeting><address><addrLine>Wave-Grad</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR. OpenReview.net</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1904.10509</idno>
		<ptr target="http://arxiv.org/abs/194.159" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno>CoRR, abs/2105.05233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jarek</forename><surname>Duda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.0271</idno>
		<title level="m">Asymmetric numeral systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiji</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>CoRR, abs/1308.0850</idno>
		<ptr target="http://arxiv.org/abs/138.85" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compression with flows via local bits-back coding</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Lohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integer discrete flows and lossless compression</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/2102.05379</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A variational perspective on diffusionbased generative models and score matching</title>
		<author>
			<persName><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>CoRR, abs/2106.02808</idno>
		<ptr target="https://arxiv.org/abs/216.288" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Locally masked convolution for autoregressive models</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI</title>
				<editor>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</editor>
		<meeting>the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rianne van den Berg, and Daniel Tarlow. Beyond in-place corruption: Insertion and deletion in denoising probabilistic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Austin</surname></persName>
		</author>
		<idno>CoRR, abs/2107.07675</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno>CoRR, abs/2105.14080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
				<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML</title>
				<meeting>the 35th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variational diffusion models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
		<idno>CoRR, abs/2107.00630</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno>CoRR, abs/2106.00132</idno>
		<ptr target="https://arxiv.org/abs/216.132" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DiffWave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2009">2009. 2011</date>
		</imprint>
	</monogr>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7806" to="7815" />
		</imprint>
	</monogr>
	<note>Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByJHuTgA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical full resolution learned lossless image compression</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10629" to="10638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyyGPPTZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
				<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gomez Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML</title>
				<meeting>the 34th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Consistency regularization for variational auto-encoders</title>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adji</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<idno>CoRR, abs/2105.14859</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FLIF: free lossless image format based on MANIAC compression</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Sneyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Wuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing, ICIP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
				<editor>
			<persName><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accelerating feedforward computation via parallel nonlinear equation solving</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
				<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Practical lossless compression with latent variables using bits back coding</title>
		<author>
			<persName><forename type="first">James</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno>CoRR, abs/1901.04866</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hilloc: lossless image compression with hierarchical latent variable models</title>
		<author>
			<persName><forename type="first">James</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
				<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">2014. June 2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NVAE: A deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">IDF++: analyzing and improving integer discrete flows for lossless compression</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR. OpenReview.net</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th ISCA Speech Synthesis Workshop</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML</title>
				<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/184.329" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno>CoRR, abs/2106.03802</idno>
		<ptr target="https://arxiv.org/abs/216.382" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predictive sampling with forecasting autoregressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Auke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Wiggers</surname></persName>
		</author>
		<author>
			<persName><surname>Hoogeboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
				<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
