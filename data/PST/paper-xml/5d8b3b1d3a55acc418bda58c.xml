<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Portuguese Named Entity Recognition using BERT-CRF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-23">23 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fábio</forename><surname>Souza</surname></persName>
							<email>fabiosouza@neuralmind.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Campinas</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">NeuralMind Inteligência Artificial</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
							<email>rodrigonogueira@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Lotufo</surname></persName>
							<email>lotufo@dca.fee.unicamp.br</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Campinas</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">NeuralMind Inteligência Artificial</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Portuguese Named Entity Recognition using BERT-CRF</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-23">23 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1909.10649v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of a trained model to downstream natural language processing tasks, such as named entity recognition (NER) and question answering. It has been shown that the leverage of pre-trained language models improves the overall performance on many tasks and is highly beneficial when labeled data is scarce. In this work, we employ a pre-trained BERT with Conditional Random Fields (CRF) architecture to the NER task on the Portuguese language, combining the transfer capabilities of BERT with the structured predictions of CRF. We explore feature-based and fine-tuning training strategies for the BERT model. Our finetuning approach obtains new state-of-the-art results on the HAREM I dataset, improving the F1-score by 3.2 points on the selective scenario (5 NE classes) and by 3.8 points on the total scenario (10 NE classes).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is the task of identifying text spans that mention named entities (NEs) and classifying them into predefined categories, such as person, organization, location or any other classes of interest. Despite being conceptually simple, NER is not an easy task. The category of a named entity is highly dependent on textual semantics and on its surrounding context. Moreover, there are many definitions of named entity and evaluation criteria, introducing evaluation complications <ref type="bibr" target="#b12">(Marrero et al., 2013)</ref>.</p><p>Current state-of-the-art NER systems employ neural architectures that have been pre-trained on language modeling tasks, such as ELMo <ref type="bibr" target="#b13">(Peters et al., 2018)</ref>, OpenAI GPT <ref type="bibr" target="#b15">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. It has been shown that language modeling pre-training significantly improves the performance of many natural language processing tasks and also reduces the amount of labeled data needed for supervised learning <ref type="bibr" target="#b9">(Howard and Ruder, 2018)</ref>.</p><p>For Portuguese NER, there are few available corpora and the annotated datasets are small. In addition, reproducing and benchmarking the results of previous works is not simple due to the variety of dataset combinations and the lack of a standardized training and evaluation methodology.</p><p>In this work, we employ a BERT model with a Conditional Random Fields (CRF) layer to the Portuguese NER task, comparing feature-based and fine-tuning based training strategies. We also discuss the main complications that we face when benchmarking the results on the utilized datasets and, with that in mind, we aim to facilitate the reproducibility of future works by making our implementation publicly available<ref type="foot" target="#foot_0">1</ref> .</p><p>The paper is organized as follows: in Section 2, we present the related works. In Section 3, we briefly describe our model architecture. In Section 4 we describe the datasets and our experimental setup. In Section 5, we present and analyze our results and we make our final remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NER systems can be based on handcrafted rules or machine learning approaches. For the Portuguese Figure <ref type="figure">1</ref>: Illustration of the evaluation procedure. Given an input document, the text is tokenized using WordPiece <ref type="bibr" target="#b18">(Wu et al., 2016)</ref> and the tokenized document is split into overlapping spans of the maximum length using a defined stride (with a stride of 3 in the example). Maximum context tokens are marked in bold. The spans are fed into BERT and then into the classification layer, producing a sequence of tag scores for each span. The sub-token entries (starting with ##) are removed from the spans and the remaining tokens are passed to the CRF layer. The maximum context tokens are selected and concatenated to form the final document prediction. language, previous works explored machine learning techniques and a few ones applied neural networks models. do Amaral and Vieira (2014) created a CRF model using 15 features extracted from the central and surrounding words. <ref type="bibr" target="#b14">(Pirovani and Oliveira, 2018</ref>) combined a CRF model with Local Grammars, following a similar approach.</p><p>Starting with <ref type="bibr" target="#b3">Collobert et al. (2011)</ref>, neural network NER systems have become popular due to the minimal feature engineering requirements, which contributes to a higher domain independence <ref type="bibr" target="#b19">(Yadav and Bethard, 2018)</ref>. The CharWNN model <ref type="bibr">(Santos and Guimaraes, 2015)</ref> extended the work of <ref type="bibr" target="#b3">Collobert et al. (2011)</ref> by employing a convolutional layer to extract character-level features from each word. These features were concatenated with pre-trained word embeddings and then used to perform sequential classification.</p><p>The LSTM-CRF architecture <ref type="bibr" target="#b10">(Lample et al., 2016)</ref> has been commonly used in NER task <ref type="bibr" target="#b2">(Castro et al., 2018;</ref><ref type="bibr" target="#b1">de Araujo et al., 2018;</ref><ref type="bibr" target="#b4">Fernandes et al., 2018)</ref>. The model is composed of two bidirectional LSTM networks that extract and combine character-level and word-level features. A sequential classification is then performed by the CRF layer. Several pre-trained word embeddings were explored by <ref type="bibr" target="#b2">Castro et al. (2018)</ref> and <ref type="bibr" target="#b4">Fernandes et al. (2018)</ref> compared it to 3 other architectures. This is the first work to explore a model that benefits from deeper pre-trained language model representations applied to the Portuguese NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section we will describe the model architecture and the training and evaluation procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT-CRF for NER</head><p>The model architecture is composed of a BERT model followed by a classification model and a Linear-Chain CRF. BERT allows input sequences of up to S tokens and outputs an encoded token sequence with hidden dimension H. The classification model projects each token's encoded representation to the tag space, i.e. R H → R K , where K is the number of tags and depends on the the number of classes and on the tagging scheme. The output scores of the classification model, P, are then fed to the CRF layer, whose parameters are a matrix of tag transitions A ∈ R K+2,K+2 . The matrix A is such that A i,j represents the score of transitioning from tag i to tag j. A includes 2 additional states: start and end of sequence.</p><p>As described by <ref type="bibr" target="#b10">Lample et al. (2016)</ref>, for an input sequence X = (x 1 , x 2 , ..., x n ) and a sequence of tag predictions y = (y 1 , y 2 , ..., y n ), the score of the sequence is defined as</p><formula xml:id="formula_0">s(X, y) = n i=0 A y i ,y i+1 + n i=1 P i,y i ,</formula><p>where y 0 and y n+1 are start and end tags. During training, the model is optimized by maximizing the log-probability of the correct tag sequence, which follows from applying softmax over all possible tag sequences' scores:</p><formula xml:id="formula_1">log(p(y|X)) = s(X, y) − log   ỹ∈Y X e s(X,ỹ)  </formula><p>(1) where Y X are all possible tag sequences. The summation in Eq. 1 is computed using dynamic programming. During evaluation, the most likely sequence is obtained by Viterbi decoding. As described in <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>, WordPiece tokenization requires predictions and losses to be computed only for the first sub-token of each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature-based and Finetuning-based</head><p>We experiment with two transfer learning approaches: feature-based and fine-tuning based. For the feature-based approach, the BERT model weights are kept frozen and only the classifier model and CRF layer are trained. The classifier model consists of a BiLSTM and a Linear layer. Instead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>. The resulting architecture resembles the LSTM-CRF model <ref type="bibr" target="#b10">Lample et al. (2016)</ref> replacing its embedding techniques by BERT.</p><p>As for the fine-tuning approach, the classifier is a linear layer and all weights are updated jointly during training. For both approaches, models without the CRF layer were also evaluated. In this case, they were optimized by minimizing the cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Document context and max context evaluation</head><p>To benefit the most from the bidirectional context present in the tokens' encoded representation provided by BERT, we use document context for input examples instead of sentence context. Following the approach of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref> on the SQuAD dataset, examples larger than S tokens are broken into spans of length up to S using a stride of D tokens. Each span is used as a separate example dur-ing training. During evaluation, however, a single token T i can be present in N = S D multiple spans s j and so may have up to N distinct tag predictions y i,j . Each token's final prediction is taken from the span where the token is closer to the central position, that is, the span where it has the most contextual information. Figure <ref type="figure">1</ref> shows an outline of the evaluation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the datasets that were used, the training setup and hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The standard datasets for training and evaluating Portuguese NER task are the HAREM Golden Collections (GC) <ref type="bibr" target="#b17">(Santos et al., 2006;</ref><ref type="bibr" target="#b5">Freitas et al., 2010)</ref>. We use the GCs of the First HAREM evaluation contests, which is divided in two subsets: First HAREM and MiniHAREM. Each GC contains manually annotated named entities of 10 classes: Location, Person, Organization, Value, Date, Title, Thing, Event, Abstraction and Other.</p><p>We use the GC of First HAREM as training set and the GC of MiniHAREM as test set. The experiments are conducted on two scenarios: a Selective scenario, with 5 entity classes (Person, Organization, Location, Value and Date) and a Total scenario, that considers all 10 classes. This is the same setup used by <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>.</p><p>Vagueness and indeterminacy: some text segments of the GCs contain &lt;ALT&gt; tags that enclose multiple alternative named entity identification solutions. Additionally, multiple categories may be assigned to a single named entity. These criteria were adopted in order to take into account vagueness and indeterminacy that can arise in sentence interpretation <ref type="bibr" target="#b17">(Santos et al., 2006)</ref>.</p><p>Despite enriching the datasets by including such realistic information, these aspects introduce benchmarking and reproducibility complications when the datasets are used in single-label setups, since they imply the adoption of heuristics to select one unique solution for each vague or unde-tations produced by BERT instead of fixed word and character embeddings. The post-processing step of filtering out invalid transitions for the IOB2 scheme increases the F1-scores by 2.24 and 1.26 points, in average, for the feature-based and finetuning approaches, respectively. This step produces a reduction of 0.44 points in the recall, but boosts the precision by 3.85 points, in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a new state-of-the-art on the HAREM I corpora by fine-tuning a pre-trained BERT-CRF architecture on the Portuguese NER task. Considering the issues regarding preprocessing and dataset decisions that affect evaluation compatibility, we gave special attention to reproducibility of our results by making our code and datasets publicly available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,72.00,62.81,453.55,157.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset and tokenization statistics for the HAREM I corpora. The Tokens column refers to whitespace and punctuation tokenization. The Entities column comprises the two defined scenarios.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Documents Spans Tokens WordPiece tokens Entities (selective/total)</cell></row><row><cell>First HAREM</cell><cell>129</cell><cell>718</cell><cell>95585</cell><cell>126169</cell><cell>4151 / 5017</cell></row><row><cell>MiniHAREM</cell><cell>128</cell><cell>429</cell><cell>64853</cell><cell>87060</cell><cell>3018 / 3642</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code will be available soon at https: //gist.github.com/fabiocapsouza/ 62c98576d1c826894be2b3ae0993ef53</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">https://www.clips.uantwerpen.be/ conll2002/ner/bin/conlleval.txt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>7 Acknowledgements R Lotufo acknowledges the support of the Brazilian government through the CNPq Fellowship ref. 310828/2018-0.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total scenario</head><p>Selective scenario Precision Recall F1 Precision Recall F1 CharWNN <ref type="bibr">(Santos and Guimaraes, 2015)</ref> 67  termined segment and/or entity. To resolve each &lt;ALT&gt; tag in the datasets, our approach is to select the alternative that contains the highest number of named entities. In case of ties, the first one is selected. To resolve each named entity that is assigned multiple classes, we simply select the first valid class for the scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training setup</head><p>We use a BERT BASE model, that has H = 768, S = 512 and is initialized with the pre-trained weights of bert-base-multilingual-cased 2 model, which was trained on 104 languages and has a vocabulary of 119547 WordPiece tokens. We use the PyTorch BERT implementation by HuggingFace 3 . A stride of D = 128 tokens is used to split the input examples into spans, as described in 3.3. Table <ref type="table">1</ref> contains some dataset statistics. We use the IOB2 tagging scheme. We use a batch of size 16. The model parameters were divided in two groups with different learning rates: 5e-5 for BERT model and 1e-3 for the rest. The numbers of epochs are 100 for BERT-LSTM, 50 for BERT-LSTM-CRF and BERT, and 15 for BERT-CRF. The number of epochs was found using a development set comprised of 10% of the First HAREM training set. We use the customized Adam optimizer of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>.</p><p>For the feature-based approach, we use a BiLSTM with 1 layer and hidden dimension of 100 units for each direction. To deal with class imbalance, we initialize the linear layer bias term of the "O" tag with value of 6 in order to promote a better stability in early training <ref type="bibr" target="#b11">(Lin et al., 2017)</ref>. We also use a weight of 0.01 for "O" tag losses when not using a CRF layer. When evaluating, we apply a post-processing step that removes all invalid tag predictions for the IOB2 scheme, such as "I-" tags coming directly after "O" tags or after an "I-" tag of a different class. This post-processing step trades off recall for a possibly higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The main results of our experiments are presented in Table <ref type="table">2</ref>. We compare the performances of our feature-based and fine-tuning based models on the two scenarios (total and selective) to the works of <ref type="bibr">Santos and Guimaraes (2015)</ref> and <ref type="bibr" target="#b2">Castro et al. (2018)</ref>. To make the results comparable to both works, all metrics are computed using CoNLL 2003 evaluation script 4 , that consists of a micro F1-score on entity-level considering only strict exact matches.</p><p>The BERT-CRF model outperforms the LSTM-CRF both on the total and selective scenarios, improving the F1-score by about 3.8 points on the total scenario and 3.2 points on the selective scenario. We also report results removing the CRF layer in order to evaluate the performance of BERT model alone. BERT also outperforms previous works, even without the enforcement of sequential classification provided by the CRF layer. The CRF layer increases the F1-score across all models and scenarios by 1.4 points in average. The models of the feature-based approach perform significantly worse compared to the ones of the fine-tuning approach, as expected. BERT-LSTM-CRF outperforms the reported results of LSTM-CRF <ref type="bibr" target="#b2">(Castro et al., 2018)</ref> by about 1.5 points on the selective scenario and 1.8 points on the total scenario. The main difference between these two models is the usage of contextual token represen-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nerp-crf: A tool for the named entity recognition using conditional random fields</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Vieira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguamática</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="49" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lener-br: A dataset for named entity recognition in brazilian legal text</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Henrique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luz</forename><surname>De Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teófilo</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato Rr De</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matheus</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Bermejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Processing of the Portuguese Language</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Portuguese named entity recognition using lstm-crf</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quinta</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nádia</forename><surname>Félix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Processing of the Portuguese Language</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">2011. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Lopes Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Oliveira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="284" to="289" />
		</imprint>
	</monogr>
	<note>Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Second harem: advancing the state of the art of named entity recognition in portuguese</title>
		<author>
			<persName><forename type="first">Cláudia</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Gonc ¸alo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName><surname>Santos</surname></persName>
		</author>
		<editor>Nicoletta Calzolari</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>In quot. Khalid Choukri</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</author>
		<imprint>
			<date>Jan Odijk</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rosner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2010)</title>
				<meeting>the International Conference on Language Resources and Evaluation (LREC 2010)</meeting>
		<imprint>
			<publisher>European Language Resources Association. European Language Resources Association</publisher>
			<date type="published" when="2010-05-23">Valletta 17-23 May de 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Named entity recognition: fallacies, challenges and opportunities</title>
		<author>
			<persName><forename type="first">Mónica</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julián</forename><surname>Urbano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Sánchez-Cuadrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Morato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Miguel Gómez-Berbís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Standards &amp; Interfaces</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="482" to="489" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Portuguese named entity recognition using conditional random fields and local grammars</title>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Pirovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Time Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<idno type="arXiv">arXiv:1505.05008</idno>
		<title level="m">Boosting named entity recognition with neural character embeddings. Computing Research Repository</title>
				<imprint>
			<publisher>Cicero Nogueira dos Santos and Victor Guimaraes</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Harem: An advanced ner evaluation contest for portuguese</title>
		<author>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Seco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Vilela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Version 2</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
