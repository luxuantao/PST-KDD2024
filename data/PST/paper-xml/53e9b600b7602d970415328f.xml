<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENT SCHEMES FOR TOTAL VARIATION MINIMIZATION UNDER CONSTRAINTS IN IMAGE PROCESSING *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-04-03">April 3, 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Weiss</surname></persName>
							<email>pierre.weiss@sophia.inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ARIANA, projet commun CNRS/INRIA</orgName>
								<orgName type="institution" key="instit2">UNSA</orgName>
								<orgName type="institution" key="instit3">INRIA Sophia Antipolis</orgName>
								<address>
									<addrLine>route des Lu-cioles</addrLine>
									<postCode>2004, BP93, 06902</postCode>
									<settlement>Sophia Antipolis Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laure</forename><surname>Blanc-F Éraud</surname></persName>
							<email>laure.blancferaud@sophia.inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ARIANA, projet commun CNRS/INRIA</orgName>
								<orgName type="institution" key="instit2">UNSA</orgName>
								<orgName type="institution" key="instit3">INRIA Sophia Antipolis</orgName>
								<address>
									<addrLine>route des Lu-cioles</addrLine>
									<postCode>2004, BP93, 06902</postCode>
									<settlement>Sophia Antipolis Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gilles</forename><surname>Aubert</surname></persName>
							<email>gaubert@math.unice.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Laboratoire J.A. Dieudonné</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6621</orgName>
								<orgName type="institution">Université de Nice Sophia-Antipolis</orgName>
								<address>
									<addrLine>Parc Val-rose</addrLine>
									<postCode>06108, Cedex 2</postCode>
									<settlement>Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFFICIENT SCHEMES FOR TOTAL VARIATION MINIMIZATION UNDER CONSTRAINTS IN IMAGE PROCESSING *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-04-03">April 3, 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">2E8206614C66D5557831FC394D979567</idno>
					<idno type="DOI">10.1137/070696143</idno>
					<note type="submission">Received by the editors July 3, 2007; accepted for publication (in revised form) November 17, 2008;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>l 1 -norm minimization</term>
					<term>total variation minimization</term>
					<term>l p -norms</term>
					<term>duality</term>
					<term>gradient and subgradient descents</term>
					<term>Nesterov scheme</term>
					<term>bounded and nonbounded noises</term>
					<term>texture+geometry decomposition</term>
					<term>complexity AMS subject classifications. 65K05</term>
					<term>65K10</term>
					<term>68U10</term>
					<term>94A08</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents new fast algorithms to minimize total variation and more generally l 1 -norms under a general convex constraint. Such problems are standards of image processing. The algorithms are based on a recent advance in convex optimization proposed by Yurii Nesterov. Depending on the regularity of the data fidelity term, we solve either a primal problem or a dual problem. First we show that standard first-order schemes allow one to get solutions of precision in O( <ref type="formula">1</ref>2 ) iterations at worst. We propose a scheme that allows one to obtain a solution of precision in O( <ref type="formula">1</ref>) iterations for a general convex constraint. For a strongly convex constraint, we solve a dual problem with a scheme that requires O( <ref type="formula">1</ref>√ ) iterations to get a solution of precision . Finally we perform some numerical experiments which confirm the theoretical results on various problems of image processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction.</head><p>In this paper we are interested in the fast resolution of a class of image restoration and decomposition problems that can be written under the general constrained form <ref type="bibr">(1.1)</ref> inf</p><formula xml:id="formula_0">u∈R n ,F (u)≤α (||∇u|| 1 )</formula><p>or the "equivalent" Lagrangian form (1.2) inf u∈R n (||∇u|| 1 + γF (u)) .</p><p>R n is the discrete space of 2D images (n is the number of pixels). ||∇u|| 1 corresponds to the discrete total variation (see the appendix for the discretization of differential operators). F is a convex proper function. We will give particular attention to functions F that write as l p -norms of affine transforms of the images.</p><p>In section 3 we review the applications of such a formalism and show that it is widely used. This is certainly due to the fact that total variation has interesting theoretical properties and leads to good practical results. The difficulty in minimizing it lies in the nondifferentiability of the l 1 -norm. It makes it a challenging task to design efficient numerical methods. This is very important for image processing applications which involve huge dimension problems. A lot of different techniques have been proposed. Some are PDE based with explicit <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b48">47]</ref>, semi-implicit <ref type="bibr" target="#b30">[30]</ref>, or fixed point <ref type="bibr" target="#b52">[51]</ref> schemes. Others are based on the minimization of a discretized energy. Those include subgradient descents <ref type="bibr" target="#b17">[17]</ref> and subgradient projections <ref type="bibr" target="#b18">[18]</ref>, Newton-like methods <ref type="bibr" target="#b32">[32]</ref>, second-order cone programming <ref type="bibr" target="#b26">[26]</ref>, interior point methods <ref type="bibr" target="#b24">[24]</ref>, or graph-based approaches <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b12">12]</ref>. Recently, some authors tried to use primal-dual or dual-only approaches <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b11">11]</ref>.</p><p>In this work, we propose new convergent schemes to solve (1.1) and <ref type="bibr">(1.2)</ref>. They are all based on first-order explicit schemes proposed recently by Nesterov <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref>. These schemes are given with explicit convergence rates (which is seldom seen in the literature), are optimal with respect to a certain class of convex problems, require little memory, and are easy to parallelize and implement. <ref type="foot" target="#foot_0">1</ref> We compare their efficiency with some other classical first-order schemes. We show their theoretical and practical superiority.</p><p>Depending on the regularity of F , we propose two different approaches motivated by the maximization of the theoretical rates of convergence. For general convex F , we follow the approach of Nesterov in <ref type="bibr" target="#b38">[38]</ref> and use a smooth approximation of the total variation. Doing so, getting a solution of precision requires O 1 iterations while most first-order schemes require O 1 2 iterations. For strongly convex F (typically l 2 -norms), we show that the resolution of a dual problem with a Nesterov's scheme leads to algorithms demanding O 1 √ iterations to get an -solution. The outline of the paper is as follows:</p><p>• In section 2 we settle the main notations and definitions.</p><p>• In section 3 we show that many image processing problems such as restoration or decomposition can be expressed as (1.1) or (1.2).</p><p>• In section <ref type="bibr" target="#b3">4</ref> we analyze two commonly used first-order approaches to solve problem (1.1). • In section 5 we detail the proposed algorithm to solve the constrained problem <ref type="bibr">(1.1)</ref>. It is based on a regularization of the total variation followed by a fast Nesterov's algorithm. Its convergence rate outperforms the other classical schemes by one order of magnitude. • In section 6 we give an algorithm that solves the lagrangian problem (1.2) for strongly convex function F . It is based on the resolution of a dual problem with a Nesterov's scheme. • In section 7, we finally compare our approach with some other existing firstorder methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notations and definitions.</head><p>2.1. Notations. Let us describe the notations we use throughout this paper.</p><p>To simplify the notations, we use X = R n , Y = X × X, and J(u) = ||∇u|| 1 .</p><p>All the theories developed in this paper can be applied to color images using, for instance, color total variation <ref type="bibr" target="#b7">[8]</ref>. To simplify the notations, we focus on gray-scale images.</p><p>ū denotes a solution of (1.1) or (1.2). f ∈ X will denote the given observed datum.</p><p>For u ∈ X, u i ∈ R denotes the ith component of u.</p><p>For g ∈ Y , g i ∈ R 2 denotes the ith component of g, and g i = (g i,1 , g i,2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2049</head><p>•, • X denotes the usual scalar product on X. For u, v ∈ X we have <ref type="bibr">(</ref> </p><formula xml:id="formula_1">(|g i | 2 ) .</formula><p>Let A be a linear invertible transform. A * denotes its complex conjugate. A - * denotes the complex conjugate of A -1 .</p><p>Finally a is the integer part of a ∈ R. <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b47">46]</ref>. Definition 2.1 (Euclidean projector). Let K ⊂ X be a convex set. The Euclidean projector on K is defined by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Definitions and some recalls of convex optimization</head><formula xml:id="formula_2">Π K (x) = arg min u∈K (|u -x| 2 ) .</formula><p>Definition 2.2 (Euclidean norm of an operator). Let B be a linear operator from X to Y . The Euclidean norm of B is defined by</p><formula xml:id="formula_3">||B|| 2 = max x∈X,|x|2≤1</formula><p>(||Bx|| 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.3 (proper function).</head><p>A convex function F on X is proper if and only if F is not identically equal to +∞ and that it does not take the value -∞ on X.</p><p>Definition 2.4 (indicator function). Let K ∈ X be a nonempty closed convex subset of X. The indicator function of K, denoted χ K , is defined by</p><formula xml:id="formula_4">(2.7) χ K (x) = 0 if x ∈ K, ∞ otherwise.</formula><p>Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Definition 2.5 (subdifferential and subgradient). Let J : X → R be a convex function. The subdifferential of J at point u ∈ X is defined by</p><formula xml:id="formula_5">(2.8) ∂J(u) = {η ∈ X, J(u) + η, (x -u) X ≤ J(x), ∀x ∈ X}. η ∈ ∂J(u) is called a subgradient. Definition 2.6 (L-Lipschitz differentiable function). A function F defined on K is said to be L-Lipschitz differentiable if it is differentiable on K and that |∇F (u 1 ) - ∇F (u 2 )| 2 ≤ L|u 1 -u 2 | 2 for any (u 1 , u 2 ) ∈ K 2 .</formula><p>Definition 2.7 (strongly convex differentiable function). A differentiable function F defined on a convex set K ∈ X is said to be strongly convex if there exists σ &gt; 0 such that</p><formula xml:id="formula_6">(2.9) ∇F (u) -∇F (v), u -v X ≥ σ 2 |u -v| 2 2 for any (u, v) ∈ K 2 . σ is called the convexity parameter of F . Note that property (2.9) implies that |∇F (u) -∇F (v)| 2 ≥ σ 2 |u -v| 2 . Definition 2.8 (Legendre-Fenchel conjugate). Let G be a convex proper appli- cation from X to R ∪ {∞}. The conjugate function of G is defined by (2.10) G * (y) = sup x∈X ( x, y X -G(x)).</formula><p>G * is a convex proper function. Moreover, we have G * * = G. Definition 2.9 ( -solutions). Let ū be a solution of <ref type="bibr">(1.1)</ref>.</p><formula xml:id="formula_7">An -solution of problem (1.1) is an element u of {u, F (u) ≤ α} satisfying ||∇u || 1 -||∇ū|| 1 ≤ .</formula><p>Let ū be a solution of <ref type="bibr">(1.2</ref>). An -solution of problem (1.2) is an element u of X satisfying</p><formula xml:id="formula_8">||∇u || 1 + γF (u ) -(||∇ū|| 1 + γF (ū)) ≤ .</formula><p>3. Presentation of some applications. Many image processing models use the total variation J(u) = ||∇u|| 1 as a prior on the images. This quantity somehow measures the oscillations of an image. It was introduced by Rudin, Osher, and Fatemi (ROF) in <ref type="bibr" target="#b48">[47]</ref> as a regularizing criterion for image denoising. Its main interest lies in the fact that it regularizes the images without blurring the edges. Nowadays it is appreciated for its ability to model the piecewise smooth or constant parts of an image. In this section, we give a nonexhaustive review of the different applications in which it is involved. We give particular attention to functions F that write</p><formula xml:id="formula_9">(3.1) F (u) = |λ(Au -f )| p ,</formula><p>where A is a linear invertible transform (identity, wavelet transform, Fourier transform, . . . ), λ is a diagonal matrix whose elements belong to [0, ∞], p belongs to {1, 2, ∞}, and f is a given datum. Let us show that this formalism covers a wide range of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.</head><p>A = Id, p ∈ {1, 2, ∞}-denoising or decomposition. Many image degradation models write f = u + b. u is the original image, b is a white additive noise, and f is the degraded observation. Suppose that we have a probability P (u) Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php over the space of images that is proportional to exp(-J(u)). <ref type="foot" target="#foot_1">2</ref> Then it can be shown using the Bayes rule that the "best" way to retrieve u from f using the maximum a posteriori estimator is to solve the following problem:</p><formula xml:id="formula_10">(3.2) inf u∈X,|u-f |p≤α (J(u))</formula><p>with p = 1 for impulse noise <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b19">19]</ref>, p = 2 for Gaussian noise <ref type="bibr" target="#b48">[47]</ref>, p = ∞ for uniform noise <ref type="bibr" target="#b53">[52]</ref>, and α a parameter depending on the variance of the noise. The noise might have a different variance on different parts of the image. In this case, we can solve the problem</p><formula xml:id="formula_11">(3.3) inf u∈X,|λ(u-f )|p≤α (J(u)) ,</formula><p>where λ = diag(λ i ) with λ i ∈ [0, ∞] is a diagonal matrix that will allow one to treat differently the different regions of the image. On pixels where λ i = ∞ the model will impose ūi = f i . On pixels where λ i = 0, the value of ūi will depend only on the prior J. This idea was proposed in <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b6">7]</ref>. This also allows one to do tasks like inpainting <ref type="bibr" target="#b15">[15]</ref>. Recently, the BVl 1 model was also shown to be an efficient model for the decomposition of an image into a cartoon and a texture <ref type="bibr" target="#b54">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A = wavelet transform, p ∈ {1, ∞}.</head><p>In this part, we describe three applications. Namely, the restoration of compressed images, the restoration of images that have been thresholded in the wavelet domain, and the denoising of white noises.</p><p>• A classical way to compress a signal is to:</p><p>1. Transform it with some linear, bijective application. 2. Quantize the obtained coefficients to reduce the entropy.</p><p>3. Use a lossless compression algorithm on the quantized coefficients. In image compression the first used transform was the local cosine transform in jpeg. The new standard is jpeg2000 which uses a wavelet transform. This kind of compression introduces artefacts like oscillations near the edges. Let u be an original image, and f a compressed image. The degradation operator Ψ can be written</p><formula xml:id="formula_12">(3.4) Ψ(u) = A -1 (Q(Au)),</formula><p>where Q is a uniform or nonuniform quantizer and A is a linear transform (local cosine transform, wavelet transform, . . . ). A natural way to recover u is to look for the image of minimal total variation in the convex set Ψ -1 (f ) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">21]</ref>. This amounts to solving inf</p><formula xml:id="formula_13">u∈X,∀i∈[1..n], |(A(u-f ))i|≤ α i 2 (J(u)) ,</formula><p>where α i stands for the quantization steps. This problem can easily be redefined as</p><formula xml:id="formula_14">(3.5) inf u∈X,|λA(u-f )|∞≤1 (J(u))</formula><p>with the diagonal coefficients of λ belonging to [0, ∞].</p><p>• Wavelet thresholding is widely used to denoise signals. Such operations show good performances, but introduce oscillatory artefacts when using nonredundant wavelet transforms. Solving a problem similar to (3.5), (A being a wavelet transform) can be shown to reduce those artefacts. • Recently a model similar to <ref type="bibr">(3.5)</ref>, with an l 1 -norm instead of the l ∞ -norm was proposed for image denoising <ref type="bibr" target="#b22">[22]</ref>. We refer the reader to <ref type="bibr" target="#b22">[22]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.</head><p>A = Fourier transform, p = 2-image deconvolution, image zooming.</p><p>• One of the fundamental problems of image processing is the deblurring. A common way to model image degradation is f = h u+ b. u is a given original image, b is a white Gaussian noise, and h is a convolution kernel representing the degradation due to the optical system and sensors. To retrieve the original image, we can solve the following problem:</p><formula xml:id="formula_15">(3.6) inf u∈X,|h u-f |2≤α (J(u)) .</formula><p>The operator h is linear; it can thus be represented by a n × n matrix H. It is shown in <ref type="bibr" target="#b40">[40]</ref> that the FFT diagonalizes H if denotes the convolution operation with periodic boundary conditions and the DCT diagonalizes H if h is symmetric and denotes the convolution with Neumann boundary conditions. In any case, we see that H = A -1 λA, λ being a diagonal matrix and A denoting either the FFT or the DCT. As both transforms are isometries from X to X, we have</p><formula xml:id="formula_16">(3.7) |h u -f | 2 = |Hu -f | 2 = |λAu -Af | 2 .</formula><p>Finally (3.6) is equivalent to</p><formula xml:id="formula_17">(3.8) inf u∈X,|λAu-Af |2≤α</formula><p>(J(u)) .</p><p>• In view of Shannon's theorem, one could think that the best way to zoom an image is to use a zero-padding technique. Unluckily, this introduces oscillations near the edges. A simple way to avoid them is to solve the following problem</p><formula xml:id="formula_18">(3.9) inf u∈X,|λ(Au-f )|2≤α (J(u))</formula><p>with f the zero-padded Fourier coefficients of the reduced image, λ i = ∞ where f i is known, and λ i = 0 otherwise. This problem is a particular instance of a more general class of zooming techniques proposed in <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Summary.</head><p>We summarize the applications detailed previously in Table <ref type="table" target="#tab_1">1</ref>.</p><p>This formalism also allows one to do image cartoon + texture decompositions <ref type="bibr" target="#b34">[34]</ref>, inpainting <ref type="bibr" target="#b15">[15]</ref>, and restoration with perturbed sampling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Considering pseudoinvertible transforms it would include other interesting applications like denoising using redundant transforms (dictionaries, curvelets, . . . ) <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b8">9]</ref> or image decompositions <ref type="bibr" target="#b50">[49]</ref>. Let us now look at the numerical algorithms to solve <ref type="bibr">(1.1)</ref>  Gaussian noise denoising <ref type="bibr" target="#b48">[47]</ref> Bounded noise denoising <ref type="bibr" target="#b53">[52]</ref> A = Fourier transform Robust deconvolution <ref type="bibr" target="#b24">[24]</ref> Image deconvolution <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b6">7]</ref>, image zooming <ref type="bibr" target="#b33">[33]</ref> No known reference A = Wavelet, local cosine transform Image decomposition or denoising <ref type="bibr" target="#b22">[22]</ref> Image denoising (no known reference) Compression noise denoising <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b16">16]</ref> 4. Classical first-order approaches. Problem (1.1) covers many useful applications, but it is difficult to solve and many currently used algorithms are slow. This clearly limits the industrial interest for such models. In this section, we show that two commonly used approaches require O 1 2 iterations to provide an -solution. With such a rate getting a 10 -3 -solution requires (on the worst case) of order 10 6 iterations. </p><formula xml:id="formula_19">u 0 ∈ K u k+1 = Π K u k -t k η k |η k |2 .</formula><p>Here, t k &gt; 0 for any k, η k is any element of ∂J(u k ) (see (2.8)), and Π K is the Euclidean projector on K = {u, F (u) ≤ α}. It was proposed recently in some image processing papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">17]</ref>. This kind of scheme has two severe drawbacks. First, it is difficult to design the sequence {t k }. Secondly, even if the sequence {t k } is defined optimally, it might be very slow. It is shown in <ref type="bibr" target="#b35">[35]</ref> that any algorithm only using the sequences J(u k ) and ∂J(u k ) has a worst case complexity of O 1 2 . We refer the reader to <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29]</ref> and <ref type="bibr" target="#b17">[17]</ref> for optimal choices of the sequence {t k } in Algorithm 4.1. Let us finally precise that scheme (4.1) might converge much faster if J belongs to certain function classes (see for instance <ref type="bibr" target="#b47">[46]</ref>), but total variation does not possess the required properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Smoothing and projected gradient descent. Another widely spread</head><p>technique consists in smoothing the total variation <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b52">51]</ref> by</p><formula xml:id="formula_20">(4.2) Jμ (u) = n i=1 |(∇u) i | 2 + μ 2</formula><p>and use a projected gradient descent with constant step to minimize it. Let us analyze its rate of convergence. Proposition 4.1. The following algorithm: The proof is given in the appendix. To get an -solution, we thus need to choose μ of order n and N of order O 1</p><formula xml:id="formula_21">(4.3) ⎧ ⎨ ⎩ u 0 ∈ K u k+1 = Π K u k -τ div ∇u √ |∇u| 2 +μ</formula><p>2 . The two strategies presented are widely used but require large computing times to get acceptable estimates of the solutions. In the following sections we introduce much faster algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>A new algorithm to minimize the total variation under simple constraints. In <ref type="bibr" target="#b38">[38]</ref>, Nesterov presents an efficient scheme to minimize nondifferentiable convex functions on convex sets. His idea is as follows:</p><p>• Approximate the nondifferentiable function by a differentiable one.</p><p>• Compensate the approximation error using a fast scheme adapted to differentiable functions. In this section, we show how to apply his ideas to total variation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">How to smooth the total variation?</head><p>Following the ideas in <ref type="bibr" target="#b38">[38]</ref>, we use a smooth approximation of J. First note that (5.1)</p><formula xml:id="formula_22">J(u) = sup q∈Y,||q||∞≤1</formula><p>( ∇u, q Y ) .</p><p>The approximation we propose writes</p><formula xml:id="formula_23">(5.2) J μ (u) = sup q∈Y,||q||∞≤1 ∇u, q Y - μ 2 ||q|| 2 2 + nμ 2 .</formula><p>This corresponds to the Moreau-Yosida regularization. It is easily shown that</p><formula xml:id="formula_24">J μ (u) = n i=1 ψ μ (|(∇u) i |) with (5.3) ψ μ (x) = |x| if |x| ≥ μ, x 2 2μ + μ 2 otherwise.</formula><p>ψ μ is called Huber function. J μ seems more appropriate than Jμ defined in (4.2), as both approximations are</p><formula xml:id="formula_25">||div || 2 2 μ Lipschitz differentiable, 3 but (5.4) 0 ≤ Jμ (u) -J(u) ≤ nμ while (5.5) 0 ≤ J μ (u) -J(u) ≤ nμ 2 .</formula><p>The approximation J μ thus seems "twice" better. Let us note that</p><formula xml:id="formula_26">(5.6) ∇J μ (u) = -div (Ψ) with Ψ i = (∇u)i |(∇u)i| if |(∇u) i | ≥ μ, (∇u)i μ otherwise.</formula><p>In all numerical experiments we perform, the minimization of (5.3) leads to solutions that have a lower total variation than (4.2), but the visual aspect of the solutions are the same. As the complexity of computing ∇J μ or ∇ Jμ is the same, we think that using J μ definitely is a better choice if one aims at approximating the total variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Nesterov's scheme for differentiable function. In [38], Nesterov presents</head><p>an O 1 √ algorithm adapted to the problem</p><formula xml:id="formula_27">(5.7) inf u∈K (E(u)) ,</formula><p>where E is any convex, L-Lipschitz differentiable function, and K is any convex, closed set. For this class of problems, it can be shown that no algorithm-only using the values and gradients of E-has a better rate of convergence than O 1 √ uniformly on all problems of type (5.7). Nesterov's algorithm is thus optimal for this class of problems. In this algorithm, two sequences {x k }, {y k } ∈ K are updated recursively in order to satisfy ∀x ∈ K</p><p>(5.8)</p><formula xml:id="formula_28">A k E(y k ) ≤ L 2 ||x -x 0 || 2 + k i=0 α i (E(x i ) + ∇E(x i ), x -x i X ).</formula><p>In this equation α i is a sequence of increasing coefficients and A k = k i=0 α i will define the rate of convergence. The right-hand side of (5.8) is an approximation of</p><formula xml:id="formula_29">A k E(x). The linear part is a lower approximation of A k E(x) and a fortiori of A k E(x). Condition (5.8) thus ensures that E(y k ) -E(x) ≤ L 2A k ||x -x 0 || 2 .</formula><p>The idea underlying this condition is to exploit the fact that the gradient of a convex function not only gives its local ascent direction but also indicates facts about its global topological properties. It is thus possible-as in the conjugate gradient algorithm-to accelerate the convergence rate of the first-order schemes by using the information brought by the gradients at all iterations. That is why the right-hand side of (5.8) is a linear combination of the gradients. Based on these ideas, Nesterov shows the following result in his paper <ref type="bibr" target="#b38">[38]</ref>.</p><p>Theorem 5.1. Let ū be a solution of (5.7). The following algorithm Algorithm 1: Accelerated gradient descent.</p><p>Input: Number of iterations N , a starting point x 0 ∈ K. Output: y N an estimate of ū. begin</p><formula xml:id="formula_30">1 Set G -1 = 0 2 Set L =Lipschitz constant of ∇E 3 for k going from 0 to N do 4 Set η k = ∇E(x k ). 5 Set y k = arg min y∈K η k , y -x k X + 1 2 L||y -x k || 2 . 6 Set G k = G k-1 + k + 1 2 η k . 7 Set z k = arg min z∈K L σ d(z) + G k , z X . 8 Set x k+1 = 2 k + 3 z k + k + 1 k + 2 y k . 9 end 10 end 11 ensures that (5.9) 0 ≤ E(y k ) -E(ū) ≤ 4Ld(ū) σ(k + 1)(k + 2)</formula><p>. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>At step 6, || • || denotes any norm, at step 8, d is any convex function satisfying d(x) ≥ σ</head><p>2 ||xx 0 || 2 for some element x 0 ∈ K, and σ is the convexity parameter of d. Using inequality (5.9), it is easily seen that getting an -solution does not require more than 4Ld(ū) iterations. This shows that Algorithm 1 is an O( <ref type="formula">1</ref>√ ) algorithm. Supposing that steps 3 and 5 are achievable, this scheme has many qualities. It is simple to implement, does not require more than five times the size of the image, and it is theoretically optimal (see <ref type="bibr" target="#b37">[37]</ref> for a precise definition of its optimality). Let us remind that the classical projected gradient descent is an O 1 algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.</head><p>Solving the constrained problem for some convex functions F . The scheme we propose consists in solving <ref type="bibr">(5.10)</ref> inf</p><formula xml:id="formula_31">u∈R n ,F (u)≤α (J μ (u))</formula><p>with Algorithm 1. Let us precise how to achieve steps 3 and 5 and how to choose the regularization parameter μ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">How to achieve</head><formula xml:id="formula_32">(x) = 1 2 |x -x 0 | 2 2</formula><p>, where x 0 ∈ K is the center of K or an estimate of the solution. With this choice, the convexity parameter of d is σ = 1. We have to find (preferably in closed form) the expressions of the arg min at steps 3 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 5.2. With the above choices, step 6 reduces to y</head><formula xml:id="formula_33">k = Π K (x k -η k L ) and step 8 reduces to z k = Π K (x 0 -G k L )</formula><p>. Π K stands for the Euclidean projector on K.</p><p>Proof. Let us solve the problem arg min y∈K (f (y)) with f (y) = η, y X + L 2 |y -x| 2 2 . From first-order optimality conditions, we get that the solution ȳ of this problem satisfies (-∇f (ȳ)), wȳ X ≤ 0 for any w ∈ K. This is equivalent to (x -η L )ȳ, wȳ X ≥ 0 for any w ∈ K and finally, thanks to projection theorem, to ȳ = Π K (x -η L ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">How to choose μ and the number of iterations.</head><p>In <ref type="bibr" target="#b43">[42]</ref>, the author shows that the singularity at 0 of the l 1 -norm is responsible for the so-called staircase effect: The solutions of total variation problems are-roughly speaking-piecewise constant. A way to avoid that is to use a regularized operator such as (5.3). In practice, in the case of denoising, this leads to better SNR and more satisfying visual results. Figure <ref type="figure" target="#fig_0">1</ref> illustrates this fact. Lena image is degraded adding Gaussian noise with different variances. Then it is denoised using various μ parameters. It can be seen that the optimal μ value is around 0.002 independently of the initial SNR. In restoration applications, for natural images of amplitude 1, our experiments led us to the conclusion that the optimal μ should be taken in the range [0.001, 0.005] and that few visual differences are observed in that range.</p><p>In some situations we would like to exactly minimize the total variation (μ = 0). Making a regularization might thus seem unappropriate. Actually the following proposition shows that smoothing the total variation is still a good solution.  Proof. Let Jμ denote the solution of (5.10), let ū be the solution of problem (1.1)</p><formula xml:id="formula_34">and L = ||div || 2 2 μ</formula><p>. We have</p><formula xml:id="formula_35">J(y k ) (5.5) ≤ J μ (y k ) (5.11) (5.9) ≤ Jμ + 4LD k 2 (5.12) ≤ J μ (ū) + 4LD k 2 (5.13) (5.5) ≤ J + nμ 2 + 4LD k 2 . (5.14) We thus obtain 0 ≤ J(y k )-J ≤ 4LD k 2 + nμ 2 .</formula><p>To obtain an -solution, it is thus sufficient to have 4LD</p><formula xml:id="formula_36">k 2 + nμ 2 ≤ . Setting μ ≤ 2 n and k = 4||div || 2 2 D μ( -nμ 2 )</formula><p>+ 1 thus gives ansolution. To get the result, it suffices to maximize the denominator in the previous equation.</p><p>We thus gain one order in the convergence rate compared to classical algorithms. It shows that smoothing the total variation is a good way to exploit its structure. Unfortunately, in most problems, knowing that |J(y k ) -J(ū)| ≤ does not bring any quantitative information on more important features like |y k -ū| ∞ . Thus, we cannot use the bound (5.9) to define the number of iterations. Experimentally, for images rescaled in [0, 1], we can check that the solution of (5.10) obtained by choosing μ = 0.01 is very close perceptually to the solution of (1.1). Choosing μ = 0.002 leads to solutions that are perceptually identical to the solution of (1.1), independently of the problem dimension. From this remark, we infere that a sufficient precision lies in [0.002 n 2 , 0.01 n 2 ]. Thus, using proposition (5.3) the approach we suggest consists in choosing μ ∈ [0.001, 0.005] and there is no reason to choose N &gt; 2</p><formula xml:id="formula_37">√ 2||div ||2 √ Dn nμ</formula><p>. In all cases we studied (except deconvolution) D ∼ θn, with θ ∈]0, 1[. Thus, the maximum iterations needed to get a good approximate solution is</p><formula xml:id="formula_38">(5.15) N = 2 √ 2||div || 2 √ θ μ .</formula><p>This quantity does not exceed 8000 iterations for the worst case problem, and lies in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">400]</ref> for most practical applications. The theoretical rate of convergence leads to low iterations number and computing times. Let us show how to apply the ideas presented for some applications. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php </p><formula xml:id="formula_39">(E μ (z)) with E μ (z) = J μ (A -1 (z + f ))</formula><p>. The solution ū of (5.10) can be retrieved from the solution z using formula ū =</p><formula xml:id="formula_40">A -1 (z + f ). It is easy to show that E μ is L-Lipschitz differentiable with L = ||div || 2 2 ||A -1 || 2 2 μ</formula><p>. For all invertible transforms the final algorithm to solve (5.17) writes: Algorithm 2: Nesterov's scheme for problem <ref type="bibr">(5.10)</ref>.</p><p>Input: Number of iterations N and regularization parameter μ.</p><p>(depending on the precision required).</p><p>Output: u N an estimate of ū. begin</p><formula xml:id="formula_41">1 Set G -1 = 0. 2 Set L = ||div || 2 2 ||A -1 || 2 2 μ . 3 Set x k = 0. 4 for k going from 0 to N do 5 Set η k = A - * ∇J μ (A -1 (x k + f )). 6 Set y k = Π K x k - η k L . 7 Set G k = G k-1 + k + 1 2 η k . 8 Set z k = Π K - G k L . 9 Set x k+1 = 2 k + 3 z k + k + 1 k + 2 y k . 10 end 11 Set u N = A -1 (y k-1 + f ). 12 end<label>13</label></formula><p>At steps 7 and 9, K = {u ∈ X, |λu| p ≤ α}. We refer the reader to the appendix for the expressions of the projections on weighted l p -balls.</p><p>Figure <ref type="figure">2</ref> shows the result of a compression noise restoration using this technique for μ = 0.06 (60 iterations until visual stability) and μ = 0.0004 (210 iterations until visual stability). Clearly, for such bounded noises it is preferable to use the regularized total variation in order to avoid the staircase effect. This was already remarked in <ref type="bibr" target="#b53">[52]</ref>. The price per iteration is about 2 wavelet transforms. We do not give our computational times as we used a slow Matlab implementation of Daubechies 9-7 wavelet Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Fig. <ref type="figure">2</ref>. Examples of image decompression: TL: original image (scaled im [0, 1]); TR: compressed image using Daubechies 9-7 wavelet transform (the implementation is similar to Jpeg2000); BL: solution of (3.5) using μ = 0.006; BR: solution of (3.5) using μ = 0.0004. transform. We refer the reader to section 7 for comparisons with other algorithms. Let us note that to our knowledge, no precise schemes exist in the literature to solve this problem. Large oscillations are removed, while thin details are preserved. The main drawback of this model is that the contrast of the details decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Deconvolution:</head><formula xml:id="formula_42">F (u) = |h u -f | 2 .</formula><p>In this paragraph we present a new way to do deconvolution using a Nesterov's scheme. This problem is particularly difficult and cannot be solved by the previous algorithm as the convolution matrices are generally noninvertible or ill-conditioned. In (3.3), we showed that problem <ref type="bibr">(5.18)</ref> inf  where A is the discrete cosine transform and λ is a diagonal matrix. In this formulation</p><formula xml:id="formula_43">u∈X,|h u-f |2≤α (J μ (u)) is equivalent to (5.19) inf u∈X,|λz-Af |2≤α J μ (A -1 z) ,</formula><formula xml:id="formula_44">z → J μ (A -1 z) is L-Lipschitz-differentiable with L = ||div || 2 2 μ</formula><p>. The interests of this formulation are that we have a very fast Newton algorithm to do projections on K = {u ∈ X, |λz -Af | 2 ≤ α} (see the appendix paragraph (A.2.3)), and that the Lipschitz constant of the gradient of z → J μ (A -1 z) does not blow up. Note that the set K = {u ∈ X, |λz -Af | 2 ≤ α} might be unbounded if λ contains zeros on its diagonal. Thus we lose the convergence rate unless we estimate an upper bound on d(ū). Practically, the Nesterov scheme remains very efficient. The cost per iteration is around 2 DCTs and 2 projections on ellipsoids. For a 256 × 256 image, the cost per iteration is 0.2 seconds (we used the dct2 function of Matlab and implemented a C code with Matlab mex compiler for the projections). Figure <ref type="figure" target="#fig_3">3</ref> shows an experimental result. We display the bottom right result to show that it is useless (for visual purposes) to choose very small μ parameters. Perceptually, the bottom left picture is the same while the computing times needed to obtain it are much lower. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Image texture + cartoon decomposition:</head><formula xml:id="formula_45">F (u) = λ|u -f | G .</formula><p>The first application of total variation in image processing was proposed by Rudin-Osher-Fatemi in <ref type="bibr" target="#b48">[47]</ref>. It consisted in choosing F (u) = |uf | 2 . In <ref type="bibr" target="#b34">[34]</ref>, Meyer studied this model theoretically, and figured out its limitation to discriminate a cartoon in a noise or a texture. He observed that this limitation could be overpassed using a different data term than the rather uninformative L 2 -distance to the data. To simplify the presentation, we present the model in the discrete setting and refer the interested reader to <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b4">5]</ref> for more details. Meyer defined a norm <ref type="bibr">(5.20</ref>)</p><formula xml:id="formula_46">||v|| G = inf g∈Y (||g|| ∞ , div (g) = v)</formula><p>and proposed to decompose an image f into a cartoon u and a texture v using the following model:</p><p>(5.21) inf</p><formula xml:id="formula_47">(u,v)∈X 2 ,f =u+v (J(u) + λ||v|| G ) .</formula><p>The G-norm of an oscillating function remains small and it blows up for characteristic function. That is why this model should permit one to better extract oscillating patterns of the images.</p><p>Meyer did not propose any numerical method to solve his problem. The first authors who tried to compute a solution were Vese and Osher in <ref type="bibr" target="#b46">[45]</ref>. Later, other authors tackled this problem. Let us cite the works of Aujol et al. in <ref type="bibr" target="#b5">[6]</ref> and of Goldfarb et al. in <ref type="bibr" target="#b26">[26]</ref>. The former is based on a first-order scheme which solves a differentiable approximation of Meyer's model, while the latter solves it exactly with second-order cone programming methods. In the following, we propose a new efficient scheme. Meyer's discretized problem writes <ref type="bibr">(5.22)</ref> inf</p><formula xml:id="formula_48">u∈X J(u) + λ inf g∈Y,div (g)=f -u (||g|| ∞ ) .</formula><p>Proposition 5.4. Problem (5.22) can be reformulated as follows:</p><p>(5.23) inf g∈Y,||g||∞≤α</p><formula xml:id="formula_49">(J(f -div (g)))</formula><p>Proof. The idea simply is to use the change of variable u = fdiv (g) in order to get an optimization problem that depends only of one variable g. The operator div is surjective from Y to X = X -{(γ, γ, . . . , γ), γ ∈ R}, so that <ref type="bibr">(5.24)</ref> inf</p><formula xml:id="formula_50">u∈X J(u) + λ inf g∈Y,div (g)=f -u (||g|| ∞ ) = inf g∈Y (J(f -div (g)) + λ||g|| ∞ ) .</formula><p>Turning the Lagrange multiplier λ into a constraint, we get the result. Instead of solving problem (5.23), we solve (5.25) inf g∈Y,||g||∞≤α</p><formula xml:id="formula_51">(J μ (f -div (g)))</formula><p>and get an O 1 algorithm. Note that the solution of (5.25) is unique while that of Meyer's model is not. Also note that if we replace the l ∞ -norm by an l 2 -norm in (5.23), we get the model of Osher-Solé-Vese <ref type="bibr" target="#b45">[44]</ref>. Formula (5.23) allows to easily constrain the properties of the g field. This might also be interesting for spatially varying processing. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php In all experiments, we process the image on Figure <ref type="figure" target="#fig_2">4</ref> and take μ = 0.001 to smooth the total variation. After 200 iterations, very little perceptual modifications are observed in all experiments, while a projected gradient descent requires around 1000 iterations to get the same result. Let us finally precise that all the texture components have the same l 2 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIERRE WEISS, LAURE BLANC-F ÉRAUD, AND GILLES AUBERT</head><p>In Figure <ref type="figure" target="#fig_5">5</ref>, we observe that Meyer's model does not allow one to retrieve correctly the oscillating patterns of the clothes of Barbara. It can be shown that the amplitude of the texture (v component) is bounded by a parameter depending linearly on α in (5.23). That might explain the deceiving result. On the given example, Osher-Solé-Vese's model gives more satisfying results. This was already observed in <ref type="bibr" target="#b54">[53]</ref>.</p><p>In Figure <ref type="figure" target="#fig_6">6</ref>, we show some decomposition results using the BVl 1 model. We also show the result of (5.23) with an l 1 -norm instead of the l ∞ -norm. We present this model to alleviate the curiosity of the reader as no theoretical justification is given.</p><p>The BVl 1 model correctly separates the oscillating patterns and the geometry. The same observation holds when minimizing the l 1 -norm of the g field in (5.23). We remark that both decompositions are very similar, except that the cartoon component of the BVl 1 model is slightly less blurred than that of the new model and that the new model better extracts the oscillating patterns (chair and clothes, for instance). We think that the blurring effect is due to the numerical scheme which is slightly more diffusive for the new model as it is based on fourth-order finite differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">A new algorithm to solve the lagrangian problem for strongly convex data term.</head><p>Having the previous section in mind, a straightforward approach to solve (1.2) is to smooth the total variation; if F is nonsmooth, it can be smoothed too, and then one just needs to use a fast scheme like (1) adapted to the unconstrained minimization of Lipschitz differentiable functions <ref type="bibr" target="#b36">[36]</ref>. This method should be efficient, but in the case of strongly convex F -which notably corresponds to l 2 -data fidelity term-one can do much better. We present an O( 1 √ ) algorithm rather than the previous O( 1 ) algorithm. The proposed algorithm can notably solve the problem of Rudin-Osher-Fatemi with local constraints, the problem of deconvolution in the case of an invertible transform, and the cartoon+texture decomposition model of Vese-Osher.</p><p>Instead of directly attacking (1.2) we can solve its dual problem for which no smoothing is needed. The key idea is that for strongly convex F , F * is Lipschitz Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php differentiable. We first recall some facts of convex analysis (see <ref type="bibr" target="#b23">[23]</ref>, for a complete reference).</p><p>Let F : X → R and G : Y → R be two convex proper functions. Let P be the primal problem <ref type="bibr">(6.1)</ref> inf</p><formula xml:id="formula_52">u∈X (G(∇u) + F (u)).</formula><p>The dual problem P * is then defined by (6.2) inf q∈Y (G * (-q) + F * (-div(q))) .</p><p>Let ū and q be the solutions of P and P * , respectively. Those solutions are related through the extremality relations (6.3) F (ū) + F * (-div(q)) = -div(q), ū X Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and (6.4)</head><p>G(∇ū) + G * (-q) = -q, ∇ū Y .</p><p>Furthermore we have (6.5) G(∇ū) + F (ū) = -(G * (-q) + F * (-div(q))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Application to our problem.</head><p>To apply the previous theory to our problem, we take (6.6)</p><formula xml:id="formula_53">G(q) = ||q|| 1 .</formula><p>We wish to solve</p><formula xml:id="formula_54">(6.7) inf u∈X (G(∇u) + F (u)) ,</formula><p>where F is differentiable, strongly convex, with convexity parameter σ. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Theorem 6.1. The dual problem of (6.7) is defined by <ref type="bibr">(6.8)</ref> inf q∈K (F * (-div(q)))</p><formula xml:id="formula_55">with K = {q ∈ Y, ||q|| ∞ ≤ 1}. The application H : q → F * (-div(q)) is L-Lipschitz differentiable, with L = 2||div|| 2 2 σ</formula><p>. Problem (6.8) can thus be solved with a Nesterov scheme (no smoothing is needed!). ū can be retrieved from the solution q of (6.8) using (6.9) ū = ∇F * (-div(q)); moreover (6.10) ∇ū = q|∇ū|.</p><p>This method thus amounts to evolving the orientation of the level lines of u instead of its intensity. Proof.</p><p>1. Let us compute G * :</p><formula xml:id="formula_56">G * (q) = sup r∈Y ( q, r Y -||r|| 1 ) (6.11) = sup t&gt;0 sup ||r||1=t ( q, r Y -t) (6.12) = sup t&gt;0 (t||q|| ∞ -t) (6.13) = χ K (q) (6.14) with K = {q ∈ Y, ||q|| ∞ ≤ 1}. 2. Let us show F * is 2 σ -Lipschitz differentiable. (6.15) F * (u) = sup v∈X ( u, v X -F (v))</formula><p>First, note that F * is convex (see section 2). As F is strictly convex, the solution of problem (6.15) exists and is unique. Let v(u) denote the arg max in <ref type="bibr">(6.15)</ref>. From uniqueness of the solution of (6.15), we get that F * is differentiable and its derivative is v(u). From the optimality conditions we get that u -∇F (v(u)) = 0. Thus for any (u 1 , u 2 ) ∈ X 2 (6. <ref type="formula">16</ref>) </p><formula xml:id="formula_57">∇F (v(u 1 )) -∇F (v(u 2 )) = u 1 -u 2 and |u 1 -u 2 | 2 = |∇F (v(u 1 )) -∇F (v(u 2 ))| 2 (6.17) ≥ σ 2 |v(u 1 ) -v(u 2 )| 2 (6.18) ≥ σ 2 |∇F * (u 1 ) -∇F * (u 2 )| 2 . (<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Let us show (6.9). The first extremality relation gives</head><formula xml:id="formula_58">F (ū) = -div(q), ū X -F * (-div(q)).</formula><p>We also recall that</p><formula xml:id="formula_59">F * * (u) = F (u). So that F (ū) = sup v∈X ( ū, v X -F * (v)).</formula><p>Those two equations imply that -div(q) cancels the derivative of v → ū, v X -F * (v). This ends the proof. 4. Finally let us show (6.10). It is done using the second extremality relation</p><formula xml:id="formula_60">G(∇ū) = G * * (∇ū) (6.20) = sup q∈Y ( ∇ū, q Y -G * (q)) (6.21) = -q, ∇ū Y -G * (-q). (6.22)</formula><p>Thus -q solves problem (6.21). This yields the existence of multipliers μ i such that</p><formula xml:id="formula_61">(6.23) (∇ū) i -μ i qi = 0 with μ i = 0 if |q i | 2 &lt; 1 or μ i &gt; 0 if |q i | 2 = 1.</formula><p>In both cases we get</p><formula xml:id="formula_62">μ i = |(∇ū) i | 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Nesterov's algorithm in the general strongly convex case.</head><p>Nesterov's algorithm applied to problem (6.8) writes: Algorithm 3: Nesterov's scheme for problem <ref type="bibr">(6.8)</ref>.</p><p>Input: Number of iterations N . Output: u N an estimate of ū. begin Set η k = ∇(∇F * (-div(x k ))).</p><formula xml:id="formula_63">1 Set G -1 = 0. 2 Set L =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>Set</p><formula xml:id="formula_64">y k = Π K x k - η k L . 7 Set G k = G k-1 + k + 1 2 η k . 8 Set z k = Π K - G k L . 9 Set x k+1 = 2 k + 3 z k + k + 1 k + 2 y k . 10 end 11 end<label>12</label></formula><p>This algorithm returns a variable y N that should be close to the solution of the dual problem. To retrieve the solution in the primal problem, we could thus set <ref type="bibr">(6.24)</ref> u N = ∇F * (-div(y N )).</p><p>Furthermore, using (5.9), we get that</p><formula xml:id="formula_65">(6.25) 0 ≤ F * (-div(y N )) -F * (-div(q)) ≤ 8||div|| 2 2 n σ(k + 1)(k + 2)</formula><p>, Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php which is a convergence rate on the dual variable. Another choice consists in using a linear combination of the solutions at all iterations. It leads to better practical efficiency and allows one to prove convergence rates for the primal variables. Let us set ūN = 2 (N +1)(N +2) N i=1 (i + 1)u i with u i = ∇F * (-div(x i )) and φ * (u) = ||∇u|| 1 + F (u). The following proposition summarizes the rates of convergence we obtain on the variable ūN : Proposition 6.2. Algorithm 3 ensures that</p><formula xml:id="formula_66">(6.26) 0 ≤ φ * (ū N ) -φ * (ū) ≤ 4||div|| 2 2 n σ(N + 1)(N + 2) and (6.27) |ū N -ū| 2 2 ≤ 8||div|| 2 2 n σ 2 N 2 .</formula><p>The proof is given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Application example:</head><formula xml:id="formula_67">F (u) = |λ(Au -f )| 2</formula><p>2 . An important class of strongly convex functions writes:</p><formula xml:id="formula_68">F (u) = |λ(Au -f )| 2</formula><p>2 with A a bijective linear application and λ = diag(λ i ) a diagonal matrix with</p><formula xml:id="formula_69">λ i ∈]0, ∞]. Let λ -= min i λ i and λ -(A) denote the smallest eigenvalue of A. Proposition 6.3. F * is L-Lipschitz differentiable, with L ≤ 1 2λ 2 -λ-(A) 2 . More- over (6.28) F * (v) = A -1 f, v + 1 4 |λ -1 A - * v| 2 2 .</formula><p>Proof. F is obviously differentiable, with derivative ∇F (u) = 2A * λ 2 (Auf ). Thus</p><formula xml:id="formula_70">|∇F (u) -∇F (v)| 2 = 2|A * λ 2 A(u -v)| 2 ≥ 2λ 2 -λ 2 -(A)|u -v| 2 .</formula><p>F is thus strongly convex with convexity parameter σ = 4λ 2 -λ -(A) 2 . Then</p><formula xml:id="formula_71">F * (v) = sup u∈X u, v X -|λ(Au -f )| 2 2 (6.29) = sup w∈X A -1 (λ -1 w + f ), v -|w| 2 2 (6.30) = A -1 f, v + sup r≥0 r|λ -1 A - * v| 2 2 -r 2 |λ -1 A - * v| 2 2 , (6.31)</formula><p>and we get the result by canceling the derivative of r → rr 2 .</p><p>To solve problem</p><formula xml:id="formula_72">(6.32) inf u∈X J(u) + |λ(Au -f )| 2 2 ,</formula><p>the approach we propose consists in solving its dual problem (6.8) using a Nesterov algorithm. Setting K = {q ∈ Y, ||q|| ∞ ≤ 1}, the algorithm is as follows: Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Algorithm 4: Modified Nesterov scheme for problem <ref type="bibr">(6.32)</ref>.</p><p>Input: Number of iterations N . Output: u N an estimate of ū. begin</p><formula xml:id="formula_73">1 Set G -1 = 0. 2 Set L = ||div|| 2 2 2λ 2 -λ-(A) 2 3 Set ūN = 0 4 Set x k = 0. 5 for k going from 0 to N do 6 Set η k = ∇(A -1 f ) - 1 2 ∇A -1 λ -2 A - * div(x k ). 7 Set y k = Π K x k - η k L . 8 Set G k = G k-1 + k + 1 2 η k . 9 Set z k = Π K - G k L . 10 Set x k+1 = 2 k + 3 z k + k + 1 k + 2 y k . 11 Set ū = ū + (k + 1)(A -1 f - 1 2 A -1 λ -2 A - * div(x N )) 12 end 13 Set ūN = 2 (N + 1)(N + 2) ū. 14 end<label>15</label></formula><p>Note that in this formulation, we optimize a variable in Y = X × X instead of X. Using (6.26), we get that (6.33)</p><formula xml:id="formula_74">φ * (ū N ) -φ * (ū) ≤ ||div|| 2 2 n λ 2 -λ 2 -(A)N 2 . 7.</formula><p>Numerical results and discussion. We cannot do an exhaustive comparison of all numerical methods that solve total variation problems. The bibliography about this problem contains more than 50 items. Most are time consuming to implement and their efficiency heavily depends on some choices like preconditionners. We thus restrict our experimental numerical comparisons to widely used first-order methods. Namely: the projected gradient descent and the projected subgradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Some comparisons for the Rudin-Osher-Fatemi problem.</head><p>The problem we choose for numerical comparisons is the Rudin-Osher-Fatemi model. The reasons for this choice are that many recent papers give convergence results on that problem and that it allows one to compare the primal and dual approaches. It consists in solving <ref type="bibr">(7.1)</ref> inf</p><formula xml:id="formula_75">u∈X J(u) + λ 2 |u -f | 2 2</formula><p>or equivalently</p><formula xml:id="formula_76">(7.2) inf u∈X,|u-f |2≤α</formula><p>(J(u)) . Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Finding λ and α such that the solution of (7.1) is the same as that of (7.2) is not straightforward. To find those parameters, we let the Nesterov method applied to the dual problem of (7.1) run until convergence. This provides a solution ūλ . Then we set α = |ū λf | 2 . Let us describe the methods we implement for comparisons:</p><p>Algo1 is the presented Nesterov approach applied to the dual problem <ref type="bibr">(6.8)</ref>. This corresponds to Algorithm 4. Algo2 is the projected gradient applied to the dual problem <ref type="bibr">(6.8)</ref>. This approach is slightly faster than Chambolle's initial algorithm <ref type="bibr" target="#b11">[11]</ref> (see <ref type="bibr" target="#b12">[12]</ref> for a comparison). Algo3 is the presented Nesterov + smoothing approach (2). Algo4 is a projected gradient descent with optimal constant step</p><formula xml:id="formula_77">(7.3) u 0 = f u k+1 = Π K (u k -t∇J μ (u k )). We set K = {u ∈ X, |u -f | 2 2 ≤ α 2 }.</formula><p>The optimal step can be shown to be t = 2μ ||div|| 2 2 <ref type="bibr" target="#b47">[46]</ref>. We use the 256 × 256 Lena image rescaled in [0, 1]. We add a white gaussian noise (σ = .15). This corresponds to the images in Figure <ref type="figure" target="#fig_8">7</ref>. The bottom-left figure (BL) is given in order to show that the smoothing technique gives satisfying results in a really small number of iterations. The curves in Figure <ref type="figure">8</ref> compare the distance from the current estimate to the minimizer w.r.t. the number of iterations for the different methods.</p><p>First note that the precision of the smoothing approaches (Algo3 and Algo4) is bounded below by a positive constant due to the approximation error. Therefore, it is useless-for a fixed regularization parameter μ-to iterate too much.</p><p>The dual problem solved with a Nesterov scheme (Algo1) clearly outperforms all tested approaches. Algo2 seems to be the second most efficient algorithm. However, it leads to precise solutions much slower.</p><p>In the primal formulation, Nesterov's algorithm (Algo3) outperforms the projected gradient descent (Algo4). In any case we see that it is preferable to use Nesterov's scheme compared to simpler schemes as the projected gradient descent. When it is possible (strongly convex data term), it is very interesting to solve a dual problem.</p><p>Let us finally precise that depending on the applications, the computational effort per iteration of Nesterov's technique is between one and twice that of the projected gradient descent. Now let us compare more precisely the efficiency of the projected gradient descent applied to the dual of (7.1) (Algo2), with the "smoothing + Nesterov" technique (Algo3). In Figure <ref type="figure">9</ref>, we treat the original Lena image and set λ = 1. We can see that for any iterations number k, there exists a pair (k, μ) such that the precision obtained by Algo2 is the same as that obtained by Algo3. It indicates that the "smoothing + Nesterov" algorithm has roughly the same efficiency as Chambolle's scheme. Note that this algorithm can be used in a much wider class of constrained problems.</p><p>Another interesting remark is that the best precision that can be obtained with the smoothing technique depends linearly on μ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparisons for other constrained problems.</head><p>In this paragraph, we focus on the primal formulations. Our aim is to compare three different algorithms for solving the constrained total variation problem (1.1) with different functions F . Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php The tested algorithms are Algo3 and Algo4 setting μ = 0.001<ref type="foot" target="#foot_3">4</ref> and Algo5 which is described below:</p><p>Algo5 is a projected subgradient descent with optimal step (4.1). η k must belong to ∂J(u k ) for convergence. We choose:</p><p>(7.4)</p><formula xml:id="formula_78">η k = -div(Ψ) with Ψ i = (∇u k )i |(∇u k )i|2 if |(∇u k ) i | 2 &gt; 0 0</formula><p>o t h e r w i s e and t k = J(u k )- J |η k |2 . This step is optimal in the sense that it leads to an O 1 2 algorithm. As J is unknown, some authors try to evaluate it iteratively <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b17">17]</ref>. To find it, we just let a program (Nesterov) run until convergence, and get the optimal value J. Clearly this method is not usable in practice but serves as a reference. Fig. <ref type="figure">9</ref>. Comparison of the projected gradient applied to the dual of (7.1) with the Nesterov's scheme applied to the smoothed version of (7.2).</p><p>We test the efficiency of the method under various constraints. The tested problems are:</p><p>• The Rudin-Osher-Fatemi problem (Figure <ref type="figure" target="#fig_10">10</ref>) which consists in choosing</p><formula xml:id="formula_79">F (u) = |u -f | 2 .</formula><p>• The BV-L1 problem (Figure <ref type="figure" target="#fig_11">11</ref>). It consists in choosing</p><formula xml:id="formula_80">F (u) = |u -f | 1 .</formula><p>• The deconvolution problem (Figure <ref type="figure" target="#fig_0">12</ref>), which consists in choosing F (u) = |h uf | 2 . We notice that in all cases, Nesterov's scheme (Algo3) achieves much better than the projected gradient descent (Algo4). The projected subgradient descent with optimal step (Algo5) decreases the cost function very fast in the first iterations. Asymp-Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php  totically, its rate of convergence seems to be lower than the proposed "Nesterov + smoothing" approach. Our conclusion is that the projected subgradient descent with precomputed sequences {t k } might be of interest to get approximate solutions in just a few iterations. To get accurate solutions in slightly higher computing times, the proposed approach "Nesterov + smoothing" approach is very appropriate. faster than the projected gradient descent. The algorithm we apply <ref type="bibr" target="#b38">[38]</ref> dates from 2005. The first optimal scheme was proposed in 1983 <ref type="bibr" target="#b36">[36]</ref> and it seems that it is the one most people test. Our experiences confirm that the new proposed schemes are really efficient in practice, as is shown by Nesterov in his papers. Let us finally mention that the Nesterov's schemes were recently generalized and accelerated for a broader class of problems in <ref type="bibr" target="#b39">[39]</ref>. Nesterov gives further numerical results on l 1 type problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.">Comparisons with other methods.</head><p>Second-order methods are commonly used to solve problem (1.2) and it seems they represent the closest rivals of our approach. Many papers suggest the use of half-quadratic minimization <ref type="bibr" target="#b25">[25]</ref> which was shown recently to be equivalent to quasi-Newton techniques <ref type="bibr" target="#b41">[41]</ref>. Those methods are proved to converge linearly <ref type="bibr" target="#b0">[1]</ref>. Such a rate is better asymptotically than our polynomial energy decays. This results in convergence after fewer iterations. The counterpart clearly is the need to solve a huge linear system at each iteration. The efficiency of this method strongly depends on the conditioning number of the system and the choice of preconditioners. It is thus difficult to compare both approaches.</p><p>Second-order cone programming was proposed recently <ref type="bibr" target="#b54">[53]</ref> and leads to very precise solutions, but the computing times seem to be very high. It is definitely a good choice to get "exact" solutions.</p><p>A very promising approach based on graph-cuts was proposed recently <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b12">12]</ref>. The authors solve (1.2) for A = Id and p ∈ {1, 2, ∞}. They show that they get exact solutions (up to a quantization parameter) in a finite number of iterations. That kind of approach continues being improved <ref type="bibr" target="#b20">[20]</ref> and is clearly faster than our approach for the BV -L 1 problem. However, the approach presented in this paper allows one to solve a much larger class of problems with a good efficiency and simpler implementations.</p><p>We think that very precise solutions are not needed in general in image processing. The visual system is unable to detect small perturbations. <ref type="foot" target="#foot_4">5</ref> Our method is thus very competitive with previously proposed schemes. It leads to precise enough solutions in short times and the algorithms are very easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion.</head><p>We presented efficient first-order algorithms to minimize the total variation under many smooth or nonsmooth convex sets. Those schemes are simple to implement and present low computing times. They are based on a recent advance <ref type="bibr" target="#b38">[38]</ref> in convex optimization. Their efficiency is comparable or better than state of the art methods.</p><p>In this paper we focused on total variation problems. It is straightforward to replace the operators ∇ and -div of the paper by other linear transforms B and B * . This would allow one to solve efficiently many other interesting problems like sparse reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A.</head><p>A.1. Discretization of differential operators. In this section, to simplify the notations, we denote u(i, j) the value of u on pixel (i, j). n x and n y will represent the number of pixels in the horizontal and vertical directions, respectively.</p><p>To discretize the gradient we used in all experiments the following classical firstorder scheme borrowed from <ref type="bibr" target="#b11">[11]</ref>. For u ∈ X</p><formula xml:id="formula_81">(A.1) (∇u)(i, j) = ((∂ 1 u)(i, j), (∂ 2 u)(i, j)). ∇u is an element of Y . (A.2) (∂ 1 u)(i, j) = u(i + 1, j) -u(i, j) if i &lt; n x 0 i fi = n x (A.3) (∂ 2 u)(i, j) = u(i, j + 1) -u(i, j) if j &lt; n y 0 i fj = n y</formula><p>This definition allows one to define the divergence properly by duality, imposing</p><formula xml:id="formula_82">(A.4) ∇u, p Y = -u, div (p) X .</formula><p>Simple computation gives</p><formula xml:id="formula_83">(A.5) (div (p))(i, j) = ⎧ ⎨ ⎩ p 1 (i, j) -p 1 (i -1, j) if 1 &lt; i &lt; n x p 1 (i, j) i f i = 1 -p 1 (i -1, j) i fi = n x + ⎧ ⎨ ⎩ p 2 (i, j) -p 2 (i, j -1) if 1 &lt; j &lt; n y p 2 (i, j) i f j = 1 -p 2 (i, j -1) if j = n y</formula><p>Note that the operator div is surjective from Y to X -{(γ, γ, . . . , γ), γ ∈ R}. Moreover it can be shown <ref type="bibr" target="#b11">[11]</ref>  Let ȳ denote the solution of (A.6). A first important remark that holds for any p is that if λ i = 0, then ȳi = x i . If λ i = ∞, then ȳi = f i . Thus in all projection algorithms the first step is to set all those known values. This allows us to restrict our attention to the elements λ i ∈]0, ∞[. A.2.1. Projections on weighted l ∞ -balls. The simplest projector is the one on weighted l ∞ -balls. It writes in closed form </p><formula xml:id="formula_84">(A.7) ȳi = ⎧ ⎨ ⎩ x i if |λ i (f i -x i )| ≤ α f i + x i -f i |x i -f i | α λ i otherwise A.</formula><formula xml:id="formula_85">Ψ(σ) = n i=1 |λ i ūi | (A.11) = i,|xi|≥σλi/2 λ i (|x i | -σλ i /2) (A.12) = i,yi≥σ λ i |x i | -σλ 2 i /2 (A.13) with y i = 2|xi|</formula><p>λi . Now, it is important to remark that Ψ is a piecewise linear decreasing function. The changes of slopes might only occur at values σ = y j . Thus, an algorithm to find σ is the following:  Contrarily to the l ∞ and l 1 cases, we do not propose an exact solution to this problem. We give an algorithm that leads to solutions that have the level of precision of the machine.</p><formula xml:id="formula_86">(y j(k) ) = E(k) = n i=k λ j(i) |x j(i) |- y j(k) λ 2 j(i) 2 . E is decreasing. [O(n) operations] 4. -If E(1) &lt; α, set a1 = 0, b1 = |λx| 1 , a 2 = y j(1) , b2 = E(1). [O(1) operations] -Otherwise, find k s.t. E( k) ≥ α and E( k + 1) &lt; α. Set a1 = y j( k) , b1 = |E( k)| 1 , a 2 = y j( k+1) , b2 = E( k + 1). [O(n) operations]</formula><p>• First notice that ȳ = x if |λx| 2 2 ≤ α. • Otherwise it can be shown using Lagrange multipliers that the solutions of (A. Theoretically, this scheme converges superlinearly. In all our numerical experiments on deconvolution, we never needed more than 15 iterations to get a 10 -15 precision, and the ellipsoids are degenerate in that case. The average number of iterations is 6. The projection on a weighted l 2 -balls is thus very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Proposition 4.1.</head><p>Proof. Let Jμ denote the solution of (5.10) and J denote the solution of (1.1). Using (5.5), we easily get that | J -Jμ | ≤ nμ 2 . The projected gradient is an O( LD k ) algorithm, where L is the Lipschitz constant of the gradient of the function to be minimized and D is the squared euclidean distance from the initial point to the solution (see for instance <ref type="bibr" target="#b47">[46]</ref>). Thus Algorithm 4. Let us introduce the notations. We set</p><formula xml:id="formula_87">α i = i+1 2 , A k = k i=0 α i = (k+1)(k+2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>. We denote φ(q) = F * (-div (q)) and φ * (u) = ||∇u|| 1 + F (u). Equation (A.16) reduces to φ(q) = -φ * (ū).</p><p>Let us introduce the function</p><formula xml:id="formula_88">(A.17) Ψ k (x) = L 2 ||x -x 0 || 2 2 + k i=0 α i (φ(x i ) + ∇φ(x i ), x -x i Y ),</formula><p>where L is the Lipschitz constant of φ. We proved in (6.1) that L ≤</p><formula xml:id="formula_89">||div || 2 2 σ</formula><p>. Using (5.8), we get that Algorithm 3 ensures (A.18)</p><formula xml:id="formula_90">A k φ(y k ) ≤ inf x∈K Ψ k (x)</formula><p>We have φ(x k ) = F * (-div (x k )) = sup u∈X u, -div (x k ) X -F (u) . Let u k denote the solution of that problem. It is unique as F is strongly convex. As F is differentiable, u k satisfies: -div (x k ) -∇F (u k ) = 0. So that φ(x k ) = u k , ∇F (u k ) &gt; -F (u k ). Moreover as F * is defined as a supremum, and its derivative is given by ∇F * (-div (x k )) = u k . So that φ(x k + h)φ(x k ) = ∇F * (-div (x k )), -div (h) X + o(||h|| 2 ) (A. Thus we get ∇φ(x k ) = ∇u k . Replacing φ(x i ) and ∇φ(x i ) by their expressions in terms of u i , we get </p><formula xml:id="formula_91">φ(x i ) + ∇φ(x i ), x -x i Y (A.22) = u i , ∇F (u i ) X -F (u i ) + u i , div (x i ) X + ∇u i , x Y (A.23) = -F (u i ) + ∇u i , x Y (A.24) Let us denote: ūk = k i=0 α i A k u i . As F is convex, F (ū k ) ≤ k i=1 α i A k F (u i ). So that inf x∈K Ψ k (x) (A.25) = inf x∈K L 2 ||x -x 0 || 2 2 -A k k i=1 α i A k F (u i ) + A k ∇ū k , x Y (A.26) ≤ -A k F (ū k ) + inf x∈K L 2 ||x -x 0 || 2 2 + A k ∇ū k , x &gt; (A.27) ≤ -A k (F (ū k ) + ||∇u k || 1 ) + L 2 || - ∇ū k |∇ū k | -x 0 ||</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 1 .</head><label>1</label><figDesc>Projected subgradient descents. Maybe the most straightforward algorithm to solve (1.1) for general convex function F is the projected subgradient descent algorithm. It writes (4.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proposition 5 . 3 .</head><label>53</label><figDesc>Let K = {u ∈ X, F (u) ≤ α} and D = max u∈K (d(u)). Algorithm 1 applied to problem (5.10) with μ = n and N = 2 √ 2||div ||2 √ Dn + 1 ensures that |J(y N ) -J | ≤ where J denotes the infimum of (1.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 4 .</head><label>4</label><figDesc>Some application examples. 5.4.1. Restoration involving linear invertible transforms: F (u) = |λ(Au f )| p with p ∈ {1, 2, ∞}. We showed in section 3 that one of the most interesting data terms is F (u) = |λ(Auf )| p where p ∈ {1, 2, ∞}, A is a linear invertible transform, λ is a diagonal matrix with elements in [0, ∞] n , and f are the given data. To apply Algorithm 1 to problem (5.10), we need to be able to compute projections on {u, |λ(Auf )| p ≤ α}. As this might be cumbersome, we use the change of variable (5.16) z = Auf and solve the equivalent problem (5.17) inf z,|λz|p≤α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image deconvolution: TL: original image; TR: convolved noisy image; BL: solution of (5.19) μ = 0.001, N = 150; BR: solution of (5.19) μ = 10 -7 , N = 10 5 .</figDesc><graphic coords="14,72.24,277.66,178.36,178.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Image to be decomposed.</figDesc><graphic coords="16,178.80,97.18,154.60,154.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Cartoon + texture decompositions. Top: Meyer's model. Bottom: Osher-Solé-Vese's model.</figDesc><graphic coords="17,72.24,277.66,178.36,178.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Cartoon + texture decompositions. Top: BV -l 1 model. Bottom: Result of minimizing the l 1 -norm of the g field in (5.23).</figDesc><graphic coords="18,72.24,277.66,178.36,178.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 Set x k = 0. 4 for k going from 0 to N do 5</head><label>345</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. TL: original image (scaled in [0, 1]); TR: Noisy Lena; BL: solution of (7.2) obtained using Algo3 and setting μ = 0.01 and N = 50 iterations; BR: exact solution of (7.2) obtained using Algo1 until convergence.</figDesc><graphic coords="24,72.24,277.66,178.36,178.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 Fig. 8 .</head><label>38</label><figDesc>Fig. 8. Convergence rate comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. ||∇u k || 1 -||∇ū|| 1 in log scale for ROF problem. Results on Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. ||∇u k || 1 -||∇ū|| 1 in log scale for BV-L1 problem. Results on Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1 .Fig. 12 .</head><label>112</label><figDesc>Fig. 12. ||∇u k || 1 -||∇ū|| 1 in log scale for deconvolution problem. Results on Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5 .A. 2 . 3 .</head><label>523</label><figDesc>Set σ = (a2-a1)α+b2a1-b1a2 b2-b1 . [O(1) operations] 6. Set ū = u(σ) using (A.10). [O(n) operations] Projections on weighted l 2 -balls. The projection on a weighted l 2ball (an ellipsoid) writes (A.14) Π K (x) = arg min {y,|λy| 2 2 ≤α} |y -x| 2 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 n i=1 λ 4 i x 2 i(σ k λ 2 i +1) 3 . 4 .. 5 .</head><label>222345</label><figDesc>σ|λ i | 2 + 1for some parameter σ &gt; 0. Moreover, we know that|λȳ| 2 2 = α. Let Ψ(σ) = n i=1 | λixi σ|λi| 2 +1 | 2 2 .We are looking for a parameter σ s.t. Ψ(σ) = α. It can be shown that Ψ is convex decreasing. To find σ we can use a Newton method. It writes:1. Set k = 0, σ k = 0. 2. Compute α k = Ψ(σ k ). 3. Compute β k = Ψ (σ k ) = -Set σ k+1 = σ k + (α-α k ) β k Set k = k + 1,go back to 2 until |α k -α| ≤ . 6. Set ȳ = x σ k |λ| 2 +1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>3 ensures that | Jμ (u k ) -Jμ | ≤ C kμ where C is some constant independent of μ and k. Combining those two inequalities, we get that |J(u k ) -J| ≤ C kμ + nμ 2 . To get an -solution, it is thus sufficient to have C kμ + nμ 2 &lt; . It is the case if k = C μ( -nμ 2 ) + 1. Maximizing the denominator in this expression, we get that the optimal pair (μ, k) is μ = n and k = Cn 2 2 + 1 . Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php A.4. Proof of Proposition 6.2. Proof. A direct consequence of equality (6.5) is (A.16) inf q∈K (F * (-div (q))) =inf u∈X (||∇u|| 1 + F (u)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>19) = ∇(∇F * (-div (x k ))), h Y + o(||h|| 2 ) (A.20) = ∇u k , h Y + o(||h|| 2 ) (A.<ref type="bibr" target="#b21">21)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>and (1.2). Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Summary of the problems covered by our formalism.</figDesc><table><row><cell></cell><cell>p = 1</cell><cell>p = 2</cell><cell>p=∞</cell></row><row><cell>A = Identity</cell><cell>Impulse noise denois-</cell><cell></cell></row><row><cell></cell><cell>ing, image decompo-</cell><cell></cell></row><row><cell></cell><cell>sition [2, 43, 13, 19]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For simplicity and because we found no numerical interest in other choices, we choose the Euclidean norm |•| 2 and set d</figDesc><table /><note><p>steps 6 and 8. To apply Algorithm 1, we first have to choose a norm ||•|| and a function d.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php SNR of the denoised image w.r.t. the μ parameter.</figDesc><table><row><cell></cell><cell>34</cell><cell></cell><cell cols="2">INITIAL SNR : 34.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNR of the denoised image</cell><cell>24 26 28 30</cell><cell></cell><cell cols="2">INITIAL SNR : 20.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SNR of the denoised image</cell><cell>24.85 24.81 24.82 24.83 24.84</cell><cell></cell><cell cols="3">Initial SNR : 20.42 dB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell cols="2">INITIAL SNR : 12.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-6 18</cell><cell>-5</cell><cell>-4</cell><cell>-3</cell><cell>log 10 (µ)</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>1</cell><cell>-6</cell><cell>-5.5</cell><cell>-5</cell><cell>-4.5</cell><cell>-4</cell><cell>-3.5 (µ) 10 log</cell><cell>-3</cell><cell>-2.5</cell><cell>-2</cell><cell>-1.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.<ref type="bibr" target="#b19">19)</ref> This shows that F * is 2 σ -Lipschitz differentiable. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php PIERRE WEISS, LAURE BLANC-F ÉRAUD, AND GILLES AUBERT</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Projections on weighted l p -balls (p ∈ {1, 2, ∞}).</head><label></label><figDesc>that ||div || 2 ≤ 2 Until now, we supposed that we could do Euclidean projections on weigthed l p -balls. Some projection operators are not straightforward to implement and we propose solutions to that problem. Let K = {y ∈ X, |λ(yf )| p ≤ α}, where λ is a diagonal matrix whose elements λ i belong to [0, ∞]. The problem of projection on K can be written analytically Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell></cell><cell></cell><cell>√ 2.</cell></row><row><cell>A.2. (A.6)</cell><cell>Π K (x) = arg min y∈K</cell><cell>|y -x| 2 2 .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>2.2. Projections on weighted l 1 -balls.</head><label></label><figDesc>Up to a change of variable, the projection on a weighted l 1 -ball writes</figDesc><table><row><cell>(A.8)</cell><cell cols="2">Π K (x) = arg min</cell><cell cols="3">|u -x| 2</cell></row><row><cell></cell><cell></cell><cell cols="2">u,|λu|1≤α</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the</cell></row><row><cell cols="6">solution of (A.8) is given by the solution of the Lagrangian problem</cell></row><row><cell>(A.9)</cell><cell cols="2">Π K (x) = arg min u∈R n</cell><cell cols="3">|u -x| 2 2 + σ|λu| 1 .</cell></row><row><cell cols="5">The solution of this problem is in closed form</cell></row><row><cell>(A.10)</cell><cell>u(σ) i =</cell><cell cols="2">x i -sgn(x i ) 0</cell><cell>σλ i 2</cell><cell>σλ i 2 o t h e r w i s e if |x i | ≥</cell></row><row><cell cols="2">Let Ψ(σ) = |λu(σ)|</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p>2</p>with λ i ∈]0, ∞[ and α &gt; 0.</p>• First notice that if |λx| 1 ≤ α, then ū = x.</p>• In the other cases, existence and uniqueness of a minimizer results from strict convexity of |u -x| 2 2 and convexity of K. There exists σ ∈ [0, ∞[ s.t. 1 . Our problem is to find σ such that Ψ(σ) = α. Ψ is a convex function (thus continuous) and decreasing. Moreover Ψ(0) = |λx| 1 , and lim σ→∞ Ψ(σ) = 0. From intermediary values theorem, for any α ∈ [0, |λx| 1 ], there exists σ s.t. Ψ(σ) = α.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>1. For i ∈ [1..n], compute y i = 2|xi| λi . [O(n) operations] 2. Using a sort function, store the permutation j s.t. k → y j(k) is increasing. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 3. Compute the partial sums: Ψ</figDesc><table /><note><p>[O(n)log(n) operations]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php At line (A.28), qk = ∇ū k |∇ū k | is defined only on the set U = {p, (|∇ū k |) p = 0}. On the complement of U a possible choice is to set qk p = x 0 p . Now, taking x 0 = 0, we have sup x∈K ||x 0 -x|| 2 2 = n. So that finally inf x∈K Ψ k (x) ≤ -A k φ * (ū k ) + Ln 2 . Using (A.18), we get (A.30)φ * (ū k ) ≤ -φ(y k ) + Ln 2A k . (ū k )φ * (ū) ≤ -φ(y k ) + φ(q) + Ln 2A k .Furthermore, as y k ∈ K, -φ(y k ) + φ(ȳ) ≤ 0. Finally, replacing every expression by their majoration, we get the first result (A.32) φThen, we can remark that φ * is strongly convex. Thus it satisfies (see<ref type="bibr" target="#b47">[46]</ref>): φ (ū) ≥ σ 2 |u -ū| 2 2 . Using (A.32) it is thus straightforward to get</figDesc><table><row><cell cols="2">As φ(q) = -φ  *  (ū), we have</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(A.31)</cell><cell cols="6">φ  4||div || 2 2 n σ(k + 1)(k + 2)</cell><cell>.</cell></row><row><cell>(A.33)</cell><cell cols="4">|ū k -ū| 2 ≤</cell><cell>2</cell><cell>√ 2||div || 2 σk</cell><cell>√ n</cell><cell>.</cell></row><row><cell>(A.28)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 2</cell></row><row><cell>(A.29)</cell><cell>≤ -A k φ  *  (ū k ) +</cell><cell>L 2</cell><cell>|| -</cell><cell cols="3">∇ū k |∇ū k |</cell><cell>-x 0 || 2 2 .</cell></row></table><note><p><p>* * (ū k )φ * (ū) ≤ * (u + h) ≥ φ * (u) + η, h X + σ 2 |h| 2 2</p>for any η ∈ ∂φ * (x) and any h ∈ X. In particular φ * (u)φ *</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is an important feature for Graphic Processing Unit or programmable logic device programming.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is possible if we suppose that images have a bounded amplitude. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The Lipschitz constant determines the convergence rate of most first-order schemes. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This leads to solutions that are perceptually identical to the solutions using μ = 0. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>A uniform noise of amplitude 5 is almost invisible for images of amplitude 256. Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Downloaded 12/30/12 to 129.173.72.87. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. The first author would like to thank Alexis Baudour for useful mathematical discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On global and local convergence of half-quadratic algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1130" to="1142" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A property of the minimum vectors of a regularizing functional defined by means of the absolute norm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alliney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="913" to="917" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Total variation regularized image restoration: The case of perturbed sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="235" to="272" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapted total variation for artifact free decompression of jpeg images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Froment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image decomposition into a bounded variation component and an oscillating component</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Féraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="71" to="88" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Contribution à l&apos;analyse de textures en traitement d&apos;images par méthodes variationnelles et équations aux dérivées partielles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Aujol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Thèse de l&apos;université de Nice</publisher>
			<pubPlace>Sophia-Antipolis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Total variation image restoration with local constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rougé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="95" to="122" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color TV: Total variation methods for restoration of vector valued images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Blomgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="304" to="309" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New multiscale transforms, minimum total variation synthesis: Application to edge-preserving image reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1519" to="1543" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very efficient first order schemes for solving inverse problems in image restoration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carlavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Féraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 12/30/12 to 129.173.72.87</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An algorithm for total variation minimization and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Total variation minimization and a class of binary MRF models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<title level="s">Lecture Notes in Comput. Sci.</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="page" from="136" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aspects of total variation regularized L 1 function approximation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Esedoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1817" to="1837" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A nonlinear primal-dual method for total variationbased image restoration</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1964" to="1977" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image Processing and Analysis -Variational, PDE, Wavelet, and Stochastic Methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining the calculus of variations and wavelets for image enhancement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An adaptive level set method for nondifferentiable constrained image recovery</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1295" to="1304" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image restoration subject to a total variation constraint</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1295" to="1304" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image restoration with discrete constrained total variation part I: Fast and exact optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="261" to="276" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Global optimization for first order Markov random fields with submodular priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
			<publisher>UCLA-CAM Report</publisher>
			<biblScope unit="page" from="8" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconstruction of wavelet coefficients using total variation minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Froment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1754" to="1767" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising of frame coefficients using L1 data-fidelity term and edge-preserving regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="547" to="576" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convex Analysis and Variational Problems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ekeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies Math. Appl</title>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>American Elsevier</publisher>
			<pubPlace>Amsterdam, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient minimization of mixed l 2 -l 1 and l 1 -l 1 norms for image restoration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1881" to="1902" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear image recovery with half-quadratic regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="932" to="946" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Second-order cone programming methods for total variation based image restoration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="622" to="645" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An infeasible primal-dual algorithm for TV-based infconvolution-type image restoration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hintermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The efficiency of subgradient projection methods for convex optimization. Part I: General level methods</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kiwiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="660" to="676" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The efficiency of subgradient projection methods for convex optimization. Part II: Implementations and extensions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kiwiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="677" to="697" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An efficient operator splitting method for noise removal in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comp. Phys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Solving a variational image restoration model which involves L ∞ constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lintner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Malgouyres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="815" to="831" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A computational algorithm for minimizing total variation in image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Santosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="987" to="995" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edge direction preserving image zooming: A mathematical and numerical analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Malgouyres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guichard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Oscillating patterns in image processing and in some nonlinear evolution equations, The Fifteenth Dean Jaqueline B</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Lewis Memorial Lectures</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Problem Complexity and Method Efficiency in Optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A method for unconstrained convex minimization problem with the rate of convergence O( -2 )</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doklady AN SSSR</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introductory Lectures on Convex Optimization: A Basic Course</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program., Ser. A</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Gradient methods for minimizing composite objective function, CORE discussion paper</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A fast algorithm for deblurring models with Neumann boundary conditions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="851" to="866" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The equivalence of half-quadratic minimization and the gradient linearization iteration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1623" to="1627" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure Blanc-F</forename><surname>Éraud</surname></persName>
		</author>
		<author>
			<persName><surname>Gilles Aubert</surname></persName>
		</author>
		<idno>Downloaded 12/30/12 to 129.173.72.87</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local strong homogeneity of a regularized estimator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="633" to="658" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A variational approach to remove outliers and impulse noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="99" to="120" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image decomposition and restoration using total variation minimization and the H -1 norm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="349" to="370" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling textures with total variation minimization and oscillating patterns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="553" to="572" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Introduction to Optimization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Translation Series in Mathematics and Engineering, Optimization Software</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Total variation based image restoration with free local constraints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image decomposition via the combination of sparse representation and a variational approach</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1570" to="1582" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Quantization noise removal for optimal transform decoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tramini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="381" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Iterative methods for total variation denoising</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Oman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Some applications of l ∞ -constraints in image processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Féraud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA Research Report</title>
		<imprint>
			<biblScope unit="volume">6115</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A comparison of total variation based texture extraction models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Representation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="240" to="252" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
