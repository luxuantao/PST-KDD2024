<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Canonical Correlation Analysis to Self-supervised Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>daviwipf@amazon.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">AWS Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Canonical Correlation Analysis to Self-supervised Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised learning (SSL) has been a promising paradigm for learning useful representations without costly labels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref>. In general, it learns representations via a proxy objective between inputs and self-defined signals, among which contrastive methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> have achieved impressive performance on learning image representations by maximizing the mutual information of two views (or augmentations) of the same input. Such methods can be interpreted as a discrimination of a joint distribution (positive pairs) from the product of two marginal ones (negative pairs) <ref type="bibr" target="#b49">[50]</ref>.</p><p>Inspired by the success of contrastive learning in vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>, similar methods have been adapted to learning graph neural networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. Although these models have achieved impressive performance, they require complex designs and architectures. For example, DGI <ref type="bibr" target="#b47">[48]</ref> and MVGRL <ref type="bibr" target="#b14">[15]</ref> rely on a parameterized mutual information estimator to discriminate positive node-graph pairs from negative ones; GRACE <ref type="bibr" target="#b56">[57]</ref> and GCA <ref type="bibr" target="#b57">[58]</ref> harness an additional MLP-projector to guarantee sufficient capacity. Moreover, negative pairs sampled or constructed from data often play an indispensable role in providing effective contrastive signals and have a large impact on performance. Selecting proper negative samples is often nontrivial for graph-structured data, not to mention the extra storage cost for prohibitively large graphs. BGRL <ref type="bibr" target="#b38">[39]</ref> is a recent endeavor on Table <ref type="table">1</ref>: Technical comparison of self-supervised node representation learning methods. We provide a conceptual comparison with more self-supervised methods in Appendix G. Target denotes the comparison pair, N/G/F denotes node/graph/feature respectively. MI-Estimator: parameterized mutual information estimator. Proj/Pred: additional (MLP) projector or predictor. Asymmetric: asymmetric architectures such as EMA and Stop-Gradient, or two separate encoders for two branches. Neg examples: requiring negative examples to prevent trivial solutions. Space denotes space requirement for storing all the pairs. Our method is simple without any listed component and memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Target MI-Estimator Proj/Pred Asymmetric Neg examples Space</p><p>Instance-level DGI <ref type="bibr" target="#b47">[48]</ref> N-G ! --! O(N ) MVGRL <ref type="bibr" target="#b14">[15]</ref> N-G</p><formula xml:id="formula_0">! - ! ! O(N ) GRACE [57] N-N - ! - ! O(N 2 ) GCA [58] N-N - ! - ! O(N 2 ) BGRL [39] N-N - ! ! - O(N ) CCA-SSG (Ours) F-F - - - - O(D 2 )</formula><p>targeting a negative-sample-free approach for GNN learning through asymmetric architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. However, it requires additional components, e.g., an exponential moving average (EMA) and Stop-Gradient, to empirically avoid degenerated solutions, leading to a more intricate architecture.</p><p>Deviating from the large body of previous works on contrastive learning, in this paper we take a new perspective to address SSL on graphs. We introduce Canonical Correlation Analysis inspired Self-Supervised Learning on Graphs (CCA-SSG), a simple yet effective approach that opens the way to a new SSL objective and frees the model from intricate designs. It follows the common practice of prior arts, generating two views of an input graph through random augmentation and acquiring node representations through a shared GNN encoder. Differently, we propose to harness a non-contrastive and non-discriminative feature-level objective, which is inspired by the well-studied Canonical Correlation Analysis (CCA) methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>. More specifically, the new objective aims at maximizing the correlation between two augmented views of the same input and meanwhile decorrelating different (feature) dimensions of a single view's representation. We show that the objective 1) essentially pursuits discarding augmentation-variant information and preserving augmentation-invariant information, and 2) can prevent dimensional collapse <ref type="bibr" target="#b18">[19]</ref> (i.e., different dimensions capture the same information) in nature. Furthermore, our theoretical analysis sheds more lights that under mild assumptions, our model is an instantiation of Information Bottleneck Principle <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> under SSL settings <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>To sum up, as shown in Table <ref type="table">1</ref>, our new objective induces a simple and light model without reliance on negative pairs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, a parameterized mutual information estimator <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15]</ref>, an additional projector or predictor <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b38">39]</ref> or asymmetric architectures <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b14">15]</ref>. We provide a thorough evaluation for the model on seven node classification benchmarks. The empirical results demonstrate that despite its simplicity, CCA-SSG can achieve very competitive performance in general and even superior test accuracy in five datasets. It is worth noting that our approach is agnostic to the input data format, which means that it can potentially be applied to other scenarios beyond graph-structured data (such as vision, language, etc.). We leave such a technical extension for future works.</p><p>Our contributions are as follows:</p><p>1) We introduce a non-contrastive and non-discriminative objective for self-supervised learning, which is inspired by Canonical Correlation Analysis methods. It does not rely on negative samples, and can naturally remove the complicated components. Based on it we propose CCA-SSG, a simple yet effective framework for learning node representations without supervision (see Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>We theoretically prove that the proposed objective aims at keeping augmentation-invariant information while discarding augmentation-variant one, and possesses an inherent relationship to an embodiment of Information Bottleneck Principle under self-supervised settings (see Section 4).</p><p>3) Experimental results show that without complex designs, our method outperforms state-of-the-art self-supervised methods MVGRL <ref type="bibr" target="#b14">[15]</ref> and GCA <ref type="bibr" target="#b57">[58]</ref> on 5 out of 7 benchmarks. We also provide thorough ablation studies on the effectiveness of the key components of CCA-SSG (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works and Background</head><p>Contrastive Learning on Graphs. Contrastive methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> have been shown to be effective for unsupervised learning in vision, which have also been adapted to graphs. Inspired by the local-global mutual information maximization viewpoints <ref type="bibr" target="#b16">[17]</ref>, DGI <ref type="bibr" target="#b47">[48]</ref> and InfoGraph <ref type="bibr" target="#b37">[38]</ref> put forward unsupervised schemes for node and graph representation learning, respectively. MVGRL <ref type="bibr" target="#b14">[15]</ref> generalizes CMC <ref type="bibr" target="#b39">[40]</ref> to graph-structured data by introducing graph diffusion <ref type="bibr" target="#b22">[23]</ref> to create another view for a graph. GCC <ref type="bibr" target="#b32">[33]</ref> adopts InfoNCE loss <ref type="bibr" target="#b45">[46]</ref> and MoCo-based negative pool <ref type="bibr" target="#b15">[16]</ref> for largescale GNN pretraining. GRACE <ref type="bibr" target="#b56">[57]</ref>, GCA <ref type="bibr" target="#b57">[58]</ref> and GraphCL <ref type="bibr" target="#b51">[52]</ref> follow the spirit of SimCLR <ref type="bibr" target="#b4">[5]</ref> and learn node/graph representations by directly treating other nodes/graphs as negative samples. BGRL <ref type="bibr" target="#b38">[39]</ref> targets a negative-sample-free model, inspired by BYOL <ref type="bibr" target="#b11">[12]</ref>, on node representation learning. But it still requires complex asymmetric architectures.</p><p>Feature-level Self-supervised Objectives. The above-mentioned methods all focus on instancelevel contrastive learning. To address their drawbacks, some recent works have been turning to feature-level objectives. For example, Contrastive Clustering <ref type="bibr" target="#b24">[25]</ref> regards different feature dimensions as different clusters, thus combining the cluster-level discrimination with instance-level discrimination. W-MSE <ref type="bibr" target="#b7">[8]</ref> performs a differentiable whitening operation on learned embeddings, which implicitly scatters data points in embedding space. Barlow Twins <ref type="bibr" target="#b52">[53]</ref> borrows the idea of redundancy reduction and adopts a soft decorrelation term that makes the cross-correlation matrix of two views' representations close to an identity matrix. By contrast, our method is based on the classical Canonical Correlation Analysis, working by correlating the representations of two views from data augmentation and meanwhile decorrelating different feature dimensions of each view's representation.</p><p>Canonical Correlation Analysis. CCA is a classical multivariate analysis method, which is first introduced in <ref type="bibr" target="#b17">[18]</ref>. For two random variables X 2 R m and Y 2 R n , their covariance matrix is ⌃ XY = Cov(X, Y ). CCA aims at seeking two vectors a 2 R m and b 2 R n such that the correlation</p><formula xml:id="formula_1">⇢ = corr(a &gt; X, b &gt; Y ) = a &gt; ⌃ XY b p a &gt; ⌃ XX a p b &gt; ⌃ Y Y b is maximized. Formally, the objective is max a,b a &gt; ⌃ XY b, s.t. a &gt; ⌃ XX a = b &gt; ⌃ Y Y b = 1.<label>(1)</label></formula><p>For multi-dimensional cases, CCA seeks two sets of vectors maximizing their correlation and subjected to the constraint that they are uncorrelated with each other <ref type="bibr" target="#b9">[10]</ref>. Later studies apply CCA to multi-view learning with deep models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>, by replacing the linear transformation with neural networks. Concretely, assuming X 1 , X 2 as two views of an input data, it optimizes max</p><formula xml:id="formula_2">✓1,✓2 Tr P &gt; ✓1 (X 1 )P ✓2 (X 2 ) s.t. P &gt; ✓1 (X 1 )P ✓1 (X 1 ) = P &gt; ✓2 (X 2 )P ✓2 (X 2 ) = I.<label>(2)</label></formula><p>where P ✓1 and P ✓2 are two feedforward neural networks and I is an identity matrix. Despite its preciseness, such computation is really expensive <ref type="bibr" target="#b3">[4]</ref>. Fortunately, soft CCA <ref type="bibr" target="#b3">[4]</ref> removes the hard decorrelation constraint by adopting the following Lagrangian relaxation:</p><formula xml:id="formula_3">min ✓1,✓2 L dist (P ✓1 (X 1 ), P ✓2 (X 2 )) + (L SDL (P ✓1 (X 1 )) + L SDL (P ✓2 (X 2 ))) ,<label>(3)</label></formula><p>where L dist measures correlation between two views' representations and L SDL (called stochastic decorrelation loss) computes an L 1 distance between P ✓i (X i ) and an identity matrix, for i = 1, 2.</p><p>3 Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Framework</head><p>In this paper we focus on self-supervised node representation learning, where we consider a single graph G = (X, A). X 2 R N ⇥F and A 2 R N ⇥N denote node features and adjacency matrix respectively. Here N is the number of nodes within the graph and F denotes feature dimension.</p><p>Our model simply consists of three parts: 1) a random graph augmentation generator T . 2) a GNNbased graph encoder f ✓ where ✓ denotes its parameters. 3) a novel feature-level objective function based on Canonical Correlation Analysis. Fig. <ref type="figure" target="#fig_0">1</ref> is an illustration of the proposed model. # covariance matrix of each view c1 = torch.mm(z1_norm.T(), z1_norm) c2 = torch.mm(z2_norm.T(), z2_norm) iden = torch.eye(D) loss_inv = (z1_norm -z2_norm).pow <ref type="bibr" target="#b1">(2)</ref>.sum() loss_dec_1 = (c1 -iden).pow <ref type="bibr" target="#b1">(2)</ref>.sum() loss_dec_2 = (c2 -iden).pow <ref type="bibr" target="#b1">(2)</ref>.sum() loss_dec = loss_dec_1 + loss_dec_2 loss = loss_inv + lambda * loss_dec Graph augmentations. We consider the standard pipeline for random graph augmentation that has been commonly used in previous works <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b38">39]</ref>. To be specific, we harness two ways for augmentation: edge dropping and node feature masking. Edge dropping randomly drops a fraction of edges from the original graph, while node feature masking randomly masks a fraction of features for all the nodes. In this way, T is composed of all the possible graph transformation operations and each t ⇠ T denotes a specific graph transformation for graph G.</p><p>Note that we use commonly adopted augmentation methods to stay our focus on the design of objective function and conduct fair comparison with existing approaches. More complicated random augmentations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b57">58]</ref> can also be readily plugged into our model. Details for the used augmentation functions are in Appendix E.</p><p>Training. In each training iteration, we first randomly sample two graph transformations t A and t B from T , and then generate two views GA = ( XA , ÃA ) and GB = ( XB , ÃB ) according to the transformations. The two views are subsequently fed into a shared GNN encoder to generate the node embeddings of the two views:</p><formula xml:id="formula_4">Z A = f ✓ ( XA , ÃA ), Z B = f ✓ ( XB , ÃB ), where Z A , Z B 2 R N ⇥D</formula><p>and D denotes embedding dimension. We further normalize the node embeddings along instance dimension so that each feature dimension has a 0-mean and 1/ p N -standard deviation distribution:</p><formula xml:id="formula_5">Z = Z µ(Z) (Z) ⇤ p N<label>(4)</label></formula><p>The normalized ZA , ZB will be used to compute a feature-level objective in Section 3.2. To help better understand the proposed framework, we provide the PyTorch-style pseudocode for training CCA-SSG in Algorithm 1.</p><p>Inference. To generate node embeddings for downstream tasks, we put the original graph G = (X, A) into the trained graph neural network f ✓ and obtain node embeddings Z = f ✓ (X, A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Objective</head><p>Canonical Correlation Analysis has shown its great power in multi-view learning like instance recognition <ref type="bibr" target="#b3">[4]</ref>. However, it still remains unexplored to leverage CCA for self-supervised learning. Note that in SSL, one generates two sets of data from the same input through transformation or random data augmentation, which could be regraded as two views of the input data. This inspires us to introduce the following objective for self-supervised representation learning:</p><formula xml:id="formula_6">L = ZA ZB 2 F | {z } invariance term + ✓ Z&gt; A ZA I 2 F + Z&gt; B ZB I 2 F ◆ | {z } decorrelation term<label>(5)</label></formula><p>where is a non-negative hyperparameter trading off two terms. Note that minimizing the invariance term is essentially maximizing the correlation between two views as their representations are already normalized. In SSL, as the two augmented views come randomly from the same distribution, we can adopt one encoder f ✓ that is shared across two branches and seek for a regularization that encourages different feature dimensions to capture distinct semantics via the decorrelation term.</p><p>We next provide a variance-covariance perspective to the new objective, following similar lines of reasoning in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Assume that input data come from a distribution x ⇠ p(x) and s is a view of x through random augmentation s ⇠ p aug (•|x). Denote z s as the representation of s, then minimizing the invariance term, by expectation, is to minimize the variance of the normalized representation zs , conditioned on x. Also, minimizing the decorrelation term is to push the off-diagonal elements of the covariance matrix (given by two zs 's) close to 0. Formally, we have</p><formula xml:id="formula_7">L inv = ZA ZB 2 F = N X i=1 D X k=1 (z A i,j zB i,j ) 2 ⇠ = E x " D X k=1 V s|x [z s,k ] # ⇤ 2N,<label>(6)</label></formula><formula xml:id="formula_8">L dec = Z&gt; S ZS I 2 F = kCov s [ z] Ik 2 F ⇠ = X i6 =j ⇢ zs i,j 2 , for ZS 2 { ZA , ZB },<label>(7)</label></formula><p>where ⇢ is the Pearson correlation coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Advantages over Contrastive Methods</head><p>In this subsection we provide a systematic comparison with previous self-supervised methods for node representation learning, including DGI <ref type="bibr" target="#b47">[48]</ref>, MVGRL <ref type="bibr" target="#b14">[15]</ref>, GRACE <ref type="bibr" target="#b56">[57]</ref>, GCA <ref type="bibr" target="#b57">[58]</ref> and BGRL <ref type="bibr" target="#b38">[39]</ref>, and highlight the merits of CCA-SSG. A quick overview is presented in Table <ref type="table">1</ref>.</p><p>No reliance on negative samples. Most of previous works highly rely on negative pairs to avoid collapse or interchangeable, trivial/degenerated solutions <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>. E.g., DGI and MVGRL generate negative examples by corrupting the graph structure severely, and GRACE/GCA treats all the other nodes within a graph as negative examples. However, for self-supervised learning on graphs, it is non-trivial to construct informative negative examples since nodes are structurally connected, and selecting negative examples in an arbitrary manner may lead to large variance for stochastic gradients and slow training convergence <ref type="bibr" target="#b50">[51]</ref>. The recently proposed BGRL model adopts asymmetric encoder architectures for SSL on graphs without the use of negative samples. However, though BGRL could avoid collapse empirically, it still remains as an open problem concerning its theoretical guarantee for preventing trivial solutions <ref type="bibr" target="#b40">[41]</ref>. Compared with these methods, our model does not rely on negative pairs and asymmetric encoders. The feature decorrelation term can naturally prevent trivial solutions caused by the invariance term. We discuss the collapse issue detailedly in Appendix B.</p><p>No MI estimator, projector network nor asymmetric architectures. Most previous works rely on additional components besides the GNN encoder to estimate some score functions in final objectives. DGI and MVGRL require a parameterized estimator to approximate mutual information between two views, and GRACE leverages a MLP projector followed by an InfoNCE estimator. BGRL harnesses asymmetric encoder architecture which consists of EMA (Exponential Moving Average), Stop-Gradient and an additional projector. MVGRL also induces asymmetric architectures as it adopts two different GNNs for the input graph and the diffusion graph respectively. In contrast, our approach requires no additional components except a single GNN encoder.</p><p>Better efficiency and scalability to large graphs. Consider a graph with N nodes. DGI and MVGRL contrast node embeddings with graph embedding, which would require O(N ) space cost.</p><p>GRACE treats two views of the same node as positive pairs and treat views of different nodes as negative pairs, which would take O(N 2 ) space. BGRL focuses only on positive pairs, which will also take O(N ) space. By contrast, our method works on feature dimension. If we embed each node into a D-dimensional vector, the computation of the loss function would require O(D 2 ) space. This indicates that the memory cost does not grow consistently as the size of graph increases. As a result, our method is promising for handling large-scale graphs without prohibitively large space costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Insights with Connection to Information Theory</head><p>In this section we provide some analysis of the proposed objective function: 1) Interpretation of the loss function with entropy and mutual information.</p><p>2) The connection between the proposed objective and the Information Bottleneck principle. 3) Why the learned representations would be informative to downstream tasks. The proofs of propositions, theorems and corollaries are in Appendix D.</p><p>Notations. Denote the random variable of input data as X and the downstream task as T (it could be the label Y if the downstream task is classification). Note that in SSL, we have no access to T in training and here we introduce the notation for our analysis. Define S as the self-supervised signal (i.e., an augmented view of X), and S shares the same space as X. Our model learns a representation for the input, denoted by Z X and its views, denoted by Z S .</p><formula xml:id="formula_9">Z X = f ✓ (X), Z S = f ✓ (S), f ✓ (•)</formula><p>is a encoder shared by the original data and its views, which is parameterized by ✓. The target of representation learning is to learn a optimal encoder parameter ✓. Furthermore, for random variable A, B, C, we use I(A, B) to denote the mutual information between A and B, I(A, B|C) to denote conditional mutual information of A and B on a given C, H(A) for the entropy, and H(A|B) for conditional entropy. The proofs of propositions, theorems and corollaries are in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">An Entropy and Mutual Information Interpretation of the Objective</head><p>We first introduce an assumption about the distributions of P (Z S ) and P (Z S |X). Assumption 1. (Gaussian assumption of P (Z S |X) and P (Z S )):</p><formula xml:id="formula_10">P (Z S |X) = N (µ X , ⌃ X ), P (Z S ) = N (µ, ⌃).<label>(8)</label></formula><p>With Assumption 1, we can arrive at the following propositions: Proposition 1. In expectation, minimizing Eq. ( <ref type="formula" target="#formula_7">6</ref>) is equivalent to minimizing the entropy of Z S conditioned on input X, i.e., min</p><formula xml:id="formula_11">✓ L inv ⇠ = min ✓ H(Z S |X).<label>(9)</label></formula><p>Proposition 2. Minimizing Eq. ( <ref type="formula" target="#formula_8">7</ref>) is equivalent to maximizing the entropy of Z S , i.e., min</p><formula xml:id="formula_12">✓ L dec ⇠ = max ✓ H(Z S ).<label>(10)</label></formula><p>The two propositions unveil the effects of two terms in our objective. Combining two propositions, we can further interpret Eq. ( <ref type="formula" target="#formula_6">5</ref>) from an information-theoretic perspective. Theorem 1. By optimizing Eq (5), we maximize the mutual information between the augmented view's embedding Z S and the input data X, and minimize the mutual information between Z S and the view itself S, conditioned on the input data X. Formally we have min</p><formula xml:id="formula_13">✓ L ) max ✓ I(Z S , X) and min ✓ I(Z S , S|X). (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The proof is based on the facts I(Z S , X) = H(Z S ) H(Z S |X) and I(Z S , S|X) = H(Z S |X) + H(Z S |S) = H(Z S |X). Theorem 1 indicates that our objective Eq. ( <ref type="formula" target="#formula_6">5</ref>) learns representations that maximize the information of the input data, i.e., I(Z S , X), and meanwhile minimize the lost information during augmentation, i.e., I(Z S , S|X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Connection with the Information Bottleneck Principle</head><p>The analysis in Section 4.1 enables us to further build a connection between our objective Eq. ( <ref type="formula" target="#formula_6">5</ref>) and the well-studied Information Bottleneck Principle <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1]</ref> under SSL settings. Recall that the supervised Information Bottleneck (IB) is defined as follows:</p><p>Definition 1. The supervised IB aims at maximizing an Information Bottleneck Lagrangian:</p><formula xml:id="formula_15">IB sup = I(Y, Z X ) I(X, Z X ), where &gt; 0. (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>As we can see, IB sup attempts to maximize the information between the data representation Z X and its corresponding label Y , and concurrently minimize the information between Z X and the input data X (i.e., exploiting compression of Z X from X). The intuition of IB principle is that Z X is expected to contain only the information that is useful for predicting Y .</p><p>Several recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53]</ref> propose various forms of IB under self-supervised settings. The most relevant one names Self-supervised Information Bottleneck: Definition 2. (Self-supervised Information Bottleneck <ref type="bibr" target="#b52">[53]</ref>). The Self-supervised IB aims at maximizing the following Lagrangian: IB ssl = I(X, Z S ) I(S, Z S ), where &gt; 0.</p><p>Intuitively, IB ssl posits that a desirable representation is expected to be informative to augmentation invariant features, and to be a maximally compressed representation of the input.</p><p>Our objective Eq. ( <ref type="formula" target="#formula_6">5</ref>) is essentially an embodiment of IB ssl : Theorem 2. Assume 0 &lt;  1, then by minimizing Eq. ( <ref type="formula" target="#formula_6">5</ref>), the self-supervised Information Bottleneck objective is maximized, formally:</p><formula xml:id="formula_18">min ✓ L ) max ✓ IB ssl<label>(14)</label></formula><p>Theorem 2 also shows that Eq. ( <ref type="formula" target="#formula_6">5</ref>) implicitly follows the same spirit of IB principle under selfsupervised settings. As further enlightenment, we can relate Eq. ( <ref type="formula" target="#formula_6">5</ref>) with the multi-view Information Bottleneck <ref type="bibr" target="#b8">[9]</ref> and the minimal and sufficient representations for self-supervision <ref type="bibr" target="#b44">[45]</ref>: Corollary 1. Let X 1 = S, X 2 = X and assume 0 &lt;  1, then minimizing Eq. ( <ref type="formula" target="#formula_6">5</ref>) is equivalent to minimizing the Multi-view Information Bottleneck Loss in <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_19">L MIB = I(Z 1 , X 1 |X 2 ) I(X 2 , Z 1 ), where 0 &lt;  1. (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>Corollary 2. When the data augmentation process is reversible, minimizing Eq. ( <ref type="formula" target="#formula_6">5</ref>) is equivalent to learning the Minimal and Sufficient Representations for Self-supervision in <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_21">Z ssl X = arg max Z X I(Z X , S), Z sslmin X = arg min Z X H(Z X |S) s.t. I(Z X , S) is maximized. (<label>16</label></formula><formula xml:id="formula_22">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Influence on Downstream Tasks</head><p>We have provided a principled understanding for our new objective. Next, we discuss its effect on downstream tasks T . The rationality of data augmentations in SSL is rooted in a conjecture that an ideal data augmentation approach would not change the information related to its label. We formulate this hypothesis as a building block for analysis on downstream tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9]</ref>. Assumption 2. (Task-relevant information and data augmentation). All the task-relevant information is shared across the input data X and its augmentations S, i.e., I(X, T ) = I(S, T ) = I(X, S, T ), or equivalently, I(X, T |S) = I(S, T |X) = 0.</p><p>This indicates that all the task-relevant information is contained in augmentation invariant features. We proceed to derive the following theorem which reveals the efficacy of the learned representations by our objective with respect to downstream tasks. Theorem 3. (Task-relevant/irrelevant information). By optimizing Eq. (5), the task-relevant information I(Z S , T ) is maximized, and the task-irrelevant information</p><formula xml:id="formula_23">H(Z S |T ) is minimized. Formally, min ✓ L ) max ✓ I(Z S , T ) and min ✓ H(Z S |T ).<label>(17)</label></formula><p>Therefore, the learned representation Z S is expected to contain minimal and sufficient information about downstream tasks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9]</ref>, which further illuminates the reason why the embeddings given by SSL approaches have superior performance on various downstream tasks. 1 Results on Cora with authors' code is inconsistent with <ref type="bibr" target="#b14">[15]</ref>. We adopt the results with authors' code. 2 Results are from our reproducing with authors' code, as <ref type="bibr" target="#b56">[57]</ref> did not use the public splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We assess the quality of representations after self-supervised pretraining on seven node classification benchmarks: Cora, Citeseer, Pubmed, Coauthor CS, Coauthor Physics and Amazon Computer, Amazon-Photo. We adopt the public splits for Cora, Citeseer, Pubmed, and a 1:1:9 training/validation/testing splits for the other 4 datasets. Details of the datasets are in Appendix E.</p><p>Evaluation protocol. We follow the linear evaluation scheme as introduced in <ref type="bibr" target="#b47">[48]</ref>: i) We first train the model on all the nodes in a graph without supervision, by optimizing the objective in Eq. ( <ref type="formula" target="#formula_6">5</ref>). ii) After that, we freeze the parameters of the encoder and obtain all the nodes' embeddings, which are subsequently fed into a linear classifier (i.e., a logistic regression model) to generate a predicted label for each node. In the second stage, only nodes in training set are used for training the classifier, and we report the classification accuracy on testing nodes.</p><p>We implement the model with PyTorch. All experiments are conducted on a NVIDIA V100 GPU with 16 GB memory. We use the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> for both stages. The graph encoder f ✓ is specified as a standard two-layer GCN model <ref type="bibr" target="#b21">[22]</ref> for all the datasets except citeseer (where we empirically find that a one-layer GCN is better). We report the mean accuracy with a standard deviation through 20 random initialization (on Coauthor CS, Coauthor Physics and Amazon Computer, Amazon-Photo, the split is also randomly generated). Detailed hyperparameter settings are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Peer Methods</head><p>We compare CCA-SSG with classical unsupervised models, Deepwalk <ref type="bibr" target="#b31">[32]</ref> and GAE <ref type="bibr" target="#b20">[21]</ref>, and self-supervised models, DGI <ref type="bibr" target="#b47">[48]</ref>, MVGRL <ref type="bibr" target="#b14">[15]</ref>, GRACE <ref type="bibr" target="#b56">[57]</ref> and GCA <ref type="bibr" target="#b57">[58]</ref>. We also compare with supervised learning models, including MLP, Label Propagation (LP) <ref type="bibr" target="#b55">[56]</ref>, and supervised baselines GCN <ref type="bibr" target="#b21">[22]</ref> and GAT <ref type="bibr" target="#b46">[47]</ref> <ref type="foot" target="#foot_0">3</ref> . The results of baselines are quoted from <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> if not specified.</p><p>We report the node classification results of citation networks and other datasets in Table <ref type="table" target="#tab_0">2</ref> and Table <ref type="table" target="#tab_1">3</ref> respectively. As we can see, CCA-SSG outperforms both the unsupervised competitors and the fully supervised baselines on Cora and Pubmed, despite its simple architecture. On Citeseer, CCA-SSG achieves competitive results as of the most powerful baseline MVGRL. On four larger benchmarks, CCA-SSG also achieves the best performance in four datasets except Coauther-Physics. It is worth mentioning that we empirically find that on Coauthor-CS a pure 2-layer-MLP encoder is better than GNN models. This might because the graph-structured information is much less informative than the node features, presumably providing harmful signals for classification (in fact, on Coauthor-CS, linear models using merely node features can greatly outperform DeepWalk/DeepWalk+features). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study and Scalability Comparison</head><p>Effectiveness of invariance/decorrelation terms. We alter our loss by removing the invariance/decorrelation term respectively to study the effects of each component, with results reported in Table <ref type="table" target="#tab_3">4</ref>. We find that only using the invariance term will lead to merely performance drop instead of completely collapsed solutions. This is because node embeddings are normalized along the instance dimension to have a zero-mean and fixed-standard deviation, and the worst solution is no worse than dimensional collapse (i.e., all the embeddings lie in an line, and our decorrelation term can help to prevent it) instead of complete collapse (i.e., all the embeddings degenerate into a single point). As expected, only optimizing the decorrelation term will lead to poor result, as the model learns nothing meaningful but disentangled representation. In Appendix B we discuss the relationship between complete/dimensional collapse, when the two cases happen and how to avoid them.</p><p>Effect of decorrelation intensity. We study how the intensity of feature decorrelation improves/degrades the performance by increasing the trade-off hyper-parameter . Fig. <ref type="figure" target="#fig_1">2</ref> shows test accuracy w.r.t. different 's on Cora, Citeseer and Pubmed. The performance benefits from a proper selection of (from 0.0005 to 0.001 in our experiments). When is too small, the decorrelation term does not work; if it is too large, the invariance term would be neglected, leading to serious performance degrade. An interesting finding is that even when is very small or even equals to 0 (w/o L dec in Table <ref type="table" target="#tab_3">4</ref>), the test accuracy on Citeseer does not degrade as much as that on Cora and Citeseer.</p><p>The reason is that node embeddings of Citeseer is already highly uncorrelated even without the decorrelation term. Appendix F visualizes the correlation matrices without/with decorrelations.</p><p>Effect of embedding dimension. Fig. <ref type="figure" target="#fig_2">3</ref> shows the effect of the embedding dimension. Similar to contrastive methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, CCA-SSG benefits from a large embedding dimension (compared with supervised learning), while the optimal embedding dimension of CCA-SSG (512 on most benchmarks) is a bit larger than other methods (usually 128 or 256). Yet, we notice a performance drop as the embedding dimension increases. We conjecture that the CCA is essentially a dimension-reduction method, the ideal embedding dimension ought to be smaller than the dimension of input. Hence we do not apply it on well-compressed datasets (e.g. ogbn-arXiv and ogbn-product).     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability Comparison.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussions</head><p>In this paper, we have introduced CCA-SSG, a conceptually simple, efficient yet effective method for self-supervised representation learning on graphs, based on the idea of Canonical Correlation Analysis. Compared with contrastive methods, our model does not require additional components except random augmentations and a GNN encoder, whose effectiveness is justified in experiments.</p><p>Limitations of the work. Despite the theoretical grounds and the promising experimental justifications, our method would suffer from several limitations. 1) The objective Eq. ( <ref type="formula" target="#formula_6">5</ref>) is essentially performing dimension reduction, while SSL approach usually requires a large embedding dimension. As a result, our method might not work well on datasets where input data does not have a large feature dimension. 2) Like other augmentation based methods, CCA-SSG highly relies on a high-quality, informative and especially, label-invariant augmentations. However, the augmentations used in our model might not perfectly meet these requirements, and it remains an open problem how to generate informative graph augmentations that have non-negative impacts on the downstream tasks.</p><p>Potential negative societal impacts. This work explores a simple pipeline for representation learning without large amount of labeled data. However, in industry there are many career workers whose responsibility is to label or annotate data. The proposed method might reduce the need for labeling data manually, and thus makes a few individuals unemployed (especially for developing countries and remote areas). Furthermore, our model might be biased, as it tends to pay more attention to the majority and dominant features (shared information across most of the data). The minority group whose features are scare are likely to be downplayed by the algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of the proposed model: given an input graph, we first generate two views through random augmentations: edge dropping and node feature masking. The two views are subsequently put into a shared GNN encoder to generate representations. The loss function is applied on the column-normalized embedding matrix of the two views. Note that this simple yet effective pipeline can also be conceptually applied for other data like vision and texts, which we leave for future works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of .Figure 3: Effect of D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: Effect of .Figure 3: Effect of D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy on citation networks. The input column highlights the data used for training. (X for node features, A for adjacency matrix, S for diffusion matrix, and Y for node labels).</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Input</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell></cell><cell>MLP [47]</cell><cell>X, Y</cell><cell>55.1</cell><cell>46.5</cell><cell>71.4</cell></row><row><cell>Supervised</cell><cell>LP [56] GCN [22]</cell><cell>A, Y X, A, Y</cell><cell>68.0 81.5</cell><cell>45.3 70.3</cell><cell>63.0 79.0</cell></row><row><cell></cell><cell>GAT [47]</cell><cell>X, A, Y</cell><cell>83.0 ± 0.7</cell><cell>72.5 ± 0.7</cell><cell>79.0 ± 0.3</cell></row><row><cell>Unsupervised</cell><cell>Raw Features [48] Linear CCA [18] DeepWalk [32] GAE [21] DGI [48] MVGRL 1 [15] GRACE 2 [57] CCA-SSG (Ours)</cell><cell>X X A X, A X, A X, S, A X, A X, A</cell><cell>47.9 ± 0.4 58.9 ± 1.5 70.7 ± 0.6 71.5 ± 0.4 82.3 ± 0.6 83.5 ± 0.4 81.9 ± 0.4 84.2 ± 0.4</cell><cell>49.3 ± 0.2 27.5 ± 1.3 51.4 ± 0.5 65.8 ± 0.4 71.8 ± 0.7 73.3 ± 0.5 71.2 ± 0.5 73.1 ± 0.3</cell><cell>69.1 ± 0.3 75.8 ± 0.4 74.3 ± 0.9 72.1 ± 0.5 76.8 ± 0.6 80.1 ± 0.7 80.6 ± 0.4 81.6 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on co-author and co-purchase networks. We report both mean accuracy and standard deviation. Results of baseline models are from<ref type="bibr" target="#b57">[58]</ref>. X, A, Y 86.51 ± 0.54 92.42 ± 0.22 93.03 ± 0.31 95.65 ± 0.16 Supervised GAT [47] X, A, Y 86.93 ± 0.29 92.56 ± 0.35 92.31 ± 0.24 95.47 ± 0.15 GCA is essentially an enhanced version of GRACE by adopting adaptive augmentations. Both GRACE and GCA would suffer from out of memory on Coauthor-Physics using a GPU wth 16GB memory. The reported results are from authors' papers using a 32GB GPU.</figDesc><table><row><cell>Methods</cell><cell>Input</cell><cell>Computer</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>Supervised GCN [22] Unsupervised Raw Features [48] Linear CCA [18] DeepWalk [32] DeepWalk + features GAE [21] DGI [48] MVGRL [15] GRACE 1 [57] GCA 1 [58] CCA-SSG (Ours)</cell><cell cols="5">X X A X, A X, A X, A X, S, A 87.52 ± 0.11 91.74 ± 0.07 92.11 ± 0.12 95.33 ± 0.03 73.81 ± 0.00 78.53 ± 0.00 90.37 ± 0.00 93.58 ± 0.00 79.84 ± 0.53 86.92 ± 0.72 93.13 ± 0.18 95.04 ± 0.17 85.68 ± 0.06 89.44 ± 0.11 84.61 ± 0.22 91.77 ± 0.15 86.28 ± 0.07 90.05 ± 0.08 87.70 ± 0.04 94.90 ± 0.09 85.27 ± 0.19 91.62 ± 0.13 90.01 ± 0.71 94.92 ± 0.07 83.95 ± 0.47 91.61 ± 0.22 92.15 ± 0.63 94.51 ± 0.52 X, A 86.25 ± 0.25 92.15 ± 0.24 92.93 ± 0.01 95.26 ± 0.02 X, A 87.85 ± 0.31 92.49 ± 0.09 93.10 ± 0.01 95.68 ± 0.05 X, A 88.74 ± 0.28 93.14 ± 0.14 93.31 ± 0.22 95.38 ± 0.06</cell></row></table><note>1 </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 5 compares model size, training time (till the epoch that gives the highest evaluation accuracy) and memory cost of CCA-SSG with other methods, on Cora, Pubmed and Amazon-Computers. Overall, our method has fewer parameters, shorter training time, and fewer memory cost than MVGRL, GRACE and GCA in most cases. DGI is another simple and efficient model, but it yields much poorer performance. The results show that despite its simplicity and efficiency, our method achieves even better (or competitive) performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of node classification accuracy (%) on the key components of CCA-SSG.</figDesc><table><row><cell cols="2">Variants Cora Citeseer Pubmed</cell></row><row><cell>Baseline 84.2 73.1</cell><cell>81.6</cell></row><row><cell>w/o L dec 79.1 72.2 w/o Linv 40.1 28.9</cell><cell>75.3 46.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the number of parameters, training time for achieving the best performance, and the memory cost of different methods on Cora, Pubmed and Amazon-Computer. MVGRL on Pubmed and Computer requires subgraph sampling with graph size 4000. Others are full-graph.</figDesc><table><row><cell>Methods</cell><cell cols="6">Cora (N : 2,708) #Paras Time Mem #Paras Time Pubmed (N : 19,717) Mem</cell><cell cols="3">Computer (N : 13,752) #Paras Time Mem</cell></row><row><cell>DGI</cell><cell>1260K</cell><cell>6.4s</cell><cell>1.4G</cell><cell>782K</cell><cell>5.9s</cell><cell>1.9G</cell><cell>919K</cell><cell cols="2">14.1s 1.9G</cell></row><row><cell>MVGRL</cell><cell cols="3">1731K 26.9s 4.6G</cell><cell>775K</cell><cell>29s</cell><cell>5.4G</cell><cell cols="3">1049K 31.5s 5.5G</cell></row><row><cell>GRACE/GCA</cell><cell>997K</cell><cell>8.3s</cell><cell>1.7G</cell><cell>520K</cell><cell cols="2">756s 12.6G</cell><cell>273K</cell><cell>314s</cell><cell>7.6G</cell></row><row><cell>CCA-SSG(Ours)</cell><cell>997K</cell><cell>3.8s</cell><cell>1.6G</cell><cell>519K</cell><cell>9.6s</cell><cell>2.7G</cell><cell>656K</cell><cell cols="2">14.8s 2.5G</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">The BGRL<ref type="bibr" target="#b38">[39]</ref> is not compared as its source code has not been released.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941. Qitian Wu and Junchi Yan were partly supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102). We thank Amazon Web Services for sponsoring computation resources for this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations for neural network-based classification using the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Rana</forename><surname>Ali Amjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">C</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2225" to="2239" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable and effective deep CCA via soft decorrelation</title>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1488" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, Proceedings of Machine Learning Research</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Whitening for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06346</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning robust representations via multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The canonical correlations of matrix pairs and their numerical computation. Linear algebra for signal processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Ávila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sándor</forename><surname>Szedmák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khas</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="322" to="377" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On feature decorrelation in self-supervised learning</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sucheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00470</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dezhong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Contrastive clustering. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscalecontrastive siamese networks for self-supervised graph representation learning</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yizhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan-Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Shirui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised graphlevelrepresentation learning with local and global structure</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Minghao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Bingbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Hongyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GCC: graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An overview of microsoft academic service (MAS) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An information theoretic framework for multi-view learning</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLR</title>
				<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="403" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The deterministic information bottleneck</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Dj Strouse</surname></persName>
		</author>
		<author>
			<persName><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Petar Velickovic, and Michal Valko. Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06514</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Understanding self-supervised learning dynamics without contrastive pairs</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06810</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding self-supervised learning with dual deep networks</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00578</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Selfsupervised learning from a multi-view perspective</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv, 1909.01315</idno>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Barlow twins: Selfsupervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Contrastive self-supervised learning for graph classification</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05923</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12609</idno>
		<title level="m">Iterative graph self-distillation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
