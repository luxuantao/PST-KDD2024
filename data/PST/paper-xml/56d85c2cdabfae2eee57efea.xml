<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Subset Selection Approach by Gray-Wolf Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">E</forename><surname>Emary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Cairo University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Scientific Research Group in Egypt (SRGE)</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hossam</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Babes-Bolyai University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Beni-Suef University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Scientific Research Group in Egypt (SRGE)</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Crina</forename><surname>Grosan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Babes-Bolyai University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abul</forename><forename type="middle">Ella</forename><surname>Hassenian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Cairo University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Scientific Research Group in Egypt (SRGE)</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Afro-European Conf. for Ind. Advancement</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Subset Selection Approach by Gray-Wolf Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0108F577D86C11D20656F5C05CCF086A</idno>
					<idno type="DOI">10.1007/978-3-319-13572-4_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gray-wolf Optimization</term>
					<term>feature selection</term>
					<term>evolutionary computation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection algorithm explores the data to eliminate noisy, irrelevant, redundant data, and simultaneously optimize the classification performance. In this paper, a classification accuracy-based fitness function is proposed by gray-wolf optimizer to find optimal feature subset. Gray-wolf optimizer is a new evolutionary computation technique which mimics the leadership hierarchy and hunting mechanism of gray wolves in nature. The aim of the gray wolf optimization is find optimal regions of the complex search space through the interaction of individuals in the population. Compared with particle swarm optimization (PSP) and Genetic Algorithms (GA) over a set of UCI machine learning data repository, the proposed approach proves better performance in both classification accuracy and feature size reduction. Moreover, the gray wolf optimization approach proves much robustness against initialization in comparison with PSO and GA optimizers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Feature selection algorithm explores the data to eliminate noisy, irrelevant, redundant data, and simultaneously optimize the classification performance. Feature selection is one of the most important stage in data mining, multimedia information retrieval, pattern classification, and machine learning applications, which can influence the classification accuracy rate <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>The main purpose of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features <ref type="bibr" target="#b2">[3]</ref>. In real world problems, feature selection is a must due to the abundance of noisy, misleading or irrelevant features <ref type="bibr" target="#b3">[4]</ref>. By removing these factors, learning from data techniques can useful greatly. The motivation of feature selection in data mining, machine learning and pattern recognition is to reduce the dimensionality of feature space, improve the predictive accuracy of a classification algorithm, and develop the visualization and the comprehensibility of the induced concepts <ref type="bibr" target="#b4">[5]</ref>.</p><p>Genetic Algorithm (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are popular meta-heuristic optimization techniques. The Grey Wolf Optimizer (GWO) is a new optimization algorithm which simulate the grey wolves leadership and hunting manner in nature. These techniques have been inspired by simple concepts.The inspirations are typically related to physical phenomena, animals behaviors, or evolutionary concepts <ref type="bibr" target="#b5">[6]</ref>. In recent years, a lot of feature selection methods have been proposed. There are two key issues in structure a feature selection method: search strategies and evaluating measures. With respect to search strategies, complete , heuristic <ref type="bibr" target="#b6">[7]</ref> , random <ref type="bibr" target="#b7">[8]</ref> [9] strategies were proposed. And with respect to evaluating measures, these methods can be nearly divided into two classes: classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and classification independent <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The previous employs a learning algorithm to evaluate the quality of selected features based on the classification accuracies or contribution to the classification boundary, such as the so-called wrapper method <ref type="bibr" target="#b9">[10]</ref> and weight based algorithms <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. While the latter constructs a classifier independent measure to evaluate the importance of features, such as interclass distance <ref type="bibr" target="#b12">[13]</ref> mutual information <ref type="bibr" target="#b17">[18]</ref>, dependence measure <ref type="bibr" target="#b13">[14]</ref> and consistency measure <ref type="bibr" target="#b14">[15]</ref>.</p><p>In recent years, a lot of feature selection methods have been proposed. There are two key issues in structure a feature selection method: search strategies and evaluating measures. With respect to search strategies, complete , heuristic <ref type="bibr" target="#b6">[7]</ref> , random <ref type="bibr" target="#b7">[8]</ref> [9] strategies were proposed. And with respect to evaluating measures, these methods can be nearly divided into two classes: classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and classification independent <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The previous employs a learning algorithm to evaluate the quality of selected features based on the classification accuracies or contribution to the classification boundary, such as the so-called wrapper method <ref type="bibr" target="#b9">[10]</ref> and weight based algorithms <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> . While the latter constructs a classifier independent measure to evaluate the importance of features, such as inter-class distance <ref type="bibr" target="#b12">[13]</ref> mutual information <ref type="bibr" target="#b17">[18]</ref>, dependence measure <ref type="bibr" target="#b13">[14]</ref> and consistency measure <ref type="bibr" target="#b14">[15]</ref>.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, the population of particles split into a set of interacting swarms. These interacting swarms applied the simple competition method. The winner is the swarm which has a best fitness value. The loser is eject and re-initialized in the search space, otherwise the winner remains. In <ref type="bibr" target="#b22">[23]</ref>, the swarm population divided into sub-populations species based on their similarity. Then, the repeated particles are removed when particles are identified as having the same fitness. After destroying the repeated ones, the new particles are added randomly until its size is resumed to its initial size.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, the Bat Algorithm (BA) based on type of the sonar, which named echolocation behavior. The micro-bats have the capability of echolocation which attracting these bats can find their prey and discriminate different types of insects even in complete darkness.</p><p>In this paper, a classification accuracy-based fitness function is proposed by graywolf optimizer to find optimal feature subset. We compare Grey Wolf Optimizer (GWO) algorithm against Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) algorithms for feature selection by applying three different initialization methods and eight different datasets. The results reveal that the GWO resulted in a higher accuracy compared to the other two optimization algorithms.</p><p>The rest of this paper is organized as follows: Section 2 presents basics of the gray wolf optimization. Section IV presents the details of the proposed system. In section V, there are experimental results and result analysis. Finally in Section VI, conclusions and future work are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gray Wolf Optimization</head><p>Gray wolf optimization is presented in the following subsections based on the work in <ref type="bibr" target="#b24">[25]</ref>.</p><p>Inspiration. Grey wolves are considered as apex predators, meaning that they are at the top of the food chain. Grey wolves mostly prefer to live in a pack. The group size is 5-12 on average. They have a very strict social dominant hierarchy. The leaders are a male and a female, called al pha. The alpha is mostly responsible for making decisions about hunting, sleeping place, time to wake, and so on. The al pha decisions are dictated to the pack. The second level in the hierarchy of grey wolves is beta. The betas are subordinate wolves that help the alpha in decision-making or other pack activities. The beta wolf can be either male or female, and he/she is probably the best candidate to be the alpha in case one of the alpha wolves passes away or becomes very old. The lowest ranking grey wolf is omega. The omega plays the role of scapegoat. Omega wolves always have to submit to all the other dominant wolves. They are the last wolves that are allowed to eat. The fourth class is called subordinate (or delta in some references). Delta wolves have to submit to alphas and betas, but they dominate the omega. Scouts, sentinels, elders, hunters, and caretakers belong to this category. Scouts are responsible for watching the boundaries of the territory and warning the pack in case of any danger. Sentinels protect and guarantee the safety of the pack. Elders are the experienced wolves who used to be alpha or beta. Hunters help the alphas and betas when hunting prey and providing food for the pack. Finally, the caretakers are responsible for caring for the weak, ill, and wounded wolves in the pack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Model.</head><p>In the mathematical model for the GWO the fittest solution is called the alpha (α). The second and third best solutions are named beta (β ) and delta (δ ) respectively. The rest of the candidate solutions are assumed to be omega (ω). The hunting is guided by α, β , and δ and the ω follow these three candidates. In order for the pack to hunt a prey they first encircling it. In order to mathematically model encircling behavior the following equations are used 1:</p><formula xml:id="formula_0">- → X (t + 1) = -→ X p (t) + - → A . -→ D (1)</formula><p>where -→ D is as defined in 2 and t is the iteration number, -→ A , -→ C are coefficient vectors, -→ X p is the prey position and -→ X is the gray wolf position.</p><formula xml:id="formula_1">-→ D = | -→ C . -→ X p (t) - - → X (t)|<label>(2)</label></formula><p>The -→ A , -→ C vectors are calculated as in equations 3 and 4</p><formula xml:id="formula_2">-→ A = 2 -→ A . -→ r 1 -- → a (3) -→ C = 2 -→ r 2<label>(4)</label></formula><p>where components of -→ a are linearly decreased from 2 to 0 over the course of iterations and r 1 , r 2 are random vectors in [0, 1]. The hunt is usually guided by the alpha. The beta and delta might also participate in hunting occasionally. In order to mathematically simulate the hunting behavior of grey wolves, the alpha (best candidate solution) beta, and delta are assumed to have better knowledge about the potential location of prey. The first three best solutions obtained so far and oblige the other search agents (including the omegas) to update their positions according to the position of the best search agents. So, the updating for the wolves positions is as in equations 5,6,7.</p><p>-→</p><formula xml:id="formula_3">D α = | -→ C 1 . -→ X α - - → X |, -→ D β = | -→ C 2 . -→ X β - - → X |, -→ D δ = | -→ C 3 . -→ X δ - - → X | (5) -→ X 1 = | -→ X α - -→ A 1 . -→ D α |, -→ X 2 = | -→ X β - -→ A 2 . -→ D β |, -→ X 3 = | -→ X δ - -→ A 3 . -→ D δ | (6) -→ X (t + 1) = -→ X 1 + -→ X 2 + -→ X 3 3<label>(7)</label></formula><p>A final note about the GWO is the updating of the parameter -→ a that controls the tradeoff between exploation and exploitation. The parameter -→ a is linearly updated in each iteration to range from 2 to 0 according to the equation 8:</p><formula xml:id="formula_4">-→ a = 2 -t. 2 Max i ter (<label>8</label></formula><formula xml:id="formula_5">)</formula><p>where t is the iteration number and Max i ter is the total number of iteration allowed for the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Algorithm</head><p>In this section, we present the proposed GWO optimizer based on K-nearest neighbor for feature selection; see figure <ref type="figure" target="#fig_0">1</ref>. We used the principles of gray wolf optimization for the optimal feature selection problem. Each feature subset can be seen as a position in such a space. If there are N total features, then there will be 2 N different feature subset, different from each other in the length and features included in each subset. The optimal position is the subset with least length and highest classification accuracy. We used gray wolf optimization for selecting the optimal feature set. Eventually, they should converge on good, possibly optimal, positions. The GWO makes iterations of exploration of new regions in the feature space and exploiting solution until reaching near-optimal solution. The solution space in here represents all possible selections of features and hence bat positions represents binary selection of feature sets. Each feature is considered as an individual dimension ranging from -2 to 2. To decide if a feature will be selected or not its position value will be threshold with a constant threshold. The used fitness function is the classification accuracy for k-nearest neighbors (KNN) classifier on the validation set. Each individual data set is divided into three equal subsets namely training, validation and test portions. The training set and validation set are used inside the fitness function to evaluate the selection classification accuracy while the test set is used in the end of optimization to evaluate the final selection classification performance.</p><p>We made use of two fitness functions in gray-wolf optimization (GWO) for feature selection, which are KNN, andKNN s ize resembling the well-known forward selection. Forward selection starts with an empty feature set (no features) and searches for a feature subset(s) with one feature by selecting the feature that achieves the highest classification performance. Then the algorithm selects another feature from the candidate features to add to S. Feature i is selected if adding i to S achieves the largest improvement in classification accuracy. While, backward selection starts with all the available features, then candidate features are sequentially removed from the feature subset until the further removal of any feature does not increase the classification performance. Small initialization resembles forward selection, large initialization motivated by backward selection and mixed initialization aiming to take the advantages of forward and backward selection to avoid their disadvantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets and Parameters Setting</head><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the 8 used data set for further experiments. The data set are drawn from the UCI data repository <ref type="bibr" target="#b26">[27]</ref>. The data is divided into 3 equal parts one for training, the second part is for validation and the third part is for testing. We implement the GWO feature selection algorithms in MatLab R2009a. The computer used to get results is Intel (R), 2.1 GHz CPU; 2 MB RAM and the system is Windows 7 Professional. The parameter setting for the GWO algorithm is outlined in table 2. Same number of agents and same number of iterations are used for GA and PSO. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Four scenarios has been considered when we evaluate the proposed approach. They are: (1) Scenario 1: GWO, GA, and PSO features selection techniques using normal initialization, (2) Scenario 2: GWO, GA, and PSO features selection techniques using large initialization, (3) Scenario 3: GWO, GA, and PSO features selection techniques using mixed initialization, and (4) Scenario 4: GWO, GA, and PSO features selection techniques using small initialization.</p><p>Tables <ref type="table" target="#tab_2">3,</ref><ref type="table" target="#tab_3">4</ref>, 5, and 6 are showing the performance of GWO, GA, PSO algorithms on the different eight data sets. Every algorithm is applied for 5 times on every data set to be sure about the algorithm robustness and we display the average result of all solutions. Gray wolf optimization (GWO) algorithm achieves high accuracy with the different data sets and initialization methods as showing in figures 2, 3, 4, and 5.</p><p>This demonstrates that GWO shows a good balance between exploration and exploitation that results in high local optima avoidance. This superior capability is due to the adaptive value of A. As mentioned above, half of the iterations are devoted to exploration (|A| ≥ 1) and the rest to exploitation (|A| &lt; 1). This mechanism assists GWO to provide very good exploration, local minima avoidance, and exploitation simultaneously.  present the standard deviation for the obtained fitness functions after running the each optimizer for 5 runs. The obtained standard deviation for GWO is much less than the obtained standard deviation for the PSO and GA which can be considered a prove for algorithm robustness regardless of the initialization method. SO, GWO always converge to the optimal solution or near optimal one regardless of its initialization method. GWO has abrupt changes in the movement of search agents over the initial steps of optimization. This assists a meta-heuristic to explore the search space extensively. Then, these changes should be reduced to emphasize exploitation at the end of optimization. In order to observe the convergence behavior of the GWO algorithm. The exploration power of GWO allow it to tolerate for initial solution as it quickly explore many regions and select the promising ones for further search.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, a system for feature selection based on intelligent search of gray wolf optimization has be proposed. Compared with PSO and GA over a set of UCI machine learning data repository, GWO proves much better performance as well as its proves much robustness and fast convergence speed. Moreover, the gray wolf optimization approach proves much robustness against initialization in comparison with PSO and GA optimizers. Other improvement to our work may involve applying some other feature selection algorithms and different fitness functions in the future which are expected to further enhance the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall feature selection algorithm</figDesc><graphic coords="5,105.90,56.91,226.48,226.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Comparison of normal initialization results for GWO, GA, PSO with different datasets</figDesc><graphic coords="8,75.30,65.25,287.86,172.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison curve of small initialization results for GWO, GA, PSO with different datasets</figDesc><graphic coords="10,75.30,234.69,287.86,172.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Standard deviation obtained for running the different methods on the different data sets [initialization normal], (b) Standard deviation obtained for running the different methods on the different data sets [initialization large] (c) Standard deviation obtained for running the different methods on the different data sets [initialization mixed] and (d) Standard deviation obtained for running the different methods on the different data sets [initialization small]</figDesc><graphic coords="11,219.30,180.15,165.37,122.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,75.30,207.09,287.86,186.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Description of the data sets used in experiments</figDesc><table><row><cell>Dataset</cell><cell cols="2">No. of features No. of samples</cell></row><row><cell>Lymphography</cell><cell>18</cell><cell>148</cell></row><row><cell>Zoo</cell><cell>16</cell><cell>101</cell></row><row><cell>Vote</cell><cell>16</cell><cell>300</cell></row><row><cell>Breastcancer</cell><cell>9</cell><cell>699</cell></row><row><cell>M-of-N</cell><cell>13</cell><cell>1000</cell></row><row><cell>Exactly</cell><cell>13</cell><cell>1000</cell></row><row><cell>Exactly2</cell><cell>13</cell><cell>1000</cell></row><row><cell>Tic-tac-toe</cell><cell>9</cell><cell>958</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Parameter setting for gray-wolf optimization</figDesc><table><row><cell>parameter</cell><cell>value(s)</cell></row><row><cell>No of wolves</cell><cell>5</cell></row><row><cell>No of iterations</cell><cell>100</cell></row><row><cell cols="2">problem dimension same as number of features in any given database</cell></row><row><cell>Search domain</cell><cell>[0 1]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Normal initialization results for different datasets</figDesc><table><row><cell cols="2">Dataset No. GWO</cell><cell>GA</cell><cell>PSO</cell></row><row><cell>1</cell><cell cols="3">0.980952 0.976190 0.985714</cell></row><row><cell>2</cell><cell cols="3">0.733333 0.970000 1.000000</cell></row><row><cell>3</cell><cell cols="3">0.773333 0.766667 0.740000</cell></row><row><cell>4</cell><cell cols="3">0.863636 0.886364 0.886364</cell></row><row><cell>5</cell><cell cols="3">1.000000 0.943333 0.946667</cell></row><row><cell>6</cell><cell cols="3">0.797909 0.797909 0.808362</cell></row><row><cell>7</cell><cell cols="3">0.977778 0.966667 0.944444</cell></row><row><cell>8</cell><cell cols="3">1.000000 0.933333 1.000000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Large initialization results for different datasets</figDesc><table><row><cell cols="2">Dataset No. GWO</cell><cell>GA</cell><cell>PSO</cell></row><row><cell>1</cell><cell cols="3">0.976190 0.990476 0.985714</cell></row><row><cell>2</cell><cell cols="3">1.000000 0.760000 0.723333</cell></row><row><cell>3</cell><cell cols="3">0.773333 0.756667 0.780000</cell></row><row><cell>4</cell><cell cols="3">0.886364 0.863636 0.795455</cell></row><row><cell>5</cell><cell cols="3">1.000000 0.920000 0.883333</cell></row><row><cell>6</cell><cell cols="3">0.839721 0.843206 0.832753</cell></row><row><cell>7</cell><cell cols="3">0.966667 0.966667 0.966667</cell></row><row><cell>8</cell><cell cols="3">1.000000 0.966667 0.933333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Mixed initialization results for different datasets</figDesc><table><row><cell cols="2">Dataset No. GWO</cell><cell>GA</cell><cell>PSO</cell></row><row><cell>1</cell><cell cols="3">0.980952 0.985714 0.976190</cell></row><row><cell>2</cell><cell cols="3">1.000000 0.733333 0.773333</cell></row><row><cell>3</cell><cell cols="3">0.756667 0.736667 0.740000</cell></row><row><cell>4</cell><cell cols="3">0.818182 0.772727 0.772727</cell></row><row><cell>5</cell><cell cols="3">1.000000 0.966667 0.956667</cell></row><row><cell>6</cell><cell cols="3">0.832753 0.846690 0.850174</cell></row><row><cell>7</cell><cell cols="3">0.966667 0.944444 0.933333</cell></row><row><cell>8</cell><cell cols="3">0.966667 0.966667 0.900000</cell></row></table><note><p>Fig. 4. Comparison curve of mixed initialization results for GWO, GA, PSO with different datasets</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Small initialization results for different datasets</figDesc><table><row><cell cols="2">Dataset No. GWO</cell><cell>GA</cell><cell>PSO</cell></row><row><cell>1</cell><cell cols="3">0.980952 0.966667 0.985714</cell></row><row><cell>2</cell><cell>1</cell><cell cols="2">0.86 0.753333</cell></row><row><cell>3</cell><cell cols="3">0.743333 0.773333 0.726667</cell></row><row><cell>4</cell><cell cols="3">0.863636 0.840909 0.863636</cell></row><row><cell>5</cell><cell>1</cell><cell cols="2">0.986667 0.94</cell></row><row><cell>6</cell><cell cols="3">0.850174 0.860627 0.84669</cell></row><row><cell>7</cell><cell cols="3">0.977778 0.955556 0.944444</cell></row><row><cell>8</cell><cell cols="3">0.966667 0.933333 0.966667</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was partially supported by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme FP7/2007-2013/ under REA grant agreement No. 316555. This fund only apply for two co-authors (Hossam M. Zawbaa and Crina Grosan).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction using GA-PSO</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Information Sciences (JCIS)</title>
		<meeting>the Joint Conference on Information Sciences (JCIS)<address><addrLine>Kaohsiung</addrLine></address></meeting>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2006-11">October 8-11. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A framework for feature selection in high-dimensional domains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Cannas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Cagliari</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature selection for Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Combining rough and fuzzy sets for feature selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Feature Selection for Knowledge Discovery and Data Mining</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grey Wolf Optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Softw</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using rough sets with heuristics for feature selection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inform. Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="199" to="214" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimensionality reduction using genetic algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Raymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random subspace method for multivariate feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wessels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1067" to="1076" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature subset selection using the wrapper method, Overfitting and dynamic search space topology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Fall Symposium on Relevance</title>
		<meeting>AAAI Fall Symposium on Relevance</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eliminating redundancy and irrelevance using a new MLP-based feature selection method</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="315" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combined SVM-based feature selection and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The feature selection problem: Traditional methods and a new algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI 1992</title>
		<meeting>AAAI 1992<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="129" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection using rough sets theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Modrzejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning</title>
		<meeting>the European Conference on Machine Learning<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistency-based search in feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved feature selection algorithm based on SVM and correlation</title>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISNN 2006</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Żurada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3971</biblScope>
			<biblScope unit="page" from="1373" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for knowledge discovery and data mining</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Entropy Measures, Maximum Entropy and Emerging Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="115" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rough set based classication methods and extended decision tables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Deogun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Workshop on Rough Sets and Soft Computing</title>
		<meeting>of the Int. Workshop on Rough Sets and Soft Computing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A rough sets based approach to feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Annual Meeting of the Fuzzy Information, Processing NAFIPS 2004</title>
		<imprint>
			<date type="published" when="2004">June 27-30. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge discovery in databases: an attribute-oriented rough set approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Regina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiswarms, &quot;exclusion, and anti-convergence in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="459" to="472" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locating and tracking multiple dynamic optima by a particle swarm model using speciation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="440" to="458" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A New Metaheuristic Bat-Inspired Algorithm</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NICSO 2010</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>González</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pelta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cruz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Terrazas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Krasnogor</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grey Wolf Optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Engineering Software</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Particle swarm optimisation for feature selection in classification:Novel initialisation and updating mechanisms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="261" to="276" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>UCI Machine Learning Repository. University of California, School of Information and Computer Science, Irvine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
