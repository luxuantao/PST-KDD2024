<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gossip Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
							<email>devavrat@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gossip Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8678FAE5B67E6B766A03304BDF83A99D</idno>
					<idno type="DOI">10.1561/1300000014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unlike the Telephone network or the Internet, many of the next generation networks are not engineered for the purpose of providing efficient communication between various networked entities. Examples abound: sensor networks, peer-to-peer networks, mobile networks of vehicles and social networks. Indeed, these emerging networks do require algorithms for communication, computation, or merely spreading information. For example, estimation algorithms in sensor networks, broadcasting news through a peer-to-peer network, or viral advertising in a social network. These networks lack infrastructure; they exhibit unpredictable dynamics and they face stringent resource constraints. Therefore, algorithms operating within them need to be extremely simple, distributed, robust against networks dynamics, and efficient in resource utilization.</p><p>Gossip algorithms, as the name suggests, are built upon a gossip or rumor style unreliable, asynchronous information exchange protocol. Due to their immense simplicity and wide applicability, this class of algorithms has emerged as a canonical architectural solution for the next generation networks. This has led to exciting recent progress to understand the applicability as well as limitations of the Gossip algorithms. In this review, we provide a systematic survey of many of these recent results on Gossip network algorithms. The algorithmic results described here utilize interdisciplinary tools from Markov chain theory, Optimization, Percolation, Random graphs, Spectral graph theory, and Coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries 2.1 Graphs and Random Walks</head><p>We will introduce notations, definitions and some known results that will be useful throughout the paper. We start with very basic notions. An object that will play central role is the graph. A graph consists of some finite number, say n, of nodes, which will be numbered and represented as a vertex set V = {1, . . . , n}; edges representing connections between the nodes are denoted by E ⊂ V × V . Thus, a graph denoted by G will be defined by sets V and E and will be represented as G = (V, E). We will assume a graph to be undirected, i.e., if (i, j) ∈ E then (j, i) ∈ E as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The twentieth century has seen a revolution in terms of our ability to communicate at very long distances at very high speeds. This has fundamentally changed the way we live in the present world. The development of reliable and high-performance massive communication networks has been at the heart of this revolution. The telephone networks and the Internet are prime examples of such large networks. These networks were carefully engineered (and are still being engineered) for the single purpose of providing efficient communication given the available resources. In contrast to these networks, there has been a sudden emergence of different types of large networks in the past few years where the primary purpose is not that of providing communication. Examples of such networks include sensor networks, peer-to-peer (P2P) networks, mobile ad-hoc networks, and social networks.</p><p>A sensor network, made of a large number of unreliable cheap sensors, is usually deployed for the purpose of 'sensing', 'detecting' or 'monitoring' certain events. For example, smoke sensors capable of wireless transmission deployed for smoke detection in a large building, or a collection of interconnected camera sensors deployed for surveillance in a secure facility. The ability to deploy such networks anywhere with minimal cost of infrastucture has made them particularly attractive for these applications. Clearly, the primary purpose of such networks is to collect and process the sensed information by sensors rather than provide efficient communication.</p><p>The peer-to-peer networks are formed by connecting various users (e.g., computers or handheld devices) over an already existing network such as the Internet. Usually such networks are formed with minimal infrastructural support. The peers (or neighbors) are connected over an existing network and hence the advantage of using such networks is not in terms of efficiency of utilizing resources. However, a significant benefit arises in terms of reduced infrastructural support in situations like wide information dissemination. For example, in the absence of a P2P network an Internet content provider (e.g., BBC) needs to maintain a high bandwidth 'server farm' that 'streams' a popular movie or a TV show to a large number of users simultaneously. In contrast, in the presence of a P2P network a user is likely to obtain the desired popular content from a 'nearby' peer and thus distributing a large cost of 'streaming' from the 'server farm' to many 'peers'. Therefore, such an architecture can reduce the cost of content dissemination for a content provider drastically. Of course, it is likely to come at an increased cost of the network utilization. Now, whether or not the benefits obtained in terms of reduced infrastructure by utilizing P2P network for a content provider offset the increased network cost incurred by the network provider is indeed intriguing both in an engineering and an economic sense. While the recent trend suggests that it is indeed the case (e.g., advent of the BBCiPlayer [70] and adaptation of Korean ISPs <ref type="bibr">[31]</ref>), the equilibrium solution is yet to be reached.</p><p>The mobile ad-hoc network formed between vehicles arises in various scenarios, including future smart cars traveling on road, or fleets of unmanned aerial vehicles deployed for surveillance. These networks, by design, are formed for a purpose other than communication. They need algorithms for the purpose of co-ordination, consensus or flocking (e.g., see classical work by Tsitsiklis <ref type="bibr" target="#b67">[69]</ref>, more recently <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b61">63]</ref>).</p><p>Finally, we have noticed a very recent emergence of massive social networks between individuals connected over a heterogenous collection of networks. Until recently, an individual's social network usually involved only a small number of other acquintances, relatives or close friends. However, the arrival of 'social network applications' (e.g., Orkut, Facebook, etc.) has totally changed the structure of existing social networks. Specifically, the social network of an individual now includes many more acquintances than before thanks to these online applications. Furthermore, the use of handheld devices like smart phones are likely to create new ways to 'socialize' through P2P networks formed between them in the near future. Naturally, this 'globalization' and 'ubiquitous presence' of social networks bring many exciting opportunities along with extreme challenges. To realize these opportunities and to deal with the challenges, we will need new algorithms with efficient effective social communication under uncertain environmental conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">NextGen Networks: Through an Algorithmic Lens</head><p>Algorithms are key building blocks of any network architecture. For example, the Internet provides efficient communication between users through a collection of algorithms operating at the end-users and inside the network. Popular instances of such algorithms are the Transmission Control Protocol (TCP) for congestion control or Border Gateway Protocol (BGP) for routing. The above discussed emerging or next generation networks are not designed to provide efficient communication between the entities or the users networked by them. But, they do require algorithms to enable their primary applications. For example, a sensor network may require an estimation algorithm for event detection given the sensor observations; a P2P network may require a dissemination algorithm using peer information; a network of aerial vehicles may need an algorithm to reach consensus to co-ordinate their surveillance efforts, and an advertiser may need a social network algorithm for efficient 'viral' advertisement.</p><p>In most of these next generation networks, algorithms usually need to operate under an 'adverse' environment. First of all, since these networks are not built for providing communication, there is usually a lack of a reliable network infrastructure. Second, these networks are highly dynamic in the sense that nodes may join the network, leave the network, or even become intermittently unavailable in an unpredictable manner. Third, the network is usually highly resource constrained in terms of communication, computation and sometimes energy resources.</p><p>The highly constrained environment in which algorithms are operating suggest that the algorithm must posses certain properties so as to be implementable in such networks. Specifically, an algorithm operating at a node of the network should utilize information 'local' to the node and should not expect any static infrastructure. It should attempt to achieve its task iteratively and by means of asynchronous message exchanges. The algorithm should be robust against the network dynamics and should not prescribe to any 'hard-wired' implementation. And finally, the algorithm should utilize minimal computational and communication resources by performing few logical operations per iteration as well as require light-weight data structures. These constraints naturally lead to 'Gossip' algorithms, formally described next, as a canonical algorithmic architectural solution for these next generation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The Formal Agenda</head><p>We shall formally describe the quest for algorithm design for the next generation networks in this section. This will give rise to the formal definition of 'Gossip' algorithms, which will serve as the canonical algorithmic solution.</p><p>To this end, let us consider a network of n nodes denoted by V = {1, . . . , n}. Let E ⊂ V × V denote the set of (bidirectional) links along which node pairs can communicate. That is, (i, j) ∈ E if and only if nodes i, j ∈ V can communicate with each other. Let this network graph be denoted by G = (V, E). This network graph G should be thought of as changing over time in terms of V and E. As the reader will notice, the algorithms considered here will not utilize any static property of G and hence will be applicable in the presence of explicit network dynamics. For simplicity of the exposition, we shall not model the network dynamics explicitly. Let d i denote the degree of node i in G, i.e., d i = |{j ∈ V : (i, j) ∈ E}|. We will assume that the network G is connected without loss of generality; or else we can focus on different connected components separately.</p><p>We consider a class of algorithms, called 'Gossip' algorithms, that are operating at each of the n nodes of the network. Now, we present the formal definition of these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.1(Gossip algorithms).</head><p>Under a Gossip algorithm, the operation at any node i ∈ V , must satisfy the following properties:</p><p>(1) The algorithm should only utilize information obtained from its neighbors N (i) = {j ∈ V : (i, j) ∈ E}. <ref type="bibr" target="#b1">(2)</ref> The algorithm performs at most O(d i log n) amount of computation per unit time. (3) Let |F i | be the amount of storage required at node i to generate its output. Then the algorithm maintains O(poly(log n) + |F i |) amount of storage at node i during its running. (4) The algorithm does not require synchronization between node i and its neighbors, N (i). <ref type="bibr" target="#b4">(5)</ref> The eventual outcome of the algorithm is not affected by 'reasonable'<ref type="foot" target="#foot_0">1</ref> changes in N (i) during the course of running of the algorithm.</p><p>We wish to design Gossip algorithms for computing a generic network function. Specifically, let each node have some information, and let x i denote the information of node i ∈ V . The node i ∈ V wishes to compute a function f i (x 1 , . . . , x n ) using a Gossip algorithm. Also, it would like to obtain a good estimate of f i (x 1 , . . . , x n ) as quickly as possible. The question that is central to this survey is that of identifying the dependence of the computation time of the Gossip algorithm over the graph structure G and the functions of interest f 1 , . . . , f n .</p><p>Before we embark on the description and organization of this survey, some remarks are in order. First, property (3) rules out 'trivial' algorithms like first collect values x 1 , . . . , x n at each node and then compute f i (x 1 , . . . , x n ) locally for functions like summation, i.e., f i (x 1 , . . . , x n ) = n k=1 x k . This is because for such a function the length of the output is O(1) (we treat storage of each distinct number by unit space) and hence collection of all n items at node i would require storage Ω(n) which is a violation of property <ref type="bibr" target="#b2">(3)</ref>. Second, the computation of complex function (e.g., requiring beyond poly(log n) space) are beyond this class of algorithms. This is to reflect that the interest here is in functions that are easily computable, which is usually the case in the context of network applications. Third, the definition of a Gossip algorithm here should be interpreted as a rough guideline on the class of simple algorithms that are revelant rather than a very precise definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Organization</head><p>In the remainder of this survey, we provide a systematic description of the class of network functions that can be computed by means of a Gossip algorithm. A salient feature of the analysis of the algorithms described in this survey is the ability to describe the precise dependance of computation time on the network graph structure G and the function of interest. These dependancies are described in terms of 'spectral-like' graph properties. Therefore, we start with Preliminaries on graph properties and some known results that will be useful in the algorithm design and analysis. These are explained through examples of a collection of graph models throughout the survey.</p><p>The network functions for which we describe Gossip algorithms in this survey are naturally designed in a 'layered' fashion. At the bottom of the layer lies the design of a robust information layer using a Gossip algorithm. This is described in detail in Information dissemination. Here we will describe information dissemination Gossip algorithm for both unicast and multicast types of traffic scenarios. We will describe a natural relation between Percolation on graphs, information dissemination and certain spectral-like graph properties.</p><p>The simplest class of iterative algorithms, built upon an unreliable information layer, are based on linear dynamics. These algorithms have been used for solving consensus or multi-agent co-ordination problems classically. We provide a detailed account on the optimal design and analysis of such algorithms in Linear computation. Here, we shall describe the interplay between Markov chain theory, mixing times and Gossip algorithms. We also report some advances in the context of Markov chain theory due to considerations from the viewpoint of Gossip algorithms.</p><p>Linear function computation is an instance of, and essentially equivalent to, separable function computation. The quest for designing the fastest possible Gossip algorithm, in terms of its dependence on the graph structure, for separable function computation, which will be left partly unresolved by the linear dynamics based algorithms, will be brought to a conclusion in Separable function computation. Here, we shall describe an algorithm based on an 'extremal' property of the Exponential distribution. This algorithm will utilize the unreliable information layer designed in Information dissemination for the purpose of information exchange. The appropriately quantized version of this algorithm as well as information theoretic arguments suggesting its fundamental optimality will be discussed (see 'Summary') as well.</p><p>Next, we consider Gossip algorithm design for the task of scheduling in constrained queueing networks. This is a key operational question for networks such as those operating over a common wireless medium. For such a network a scheduling algorithm is required for the media access control (MAC). We describe Gossip scheduling algorithm in Network scheduling. This algorithm builds upon the separable function computation algorithm using clever randomization.</p><p>Network resource allocation is another fundamental problem that is faced while operating a communication network. Under flow-level modeling of a network, this involves solving certain network-wide or global constrained convex optimization problems. Therefore, we consider the question of designing a Gossip algorithm for a class of convex optimization problems in Network convex optimization. This algorithm, like network scheduling, builds upon the separable function computation algorithm. Specifically, it utilizes the separable function computation algorithm to design a 'distributed computation' layer.</p><p>In summary, the algorithms presented in this survey provide 'layers' of computation in a network. The key reason for the existence of such a 'layered' algorithmic architecture lies in the ability to 'functionally decompose' many interesting problems with separable function computation central to the decomposition. For this reason, Gossip algorithm for separable function computation becomes a key 'subroutine' in designing Gossip algorithms for many seemingly complex network computation problems. For these reasons, in addition to applications described in this survey, the separable function computation algorithm can be used to design Gossip algorithms for other important applications including spectral decomposition (using the algorithm of Kempe and McSherry <ref type="bibr" target="#b36">[38]</ref> and the separable function computation algorithm) and Kalman filtering.</p><p>= {j ∈ V : (i, j) ∈ E}. The degree of node i, denoted by d i is defined as</p><formula xml:id="formula_0">d i = |N (i)|. A path between two nodes i = j is denoted by a collection of edges (u k , u k+1 ) ∈ E, 0 ≤ k &lt;</formula><p>for some ≥ 1, where u 0 = i and u = j. The length of a path is defined as the number of edges that belong to the path. A path between two nodes of the shortest length is called a shortest path. If there is a path between any two nodes in V then we call the graph connected. Any connected undirected graph induces a finite valued metric on nodes in V as follows: define metric d G : V × V → N by assigning the length of shortest path between nodes i = j ∈ V as d G (i, j) and d G (i, i) = 0 for all i ∈ V . Diameter D of a connected graph G is defined as</p><formula xml:id="formula_1">D = max i,j∈V d G (i, j).</formula><p>A random walk on a graph G = (V, E) or equivalently a Markov chain with its states represented by V and transitions represented by E, is defined by an n × n non-negative valued probability transition matrix P = [P ij ], where P ij is the probability of transition from state or node i to j. We shall use the terms random walk or Markov chain associated with graph G and/or probability matrix P interchangeably throughout.</p><p>Now by definition, the probability matrix P ∈ R n×n + must satisfy</p><formula xml:id="formula_2">n j=1 P ij = 1, ∀ i ∈ V.</formula><p>Further, it is graph G conformant, i.e., if (i, j) / ∈ E then P ij = P ji = 0. A matrix P is called irreducible if for any i = j ∈ V , there exists a path (u k , u k+1 ) ∈ E, 0 ≤ k &lt; for some ≥ 1 with u 0 = i, u = j and</p><formula xml:id="formula_3">-1 k=0 P u k u k+1 &gt; 0. The matrix P is said to have period d if V can be decomposed into a disjoint union of d non-empty sets V 0 , . . . , V d-1 , i.e., V = ∪ d-1 k=0 V k ; V k ∩ V k = ∅, for 0 ≤ k = k &lt; d and for any i ∈ V k , if P ij &gt; 0 then j ∈ V (k+1) mod d . P is called aperiodic if it</formula><p>does not have periods larger than 1. Throughout, we will be interested in P that are irreducible and aperiodic unless specified otherwise.</p><p>A probability distribution π = [π i ] ∈ R n + is called a stationary distribution of probability matrix P if π T P = π T or equivalently if it satisfies the balance equations</p><formula xml:id="formula_4">π j = n i=1 π i P ij , ∀ j ∈ V.</formula><p>By the Perron-Frobenius Theorem (see book by Horn and Johnson <ref type="bibr" target="#b29">[30]</ref>) it follows that any irreducible and aperiod P has a unique stationary distribution π with all components being strictly positive. Further, for such a P if we consider its tth power, P t , then P t ij → π j as t → ∞, for all i, j ∈ V . A matrix P with such a convergence property is called 'ergodic'. For an ergodic random walk, we define the notion of an ergodic flow. Specifically, it is an n</p><formula xml:id="formula_5">× n matrix Q = [Q ij ] where Q ij = π i P ij .</formula><p>For an ergodic random walk ergodic flow Q defines P and π uniquely. This is because,</p><formula xml:id="formula_6">π i = j π j P ji = j Q ji . And if π, Q are known then P ij = Q ij /π j .</formula><p>Finally, we classify irreducible and aperiodic Markov chains into two classes: reversible and non-reversible. A Markov chain or random walk with transition matrix P and stationary distribution π is called reversible if π i P ij = π j P ji for all i, j ∈ V . A Markov chain that is not reversible is called non-reversible. A special class of reversible Markov chains are those with symmetric P , i.e., P ij = P ji for all i, j ∈ V . For such a P , it can be easily checked that π = (1/n)1 = [1/n]. That is,</p><formula xml:id="formula_7">1 T P = 1 T and P 1 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixing Time and Conductance</head><p>As noted above, for any irreducible and aperiodic P , P t converges to a matrix with all of its rows equal to π T . The question of interest in the context of algorithms based on random walk (such as the ones we will consider throughout) is that of determining the rate at which this convergence happens. More precisely, for a given ε &gt; 0 we will be interested in finding out how large a t is needed so that P t is ε close to this eventual matrix in some 'norm'. This is formalized in terms of the 'mixing time' of the random walk based on P .</p><p>Specifically, we will introduce two seemingly different but closely related definitions of mixing times. As we shall see, both of them are closely related and will be useful in characterizing mixing times for different situations depending upon the 'type' of P . First, we introduce a notion of mixing time based on a 'total variation' distance between distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1(Mixing time: total variation).</head><p>For a random walk (or Markov chain) with transition matrix P and stationary distribu-</p><formula xml:id="formula_8">tion π = [π i ], let ∆ i (t) = 1 2 n j=1 |P t ij -π j |.</formula><p>Then, the ε-mixing time is defined as</p><formula xml:id="formula_9">τ (ε, P ) = max i inf{t:∆ i (s) ≤ ε, ∀ s ≥ t}. (2.1)</formula><p>The other notion of a mixing time is based on stopping rules. A stopping rule Γ is a stopping time based on the random walk of P : at any time, it decides whether to stop or not, depending on the walk seen so far and possibly additional randomness (or coin flips). Suppose the starting node w 0 is drawn from distribution σ. The distribution of the stopping node w Γ is denoted by σ Γ = τ and call Γ the stopping rule from σ to τ . Let H(σ, τ ) be the infimum of mean length over all such stopping rules from σ to τ . This is well-defined as there exists the following stopping rule from σ to τ : select i with probability τ i and walk until getting to i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.2(Mixing time: Stopping rule). Given Markov chain</head><p>P with stationary distribution π, the stopping rule based mixing time H(P ) is defined as:</p><formula xml:id="formula_10">H(P ) = max σ H(σ, π),</formula><p>where σ is over the space of all distributions on V .</p><p>A related important notion is that of 'conductance'. Given Markov chain with transition matrix P and stationary distribution π, its conductance Φ(P ) is defined as</p><formula xml:id="formula_11">Φ(P ) = min S⊂V i∈S,j∈V \S π i P ij π(S)π(V \S) , (<label>2.2)</label></formula><p>where π(A) = i∈A π i . Some remarks about Φ(P ) are in order. For symmetric P , the stationary distribution is uniform, i.e., π = (1/n)1. Further, symmetry of P implies that i∈S,j∈S c P ij = i∈S,j∈S c P ji . Using these two properties, the Φ(P ) for such symmetric P simplifies to Φ(P ) = min</p><formula xml:id="formula_12">S⊂V :|S|≤n/2 i∈S,j∈S c P ij |S| . (2.3)</formula><p>In the context of symmetric P (which will be the case in many scenarios considered), we will use the term 'conductance' for (2.3) instead of (2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Techniques: Characterizing Mixing Time</head><p>There are many algebraic, analytic, and combinatorial techniques available to characterize the mixing time, either based on total variation or stopping time for Markov chains. Here, we list a set of known results that will be useful throughout. To learn details of these results/techniques as well as a host of other results/techniques we refer an interested reader to the excellent surveys by Lovasz and Winkler <ref type="bibr" target="#b41">[43]</ref>, Montenegro and Tetali <ref type="bibr" target="#b50">[52]</ref>.</p><p>First, we state a result that relates τ (ε, P ) and H(P ). Specifically, for any ε &gt; 0, we have</p><formula xml:id="formula_13">τ (ε, P ) = O H(P ) log 1 ε .</formula><p>Next, we present a bound on τ (ε, P ) in terms of eigenvalues. If P is reversible, then one can view P as a self-adjoint operator on a suitable inner product space and this permits us to use the well-understood spectral theory of self-adjoint operators. It is well-known that P has</p><formula xml:id="formula_14">n = |V | real eigenvalues 1 = λ 0 &gt; λ 1 ≥ λ 2 ≥ • • • ≥ λ n-1 &gt; -1.</formula><p>Then for any ε &gt; 0, τ (ε, P ) is bounded as follows:</p><formula xml:id="formula_15">λ P 2(1 -λ P ) log 1 2ε ≤ τ (ε, P ) ≤ 1 1 -λ P log 1 επ 0 ,</formula><p>where λ P = max{|λ 1 |, |λ n-1 |} and π 0 = min i π i . The 1λ P is also called the spectral gap of reversible matrix P . When, P is non-reversible we consider P P * where P * is the adjoint of P with respect to the appropriate inner product space. For P with uniform stationary distribution, P * = P T . It follows by definition that the Markov chain with P P * as transition matrix is reversible. Let 1λ P P * be the spectral gap of this reversible Markov chain. Then, for any ε &gt; 0, the ε-mixing time of the original Markov chain (with transition matrix P ) is bounded above as</p><formula xml:id="formula_16">τ (ε, P ) ≤ 2 1 -λ P P * log n ε √ π 0 . (2.4)</formula><p>Now, we present a technique known as 'the fill-up lemma' (see <ref type="bibr" target="#b0">[1]</ref>). It will be useful to bound the stopping rule based mixing time, H(P ) in certain scenarios involving non-reversible random walks. This is particularly useful when it is difficult to design an exact stopping rule with small average time-length. In that situation, the 'fill-up lemma' suggests the following two step approach.</p><p>Step 1. For a positive constant δ &gt; 0 and any starting distribution σ, design a stopping rule whose stopping distribution γ is δ-far from π in the sense that γ ≥ (1δ)π, with inequality holding component-wise. Such a construction by definition provides an upper bound on H(σ, γ).</p><p>Step 2. Now, the H can be bounded using H(σ, γ) for such a choice of γ, as</p><formula xml:id="formula_17">H ≤ 1 1 -δ H δ ,</formula><p>where</p><formula xml:id="formula_18">H δ = max σ min γ≥(1-δ)π H(σ, γ). That is, H ≤ 1 1-δ H(σ, γ).</formula><p>Finally, we provide a relation between mixing time and the conductance. For any P , the stopping rule based mixing time, H(P ) is bounded as</p><formula xml:id="formula_19">1 Φ(P ) ≤ H(P ) ≤ O log n Φ 2 (P ) .</formula><p>Let us restrict P to be reversible and let us assume that λ P = λ 1 as per above notation. This is not a restrictive assumption because for any P , the Markov chain with transition matrix (I + P )/2 has this property and it can be easily checked that the mixing time of P and that of (I + P )/2 differ only by a constant factor. Now for such a P , the conductance Φ(P ) bounds the spectral gap, 1λ 1 as</p><formula xml:id="formula_20">Φ 2 (P ) 2 ≤ 1 -λ 1 ≤ 2Φ(P ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">A Related Notion: k-Conductance</head><p>Here, we introduce a notion related to 'conductance' Φ(P ) of a probability matrix P . We define this notion building upon the simplification achieved in (2.3) for symmetric matrices. Specifically, for any</p><formula xml:id="formula_21">1 ≤ k ≤ n -1 we define k-conductance Φ k (P ) as Φ k (P ) = min S⊂V :|S|≤k i∈S,j∈S c P ij |S| . (2.5)</formula><p>The notion of k-conductance will be utilized to study information spreading algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Some Graph Models</head><p>Here, we describe various network graph models that are used in applications. We shall use these models for illustration purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Complete Graph</head><p>The complete graph represents situation when all nodes can communicate with each other. That is, essentially no graphical communication constrains. In a sense, the complete graph can be used to provide absolute bound on the best performance an algorithm can achieve when the graph structure can affect performance of the algorithm. In that sense, the complete graph serves as an important conceptual graphical structure. For a complete graph of n nodes, a natural symmetric probability matrix is P = [1/n], i.e., all entries being equal to 1/n. For such a matrix, it is easy to check that it is irreducible, aperiodic and hence ergodic. It has a uniform stationary distribution. The mixing time of P is 1 since for any starting distribution, within 1 step the distribution becomes uniform. Note that this applies to both definitions of mixing time.</p><p>Let us consider the conductance of P , Φ(P ) defined by (2.3). For any</p><formula xml:id="formula_22">S ⊂ V with |S| = k ≤ n/2, we have i∈S,j∈S c P ij |S| = 1 n |S||S c | |S| = n -k n .</formula><p>Therefore, in order to minimize the above for k ≤ n/2, we should choose k = n/2 and this yields that</p><formula xml:id="formula_23">Φ(P ) = n/2 n ≈ 1/2.</formula><p>Now, we compute the k-conductance. Using the same calculations as above, but in place of n/2 if we use k, we obtain</p><formula xml:id="formula_24">Φ k (P ) ≈ 1 - k n .</formula><p>We shall be interested in the following 'harmonic' like mean of Φ k (P ) for 1 ≤ k &lt; n, denoted by Φ(P ) and defined as</p><formula xml:id="formula_25">Φ(P ) = n-1 k=1 k Φ k (P )</formula><p>.</p><p>Then, for the above described P for the complete graph, we have</p><formula xml:id="formula_26">Φ(P ) = n-1 k=1 kn n -k = Θ(n 2 log n).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Ring Graph</head><p>The ring graph of n nodes is formed by placing n nodes on a circle (or a ring) and connecting each node to two of its nearest neighbors. This is essentially the most communication constrained graph. Qualitatively, complete graph represents one end of the spectrum of graphs while the ring graph presents the other end. In a sense, the complete graph has no 'geometry' while the ring graph has the 'strongest' possible 'geometry'. For this reason, in a sense ring graph provides an absolute bound on the worse performance a graph algorithm can achieve. Therefore, the ring graph serves as another important conceptual graphical structure. For a ring graph of n nodes, a natural symmetric probability matrix P is as follows: for each i, P ii = 1/2, P ii + = P ii -= 1/4 where i + and i - represent neighbors of i on either side. The mixing time τ (ε, P ) is Ω(n 2 ) and O(n 2 log n/ε). It can be checked that the conductance for such P is Φ(P ) = Θ(1/n). The k-conductance is Φ k (P ) = Θ(1/k). Therefore, it follows that Φ(P ) = Θ(n 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Expander Graph</head><p>Informally, an expander graph refers to a class of graphs indexed by the number of nodes that have good 'expansion' property. There are various equivalent definitions for expander graphs. Here, we will define expander graphs in terms of the graph conformant ergodic probability matrix P . Specifically, we call a sequence indexed by the number of nodes n of random walk with probability transition matrix P over graph G as 'expanding' (or G, P as expander) if there exists δ &gt; 0, independent of n so that Φ(P ) ≥ δ. This will imply that the mixing time of P essentially scales as O(log n), i.e., the random walk mixes very fast. There are various popular network models that posses this property. Specific examples include the Preferential Connectivity Model for the Internet graph and the Small World Model for social network (for certain range of parameters) with 'natural' random walk.</p><p>A special class of expander graphs that we will use for the purpose of illustration throughout is the d-regular expander for d ≥ 3. These are graph sequences such that for any number of nodes n, each node has degree d, where d is independent of n. For any graph in this sequence, it has good 'expansion' property. Specifically, for any subset S ⊂ V of size at most n/2 there exists α &gt; 0 (independent of n) such that</p><formula xml:id="formula_27">E(S, S c ) ≥ α|S|,</formula><p>where E(S, S c ) = |{(i, j) ∈ E: i ∈ S, j ∈ S c }|. For such a graph consider a natural P defined as follows:</p><formula xml:id="formula_28">P ij =        1 2 , if i = j 1 2d , if j ∈ N (i) 0, otherwise.</formula><p>Such a P is symmetric and has a uniform stationary distribution. Consider the conductance of P :</p><formula xml:id="formula_29">Φ(P ) = min S⊂V :|S|≤n/2 i∈S;j∈S c P ij |S| = min S⊂V :|S|≤n/2 i∈S;j∈S c 1 2d |S| = 1 2d min S⊂V :|S|≤n/2 E(S, S c ) |S| ≥ min S⊂V :|S|≤n/2 α|S| 2d|S| ≥ α 2d . (2.6)</formula><p>Thus, the natural P has conductance which is at least α/2d &gt; 0, independent of n. Thus, such a graph with natural probability matrix P described above is indeed an expander as per our definition. Now, we compute the k-conductance. Using the same calculations as above, but in place of n/2 if we use k, we obtain for k ≤ n/2</p><formula xml:id="formula_30">Φ k (P ) ≥ α 2d ⇒ Φ k (P ) = Θ(1).</formula><p>For k &gt; n/2, consider the following. For S such that</p><formula xml:id="formula_31">|S| = k &gt; n/2 E(S, S c ) = E(S c , S) ≥ α|S c | = α(n -|S|) = α(n -k).</formula><p>Using this, it follows that for k &gt; n/2,</p><formula xml:id="formula_32">Φ k (P ) ≥ α 2d n -k k = Ω n -k k .</formula><p>Then, calculations similar to those done for the complete graph imply that</p><formula xml:id="formula_33">Φ(P ) = O(n 2 log n).</formula><p>Existence of d-regular expanders was first established by Pinsker by means of a probabilistic argument: for d ≥ 3, random d-regular connected graphs are expanders with positive probability. Now, we know various explicit constructions of such graphs. For example, the Zig Zag construction of d-regular expander by Reingold et al. <ref type="bibr" target="#b59">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Geometric Random Graph</head><p>The Geometric Random Graph has been used successfully to model ad-hoc wireless networks. A d-dimensional Geometric Random Graph of n nodes, modeling a wireless ad-hoc network of n nodes with wireless transmission radius r, denoted as G d (n, r) is obtained as follows: place n nodes on a d-dimensional unit cube uniformly at random and connect any two nodes that are within distance r of each other.</p><p>An example of a two dimensional graph, G 2 (n, r) is shown in Figure <ref type="figure">2</ref>.1. The following is a well-known threshold result about the connectivity of G d (n, r) (for a proof, see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">58]</ref>): Lemma 2.1. There exists a constant α d &gt; 0 such that for any ε &gt; 0, if (nr</p><formula xml:id="formula_34">d (n)/log n) ≥ α d + ε then the G d (n, r(n)) is connected with probability 1 -o(1) and if (nr d (n))/log n ≤ α d -ε then G d (n, r(n)) is not connected with probability 1 -o(1).</formula><p>Here, we shall consider r(n) such that (nr d (n))/log n = ω(1), i.e., G(n, r(n)) is connected with high probability. Let d max and d min denote, respectively, the maximum and minimum vertex degree of such a G(n, r(n)). Then, it can be argued that with probability 1o(1),</p><formula xml:id="formula_35">d max = (1 + o(1))d min .</formula><p>That is, G(n, r(n)) is almost regular. Therefore, we can define the natural random walk on G(n, r(n)) with transition matrix P where</p><formula xml:id="formula_36">P ij =        1 2 , if i = j, 1 2d i , if j ∈ N (i), 0, otherwise.</formula><p>Clearly P is aperiodic (due to self-loop) and irreducible (since G(n, r(n)) is connected) with probability 1o(1). Let π be the stationary distribution of such a random walk P . Then, it follows that π i = (1 + o( <ref type="formula" target="#formula_215">1</ref>))/n with probability 1o(1). It has been established that the mixing time of such a random walk is</p><formula xml:id="formula_37">τ (ε, P ) = Ω(r(n) -2 ) and τ (ε, P ) = O(r(n) -2 log n/ε).</formula><p>Further, it has been established that the fastest mixing reversible random walk on G(n, r(n)) with uniform stationary distribution has mixing time no faster than r(n) -2 . That is, the natural random walk over G(n, r) has the mixing time of the same order as that of the random walk with the fastest mixing time. We note that for the natural random walk with probability matrix P , and for r(n) = Θ(log 3/2 n/ √ n) with a large enough constant, the conductance Φ(P ) scales like Θ * (r(n)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Historical Notes</head><p>We presented preliminaries about random walks, mixing times and graph models. The theory of mixing time is quite well developed. Various results are reported here and many others not reported here can be found in excellent surveys by Lovasz and Winkler <ref type="bibr" target="#b41">[43]</ref> and by Montenegro and Tetali <ref type="bibr" target="#b50">[52]</ref>. We make note of a result about the characterization of the fastest mixing reversible random walk on a given graph in terms of an appropriately defined Semi-Definite Program (SDP) by Boyd et al. <ref type="bibr" target="#b6">[7]</ref> due to its applicability in proving bounds on mixing times for various reversible random walks. For mixing time analysis of the natural random walk on a ring graph and on a Geometric random graph we refer the reader to the work by Boyd et al. <ref type="bibr" target="#b7">[8]</ref>. For evaluation of conductance on a ring and on a Geometric random graph we refer the reader to work by Madan et al. <ref type="bibr" target="#b43">[45]</ref>. For various expander related results we refer the reader to a set of lecture notes by Linial and Widgerson <ref type="bibr" target="#b40">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Dissemination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We are given a connected network of n nodes with a connectivity graph G = (V, E). Each node, say i ∈ V , has its own information denoted by x i . The information x i can be thought of as a collection of bits, a packet or even a real number. The interest is in spreading or disseminating this information to possibly different subsets of nodes in V . We will consider two special scenarios motivated by applications to distributed computation, estimation in sensor networks, and content distribution in peer-to-peer networks.</p><p>The first scenario consists of a situation when exactly one of the n nodes wishes to spread its information to all the remaining n -1 nodes. We will denote this scenario of single multicast as Single-piece dissemination. Formally, let an arbitrarily selected node, say i ∈ V , has a piece of information. The goal is to spread this information to all nodes in V as quickly as possible by means of a Gossip algorithm. This situation naturally arises in a sensor network deployed for some event detection such as 'smoke detection' in a building -one (or few) sensors detect smoke and they wish to 'alarm' all the sensors in the building as quickly as possible for evacuation. Another such situation is that of content distribution over a peer-to-peer network where content is arising from a large content distributor. For example, the use of the BBCiplayer by BBC <ref type="bibr">[70]</ref>. Finally, an important 'sub-routine' in many distributed computation problems corresponds to the singlepiece information dissemination scenario. This is explained in detail in Separable function computation.</p><p>The second scenario corresponds to a situation where all nodes have their individual pieces of information and each node wishes to disseminate its own information to all the other nodes. This all-to-all multicast scenario will be denoted by multi-piece dissemination. Again, the goal will be to disseminate all information to all nodes as quickly as possible by means of a Gossip algorithm. This situation arises in content distribution over a peer-to-peer network when content is generated not only by one (or few) distributors like BBC, but also by many users. This is the case for advertisement over a P2P network where many advertizers have their own information that they wish to disseminate very widely.</p><p>In what follows, we shall describe natural Gossip algorithms for both of the above scenarios. We will charcterize the performance of these algorithms in terms of certain spectral-like properties of the underlying network graph G. This will suggest that Gossip protocols are quite efficient in network resource utilization for such information dissemination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Single-Piece Dissemination</head><p>Given the network G = (V, E) of n nodes, an arbitrary node v ∈ V has a piece of information that it wishes to spread to all the other nodes as quickly as possible. We will describe a natural randomized Gossip algorithm which is very much like 'rumor mongering'.</p><p>To this end, let P = [P ij ] be an n × n graph G conformant doubly stochastic and symmetric matrix. The algorithm utilizing P is as follows. Let time be discrete and denoted by t ∈ N. Let S(t) ⊂ V denote the set of nodes that contain node v's information at time t. Initially, t = 0 and S(0) = {v}. For information spreading at time t ≥ 1, each node i ∈ V contacts one of its neighbors, say j with probability P ij ; it will not contact any other node with probability P ii . Upon contacting, if either i or j had v's information at time t -1, then both will have v's information at the end of time t.</p><p>A few remarks about the above described algorithm. First, each node can contact at most one other node; but it can be contacted by more than one node as part of the algorithm. Second, a node can spread v's information at time t only if it has received it by time t -1. Third, the information exchange protocol has both 'pull' and 'push' components, i.e., when two nodes are connected information is exchanged in both directions. As will become clear from analysis, the presence of both 'pull' and 'push' is necessary for quicker dissemination. Now the quantity of interest. For any ε &gt; 0, we wish to find the time by which all nodes have node v's information with probability at least 1ε, for any v ∈ V . Formally, the ε-dissemination time, denoted by T one spr (ε) is defined as</p><formula xml:id="formula_38">T one spr (ε) = sup v∈V inf {t: Pr(S(t) = V | S(0) = {v}) ≤ ε} .</formula><p>Recall the definition of the 'conductance' of a symmetric, doubly stochastic matrix P , denoted by Φ(P ), from Preliminaries:</p><formula xml:id="formula_39">Φ(P ) = min S⊂V :|S|≤n/2 i∈S;j∈S c P ij |S| .</formula><p>Now, we are ready to state the characterization of the spreading time of the above described natural Gossip algorithm based on P .</p><p>Theorem 3.1. Let P be an irreducible, doubly stochastic and symmetric matrix on graph G. Then, for the natural Gossip algorithm described above</p><formula xml:id="formula_40">T one spr (ε) = O log n + log ε -1 Φ(P )<label>.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Proof of Theorem 3.1</head><p>The proof is divided into two phases of the algorithm. The first phase corresponds to the time period starting at t = 0 until |S(t)| ≤ n/2.</p><p>The second phase corresponds to the time period starting after the end of the first phase and until S(t) = V . We will show, starting with S(0) = {v} for any v ∈ V , that each phase is at most O(log n + log ε -1 )/Φ(P ) with probability at least 1ε/2. This will complete the proof of Theorem 3.1. We remark that, the bound on the time length of the first phase essentially relies on the 'push' aspect of the algorithm while the bound on the time length of the second phase essentially relies on the 'pull' aspect of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.1">Phase 1: |S(t)| ≤ n/2</head><p>Fix the node v ∈ V with S(0) = {v}. We will study the evolution of the size of the set S(t) containing the information of v. Here, we are concerned with the first phase, i.e., the time duration when |S(t)| ≤ n/2. Through the time-evolution of S(t), we will bound the length of the first phase. As stated above, we will ignore the effect due to the pull aspect of the algorithm in the first phase. It can be easily argued that ignoring the pull aspect can only yield a weaker bound. For notational simplicity, we will use Φ instead of Φ(P ) by dropping reference to P . Now, we want to study increase |S(t + 1)| -|S(t)| at time t, when |S(t)| ≤ n/2. For j / ∈ S(t), let X j be an indicator random variable that is 1 if node j receives the information of v via a 'push' from some node i ∈ S(t) in time slot t + 1, and is 0 otherwise. The probability that j does not receive the information via a push is the probability that no node i ∈ S(t) contacts j, and so</p><formula xml:id="formula_41">E[X j | S(t)] = 1 -Pr(X j = 0 | S(t)) = 1 - i∈S(t) (1 -P ij ) ≥ 1 - i∈S(t) exp(-P ij ) = 1 -exp   - i∈S(t) P ij   . (3.1)</formula><p>In the above equation, we used the inequality 1x ≤ exp(-x) for x ≥ 0. The Taylor series expansion of exp(-z) about z = 0 implies that, </p><formula xml:id="formula_42">if 0 ≤ z ≤ 1, then exp(-z) ≤ 1 -z + z 2 /2 ≤ 1 -z + z/2 = 1 -z/2. (<label>3</label></formula><formula xml:id="formula_43">E[Z(L ∧ (t + 1)) | S(L ∧ t)] = Z(L ∧ t). Now, suppose that |S(t)| ≤ n/2, in which case L ∧ (t + 1) = (L ∧ t) + 1. The function g(z) = 1/z is convex for z &gt; 0, which implies that, for z 1 , z 2 &gt; 0, g(z 2 ) ≥ g(z 1 ) + g (z 1 )(z 2 -z 1 ). (3.4) Applying (3.4) with z 1 = |S(t + 1)| and z 2 = |S(t)| yields 1 |S(t + 1)| ≤ 1 |S(t)| - 1 |S(t + 1)| 2 (|S(t + 1)| -|S(t)|).</formula><p>(3.5)</p><p>We have ignored the effect of the 'pull' aspect in this phase to obtain bound on its time duration (it only worsens the bound). And due to the 'push', it easily follows that</p><formula xml:id="formula_44">|S(t + 1)| ≤ 2|S(t)|. Therefore 1 |S(t + 1)| ≤ 1 |S(t)| - 1 4|S(t)| 2 (|S(t + 1)| -|S(t)|).</formula><p>(3.6)</p><p>Combining (3.3) and (3.6), and using the fact that 1z ≤ exp(-z) for z ≥ 0, we obtain that, if</p><formula xml:id="formula_45">|S(t)| ≤ n/2, then E 1 |S(t + 1)| S(t) ≤ 1 |S(t)| 1 - Φ 8 ≤ 1 |S(t)| exp - Φ 8 .</formula><p>This implies that</p><formula xml:id="formula_46">E[Z(L ∧ (t + 1)) | S(L ∧ t)] = E exp Φ 8 (L ∧ (t + 1)) |S(L ∧ (t + 1))| S(L ∧ t) = exp Φ 8 (L ∧ t) exp Φ 8 × E 1 |S((L ∧ t) + 1)| S(L ∧ t) ≤ exp Φ 8 (L ∧ t) 1 |S(L ∧ t)| = Z(L ∧ t). Therefore, Z(L ∧ t) is a supermartingale. That is, E[Z(L ∧ t)] ≤ E[Z(L ∧ 0)] = 1, for all t &gt; 0.</formula><p>The fact that the set S(t) can contain at most the n nodes in the graph implies that</p><formula xml:id="formula_47">Z(L ∧ t) = exp Φ 8 (L ∧ t) |S(L ∧ t)| ≥ 1 n exp Φ 8 (L ∧ t) . (3.7)</formula><p>Taking expectations on both sides of (3.7) yields</p><formula xml:id="formula_48">E exp Φ 8 (L ∧ t) ≤ nE[Z(L ∧ t)] ≤ n.</formula><p>Because exp(Φ(L ∧ t)/8) ↑ exp(ΦL/8) as t → ∞, the monotone convergence theorem implies that</p><formula xml:id="formula_49">E exp ΦL 8 ≤ n.</formula><p>Applying Markov's inequality, we obtain that, for t 1 = 8(ln 2 + 2 ln n + ln ε -1 )/Φ,</p><formula xml:id="formula_50">Pr(L &gt; t 1 ) = Pr exp ΦL 8 &gt; 2n 2 ε ≤ ε 2n .</formula><p>Thus, the time duration of the first phase is O((log n + log ε -1 )/Φ) with probability at least 1ε/2n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.2">Phase 2: n/2 &lt; |S(t)| ≤ n</head><p>Now we wish to study the time duration of second phase starting at</p><formula xml:id="formula_51">|S(t)| ≥ n/2 until |S(t)| = n.</formula><p>Unlike the first phase, we will study the evolution of the size of the set of nodes that do not have the information, i.e., |S(t) c |. This quantity will decrease as the information spreads from nodes in S(t) to nodes in S(t) c . For simplicity, let us consider restarting the process from clock tick 0 after L (i.e., when at least half the nodes in the graph have the information for the first time), so that we have |S(0) c | ≤ n/2. As stated earlier, in this phase we will ignore the effect of 'push' and only consider the effect of the 'pull' aspect of the algorithm.</p><p>In time t + 1, a node j ∈ S(t) c will receive the information if it contacts a node i ∈ S(t) and pulls the information from i. As such,</p><formula xml:id="formula_52">E[|S(t) c | -|S(t + 1) c | | S(t) c ] ≥ j∈S(t) c ,i / ∈S(t) c P ji .</formula><p>Thus, we have</p><formula xml:id="formula_53">E[|S(t + 1) c | | S(t) c ] ≤ |S(t) c | - j∈S(t) c ,i / ∈S(t) c P ji = |S(t) c | 1 - j∈S(t) c ,i / ∈S(t) c P ji |S(t) c | ≤ |S(t) c | (1 -Φ) . (3.8)</formula><p>We note that this inequality holds even when |S(t) c | = 0, and as a result it is valid for all time t in the second phase. Repeated application of (3.8) yields</p><formula xml:id="formula_54">E[|S(t) c |] = E [E[|S(t) c | | S(t -1) c ]] ≤ (1 -Φ) E[|S(t -1) c |] ≤ (1 -Φ) t E[|S(0) c |] ≤ exp (-Φt) n 2 .</formula><p>For t 2 = (2lnn + lnδ </p><formula xml:id="formula_55">) c | &gt; 0) = Pr(|S(t 2 ) c | ≥ 1) ≤ E[|S(t 2 ) c |] ≤ ε 2n .</formula><p>Thus, we have obtained that the duration of the second phase is no longer than O((log n + log ε -1 )/Φ) with probability at least 1ε/2n. Therefore, by union bound it follows that the combined duration of the two phases is no longer than O((log n + log ε -1 )/Φ) with probability at least 1ε/n. Note that these bounds are independent of the starting node v ∈ V . Therefore, it implies the desired claim of Theorem 3.1</p><formula xml:id="formula_56">T one spr (ε) = O log n + log ε -1 Φ(P ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Applications</head><p>Here, we describe application of the Gossip algorithm for single-piece information dissemination for various relevant network graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.1">Complete graph</head><p>The complete graph represents a situation where all nodes can communicate with each other. That is, there are essentially no graphical communication constraints. For complete graph of n nodes, a natural symmetric probability matrix is P = [1/n], i.e., all entries being equal to 1/n. For such a matrix, as explained in Preliminaries,</p><formula xml:id="formula_57">Φ(P ) = O(1). Therefore, T one spr (ε) = O(log n + log ε -1 ). That is, for ε = Ω(1/poly(n)) we have T one spr (ε) = O(log n)</formula><p>. Now since each node can spread information to at most one other node in a given time instance, the spreading time is lower bounded by Ω(log n) for any graph. Thus the natural gossip algorithm spreads information essentially as fast as possible on complete graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.2">Ring graph</head><p>The ring graph of n nodes is formed by placing n nodes on a circle (or a ring) and connecting each node to two of its nearest neighbors. This is essentially the most communication constrained graph. Qualitatively, the complete graph represents one end of the spectrum of graphs while the ring graph presents the other. In a sense, complete graph has no 'geometry' while ring graph has the strongest possible 'geometry'.</p><p>For the ring graph of n nodes, a natural symmetric probability matrix P is as follows: for each i, P ii = 1/2, P ii + = P ii -= 1/4 where i + and i -represent neighbors of i on either side. As established in Preliminaries, Φ(P ) = Θ(1/n). Therefore, for ε = Ω(1/poly(n)) we have T ave (ε) = O(n log n). Now the ring graph has diameter n, and hence Ω(n) is a lower bound on any spreading algorithm. Thus, again the natural Gossip algorithm is essentially the fastest possible algorithm on the ring graph as well. We make a note of the fact that a simple centralized algorithm on ring graph will take only O(n) time to spread a single piece of information. However, the whole point of gossip algorithm is to be topology unaware. Therefore, even though the algorithm may be slower than optimal centralized algorithm by O(log n), such a performance is still very acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.3">Expander graph</head><p>The expander graph, by definition, is a class of graphs indexed by the number of nodes n that have good 'expansion' property. Specifically, consider a d-regular expander with all nodes having degree d. The natural probability matrix P will be such that Φ(P ) = O(1) as discussed in Preliminaries. Thus, again the T one spr (ε) = O(log n) for all ε = Ω(1/poly(n)). That is, the Gossip algorithm performs essentially as fast as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.4">Geometric random graph</head><p>The Geometric random graph over n nodes is formed by placing nodes uniformly at random in a geographic area and then connecting nodes within distance r = r(n), the connectivity radius. Such a graph, denoted as G(n, r) is extensively used for modeling wireless networks. The detailed description is provided in Preliminaries. As established there, the natural P on G(n, r) has Φ(P ) scaling as r for an appropriate choice of r. Therefore, for ε = Ω(1/poly(n)) we will have T one spr (ε) = O(r -1 log n). But the diameter of G(n, r) scales as r -1 . Thus, again the gossip algorithm spreads information essentially as fast as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Piece Dissemination</head><p>Here, we consider scenario where each node wishes to spread its own distinct message to all the other nodes as quickly as possible in the given graph G = (V, E) with natural Gossip algorithm based on a graph conformant doubly stochastic symmetric matrix P . Let m i denote the message of node i and M = {m 1 , . . . , m n } denote the set of all of these n messages.</p><p>The algorithm based on P runs in discrete time, denoted by t ∈ N, as before. Let S i (t) ⊂ M denote the set of messages node i has at the beginning of time t. Initially, t = 0 and S i (0) = {m i } for all i. Under the Gossip algorithm, at each time t node i ∈ V contacts one of its neighbors, say node j with probability P ij and does not contact any node with probability P ii . Upon contact, node i sends an arbitrary message from S i (t)\S j (t) to node j, if S i (t)\S j (t) = ∅; and node j sends an arbitrary message from S j (t</p><formula xml:id="formula_58">)\S i (t) to node i, if S j (t)\S i (t) = ∅.</formula><p>Some remarks are in order. As in the single-piece dissemination, we have both 'pull' and 'push' as part of the algorithm. Each node contacts at most one other node, but each node can be contacted by multiple nodes. For information exchange at time t, the nodes can utilize information that was present at nodes in the beginning of time t. Finally, computing S i (t)\S j (t) at node i requires information of neighbor j's message set S j (t). This can be done by various efficient data structures. However, this computation can be avoided by means of random linear codes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b52">54]</ref>. However, it comes at the cost of coding and decoding.</p><p>Now the quantity of interest. For any ε &gt; 0, we wish to find the time by which all nodes have all the messages M with probability at least 1ε. Formally, this ε all-dissemination time, denoted by T all spr (ε) is defined as</p><formula xml:id="formula_59">T all spr (ε) = inf t: Pr n i=1 {S i (t) = M } ≤ ε .</formula><p>Recall the definition of the 'k-conductance' of P , denoted by Φ k (P ), from Preliminaries:</p><formula xml:id="formula_60">Φ k (P ) = min S⊂V :|S|≤k i∈S;j∈S c P ij |S| .</formula><p>Let us define Φ(P ) as 'Harmonic' like mean of Φ k (P ), 1 ≤ k &lt; n, as</p><formula xml:id="formula_61">Φ(P ) = n-1 k=1 k Φ k (P )</formula><p>.</p><p>Now we are ready to state the characterization of the all-spreading time of the above described natural Gossip algorithm based on P .</p><p>Theorem 3.2. Let P be an irreducible, doubly stochastic, and symmetric matrix on graph G. Then, for the natural Gossip algorithm described above</p><formula xml:id="formula_62">T all spr (ε) = O Φ(P ) log ε -1</formula><p>n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Proof of Theorem 3.2</head><p>Recall that the message set at node i in the beginning of time t, denoted by S i (t) is a subset of M . Since M can have 2 n distinct subsets, effectively each node can be in one of the 2 n distinct states, each corresponding to a distinct subset of M , at any time t. Thus, under the Gossip algorithm the overall network state (S 1 (t), . . . , S n (t)) evolves over a state space of size 2 n×n . This is in sharp contrast with singlepiece dissemination where we are only interested in the evolution of one monotonically increasing set. Naturally, this makes the analysis to follow much more involved and hence delicate compared to that of single-piece dissemination.</p><p>To study this evolution over a very high-dimensional space, we introduce the notion of 'type'. Specifically, two nodes, say i and j, are called of the same 'type' at time t if S i (t) = S j (t). At time t, this notion of 'type' defines a partition of all n nodes into disjoint 'type classes', with nodes having the same set of messages, i.e., nodes of the same 'type' belong to the same 'type class'. We will call the size of a maximal type class at time t as 'maximum type-size' and denote it by A(t). We will call |S i (t)|, the number of messages at node i, as the 'dimension' of node i at time t. By natural association, we will abuse notation (hopefully without causing confusion) to call 'dimension' of a 'type class' as the dimension of any of the node belonging to that type class. Now when all nodes have received all messages, there is only one type class of size n and dimension n. Therefore, we wish to find the following stopping time:</p><formula xml:id="formula_63">inf {t: |S i (t)| = n, ∀ i ∈ V } .</formula><p>Initially, at t = 0 we have |S i (t)| = 1 for all i ∈ V . Thus, the information spreads to all the nodes when the overall dimension increases among all the nodes is n(n -1). Motivated by this, we study the overall dimension increase as the 'performance metric' of the algorithm. To this end, define the dimension increase at time t, denoted by D(t) as</p><formula xml:id="formula_64">D(t) = n i=1 (|S i (t)| -1) = n i=1 |S i (t)| -n.</formula><p>By definition, D(0) = 0 and when all nodes have all messages then D(t) = n(n -1). Therefore, we will bound T all spr (ε) by bounding the time for D(t) to become n(n -1). To this end, define</p><formula xml:id="formula_65">L k = inf{t: A(t) ≥ k} and Y k = D(t k ).</formula><p>In words, L k is the first time when any type class has at least k nodes, and Y k is the total dimension increase up to time L k . By definition,</p><formula xml:id="formula_66">L 1 = Y 1 = 0. The following result provides a lower bound on Y k . Lemma 3.3. For any 1 ≤ k ≤ n, Y k ≥ k(k -1).</formula><p>Proof. Consider the time L k , which is the first time any type class contains k nodes. At this time, there are nodes i 1 , . . . , i k ∈ V in the same type class. Since these nodes are of the same type,</p><formula xml:id="formula_67">S i 1 (L k ) = • • • = S i k (L k ). By definition, m i ∈ S i (t) for all i ∈ V, t ∈ N. Hence, for all 1 ≤ ≤ k, {m i 1 , . . . , m i k } ⊂ S i (L k ). This implies that |S i (L k )| ≥ k for 1 ≤ ≤ k. Therefore, the total dimension increase is at least (k -1)k. That is, Y k = D(L k ) ≥ k(k -1).</formula><p>Finally, by definition L n is the time when all nodes have dimension n and Y n = D(L n ) = n(n -1). That is, by the time L n all nodes have received all messages. Therefore, a revised goal is to bound L n . For this, we will bound L k+1 -L k for all 0 ≤ k &lt; n. Now, consider a time t ∈ [L k , L k+1 ). Note that all L k are stopping times. Let there be b type classes, C 1 , . . . , C b at time t. For a pair of nodes i, j, let X ij be an indicator random variable that is 1 if node i contacts node j at time t and the dimension of i increases as a result of the communication, and is 0 otherwise. Node i will contact j with probability P ij . Similarly, j contacts i with probability P ji , which is equal to P ij by symmetry of P . If S i (t) = S j (t), then there will be no increase in total dimension of either i and j upon communication between them. Now suppose S i (t) = S j (t). Then either</p><formula xml:id="formula_68">S i (t)\S j (t) = ∅ or S j (t)\S i (t) = ∅. Now if S i (t)\S j (t) = ∅,</formula><p>then upon j contacting i, dimension of the j will increase by 1. Similarly, if S j (t)\S i (t) = ∅ then dimension of i will increase by 1 upon i contacting j. In summary, if</p><formula xml:id="formula_69">S i (t) = S j (t) then E[X ij ] + E[X ji ] ≥ P ij . Now let F (t) = D(t + 1) -D(t)</formula><p>denote the total dimension increase of all nodes by the end of time t. Note that X ij indicates an increase in dimension due to the 'pull' effect. Since each node contacts at most one other node for the 'pull' aspect of the protocol, by ignoring the 'push' effect we obtain</p><formula xml:id="formula_70">F (t) ≥ i∈V j∈V :j =i X ij .</formula><p>(3.9)</p><p>From the above discussion, it follows that</p><formula xml:id="formula_71">E[F (t)] ≥ i∈V j&gt;i (E[X ij ] + E[X ji ]) ≥ i∈V j&gt;i:S i (t) =S j (t) P ij = 1 2 i,j∈V :S i (t) =S j (t) P ij = 1 2 b =1 |C | i∈C ,j ∈C P ij |C | ≥ nΦ k (P ) 2 ∆ = np k . (3.10)</formula><p>In the analysis above, we have utilized the fact that t ∈ [L k , L k+1 ) and hence the maximal type-class size, A(t) ≤ k. This provides a lower bound on the expected total dimension increase during any round in the period [L k , L k+1 ). Note that this lower bound holds for any</p><formula xml:id="formula_72">t ∈ [L k , L k+1 ) uniformly. Define Z k (t) = t-1 s=L k (F (s) -np k )1 {s&lt;L k+1 } , (<label>3.11)</label></formula><p>where</p><formula xml:id="formula_73">Z k (L k ) = 0. For t ≥ L k , Z k (t) is a submartingale, i.e., E[Z k (t + 1) | Z k (t)] ≥ Z k (t). (3.12)</formula><p>The quantity L k+1 is a stopping time with respect to the history of the algorithm. It is easy to show that E[L k+1 ] &lt; ∞ via a stochastic upper bound using a certain geometric random variable with positive probability. Moreover, the submartingale Z k (t) has bounded increments. A stopped submartingale is a submartingale, and hence we obtain</p><formula xml:id="formula_74">E[Z k (L k+1 )] ≥ E[Z k (L k )] = 0.<label>(3.13)</label></formula><p>Now, from the definitions of L k , L k+1 , Y k , Y k+1 , and (3.13), we obtain</p><formula xml:id="formula_75">E[Y k+1 -Y k ] ≥ np k E[L k+1 -L k ]. (3.14)</formula><p>Recall that L n is the time when all nodes can decode all the messages. Summing the inequality in <ref type="bibr">(3.14)</ref> for all 1 ≤ k ≤ n -1 yields</p><formula xml:id="formula_76">E[L n ] ≤ n-1 k=1 E[Y k+1 -Y k ] np k . (<label>3.15)</label></formula><p>From Lemma 3.3 and the fact that p k is monotonically non-increasing in k, the quantity in the right-hand side of the inequality in (3.15) is maximized when </p><formula xml:id="formula_77">Y k = k(k -1). Hence, E[L n ] ≤ n-1 k=1 2k np k = 2 Φ(P ) n . (<label>3</label></formula><formula xml:id="formula_78">T all spr (ε) = O Φ(P ) log ε -1 n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Applications</head><p>Here, we describe application of the Gossip algorithm for multi-piece information dissemination on various relevant network graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.1">Complete graph</head><p>As before, for the complete graph we consider the natural probability matrix P = [1/n]. For such a matrix, as explained in Preliminaries,</p><formula xml:id="formula_79">Φ(P ) = O(n 2 log n). Therefore, T all spr (ε) = O(n log n log ε -1 ). That is, for ε = Ω(1/poly(n)) we have T one spr (ε) = O(n log 2 n</formula><p>). This is only log 2 n 'slower' compared to the 'fastest' possible time of Θ(n) because each node can receive at most one message in a time-slot. Thus, the natural Gossip algorithm spreads information essentially as fast as possible on the complete graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.2">Ring graph</head><p>For the ring graph of n nodes, as before we consider the following probability matrix P : for each i, P ii = 1/2, P ii + = P ii -= 1/4 where i + and i -represent neighbors of i on either side. As established in Preliminaries, Φ(P ) = Θ(n 3 ). Therefore, for ε = Ω(1/poly(n)) we have T all spr (ε) = O(n 2 log n). Now, the ring has diameter n, and by 'proper' scheduling (everyone passes new messages to the left) it is indeed possible to spread all data to all nodes in time O(n). Thus, the above algorithm seem to be 'slower' by a factor of n compared to the best possible. We believe that this is due to weakness of the analytic method presented here and not the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.3">Expander graph</head><p>The expander graph, like the complete graph, has Φ = (n 2 log n).</p><formula xml:id="formula_80">Therefore, T all spr (ε) = O(n log n) for ε = Ω(1/poly(n)).</formula><p>That is, the Gossip algorithm performs essentially as fast as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary and Historical Notes</head><p>Here, we described Gossip based information dissemination algorithms for the single-piece and many-piece dissemination scenarios. These are the simplest possible natural algorithms. We presented a detailed analysis for the information spreading time in both single-piece and many-piece scenarios. We found that the spreading time depends on the different spectral properties of the graph: for the single-piece scenario it is the conductance and for the multi-piece scenario it is the 'harmonic-like' mean of k-conductances. The analysis presented here for single-piece information spreading is based on work by Mosk-Aoyama and Shah <ref type="bibr" target="#b53">[55]</ref> and the analysis for multi-piece spreading is based on another work by Mosk-aoyama and Shah <ref type="bibr" target="#b52">[54]</ref>. It should be noted that though here for multi-piece information spreading we use a 'BitTorrentlike' scheme, one can use a 'network coding' based approach as well (see <ref type="bibr" target="#b52">[54]</ref> for details).</p><p>Historically, the analysis of multi-piece information spreading for BitTorrent like systems was studied using a 'mean-field' approximation by Qiu and Srikant <ref type="bibr" target="#b57">[59]</ref> as well as Massoulie and Vojnovic <ref type="bibr" target="#b45">[47]</ref>. The use of network coding for efficient gossiping was introduced by Deb et al. <ref type="bibr" target="#b13">[14]</ref> for the complete graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We are given a connected network of n nodes with connectivity graph G = (V, E). Each node i ∈ V has its own real value x i ∈ R. Let x = [x i ] be the vector of these n numbers. The quantity of interest is the average of these n numbers, denoted by x ave , where x ave = n i=1 x i /n = x T 1/n. This question of computing averages in a distributed manner arises in many applications including distributed estimation, distributed control (popularly known as consensus), and distributed optimization.</p><p>A natural iterative and distributed algorithm for computing x ave is based on linear dynamics. To this end, let time be discrete (or slotted) and indexed by t ∈ N. Let y(t) = [y i (t)] denote the vector of estimates at nodes of G at time t. Initially, t = 0 and y(0) = x. The estimates at time t + 1, y(t + 1) are obtained from y(t) by selecting an</p><formula xml:id="formula_81">n × n matrix W (t) = [W ij (t)] ∈ R n×n +</formula><p>and performing a linear update</p><formula xml:id="formula_82">y(t + 1) = W (t)y(t).</formula><p>That is, each node j sends the value W ij (t)y j (t) to node i; upon receiving these values from the nodes (including from itself), node i sums them up to form its updated estimate y i (t + 1). To make such an algorithm distributed with repsect to the network graph G, we require that W (t) be graph conformant, i.e., if (i, j) / ∈ E then W ij (t) = W ji (t) = 0. From results on products of matrices <ref type="bibr" target="#b19">[20]</ref> it follows that if W (t) belongs to a finite set of paracontracting matrices (i.e., W (t)y = y ⇔ W (t)y &lt; y ) then y(t) → y * where y * ∈ ∩ i∈I H(W i ).</p><p>Here, I = {i: W i appear infinitely often in the sequence W (t)} and for i ∈ I, H(W i ) denotes the eigenspace of W i associated with eigenvalue 1. Therefore, to guarantee convergence to x ave 1, the algorithms considered here will choose W (t) with 1 as an eigenvector with eigenvalue 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Example Algorithm</head><p>Here, we present an example of a linear dynamics based algorithm that belongs to the class of algorithms described above. Specifically, we describe a time-invariant algorithm, i.e., W (t) = W . The choice of W is done in a very simple manner based on the well known Metropolis-Hasting method. Specifically, for each node i ∈ V ,</p><formula xml:id="formula_83">W ij =      1 2d , if j = i and (i, j) ∈ E, 0, if j = i and (i, j) / ∈ E, 1 -d i 2d , if j = i.</formula><p>Here d i denotes the degree of node i ∈ V and d = max i∈V d i . It is wellknown that 1 is an eigenvector of W = [W ij ] thus defined with corresponding eigenvalue 1, which is the largest possible value. Further, since G is connected it defines an irreducible, aperiodic random walk on G. Therefore, it follows that</p><formula xml:id="formula_84">y(t) = W t y(0) = W t x → x ave 1, as t → ∞.</formula><p>Thus the above class of linear dynamics based algorithms is indeed non-empty and it is quite easy to find an algorithm, i.e., a matrix W with the desired properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Quantity of Interest</head><p>Our interest is in finding a good estimate of x ave as quickly as possible using the linear dynamics based algorithm. To this end, for any ε &gt; 0 define the ε-computation time, denoted by T ave (ε), as</p><formula xml:id="formula_85">T ave (ε) = sup x∈R n inf t: Pr y(t) -x ave 1 x ≥ ε ≤ ε . (4.1)</formula><p>The probability in the above definition is with respect to possible randomness in the algorithm (i.e., the randomness in selection of sequence</p><formula xml:id="formula_86">W (t), t ∈ N).</formula><p>The algorithm performs a total of</p><formula xml:id="formula_87">|W (t)| o ∆ = |{(i, j): W ij (t) = 0}|</formula><p>computations in the time slot t. Therefore, the total computation performed to obtain an ε approximation (with probability at least 1ε) of x ave at all nodes is</p><formula xml:id="formula_88">C ave (ε) ∆ = Tave(ε)-1 t=0 |W (t)| o .</formula><p>Our goal is to design an algorithm, that is to choose a W (t), t ∈ N, so that ideally both T ave (ε) and C ave (ε) are minimized. In Gossip algorithms, it is reasonable to assume that all nodes are performing some computation in each time slot. Therefore, we will have</p><formula xml:id="formula_89">|W (t)| o = Ω(n) for all t. That is, C ave (ε) = Ω(nT ave (ε)).</formula><p>In what follows, we first consider randomized algorithms that indeed incur a cost C ave (ε) = Θ(nT ave (ε)). Therefore in such algorithms, the goal will be to minimize T ave (ε). We will characterize the T ave (ε) for these algorithms and relate them to the 'mixing time' of certain reversible random walk on the graph G. This leads to the conclusion that for graphs with good expansion property the randomized algorithms are essentially optimal in terms of T ave (ε) as well. Thus, randomized algorithms will be optimal both in terms of T ave (ε) and C ave (ε) for graphs with good expansion property.</p><p>In contrast, for graphs with 'geometry' (e.g., ring graph) the computation time T ave (ε) can be quite poor due to its relation to the mixing time of reversible random walk. To overcome this limitation, we consider determinstic linear dynamics in the second part. We present a time-invariant deterministic algorithm, i.e., W (t) = W for all t ∈ N, that has optimal T ave (ε) and near optimal C ave (ε) for graphs with 'geometry'. This algorithm is built upon certain non-reversible random walk on graph G. As a by product, this construction provides the fastest mixing random walk (Markov chain) on a graph G with mixing time being of the order of the diameter of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Randomized Algorithms and Reversible Random Walks</head><p>This section describes an algorithm where W (t) is chosen randomly in each time t ∈ N. The T ave (ε) of this randomized algorithm will relate to the mixing time of a reversible random walk. Therefore, the performance of this algorithm will be effectively characterized by reversible random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Algorithm</head><p>Let P = [P ij ] be any n × n doubly-stochastic matrix that is network graph G conformant. Here, we assume that P is symmetric, i.e., P ij = P ji for all i, j. We assume that P is chosen such that it is aperiodic and irreducible. That is, a random walk with P as its transition matrix on G is ergodic and has the uniform distribution on G as its unique stationary distribution. We will call P the probability matrix corresponding to the algorithm, because we will choose W (t) according to an i.i.d. distribution with E[W (t)] related to P . Now we describe the distribution from which W (t) is chosen. By Birkhoff-Von Neumann's theorem(e.g. <ref type="bibr" target="#b29">[30]</ref>), a non-negative doubly-stochastic matrix P can be decomposed into at most n 2 (n 2 -2n + 1 to be precise) permutation matrices:</p><formula xml:id="formula_90">P = n 2 m=1 α m Π m , α m ≥ 0, n 2 m=1 α m = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define a random (matrix) variable Π with distribution</head><formula xml:id="formula_91">Pr(Π = Π m ) = α m , 1 ≤ m ≤ n 2 .</formula><p>As part of the algorithm, we sample a permutation matrix as per the distribution of Π every time t ∈ N independently. Let Π(t) be the permutation matrix sampled at time t. The Π(t) can be thought of as a 'directed' matching of vertices:</p><formula xml:id="formula_92">i is matched to j if Π ij (t) = 1. Here i matched to j (Π ij (t) = 1) is different from j matched to i (Π ji (t) = 1).</formula><p>As part of the algorithm, if i is matched to j at time t (i.e., Π ij (t) = 1) then i updates its estimate by setting it to be the average of its own current estimate and that of j. That is, y i (t + 1) = (y i (t) + y j (t))/2. In matrix form, this corresponds to</p><formula xml:id="formula_93">y(t + 1) = 1 2 (I + Π(t)) y(t).</formula><p>That is, W (t) = (I + Π(t))/2. This completes the description of the algorithm. Note that exactly two operations per node and 2n total operations are performed in each time t in the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance</head><p>Here, we characterize T ave (ε) of the above stated randomized algorithm. Later, we shall relate it to the mixing time of a random walk on G with transition matrix given by (I + P )/2.</p><p>Theorem 4.1. For any ε ∈ (0, 1/2), the ε-computation time of the randomized algorithm based on P described above is bounded by</p><formula xml:id="formula_94">0.5 log(3ε) -1 log λ -1 ≤ T ave (ε) ≤ 3 log ε -1 log λ -1 ,</formula><p>where λ = 1 2 (1 + λ 2 (P )).</p><p>The proof of Theorem 4.1 is stated in terms of upper bound and lower bounds separately, next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.1">Upper bound</head><p>Recall that, under the randomized algorithm, we have</p><formula xml:id="formula_95">y(t + 1) = W (t)y(t),<label>(4.2)</label></formula><p>where for a random permutation matrix Π(t)</p><formula xml:id="formula_96">W (t) = 1 2 (I + Π(t)) . (4.3)</formula><p>Note that W (t) is a doubly stochastic matrix for all t. That is, it preserves the 1 norm across t, i.e.</p><formula xml:id="formula_97">y(t) T 1 = y(0) T 1 = n i=1 x i . (4.4)</formula><p>We wish to bound how quickly y(t) → x ave 1. That is, we wish to find out how quickly, the 'error' z(t) ∆ = y(t)x ave 1 goes to 0 in terms of its 2 norm. For this, we will compute the expectation of z(t) T z(t) and use Markov's inequality to obtain the desired conclusion.</p><p>To this end, let us first compute E[z(t) T z(t)]. Recall that, z(t) = y(t)x ave 1 and W (t)1 = 1. Therefore,</p><formula xml:id="formula_98">W (t)z(t) = W (t)y(t) -x ave W (t)1 = y(t + 1) -x ave 1 = z(t + 1).</formula><p>Therefore, for any t &gt; 0</p><formula xml:id="formula_99">z(t) T z(t) = z(t -1) T W (t -1) T W (t -1)z(t -1). (4.5) Now, W (t -1) is independent of z(t -1). Therefore, E[z(t) T z(t)|z(t -1)] = z(t -1) T E[W (t -1) T W (t -1)]z(t -1). (4.6)</formula><p>Due to the time-invariance of the distributions of W (•), we have that</p><formula xml:id="formula_100">E[W (t -1) T W (t -1)] = E[W (0) T W (0)]. Consider the following. W (0) T W (0) = 1 4 I + Π(0) T (I + Π(0)) = 1 4 2I + Π(0) + Π(0) T . (4.7)</formula><p>Therefore,</p><formula xml:id="formula_101">W ∆ = E[W (0) T W (0)] = 1 2 I + P , (<label>4.8)</label></formula><p>we recall that P = (P + P T )/2. Since P is symmetric, we have P = P . The doubly stochasticity of P implies that of W . Similarly, irreducibility and aperiodicity of P implies that of W . Hence, 1 is the eigenvector with the unique largest eigenvalue 1 of W . W is a symmetric matrix. By the variational characterization of eigenvalues, it follows that</p><formula xml:id="formula_102">z(t -1) T W z(t -1) ≤ λz(t -1) T z(t -1),<label>(4.9)</label></formula><p>where λ is the second largest eigenvalue of W . It should be noted that all eigenvalues of W are non-negative due to its form (4.8). Also it follows from the property of eigenvalues that</p><formula xml:id="formula_103">λ = 1 2 (1 + λ 2 (P )) ,</formula><p>where λ 2 (P ) is the second-largest eigenvalue of P . Now recursive application of (4.5) and (4.9) yields</p><formula xml:id="formula_104">E[z(t) T z(t)] ≤ λ t z(0) T z(0). (4.10) Now, z(0) T z(0) = x T x -nx 2 ave ≤ x T x. (4.11)</formula><p>From (4.10), (4.11) and an application of Markov's inequality, we have Pr</p><formula xml:id="formula_105">y(t) -x ave 1 x ≥ ε = Pr z(t) T z(t) x T x ≥ ε 2 ≤ ε -2 E[z(t) T z(t)] x T x = ε -2 λ t . (4.12)</formula><p>From (4.12), it follows that</p><formula xml:id="formula_106">T ave (ε) ≤ 3 log ε -1 log λ -1 = 3 log ε log λ .</formula><p>This completes the proof of upper bound claimed in Theorem 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.2">Lower bound</head><p>Here, we present the proof of lower bound on T ave (ε) claimed in Theorem 4.1. Recall from the arguments above that z(t + 1) = W (t)z(t).</p><p>Therefore,</p><formula xml:id="formula_107">E[z(t)] = W t z(0). (4.13)</formula><p>By definition, W = (I + P )/2 and hence it is a symmetric positivesemidefinite doubly stochastic matrix. It is irreducible and aperiodic as well. Hence it has non-negative real valued eigenvalues</p><formula xml:id="formula_108">1 = λ 1 (W ) &gt; λ 2 (W ) ≥ • • • ≥ λ n (W ) ≥ 0,</formula><p>and corresponding orthonormal eigenvectors</p><formula xml:id="formula_109">1 √ n 1, v 2 , . . . ,v n . Select x = 1 √ 2 1 √ n 1 + v 2 ⇒ z(0) = 1 √ 2 v 2 .</formula><p>For this choice of x, x = 1. Now from (4.13),</p><formula xml:id="formula_110">E[z(t)] = 1 √ 2 λ t v 2 , (<label>4.14)</label></formula><p>where as per earlier notation λ = 1 2 (1 + λ 2 (P )) = λ 2 (W ). For this particular choice of x, we are going to lower bound the ε computation time by lower bounding E[ z(t) 2 ] and using Lemma 4.2 as stated below. By Jensen's inequality and (4. <ref type="bibr" target="#b13">14)</ref>,</p><formula xml:id="formula_111">E[z(t) T z(t)] ≥ n i=1 E[z i (t)] 2 = E[z(t)] T E[z(t)] = 1 2 λ 2t (W )v T 2 v 2 = 1 2 λ 2t . (4.15)</formula><p>Lemma 4.2. Let X be a random variable such that 0 ≤ X ≤ B. Then, for any 0 &lt; ε &lt; B,</p><formula xml:id="formula_112">Pr(X ≥ ε) ≥ E[X] -ε B -ε . Proof. E[X] ≤ ε Pr(X &lt; ε) + B Pr(X ≥ ε) = Pr(X ≥ ε)(B -ε) + ε.</formula><p>Rearranging terms gives us the lemma.</p><p>From (4.14), z(t) 2 ≤ z(0) 2 ≤ 1/2. Hence Lemma 4.2 and (4.15) imply that for ε ∈ (0, 1/2)</p><formula xml:id="formula_113">Pr ( z(t) ≥ ε) &gt; ε, (4.16) for t ≤ 0.5 log(3ε) -1 log λ -1 .</formula><p>This implies the desired lower bound in Theorem 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Relation to Mixing Time</head><p>We describe the relation between the ε-computation time, T ave (ε) of the randomized algorithm based on P and the ε-mixing time based on total variation distance, τ (ε, W ) (defined in Preliminaries), of the random walk on G with transition matrix given by W = (I + P )/2.</p><p>Theorem 4.3. Let ε = n -δ for some δ &gt; 0. Then T ave (ε) of the randomized algorithm based on P and the mixing time τ (ε, W ) of the random walk with transition matrix W are related as</p><formula xml:id="formula_114">T ave (ε) = Θ log n + τ (ε, W ) .</formula><p>Proof. Recall that W = (I + P )/2 has all eigenvalues non-negative and real valued. Therefore, the second largest (in norm) eigenvalue, λ of W is equal to (1 + λ 2 (P ))/2. Since P has non-negative entries, and hence the trace of P is non-negative, the sum of all eigenvalues of P is non-negative. Since the largest eigenvalue has value 1, it must be that λ 2 (P ) ≥ -1/n. Therefore, λ ≥ (1 -1/n)/2 ≥ 1/4 for all n ≥ 2 and a network must have at least two nodes! Now, for any x ∈ (0, 1) it follows that</p><formula xml:id="formula_115">x 2 ≤ log(1 + x) ≤ x ⇒ log(1 + x) = Θ(x).</formula><p>Given this, it follows that</p><formula xml:id="formula_116">log λ = log(1 -(1 -λ)) = Θ(1 -λ).</formula><p>Therefore,</p><formula xml:id="formula_117">T ave (ε) = Θ log ε -1 1 -λ .</formula><p>For ε = n -δ , it follows that T ave (ε) = Ω(log n). Also, for such an ε,</p><formula xml:id="formula_118">T ave (ε) = Θ log ε -1 + log n 1 -λ = Ω(τ (ε, W )),<label>(4.17)</label></formula><p>where the last inequality follows from Preliminaries. From the two lower bounds on T ave (ε), we have for</p><formula xml:id="formula_119">ε = n -δ T ave (ε) = Ω(log n + τ (ε, W )). (4.18)</formula><p>Use of the lower bound on mixing time in terms of the spectral gap for reversible random walks as explained in Preliminaries, suggests that for the choice of</p><formula xml:id="formula_120">ε = n -δ , τ (ε, W ) = Ω log ε -1 1 -λ = Ω(log n).</formula><p>Therefore, from (4.17) it follows that</p><formula xml:id="formula_121">T ave (ε) = O(log n + τ (ε, W )).</formula><p>This completes the proof of Theorem 4.3.  The plot assumes ε = n -δ for some δ &gt; 0. The scale of the axis is in order notation. As shown in the figure, for P such that τ (ε, W ) = o(log n), T ave (ε) = Θ(log n); for P such that τ (ε, W ) = Ω(log n), T ave (ε) = Θ(τ (ε, W )). Thus the mixing time of the random walk based on W essentially characterizes the computation time of the algorithm.</p><p>Remark 4.1. If one utilized a deterministic algorithm, i.e., using W (t) = W every time, then the convergence will be dictated by τ (ε, W ). Thus, Theorem 4.3 suggests that by using randomization, the computation time suffers a minor additive penalty of log n; but it potentially saves computations per iteration. Specifically the deterministic algorithm based on W will require |W | o operations per iteration compared to Θ(n) operations per iteration under randomized algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Applications</head><p>Here we will utilize Theorems 4.1 and 4.3 to evaluate the performance of randomized algorithms on various graph models. We describe applications to four classes of graph models as stated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.1">Complete graph</head><p>The complete graph represents situation when all nodes can communicate with each other. That is, essentially no graphical communication constrains. For a complete graph of n nodes, a natural symmetric probability matrix is P = [1/n], i.e., all entries being equal to 1/n. For such a matrix, as explained in Preliminaries, the τ (ε, P ) = O(1) for all ε &gt; 0. Therefore, by Theorem 4.3 the T ave (ε) = O(log n) for all ε = Ω(1/poly(n)). That is, the randomized algorithm performs essentially as fast as possible; both in terms of T ave (ε) and total cost, C ave (ε).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.2">Ring graph</head><p>The ring graph of n nodes is formed by placing n nodes on a circle (or a ring) and connecting each node to two of its nearest neighbors. This is essentially the most communication constrained graph. For a ring graph of n nodes, a natural symmetric probability matrix P , obtained by the Metropolis Hasting method, is as follows: for each i, P ii = 1/2, P ii + = P ii -= 1/4 where i + and i -represent neighbors of i on either side. As established in Preliminaries, the mixing time τ (ε, P ) = Ω(n 2 ) for any ε ∈ (0, 1/2) and is O(n 2 log n) for ε = Ω(1/poly(n)). Therefore, T ave (ε) essentially scales as n 2 and C ave (ε) scales as n 3 . This clearly seems wasteful as a simple graph like a ring should have T ave (ε) more like n and not n 2 . As we shall discuss in the next section, this waste is inherently due to P being symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.3">Expander graph</head><p>As before, consider d-regular expander graphs. The natural probability matrix P has mixing time τ (ε, P ) = O(log n) for ε = Ω(1/poly(n)) as discussed in Preliminaries. Thus, again T ave (ε) = O(log n) for all ε = Ω(1/poly(n)). That is, the randomized algorithm performs essentially as fast as possible; both in terms of T ave (ε) and the total cost, C ave (ε) (which is O(n log n)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.4">Geometric random graph</head><p>The Geometric random graph over n nodes is formed by placing nodes uniformly at random in a geographic area and then connecting nodes within distance r = r(n), the connectivity radius. Recall that the detailed description is provided in Preliminaries. As established there, the natural random walk has the fastest possible mixing time scaling as n/r 2 . However, the diameter of G(n, r) is of order √ n/r. This suggests that the randomized algorithm is wasteful in such graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deterministic Algorithms and Non-Reversible Random Walks</head><p>The randomized algorithm described above has T ave (ε) essentially scaling like the mixing time. Though it performs minimal Θ(n) operations per unit time, its T ave (ε) can be rather poor for certain graphs. Specifically, for a complete graph or more generally graphs with good expansion, the randomized algorithm performs almost optimally both in terms of T ave (ε) and hence C ave (ε). However, for graphs with 'geometry', like the ring or the geometric random graph, it performs rather poorly. And, most graphs arising in practice involving wireless networks or physical entities do have 'geometry', such as wireless sensor networks deployed in some geographic area <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> or a nearest neighbor network of unmanned vehicles <ref type="bibr" target="#b61">[63]</ref>. In this section, we will put forth a determinstic algorithm based on a novel construction of random walks on geometric graphs to overcome the inefficiency of the randomized algorithm. Before we proceed further, some understanding of inherent reasons for the inefficiency of the randomized algorithm will be useful. Now the poor performance of the randomized algorithm on graphs with geometry is inherently related to the poor mixing property of the symmetric random walks on such graphs. To this end, recall from Preliminaries that for any random walk with transition matrix P , its mixing time is bounded as</p><formula xml:id="formula_122">1 Φ(P ) ≤ H(P ) ≤ O log n Φ 2 (P ) .</formula><p>In general, in most graphs with geometry that are of interest, the mixing time of the reversible walk P scales like 1/Φ 2 (P ). The conductance Φ(P ) relates to diameter D of a graph G as 1/Φ(P ) ≥ D. Therefore, in such situations the mixing time of a reversible random walk is likely to scale like D 2 , the square of the diameter. Indeed, Diaconis and Saloff-Coste <ref type="bibr" target="#b16">[17]</ref> established that for a class of graphs with geometry (i.e., polynomial growth or finite doubling dimension) the mixing time of any reversible random walk scales like at least D 2 and it is achieved by the Metropolis-Hastings' approach. Thus, reversible random walks result in rather poor performance for graphs with geometry, i.e. its mixing time is far from the best hope, the diameter D. This suggests that in order to design an efficient linear dynamics based algorithm on graphs with 'geometry', we need to look for an algorithm that relates to non-reversible random walks on the graph. It should be noted that Theorem 4.1 implies that even for non-symmetric (i.e., non-reversible) P the bound on T ave (ε) is governed by the second largest eigenvalue of (or mixing time corresponding to) 1 2 (I + P/2 + P T /2), which is sym-metric. Therefore, it is essential to consider a non-randomized, i.e., deterministic algorithm. Next, we describe a motivating example of a fast mixing random walk design based on non-reversibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Non-Reversible Random Walk: An Example</head><p>The example we describe here is based on work by Diaconis et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>They introduced a clever construction of a non-reversible random walk on the ring (and more generally ring-like) graph. To this end, consider Figure <ref type="figure" target="#fig_1">4</ref>.2(a) which describes the symmetric random walk on a ring. The non-reversible random walk constructed in <ref type="bibr" target="#b15">[16]</ref> runs on the lifted ring graph, which is denoted G 2 in Figure <ref type="figure" target="#fig_1">4</ref>.2(b). Here, by lifting we mean making additional copies of the nodes of the original graph and adding edges between some of these copies while preserving the original graph topology. And the probability of changing from the inner circle to the outer circle and vice versa are 1/n at each time. By defining transitions in this way, the stationary distribution is also preserved, i.e., the sum of stationary distributions of copies is equal to their original one. Somewhat surprisingly, they showed that this non-reversible random walk has linear mixing time O * (n).   <ref type="foot" target="#foot_3">3</ref> where π and π are the stationary distributions of P and P , respectively.</p><formula xml:id="formula_123">1/2 1/2 1/4 1/4 1-1/n 1-1/n 1/n 1/n (a) (b)</formula><formula xml:id="formula_124">u ∈ V , π(f -1 (u) ∩ T ) = (1/2)π u ,</formula><p>As we shall see later, due to property (a) in the definition it will be possible to simulate the pseudo-lifting P in the original graph G in a distributed manner, i.e., run a linear dynamics based on P on G using message exchanges local to G. Furthermore, property (b) suggests that (by concentrating on set T ), it is possible to simulate the uniform distribution exactly using the pseudo-lifting. Next, we present a construction of a pseudo-lifting with mixing time of order of D, the diameter of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.1">Construction</head><p>For a given random walk P , we will construct the pseudo-lifted random walk P of P . It may be assumed that P is given by the Metropolis-Hasting method. Without loss of generality (or rather to achieve generality), here we assume P to be the transition matrix of a random walk with an arbitrary stationary distribution π = [π i ]. Note that when specialized to π = (1/n)1, we obtain the P of interest. In what follows, we will construct the pseudo-lifted graph G by adding vertices and edges to G, and decide the values of the ergodic flows Q on G, which defines its corresponding random walk P , since recall that ergodic flow along an edge (û, v) is πu Pûv .</p><p>To this end, first select an arbitrary node v. Now, for each w ∈ V , there exist paths P wv and P vw , from w to v and v to w, respectively. We will assume that all the paths are of length D: this can be achieved by repeating the same node or using self-loops. Now, we construct a pseudo-lifted graph Ĝ starting from G.</p><p>First, create a new node v which is a copy of the chosen vertex v. Then, for every node w, add directed paths P wv , a copy of P wv , from w to v . Similarly, add P vw (a copy of P vw ) from v to w. Each addition creates D -1 new interior nodes. Thus, we have essentially created a virtual star topology using the paths of the old graph and added O(nD) new nodes (note that, every new node is a copy of an old node). Now, we define the ergodic flow Q for this graph Ĝ as follows: for an edge (i, j),</p><formula xml:id="formula_125">Q ij = δ 2D π w , if (i, j) ∈ E(P wv ) or E(P vw ) (1 -δ)Q ij , if (i, j) ∈ E(G),</formula><p>where δ ∈ (0, 1) is a constant, that will be decided later. It is easy to</p><formula xml:id="formula_126">check that ij Q ij = 1, j Q ij = j Q ji .</formula><p>Hence it defines a a random walk on Ĝ. The stationary distribution of this pseudo-lifting is</p><formula xml:id="formula_127">π i =        δ 2D π w , if i ∈ (V (P wv ) ∪ V (P vw ))\{w, v } 1 -δ + ε 2D π i , if i ∈ V (G) δ 2D , if i = v (4.19)</formula><p>Given the above definition of Q and corresponding stationary distribution π, it satisfies the pseudo-lifting definition if we choose ε such that 1/2 = ε (1 -(1/2D)) and set T = V (G), i.e., T is the set of old nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.2">Mixing time</head><p>We claim the following bound on the mixing time of the pseudo-lifting we constructed.</p><p>Theorem 4.4. The mixing time of the random walk P (equivalently, defined by</p><formula xml:id="formula_128">Q) is τ (ε, P ) = O(D log ε -1</formula><p>) for any ε ∈ (0, 1).</p><p>Proof. We will design a stopping rule where the distribution of the stopping node is π, and analyze its expected length. Then, use of the results from Preliminaries will lead to the bound on τ (ε, P ). Refer to Preliminaries for details on the use of stopping rule for bounding mixing time. Now the description of stopping rule. Starting from any node, let the random walk continue until it reaches v . Then roll a (four sides) die X with the following probability.</p><formula xml:id="formula_129">X =              0, with probability δ 2D 1, with probability δ(D-1) 2D 2, with probability 1 -δ + δ 2D 3, with probability δ(D-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D</head><p>Here δ ∈ (0, 1) is a constant, whose value will be chosen later. Depending upon the value of X, the walk will be stopped at a node to be decided as follows.</p><p>• X = 0: Stop at v . The probability for stopping at v is Pr[X = 0] = δ/2D, which is exactly π v .</p><p>• X = 1: Walk a directed path P vw , and choose an interior node of P vw uniformly at random, and stop there. For a given w, it is easy to check that the probability for walking P vw is π w . There are D -1 many interior nodes, hence, for an interior node i of P vw , the probability for stopping at i is</p><formula xml:id="formula_130">Pr[X = 1] × π w × 1 D -1 = δ 2D π w = π i .</formula><p>• X = 2: Stop at the end node w of P vw . The probability for stopping at w is</p><formula xml:id="formula_131">Pr[X = 2] × Pr[walk P vw ] = 1 -δ + δ 2D × π w = π w .</formula><p>• X = 3: Walk until getting a directed path P wv , and choose an interior node of P wv uniformly at random, and stop there. Until getting a directed path P wv , the pseudo-lifted random walk defined by Q is same as the original random walk. Since the distribution w ∈ V (G) of the walk at the end of the previous step is exactly π, it follows that the distribution π over the nodes of V (G) is preserved under this walk till walking on P wv . Calculations similar to those done in the case X = 1, we find that the probability of stopping at the interior node i of P wv is π i .</p><p>Therefore, we have established the existence of a stopping rule that takes an arbitrary starting distribution to the stationary distribution π. Now, this stopping rule has average length O(D/δ): since the probability of getting on a directed path P wv at w is Therefore, it follows that τ (ε, P ) = O(D log ε -1 /δ). Since δ is parameter of choice, we obtain that τ (ε, P ) = O(D log ε -1 ) (e.g., choose δ = 0.1). This completes the proof.</p><formula xml:id="formula_132">δ 2D /(1 -δ + (δ/2D)) = Θ(δ/D),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Non-Reversible Random Walks: The Use of Geometry</head><p>The graph topologies arising in practice, such as that of a wireless sensor network deployed in some geographic area or a nearest neighbor network of unmanned vehicle <ref type="bibr" target="#b61">[63]</ref>, possess geometry and are far from being expanders. A good model for graphs with geometry is a class of graphs with finite doubling dimension which is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.2 (Doubling Dimension</head><p>). Consider a metric space M = (X , d), where X is the set of point endowed with a metric d. Given x ∈ X , define a ball of radius r ∈ R + around x as B(x, r) = {y ∈ X : d(x, y) &lt; r}. Define</p><formula xml:id="formula_133">ρ(x, r) = inf K ∈ N: ∃ y 1 , . . . , y K ∈ X , B(x, r) ⊂ K i=1 B(y i , r/2) .</formula><p>Then, the ρ(M) = sup x∈X ,r∈R + ρ(x, r) is called the doubling constant of M and log 2 ρ(M) is called the doubling dimension of M. The doubling dimension of a graph G = (V, E) is defined with respect to the metric induced on V by the shortest path metric.</p><p>The ring graph has O(1) doubling dimension. The construction of the previous section leads to P with τ (ε, P ) scaling as D and | P | o scaling as nD for a general graph. That is, for a ring graph it will lead to (explained in detail in the next section) a deterministic linear computation algorithm with T ave (ε) scaling as n but C ave (ε) scaling as n 3 . That is, the C ave (ε) of this algorithm will be the same as that of randomized algorithm. Therefore, we wish to improve the construction of the previous section for graphs with geometry, like a ring, by utilizing their structure in terms of the size of P , i.e., | P | o .</p><p>Here, we will design a pseudo-lifting with efficient size for graphs with finite doubling dimension. To this end, recall that the basic idea for the construction of the pseudo-lifting is creating a virtual star topology using paths from every node to a fixed root, and the length of paths grows the size of the pseudo-lifting. For example, a caricature of this in the context of ring graph is shown in Figure <ref type="figure" target="#fig_1">4</ref>.3(a). To reduce the overall length of paths, we make clusters of nodes such that nodes in each cluster are close to each other, and pick a sub-root node in each cluster. Then, we build a star topology in each cluster around its sub-root and connect every sub-root to the root. This creates a 'hierarchical' star topology (or 'tree' topology). A caricature example of such a construction is shown for a line graph in Figure <ref type="figure" target="#fig_1">4</ref>.3(b). Since it needs short path lengths in each cluster, the overall length of paths decreases.</p><p>For a good clustering, we need to decide which nodes would become sub-roots. A natural candidate for them is the R-net Y ⊂ V of graph G defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.3 (R-net). For a given graph</head><formula xml:id="formula_134">G = (V, E), Y ⊂ V is an R-net if (a)</formula><p>For every v ∈ V , there exists u ∈ Y such that the shortest path distance between u and v is at most R. (b) The distance between any two y, z ∈ Y is more than R.</p><p>Such an R-net can be found in G greedily. As explained in the proof of Lemma 4.6, the small doubling dimension of G guarantees the existence of a good R-net for our purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.1">Construction</head><p>For a given random walk P , we will construct the pseudo-lifted random walk P of P using the hierarchical star topology. As before, let π and G = (V, E) be the stationary distribution and the underlying graph of P , respectively. Like the construction in Section 4.3.2.1, the pseudolifted graph G is constructed by extending G; P is defined by means of appropriate ergodic flows Q on G.</p><p>Given an R-net Y , match each node w to the nearest y ∈ Y (breaking ties arbitrarily). Let C y = {w| w matched to y} for y ∈ Y . Clearly, V = ∪ y∈Y C y . Finally, for each y ∈ Y and for any w ∈ C y we have paths P wy , P yw between w and y of length exactly R. Also, for each y ∈ Y , there exists P yv , P vy between y and v of length exactly D (we allow the repetition of nodes to reach this length exactly). Now, we construct the pseudo-lifted graph Ĝ. As the construction in Section 4.3.2.1, select an arbitrary node v ∈ V and create its copy v again. Further, for each y ∈ Y create two copies y 1 and y 2 . Now, add directed paths P wy , a copy of P wy , from w to y 1 and add P yv , a copy of P yv , from y 1 to v . Similarly, add P vy and P yw between v , y 2 and y 2 , w. This construction adds 2D|Y | + 2Rn edges to G, giving Ĝ. Now, the ergodic flow Q on Ĝ is defined as follows: for any (i, j) of Ĝ,</p><formula xml:id="formula_135">Q ij =        δ 2(R+D) π w , if (i, j) ∈ E(P wy ) or E(P yw ), δ 2(R+D) π(C y ), if (i, j) ∈ E(P yv ) or E(P vy ), (1 -δ)Q ij , if (i, j) ∈ E(G),</formula><p>where π(C y ) = w∈Cy π w and δ ∈ (0, 1) is a constant to be decided later. It can be checked that</p><formula xml:id="formula_136">ij Q ij = 1, j Q ij = j Q ji .</formula><p>Hence it defines a random walk on Ĝ. The stationary distribution of this pseudolifted chain is</p><formula xml:id="formula_137">π i =                δ 2(R+D) π w , if i ∈ (V (P wy ) ∪ V (P yw ))\{w, y 1 , y 2 }, δ 2(R+D) π(C y ), if i ∈ (V (P yv ) ∪ V (P vy ))\{v }, 1 -δ(1 - δ 2(R+D) ) π i , if i ∈ V (G), δ 2(R+D) , if i = v .</formula><p>To establish this as the pseudo-lifting of the original random walk P , consider T = V (G) and δ, where 1/2 = δ (1 -1/(2(R + D))). The G has exactly |E| + 2Rn + 2D|Y | edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.2">Mixing time and size</head><p>We analyze performance of the above stated construction of P in terms of mixing time and size. First, we establish that the mixing time of O(D) is preserved under this efficient construction. Proof. Consider the following stopping rule. Walk until visiting v , and roll a (six sided) die X with the following probability.</p><formula xml:id="formula_138">X =                          0, with probability δ 2(R+D) , 1, with probability δD 2(R+D) , 2, with probability δ(R-1)</formula><p>2(R+D) , 3, with probability 1δ(1 -δ 2(R+D) ), 4, with probability δ(R-1) 2(R+D) , 5, with probability δD 2(R+D) . Depending on the value of X,</p><p>• X = 0: Stop at v . • X = 1: Walk on a directed path P vy , and choose its interior node uniformly at random, and stop there. • X = 2: Walk until getting a directed path P yw , and choose its interior node uniformly at random, and stop there. • X = 3: Walk until getting to an old node in V (G), and stop there. • X = 4: Walk until getting a directed path P wy , and choose its interior node uniformly at random, and stop there. • X = 5: Walk until getting a directed path P yv , and choose its interior node uniformly at random, and stop there.</p><p>It can be checked, using arguments similar to that in the proof of Theorem 4.4, that the distribution of the stopped node is precisely π. Also, we can show that the expected length of this stopping rule is </p><formula xml:id="formula_139">O((R + D)/δ) = O(D/δ) = O(D).</formula><formula xml:id="formula_140">| E| = O(Dn 1-1 ρ+1 ).</formula><p>Proof. The property of the doubling dimension graph implies that there</p><formula xml:id="formula_141">exists an R-net Y such that |Y | ≤ (2D/R) ρ (cf. [2]). Consider R = D2 ρ ρ+1 n -1 ρ+1 . This is an appropriate choice because R = D2 ρ ρ+1 n -1 ρ+1 &gt; Dn -1 ρ+1 &gt; n 1 ρ -1 ρ+1 &gt; 1 (the second inequality is from n ≤ D ρ ).</formula><p>Given this, the size of the pseudo-lifted graph Ĝ is</p><formula xml:id="formula_142">| E| = |E| + 2Rn + 2D|Y | ≤ |E| + 2D 2 ρ ρ+1 n 1 ρ+1 n + 2D 2 n 1 ρ+1 2 ρ ρ+1 ρ = |E| + O(Dn 1-1 ρ+1 ). Since |E| = O(n) and D = Ω(n 1/ρ ), we have that | Ê| = O(Dn 1-1 ρ+1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Back to Averaging: Deterministic Algorithm</head><p>We wish to compute x ave = ( i x i )/n where x i ∈ R is the value of node i on a given graph G = (V, E). In what follows, we consider x i ≥ 0. This is without loss of generality since one may run an algorithm to compute the average of non-negative valued and negative valued numbers separately. Now let P be a doubly stochastic matrix over G of doubling dimension ρ such as the one obtained through the Metropolis-Hasting method. Let P be the efficient pseudo-lifting of P over the lifted graph Ĝ = ( V , Ê) with stationary distribution π over V . Recall that each node of V is part of V . Let V (G) ⊂ V denote these nodes in what follows. Now consider the following deterministic linear algorithm over V based on P . Let ŷ(0) = x, with x = [x i ] where xi = x i for i ∈ V (G) and xi = 0 otherwise. Now perform the following iterative linear computation over Ĝ:</p><formula xml:id="formula_143">ŷ(t + 1) = P ŷ(t).</formula><p>The above algorithm is described for graph Ĝ, but our interest is in running the algorithm over G. Therefore, we describe an implementation of the above linear algorithm based on P on G.</p><p>As noted earlier, V (G) ⊂ V are nodes V of G. Now, under the above described linear algorithm, each node of V communicates with its neighbors connected via Ê. But recall û and v are connected in Ĝ only if they are 'copies' of nodes u and v such that (u, v) ∈ E. Therefore, if each node û ∈ V is 'simulated' by node u, where û is a copy of u, then all communications performed in each iteration based on P can be performed as local communications over the graph G. That is, G can indeed implement the linear dynamics based on P in a distributed manner. Finally, observe that each node in V (G) ⊂ V is hosted at its corresponding original node in V of G in the above implementation. Therefore, if each node i ∈ V (G) learns the estimate x ave then in the above implementation each node in V learns the estimate of x ave as well. This completes the description of the algorithm. Now, we describe the convergence properties of the estimates of nodes in V (G).</p><p>Lemma 4.7. Under the above algorithm, consider the estimates of nodes</p><formula xml:id="formula_144">V = V (G) ⊂ V . That is, for i ∈ V (G) consider ŷi (t). Then, ŷi (t) → x ave /2 as t → ∞. Further, for t ≥ τ (ε/ √ n, P ) and i ∈ V (G), |ŷ i (t) -x ave /2| ≤ ε x 2 .</formula><p>Proof. Since π is the left eigenvector of P with the largest (unit) eigenvalue, i.e., πT P = πT ; we have</p><formula xml:id="formula_145">lim t→∞ e T i P t → πT , ∀ i,</formula><p>where e i is the vector with its ith entry being 1 and the rest being 0. This implies that, for any i</p><formula xml:id="formula_146">lim t→∞ P t ij = πj , ∀ i. (4.20)</formula><p>Further, by the definition of the mixing time, it follows that for t ≥ τ (ε, P ),</p><formula xml:id="formula_147">P t ji -πi ≤ ε, ∀ j. (4.21)</formula><p>Now ŷ(t + 1) = P ŷ(t). That is, ŷ(t) = P t ŷ(0). Therefore, for any i</p><formula xml:id="formula_148">ŷi (t) = j P t ij ŷj (0). Also, j ŷj (0)π j = j∈V (G) x j πj = j∈V (G)</formula><p>x j 1 2n = x ave /2.</p><p>In the above equations, we have used the fact that ŷj (0) = 0 if j ∈ V \V (G) and ŷj (0) = x j for j ∈ V (G); πj = 1/2n for j ∈ V (G) by definition of pseudo-lifting. Putting the above together, for i ∈ V (G)</p><formula xml:id="formula_149">|ŷ i (t) -x ave /2| = j P t ij ŷj (0) - j πj ŷj (0) ≤ j ŷj (0)| P t ij -πj |. (4.22)</formula><p>From (4.20) to (4.22) it follows that for t ≥ τ (ε, P ) and for i ∈ V (G)</p><formula xml:id="formula_150">|ŷ i (t) -x ave /2| ≤ ε j ŷj (0) = ε x 1 = nx ave ≤ ε √ n x 2 . (4.23)</formula><p>In above, we have used the fact that for any non-negative valued n-dimensional vector x, √ n x 2 ≥ x 1 . By choosing ε/ √ n in place of ε we obtain the desired claim and we complete the proof. </p><formula xml:id="formula_151">k ) O(D) : O(n 1 k ) D : n 1 k Size(dbl. dim.)ρ: k-grid graph Θ(n) : Θ(n) O(n ρ ρ+1 D) : O(n 1+ 1 k(k+1) ) n : n Total # of operations: k-grid graph Ω n Φ 2 (P ) : O * (n 1+ 2 k ) O(n ρ ρ+1 D 2 ) : O * (n 1+ k+2 k(k+1) ) nD : n 1+ 1 k<label>2</label></formula><p>Note: Optimal denotes lower bound for any distributed or message-passing algorithm.</p><p>The above suggests that, if we restrict our attention to nodes in V (G), then their estimates converge to x ave . Specifically, let the n-dimensional vector ỹ(t) = [ỹ i (t)] ∈ R n be defined as ỹi (t) = 2ŷ i (t) for i ∈ V (G). Then, from (4.23) it follows that for t ≥ τ (ε/2n, P )</p><formula xml:id="formula_152">ỹ(t) -x ave 1 2 ≤ ε x 2 . (<label>4.24)</label></formula><p>Thus, we have T ave (ε) ≤ τ (ε/2n, P ). Therefore,</p><formula xml:id="formula_153">C ave (ε) = O | P | o T ave (ε) = O | P | o τ (ε/2n, P ) .</formula><p>This completes the description and analysis of the linear deterministic algorithm based on P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Application</head><p>Here, we describe application of the non-reversible random walk based determinstic algorithm described above for a class of graphs with geometry. As discussed above, for graphs with finite doubling dimension, our algorithm performs well. Specifically, for a graph with doubling dimension ρ and diameter D, the above results imply that the deterministic algorithm has T ave (ε) scaling as D and C ave (ε) scaling as D times O * (Dn 1-1/(ρ+1) ). Now any algorithm must take O(D) as T ave (ε) and C ave (ε) as Dn. Thus, the effective 'loseness' in our algorithm compared to any other algorithm is no worse than O(Dn</p><formula xml:id="formula_154">-1 1+ρ</formula><p>). Now the quintessential example of graphs with finite doubling dimension is the k-dimensional grid; one-dimensional grid being a line graph (or its symmetrized version is the ring graph). For k-dimensional grid, the diameter D scales as n 1/k . Therefore, for our algorithm T ave (ε) scales as Θ(n 1/k ) and the total computation C ave (ε) scales as O(n 1+(1/k)+(1/(k(k+1))) ). In contrast, for any algorithm T ave (ε) must scale at least as n (1/k) and C ave (ε) must scale at least as n 1+(1/k) . Thus, for a k-dimensional grid the possible loss in our algorithm is no more than O(n 1/(k(k+1)) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary</head><p>We considered linear dynamics based algorithms for the computing average of numbers in the network graph. First, we described a randomized algorithm in which a random permutation is chosen everytime so that the effective induced matrix corresponds to a certain symmetric doubly stochastic matrix on the graph. Equivalently, the algorithm corresponds to a certain randomized pair-wise 'averaging' based on a reversible random walk on the network graph. This algorithm performs minimal number of operations, i.e., on the order of the number of nodes n, per unit time in any network. The computation time scales as log n plus the mixing time of a related reversible random walk on the graph. Therefore, for graphs with 'good expansion' such as the complete graph or the constant degree expander, the computation time is essentially the minimal O(log n). However, this algorithm performs very poorly over graphs with 'geometry'. This is because the mixing time of a reversible random walk is inherently very large on graphs with geometry.</p><p>Motivated to improve performance, we considered algorithms based on non-reversible random walks. The performance of the randomized algorithm is inherently related to the reversible random walk, and hence we considered a deterministic algorithm. This comes at an additional cost of an increased number of operations per unit time. Therefore, we considered the question of designing non-reversible random walks on any graph with minimal mixing time and minimal 'size'. The notion of pseudo-lifting led to the design of non-reversible random walks with mixing times of the order of the diameter and small size. For graphs with 'geometry', i.e., graphs with finite doubling dimension, this was further improved in terms of size using the graph structure.</p><p>The averaging algorithms based on such non-reversible random walks take minimal number of iterations of the order of the diameter for any graph. For graphs with doubling dimension they have near optimal overall computation cost. In summary, the randomized algorithm is near optimal for graphs with good expansion, i.e., graphs without geometry; the deterministic algorithm based on a non-reversible random walk is near optimal for graphs with geometry.</p><p>We note that the deterministic construction based on pseudo-lifted Markov chain does incur a reasonable (polynomial in n) overhead. However, this cost can be thought of as amortized over time if the similar construction is used for many computations. Now, if topology is dynamic then it requires re-construction of pseudo-lifted chain often. Understanding the robustness of the pseudo-lifted chain and constructing pseudo-lifted chain with minimal overhead are natural questions of interest for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Historical Notes</head><p>The recently emerging network paradigms such as sensor networks, peer-to-peer networks and surveilance networks of unmanned vehicles have led to the requirement of designing distributed, iterative and efficient algorithms for estimation, detection, optimization and control. Such algorithms provide scalability and robustness necessary for the operation of such highly distributed and dynamic networks. Motivated by applications of linear estimation in sensor networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b67">69]</ref>, information exchange in peer-to-peer networks <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b53">55]</ref> and reaching consensus in a network of unmanned vehicles <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b61">63]</ref>, we considered the problem of computing the average of numbers in a given network in a distributed manner. Specifically, we considered the class of algorithms for computing the average using distributed linear iterations. This approach was pioneered by Tsitsiklis et al. <ref type="bibr" target="#b67">[69]</ref>. In the applications of interest, the rate of convergence of the algorithm strongly affect its performance. For example, the rate of convergence of the algorithm determines the agility of the distributed estimator to track the desired value <ref type="bibr" target="#b7">[8]</ref> or the error in the distributed optimization algorithm <ref type="bibr" target="#b54">[56]</ref>. This motivated us to consider the rate of convergence as the primary performance metric for such linear algorithms.</p><p>We considered two classes of algorithms: randomized and determinstic. The randomized algorithm reported here is based on work by Boyd et al. <ref type="bibr" target="#b7">[8]</ref>. The deterministic algorithm reported here is based on work by Jung et al. <ref type="bibr" target="#b32">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separable Function Computation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>As usual, we are given an arbitrary connected network, represented by an undirected graph G = (V, E), with |V | = n nodes. Two nodes i and j can communicate with each other if (and only if) (i, j) ∈ E. We assume that, in a given time-slot each node can contact at most one other node. However, a node can be contacted by multiple nodes simultaneously.</p><p>Let 2 V denote the power set of the vertex set V (the set of all subsets of V ). Let x = [x i ] ∈ R n denote a real valued n-dimensional vector. We call a function f : R n × 2 V → R separable if there exist functions f 1 , . . . , f n such that, for all x ∈ R n and S ⊆ V ,</p><formula xml:id="formula_155">f (x, S) = i∈S f i (x i ).</formula><p>(5.1)</p><p>Let F be the class of separable functions f for which f i (x) ≥ 1 for all x ∈ R and i = 1, . . . , n. Here, the lower bound of 1 on value of f i (•) is chosen for simplicity; in general it can be an arbitrary fixed positive constant.</p><p>Here, we wish to consider the question of designing a Gossip algorithm for the following separable function computation problem. Let x i denote the initial value of node i ∈ V and x = [x i ] represent the vector of these initial values. Let f ∈ F be a given separable function. Then, all nodes in V wish to compute an estimate of f (x, V ).</p><p>Clearly, the separable function computation stated above is equivalent to the following 'simpler' summation problem. Each node, say i ∈ V has value x i ≥ 1. Let x = [x i ] denote the vector representation of these values. All nodes wish to compute an estimate of the summation x sum = i x i using a Gossip algorithm as quickly as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Algorithm</head><p>We will describe a Gossip algorithm for the summation problem to estimate x sum at all nodes as quickly as possible. This algorithm will build on the information dissemination Gossip algorithm described earlier.</p><p>We start with the description of a minimum computation algorithm that utilizes the information dissemination. Next, we state useful properties of the Exponential distribution. Finally, we will describe the summation algorithm that is obtained through a combination of computing the minimum and the properties of the Exponential distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Minimum Computation</head><p>We start with the description of a Gossip algorithm for minimum computation based on the information dissemination algorithm. To this end, let each of the n nodes have distinct non-negative real values: let u i be the value of node i. All nodes wish to compute u * = min i u i .</p><p>Let P be a doubly stochastic symmetric network graph G conformant irreducible matrix. Time is discrete and denoted by t ∈ N. At each time t, each node i contacts its neighbor j ∈ V with probability P ij ; it does not contact any node with probability P ii . Upon making contact, both nodes i and j exchange all 'relevant' information. Now, we describe the information exchange aspect in the minimum computation algorithm.</p><p>Let ûi (t) be the estimate of u * at node i at time t. Initially, t = 0 and ûi (0) = u i . As described above, at time t if a node i ∈ V contacts another node j then i sends ûi (t) to j and j sends ûj (t) to i. Upon receiving various estimates, each node i ∈ V sets its new estimate ûi (t + 1) as the minimum of its own value ûi (t) and all the received values.</p><p>We claim that, under the above described minimum computation algorithm, all nodes have the correct minimum by time T one spr (ε) with probability at least 1ε. To see this, suppose that node i ∈ V be such that u * = u i . Then, under the algorithm described the minimum 'spreads' in the same manner as in the setting of the single-piece dissemination. By definition, by time T one spr (ε) all nodes will have the singlepiece information with probability at least 1ε. Therefore, it follows that the minimum computation for all nodes under the above algorithm takes no more than T one spr (ε) with probability at least 1ε. By Theorem 3.1, we have</p><formula xml:id="formula_156">T one spr (ε) = O log n + log ε -1 Φ(P ) .</formula><p>Finally, consider the problem of computing r different minimums. Let each node i ∈ V have r distinct values u i (1), . . . , u i (r). For 1 ≤ ≤ r, let</p><formula xml:id="formula_157">u * ( ) = min i u i ( ).</formula><p>Then each node wishes to find out the r different minimums, u * (1), . . . , u * (r). It should be noted that by an application of the union bound and a 'round-robin' style parallel scheduling of r minimum computation algorithms as describe above, th computation time for r different minimums is bounded above by rT one spr (ε) with probability at least 1rε. That is, r different minimums can be computed with probability at least 1ε in time T min (r, ε), where</p><formula xml:id="formula_158">T min (r, ε) = O r(log n + log r + log ε -1 ) Φ(P ) . (5.2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">A Useful Extremal Property</head><p>We describe an extremal property of Exponential random variables that will play a crucial role in designing Gossip algorithm for summation.</p><p>Formally, we state it as follows.</p><p>Property </p><formula xml:id="formula_159">∈ R + , Pr(U &gt; z) = exp(-µz).</formula><p>Using this fact and the independence of the random variables W i , we compute Pr(W &gt; z) for any z ∈ R + .</p><formula xml:id="formula_160">Pr(W &gt; z) = Pr(∩ n i=1 {W i &gt; z}) = n i=1 Pr(W i &gt; z) = n i=1 exp(-λ i z) = exp -z n i=1 λ i .</formula><p>This establishes the property stated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Concentration of the Exponential Distribution</head><p>We state the concentration of the empirical mean of i.i.d. Exponential random variables. This will be useful in determining values of parameters in our summation algorithm. </p><formula xml:id="formula_161">R k = 1 k k i=1 Y i .</formula><p>Then, for any δ ∈ (0, 1/2),</p><formula xml:id="formula_162">Pr R k - 1 λ ≥ δ λ ≤ 2 exp - δ 2 k 3 . (5.3)</formula><p>Proof. By definition,</p><formula xml:id="formula_163">E[R k ] = 1 k k i=1 λ -1 = λ -1 .</formula><p>Now the inequality in (5.3) follows directly from the well-known Large Deviation Principle implied by Cramér's Theorem (see <ref type="bibr" target="#b14">[15]</ref>, pp. <ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">35)</ref> for the empirical mean of i.i.d. random variables and the distributional property of Exponential random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Algorithm: Description and Performance</head><p>Now we are ready to describe the algorithm for summation computation. Recall that each node i has a positive real value x i ≥ 1. Each node i wishes to compute estimates of x sum . Specifically, suppose all nodes wish to compute an estimate of x sum within [(1δ)x sum , (1 + δ)x sum )] for some δ &gt; 0. We describe an algorithm that computes the estimation of x sum at all nodes within this δ-accuracy with probability at least 1ε for a given ε ∈ (0, 1/2).</p><p>To this end, let r(ε, δ) = 3δ -2 ln(4/ε). This selection is inspired by Property 5.2 for δ-accuracy with high enough probability. Inspired by Property 5.1, each node generates r(ε, δ) random numbers as follows: node i generates r ∆ = r(ε, δ) random numbers y i (1), . . . , y i (r) by sampling Exponential distribution with parameter x i . For 1 ≤ ≤ r,</p><formula xml:id="formula_164">y * ( ) = min i y i ( ).</formula><p>All nodes compute these r = r(ε, δ) minimums using the minimum computation algorithm. Let ŷi ( ) be the estimate of the minimum y * ( ) at node i (say after long enough time) for 1 ≤ ≤ r(ε, δ). Then node i generates an estimate of x sum as r( r =1 ŷi ( )) -1 . Here, this choice of estimator is clearly inspired by Property 5.1 with the "confident accuracy" supplied by Property 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.2">Ring graph</head><p>Consider the ring graph of n nodes with a natural symmetric probability matrix P , obtained by the Metropolis-Hasting method, i.e., for each i, P ii = 1/2, P ii + = P ii -= 1/4 where i + and i -represent neighbors of i on either side. As established in Preliminaries,</p><formula xml:id="formula_165">Φ(P ) = O(1/n) for such P . Therefore, T sum (ε, δ) = O(δ -2 n log n) for ε = Ω(1/poly(n)).</formula><p>Since the diameter of a ring scales as n, this is again as fast as possible for any fixed δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.3">Expander graph</head><p>For a d-regular expander with all nodes having degree d, the natural P has Φ(P ) = O <ref type="bibr" target="#b0">(1)</ref>. Therefore, like the complete graph</p><formula xml:id="formula_166">T sum (ε, δ) = O(δ -2 log n) for ε = Ω(1/poly(n)).</formula><p>That is, the algorithm performs computation essentially as fast as possible for any fixed δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.4">Geometric random graph</head><p>For the geometric random graph over n nodes and connectivity radius r = r(n) beyond the connectivity threshold, as established in Preliminaries, the natural probability matrix P on it has Φ(P ) essentially scaling as r. Therefore, we will have T sum (ε, δ) = O * (δ -2 r -1 ) for ε = Ω(1/poly(n)). Again, since the diameter of G(n, r) scales like 1/r, the algorithm performs computation essentially as fast as possible for any fixed δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summary</head><p>We described a Gossip algorithm for computing separable functions or equivalently computing the summation of distinct numbers in a network. The algorithm naturally builds over single-piece information dissemination by means of a natural minimum computation algorithm. This is made possible by a probabilistic transformation implied using an extremal property of the Exponential distribution: computing the summation can be performed by the computation of certain relevant minima. The computation time of the algorithm essentially scales as that of the computation of the minimum or as single-piece information dissemination.</p><p>We remark on some features of the algorithm. The minimum computation algorithm can be implemented under a totally asychronous computational model. Therefore, the algorithm described here is quite robust with respect to the 'time-model'. The computation time of the algorithm is minimal for most of the reasonably regular graphs and scales like their diameter. The algorithm, as described, does not have a locally verifiable 'stopping condition'. However, a natural probabilistic stopping condition arises as an implication of Theorem 5.1 as follows. Since Φ(P ) for most of the reasonable graphs is no smaller than 1/n, if a node's estimate does not change for O(n log n) time slots (with large enough constant), then its estimate is correct with high enough probability as long as the number of nodes in the network is no larger than n. Now, we remark on a weakness of the algorithm compared to the linear computation algorithm. Recall that the linear computation algorithm has time scaling proportional to log ε -1 for ε-accurate estimate with probability at least 1ε. That is, it has scaling proportional to log ε -1 and log δ -1 for δ-accurate estimate with probability 1ε. In contrast, the algorithm describe here has scaling δ -2 and log ε -1 for δ-accurate estimate with probability 1ε. Therefore, the algorithm decribed here performs rather poorly compared to the linear computaton for very small δ, equivalently very high accuracy.</p><p>Next, we remark on the 'implementation' of the algorithm. The algorithm described here requires exchange of 'real numbers'. However, any system implementation would require the exchange of bits and not real numbers. In a recent work by Ayaso et al. (see <ref type="bibr" target="#b2">[3]</ref> for a detailed account of this work) a quantization of this algorithm is proposed. This leads to a slow-down of the computation time stated in Theorem 5.1 by a factor of log 1/ε for retaining accuracy within 1 ± ε. We also make a note of the following. The effective quantization of the linear dynamics based algorithms is far from clear and satisfactory analysis of the natural quantization of such algorithms seem non-trivial and is not known (to the best of author's knowledge). We refer an interested reader to a recent work by Kashyap et al. <ref type="bibr" target="#b33">[35]</ref> that proposes a mechanism to make the linear dynamics based quantized algorithm converge.</p><p>Finally, we remark on the 'optimality' of the algorithm. Under a natural probabilistic model, in the recent work Ayaso et al. <ref type="bibr" target="#b2">[3]</ref> have established the optimality of the (quantized version of the) algorithm in terms of its depedence on the graph structure. Specifically, they established that any algorithm utilizing information spreading based on the probability matrix P cannot have a computation time scaling faster than 1/Φ(P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Historical Notes</head><p>The key concept behind the algorithm described here is the use of probabilistic extremal property for separable function. The concentration of the maximums of independent Geometric random variables was first used by Flajolet and Martin <ref type="bibr" target="#b22">[23]</ref> to count distinct elements in a database. This idea was further improved by various authors over the past 25 years. For example, recent works by Considine et al. <ref type="bibr" target="#b11">[12]</ref> and Enachescu et al. <ref type="bibr" target="#b20">[21]</ref> build on <ref type="bibr" target="#b22">[23]</ref> for approximate computation in sensor databases and sensor networks. The algorithm presented here is based on work by Mosk-Aoyama and Shah <ref type="bibr" target="#b53">[55]</ref>.</p><p>Wireless networks are becoming the architecture of choice for designing ad-hoc networks, metro-area networks and mesh-networks. The tasks of resource allocation and scheduling are essential for good network utilization. The multi-access capability of the wireless medium makes algorithm design for such networks intrinsically different and more challenging than its wireline counter-part. Further, wireless architectures require that algorithm be distributed and simple.</p><p>Here, we consider a Gossip based algorithm for scheduling nodes that wish to access a common wireless medium. The algorithm builds upon the information dissemination and the separable function computation algorithm along with an additional simple randomized sampling mechanism. Somewhat surprisingly, this algorithm utilizes the network capacity to the fullest despite its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>We consider an abstract model of a wireless network represented by a graph G = (V, E) with |V | = n wireless nodes and edges represented by E. As before, let N (i) = {j ∈ V : (i, j) ∈ E} denote the set of neighbors of i ∈ V . We assume that network operates under the classical indepedent set interference model. That is, if a node v is transmitting then all of its neighbors in N (i) must not transmit at the same time. It should be noted that other popular combinatorial interference models are (computationally) equivalent to the independent set model. Therefore, even though the treatment here seems restricted to wireless networks, it naturally extends to other communication models.</p><p>The network operates in discrete time, i.e., time is slotted and t ∈ N denotes the time. Each node i ∈ V , capable of wireless transmission, can transmit at the unit rate to any of its neighbors. We ignore the power control for simplicity, but as an informed reader may notice, it can be easily included in the model. At each node, packets (of unit size) are arriving according to an external arrival process. Let Ā(t) = [ Āi (t)] denote the cumulative arrival process until time t ∈ N, i.e., Āi (t) is the total number of packet that have arrived at node i in the time interval [0, t]; Ā(0) = 0. Let A i (t) = Āi (t) -Āi (t -1) be the number of packets arriving at node i in time slot t. We assume that at most one packet can arrive at a node i in a time slot, i.e., A i (t) ∈ {0, 1}. We assume that arrivals happen in the middle of the time-slot. Finally, we assume that</p><formula xml:id="formula_167">A i (•) are Bernoulli i.i.d. random variable with Pr(A i (t) = 1) = λ i . Let λ = [λ i ] ∈ R n</formula><p>+ denote the arrival rate vector. For simplicity we assume that the network is single-hop,<ref type="foot" target="#foot_4">1</ref> i.e., data arriving at a node i will depart the network after its transmission. Let Q i (t) denote the queue-size, or number of packets waiting, at node i at the end of the time-slot t with Q(t) = [Q i (t)]. We assume the system starts empty, i.e., Q(0) = 0. Let D(t) = [ Di (t)] denotes the cumulative departure process from Q(t); D(t) = D(t) -D(t -1) = [D i (t)] denote the number of departures in time slot t. We assume that departures happen in the beginning of a time slot. Then,</p><formula xml:id="formula_168">Q(t) = Q(0) + Ā(t) -D(t) = Ā(t) -D(t) = Q(t -1) + A(t) -D(t). (<label>6.1)</label></formula><p>Now departures happen according to a scheduling algorithm which satisfies the interference constraint that no two neighboring nodes are transmitting data in the same time slot. To this end, let I denote the set of all independent sets of G. Formally define</p><formula xml:id="formula_169">I ∆ = {S ⊂ V : S = ∅ or, if i, j ∈ S then (i, j) / ∈ E} .</formula><p>And each time slot the scheduling algorithm schedules nodes of an independent set I ∈ I to transmit packets. In what follows, we will denote independent set I as vector I = [I i ] with I i ∈ {0, 1} and</p><formula xml:id="formula_170">I i = 1</formula><p>indicates that node i is in I. Let I(t) ∈ I be the schedule chosen by algorithm in the beginning of the time slot t. Then,</p><formula xml:id="formula_171">Q(t) = Q(t -1) + A(t) -I(t)1 {Q(t-1)&gt;0} ,<label>(6.2)</label></formula><p>where 1 {Q(t-1)&gt;0} is to make sure that if I i (t) = 1 but Q i (t -1) = 0 then there can be no departure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Performance Metric and a Desirable Algorithm</head><p>We say that a network is stable for a given λ ∈ R n + under a particular scheduling algorithm if</p><formula xml:id="formula_172">lim sup t→∞ E[Q i (t)] &lt; ∞, ∀ i ∈ V.</formula><p>Such an algorithm will be said to provide 100% throughput or be throughput optimal.</p><p>From <ref type="bibr" target="#b65">[67]</ref>, it is clear that the set of all λ ∈ R n + for which there exists a scheduling policy so that the system stablity is given by Λ = Co(I), where Co(I) is the convex hull of I in R n + . Hence, we call Co(I) the throughput region of the system.</p><p>In the work by Tassiulas and Ephremides <ref type="bibr" target="#b65">[67]</ref>, it was shown that a 'maximum weight independent set' scheduling algorithm is stable for all λ ∈ Co(I), where the schedule or independent set I(t) chosen at time t is such that</p><formula xml:id="formula_173">I(t) ∈ arg max I∈I I, Q(t -1) , with the notation that A, B ∆ = i∈V A i B i .</formula><p>A striking property of this 'maximum weight' algorithm is its ability to utilize network resources to the fullest without actually 'learning' or 'knowing' the arrival rates or any other network parameters. In that sense, the maximum weight algorithm is 'universal'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">The Question</head><p>Now the maximum weight independent set algorithm is desirable in terms of performance. However, it requires solving the maximum weight independent set problem in the network graph every time. However, finding a maximum weight independent set in general is NP-hard <ref type="bibr" target="#b24">[25]</ref> and even hard to approximate within n 1-o(1) (B/2 O( √ log B) for a graph with node degree B) factor <ref type="bibr" target="#b66">[68]</ref>. This brings us to the following challenging question: is it even possible to have any throughput optimal, polynomial (in n) time distributed algorithm?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Scheduling Algorithm</head><p>In what follows, we shall address the above mentioned question by designing a Gossip algorithm for scheduling in the network. That is, the algorithm by design is extremely simple and totally distributed. Somewhat surprisingly, it turns out to be throughput optimal. The algorithm we describe will be denoted by SCH. It is based on two distributed sub-routines, SAMP and COMP, which we shall describe before the description of SCH. It should be noted that COMP directly utilizes the summation algorithm from Separable function computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Random Sampler: SAMP</head><p>We describe a simple, distributed sampling algorithm SAMP to sample an independent set from I. This algorithm may not sample independent sets uniformly from I, but it samples each of them with strictly positive probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAMP</head><p>• Each node i ∈ V chooses I i = 0 or 1 with probability 1/2 independently.</p><p>• If node i finds any j ∈ N (i) such that I j = 1, it immediately sets I i = 0. • Now, output I = [I i ] as a sampled independent set. Now we state the main property of the sampling distribution induced by algorithm SAMP. Property 6.1. Algorithm SAMP samples independent sets of graph G in distributed manner so that each independent set is sampled with probability at least 2 -n . Node i performs O(|N (i)|) operations, and the algorithm in total performs</p><formula xml:id="formula_174">O(|E| + n) ≤ O(n 2 ) operations.</formula><p>Proof. Since each node selects 0 or 1 independently with probability 1/2, each one of the 2 n assignment of {0, 1} n is equally likely (i.e., probability 2 -n ). Each independent set corresponds to one such assignment in {0, 1} n . As part of the algorithm, if random node assignment is by itself an independent set, then it can be easily checked that the final output is that particular independent set only. Thus, each independent set has at least 2 -n probability of being selected. It can be easily checked that the output is always an independent set of G including ∅.</p><p>Now, the number of operations done by the algorithm are n random coin tosses. Each node i ∈ V performs O(|N (i)|) comparisons. Equivalently, there are at most O(|E|) operations for all nodes. These are all extremely simple distributed operations. Now for any graph, |E| = O(n 2 ). Hence, the total number of operations is O(n 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Comparator: COMP</head><p>The purpose of the algorithm COMP is to compute the summation of node weights (approximately) for a given independent set. A useful property of this algorithm is that all nodes obtain the same estimate, and hence it allows for distributed decision in SCH.</p><p>Formally, given an independent set I = [I i ] and node weights W = [W i ], we wish to estimate I, W . Equivalently, each node i ∈ V has a number x i = I i W i and it wishes to estimate x sum = i x i . The weights correspond to the queue-size. Therefore, x i ∈ N. By requiring that nodes with x i = 0 do not participate (but help in computation), we will have that for all participating nodes x i ≥ 1. Thus, in effect we have nodes with x i ≥ 1 and we wish to compute x sum = i x i . This is precisely the question considered in Separable function computation (SFC).</p><p>The summation algorithm designed in (SFC) computes an estimate, ŵ(I) of I, W in time O(δ -2 n/Φ(P )) over graph G using Gossip information exchange based on the probability matrix P (see Theorem 5.1, with choice of ε = 3 -n ) with the following property:</p><formula xml:id="formula_175">Pr ( ŵ(I) / ∈ ((1 -δ) I, W , (1 + δ) I, W )) ≤ 3 -n . (<label>6.3)</label></formula><p>We summarize this as the following formal property.</p><p>Property 6.2. The algorithm COMP with parameter δ &gt; 0 produces an estimate ŵ(I) of the weight of an independent set I, w(I)</p><formula xml:id="formula_176">∆ = I, W so that ŵ(I) ∈ ((1 -δ)w(I), (1 + δ)w(I)) with probability at least 1 - 3 -n in time O(nδ -2 /Φ(P ))</formula><p>over G based on probability matrix P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Scheduling Algorithm SCH</head><p>Now, we describe the scheduling algorithm SCH. As described below, it utilizes the distributed algorithms SAMP and COMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCH</head><p>• The algorithm will use parameter δ &gt; 0.</p><p>• Let I(t) be the independent set schedule chosen by the algorithm at time t. • At time t + 1, choose schedule I(t + 1) as follows:</p><p>-Generate a random independent set R(t + 1) using SAMP.</p><p>-Obtain esimates ŵ(I(t)), ŵ(R(t + 1)) of weights I(t), Q(t) and R(t + 1), Q(t) , respectively within accuracy (1 ± δ/8) using COMP with parameter δ/8.</p><p>-If ŵ(R(t + 1)) &gt; (1 + δ/8)/(1δ/8) ŵ(I(t)), then set I(t + 1) = R(t + 1). Else, set I(t + 1) = I(t).</p><p>• Repeat the above algorithm every time.</p><p>Some remarks are in order. The algorithm SCH takes O(nδ -2 /Φ(P )) time-steps or equivalently, a total of O(|E| + n 2 δ -1 /Φ(P )) distributed operations to compute a new schedule. In general, for any reasonable graph, Φ(P ) = Ω(1/n) and |E| ≤ n 2 . Therefore, SCH computes a new schedule in O(n 2 δ -2 ) time-steps or total of O(n 3 δ -2 ) operations for any network graph G.</p><p>The algorithm SCH can be further 'slowed' down by running it once every T steps. For any finite T , including T = O(n 3 δ -2 ), the resulting algorithm will remain stable. Thus, in effect it leads to a stable algorithm that performs O(1) overall operations for hard constraints like the independent set. It should be noted that this does not come for free: there is an increase in the average queue-sizes upon 'slowing down' the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance of Scheduling Algorithm</head><p>Here, we shall establish the stability or throughput optimality property of SCH. We start with necessary technical preliminaries and then present the formal theorem followed by its proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Technical Preliminaries</head><p>We present useful technical preliminaries here. Consider a discrete time Markov chain on a countable state space S = N M for some finite integer M . Let X(t) denote the random state of the Markov chain at time t ∈ N. Let X(0) = 0 (can be any arbitrary 'good' state). Let L: S → [0, ∞) and f : S → [0, ∞) be any non-negative valued functions with L(0) = 0. The following is a well-known result and can be found in the book by Meyn and Tweedie <ref type="bibr" target="#b48">[50]</ref>. Proposition 6.1. Let a Markov chain be aperiod and irreducible. Let there exists a finite set C ⊂ S such that Markov chain satisfies the for some large enough (exponentially dependent on n) B. This will immediately imply that <ref type="bibr">(6.5)</ref> for some finite set C, constant B and φ &gt; 0. By Proposition 6.1 and the fact that the number of arrivals in a time interval of length T is at most nT , we will obtain the desired conclusion that</p><formula xml:id="formula_177">E[L(t k+1 )|Q(t k )] ≤ L(t k ) -φ Q(t k ), 1 + B1 {Q(t k )∈C} ,</formula><formula xml:id="formula_178">lim sup t→∞ E[ Q(t), 1 ] &lt; ∞.</formula><p>Next, we proceed towards proving (6.4). Given Q(t k ) = Q(kT ), we wish to study the average drift, L(t k+1 ) -L(t k ): let I(t) be the independent set schedule chosen by SCH at time t. Define</p><formula xml:id="formula_179">∆ i (t + 1) = A i (t + 1) -D i (t + 1).</formula><p>From the queueing dynamics in (6.1),</p><formula xml:id="formula_180">L(t + 1) -L(t) = Q(t + 1), Q(t + 1) -Q(t), Q(t) = i (Q i (t + 1) -Q i (t))(Q i (t + 1) + Q i (t)) = i ∆ i (t + 1)(2Q i (t) + ∆ i (t + 1)) = i ∆ 2 i (t + 1) + 2Q i (t)∆ i (t + 1).<label>(6.6)</label></formula><p>We will use the following facts: for all t, (1)</p><formula xml:id="formula_181">Q i (t)D i (t + 1) = Q i (t)I i (t + 1),<label>(2)</label></formula><p>∆ 2 i (t + 1) ≤ 1. By telescopic summation of (6.6) for t = t k , . . . , t k+1 -1, we obtain</p><formula xml:id="formula_182">L(t k+1 ) -L(t k ) ≤ nT + 2 t k+1 -1 t=t k Q(t), ∆(t + 1) . (6.7)</formula><p>Since the arrival process is Bernoulli i.i.d. with arrival rate vector λ,</p><formula xml:id="formula_183">E[L(t k+1 ) -L(t k ) | Q(t k )] ≤ nT + 2 t k+1 -1 t=t k E[ Q(t), λ -I(t + 1) |Q(t k )].<label>(6.8)</label></formula><p>We know that λ ∈ (1δ)Co(I), i.e., λ ≤ j α j I j , α j ≥ 0, I j ∈ I,</p><formula xml:id="formula_184">j α j = 1 -δ. Define I * (t) = arg max I∈I I, Q(t -1) , W * (t) = I * (t), Q(t -1) , W (t) = I(t), Q(t -1) , ∆(t) = W * (t) -W (t).</formula><p>Now, since at most n arrival and n departures can happen in a timeslot, we have |W * (t + s) -W * (t)| ≤ 2ns for every t, s. Some rearrangements, the above discussions and definitions yield the following:</p><formula xml:id="formula_185">E[L(t k+1 ) -L(t k ) | Q(t k )] ≤ nT + 2 t k+1 -1 t=t k E[∆(t + 1) -δW * (t + 1)|Q(t k )] ≤ nT + 4nT 2 -δT W * (t k ) + 2 t k+1 -1 t=t k E[∆(t)|Q(t k )].<label>(6.9)</label></formula><p>Note that so far, the derivation has been independent of the algorithm. Now, we bound the term t E[∆(t)|Q(t k )] using the property of SCH.</p><p>To this end, first some useful definitions and facts. Define</p><formula xml:id="formula_186">Z = inf m≥1 {R(t k + m) = I * (t k + m)}, Z 1 = inf m≥0</formula><p>{Guarantee of (6.3) due to COMP does not hold at t k + Z + m}.</p><p>By Property 6.1, we know that</p><formula xml:id="formula_187">E[min{Z, T } | Q(t k )] ≤ E[Z | Q(t k )] = E[Z] ≤ 2 n .</formula><p>Define T = T -min{T, Z 1 }. By Property 6.2 of COMPand an application of the union bound, we have Z 1 ≤ T with probability at most T 3 -n . Hence,</p><formula xml:id="formula_188">E[ T | Q(t k )] ≤ E[ T ] ≤ T 2 3 -n . Now, we are ready to bound t E[∆(t) | Q(t k )]: define A = [t k + Z, t k + Z + Z 1 ] ∩ [t k + 1, t k+1 ],</formula><p>and B = [t k + 1, t k+1 ]\A. On the starting time of A, SAMP picks the maximum weight independent set of that time, i.e., I * (τ k + Z). If Z 1 &gt; 0 (i.e., A = ∅), then by the property of COMPand SCH, we will have a schedule I(t k + Z) such that</p><formula xml:id="formula_189">W (t k + Z) ≥ 1 -δ/8 1 + δ/8 2 W * (t k + Z) ≈ (1 -δ/2)W * (t k + Z).</formula><p>Now, for t &gt; t k + Z and t ∈ A, by definition of Z 1 and property of SCH, we have that</p><formula xml:id="formula_190">W (t) ≥ W (t -1) -n.</formula><p>Putting the above discussion together with some re-arrangement and some bounds discussed above give: Replacing (6.10)-(6.12) in (6.9), we have <ref type="bibr">(6.13)</ref> since T = 2.2 n and W * (t k ) ≥ Q(t k ), 1 /n for any graph G. Inequality (6.13) is the same as (6.4), hence we have proved the desired property to establish positive recurrence.</p><formula xml:id="formula_191">t∈A E[∆(t)|Q(t k )] ≤ 2nT 2 + δT 2 W * (t k ). (<label>6</label></formula><formula xml:id="formula_192">E[L(t k+1 ) -L(t k )|Q(t k )] ≤ (nT + 8nT 2 ) -0.4δW * (t k ) = - 0.4δ n Q(t k ), 1 + O(5 n ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Relation to Other Models 89</head><p>Next, we prove the claim for the average queue-size, E[ Q(t), 1 ] as t → ∞. Taking the expectation w.r.t. Q(t k ) in <ref type="bibr">(6.13)</ref>,</p><formula xml:id="formula_193">E[L(t k+1 ) -L(t k )] ≤ - 0.4δ n E[ Q(t k ), 1 ] + O(5 n ). (6.14)</formula><p>Telescopically, sum (6.14) for k = 0, . . . , K -1; using the fact that L(•) ≥ 0 along with some re-arrangement yields</p><formula xml:id="formula_194">1 K K-1 k=0 E[ Q(t k ), 1 ] ≤ O n5 n δ .</formula><p>Using the fact that for t ∈ (t k , t k+1 ], Q(t), 1 ≤ Q(t k ), 1 + nT , taking K → ∞, and using the above inequality gives us lim sup</p><formula xml:id="formula_195">t→∞ 1 t t-1 s=0 E[ Q(s), 1 ] = O(6 n ).<label>(6.15)</label></formula><p>Given (6.13), the implication of Proposition 6.1(b) and <ref type="bibr">(6.15)</ref>, and the relation between the cesaro limit and the limit of a sequence implies that <ref type="bibr">.16)</ref> This completes the proof of Theorem 6.2.</p><formula xml:id="formula_196">lim t→∞ E[ Q(t), 1 ] = O(6 n ). (<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Relation to Other Models</head><p>The above described model ignores the multi-hop setup. However, we have done so to keep the exposition simple. The scheduling algorithm of interest with the independent set interference constraint remain the same as maximum weight independent set but with weights being somewhat different. To explain this, we give example of two such well-known scenarios as follows.</p><p>1. Multi-hop queuing network. This network was considered in <ref type="bibr" target="#b65">[67]</ref>. For a given network G, let S be the set of data-flows with arrival rate λ s for flow s ∈ S. Let f s and d s denote source and destination node respectively for flow s ∈ S. The routing is assumed to be pre-determined in the network. If s passes through v ∈ V then let h(v, s) ∈ V denote its next hop unless v = d s in which case its data departs from G. Let Q vs (t) denote the queue-size of flow s at node v at time t. Define</p><formula xml:id="formula_197">W vs (t) = Q vs (t) -Q h(v,s)s (t), if v = d s , 0, if v = d s .</formula><p>Define W v (t) = max s∈S W vs (t) and</p><formula xml:id="formula_198">W (t) = [W v (t)].</formula><p>Then the throughput optimal (stable) algorithm of <ref type="bibr" target="#b65">[67]</ref> chooses I * (t) as the schedule which is a maximum weight independent set with respect to W (t -1), i.e., I * (t) = arg max I∈I I, W (t -1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Joint resource allocation &amp; scheduling.</head><p>In <ref type="bibr" target="#b38">[40]</ref>, it is very wellexplained that the problem of congestion control and scheduling decomposes into two weakly coupled sub-problems: (i) congestion control, and (ii) scheduling. We describe the link-level scheduling problem. We urge an interested reader to go through <ref type="bibr" target="#b38">[40]</ref> for details. The setup of the problem is the same as in the previous example with difference that routing is not pre-determined. The coupling of congestion control and scheduling happens via Lagrange multipliers q(t) = [q e (t)] e∈E . With the interference model of this paper, the scheduling problem boils down to the selection of maximum weight independent set I * (t) with respect to weight W (t -1) = [W v (t -1)], where W v (t -1) = max e:e=(u,v)∈E q e (t -1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Summary</head><p>Here, we presented a Gossip algorithm for multi-access scheduling in a wireless network under the independent set interference constraint. The algorithm presented builds upon the separable function computation algorithm and utilized a distributed sampler. Somewhat surprisingly, this rather simple algorithm turns out to be throughput optimal. The algorithm computes a new schedule in time that is inversely proportional to 1/Φ(P ), where Φ(P ) is the conductance of the Gossip probability matrix. Thus the algorithm's computation time depends on the underlying network graph structure through this dependence on the reciprocal of Φ(P ). Now some remarks are in order about the practicality of this algorithm. In author's opinion, the algorithm as stated is unlikely to be useful for practice due to amount of the overhead involved in terms of control information that is exchanged in the network for finding a new schedule every time. However, the algorithm presented here provides a proof-of-concept for existence of simple, distributed throughput optimal algorithm. Better implementable gossip style throughput optimal scheduling algorithm remain of interest for future research. We take note of an exciting recent progress towards this goal by Rajagopalan et al. <ref type="bibr" target="#b58">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Historical Notes</head><p>We present a brief summary of previous work on network scheduling algorithms. The result by Tassiulas and Ephremides <ref type="bibr" target="#b65">[67]</ref> established that the 'max-weight scheduling' policy is throughput optimal for a large class of scheduling problems. This result has been very influential in the design of scheduling algorithms since then. Application to inputqueued switches led to an excellent development of theory and practice of algorithms for scheduling under matching constraints: notably, the results of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b66">68]</ref>. A recent interest in wireless networks has led to proposal of distributed scheduling algorithms under matching constraints <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">41]</ref>. Most of these algorithms, based on finding the maximal matching, guarantee only a constant fraction of throughput. Recently, Modiano et al. <ref type="bibr" target="#b49">[51]</ref> exhibited a throughput optimal, extremely simple distributed scheduling algorithm with matching constraints. This algorithm, as discussed in <ref type="bibr" target="#b38">[40]</ref>, easily extends to provide a throughput optimal algorithm for the resource allocation and scheduling problem under matching constraints. The algorithm presented here is a natural generalization of <ref type="bibr" target="#b49">[51]</ref> and was discussed in work by Jung and Shah <ref type="bibr" target="#b31">[33]</ref>. The application of this algorithm to the joint scheduling and congestion control problem is described in a recent work by Eryilmaz et al. <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Convex Optimization</head><p>The efficient utilization of network resources requires a good resource allocation algorithm. Usually, such an algorithm is required to solve a network-wide global optimization problem. Many of the important network resource allocation problems can be viewed as convex optimization (minimization) problems with linear constraints. In the context of next generation networks, such as P2P or sensor networks, these need to be implemented through Gossip mechanism. This motivates the study of Gossip algorithms for global optimization problems without relying on any form of network infrastructure.</p><p>In the classical literature, convex minimization problems with linear constraints are known to be solvable through iterative algorithms by means of the dual decomposition or primal-dual algorithms. However, in most of the network resource allocation problems, these are not 'truly distributed' or 'local' or in our terminology, Gossip algorithms. This is because algorithms known in the classical literature are 'distributed' with respect to the "constraint graph" of the problem and not the 'network graph'.</p><p>To explain this subtle but important difference, as before consider a connected network of n nodes with network graph G = (V, E) with V = {1, . . . , n} and E representing edges along which communication is feasible. Now suppose each node i ∈ V has a non-negative variable x i associated with it. The goal is to assign values for the x i 's to optimize a global network objective function under network resource constraints. We assume that the global objective function f : R n + → R is separable in the sense that f (x) = n i=1 f i (x i ) for any variable assignment x ∈ R n + . The feasible region is described by a set of nonnegative linear constraints. Let us consider a specific example.</p><p>Example 7.1 (Network resource allocation). Given a connected edge capacitated network G = (V, E), with each edge in E having nonnegative capacity, each user i ∈ V wishes to transfer data to a specific destination along a particular path in the network, and has a utility function that depends on the rate x i that the user is allocated. The goal is to maximize the global network utility, which is the sum of the utilities of individual users. The rate allocation x = (x i ) must satisfy capacity constraints, which are linear. Now the constraint graph of the above optimization problem, denoted by G C = (V C , E C ) where V C corresponds to variables and edges in E C correspond to pairs of variables that participate in a common constraint. For example, in the network resource allocation problem above, the constraint graph G C contains an edge between two users if and only if their paths intersect. Operationally, Gossip algorithm for rate allocation must be local with respect to the network graph G. Note that typically G C ⊆ G (i.e., E C ⊆ E), and hence a fully distributed algorithm with respect to G C is not fully distributed with respect to G; hence can satisfy 'Gossip' properties.</p><p>In summary, most of the classical algorithms from optimization theory (see Historical notes for details and references therein) provide distributed algorithms with respect to the constraint graph G C , and not with respect to the underlying network graph G. Here, we shall describe Gossip algorithm that is distributed with respect to G and naturally builds on the summation algorithm described in Separable function computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>Here, we consider the problem of minimizing a convex separable function over linear inequalities. As noted earlier, let G = (V, E) be the connected network graph with V = {1, . . . , n}. Each node i ∈ V has non-negative decision variable x i ∈ R + .</p><p>We consider convex minimization problems of the following general form.</p><formula xml:id="formula_199">minimize f (x) ∆ = n i=1 f i (x i ) (P) subject to Ax = b x ∈ R n + .</formula><p>Here, we assume that the objective function is separable, i.e., f (x) = n i=1 f i (x i ), and each f i : R + → R is twice differentiable and strictly convex, with lim x i ↓0 f i (x i ) &lt; ∞ and lim x i ↑∞ f i (x i ) = ∞. We will call this optimization problem as the primal problem (P).</p><p>The constraints are linear equality constraints of the form Ax = b with matrix A ∈ R m×n + and a vector b ∈ R m ++ , and non-negativity constraints x i ≥ 0 on the variables. We assume that m ≤ n, and that the matrix A has linearly independent rows. For i = 1, . . . , n, let a i = [A 1i • • • A mi ] T denote the ith column of the matrix A. In this distributed setting, we assume that node i is given the vectors b and a i , but not the other columns of the matrix A.</p><p>The goal is to design a Gossip algorithm that produces an εapproximately feasible solution with objective function value close to that of an optimal feasible solution for a given error parameter ε &gt; 0. We would like the running time of the algorithm to be polynomial in 1/ε, the number of constraints m, the inverse of the conductance, Φ(P ) of the Gossip based information exchange probability matrix P graph. Also we shall allow reasonable dependence of running time on the property of the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Algorithm: Description and Performance Analysis</head><p>We describe a Gossip algorithm for solving the optimization problem with the above stated desired properties. The algorithm is based on the Lagrange dual problem. Due to the separable objective function, its dual problem can be decomposed so that an individual node can recover the value of its variable in a primal solution from a dual feasible solution. The dual problem is solved via a dual ascent algorithm. The standard approach for designing such an algorithm only leads to a distributed algorithm with respect to the constraint graph of the problem. To design a Gossip algorithm there are two main challenges: (a) making the algorithm distributed with respect to the network graph G, and (b) respecting the non-negativity constraints on the variables.</p><p>The first challenge is resolved by utilizing the Gossip summation algorithm from Separable function computation as a subroutine. In a sense, the summation algorithm provides 'a distributed layer' for solving the optimization problem. The second challenge is resolved by use of a barrier function that is inspired by (centralized) interior-point mathematical programming algorithms.</p><p>In what follows, we start with some necessary notations and preliminaries. Then, we shall describe the algorithm and its properties. Finally, we shall provide proof of its properties in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Notations and Preliminaries</head><p>For a vector x ∈ R n , by x we denote the 2 -norm of the vector. The ball of radius r around x is defined as B(x, r) = {y : yx ≤ r}. For a real matrix M , we write σ min (M ) and σ max (M ) to denote the smallest and largest singular values, respectively, of M , so that σ min (M ) 2 and σ max (M ) 2 are the smallest and largest eigenvalues of M T M . Note that σ min (M ) = min{ Mz | z = 1} and σ max (M ) = max{ Mz | z = 1}. If M is symmetric, then the singular values and the eigenvalues of M coincide, so σ min (M ) and σ max (M ) are the smallest and largest eigenvalues of M .</p><p>Associated with the primal problem (P) is the Lagrangian function L(x, λ, ν) = f (x) + λ T (Axb)ν T x, which is defined for λ ∈ R m and ν ∈ R n , and the Lagrange dual function</p><formula xml:id="formula_200">g(λ, ν) = inf x∈R n + L(x, λ, ν) = -b T λ + n i=1 inf x i ∈R + f i (x i ) + a T i λ -ν i x i .</formula><p>The following problem is the Lagrange dual problem to (P).</p><p>maximize g(λ, ν)</p><formula xml:id="formula_201">(D) subject to ν i ≥ 0, i = 1, . . . , n</formula><p>Although, we seek a solution to the primal problem (P), to avoid directly enforcing the non-negativity constraints, we introduce a logarithmic barrier. For a parameter θ &gt; 0, we consider the following primal barrier problem.</p><formula xml:id="formula_202">minimize f (x) -θ n i=1 ln x i (P θ ) subject to Ax = b</formula><p>The Lagrange dual function corresponding to (P θ ) is</p><formula xml:id="formula_203">g θ (λ) = -b T λ + n i=1 inf x i ∈R ++ f i (x i ) -θ ln x i + a T i λx i ,</formula><p>and the associated Lagrange dual problem is the following unconstrained optimization problem.</p><formula xml:id="formula_204">maximize g θ (λ) ( D θ ) over λ ∈ R m .</formula><p>We assume that the primal barrier problem (P θ ) is feasible; that is, there exists a vector x ∈ R n + such that Ax = b. Under this assumption, the optimal value of (P θ ) is finite, and Slater's condition implies that the dual problem (D θ ) has the same optimal value, and there exists a dual solution λ * that achieves this optimal value <ref type="bibr" target="#b8">[9]</ref>. Furthermore, because (D θ ) is an unconstrained maximization problem with a strictly concave objective function, the optimal solution λ * is unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.1">Some useful properties</head><p>For a vector of dual variables λ ∈ R m , let x(λ) ∈ R n ++ denote the corresponding primal minimizer in the the Lagrange dual function:</p><formula xml:id="formula_205">for i = 1, . . . , n, x i (λ) = arg inf x i ∈R ++ f i (x i ) -θ ln x i + a T i λx i . (7.1)</formula><p>For this primal x(λ) based on dual λ, we denote the 'violation of equality' Ax(λ)b , as p(λ). Now, we can solve for each</p><formula xml:id="formula_206">x i (λ) explicitly. As f i (x i ) -θ ln x i + a T i λx i is convex in x i , f i (x i (λ)) - θ x i (λ) + a T i λ = 0. (7.2) Define h i : R ++ → R by h i (x i ) = f i (x i ) -θ/x i ; since f i is convex, h i is</formula><p>strictly increasing and hence has a well-defined and strictly increasing inverse. We then have</p><formula xml:id="formula_207">x i (λ) = h -1 i -a T i λ .</formula><p>Also, we assume that, given a vector λ, a node i can compute x i (λ). This is reasonable since computing x i (λ) is simply an unconstrained convex optimization problem in a single variable (7.1), which can be done by several methods, such as Newton's method.</p><p>Next, in our convergence analysis, we will argue about the gradient of the Lagrange dual function g θ . A calculation shows that</p><formula xml:id="formula_208">∇g θ (λ) = -b + n i=1 a i x i (λ) = Ax(λ) -b.<label>(7.3)</label></formula><p>We will use p(λ) to denote ∇g θ (λ) = Ax(λ)b for a vector λ ∈ R m . We note that at the optimal dual solution λ * , we have p(λ * ) = 0 and Ax(λ * ) = b.</p><p>To control the rate of decrease in the gradient norm p(λ), we must understand the Hessian of g θ . For j 1 , j 2 ∈ {1, . . . , m}, component (j 1 , j 2 ) of the Hessian ∇ 2 g θ (λ) of g θ at a point λ is</p><formula xml:id="formula_209">∂g θ (λ) ∂λ j 1 ∂λ j 2 = n i=1 A j 1 i ∂x i (λ) ∂λ j 2 = - n i=1 A j 1 i A j 2 i h -1 i -a T i λ . (7.4)</formula><p>As the functions h -1 i are strictly increasing, min =1,...,n h -1 -a T λ &gt; 0.</p><p>Hence, for any µ ∈ R m ,</p><formula xml:id="formula_210">µ T ∇ 2 g θ (λ)µ = m j 1 =1 µ j 1 m j 2 =1 ∂g θ (λ) ∂λ j 1 ∂λ j 2 µ j 2 = - m j 1 =1 µ j 1 m j 2 =1 n i=1 A j 1 i A j 2 i h -1 i -a T i λ µ j 2 = - n i=1 h -1 i -a T i λ m j 1 =1 A j 1 i µ j 1 m j 2 =1 A j 2 i µ j 2 = - n i=1 h -1 i -a T i λ a T i µ 2 ≤ -min =1,...,n h -1 -a T λ A T µ T A T µ &lt; 0,<label>(7.5)</label></formula><p>and g θ is a strictly a concave function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Description of Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.1">Basic algorithm</head><p>We consider an iterative algorithm for obtaining an approximate solution to (P), which uses gradient ascent for the dual barrier problem (D θ ). The algorithm generates a sequence of feasible solutions λ 0 , λ 1 , λ 2 , . . . for (D θ ), where λ 0 is the initial vector. To update λ k-1 to λ k in an iteration k, the algorithm uses the gradient ∇g θ λ k-1 to determine the direction of the difference λ kλ k-1 . We assume that the algorithm is given as inputs to the initial point λ 0 , and an accuracy parameter ε, such that ε ∈ (0, 1). The goal of the algorithm is to find a point x ∈ R n + that is nearly feasible in the sense that Axb ≤ ε b , and that has objective function value close to that of an optimal feasible point.</p><p>In this section, we describe the operation of the algorithm under the assumption that the algorithm has knowledge of certain parameters that affect its execution and performance. We refer to an execution of the algorithm with a particular set of parameters as an inner run of the algorithm. To address the fact that these parameters would not be available to the algorithm at the outset, we add an outer loop to the algorithm. The outer loop uses binary search to find appropriate values for the parameters, and performs an inner run for each set of parameters encountered during the search. Section 7.2.2.3 discusses the operation of the outer loop of the algorithm. The choice of these parameters is inspired by convergence analysis of the algorithm and that is the reason for defering the discussion of setting parameters later.</p><p>An inner run of the algorithm consists of a sequence of iterations. Iteration k, for k = 1, 2, . . . , begins with a current vector of dual variables λ k-1 , from which each node i computes x i λ k-1 . Let s k-1 = Ax λ k-1 , so that by <ref type="bibr">(7.3)</ref> </p><formula xml:id="formula_211">∇g θ λ k-1 = s k-1 -b.</formula><p>In order for the algorithm to perform gradient ascent, each node must compute the vector s</p><formula xml:id="formula_212">k-1 . A component s k-1 j = n i=1 A ji x i λ k-1 of s k-1</formula><p>is the sum of the values y i = A ji x i λ k-1 for those nodes i such that A ji &gt; 0. This is where we need 'distributed layer' provided by the summation algorithm described in Separable function computation.</p><p>Specifically, nodes apply this summation Gossip algorithm (m times, one for each component) to compute a vector ŝk-1 , where ŝk-1 j is an estimate of s k-1 j for j = 1, . . . , m. Recall that the summation algorithm takes as input parameters an accuracy ε 1 and an error probability δ. They provide estimate ŝk-1 j of s k-1 j for a particular value of j so that</p><formula xml:id="formula_213">(1 -ε 1 ) s k-1 j ≤ ŝk-1 j ≤ (1 + ε 1 ) s k-1 j (7.6)</formula><p>with probability at least 1δ. This computation takes O(ε -2 1 log δ -2 / Φ(P )) time for δ ≤ 1/n (which will be the case here). Recall that P denotes the information exchange probability matrix utilized by the Gossip algorithm.</p><p>In the analysis of an inner run, we assume that each invocation of the summation routine succeeds, so that (7.6) is satisfied. Provided we choose δ sufficiently small (see Section 7.2.2.3), this assumption will hold with high probability.</p><p>A description of an iteration k of an inner run of the algorithm is shown in Figure <ref type="figure">7</ref>.1. The values for the step size t and the error Inner run: Iteration k 1. For j = 1, . . . , m, the nodes compute an estimate ŝk-1</p><formula xml:id="formula_214">j of s k-1 j = n i=1 A ji x i λ k-1 . 2.</formula><p>The nodes check the following two stopping conditions.</p><p>(</p><formula xml:id="formula_215">-ε 1 ) 1 - 2 3 ε b ≤ ŝk-1 ≤ (1 + ε 1 ) 1 + 2 3 ε b . (7.7) ŝk-1 -b ≤ 2 3 ε + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b . (7.8)<label>1</label></formula><p>If both conditions (7.7) and (7.8) are satisfied, the inner run terminates, producing as output the vector x λ k-1 . 3. The nodes update the dual vector by setting ∆λ k-1 = ŝk-1b, and tolerance ε 1 will follow next. An inner run is essentially standard gradient ascent, where the stopping criterion (sufficiently small gradient norm) is modified to reflect the potential error in nodes' estimates of the gradient.</p><formula xml:id="formula_216">λ k = λ k-1 + t∆λ k-1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.2">Choosing the parameters</head><p>The step size t and the convergence rate of our algorithm are governed by the variation in curvature of the Lagrange dual function. Intuitively, regions of large curvature necessitate a small step size to guarantee convergence, and if small steps are taken in regions with small curvature, then progress toward an optimal solution is slow. Examining the Hessian of the Lagrange dual function (7.4), we see that curvature variation depends both on variation in (h -1 i ) , which roughly corresponds to variation in the curvature of the f i 's, and on the variation in the singular values of A T . Precisely, note that</p><formula xml:id="formula_217">h -1 i -a T i λ = 1 h i h -1 i -a T i λ = 1 f i h -1 i -a T i λ + θ (h -1 i (-a T i λ)) 2 ,</formula><p>and, for a distance r &gt; 0, define</p><formula xml:id="formula_218">q(r) = min =1,...,n min λ∈B(λ * ,r) h -1 i -a T i λ Q(r) = max =1,...,n max λ∈B(λ * ,r) h -1 i -a T i λ</formula><p>First, we describe the outer loop of the algorithm. The purpose of the outer loop is to invoke inner runs with various parameter values, and to terminate runs if they do not end in the allotted number of iterations.</p><p>As the outer loop does not know the values q λ 0λ * σ min A T 2 and Q λ 0λ * σ max A T 2 , it uses binary search to choose the parameter values for the inner runs. The algorithm (and its analysis) remains valid if we replace the former product with a lower bound on it, and the latter product with an upper bound on it. Let U &gt; 0 be an upper bound on the ratio between the largest and the smallest possible values of these two products.</p><p>The outer loop enumerates log U possible values q 1 , q 2 , . . . , q log U for q λ 0λ * σ min A T 2 , with q +1 = 2q for each . Similarly, it considers values</p><formula xml:id="formula_219">Q 1 , Q 2 , . . . , Q log U for Q λ 0 -λ * σ max A T 2 .</formula><p>For each pair of values (q 1 , Q 2 ) such that q 1 ≤ Q 2 , it computes an upper bound T (q 1 , Q 2 ) on the number of iterations required for an inner run with these parameter values. As stated later in Theorem 7.8, this value is bounded above as</p><formula xml:id="formula_220">T (q 1 , Q 2 ) = O Q 2 2 q 2 1 log p(λ 0 ) ε b .</formula><p>Recall that p(λ) = Ax(λ)b . Now, the outer loop sorts the T (q 1 , Q 2 ) values, and executes inner runs according to this sorted order. When an inner run is executed with parameter values (q 1 , Q 2 ), the outer loop lets it execute for T (q 1 , Q 2 ) iterations. If it terminates due to the stopping conditions being satisfied within this number of iterations, then by Theorem 7.8 the solution x(λ) produced will satisfy</p><formula xml:id="formula_221">Ax(λ) -b ≤ ε b ,</formula><p>and so the outer loop outputs this solution. On the other hand, if the stopping conditions for the inner run are not satisfied within the allotted number of iterations, the outer loop terminates the inner run, and then executes the next inner run with new parameter values according to the order induced by T (q 1 , Q 2 ).</p><p>By the choice of q 1 and Q 2 , there exist q * 1 and Q</p><formula xml:id="formula_222">* 2 such that q λ 0 -λ * σ min A T 2 /2 ≤ q * 1 ≤ q λ 0 -λ * σ min A T 2 and Q λ 0 -λ * σ max A T 2 ≤ Q * 2 ≤ 2Q λ 0 -λ * σ max A T 2 . For the parameter pair (q * 1 , Q * 2 ), T (q * 1 , Q * 2 )</formula><p>is, up to constant factors, the bound in Theorem 7.8. As we shall see, the bound established in Theorem 7.8 in the future section, will imply that for given value of R, the inner loop algorithm must terminate within iterations O(R 2 log(p(λ 0 )/ε b )). Therefore, when the outer loop reaches the pair (q * 1 , Q * 2 ), the corresponding inner run will terminate with the stopping conditions satisfied in the number of iterations specified in Theorem 7.8. Since the inner runs executed prior to this one will also be terminated in at most this number of iterations, and there are at most log 2 U such runs, we obtain the following upper bound on the total number of iterations executed by the algorithm.</p><p>Lemma 7.1. The total number of iterations executed in all the inner runs initiated by the outer loop is</p><formula xml:id="formula_223">O R 2 log p λ 0 ε b log 2 U .</formula><p>Now, we describe the only remaining aspect of the termination condition. Recall that in an iteration k of an inner run, the nodes must compute an estimate ŝk-1 j for each of the m components of the vector s k-1 j . As such, the summation routine must be invoked m times in each iteration. Recall that the algorithm should terminate after O(ε -2 1 log 2 δ -1 /Φ(P )) iterations. Therefore, if nodes have estimate of upper bound on n (number of nodes) and lower bound on Φ(P ) then they can determine when to stop. It should be noted that a trivial lower bound of 1/n on Φ(P ) can be utilized for any reasonable graph; and hence only an upper bound on n is necessary information for determining stopping condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Performance: Convergence and Correctness</head><p>Here, we describe the correctness and convergence properties of the algorithm described above. To this end, we need to set the value of δ utilized in the summation algorithm for the failure probability. The Lemma 7.1 and union bound suggests that in order for all the summation computation to satisfy condition (7.6) with probability at least</p><formula xml:id="formula_224">1 -1/n 2 , it is sufficient to set δ ≤ n 2 mR 2 log p λ 0 ε b log 2 U -1</formula><p>.</p><p>We will also set ε 1 = εα/3. This along with the above choice of δ leads to the following bound on each summation subroutine:</p><formula xml:id="formula_225">O   R 2 ε 2 Φ(P ) log nmR log p λ 0 ε b log U 2   ∆ = O * R 2 ε 2 Φ(P )</formula><p>,</p><p>where we have ignored poly-logarithmic terms in the O * (•) notation to highlight the key dependence of the running time. Here, note that the total number of operations performed by the algorithm is m times larger than the above stated bound since there are m summations we need to perform for each iteration of the inner run. Thus resulting algorithm (with all choices of parameters and stopping condition) has the following convergence and correctness property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 7.2 (Main Theorem).</head><p>The algorithm produces a solution x(λ) that satisfies the following properties with high probability (i.e., probability</p><formula xml:id="formula_226">≥ 1 -1/n 2 ) in time O * (mε -2 R 2 /Φ(P )): (a) Ax(λ) -b ≤ ε b , (b) f (x(λ)) ≤ OPT + ε b λ + nθ,</formula><p>where OPT is the cost of optimal assignment of the primal optimization problem (P) and λ ≤ λ 0 + 2 λ *λ 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Analysis of Algorithm</head><p>In this section, we shall establish the proof of Theorem 7.2. In order to establish the proof, we will state a sequence of results (in terms of Lemmas and a Theorem) whose proofs are defered to the next section. Using these results, we will conclude the proof.</p><p>To this end, first we wish to establish bound on the number of iterations required by the algorithm used for inner run to obtain a solution x λ k such that Ax λ kb ≤ ε b , and we also prove an approximation bound on the objective function value of the final solution. We assume in this analysis that the summation subroutine used by an inner run is always successful; that is, (7.6) holds for every sum computation. Furthermore, we assume that an inner run executes until both stopping conditions are satisfied.</p><p>The possibility of an inner run being terminated by the outer loop was addressed in Section 7.2.2.3. As explained there, in what follows we will establish bound on number of iterations taken by inner loop for given value of R in Theorem 7.8. And hence for a correct pair of values q 1 , Q 2 such that R/4 ≤ Q 2 /q 1 ≤ 4R, the outer loop will not terminate the inner loop before both stopping conditions are satisfied. Therefore, in the analysis that follows to establish the validity of this claim we can safely ignore the possibility of outer loop terminating inner loop. We shall also assume that the value of R is known in the analysis to follow.</p><p>To this end, first we consider the extent to which ∆λ k-1 deviates from the correct gradient ∇g θ λ k-1 , provided that the inner run does not terminate in iteration k. To this end, let u k-1 = ŝk-1s k-1 be a vector representing the error in the computation of s k-1 . Note that ∆λ k-1 = ∇g θ λ k-1 + u k-1 . Lemma 7.3. If the stopping conditions (7.7) and (7.8) are not both satisfied in iteration k, then</p><formula xml:id="formula_227">u k-1 ≤ α 1 2 + ε 3 ∇g θ λ k-1 (7.10) and 1 -α 1 2 + ε 3 ∇g θ λ k-1 ≤ ∆λ k-1 ≤ 1 + α 1 2 + ε 3 ∇g θ λ k-1 . (7.11)</formula><p>Next, we develop some inequalities that will be useful in understanding the evolution of an inner run from one iteration to the next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 7.4. For any two points</head><formula xml:id="formula_228">ρ 1 , ρ 2 ∈ B λ * , λ 0 -λ * , Ax ρ 2 -Ax ρ 1 ≤ Q λ 0 -λ * σ max A T 2 ρ 2 -ρ 1 (7.12)</formula><p>and We now show that all the dual vectors generated by an inner run are as close to the optimal solution λ * as the initial point λ 0 . Lemma 7.6. For each iteration k executed by an inner run, λ k-1 ∈ B λ * , λ 0λ * .</p><formula xml:id="formula_229">∇g θ ρ 2 -∇g θ ρ 1 T ρ 2 -ρ 1 ≤ -q λ 0 -λ * σ min A T 2 ρ 2 -ρ 1 2 . (<label>7</label></formula><p>To establish that an inner run makes progress as it executes iterations, we show that the norm of the gradient of g θ λ k , p λ k = Ax λ kb , decreases by a multiplicative factor in each iteration. Lemma 7.7. For each iteration k executed by an inner run in which the stopping conditions are not satisfied,</p><formula xml:id="formula_230">∇g θ λ k ≤ 1 - 1 4R 2 ∇g θ λ k-1 .</formula><p>Lemma 7.7 implies an upper bound on the number of iterations executed by an inner run. Finally, we bound the difference between the objective function value of the solution produced by an inner run and the optimal value of the primal problem. Let OPT denote the optimal value of (P). Since the dual solution λ produced by the algorithm satisfies λ ≤ λ 0 + 2 λ 0λ * , by choosing the parameters ε and θ appropriately, the approximation error can be made as small as desired (though, of course, the convergence time increases as each of these parameters decreases).</p><p>Proof. [Theorem 7.2] In order to prove the claims of the Theorem, in light of Theorem 7.8 and Corollary 7.9, it is sufficient to establish that our algorithm indeed executes the inner loop with the a good estimate of R upto O * (R 2 ) iterations with high probability. As explained in Section 7.2.2.3, the appropriate choice of δ along with binary search over proper set of log 2 U , q 1 , Q 2 pairs the algorithm will indeed have this property. Therefore, the output of the algorithm is indeed satisfying (a) and (b) properties with high probability. The bound on computation time follows from the accounting performed in Section 7.2.3 along with Lemma 7.1. This completes the proof of Theorem 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4.1">Remaining proofs</head><p>Here, we present the remaining proofs of the results stated above to establish Theorem 7. Suppose that (7.7) is satisfied and (7.8) is not satisfied. Note that (7.6) implies that u k-1 ≤ ε 1 s k-1 , and so (7.7) and (7.6) yield</p><formula xml:id="formula_231">u k-1 ≤ ε 1 s k-1 ≤ ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b . (7.15)</formula><p>By the triangle inequality and (7.15),</p><formula xml:id="formula_232">∆λ k-1 = ŝk-1 -b = ∇g θ λ k-1 + u k-1 ≤ ∇g θ λ k-1 + u k-1 ≤ ∇g θ λ k-1 + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b ,</formula><p>and so the fact that (7.8) is not satisfied implies that </p><formula xml:id="formula_233">∇g θ λ k-1 ≥ ŝk-1 -b -ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b &gt; 2 3 ε + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b -ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b = 2 3 ε b . (<label>7</label></formula><formula xml:id="formula_234">∇g θ λ k-1 &gt; 2 3 ε b .</formula><p>Now, applying the triangle inequality yields</p><formula xml:id="formula_235">u k-1 ≤ ε 1 s k-1 ≤ ε 1 ∇g θ λ k-1 + b ≤ ε 1 1 + 3 2ε ∇g θ λ k-1 = α 1 2 + ε 3 ∇g θ λ k-1 ,</formula><p>where the last equality follows from the fact that ε 1 = εα/3. This proves inequality in (7.10), and the inequalities in <ref type="bibr">(7.11)</ref> follow from (7.10) and the triangle inequality.</p><p>Proof of Lemma 7.4: Let ρ 1 , ρ 2 denote the line segment joining ρ 1 and ρ 2 . Since B λ * , λ 0λ * is a convex set, for any i = 1, . . . , n and any λ ∈ ρ 1 , ρ 2 , h -1 i -a T i λ ≤ Q λ 0λ * . As a result,</p><formula xml:id="formula_236">x i ρ 2 -x i ρ 1 = h -1 i -a T i ρ 2 -h -1 i -a T i ρ 1 ≤ Q λ 0 -λ * a T i ρ 2 -ρ 1 = Q λ 0 -λ * a T i ρ,</formula><p>where ρ ∈ R m is defined by ρ j = |ρ 2 jρ 1 j | for j = 1, . . . , m. This implies that</p><formula xml:id="formula_237">Ax ρ 2 -Ax ρ 1 = A x ρ 2 -x ρ 1 ≤ Q λ 0 -λ * AA T ρ ≤ Q λ 0 -λ * σ max AA T ρ = Q λ 0 -λ * σ max A T 2 ρ 2 -ρ 1 ,</formula><p>and the inequality in (7.12) is proved.</p><p>For any λ ∈ ρ 1 , ρ 2 and any µ ∈ R m , a calculation analogous to the one in (7.5) yields</p><formula xml:id="formula_238">µ T ∇ 2 g θ (λ)µ = - m j 1 =1 µ j 1 m j 2 =1 n i=1 A j 1 i A j 2 i h -1 i -a T i λ µ j 2</formula><p>≤ -q λ 0λ * µ T AA T µ ≤ -q λ 0λ * σ min AA T µ = -q λ 0λ * σ min A T 2 µ (7.17)</p><p>From the second-order expansion of the function g θ , there exist vectors µ 1 , µ 2 ∈ ρ 1 , ρ 2 such that</p><formula xml:id="formula_239">g θ ρ 2 = g θ ρ 1 + ∇g θ ρ 1 T ρ 2 -ρ 1 + 1 2 ρ 2 -ρ 1 T ∇ 2 g θ µ 1 ρ 2 -ρ 1 g θ ρ 1 = g θ ρ 2 + ∇g θ ρ 2 T ρ 1 -ρ 2 + 1 2 ρ 1 -ρ 2 T ∇ 2 g θ µ 2 ρ 1 -ρ 2</formula><p>Adding the two equations and applying (7.17) yields</p><formula xml:id="formula_240">∇g θ ρ 2 -∇g θ ρ 1 T ρ 2 -ρ 1 = 1 2 ρ 2 -ρ 1 T ∇ 2 g δ µ 1 ρ 2 -ρ 1 + 1 2 ρ 1 -ρ 2 T ∇ 2 g δ µ 2 ρ 1 -ρ 2</formula><p>≤ -q λ 0λ * σ min A T 2 ρ 2ρ 1 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Algorithm: Description and Performance Analysis 111</head><p>This establishes the inequality in (7.13) and completes the proof of the lemma.</p><p>Proof of <ref type="bibr">Corollary 7.5:</ref> This follows from an application of Lemma 7.4 with ρ 1 = λ * and ρ 2 = λ, using the additional observations that ∇g θ (λ) = Ax(λ)b = Ax(λ) -Ax (λ * ), and ∇g θ (λ * ) = 0 because λ * is an optimal solution to (D θ ).</p><p>Proof of <ref type="bibr">Lemma 7.6:</ref> The proof is by induction on k, the iteration number. For the base case k = 1, λ k-1λ * = λ 0λ * . In the inductive case, we assume that the statement is true for an iteration k, and we show that it then holds for iteration k + 1, where k ≥ 1. As such, the inductive hypothesis is that λ k-1 ∈ B λ * , λ 0λ * . If the algorithm executes iteration k + 1, then it does not terminate in iteration k, and λ kλ k-1 = t∆λ k-1 . The squared distance from λ k to λ * can be expressed as follows.</p><formula xml:id="formula_241">λ k -λ * 2 = λ k -λ k-1 + λ k-1 -λ * 2 = λ k-1 -λ * 2 + λ k -λ k-1 2 + 2 λ k -λ k-1 T λ k-1 -λ * = λ k-1 -λ * 2 + t 2 ∆λ k-1 2 + 2t ∆λ k-1 T λ k-1 -λ * (7.18)</formula><p>The third term in the right-hand side of (7.18) can be bounded from above by applying the inductive hypothesis, Corollary 7.5, Lemma 7.3, and the Cauchy-Schwarz inequality.</p><formula xml:id="formula_242">∆λ k-1 T λ k-1 -λ * = ∇g θ λ k-1 + u k-1 T λ k-1 -λ * ≤ -q λ 0 -λ * σ min A T 2 λ k-1 -λ * 2 + u k-1 λ k-1 -λ * ≤ λ k-1 -λ * 2 α 1 2 + ε 3 Q λ 0 -λ * σ max A T 2 -q λ 0 -λ * σ min A T 2</formula><p>Substituting this inequality into (7.18), and again applying the inductive hypothesis and Lemma 7.3, yields</p><formula xml:id="formula_243">λ k -λ * 2 ≤ λ k-1 -λ * 2 1 + t 2 1 + α 1 2 + ε 3 2 × Q λ 0 -λ * σ max A T 2 2 + 2t α 1 2 + ε 3 Q λ 0 -λ * σ max A T 2 -q λ 0 -λ * σ min A T 2 .</formula><p>As αQ λ 0λ * σ max A T 2 = q λ 0λ * σ min A T 2 /6, we will have the sequence of inequalities λ kλ * ≤ λ k-1λ * ≤ λ 0λ * provided that</p><formula xml:id="formula_244">t ≤ 2 -1 3 1 2 + ε 3 q λ 0 -λ * σ min A T 2 1 + α 1 2 + ε 3 2 Q ( λ 0 -λ * ) σ max (A T ) 2 2 .</formula><p>The step size in (7.9) used by an inner run satisfies this inequality because ε ≤ 1. This completes the proof of the inductive case and of the lemma.</p><p>Proof of Lemma 7.7: If the stopping conditions are not satisfied in iteration k, then Lemma 7.6 implies that λ k-1 , λ k ∈ B λ * , λ 0λ * . The squared norm of the gradient of g θ at λ k can be expressed as</p><formula xml:id="formula_245">∇g θ λ k 2 = ∇g θ λ k -∇g θ λ k-1 + ∇g θ λ k-1 2 = ∇g θ λ k-1 2 + ∇g θ λ k -∇g θ λ k-1 2 + 2 ∇g θ λ k -∇g θ λ k-1 T ∇g θ λ k-1 . (7.19)</formula><p>An upper bound on the second term in the right-hand side of <ref type="bibr">(7.19)</ref> follows from Lemmas 7.3 and 7.4.</p><formula xml:id="formula_246">∇g θ λ k -∇g θ λ k-1 = Ax λ k -b -Ax λ k-1 -b = Ax λ k -Ax λ k-1 ≤ Q λ 0 -λ * σ max A T 2 λ k -λ k-1 = tQ λ 0 -λ * σ max A T 2 ∆λ k-1 ≤ t 1 + α 1 2 + ε 3 Q λ 0 -λ * σ max A T 2 ∇g θ λ k-1 (7.20)</formula><p>To bound the third term in the right-hand side of (7.19), we again apply Lemmas 7.3 and 7.4.</p><formula xml:id="formula_247">∇g θ λ k -∇g θ λ k-1 T ∇g θ λ k-1 = ∇g θ λ k -∇g θ λ k-1 T ∆λ k-1 -u k-1</formula><p>≤ -tq λ 0λ * σ min A T 2 ∆λ k-1 2 </p><formula xml:id="formula_248">+ u k-1 ∇g θ λ k -∇g θ λ k-1 ≤ -t 1 -α 1 2 + ε 3 2 q λ 0 -λ * σ min A T 2 ∇g θ λ k-1 2 + tα 1 2 + ε 3 1 + α 1 2 + ε 3 Q λ 0 -λ * × σ max A T 2 ∇g θ λ k-1 2 . (<label>7</label></formula><formula xml:id="formula_249">∇g θ λ k 2 ≤ ∇g θ λ k-1 2 1 + t 2 1 + α 1 2 + ε 3 2 × Q λ 0 -λ * σ max A T 2 2 + 2t 1 6 1 2 + ε 3 1 + α 1 2 + ε 3 -1 -α 1 2 + ε 3</formula><p>Since α ≤ 1/6 and ε ≤ 1, it follows that</p><formula xml:id="formula_250">∇g θ λ k 2 ≤ ∇g θ λ k-1 2   1- q λ 0 -λ * σ min A T 2 2Q ( λ 0 -λ * ) σ max (A T ) 2 2   ,</formula><p>and the proof is complete.</p><p>Proof of Theorem 7.8: If an inner run terminates with a solution x(λ), then the stopping conditions (7.7) and (7.8) are both satisfied for the estimate ŝ = s + u of the vector s = Ax(λ). Applying (7.6) and the triangle inequality yields</p><formula xml:id="formula_251">Ax (λ) -b = s -b ≤ ŝ -b + u ≤ ŝ -b + ε 1 s ≤ 2 3 ε + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b = 2 3 ε + 2εα 3 1 + ε 1 1 -ε 1 1 + 2 3 ε b .</formula><p>Because ε ≤ 1 and α ≤ 1/6, ε 1 = εα/3 ≤ 1/18, and so we obtain </p><formula xml:id="formula_252">≤ s k-1 -b + u k-1 ≤ 2 3 ε b + ε 1 s k-1 ≤ 2 3 ε + ε 1 1 + 2 3 ε b ≤ 2 3 ε + ε 1 1 + ε 1 1 -ε 1 1 + 2 3 ε b ,</formula><p>and so (7.8) is satisfied as well. Thus, if s k-1b ≤ (2/3)ε b , then the inner run will terminate in iteration k. Repeated application of Lemma 7.7 implies that, if an inner run does not terminate in or before an iteration k, then</p><formula xml:id="formula_253">∇g θ λ k ≤ 1 - 1 4R 2 k 2 p λ 0 .</formula><p>For k ≥ 8R 2 ln 3p λ 0 2ε b , we have ∇g θ λ k ≤ (2/3)ε b , and hence the stopping conditions will be satisfied and an inner run will terminate in the claimed number of iterations.</p><p>Proof of Corollary 7.9: Given the solution x(λ) produced by an inner run, define a vector ν (λ) ∈ R n ++ by, for all i = 1, . . . , n, ν i (λ) = θ x i (λ) .</p><p>The pair (λ, ν (λ)) is a feasible solution to the dual problem (D) with objective function value g(λ, ν(λ)) = inf</p><formula xml:id="formula_254">x∈R n + L(x, λ, ν(λ)) = -b T λ + n i=1 inf x i ∈R + f i (x i ) + a T i λ - θ x i (λ) x i .</formula><p>As the components of the vector x(λ) satisfy (7.2), we have L(x(λ), λ, ν(λ)) = g(λ, ν(λ)).</p><p>From the definition of the Lagrangian and the fact that (λ, ν(λ)) is feasible for (D), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Historical Notes</head><p>The design of distributed algorithms for convex minimization with linear constraints has been of interest since the early 1960s. The essence of the work before the mid-1980s is well documented in the book by Rockafellar <ref type="bibr" target="#b60">[62]</ref>. Rockafellar <ref type="bibr" target="#b60">[62]</ref> describes distributed algorithms for monotropic programs, which are separable convex minimization problems with linear constraints. These algorithms leverage the decomposable structure of the Lagrange dual problem arising from the separable primal objective. This structure has also been used to design parallel and asynchronous algorithms for monotropic programs; see the book by Bertsekas and Tsitsiklis <ref type="bibr" target="#b4">[5]</ref> for further details. All of these algorithms are by design distributed with respect to an appropriate constraint graph G C , as opposed to an underlying network G. For the special case of a network routing problem, the distributed algorithm of Gallager <ref type="bibr" target="#b23">[24]</ref> is intuitively "closer" to being distributed with respect to G; however, it still requires direct access to route information and hence is fully distributed with respect to the constraint graph G C only.</p><p>The network resource allocation problems that motivate the algorithm described here are special cases of monotropic programs. Kelly et al. <ref type="bibr" target="#b34">[36]</ref> used these known distributed algorithmic solutions to explain the congestion control protocols for the resource allocation problem. Moreover, they show that in an idealized model with perfect feedback (in the form of packet drops) by network queues, these algorithms can also be interpreted as distributed over G. See also Garg and Young <ref type="bibr" target="#b25">[26]</ref> for similar results that emphasize the rate of convergence to an optimal solution. See the book by Srikanthan <ref type="bibr" target="#b64">[66]</ref> for further work on congestion control.</p><p>Flow control also serves as the motivation for the work of Bartal et al. <ref type="bibr" target="#b3">[4]</ref> on distributed algorithms for positive linear programming (building on earlier work by Papadimitriou and Yannakakis <ref type="bibr" target="#b55">[57]</ref> and Luby and Nisan <ref type="bibr" target="#b42">[44]</ref>). In this model, there is a primal agent for each primal variable and a dual agent for each dual variable (or primal constraint). In <ref type="bibr" target="#b3">[4]</ref>, direct communication is permitted between a dual agent and all of the primal agents appearing in the corresponding constraint; in this model, <ref type="bibr">Bartal et al. [4]</ref> give a decentralized algorithm that achieves a (1 + ε)-approximation in a polylogarithmic number of rounds. We note that the results presented here are from work by Moskaoyama et al. <ref type="bibr" target="#b51">[53]</ref>. Again, a remark about practicality of this algorithm is in order. Indeed, like most solutions listed above, the algorithm presented here is unlikely to be useful as is in practice. However, it is a proof-of-concept for existence of such distributed solution and variant of it is likely to be useful in practice. Better solutions with practical utility naturally form topic of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We considered the question of designing Gossip algorithms motivated by applications to next generation networks such as sensor networks, peer-to-peer networks, mobile networks of vehicles, social networks, etc. These algrotihms are built upon a gossip or rumor style unreliable, asynchronous information exchange protocol. Due to immense simplicity and wide applicability, this class of algorithms have emerged as a canonical architectural solution for these next generation networks.</p><p>We started with the description of Gossip algorithm for information exchange. On this Gossip based information layer, we presented design of the linear dynamics based algorithm as well as the separable function computation algorithms. These algorithms were further utilized to design the network scheduling and the network convex optimization algorithm. Thus, in effect we described a whole 'Gossip based network algorithmic stack' here. An important conclusion is that the performance of Gossip algorithms is strongly dependent on the spectral properties of underlying network graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 . 1</head><label>21</label><figDesc>Fig. 2.1 An example of a Geometric Random Graph in two-dimensions. A node is connected to all other nodes that are within distance r of itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.1 is a pictorial description of Theorem 4.3. The x-axis denotes the mixing time and y-axis denotes the computation time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 . 1</head><label>41</label><figDesc>Fig. 4.1 Graphical interpretation of Theorem 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>2(b) explains their construction for the ring graph. Note that each node has two copies and the lifted graph is essentially composed of two rings: an inner ring and an outer ring. The transition on the inner circle forms a clockwise circulation and the transition on the outer circle forms a counterclockwise circulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.2 (a) Symmetric P on the ring graph G 1 . (b) Non-reversible P on the lifted ring graph G 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 4 . 1 (</head><label>41</label><figDesc>Pseudo-Lifting).A graph Ĝ = ( V , Ê) and a random walk P on it are called a pseudo-lifting of graph G = (V, E) and random walk P if there exists a many-to-one function f : V → V , T ⊂ V with |T | = |V | such that the following holds: (a) for any û, v ∈ V , (û, v) ∈ Ê only if (f (û), f(v)) ∈ E, and (b) for any</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the expected number of walks until visiting v and getting a directed path when X = 3 are O(D/δ) = O(D) in both cases. Now, τ (ε, P ) = O(H( P ) log ε -1 ) (see Preliminaries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 . 3</head><label>43</label><figDesc>Fig. 4.3 For a given line graph with n nodes, (a) is the star topology which was used in the construction of the pseudo-lifted graph in Section 4.3.2.1, and (b) is the hierarchical star topology which will be used in this section for the new construction of the pseudo-lifting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Lemma 4 . 5 .</head><label>45</label><figDesc>The mixing time of the random walk P defined by Q is O(D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 . 1</head><label>71</label><figDesc>Fig. 7.1 The kth iteration of an inner run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>. 13 ) 7 . 5 .</head><label>1375</label><figDesc>Corollary For anyλ ∈ B λ * , λ 0λ * , ∇g θ (λ) ≤ Q λ 0λ * σ max A T 2 λλ * ,and∇g θ (λ) T (λλ * ) ≤ -q λ 0λ * σ min A T 2 λλ * 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Theorem 7 . 8 .</head><label>78</label><figDesc>An inner run terminates after O R 2 log p λ 0 ε b iterations with a solution x(λ) such that Ax (λ)b ≤ ε b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Corollary 7 . 9 .</head><label>79</label><figDesc>The objective function value of the solution x(λ) produced by an inner run satisfiesf (x(λ)) ≤ OPT + ε b λ + nθ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 . 3 ε b or ŝk- 1 &gt; ( 1 + ε 1</head><label>23111</label><figDesc>Proof ofLemma 7.3:  If (7.7) is not satisfied, then ŝk-1 &lt; (1ε 1 ) 1 -2By the triangle inequality, this implies that∇g θ λ k-1 = s k-1b ≥ s k-1 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Ax (λ)b ≤ ε b . Now, consider an iteration k such that s k-1b ≤ (2/3)ε b . Since s k-1b ≤ s k-1b , (7.6) implies that (1ε 1 ) 1 -2 3 ε b ≤ ŝk-1 ≤ (1 + ε 1 ) 1 +23 ε b , and (7.7) is satisfied. Moreover, ŝk-1b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>f</head><label></label><figDesc>(x(λ)) + λ T (Ax(λ)b)ν(λ) T x(λ) = L(x(λ), λ, ν(λ)) = g(λ, ν(λ)) ≤ OPT.Applying the Cauchy-Schwarz inequality and Theorem 7.8 yields the claimed upper bound on the objective function value of the vector x(λ).f (x(λ)) ≤ OPTλ T (Ax(λ)b) + ν(λ) T x(λ) ≤ λ Ax(λ)b + n i=1 θ x i (λ)x i (λ)≤ OPT + ε b λ + nθ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Now, for the purpose of analysis, consider dividing time into epochs of length 4 Φ(P )/n, and executing the information dissemination algorithm from the initial state in each epoch, independently of the other epochs. The probability that, after log ε -1 epochs, some execution of the algorithm has run to completion in its epoch is greater than 1ε. Using the running time of this virtual process as a stochastic upper bound on the running time of the actual algorithm, we conclude that</figDesc><table /><note><p>.16) By Markov's inequality, the inequality in (3.16) implies that Pr(L n &gt; 4 Φ(P )/n) &lt; 1/2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 Thus, effectively (i.e., upto log n factor) the mixing time scales like diameter n. It should be noted that because lifting preserves the graph topology and the stationary distribution, it is possible to simulate this lifted random walk on the original graph by expanding the state appropriately. Equivalently, if used for linear averaging it is possible to use lifted random walks by running iterations with extra states.</figDesc><table /><note><p><p><p>4.3.2 Non-Reversible Random Walks: General Graph</p>Given graph G and a symmetric doubly stochastic matrix P (say, obtained by Metropolis-Hasting method), here we describe a construction of a 'lifted graph' Ĝ and a non-reversible random walk with transition matrix P on Ĝ so that the mixing time of P is of the order of the diameter D of G. This construction will result in an increase in the size of Ĝ, i.e., increase in | P | o compared to |P | o . Later we will improvise over this construction for graphs with 'geometry' to obtain P with | P | o close to |P | o . First, we formally define the notion of lifting, called 'Pseudo-lifting'.</p>2   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>1 Comparison of the determinstic linear algorithm based on pseudo-lifting with the randomized algorithm based on Metropolis-Hasting method.</figDesc><table><row><cell></cell><cell></cell><cell>Rand. algo.</cell><cell>Det. algo.</cell><cell>Optimal</cell></row><row><cell>Running time: k-grid graph</cell><cell>Ω</cell><cell>1 Φ 2 (P ) : O</cell></row></table><note><p>* (n</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>5.1. Let W 1 , . . . , W n be n independent random variables such that, for i = 1, . . . , n, the distribution of W i is Exponential with rate λ i &gt; 0, i.e., E[W i ] = 1/λ i . Let W be the minimum of W 1 , . . . , W n . Then, W is an Exponential random variable of rate λ = n i=1 λ i , i.e., E[W] = 1/λ. Proof. For an Exponential random variable U with rate µ, for any z</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>.16) Combining (7.14) and (7.16), it follows that if the two stopping conditions are not both satisfied, then</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>.21) Substituting (7.20) and (7.21) in (7.19) yields</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>By a reasonable change, here we mean dynamics that allow for a possibility of eventual computation of the desired function in a distributed manner. For example, if a node i becomes disconnected from the rest of the graph forever, then it will consist of unreasonable change as per our terminology.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>For a function f : N → R + , O * (f (n)) := O(f (n)poly(log n)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>It is not called lifting because, the term lifting was used in<ref type="bibr" target="#b15">[16]</ref> differently and Pseudolifting used here is in a sense a 'relaxation' of lifting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p><ref type="bibr" target="#b2">3</ref> In fact, 1/2 can be replaced by δ for any constant δ ∈ (0, 1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4"><p>The model ignores multi-hop situation. However, as explained in<ref type="bibr" target="#b65">[67]</ref>, the algorithm presented can be easily extended for the case of multi-hop situation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to acknowledge support of NSF projects CNS 0626764, CCF 0728554 and HSD 0729361 while this article was written. He wishes to thank Damon Mosk-Aoyama for various collaborations on the topic of Gossip algorithms that have provided a bulk of the material for this article. Author wishes to thank Jinwoo Shin for various useful discussions. Finally, he also wishes to thank Tauhid Zaman and an anonymous reviewer for suggestions to improve the readability of this article.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Now the algorithm for computing r minimums takes O(r(log n + log r + log ε -1 )/Φ(P )) with probability at least 1ε, for ε ∈ (0, 1/2) as per <ref type="bibr">(5.2)</ref>. Therefore, for r = r(ε, δ) it takes time O(δ -2 log(nε -1 δ -1 )/ Φ(P )) for all nodes to compute the minimum with probability at least 1ε/2. From Properties 5.1 and 5.2, after the minimum computation algorithm stops at all nodes, each node has an estimate of x sum such that it is within [(1δ)x sum , (1 + δ)x sum ) for all δ ∈ (0, 1/2) with probability at least 1ε/2. Therefore, it follows by the union bound that under the above stated algorithm all nodes have an estimation of x sum within 2δ-accuracy with probability at least 1ε after time O(δ -2 log(nε -1 δ -1 )/Φ(P )). Therefore, we conclude the following result.</p><p>Theorem 5.1. Let P be an irreducible, doubly stochastic and symmetric matrix on graph G. Then, for any ε, δ ∈ (0, 1/2) the summation computation algorithm described above based on P , leads to an estimation of x sum within [(1δ)x sum , (1 + δ)x sum ] for all nodes in V with probability at least 1ε within time T sum (ε, δ) where</p><p>δ 2 Φ(P ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Applications</head><p>Here, we will utilize Theorem 5.1 to evaluate the performance of the Gossip algorithm for summation or equivalently separable function computation on various graph models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.1">Complete graph</head><p>Recall that the complete graph represents situations when all nodes can communicate with each other. For a complete graph of n nodes with natural symmetric probability matrix P = [1/n], as explained in Preliminaries,</p><p>That is, the algorithm performs computation essentially as fast as possible for any fixed δ.</p><p>with B &gt; 0 a constant and sup x∈C L(x) &lt; ∞. Then, (a) Markov chain is positive recurrent with a unique stationary</p><p>Such results are popularly known as Foster-Lyapunov criteria for establishing positive recurrence of Markov chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Stability: Theorem and Proof</head><p>Here, we state the formal result establishing the throughput optimality of SCH and its proof in detail.</p><p>Theorem 6.2. The algorithm SCH described above using parameter δ &gt; 0 is stable as long as λ ∈ (1δ)Co(I). Further,</p><p>Proof. At time t, define Lyapunov function</p><p>We will study the 'average drift' in L(•) at time slots t k = kT for large enough T (will be 2.2 n ) so that for λ ∈ (1δ)Co(I)</p><p>Our step size and convergence rate will depend on a parameter R ≥ 1, defined as</p><p>The parameter R measures the maximum curvature variation of the Lagrange dual function only in a ball of radius λ 0λ * around the optimal dual solution λ * ; this is because the sequence of dual solutions generated by our algorithm grows monotonically closer to λ * , and we are concerned only with variation in the region in which our algorithm executes (as opposed to the entire feasible region, which is all of R m ). Thus a better initial estimate of the optimal dual solution yields a tighter bound on curvature variation and a better convergence result.</p><p>In the convergence analysis that will follow the algorithm description, first we shall assume that the inner run knows values of numerator and denominator of R. Later in the analysis, justification for this assumption will be provided by means of a binary search based algorithm that will estimate it. Now, define α = 1/6R. For the summation subroutine, nodes use the accuracy parameter ε 1 = εα/3, where ε is the error tolerance given to the distributed algorithm. For gradient ascent, nodes compute and employ the following step size:</p><p>Here, t &gt; 0 since α ≤ 1/6 and ε ≤ 1. An inner run continues to execute iterations for increasing values of k until both stopping conditions are satisfied, or the outer loop of the algorithm terminates the inner run as described in Section 7.2.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.3">Outer loop and stopping conditions</head><p>Here, we primarily describe the outer loop of the algorithm that leads to determination of parameters used by the inner loop. The termination or stopping conditions for the algorithm will be described as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations and Acronyms</head><p>Z, The set of all integers {. . . , -2, -1, 0, 1, 2, . . .}. N, The non-negative integers. R, R + and R ++ , Real numbers, non-negative real numbers and strictly positive real numbers respectively.</p><p>Bold small letters, (e.g., x), a vector [x i ] ∈ X n of real numbers (X = R), or integers (X = Z). Usually, vectors will be assumed to be represented in the 'column' form. 0 and 1 vector of all zeros and all ones, respectively. 1 {•} , Indicator function for checking the 'truth' of the condition '•', i.e., 1 true = 1 and 1 false = 0. 1 {x} , A vector whose ith component is</p><p>x, y , n i=1 x i y i (for n-dimensional vectors x and y).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some inequalities for reversible Markov chains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="564" to="576" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Plongements lipschitziens dans R n</title>
		<author>
			<persName><forename type="first">P</forename><surname>Assouad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Société Mathématique de France</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="448" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Information theoretic approaches to distributed function computation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ayaso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast, distributed approximation algorithms for positive linear programming with applications to flow control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1261" to="1279" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Parallel and Distributed Computation: Numerical Methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convergence in multiagent coordination, consensus, and flocking</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olshevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint 44th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC&apos;05)</title>
		<imprint>
			<date type="published" when="2005-12">December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastest mixing Markov chain on a graph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randomized gossip algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transaction on Networking</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="2508" to="2530" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Throughput guarantees through maximal scheduling in wireless networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaporkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Allerton Conference on Communication Control and Computing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal cross-layer congestion control, routing and scheduling design in ad-hoc wireless networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate aggregation techniques for sensor databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Considine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Byers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The throughput of switches with and without speedup</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom</title>
		<meeting>IEEE Infocom</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algebraic gossip: A network coding approach to optimal multiple rumor mongering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Médard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choute</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large Deviations Techniques and Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zeitouni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis of a non-reversible Markov chain sampler</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="726" to="752" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moderate growth and random walk on finite groups</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saloff-Coste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geometric and Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geographic gossip: Efficient aggregation for sensor networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International ACM/IEEE Symposium on Information Processing in Sensor Networks (IPSN &apos;06)</title>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal throughputdelay scaling in wireless networks-part I: The fluid model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mammen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2568" to="2592" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the convergence of asynchronous paracontractions with applications to tomographic reconstruction from incomplete data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koltracht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and Its Applications</title>
		<imprint>
			<biblScope unit="issue">130</biblScope>
			<biblScope unit="page" from="65" to="82" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scale free aggregation in sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enachescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Algorithmic Aspects of Wireless Sensor Networks</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed cross-layer algorithms for the optimal control of multi-hop wireless networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eryilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Modiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>accepted to appear</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic counting algorithms for data base applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="209" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A minimum delay routing algorithm using distributed computation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On-line end-to-end congestion control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Randomized scheduling algorithms for high-aggregate bandwidth switches</title>
		<author>
			<persName><forename type="first">P</forename><surname>Giaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Select Areas Communication High-performance Electronic Switches/Routers for High-speed Internet</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="546" to="559" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The capacity of wireless networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Information Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="404" />
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Link scheduling in polynomial time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coordination of groups of mobile autonomous agents using nearest neighbor rules</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="988" to="1001" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Low delay scheduling in wireless network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISIT</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimizing rate of convergence for iterative algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>accepted to appear</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Quantized consensus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rate control for communication networks: Shadow prices, proportional fairness and stability</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maulloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="252" />
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gossip-based computation of aggregate information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dobra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS &apos;03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">482</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A decentralized algorithm for spectral analaysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theory of Computing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective information dissemination in P2P networks: Problems and solutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koubarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tryfonopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Drougas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="71" to="76" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A tutorial on cross-layer optimization in wireless networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Submitted, Available Through csl.uiuc.edu/rsrikant</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Impact of imperfect scheduling in wireless networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Lecture notes on Expander Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<ptr target="http://www.math.ias.edu/∼avi/BOOKS/expanderbookr1.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mixing times</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microsurveys in Discrete Probability</title>
		<title level="s">DIMACS Series in Discrete Mathematics and Theoretical Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Aldous</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Propp</surname></persName>
		</editor>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="85" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A parallel approximation algorithm for positive linear programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 25th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Product multi-commodity flow in wireless networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Leveque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1460" to="1476" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tag: A tiny aggregation service for ad-hoc sensor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="131" to="146" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Coupon replication systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Massoulié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vojnovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMETRICS/Performance</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">iSLIP: A scheduling algorithm for input-queued switches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Achieving 100% throughput in an input-queued switch</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom</title>
		<meeting>IEEE Infocom</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
		<title level="m">Markov Chains and Stochastic Stability</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Maximizing throughput in wireless network via gossiping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Modiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zussman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMETRICS/Performance</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mathematical aspects of mixing times in Markov chains</title>
		<author>
			<persName><forename type="first">R</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tetali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="354" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fully distributed algorithms for convex optimization problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mosk-Aoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roughgarden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Distributed Computation (DISC)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Information dissemination via network coding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mosk-Aoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ISIT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast distributed algorithms for computing separable functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mosk-Aoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2997" to="3007" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distributed subgradient methods for multi-agent optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nedic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LIDS Report 2755, to appear in IEEE Transactions on Automatic Control</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Linear programming without the matrix</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth Annual ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Oxford Studies in Probability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Penrose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
	<note>Random Geometric Graphs</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modeling and performance analysis of bittorrent-like peer-to-peer networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Network adiabatic theorem: An efficient randomized protocol for contention resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM SIGMETRICS/Performance</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Entropy waves, The zig-zag graph product, and new constant-degree expanders and extractors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Network Flows and Monotropic Optimization</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1984">1998. 1984</date>
		</imprint>
	</monogr>
	<note>republished by Athena Scientific</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On traveling salesperson problems for Dubins&apos; vehicle: stochastic and dynamic environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Savla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CDC-ECC</title>
		<meeting><address><addrLine>Seville, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12">December 2005</date>
			<biblScope unit="page" from="4530" to="4535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stable algorithms for input queued switches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Allerton Conference on Communication, Control and Computing</title>
		<meeting>Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimal scheduling algorithm for input queued switch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wischik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The Mathematics of Internet Congestion Control</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Birkhäuser</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ephremides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1936" to="1948" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Non-approximability results for optimization problems on bounded degree instances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM STOC</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Problems in decentralized decision making and computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Information and Decision Systems, MIT</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
