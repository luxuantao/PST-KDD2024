<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximation Algorithms for the Metric Labeling Problem via a New Linear Programming Formulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2000-09-22">September 22, 2000</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chandra</forename><surname>Chekuri</surname></persName>
							<email>chekuri@research.bell-labs.com.</email>
						</author>
						<author>
							<persName><forename type="first">Sanjeev</forename><surname>Khanna</surname></persName>
							<email>sanjeev@cis.upenn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer &amp; Information Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonid</forename><surname>Zosin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Bell Labs</orgName>
								<address>
									<addrLine>600 Mountain Ave</addrLine>
									<postCode>07974</postCode>
									<settlement>Murray Hill</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">ITG Inc. This work done while at NEC Research Institute, Inc</orgName>
								<address>
									<addrLine>4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Bell Labs</orgName>
								<address>
									<addrLine>600 Mountain Ave</addrLine>
									<postCode>07974</postCode>
									<settlement>Murray Hill</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Dept</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Technion</settlement>
									<region>Haifa</region>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Approximation Algorithms for the Metric Labeling Problem via a New Linear Programming Formulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2000-09-22">September 22, 2000</date>
						</imprint>
					</monogr>
					<idno type="MD5">3531504149C9183CA79B927D975B86A3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider approximation algorithms for the metric labeling problem. Informally speaking, we are given a weighted graph that specifies relations between pairs of objects drawn from a given set of objects. The goal is to find a minimum cost labeling of these objects where the cost of a labeling is determined by the pairwise relations between the objects and a distance function on labels; the distance function is assumed to be a metric. Each object also incurs an assignment cost that is label, and vertex dependent. The problem was introduced in a recent paper by Kleinberg and Tardos [19], and captures many classification problems that arise in computer vision and related fields. They gave an O(log k log log k) approximation for the general case where k is the number of labels and a 2-approximation for the uniform metric case. More recently, Gupta and Tardos [14] gave a 4-approximation for the truncated linear metric, a natural non-uniform metric motivated by practical applications to image restoration and visual correspondence.</p><p>In this paper we introduce a new natural integer programming formulation and show that the integrality gap of its linear relaxation either matches or improves the ratios known for several cases of the metric labeling problem studied until now, providing a unified approach to solving them. Specifically, we show that the integrality gap of our LP is bounded by O(log k log log k) for general metric and 2 for the uniform metric thus matching the ratios in <ref type="bibr" target="#b18">[19]</ref>. We also develop an algorithm based on our LP that achieves a ratio of 2 + p 2 ' 3:414 for the truncated linear metric improving the ratio provided by <ref type="bibr" target="#b13">[14]</ref>. Our algorithm uses the fact that the integrality gap of our LP is 1 on a linear metric. We believe that our formulation has the potential to provide improved approximation algorithms for the general case and other useful special cases. Finally, our formulation admits general non-metric distance functions. This leads to a non-trivial approximation guarantee for a non-metric case that arises in practice <ref type="bibr" target="#b19">[20]</ref>, namely the truncated quadratic distance function. We note here that there are non-metric distance functions for which no bounded approximation ratio is achievable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motivated by certain classification problems that arise in computer vision and related fields, Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> recently introduced the metric labeling problem. In a typical classification problem, one wishes to assign labels to a set of objects to optimize some measure of the quality of the labeling. The metric labeling problem captures a broad range of classification problems where the quality of a labeling depends on the pairwise relations between the underlying set of objects. More precisely, the task is to classify a set V of n objects by assigning to each object a label from a set L of labels. The pairwise relationships between the objects are represented by a weighted graph G = (V; E) where w(u; v) represents the strength of the relationship between u and v. The objective is to find a labeling, a function f : V ! L that maps objects to labels, where the cost of f, denoted by Q(f), has two components.</p><p>For each u 2 V , there is a non-negative assignment cost c(u; i) to label u with i. This cost reflects the relative likelihood of assigning labels to u.</p><p>For each pair of objects u and v, the edge weight w(u; v) measures the strength of their relationship. The assumption is that strongly related objects should be assigned labels that are close. This is modeled in the objective function by the term w(u; v) d(f(u); f(v)) where d( ; ) is a metric on the labels L.</p><formula xml:id="formula_0">Thus Q(f) = X u2V c(u; f(u)) + X (u;v)2E w(u; v) d(f(u); f(v))</formula><p>and the goal is to find a labeling of minimum cost. We remark that if the distance function d is not a metric, then determining whether a graph can be colored by k colors is a special case of the labeling problem.</p><p>A prototypical application of the metric labeling problem is the image restoration problem in computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. In the image restoration problem, the goal is to take an image corrupted by noise and restore it to its "true" version. The image consists of pixels and these are the objects in the classification problem. Each pixel has a discretized intensity value associated with it that is possibly corrupted and we would like to restore it to its true value. Thus the labels correspond to intensity values and the goal is to assign a new intensity to each pixel. Further, neighbouring pixels are assumed to be close to each other, since intensities change smoothly. Thus neighbouring pixels have an edge between with a positive weight (assume a uniform value for concreteness). The original or observed intensities are assumed to be close to true values 1 . The assignment cost is some positive cost associated with changing the intensity from its original value to a new value, the larger the change, the larger the cost. To see how the cost function models the restoration consider a pixel corrupted by noise resulting in it having an observed intensity very different from all its neighbouring pixels. By changing the pixel's intensity we incur a cost of relabeling but that can be offset by the edge cost saved by being closer to its neighbours. The assignment costs weigh the labeling in favor of the original values since most of the intensities are likely to be correct (see also the footnote).</p><p>The metric labeling problem naturally arises in many other applications in image processing and computer vision. Researchers in these fields have developed a variety of good heuristics that use classical combinatorial optimation techniques such as network flow and local search, to solve these problems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> formalized the metric labeling problem and its connections to Markov random fields and other classification problems (see <ref type="bibr" target="#b18">[19]</ref> for a thorough description of the various connections). Metric 1 This assumption is based on the connection of the labeling problem to the theory of Markov Random Fields (MRFs). In this theory the observed data or labeling of the objects is assumed to be obtained from a true labeling by a perturbation of independent random noise. The idea is to decide the most probable labeling given the observed data. A MRF can be defined by a graph on the objects with edges indicating dependencies between objects. Under the assumption that the probability that an object's label depends only on its neighbours labels, and if the MRF satisfies two standard assumptions of homogeneity and pairwise interactions, the labeling problem can be restated as the problem of finding a labeling f that maximizes the a posteriori probability Pr f jf 0</p><p>] where f 0 is the observed labeling. We refer the reader to <ref type="bibr" target="#b18">[19]</ref> for more details on the connection of metric labeling to MRFs. labeling also has rich connections to some well known problems in combinatorial optimization. It is related to the quadratic assignment problem, an extensively studied problem in Operations Research. A special case of metric labeling is the 0-extension problem, studied by Karzanov <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. There are no assignment costs in this problem, however, the graph contains a set of terminals, t 1 ; : : : ; t k , where the label of terminal t i is fixed in advance to i and the non-terminals are free to be assigned to any of the labels. Clearly, the 0-extension problem generalizes in a strong way the well-studied multi-way cut problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. Karzanov <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> showed that certain special cases (special metrics) of the 0-extension problem can be solved optimally in polynomial time.</p><p>Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> obtained an O(log k log log k) approximation for the general metric labeling problem, where k denotes the number of labels in L and a 2-approximation for the uniform metric. However, the best lower bound on approximability for this problem is only max-snp hardness which follows from the max-snp hardness of the multi-way cut problem. Given the rich connections of this problem to other well-studied optimization problems and a variety of applications, a natural interesting question is to determine the approximability of the general problem as well as important special cases that arise in practice.</p><p>The Truncated Linear Metric There are many special cases of the problem that are interesting in their own right from both theoretical and applications point of view. One reason to solve special cases is because they give insight into the problem. In <ref type="bibr" target="#b18">[19]</ref> the algorithm for the uniform case was used to approximate the general case. Very recently, Gupta and Tardos <ref type="bibr" target="#b13">[14]</ref> considered the truncated linear metric, a special case motivated by its direct applications to two problems in computer vision, namely image restoration and visual correspondence. We briefly describe the application of this metric to the image restoration problem discussed earlier, see <ref type="bibr" target="#b13">[14]</ref> for more details. Consider the case of grey scale images where the intensities are integer values. Our earlier description assumed that intensities of neighbouring pixels should be close since the image is typically smooth. This motivates a linear-like metric on the labels where the distance between two intensities i and j is ji ? jj.</p><p>However the assumption of closeness breaks down at object boundaries (here we are referring to objects in the image) where sharp changes in intensities happen. Thus, for the metric to be robust, neighbouring pixels that are actually at object boundaries (and hence differ in their intensities by large amounts) should not be penalized by arbitrarily large quantities. This motivated Gupta and Tardos to consider a truncated linear metric, where the distance between two intensities i and j is given by d(i; j) = min(M; ji ? jj). Thus, the penalty is truncated at M; this is a natural (and non-uniform) metric for the problem at hand. A very similar reasoning applies to the visual correspondence problem, where the objective is to compare two images of the same scene for disparity. The labels here correspond to depth of the point in the image from the camera. For the truncated linear metric a 4-approximation was developed in <ref type="bibr" target="#b13">[14]</ref> using a local search algorithm. The local moves in their algorithm make use of the flow network developed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> that shows that the linear metric case can be solved to optimality.</p><p>For the image restoration application other distance functions have also been studied. In particular the quadratic distance function d(i; j) = ji ? jj 2 and its truncated version d(i; j) = minfM; ji ? jj 2 g have been considered (see <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b15">[16]</ref>). Unfortunately both these functions do not form a metric, however as we discuss shortly, we are able to provide non-trivial approximation ratios for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Results</head><p>In this paper we address the problem of obtaining improved approximation guarantees for the metric labeling problem. Kleinberg and Tardos in their work <ref type="bibr" target="#b18">[19]</ref> point out the difficulty of the general case as having to do with the absence of a "natural" IP formulation for the problem. They give an IP formulation for tree metrics and use Bartal's probabilistic tree approximations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to reduce the general metric to a tree metric.</p><p>In this work we give a natural IP formulation for the general problem. An advantage of our formulation is that it is applicable even to distance functions that are not metrics, for example the the quadratic distance function mentioned above. We substantiate the strength of this formulation by deriving both known results and new results using its linear relaxation. In particular, we show the following results on the integrality gap of our formulation.</p><p>O(log k log log k) for general metrics and a factor 2 for the uniform metric.</p><p>1 for the linear metric and distances on the line defined by convex functions (not necessarily metrics). O( p M) for the truncated quadratic distance function.</p><p>The last result improves upon a 2M approximation known earlier. The integrality gaps we show for our formulation either match or improve upon the best previous approximation ratios for most of the cases known to us (in addition to the above we can also show that if G is a tree the integrality gap is 1). Our formulation allows us to present these results in a unified fashion. In the process we also improve the 4 approximation of Gupta and Tardos <ref type="bibr" target="#b13">[14]</ref> for the important case of the truncated linear metric. We also show a reduction from the case with arbitrary assignment costs c(u; i) to the case where c(u; i) 2 f0; 1g for all u and i. The reduction preserves the graph G and the optimal solution, but increases the size of the label space from k labels to nk labels. Though this is undesirable in practice, it is useful from a theoretical point of view. In particular, this result implies that in order to find an O(1)-approximation for the general case, it suffices to focus attention on restricted instances with no assignment costs, but where each vertex is required to be assigned a label from its own set of labels. We believe our results and techniques are a positive step towards obtaining improved bounds for the general as well as interesting special cases of the metric labeling problem.</p><p>Independent of our work, Calinescu, Karloff, and Rabani <ref type="bibr" target="#b6">[7]</ref> considered approximation algorithms for the 0-extension problem. They consider a linear programming formulation (which they call the metric relaxation) where they associate a length function with every edge of the graph and require that: (i) the distance between terminals t i and t j , for 1 i; j k, is at least d(i; j), and (ii) the length function is a semi-metric. We note that their formulation does not apply to the general metric labeling problem. They obtain an O(log k)-approximation algorithm for the 0-extension problem using this formulation (and O(1)-approximation for planar graphs). Our linear programming formulation (when specialized to the 0-extension problem) induces a feasible solution for the metric relaxation formulation, by defining the length of an edge to be its transshipment cost (see Section 2). It is not hard to verify that this length function is a semi-metric. Calinescu, Karloff, and Rabani <ref type="bibr" target="#b6">[7]</ref> also show a gap of ( p log k) on the integrality ratio of their formulation. It appears that their lower bound proof does not carry over (in any straight forward way) to our formulation. We note that the metric relaxation formulation optimizes over the set of all semi-metrics, while our formulation optimizes only over a subset of the semi-metrics. Whether our formulation is strictly stronger than the metric relaxation of <ref type="bibr" target="#b6">[7]</ref> is an interesting open problem.</p><p>Outline: Section 2 describes our linear programming formulation for the general metric labeling problem. In Section 3, we analyze our formulation for uniform and linear metrics. Building on our rounding scheme for linear metric, we design and analyze a rounding procedure for the truncated linear metric in Section 4. In Section 5, we study the general metric labeling problem and show that the integrality gap of our formulation is bounded by O(log k log log k). We also describe here a transformation that essentially eliminates the role of the label cost assignment function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Linear Programming Formulation</head><p>We present a new linear integer programming formulation of the metric labeling problem. Let x(u; i) be a f0; 1g-variable indicating that vertex u is labeled i. Let x(u; i; v; j) be a f0; 1g-variable indicating that for edge (u; v) 2 E, vertex u is labeled i and vertex v is labeled j.</p><p>Constraints (1) simply express that each vertex has to receive some label. Constraints (2) force consistency in the edge variables: if x(u; i) = 1 and x(v; j) = 1 they force x(u; i; v; j) to be 1. Constraints (3) express the fact that (u; i; v; j) and (v; j; u; i) refer to the same edge; the redundancy helps in notation. We obtain a linear relaxation of the above program by allowing the variables x(u; i) and x(u; i; v; j) to take any non-negative value.</p><p>We note that equality in ( <ref type="formula">2</ref>) is important for the linear relaxation.</p><formula xml:id="formula_1">(I) min X u2V k X i=1 c(u; i) x(u; i) + X (u;v)2E k X i=1 k X j=1 w(u; v) d(i; j) x(u; i; v; j) subject to: k X i=1 x(u; i) = 1 8 v 2 V (1) k X j=1</formula><p>x(u; i; v; j) ? x(u; i) = 0 8 u 2 V , v 2 N(u), and i 2 1; : : : ; k <ref type="bibr" target="#b1">(2)</ref> x(u; i; v; j) ? x(v; j; u; i) = 0 8 u; v 2 V , and labels i; j 2 1; : : : ; k <ref type="bibr" target="#b2">(3)</ref> x(u; i) 2 f0; 1g 8 u 2 V and i 2 1; : : : ; k</p><p>x(u; i; v; j) 2 f0; 1g 8 (u; v) 2 E and i; j 2 1; : : : ; k (5)</p><p>With each edge (u; v) 2 E we associate a complete bipartite graph H(u; v) The vertices of H(u; v) are fu 1 ; : : : ; u k g and fv 1 ; : : : ; v k g, i.e., they represent all possible labelings of u and v. There is an edge (u i ; v j ) connecting the pair of vertices u i and v j , 1 i; j k. In the sequel, we refer to the edges of H(u; v) as links to distinguish them from the edges of G. Suppose that the value of the variables x(u; i) for all u and i has been determined. For an edge (u; v) 2 E, we can interpret the variables x(u; i; v; j) from a flow perspective. The contribution of edge (u; v) 2 E to the objective function of the linear program is the cost of the optimal transshipment of flow between fu 1 ; : : : ; u k g and fv 1 ; : : : ; v k g, where: (i) the supply of u i is x(u; i) and the demand of v j is x(v; j) for 0 i; j k, (ii) the cost of shipping a unit flow from u i to v j is d(i; j). (The choice of the "supply" side and the "demand" side is arbitrary.) For the rest of the paper the quantity d LP (u; v) refers to the LP distance between u and v and is defined to be P i;j d(i; j) x(u; i; v; j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Uniform Metrics and Linear Metrics</head><p>We now analyze the performance of our linear programming formulation on two natural special cases, namely, uniform metrics and linear metrics. Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> showed a 2-approximation for the uniform metric case. Their approach is based on rounding the solution of a linear program formulated specifically for uniform metrics. We will show that our linear programming formulation dominates the one in <ref type="bibr" target="#b18">[19]</ref>, and thus also has an integrality gap of at most 2. For the case of linear metrics, Boykov et al <ref type="bibr" target="#b2">[3]</ref>, and Ishikawa and Geiger <ref type="bibr" target="#b14">[15]</ref>, have obtained exact algorithms, by reducing the problem to a minimum fs; tg-cut computation. We show here that, on linear metrics, our linear programming formulation gives an exact algorithm as well. Our analysis for the linear metric case plays an important role in the algorithm for the truncated linear metric case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Uniform Metric Case</head><p>Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> formulated a linear program for the uniform metric and suggested the following iterative algorithm for rounding a solution to linear program (KT) below. Initially, no vertex is labeled. Each iteration consists of the following steps: (i) choose a label uniformly at random from 1; : : : ; k (say i), (ii) choose a real threshold uniformly at random from 0; 1], (iii) for all unlabeled vertices u 2 V , u is labeled i if x(u; i).</p><p>The algorithm terminates when all vertices are labeled. Kleinberg and Tardos <ref type="bibr" target="#b18">[19]</ref> showed that the expected cost of a labeling obtained by this algorithm is a 2-approximation.</p><formula xml:id="formula_3">(KT) min X v2V k X i=1 c(u; i) x(u; i) + 1 2 X (u;v)2E k X i=1 w(u; v) jx(u; i)?x(v; i)j subject to: k X i=1 x(u; i) = 1 8 u 2 V</formula><p>x(u; i) 0 8 u 2 V and i 2 1; : : : ; k</p><p>We show that applying the rounding algorithm to an optimal solution obtained from linear program (I), yields the same approximation factor. Let x be a solution to (I). Note that for both (I) and (KT) the variables x(u; i) completely determine the cost. We will show that cost of (KT) on x is smaller than that of (I). Both linear programs (I) and (KT) coincide regarding the labeling cost. Consider edge (u; v) 2 E. We show that the contribution of (u; v) to the objective function of (I) is at least as high as the contribution to the objective function of (KT). The last term is the contribution of (u; v) to the objective function of (I). We note that the example used in <ref type="bibr" target="#b18">[19]</ref> to show that the integrality gap of (KT) is at least 2 ? 1=k, can be used to show the same gap for (I) as well.</p><formula xml:id="formula_4">1 2 k X i=1 jx(u; i) ? x(v; i)j = 1 2 k X i=1 k X j=1 x(u; i; v; j) ? k X j=1 x(v; i; u; j) 1 2 k X i=1 k X j=1;j6 =i x(u; i; v; j) + k X j=1;j6 =i x(v; i; u; j) =<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Line Metric Case</head><p>We now turn to the case of a linear metric and show that the value of an integral optimal solution is equal to the value of a fractional optimal solution of the LP (I). Without loss of generality, we can assume that the labels of the metric are integers 1; 2; : : : ; k. If the label set contains non-consecutive integers, we can add all the "missing" intermediate integers to the label set and set the cost of assigning them to every vertex to be infinite. In fact the linear program and the rounding can be generalized to the case where the labels are arbitrary points on the real line without any difficulty.</p><p>The rounding procedure: Let x be an optimal fractional solution to the linear program. We round the fractional solution as follows. Let be a real threshold chosen uniformly at random from 0; 1]. For all i, 1 i k, let</p><formula xml:id="formula_5">(u; i) = i X j=1</formula><p>x(u; j):</p><p>Each vertex u 2 V is labeled by the unique label i that satisfies (u; i ? 1) &lt; (u; i). Clearly all vertices are labeled since (u; k) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis:</head><p>We analyze the expected cost of the assignment produced by the rounding procedure. For each vertex u 2 V , let L(u) be a random variable that assumes the different labels that u can be assigned by the rounding procedure. It can be readily verified that the probability that L(u) = i is equal to x(u; i). This means that the expected labeling cost of vertex v is equal to P k i=1 x(u; i) c(u; i) which is precisely the assignment cost of u in the linear program (with respect to solution x). We now fix our attention on the expected cost of the edges.</p><formula xml:id="formula_6">Lemma 3.1 Consider edge (u; v) 2 E. Then, E d((L(u); L(v))] = k X i=1 j (u; i) ? (v; i)j:</formula><p>Proof. For the sake of the analysis we define auxiliary binary random variable Z 1 ; : : : ; Z k?1 as follows. The variable Z i is 1 if minfL(u); L(v)g i and maxfL(u); L(v)g &gt; i, and is 0 otherwise. In other words Z i is 1 if i is in the interval defined by L(u) and L(v). It is easy to see that</p><formula xml:id="formula_7">d(L(u); L(v)) = k?1 X i=1 Z i : Therefore E d(L(u); L(v))] = P k?1 i=1 E Z i ]. We claim that E Z i ] = Pr Z i = 1] = j (u; i) ? (v; i)j:</formula><p>The lemma easily follows from this claim. To prove the claim, assume w.l.o.g. that (u; i) (v; i). If &lt; (v; i) it is clear that L(u); L(v) i and if &gt; (u; i) then L(u); L(v) &gt; i: in both cases Z i = 0.</p><p>If 2 ( (v; i); (u; i)] then L(u) i and L(v) &gt; i which implies that Z i = 1. Therefore Pr Z i = 1] is exactly j (u; i) ? (v; i)j.</p><p>2</p><p>We now estimate the contribution of an edge (u; v) 2 E to the objective function of the linear program.</p><p>As indicated in Section 2, the contribution is equal to the cost of the optimal transshipment cost of flow in the complete bipartite graph H(u; v) between fu 1 ; : : : ; u k g and fv 1 ; : : : ; v k g, where the supply of u i is x(u; i) and the demand of v j is x(v; j) for 1 i; j k. Recall that d LP (u; v) = P i;j d(i; j) x(u; i; v; j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.2 For the line metric</head><formula xml:id="formula_8">d LP (u; v) k X i=1 j (u; i) ? (v; i)j:</formula><p>Proof. The crucial observation in the case of a linear metric is that flow can be "uncrossed". Let i i 0 and j j 0 . Suppose that " amount of flow is sent from u i to v j 0 and from u i 0 to v j . Then, uncrossing the flow, i.e., sending " amount of flow from u i to v j and from u i 0 to v j 0 will not increase the cost of the transshipment.</p><p>This means that the amount of flow sent from fu 1 ; : : : ; u i g to fv 1 ; : : : ; v i g, for all i, 1 i k, is precisely minf (u; i); (v; i)g. Therefore, j (v; i) ? (u; i)j amount of flow is sent "outside" the label set 1; 2; : : : ; i and can be charged one unit of cost (with respect to i). Applying this argument to all values of i, 1 i k, we get that the cost of the optimal transshipment of flow is precisely P k i=1 j (u; i) ? (v; i)j. The lemma follows. 2 Hence, together with Lemma 3.1, we get that the expected cost of edge (u; v) 2 E after the rounding is no more than its contribution to the objective function of the linear program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1</head><p>The integrality gap of the LP (I) for the line metric is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distance functions on the Line defined by Convex functions</head><p>We now consider distance functions on the labels 1; : : : ; k on the integer line defined by strictly convex functions, that is d(i; j) = f(ji ? jj) where f is convex and increasing. Notice that such distance functions do not satisfy the metric property, in fact d is a metric if and only if f is concave and increasing. Our motivation for studying these metrics comes from the quadratic function d(i; j) = ji ? jj 2 which is of particular interest in the image restoration application <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref> described earlier. We can show the following.</p><p>Theorem 2 For any distance function on the line defined by a convex function, the integrality gap of our LP formulation is 1.</p><p>The proof is based on the following facts, the proofs of which we omit from this version. Consider a feasible solution x to the LP. For any edge (u; v), if f is convex, the optimal cost transshipment flow in H(u; v) is noncrossing. Further, for the rounding that we described, if the flow is non-crossing, Pr L(u) = i and L(v) = j] = x(u; i; v; j). The theorem follows from this last fact trivially. Ishikawa and Geiger <ref type="bibr" target="#b15">[16]</ref> show that the flow graph construction for the line metric can be extended for convex functions to obtain an optimal solution. The advantage of our approach is that the solution is obtained from a general formulation. This allows us to extend the ideas to obtain the first non-trivial approximation for the truncated quadratic distance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improved Approximation for the Truncated Line Metric</head><p>In this section we use our LP formulation to give a (2 + p 2)-approximation algorithm for the truncated line metric case. This improves the 4-approximation provided in <ref type="bibr" target="#b13">[14]</ref>. We prefer to view the metric graph as a line with the truncation to M being implicit. This allows us to use, with appropriate modifications, ideas from Section 3.2. We round the fractional solution x once again using only the variables x(u; i). Let M 0 M be an integer parameter that we will fix later. We repeat the following iteration until all vertices are assigned a label.</p><p>Pick an integer `uniformly at random in ?M 0 + 2; k]. Let I `be the interval `; `+ M 0 ? 1].</p><p>Pick a real threshold uniformly at random from 0; 1].</p><p>Let u be an unassigned vertex. If there is a label i 2 I `such that i?1 X j=`x (u; j) &lt; i X j=`x (u; j); we assign i to u. Otherwise u is unassigned in this iteration.</p><p>The above algorithm generalizes the rounding algorithms for the uniform and line metrics in a natural way. Once the index `is picked, the rounding is similar to that of the line metric in the interval I `. The difference is that a vertex might not get a label in an iteration. If two vertices u and v get separated in an iteration, that is only one of them gets assigned, our analysis will assume that their distance is M. This is similar to the analysis for the uniform metric case. Our improvement comes from a careful analysis of the algorithm which treats links differently based on whether their distance is linear or truncated. The analysis guides the choice of M 0 to obtain the best guarantee.</p><p>Let L(u) and L(v) be random variables that indicate the labels that get assigned to u and v by the algorithm. Lemma 4.1 In any given iteration, the probability of an unassigned vertex u getting a label i in that iteration is exactly x(u; i) M 0 =(k + M 0 ? 1). The probability of u getting assigned in the iteration is M 0 =(k + M 0 ? 1).</p><p>Therefore Pr L(u) = i] = x(u; i). Proof. If `is picked in the first step of an iteration, the probability of assigning i to u is exactly x(u; i), if i 2 I `, and zero otherwise. The number of intervals that contain i is M 0 , hence the probability of u getting i in an iteration is simply x(u; i) M 0 =(k + M 0 ? 1). 2</p><p>It also follows from Lemma 4.1 that with high probability all vertices are assigned in O(k log n) iterations. The following lemma bounds the expected distance between L(u) and L(v) as a function of M 0 . Recall d LP (u; v) = P i;j d(i; j) x(u; i; v; j). Lemma 4. <ref type="bibr" target="#b1">2</ref> The expected distance between L(u) and L(v) satisfies the following inequality.</p><formula xml:id="formula_9">E d(L(u); L(v))] (2 + maxf 2M M 0 ; M 0 M g)d LP (u; v):</formula><p>Theorem 3 There is a (2 + p 2)-approximation algorithm for the metric labeling problem when the metric is truncated linear.</p><p>Proof. We note that the algorithm and the analysis easily generalizes to the case when M 0 is a real number. We choose M 0 = p 2M and the theorem follows from Lemmas 4.1 and 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>For the rest of the section we will restrict our attention to one particular edge (u; v) 2 E(G) and analyse the affect of the rounding on the expected distance with the goal of proving Lemma 4.2. To analyze the process we consider an iteration before which neither u and v are assigned. If in the current iteration only one of u or v is assigned a label, that is they are separated, we will pay a distance of M. If both of them are assigned then we pay a certain distance based on their labels in the interval. The main quantity of interest is the expected distance between the labels of u and v in a single iteration conditioned under the event that both of them are assigned in this iteration. Recall that a link refers to the edges in the complete bipartite graph H(u; v).</p><p>Given an interval I `= `; `+ M 0 ? 1] we partition the interesting links for I `into three categories, internal, left crossing, and right crossing. The internal links denoted by INT(I `) are all the links (u; i; v; j) with i; j 2 I `, the left crossing links denoted by LCROSS(I `) are those with minfi; jg &lt; `and and maxfi; jg 2 I `, and right crossing link denoted by RCROSS(I `) are those with minfi; jg 2 I `and and maxfi; jg &gt; `+ M 0 ? 1. It is clear that no edge is both left and right crossing. Let CROSS(I `) denote LCROSS(I `) RCROSS(I `). See Figure <ref type="figure" target="#fig_2">1</ref> for an example. It is easy to see that e is not relevant to I `if i; j = 2 I `.</p><p>We set up some notation that we use for the rest of this section. For a link e = (u; i; v; j) let d e = d(i; j) and jej = ji ? jj be the truncated linear and linear distance respectively between i and j. The quantity x e denotes x(u; i; v; j). Consider a link e that crosses the interval I `and let i be the label of e that is internal to I `. We denote by e `the quantity (`+ M 0 ? 1 ? i), the linear distance from i to the right end of interval. The quantity x(u; I `) refers to P i2I `x(u; i), the flow of u assigned by the LP to labels in I `. Lemma 4. <ref type="bibr" target="#b2">3</ref> The probability of u and v being separated given that `was chosen in the first step of the iteration is at most P e2C ROSS(I `) x e .</p><p>Proof. The probability of separation is exactly jx(u; I `) ?x(v; I `)j which can easily be seen to be upper bounded by P e2C ROSS(I `) x e .</p><p>2 Lemma 4. <ref type="bibr" target="#b3">4</ref> For two vertices u and v unlabeled before an iteration let p `be the expected distance between them conditioned on the event that `was chosen in the first step of the iteration and both were assigned a label in I `. We provide some intuition before giving the formal proof. Once `is fixed, the rounding is exactly the same as that for the line metric, restricted to the interval I `. From Lemma 3.1 we know the exact expected cost of the rounding. However we do not have an equivalent of Lemma 3.2 to bound the LP cost because I `is only a subinterval of the full line and also because of the truncation. The main difficulty is with links that belong to CROSS(I `). By charging each of the crossing links e, an amount equal e `(instead of d e that LP paid), we are able to pay for the expected cost of rounding. In other words we charge the interesting links of I `to pay for the optimal linear metric transshipment flow induced by the fractional values x(u; i) and x(v; i), i 2 I `when restricted to I `.</p><p>Proof. Fix an `and w.l.o.g. assume that x(u; I `) x(v; I `). With probability q = x(v; I `) both u and v get labels from I `. We analyze the expected distance conditioned on this event. Once the interval I `is fixed the rounding is very similar to that of the linear metric case. For 0 i &lt; M 0 let (u; i) = P j+i j=`x (u; j). The quantity (u; i) sums the amount of flow of u in the first i + 1 labels of the interval I `. In the following analysis we assume that distances within I `are linear and ignore truncation. This can only hurt us. Following the reasoning in Lemma 3.1 the expected distance between u and v is equal to P M 0 ?1 i=0 j minfq; (u; i)g ? (v; i)j which we upper bound by P M 0 ?1 i=0 j (u; i) ? (v; i)j. We claim that M 0 ?1 X i=0 j (u; i) ? (v; i)j X e2C ROSS(I `) e `xe + X e2IN T(I `) jejx e : To prove this claim we consider each link e 2 CROSS(I `) INT(I `) and sum its contribution to the terms q i = j (u; i) ? (v; i)j, 0 i &lt; M 0 . Let e = (u; a; v; b) 2 INT(I `). It is clear that e contributes exactly x e to q i if a i and b &gt; i or if a &gt; i and b i. Otherwise its contribution is 0. Therefore the overall contribution of e to P q i is x e ja ? bj = x e jej.</p><p>Now suppose e = (u; a; v; b) 2 LCROSS(I `). Assume w.l.o.g. that a `and b &lt; `, the other case is similar. Link e will contribute x e to (u; i) for a ? ` i &lt; M 0 and contributes 0 to (v; i) for 0 i &lt; M 0 since b is outside the interval I `. Therefore the contribution of e to q i is x e for a ? ` i &lt; M 0 and 0 otherwise, the overall contribution of e to P q i is x e j`+ M 0 ? 1 ? aj = e `xe . A similar argument holds for the case when e 2 RCROSS(I `). We omit the details.</p><p>2</p><p>Proof of Lemma 4.2. For a given iteration before which neither u nor v has a label, let Pr u ^v], Pr u v], and Pr u _ v] denote the probabilities that u and v are both assigned, exactly one of them is assigned, and at least one of them is assigned respectively. We upper bound the quantity E d(L(u); L(v))] as follows. If u and v has only an O(1) integrality gap on HST metrics. Since any arbitrary k-point metric can be probabilistically approximated by an HST metric with an O(log k log log k) distortion <ref type="bibr" target="#b1">[2]</ref>, their result follows. In contrast, our approach is based on directly using our LP formulation on the given general metric. As a first step, we use the LP solution to identify a deterministic HST metric approximation of the given metric such that the cost of our fractional solution on this HST metric is at most O(log k log log k) times the LP cost on the original metric. This first step is done by using the following proposition from Charikar et al <ref type="bibr" target="#b7">[8]</ref>.</p><p>Proposition 1 Let d be an arbitrary k-point metric and let be a non-negative function defined over all pairs of points in the metric. Then d can be deterministically approximated by an HST metric d T such that X i;j (i; j) d T (i; j) O(log k log log k) X i;j (i; j) d(i; j):</p><p>Given an optimal solution to our LP formulation, we apply Proposition 1 with the weight function (i; j) = P (u;v)2E w(u; v) x(u; i; v; j) for 1 i; j k. Thus (i; j) is the fractional weight of edges between i and j. Let d T denote the resulting HST metric. Since we are changing only the metric on the labels and not the assignments provided by the LP, the fractional solution is a feasible solution for this new metric and has cost at most O(log k log log k) C where C is the optimal LP cost for the original metric. Thus if we can now round our fractional solution on d T by introducing only a constant factor increase in the solution cost, we will obtain an O(log k log log k)-approximation algorithm. We prove this by showing that on any tree metric, our LP formulation is at least as strong as the Kleinberg-Tardos LP formulation (for tree metrics). We omit the proof and state the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5</head><p>The integrality gap of LP (I) on a k-point metric is O(log k log log k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reduction to Zero-Infinity Assignment Costs</head><p>We now describe a transformation for the general problem that essentially allows us to eliminate the label assignment cost function. This transformation reduces an instance with arbitrary label assignment cost function c to one where each label assignment cost is either 0 or 1. We refer to an instance of this latter type as zero-infinity instance. Our transformation exactly preserves the cost of any feasible solution but in the process increases the number of labels by a factor of n. This provides some evidence that the label cost assignment function does not play a strong role in determining the approximability threshold of the metric labeling problem. In particular, existence of a constant factor approximation algorithm for zero-infinity instance would imply a constant factor approximation algorithm for general instances as well.</p><p>From an instance I = hc; d; w; L; G(V; E)i of the general problem, we create an instance I 0 = hc 0 ; d 0 ; w; L 0 ; G(V; E)i of the zero-infinity variant as follows. We define a new label set L 0 = fi u j i 2 L and u 2 V g i.e. we make a copy of L for each vertex in G. The new label cost assignment function is given by c 0 (u; i v ) = 0 if v = u and 1 otherwise. Thus each vertex has its own copy of the original label set and any finite cost solution to I 0 would assign each vertex a label from its own private copy.</p><p>Let W u = P v2N (u) w(u; v) for any vertex u 2 V . The new distance metric on L 0 is defined in terms of the original distance metric as well as the original label cost assignment function. For i 6 = j or u 6 = v, d 0 (i u ; j v ) = d(i; j) + c(u; i) W u + c(v; j) W v ;</p><p>and d 0 (i u ; i u ) = 0. It can be verified that d 0 is indeed a metric, and that any solution to instance I can be mapped to a solution to instance I 0 , and vice versa, in a cost-preserving manner. We omit details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 6</head><p>If there exists a f(n; k)-approximation algorithm for the zero-infinity instances, there exists a f(n; nk)approximation algorithm for general instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 + p 2 ' 3 :</head><label>223</label><figDesc>414 for the truncated linear metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>u; i; v; j) since x(u; i; v; j) = x(v; i; u; j) ; j) x(u; i; v; j) since d(i; i) = 0 and d(i; j) = 1; i 6 = j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of interesting links for I `.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>I `) e `xe + X e2IN T(I `) jejx e :</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Olga Veksler and Mihalis Yannakakis for useful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>are separated in some iteration we upper bound their resulting distance by M. Using this simplification we have the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E d(L(u); L(v))]</head><p>Pr u v] M + Pr u ^v] E d(L(u); L(v))ju ^v] Pr u _ v] :</p><p>We upper bound the right hand side of the above equation as follows.</p><p>We lower bound Pr u _ v] by Pr u] which by Lemma 4.1 is equal to M 0 =(k + M 0 ? 1).</p><p>We upper bound Pr u v] by 1 k+M 0 ?1 P `Pe2C ROSS(I `) x e using Lemma 4.3. By the definition of p `in Lemma 4.4 we get the following.</p><p>Putting all these together and using Lemma 4.4 to bound p `we get  </p><p>Proof. We evaluate d e separately for three different types of edges based on their lengths. Recall that M 0 M and hence d e M M 0 for all links e. Let e correspond to the link (u; i; v; j) in H(u; v) and w.l.o.g. assume that i j, the other case is similar, recall that jej = ji ? jj. jej M 0 : In this case it is clear that e is not an internal edge for any interval. Also d e = M. Therefore  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Truncated Quadratic Distance on the Line</head><p>Consider the label space 1; 2; : : : ; k on the line where the distance function d(i; j) = minfM; ji ? jj 2 g. This is the truncated version of the quadratic distance. We note that the quadratic distance is not a metric. However as mentioned earlier this distance function arises in image processing applications. In Subsection 3.3 we showed that our LP formulation gives an optimal solution for the quadratic distance on the line. For the truncated version of this distance we can use the algorithm from Section 4. By choosing M 0 = p M we can show the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4 The integrality gap of our LP on the truncated quadratic distance is O( p M).</head><p>The best approximation ratio for this problem that follows from prior work is 2M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">General Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Integrality Gap on General Metrics</head><p>We now show that the integrality gap of our LP formulation is O(log k log log k) on general metrics. This gives an alternative way of obtaining the result of Kleinberg-Tardos <ref type="bibr" target="#b18">[19]</ref>. The latter algorithm uses the approach of first approximating the given metric probabilistically by a hierarchically well-separated tree (HST) metric <ref type="bibr" target="#b0">[1]</ref> and then using an LP formulation to solve the problem on tree metrics. The Kleinberg-Tardos LP formulation</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic approximation of metric spaces and its algorithmic applications</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 37th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On approximating arbitrary metrics by tree metrics</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM Symposium on Theory of Computing</title>
		<meeting>the 30th Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Markov random fields with efficient approximations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="648" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision</title>
		<meeting>the 7th IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new algorithm for energy minimization with disconitnuities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved approximation algorithms for multiway cut</title>
		<author>
			<persName><forename type="first">G</forename><surname>Calinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="48" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Approximation algorithms for the 0-extension problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Calinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Karloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rounding via trees: deterministic approximation algorithms for group steiner trees and k-median</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM Symposium on Theory of Computing</title>
		<meeting>the 30th ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modeling and Applications of Stochastic Processes, chapter Markov random fields for image modeling and analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The complexity of multiterminal cuts</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dalhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SICOMP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="864" to="894" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rounding algorithms for a geometric embedding of multiway cut</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="668" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random field models in image analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exact maximum a posteriori estimation for binary images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seheult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constant factor approximation algorithms for a class of classification problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual ACM Symposium on the Theory of Computing</title>
		<meeting>the 32nd Annual ACM Symposium on the Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Segmentation by grouping junctions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mapping Image Restoration to a Graph Problem</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing</title>
		<meeting>the 1999 IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="20" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum 0-extension of graph metrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karzanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europ. J. Combinat</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="71" to="101" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A combinatorial algorithm for the minimum (2; r)-metric problem and some generalizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karzanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="569" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 40th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Markov Random Field Modeling in Computer Vision</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A maximum-flow formulation of the N-camera stereo correspondence problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="492" to="499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
