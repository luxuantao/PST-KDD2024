<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Adversarially Robust Representation Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangya@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junru</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
							<email>wangchunping02@xinye.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">FinVolution Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Adversarially Robust Representation Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they can be generalized to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques for endto-end graph representation learning methods require prespecified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (i.e., node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs, a common mathematical abstraction for modeling pairwise interactions between objects, are widely applied in numerous domains, including bioinformatics, social networks, and finance. Owing to their prevalence, deep learning on graphs, such as graph neural networks (GNNs) <ref type="bibr" target="#b15">(Kipf et al. 2017;</ref><ref type="bibr" target="#b10">Hamilton et al. 2017)</ref>, have recently undergone rapid development, and made major progress in various analytical tasks, including node classification <ref type="bibr" target="#b15">(Kipf et al. 2017;</ref><ref type="bibr" target="#b10">Hamilton et al. 2017)</ref>, link prediction <ref type="bibr" target="#b15">(Kipf et al. 2016)</ref>, and graph Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y i b i o A h X q o g h U i r 8 3 M p 9 J n o v U n Q = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C G 5 c V + p L O U D J p p g 3 N Z I Y k I 5 S h v + H G h S J u / R l 3 / o 2 Z d h b a e i B w O O d e 7 s k J E s G 1 c Z x v V N r Y 3 N r e K e 9 W 9 v Y P D o + q x y d d H a e K s g 6 N R a z 6 A d F M c M k 6 h h v B + o l i J A o E 6 w X T u 9 z v P T G l e S z b Z p Y w P y J j y U N O i b G S 5 0 X E T C g R 2 e M c D 6 s 1 p + 4 s g N e J W 5 A a F G g N q 1 / e K K Z p x K S h g m g 9 c J 3 E + B l R h l P B 5 h U v 1 S w h d E r G b G C p J B H T f r b I P M c X V h n h M F b 2 S Y M X 6 u + N j E R a z 6 L A T u Y Z 9 a q X i / 9 5 g 9 S E t 3 7 G Z Z I a J u n y U Y k Q j h E R q Q j q E c h U T 5 y e S f F B 4 a p Q f 7 Q p r i G k 7 U 3 x M J C p U a h 4 H p z E 5 U s 1 4 m / u d 1 Y t 0 / 9 x P K o 1 g T j q e L + j G D W s A s H N i j k m D N x o Y g L K m 5 F e I h k g h r E 2 H R h O D O v j x P m p W y e 1 K u X J + W q v U 8 j g L Y B w f g G L j g D F T B F a i B B s D g A T y B F / B q P V r P 1 p v 1 P m 1 d s P K Z P f A H 1 s c 3 U Z C c 6 g = = &lt; / l a t e x i t &gt;</p><formula xml:id="formula_0">J g K b G K c F 4 B H X D F q x M w S Q h W 3 W T G d E E W o s T V V b A n u 6 p f X S b d R d 6 / q j Y f r W r N d 1 F G G M z i</formula><p>Figure <ref type="figure">1</ref>: Overview of a graph pre-training pipeline under adversarial attacks. If the graph encoder is vulnerable to the attacks, the adversarial risk would propagate to every downstream task via the perturbed graph representation.</p><p>classification <ref type="bibr" target="#b27">(Xu et al. 2019b</ref>). However, most deep learning models on graphs are trained with task-specific labels in an end-to-end manner for a particular task. This motivates some recent efforts to pre-train an expressive graph encoder on unlabeled data and further feed the learned representations to (supervised/unsupervised) off-the-shelf machine learning models for relevant downstream tasks <ref type="bibr" target="#b12">(Hu et al. 2019</ref><ref type="bibr" target="#b13">(Hu et al. , 2020;;</ref><ref type="bibr" target="#b17">Qiu et al. 2020)</ref>. The pre-training models on graphs enable the learned representations to be directly applicable to different applications with a simple and inexpensive machine learning model attached after the encoded representations. Despite the promising results achieved by deep learning models on graphs, recent studies have shown that these models are vulnerable to adversarial attacks <ref type="bibr" target="#b5">(Dai et al. 2018;</ref><ref type="bibr" target="#b32">Zügner et al. 2019a;</ref><ref type="bibr" target="#b1">Bojchevski et al. 2019a;</ref><ref type="bibr" target="#b23">Xu et al. 2020b;</ref><ref type="bibr" target="#b30">Zheng et al. 2021)</ref>. In other words, even imperceptible perturbations on graph topology and node attributes can significantly affect the learned graph representation, thereby degrading the performance of downstream tasks <ref type="bibr" target="#b4">(Chen et al. 2020;</ref><ref type="bibr" target="#b25">Xu et al. 2020c</ref>). This so-called adversarial vulnerability has given rise to tremendous concerns regarding the utilization of deep learning models on graphs, especially in security-critical applications such as drug discovery <ref type="bibr" target="#b8">(Gilmer et al. 2017</ref>) and financial surveillance <ref type="bibr" target="#b29">(Yang et al. 2019</ref><ref type="bibr" target="#b28">(Yang et al. , 2021))</ref>. However, the adversarial vulnerability of pre-training models on graphs is far overlooked. In this work, we show that graph pre-training models also suffer from the adversarial vulnerability problem. Actually, owing to the complicated and deep structure, the graph encoder is more vulner-able to adversarial attacks than the simple machine learning models used for downstream tasks in a graph pre-training pipeline <ref type="bibr">(Tanay et al. 2016)</ref>. As Figure <ref type="figure">1</ref> shows, once the graph encoder is vulnerable to adversarial attacks, the adversarial risk would propagate to every task via the perturbed representations.</p><p>Most efforts targeted at this adversarial vulnerability problem focus on supervised, end-to-end models designed for a particular application scenario <ref type="bibr" target="#b33">(Zügner et al. 2019b;</ref><ref type="bibr" target="#b2">Bojchevski et al. 2019b;</ref><ref type="bibr" target="#b20">Wang et al. 2019;</ref><ref type="bibr" target="#b14">Jin et al. 2020;</ref><ref type="bibr" target="#b24">Wang et al. 2021;</ref><ref type="bibr" target="#b24">Xu et al. 2021)</ref>. However, the dependency on the supervised information largely limits the scope of their application and usefulness. For example, these models do not perform well on downstream tasks in which training labels are missing, e.g., community detection in social networks. In addition, training multiple models for different downstream tasks is both costly and insecure <ref type="bibr" target="#b7">(Feurer et al. 2015</ref>). In contrast, robust unsupervised pre-training models can easily handle the above issues because adversarial attacks are identified and blocked before propagating to downstream tasks. Moreover, these models are applicable to a more diverse group of applications, including node classification, link prediction, and community detection. Unfortunately, however, these robust pre-training models under the unsupervised setting still remain largely unexplored.</p><p>There are many interesting yet challenging questions in this new field of research. Conventionally, the robustness of a model is defined based on the label space <ref type="bibr" target="#b22">(Xu et al. 2020a;</ref><ref type="bibr" target="#b33">Zügner et al. 2019b;</ref><ref type="bibr" target="#b2">Bojchevski et al. 2019b)</ref>, which is not the case in our setting. Thus the first difficulty we meet is to quantify the robustness of an unsupervised model (without the knowledge of the true or predicted labels).</p><p>To overcome the above challenge, in this paper, we first introduce the graph representation vulnerability (GRV), an information theoretic-based measure used to quantify the robustness of a graph encoder. We then formulate an optimization problem to study the trade-off between the expressive power of a graph encoder and its robustness to adversarial attacks, measured in GRV. However, how to efficiently compute or approximate the objective of the optimization problem becomes the next issue. First, it remains a big problem on how to describe the ability of the attack strategies or the boundary of perturbations, because adversarial attacks on graphs perturb both the discrete graph topology and the continuous node attributes. Second, the rigorous definition of the objective is intractable.</p><p>To handle the above issues, we first quantify the ability of adversarial attacks using Wasserstein distance between probability distributions, and provide a computationally efficient approximation for it. We then adopt a variant of projected gradient descent method to solve the proposed optimization problem efficiently. A sub-optimal solution for the problem gives us a well-qualified and robust graph encoder.</p><p>Last but not least, we explore several interesting theoretical connections between the proposed measure of robustness (GRV) and the classifier robustness based on the label space. To show the practical usefulness of our model, we apply the learned representations to three different downstream tasks. Experimental results reveal that under adversarial attacks, our model beats the best baseline by an average of +1.8%, +1.8%, and +45.8% on node classification, link prediction, and community detection task, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Notations</head><p>In most cases, we use upper-case letters (e.g., and ) to denote random variables and calligraphic letters (e.g., X and Y) to denote their support, while the corresponding lower-case letters (e.g., and ) indicate the realizations of these variables. We denote the probability distributions of the random variables using subscripts (e.g., and ) and the corresponding empirical distributions with hat accents (e.g., ˆ and ˆ ). We use bold upper-case letters to represent matrices (e.g., A). When indexing the matrices, A denotes the element at the -th row and the -th column, while A represents the vector at the -th row. Let (X, ) denote the metric space, where : X × X → R is a distance function on X. The set of all probability measures on X is M (X).</p><p>We assume a generic unsupervised graph representation learning setup. In brief, we are provided with an undirected and unweighted graph G = (V, E) with the node set V = { 1 , 2 , ..., |V| } and edge set E ⊆ V × V = { 1 , 2 , ..., |E | }. We are also provided with the adjacency matrix A ∈ {0, 1} |V|× |V| of the graph G, a symmetric matrix with elements A = 1 if ( , ) ∈ E or = , and A = 0 otherwise. We augment G with the node attribute matrix X ∈ R |V|× if nodes have attributes. Accordingly, we define our input as = ( , ) ∈ S; thus, we can conceive of as the attribute matrix and as the adjacency matrix of G under a transductive learning setting, while and are the adjacency matrix and attribute matrix respectively of a node's subgraph under an inductive learning setting. We define an encoder : S → Z, which maps an input = ( , ) ∈ S to a representation ( , ) ∈ Z, and a simple machine learning model : Z → Y that maps a representation ∈ Z to a label ( ) ∈ Y. We go on to define = • as their composition, such that ( • ) ( , ) = ( ( , )). The mutual information between two random variables and is denoted by I( ; ). Specifically, it is defined as the Kullback-Leibler divergence between the joint distribution ( , ) and the product of the marginal distributions ( ) ( ).</p><p>Admissible perturbations on graphs. The Wasserstein distance can be conceptualized as an optimal transport problem: we wish to transport the mass with probability distribution into another distribution at the minimum cost. Formally, the -th Wasserstein distance between and is</p><formula xml:id="formula_1">= ( , ) = inf ∈Π( , ) ∫ S 2 ( , ) ( , )<label>1/</label></formula><p>, where Π( , ) denotes the collection of all measures on S × S with marginal and , respectively. The choice of ∞-Wasserstein distance (i.e., = ∞) is conventional in learning graph representations <ref type="bibr" target="#b3">(Champion et al. 2008)</ref>.</p><p>Based on ∞-Wasserstein distance, we can quantify the ability of the adversarial attacks. An attack strategy is modeled as a probability distribution close to that of = ( , ), and all possible attack strategies stay in a ball around the genuine distribution , with a pre-defined budget &gt; 0:</p><formula xml:id="formula_2">B ∞ ( , ) = { ∈ M ( ) : ∞ ( , ) ≤ }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graphs Representations Robust to Adversarial Attacks</head><p>In a widely adopted two-phase graph learning pipeline, the first step is to pre-train a graph encoder (without the knowledge of any labels), which maps the joint input space S (i.e., the graph topology A and node attributes X) into some, usually lower-dimensional, representation space Z. Then the encoded representation is used to solve some target tasks.</p><p>In this section, we explain how to obtain a well-qualified graph representation robust to adversarial attacks. We first propose a measure to quantify the robustness without label information in §3.1. In §3.2, we formulate an optimization problem to explore the trade-off between the expressive power and the robustness of the graph encoder. We then describe every component in the proposed optimization problem, and explain how we obtain a sub-optimal solution efficiently in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantifying the Robustness of Graph Representations</head><p>In this section, we propose the graph representation vulnerability (GRV) to quantify the robustness of an encoded graph representation. Intuitively, the learned graph representation is robust if its quality does not deteriorate too much under adversarial attacks. Now we introduce in detail how to measure the quality of representations using MI, and how to describe the difference of representation quality before and after adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The use of mutual information.</head><p>A fundamental challenge to achieving a qualified graph representation is the need to find a suitable objective that guides the learning process of the graph encoder. In the case of unsupervised graph representation learning, the commonly used objectives are random walk-based <ref type="bibr">(Perozzi et al. 2014;</ref><ref type="bibr" target="#b9">Grover et al. 2016)</ref> or reconstruction-based <ref type="bibr" target="#b15">(Kipf et al. 2016)</ref>. These objectives impose an inductive bias that neighboring nodes have similar representations. However, the inductive bias is easy to break under adversarial attacks <ref type="bibr" target="#b14">(Jin et al. 2020;</ref><ref type="bibr" target="#b7">Entezari et al. 2020)</ref>, because the connections among local neighborhoods are prone to be broken under adversarial attacks. As an alternative solution, we turn to maximize the MI between the input attributed graph and the representation output by the encoder, i.e., I( ; ( )), from a more global view. In our case, maximizing the I( ; ( )) encourages the representations to be maximally informative about the input graph and to avoid the above-mentioned inductive bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph representation vulnerability.</head><p>In addition to the measure of the quality of a graph representation, we also need to describe the robustness of a representation. Intuitively, an encoder is called robust if the value of the objective stays relatively stable after tiny perturbations on the input. Therefore, the encoder is robust if the MI before and after attack are close to each other. Thus, we propose the graph repre-sentation vulnerability (GRV) to quantify this difference:</p><formula xml:id="formula_3">GRV ( ) = I( ; ( )) − inf ∈B ∞ ( , ) I( ; ( )),<label>(1)</label></formula><p>where = ( , ) is the random variable following the benign data distribution, and = ( , ) follows the adversarial distribution. The first term I( ; ( )) in ( <ref type="formula" target="#formula_1">1</ref>) is the MI between the benign graph data and the encoded representation, while the term I( , ( )) uses the graph data after attack. The attack strategy ★ that results in the minimum MI is called the worst-case attack, and is defined as</p><formula xml:id="formula_4">★ = argmin ∈B ∞ ( , ) I( ; ( )).</formula><p>Hence by definition, the graph representation vulnerability (GRV) describes the difference of the encoder's behavior using benign data and under the worst-case adversarial attack. A lower value of GRV ( ) implies a more robust encoder to adversarial attacks. Formally, an encoder is called ( , )robust if GRV ( ) ≤ .</p><p>An analogy to the graph representation vulnerability (GRV) has been studied in the image domain <ref type="bibr" target="#b31">(Zhu et al. 2020)</ref>. However, the extension of <ref type="bibr" target="#b31">(Zhu et al. 2020)</ref> to the graph domain requires nontrivial effort. An image is considered to be a single continuous space while a graph is a joint space S = (A, X), consisting of a discrete graph-structure space A and a continuous feature space X. Moreover, the perturbation on the joint space (A, X) is difficult to track because a minor change in the graph topology or node attributes will propagate to other parts of the graph via edges. This is different in the image domain, where the distributions of all the pixels are assumed to be i.i.d.. Therefore, the discrete nature of graph topology and joint space (A, X) make the worst-case adversarial attack extremely difficult to estimate. Thus, the optimization method we apply is substantially different from that in <ref type="bibr" target="#b31">(Zhu et al. 2020</ref>); see §3.2 and §3.3. Furthermore, more complicated analysis is needed to verify our approach in theory; see §4 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Problem</head><p>The trade-off between model robustness and the expressive power of encoder has been well-studied <ref type="bibr">(Tsipras et al. 2019;</ref><ref type="bibr">Zhang et al. 2019</ref>). In our case, this trade-off can be readily explored by the following optimization problem</p><formula xml:id="formula_5">maximize ℓ 1 (Θ) = I( ; ( )) − GRV ( ),<label>(2)</label></formula><p>where the optimization variable is the learnable parameters Θ of the encoder , and &gt; 0 is a pre-defined parameter. However, in practice, the "most robust" encoder is usually not the desired one (as it sacrifices too much in the encoder's expressive power). An intuitive example for the "most robust" encoder is the constant map, which always outputs the same representation whatever the input is. Hence, a "robust enough" encoder would be sufficient, or even better. To this end, we add a soft-margin to GRV, and obtain the following optimization problem</p><formula xml:id="formula_6">maximize ℓ 2 (Θ) = I( ; ( )) − max {GRV ( ), }. (3)</formula><p>The second term is positive if GRV &gt; and constant otherwise. As a result, when the encoder is sufficiently robust, the second term in ℓ 2 does no contribution, and thus Problem (3) turns to the standard MI maximization using benign data. Furthermore, when = 1, Problem (3) can be divided into two simple sub-problems, depending on the value of GRV ( ):</p><formula xml:id="formula_7">       max Θ inf ∈B ∞ ( , ) I( ; ( )), if GRV &gt; max Θ I( ; ( )), otherwise.<label>(4)</label></formula><p>In this case ( = 1), when GRV ( ) &gt; , the problem maximizes the MI under the worst-case adversarial attack. In other words, the robust encoder tries to maintain the mutual dependence between the graph data and the encoded representation, under all kinds of adversarial attacks. When the encoder is sufficiently robust (i.e., GRV ( ) ≤ ), the problem turns to maximize the encoder's expressive power. GNN as the parameterized encoder. GNN has been extensively used as an expressive function for parameterizing the graph encoder <ref type="bibr" target="#b15">(Kipf et al. 2016</ref><ref type="bibr" target="#b15">(Kipf et al. , 2017))</ref>. In this paper, we adopt a one-layer GNN:</p><formula xml:id="formula_8">(A, X) = ( D−1/2 Â D−1/2 XΘ),</formula><p>where Â is the adjacency matrix with self-loops, D is the corresponding degree matrix, is the ReLU function, and Θ is the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate Solution</head><p>Although we formulate robust graph learning as an optimization problem (4), this problem is still difficult to solve for several reasons. First of all, the mutual information I( , ( )) is extremely hard to compute, mainly because = ( , ) is a joint random variable involving a high-dimensional discrete variable . In addition, the search space of the adversarial attacks, B ∞ ( , ), is intractable to quantify: There is no conventional or well-behaved choice for the distance metric in such a complicated joint space. Even when we know the metric, the distance between two random variables is difficult to calculate. Apart from the above challenges, the classical, well-known projected gradient descent algorithm does not work in the joint space = ( , ), and thus the worst-case adversarial attack ★ is no way to find. Therefore, we further address the above issues in detail. MI estimation. Directly computing I( ; ( )) in Problem (4) is intractable, especially for a joint distribution = ( , ) which includes a high-dimensional discrete random variable . Some authors propose to maximize the average MI between a high-level "global" representation and local input regions, and show significant improvement in the quality of representations <ref type="bibr" target="#b11">(Hjelm et al. 2019;</ref><ref type="bibr" target="#b19">Veličković et al. 2019)</ref>. Inspired by recent work Deep Graph Infomax <ref type="bibr" target="#b19">(Veličković et al. 2019)</ref>, we use a noise-contrastive type objective as an approximation of I( ; ( )):</p><formula xml:id="formula_9">ℓ enc ( , ) = E [log D ( , G )] + E ˜ [log (1 − D ( ˜ , G ))] , (5)</formula><p>where denotes the local/node representation obtained from a GNN encoder; G = sigmoid (E ( )) denotes the global/graph representation; ˜ is the random variable of negative examples, and ˜ is the realization of ( ˜ ). The critic function D ( , G ) represents the score assigned to a pair of local and global representations obtained from the natural samples (i.e., the original graph), while D ( ˜ , G ) is that obtained from negative samples. We adopt D Φ = sigmoid( Φ G ), where Φ is a learnable scoring matrix. Finally, in practice, the expectation over an underlying distribution is typically approximated by the mean of independent samples {( , )} ∈ [ ] . Adversarial distribution estimation. Besides the estimation of MI, another challenge in solving Problem (4) is how to find the worst-case adversarial distribution ★ ∈ B ∞ ( , ).</p><p>Here, we elaborate the three difficulties in finding ★ , and explain in detail how we solve them one by one.</p><p>First, it is difficult to choose an appropriate metric on the joint space S = (A, X) that faithfully measures the distance between each pair of point elements. An intuitive choice for the distance between any pair of 1 = ( 1 , 1 ) and 2 = ( 2 , 2 ) in the joint metric space (A, A ) and (X, X ) would be the -norm</p><formula xml:id="formula_10">A ( 1 , 2 ), X ( 1 , 2 )</formula><p>. However, this intuition fails in our case because the changes in both the graph topology and the node attributes are not in the same order of magnitude. Thereby, we have to consider the perturbations in A and X separately. With a little abuse of notation, we redefine the perturbation bound as follows:</p><formula xml:id="formula_11">B ∞ ( , , , ) = {( , ) ∈ M (A) × M (X) | ∞ ( , ) ≤ , ∞ ( , ) ≤ },</formula><p>where the small positive numbers and play the role of perturbation budget now. This is indeed a subset of the previous search space B ∞ ( , ). Moreover, although the search space has been restricted, the ∞-Wasserstein constrained optimization problem remains intractable: We still have no clue about the underlying probability distribution. Similar to what we did to estimate MI, we turn to replace the real data distribution with an empirical one. Suppose we have a set of i.i.d. samples {( , )} ∈ [ ] (note that = 1 under a transductive learning setting), based on which we can compute the empirical distribution ( ˆ , ˆ ). The empirical search space is</p><formula xml:id="formula_12">B { } =1 , { } =1 , , = ( ˆ , ˆ ) − 0 ≤ , − ∞ ≤ , ∈ [ ] ,</formula><p>where ˆ and ˆ are the empirical distributions computed from the perturbed samples {( ,</p><formula xml:id="formula_13">)} ∈ [ ] .</formula><p>Here we use the cardinality (i.e., 0 -norm) to measure the change in graph topology, and the ∞ -norm to measure the change in continuous node attributes (when node attributes are discrete, or even binary, we can also use 0 -norm for them). Finally, we notice that the empirical space</p><formula xml:id="formula_14">B { } =1 , { } =1 , , is again a subset of B ∞ ( ˆ , ˆ , , ).</formula><p>The remaining question is how to efficiently find the worst-case adversarial attack. The classical choice for the image domain, i.e., the projected gradient descent (PGD) method <ref type="bibr">(Madry et al. 2018)</ref>, is no longer applicable in our case, as the graph topology is a Boolean random matrix. As a remedy for the discrete case, we adopt a projected gradient descent topology attack for graph topology <ref type="bibr" target="#b26">(Xu et al. 2019a)</ref>. More specifically, we first find a convex hull of the discrete feasible set, and apply the projected gradient method. A binary sub-optimal solution of worst-case is then recovered using random sampling. This projected gradient descent topology attack helps us identify the worst-case adversarial example efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Connection to Label Space</head><p>In this section, we examine the ability of the proposed robust graph encoder of blocking perturbation and benefiting downstream tasks. To better understand the power of our robust model, we establish a theoretical connection between the robustness of representations (measured by our proposed GRV) and the robustness of the potential model built upon the representations. We take node classification as an example task, and the result can be easily generalized to other classical graph learning tasks. We first introduce the concept of adversarial gap (AG) to measure the robustness of the downstream node classifier, and then explore some interesting theoretical connections between GRV and AG. Adversarial gap. Adversarial gap (AG) is a classical measure of robustness for node classification in inductive learning. Let and be the adjacency matrix and the attribute matrix of an induced subgraph. Denote by (S, ) the input metric space and Y the space of labels. For a node classifier : S → Y, we define the adversarial risk of with the budget ≥ 0 as</p><formula xml:id="formula_15">AdvRisk ( ) = P ∃ = ( , ) ∈ B ( , ), s.t. ( , ) ≠ , where B ( , ) = { ∈ X | ( , ) ≤ }.</formula><p>The adversarial gap (AG) is then defined as</p><formula xml:id="formula_16">AG ( ) = AdvRisk ( ) − AdvRisk 0 ( ),</formula><p>which measures the relative vulnerability of the given model . Apparently from the definition, a smaller value of AG (or AdvRisk) implies a more robust node classifier . Table <ref type="table" target="#tab_0">1</ref> briefly summarizes the robustness measures, including AG, RV and GRV. The traditional model robustness, adversarial gap (i.e., AG ( ) and AG ( )), is based on the label space Y, while the MI-based robustness measures (i.e., RV * ( ) and GRV * ( )) are built upon the representation space Z. The prior work <ref type="bibr" target="#b31">(Zhu et al. 2020)</ref>, which defines RV ( ) on a single input space X in the image domain, has shown that RV ( ) has a clear connection with classifier robustness. Comparatively, the graph representation venerability GRV ( ) is defined on a joint input space (A, X) The subscript denotes the perturbation budget of (i.e., the image) on the image domain, while the subscript denotes the perturbation budget of ( , ) on the graph domain.</p><formula xml:id="formula_17">Robustness measure Domain Input space Output space AG ( ) Image Single X Y AG ( ) Graph Joint (A, X) Y RV ( ) Image Single X Z GRV ( ) Graph Joint (A, X) Z</formula><p>in the graph domain. Thus the new definition is essentially different from the one on the image domain due to the existence of both discrete and continuous input data structures.</p><p>In what follows, some interesting theoretic are presented to show inherent relationship between the graph representation vulnerability GRV ( ) and the adversarial gap AG ( ). We first work on two special cases under each of the following assumptions. Both assumptions are imposed on the statistical independence between the input random variables (i.e., or ) and the output label . ). Thus, the representation of each node depends only on its one-hop neighbors, and then the corresponding column of A can be used directly to compute the representation for each node. Additionally, inspired by <ref type="bibr" target="#b16">(Miyato et al. 2017;</ref><ref type="bibr" target="#b6">Dai et al. 2019)</ref>, in which perturbation on intermediate representations is defined, we opt to define the adversarial distribution w.r.t instead of that w.r.t and respectively. This assumption is reasonable owing to our focus on the robustness of our model rather than the real attack strategies. Accordingly, we assume that the set of adversarial distributions is ∼ Bernoulli(0.5 + • ( − 0.5)) hold, but = 1 , = 1, . . . , | | and the set of encoders follows such that</p><formula xml:id="formula_18">B ∞ ( , ) = { ∈ M (H ) : ∞ ( , )<label>≤</label></formula><formula xml:id="formula_19">E = { : ( , ) ↦ → sgn[( − 0.5| |1 ) ] | 2 = 1}</formula><p>, which can be regarded as the non-attribute case. Then, given ≥ 0, for any ∈ E, we have</p><formula xml:id="formula_20">GRV ( ) ≥ 1 − 0.5 − 0.5AG ( * • ) (7a) GRV ( ) ≤ 1 − 0.5 − AG ( * • )<label>(7b)</label></formula><p>Similarly, we have GRV ∝ AG in Theorem 4.2. Note that Theorems 4.1 and 4.2 still hold when contains self-loops. General case. We illustrate a more general case in which is dependent on both and . In the general case, we can extend <ref type="bibr">(Zhu et al. 2020, Theorem 3.4</ref>) to the domain. Regardless of the encoder, the theorem below provides a general lower bound of adversarial risk over any downstream classifiers that involves both MI and GRV. Theorem 4.3 <ref type="bibr" target="#b31">(Zhu et al. 2020)</ref>. Let (S, ) be the input metric space, Z be the representation space and Y be the label space. Assume that the distribution of labels over Y is uniform and is the random variable following the joint distribution of inputs . Further suppose that F is the set of downstream classifiers. Given ≥ 0, inf</p><formula xml:id="formula_21">∈ F AdvRisk ( • ) ≥ 1 − ( ; ( )) − GRV ( ) + log 2 log |Y|</formula><p>holds for any encoder . Theorem 4.3 suggests that lower adversarial risk over all downstream classifiers cannot be achieved without either lower GRV or higher MI between and ( ). It turns out that jointly maximizing ( ; ( )) and minimizing GRV ( ) enables the learning of robust representations. Note that Theorem 4.3 also holds in the graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In the experiments, we train our model in a fully unsupervised manner, and then apply the output representations to three graph learning tasks. Compared with non-robust and other robust graph representation models, the proposed model produces more robust representations to defend adversarial attacks. Furthermore, the superiority of our model still holds under different strengths of attacks and under various attack strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>For evaluation, we use three datasets, Cora, Citeseer and Polblogs, and compare our model with the following baselines.</p><p>• We also include Ours-soft, an variant of our model which removes soft margin on GRV.</p><p>Implementation details. In the training phase, we adopt the projected gradient descent topology attack <ref type="bibr" target="#b26">(Xu et al. 2019a</ref>) and PGD attack <ref type="bibr">(Madry et al. 2018)</ref> to construct adversarial examples of and , respectively. We set = 5e-3, = 0.4|E|, and = 0.1. For Polblogs, we do not perform attacks on the pseudo node attributes. In evaluation, we use the same attack strategy as in training, but set = 0.2|E| to satisfy imperceptible constraint. Considering the training efficiency and the real attack during evaluation, the step size and iteration number are set different. Note that DeepWalk and RSC both require the entire graph, and thus we retrain them using polluted data. The evaluation is performed on node classification, link prediction, and community detection. We run 10 trials for all the experiments and report their average performance and standard deviation. Codes are available at: https://github.com/galina0217/robustgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Performance on downstream tasks. Table <ref type="table" target="#tab_1">2</ref> summarizes the performance of different models in three tasks. We see that our model beats the best baseline by an average of +1.8% on the node classification, +1.8% on the link prediction and +45.8% on the community detection. It's worth noting that, in community detection, adversarial attacks can cause dramatic influence on model performance because the task itself is very sensitive to the global graph topology. The difference between the performance of our model and that of those non-robust graph learning models indicates the importance of defense. Moreover, our model still stands out with huge lead when compared with existing defense models. Besides, the ablation study, i.e., comparing the last two rows in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Increasing perturbation rate ✏ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "  Performance under different rates of perturbation. We further compare our model with several strong competitors under various strength of adversarial attacks on the graph topology and the node attributes, by choosing different perburation rates and , respectively. We use the node classification task and the Cora dataset as an illustrative example. As shown in Figure <ref type="figure" target="#fig_5">2</ref>, the performance of our model is consistently superior to other competitors, both on average and in worst-case. Note that the strong competitor DGI generates negative samples in the training phase, and this might explain the robustness of the DGI model. Comparably, the high standard deviation of DGI-SVD might be attributed to the continuous low-rank approximation of the adjacency matrix: the output of truncated SVD is no longer a 0-1 matrix, which violates the discrete nature of graph topology.</p><formula xml:id="formula_22">V 1 g d x y R u E I g f V V D A l m o C I P T M G a 8 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 0 9 S w X 5 A G 8 p m O 2 m X b j Z x d y O U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K n R 4 m m o t Y 9 s s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + b 3 T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 b W f c Z m k B i V b L A p T Q U x M Z s + T A V f I j J h Y Q p n i 9 l b C R l R R Z m x E J R u C t / z y K m n V q t 5 F t X Z / W a n f 5 X E U 4 Q R O 4 R w 8 u I I 6 3 E I D m s B A w D O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 9 R u 5 A w &lt; / l a t e x i t &gt; ✏ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V 1 g d x y R u E I g f V V D A l m o C I P T M G a 8 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 0 9 S w X 5 A G 8 p m O 2 m X b j Z x d y O U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K n R 4 m m o t Y 9 s s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + b 3 T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 b W f c Z m k B i V b L A p T Q U x M Z s + T A V f I j J h Y Q p n i 9 l b C R l R R Z m x E J R u C t / z y K m n V q t 5 F t X Z / W a n f 5 X E U 4 Q R O 4 R</formula><formula xml:id="formula_23">1 A 8 + X i D 1 c p k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P B i y e p Y D + g D W W z 2 b R r N 7 t h d y K U 0 v / g x Y M i X v 0 / 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g B j 3 v 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l l G Z p q x J l V C 6 E x L D B J e s i R w F 6 6 S a k S Q U r B 2 O b m Z + + 4 l p w 5 V 8 w H H K g o Q M J I 8 5 J W i l V i 9 i A k m / X P G q 3 h z u K v F z U o E c j X 7 5 q x c p m i V M I h X E m K 7 v p R h M i E Z O B Z u W e p l h K a E j M m B d S y V J m A k m 8 2 u n 7 p l V I j d W 2 p Z E d 6 7 + n p i Q x J h x E t r O h O D Q L H s z 8 T + v m 2 F 8 H U y 4 T D N k k i 4 W x Z l w U b m z 1 9 2 I a 0 Z R j C 0 h V H N 7 q 0 u H R B O K N q C S D c F f f n m V t G p V / 6 J a u 7 + s 1 O / y O I p w A q d w D j 5 c Q R 1 u o Q F N o P A I z / A K b 4 5 y X p x 3 5 2 P R W n D y m W P 4 A + f z B 5 Z 2 j y w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S F A 2 6 H c z K S P D y + k 0 1 A 8 + X i D 1 c p k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P B i y e p Y D + g D W W z 2 b R r N 7 t h d y K U 0 v / g x Y M i X v 0 / 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g B j 3 v 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l l G Z p q x J l V C 6 E x L D B J e s i R w F 6 6 S a k S Q U r B 2 O b m Z + + 4 l p w 5 V 8 w H H K g o Q M J I 8 5 J W i l V i 9 i A k m / X P G q 3 h z u K v F z U o E c j X 7 5 q x c p m i V M I h X E m K 7 v p R h M i E Z O B Z u W e p l h K a E j M m B d S y V J m A k m 8 2 u n 7 p l V I j d W 2 p Z E d 6 7 + n p i Q x J h x E t r O h O D Q L H s z 8 T + v m 2 F 8 H U y 4 T D N k k i 4 W x Z l w U b m z 1 9 2 I a 0 Z R j C 0 h V H N 7 q 0 u H R B O K N q C S D c F f f n m V t G p V / 6 J a u 7 + s 1 O / y O I p w A q d w D j 5 c Q R 1 u o Q F N o P A I z / A K b 4 5 y X p x 3 5 2 P R W n D y m W P 4 A + f z B 5 Z 2 j y w = &lt; / l a t e x i t &gt;</formula><p>Performance under other attack strategies. In practice, we do not know which kind of attack strategies the malicious users are going to use. Thus it is interesting and important to know the performance of our model across different types of adversarial attacks. We adapt some common attack strategies to the unsupervised setting and use them as baselines. 1) Degree/Betw/Eigen: flip edges based on the sum of the degree/betweenness/eigenvector centrality of two end nodes; 2) DW <ref type="bibr" target="#b1">(Bojchevski et al. 2019a</ref>): a black-box attack method designed for DeepWalk. We set the size of the sampled can- didate set to 20K, as suggested in <ref type="bibr" target="#b1">(Bojchevski et al. 2019a</ref>). This time we consider the node classification task on Polblogs for illustration. This choice is convincing because all the above attack strategies only vary the graph topology, which is the only information we know about Polblogs. Results in Table <ref type="table" target="#tab_3">3</ref> show that our model's superiority persists in three attack strategies out of four. Comparison between Table 2 and Table <ref type="table" target="#tab_3">3</ref> shows that the projected gradient descent topology attack via MI is the most effective attack strategy used here, which verifies that our model learns the worst adversarial example that deteriorates the performance most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we study unsupervised robust representation learning on graphs. We introduce the graph representation vulnerability to quantify the robustness of an unsupervised graph encoder. After that we propose a robust unsupervised graph model that can enhance robustness as well as improve expressive power. We further build sound theoretical connections between GRV and one example task, node classification. Extensive experimental results demonstrate the effectiveness of our method on blocking perturbations on input graphs, regardless of the downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>H S 3 D h B p p w D y 3 o A I U E n u E V 3 l C K X t A 7 + l i O l l C x c w p / g D 5 / A P V 9 k a 8 = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4= " M q d b b 7 W p X z Z t Y Q e z q v O p m W 6 2 U Z c = " &gt; A A A C D 3 i c b V D L S s N A F J 3 4 r P U V d e l m s C i u S l I F x V X B j c u K f W E T y m Q 6 b Y d O Z s L M R C k h f + D G X 3 H j Q h G 3 b t 3 5 N 0 7 a g N p 6 4 M L h n H u 5 9 5 4 g Y l R p x / m y F h a X l l d W C 2 v F 9 Y 3 N r W 1 7 Z 7 e p R C w x a W D B h G w H S B F G O W l o q h l p R 5 K g M G C k F Y w u M 7 9 1 R 6 S i g t f 1 O C J + i A a c 9 i l G 2 k h d + 4 h c Q C 9 E e o g RS 2 5 S 6 E k 6 G G o k p b j / 0 W / T r l 1 y y s 4 E c J 6 4 O S m B H L W u / e n 1 B I 5 D w j V m S K m O 6 0 T a T 5 D U F D O S F r 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Topology-aware: given ⊥ , ( | , ) = ( | ) • Attribute-aware: given ⊥ , ( | , ) = ( | ) Special cases. To obtain a tractable surrogate model, we consider a simplified GNN-based encode architecture = (Wu et al. 2019a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>}, where H = { : ∀ ∈ A, ∈ X}. In Theorems 4.1 and 4.2, we denote by ∈ {0, 1} |V | one column in A and = X. The subscript of GRV, AdvRisk and AG represents that they are defined via B ∞ ( , ), while F = { : ↦ → } denotes the set of non-trivial downstream classifiers, * = arg min ∈ F AdvRisk ( • ) is the optimal classifier built upon , and is the binary entropy function. Moreover, when indexing and , denotes the -th entry of and denotes the -th row of . Theorem 4.1 (Topology-aware) Let (A, • 0 )and (X, •) be the input metric spaces, Y = {−1, +1} be the label space and Z = {−1, +1} be the representation space. The set of encoders with Θ ∈ R |V| is as follows: E = { : ( , ) ∈ S ↦ → sgn[ 2 ) where = 1, 2, . . . , | | and 0 &lt; &lt; 1. Then, given ≥ 0, for any ∈ E, we obtain GRV ( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Non-robust graph representation learning: 1) Raw: concatenating graph topology and node attributes (only graph topology for Polblogs); 2) DeepWalk (Perozzi et al. 2014): a random walk-based unsupervised graph model; 3) Deep-Walk+X: concatenating the Deepwalk embedding and the node attributes; 4) GAE (Kipf et al. 2016): variational graph auto-encoder and 5) DGI (Veličković et al. 2019): another unsupervised graph model based on MI. • Defense models: 1) Dwns_AdvT (Dai et al. 2019): a defense model designed for Deepwalk; 2) RSC (Bojchevski et al. 2017): a robust unsupervised graph model via spectral clustering; 3) DGI-EdgeDrop (Rong et al. 2020): a defense model that works by dropping 10% of edges during training DGI; 4) DGI-Jaccard (Wu et al. 2019b): DGI applied to a pruned adjacency matrix in which nodes with low Jaccard similarity are forced to be disconnected; and 5) DGI-SVD (Entezari et al. 2020): DGI applied to a lowrank approximation of the adjacency matrix obtained by truncated SVD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>w 8 u I I 6 3 E I D m s B A w D O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 9 R u 5 A w &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " S F A 2 6 H c z K S P D y + k 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of different models under various perturbation rates and . The downstream task is node classification and we use the Cora dataset for illustration. The shaded area indicates the standard deviation (×0.1) over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of robustness measures. Adversarial gap (AG) is built on the label space Y, while representation vulnerability (RV) and graph representation vulnerability (GRV) are MI-based measures built on the representation space Z.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>, shows the superiority of the soft margin on GRV.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Node classification (Acc%)</cell><cell cols="3">Link prediction (AUC%)</cell><cell cols="2">Community detection (NMI%)</cell></row><row><cell>Model</cell><cell>Dataset</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Polblogs</cell><cell>Cora</cell><cell cols="2">Citeseer Polblogs</cell><cell>Cora</cell><cell>Citeseer Polblogs</cell></row><row><cell>Raw</cell><cell></cell><cell cols="7">57.4±3.0 49.7±1.6 73.9±0.9 60.5±0.1 50.2±0.5 89.0±0.4 9.7±7.5</cell><cell>1.0±0.5</cell><cell>0.2±0.1</cell></row><row><cell cols="2">DeepWalk</cell><cell cols="8">56.2±1.1 16.5±0.9 80.4±0.5 55.4±0.8 50.3±0.3 89.2±0.7 34.6±0.6 11.1±1.0 0.4±0.5</cell></row><row><cell cols="2">DeepWalk + X</cell><cell cols="2">59.3±0.4 26.5±0.5</cell><cell>-</cell><cell cols="2">55.9±0.6 50.9±0.3</cell><cell>-</cell><cell cols="2">34.2±3.7 11.1±1.3</cell><cell>-</cell></row><row><cell>GAE</cell><cell></cell><cell cols="8">14.0±1.2 16.2±1.1 49.9±1.2 52.4±1.4 50.9±1.8 50.5±1.3 10.9±2.1 1.4±1.7</cell><cell>9.2±1.0</cell></row><row><cell>DGI</cell><cell></cell><cell cols="8">69.3±2.8 53.2±2.2 75.2±2.4 68.6±0.4 57.6±2.1 91.2±1.1 30.3±3.5 8.5±3.8</cell><cell>6.0±5.6</cell></row><row><cell cols="2">Dwns_AdvT</cell><cell cols="8">59.2±1.2 25.0±1.0 80.7±0.5 56.0±0.7 50.7±0.4 89.5±0.8 35.0±0.7 11.5±1.0 0.9±0.7</cell></row><row><cell>RSC</cell><cell></cell><cell cols="7">46.9±3.5 34.0±2.2 58.9±1.7 52.5±0.4 57.2±0.2 61.5±0.4 4.9±0.7</cell><cell>1.8±0.4</cell><cell>4.4±4.3</cell></row><row><cell cols="10">DGI-EdgeDrop 56.0±4.3 49.0±4.5 79.8±1.7 66.2±0.8 61.3±0.9 89.3±1.6 30.1±6.8 7.34±0.8 9.0±7.8</cell></row><row><cell cols="2">DGI-Jaccard</cell><cell cols="8">69.4±2.8 57.1±1.3 79.3±0.8 63.8±0.8 57.6±1.0 84.7±0.9 16.4±1.1 6.1±0.6 12.9±0.0</cell></row><row><cell cols="2">DGI-SVD</cell><cell cols="8">68.1±8.0 56.1±16.4 81.6±0.7 60.1±0.8 54.7±1.3 85.2±0.7 16.2±0.9 6.5±0.8 13.0±0.0</cell></row><row><cell cols="2">Ours-soft</cell><cell cols="8">69.4±0.7 57.5±2.0 79.7±2.1 68.1±0.3 58.2±1.3 90.3±0.5 39.2±8.8 23.5±1.9 12.6±9.6</cell></row><row><cell>Ours</cell><cell></cell><cell cols="8">70.7±0.9 58.4±1.4 82.7±2.2 69.2±0.4 59.8±1.3 91.8±0.4 41.4±4.7 23.6±2.8 14.8±2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results for the node classification, link prediction and community detection tasks using polluted data.</figDesc><table><row><cell></cell><cell>78</cell><cell></cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DGI</cell></row><row><cell>Accuracy (%)</cell><cell>66 70 74</cell><cell></cell><cell cols="2">DGI-Jaccard DGI-SVD</cell></row><row><cell></cell><cell>62</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1e-3</cell><cell>5e-3 1e-2</cell><cell>5e-2 1e-1</cell><cell>5e-1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DeepWalk 87.8±0.9 83.5±1.2 84.3±1.0 87.7±0.9 DeepWalk + X 85.8±2.7 82.7±2.1 85.0±1.1 88.3±0.9 GAE 83.7±0.9 81.0±1.6 81.5±1.4 85.4±1.1 DGI 86.6±1.1 84.8±1.2 84.8±1.0 86.4±1.1 Dwns_AdvT 88.0±1.0 84.1±1.3 84.6±1.0 88.0±0.8 RSC 52.1±1.3 51.9±0.7 51.4±0.5 52.6±1.1 DGI-EdgeDrop 87.1±0.3 87.0±0.6 80.5±0.5 86.3±0.3 DGI-Jaccard 82.1±0.3 80.7±0.4 80.6±0.3 82.2±0.2 DGI-SVD 86.5±0.2 85.6±0.2 86.1±0.2 85.3±0.3 Defense against different attackers on Polblogs for the node classification task.</figDesc><table><row><cell>Model Attacker</cell><cell>Degree</cell><cell>Betw</cell><cell>Eigen</cell><cell>DW</cell></row><row><cell>Raw</cell><cell cols="4">87.4±0.3 84.1±0.8 86.4±0.6 87.9±0.4</cell></row><row><cell>Ours-soft</cell><cell cols="4">88.5±0.7 85.7±1.5 86.2±0.4 88.7±0.7</cell></row><row><cell>Ours</cell><cell cols="4">89.3±0.7 86.3±1.2 86.7±0.4 89.0±0.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">The Thirty-Sixth AAAI Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by NSFC (62176233), the National Key Research and Development Project of China (2018AAA0101900), NSF III-1705169, Okawa Foundation Grant, Amazon Research Awards, Picsart gift, NSFC (71531006), the Major Scientific Project of Zhejiang Laboratory (2020MC0AE01) and the Fundamental Research Funds for the Central Universities (Zhejiang University New Generation Industrial Control System (NGICS) Platform)</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>L S g M x F L 3 j s 9 b X q E s 3 w S J U k D J T B d 0 I F T c u K / Y F 7 V A y a d q G Z h 4 k G a E M 8 w t u / B U 3 L h R x 6 8 6 d f 2 O m H X y 0 H g i c e 8 6 9 5 N 7 j h p</p><p>b A n u 6 p f X S b d R d 6 / q j Y f r W r N d 1 F G G M z i H S 3 D h B p p w D y 3 o A I U</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Theorem 4.1 reveals an explicit connection between GRV ( ) and AG ( * • ) achieved by the best classifier in the topology-aware case. We note that ( ) is concave on (0, 1) and that the maximum of is attained uniquely at = 0.5. Thus, a smaller GRV implies a smaller AG, and vice versa.</p><p>Theorem 4.2 (Attribute-aware) Let (A, • 0 ) and (X, • ) be the input metric spaces, Y = {−1, +1} be the label space and Z = {−1, +1} be the representation space. Suppose that the set of encoders is as in (6). Assume that the samples Then, given ≥ 0, for any ∈ E, we have:</p><p>Next, consider a simpler case in which u.a.r. ∼ {−1, +1},</p><p>that is, will aggregate 0 samples with = +1 and 1 samples with = −1. Further suppose that the set of encoders is as presented in (6). Then, given ≥ 0, (7) also holds for any ∈ E.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Certifiable robustness to graph perturbations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="1656" to="1665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ∞-Wasserstein Distance: Local Solutions and Existence of Optimal Transport Maps</title>
		<author>
			<persName><forename type="first">T</forename><surname>Champion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05730</idno>
		<title level="m">A survey of adversarial learning on graph</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adversarial Training Methods for Network Embedding. In WWW</title>
		<imprint>
			<biblScope unit="page" from="329" to="339" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2015">2020. 2015</date>
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
	<note>WSDM</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2017. 2018</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2017. 2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>Deepwalk: Online learning of social representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690,abs/1608.07690</idno>
	</analytic>
	<monogr>
		<title level="j">Tsipras</title>
		<editor>ICLR. Tanay, T.</editor>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2016. A boundary tilting persepective on the phenomenon of adversarial examples. Robustness may be at odds with accuracy. In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Certified robustness of graph neural networks against adversarial structural perturbation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
	<note>Deep graph infomax</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04429</idno>
	</analytic>
	<monogr>
		<title level="m">GraphDefense: Towards robust graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019a</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="volume">ĲCAI</biblScope>
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defenses in images, graphs and text: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="178" />
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06757</idno>
		<title level="m">Query-free Black-box Adversarial Attacks on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NetRL: Task-aware Network Denoising via Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust Network Enhancement from Flawed Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">ĲCAI</biblScope>
			<biblScope unit="page" from="3961" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mining Fraudsters and Fraudulent Strategies in Large-Scale Mobile Social Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="179" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Theoretically Principled Trade-off between Robustness and Accuracy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="7472" to="7482" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS D&amp;B</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning adversarially robust representations via Worst-Case mutual information maximization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11609" to="11618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Certifiable robustness and robust training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="246" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
