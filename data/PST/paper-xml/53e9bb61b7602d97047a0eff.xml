<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Pay-As-You-Consume Cloud Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shadi</forename><surname>Ibrahim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
							<email>bshe@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University Singapore</orgName>
								<address>
									<postCode>639798</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
							<email>hjin@hust.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Cluster and Grid Computing Lab Services Computing Technology and System Lab Huazhong</orgName>
								<orgName type="institution">University of Science and Technology Wuhan</orgName>
								<address>
									<postCode>430074</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Pay-As-You-Consume Cloud Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20D5E6C6E570DF96418666310DBBD325</idno>
					<idno type="DOI">10.1109/SCC.2011.38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud Computing</term>
					<term>Virtualization</term>
					<term>Pay-As-You-Go</term>
					<term>Pay-As-You-Consume</term>
					<term>Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cloud computing enables users to perform their computation tasks in the public virtualized cloud using a payas-you-go style. Current pay-as-you-go pricing schemes typically charge on the incurred virtual machine hours. Our case studies demonstrate significant variations in the user costs, indicating significant unfairness among different users from the micro-economic perspective. Further studies reveal the reason for such variations is interference among concurrent virtual machines. The amount of interference cost depends on various factors, including workload characteristics, the number of concurrent VMs, and scheduling in the cloud. In this paper, we adopt the concept of pricing fairness from micro economics, and quantitatively analyze the impact of interference on the pricing fairness. To solve the unfairness caused by interference, we propose a pay-as-you-consume pricing scheme, which charges users according to their effective resource consumption excluding interference. The key idea behind the pay-as-you-consume pricing scheme is a machine learning based prediction model of the relative cost of interference. Our preliminary results with Xen demonstrate the accuracy of the prediction model, and the fairness of the pay-as-you-consume pricing scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Cloud computing has recently emerged as a popular paradigm for harnessing a large number of commodity machines in the cloud. In such a paradigm, users are allowed to use the computation resources with respect to a pricing scheme similar to the economic exchanges in the utility market place. However, unlike the utility market, which typically has standard fine-grained charging units (such as kilowatt per hour (kwh) in electricity market), there are no standard pricing units in a cloud environment 1 , particularly for computation as a services cloud. One of the common schemes used by recent cloud providers is primarily based on virtual machine (VM) hours on the virtualized cloud environment (e.g. Amazon charges per small instance hour at $0.085 <ref type="bibr">[1]</ref>). Many existing studies <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b2">4]</ref> have focused on 1 Infrastructure as a Service cloud can be classified into Storage as a Service and Computation as a Service. Our focus is on the pricing scheme in the CaaS, as in the SaaS we already have well defined fine-grained charging unite, data size/transfer per Gigabyte.</p><p>reducing the virtual machine hour usage. In contrast, we investigate the variance on the virtual machine hour usage in the cloud.</p><p>A cloud is a multi-tenant infrastructure -users may share the same physical infrastructure. Hence, there are interferences between VMs, such as interferences in disk and network I/O, and CPU (since the L2 data cache is shared on multi-core processors). For example, a user may have multi VM instances running within shared physical servers, on which the resource consumption of each VM varies due to the interference. This sharing leads to several fundamental observations: interferences may increase the running time of a certain task on a VM, resulting in unfairness between users (not only do interferences result in lower performance and higher cost for user, but the cost of interferences can also vary with users). Nevertheless, the amount of interference cost depends on various factors, such as an application's type, the number of VMs, and VM scheduling algorithms in the hypervisor <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8]</ref>.</p><p>Since cloud computing is an economically-driven distributed system paradigm, pricing fairness is an important economic feature for a good pricing scheme <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10]</ref>. In economics, pricing fairness includes personal and social fairness. Personal fairness is subjective, and typically means that the pricing should be low enough, while social fairness mainly investigates whether users have the same financial cost for the same task. An unfair pricing scheme could foster dissatisfaction from users, and eventually the provider could lose customers. A previous study <ref type="bibr" target="#b9">[11]</ref> has demonstrated the cost variance between different runs on Amazon EC2. In this study, we provide a quantitative study of the pricing fairness with respect to the virtualization internals, and investigate how we can remedy the pricing unfairness.</p><p>As a start to understanding these two aspects in the pricing fairness, we define two metrics to quantify these two kinds of fairness. For the same task, we use the difference in the cost for the same user when running the VM instance alone and in the presence of other VM instances to measure the personal fairness, and we use the difference in the cost among different users to measure the social fairness. Since in the cloud users run different tasks, we extend the social fairness to be the difference in the ratio of the extra cost caused by interference to the total cost. Accordingly, we study the performance interference caused by the abovementioned interference factors in our local Xen virtualized cloud with a focus on both the personal and social fairness of users cost. We quantify the cost interference to be the difference between the concurrent execution and the execution without interference. We define the Effective Virtual Machine Time (EVMTime) to finish a task -the amount of time when the VM is only running on the physical machine -as the charging unit. With the two aforementioned fairness metrics, we observe that the current pricing scheme based on virtual machine time is neither personally or socially fair. To remedy the unfairness caused by interference, we propose a pay-as-you-consume pricing scheme, which charges users according to their effective resource consumption excluding interference. Thus, the payas-you-consume scheme reflects the real cost of executing the task and provides a fair cost to users, by means of the Effective Virtual Machine Time. Accordingly, our pricing scheme embraces an intelligent prediction model on the relative cost of interference.</p><p>Unfortunately, the cost estimation of the interference is a challenging issue, due to the following factors. The interference is caused by congestion in the shared resources: CPU, disk I/O and network I/O. Even worse, the fairness in individual resource does not guarantee global fairness, due to the misalignment among the scheduling in the individual resources. Another difficulty is that the hypervisor does not have full knowledge of the application running in the VMs. Intuitively; it seems to be very complicated task, even impossible, to model the interference in virtualized environments, but motivated by the huge success and accuracy level of using machine learning techniques for enhancing the system performance in storage systems <ref type="bibr" target="#b10">[12]</ref>, we propose to use machine learning techniques, particularly Support Vector Machine (SVM) algorithms, to automatically identify the key parameters affecting the interference. Our preliminary experimental results with Xen demonstrate the accuracy of the prediction model, and the fairness of the payas-you-consume pricing scheme. For example, our results of the data collected from the I/O benchmark, using libsvm <ref type="bibr" target="#b11">[13]</ref> (a popular machine learning toolkit) demonstrate that our predication model can achieve around 90% accuracy in predicting the interference score for different I/O workloads. Thus, our pay-as-you-consume model can offer better fairness for users in terms of personal and social fairness.</p><p>The paper is organized as follows. Section 2 discusses the interference in Xen, followed by the motivating performance results in section 3. In section 4, we introduce the pay-as-you-consume model. We discuss related work in section 5 and conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. INTERFERENCE IN XEN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>The Xen hypervisor is a para-virtualizing virtual machine monitor (VMM) <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b13">15]</ref>, in which the machine architecture presented to an operating system is not identical to the underlying hardware. The Xen hypervisor is responsible for resource (CPU, memory and I/O device, etc.) allocation for the various virtual machines running on the same hardware device. There is an initial domain, called Domain 0, which is a modified Linux kernel. Dom0 is a unique virtual machine running on the Xen VMM that has privilege to access physical I/O devices as well as interact with the other VMs. Other VMs sharing the same host with Dom0 are called DomainUs or Guest Os.</p><p>Xen Schedulers. Xen is unique among VMM software because it allows users to choose among different CPU schedulers and I/O schedulers. From version 3.1.0, Xen has two different CPU schedulers available, Credit and Simple Earliest Deadline First (SEDF), both allowing users to specify CPU allocation via CPU weights. Moreover, there are currently four available I/O schedulers in the 2.6 Linux kernels: Noop, Anticipatory, Deadline, and Complete Fair Queuing scheduler (CFQ). Furthermore, users can select the I/O schedulers on the fly in both Dom0 and DomUs. For more details about CPU and I/O schedulers used in Xen, readers can refer to <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intra-machine Interference in Xen</head><p>The Xen hypervisor is responsible for providing isolation among the virtual machines and managing their access to hardware resources, that is, the Xen hypervisor performs functions such as scheduling processes and allocating memory among different guest operating systems. As the hardware resources are shared by multiple VMs, the current virtualized system experiences unpredictable and unstable performance, not to mention the performance degradation in some scenarios <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17]</ref>. Different software solutions <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19]</ref> can be adopted to reduce the interference in the shared environment. Previous studies have revealed that the reason of such behavior is due to VMs interference. Interference in a virtualized environment is caused by two conflated reasons as explained below:</p><p>• Inherited Interference. This is the interference caused by the underlying technology, hardware and software, which is still a key problem in traditional (nonvirtualized) systems, such as the shared L2 data cache on multi-core processors <ref type="bibr" target="#b18">[20]</ref> (hardware). There is also the interference by selecting the right scheduler, CPU and I/O schedulers, within the operating system (software) when diverse applications are introduced <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7]</ref>. • VMM interference. The interference caused by the architecture of the Xen hypervisor, which is the tradeoff between risk isolation and fairness. In particular, we have driver domain interference as it provides access to the actual hardware I/O devices. Thus I/O resources will be shared by multiple VMs. In addition to the interference introduced by the resources contention between Dom0 and other domains that are running CPU applications, when I/O applications are performed <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b19">21]</ref>, all the traffic must pass through the driver domain.</p><p>Due to the various shared resources, interference does occur, especially when diverse applications are introduced on different VMs. This situation is even worse in the cloud, as a provider who is responsible to maintain and configure the VMM schedulers has no knowledge about the applications being run by users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EMPIRICAL STUDY ON XEN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Micro Benchmarks</head><p>Looking at case studies about cloud providers <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b21">23]</ref>, we identify several popular applications, such as web-related tasks, storage backup, and high performance computing. As a start, we use the following micro benchmarks to mimic the workload in these popular applications. These include Postmark (I/O-intensive benchmark) <ref type="bibr" target="#b22">[24]</ref> and PARSEC (we chose BlackScholes as an example to study the pricing fairness for CPU-intensive applications) <ref type="bibr" target="#b23">[25]</ref> running on a single machine. We use the same settings as <ref type="bibr" target="#b9">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>Our experiment is conducted on a physical node, equipped with two 2-core 2.33GHz Xeon processors, 4GB of memory and 500GB of disk, running CentOS. All results described in this section are obtained using Xen version 3.4.2. All the virtual machines used in our experiments are configured with 1 VCPU pinned to its own core and 768MB of memory and 60GB of virtual disk. We adopt the pricing scheme from Amazon: $0.085 for a small virtual machine instance <ref type="bibr">[1]</ref>. We conduct our experiments on our local test bed so we have full control of the environment to get detail results of how the system internals affect the cost. We study the price fairness through evaluating different consolidation strategies as well as the Xen scheduler.</p><p>Metrics. We use the following metrics to evaluate the price fairness. The personal fairness is based on the extra cost caused by interference. We use the following formula:</p><formula xml:id="formula_0">= Extra Cost By Interference Interference Cost Total Cost</formula><p>(1)</p><p>In order to find the interference cost fairness (social fairness), we use Jain's fairness measure <ref type="bibr" target="#b24">[26]</ref> to quantify the fairness among the VMs when running different applications within the same physical node:</p><formula xml:id="formula_1">( ) = = × = ∑ ∑ 2 m i i 1 m 2 i i 1 x m x Fairness (2)</formula><p>where x i denotes the interference cost of VM i , and m is the number of the VMs within the same physical node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) The Impacts of the VM Consolidation on Fairness</head><p>In order to elaborate the impacts of VM consolidation on the fairness of users' cost, we vary the number of VMs which are deployed within the host to two and three VMs, while we run similar applications in the background 2 .</p><p>Personal Fairness. We observe that, in the presences of VM consolidation, the current pricing scheme is far from being fair. Moreover, the interference cost is increasing with the number of VMs that are deployed within the same host and varies according to the running application.</p><p>For instance, the interference cost of VM when running the I/O intensive application Postmark has dramatically increased with different consolidation levels. As shown in Fig. <ref type="figure" target="#fig_1">1-a</ref> Fig. <ref type="figure" target="#fig_1">1</ref>-a shows that the interference cost of the VM when running CPU intensive applications has slightly increased with different consolidations. The interference cost is 5% and 19% when it shares the resources with two VMs and three VMs, respectively. This can be explained due to the cache interference between the different CPUs, which is relatively small. In our study all the VCPUs are pinned to a specific CPU core, and obviously the interference is increasing as the number of VMs increases.</p><p>Social Fairness. As shown in Fig. <ref type="figure" target="#fig_1">1-c</ref>, the social fairness is nearly optimal for the same applications regardless the VM consolidation. For example, the proportion of social fairness is nearly 99.5% when all VMs are running I/O intensive applications, because the default I/O VMM scheduler (CFQ) guarantees fairness in sharing the I/O resources amongst different VMs <ref type="bibr" target="#b5">[7]</ref>. Moreover, the social 2 We refer to the applications running on other VMs as background fairness is nearly 96% when all the VMs are running CPU intensive applications, which is due to the fairness of the default CPU scheduler, the credit scheduler <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b25">27]</ref>. In summary, we observe that for the same application, due to the resource contention in the presence of VM consolidation, the current pricing scheme based on a virtual machine time is not personally fair, while it is socially fair because the default VMM scheduler (CPU and I/O schedulers) tends to fairly share the resources among different VM instances.</p><p>2) The Impacts of the Application Types on Fairness In the cloud users may run different types of applications simultaneously, where the key difference is that they consume different types of resources (e.g., CPU, memory, network or disk). Thus we fix the number of VMs which are deployed within the physical host to three VMs while varying the applications which are running on each between the Postmark and BlackScholes benchmarks.</p><p>Personal Fairness. In Fig. <ref type="figure" target="#fig_2">2</ref>-a we see that the interference cost in a VM when running I/O intensive applications in two scenarios varies with the applications' diversity. When the two background applications are both CPU applications, the interference cost of VM has increased slightly which can be explained mainly because of the priority boost impacts in the Xen credit scheduler <ref type="bibr" target="#b4">[6]</ref>, that is, when an I/O event is incurred the credit scheduler will be invoked and boost the priority of an idle domain receiving an I/O event. As a result the I/O application will perform very close to the case where it does not share the physical host with any VM. However, the I/O application will suffer a slight degradation due to the CPU interference, that is, the cache interference between the VCPU in Dom0 and the VCPUs in others domains running CPU applications, knowing that CPU overhead in Dom0 is caused by the memory page exchange by the I/O application <ref type="bibr" target="#b19">[21]</ref>. The interference cost in the VM Postmark is increasing as the number of the VMs with similar applications is increasing.</p><p>On the other hand and for the same reasons, partially due to L2 cache interference and mainly because of the priority boost impacts in the Xen credit scheduler <ref type="bibr" target="#b4">[6]</ref>, the VM with CPU intensive application suffers from relatively higher interference when both background applications are I/O applications. In contrast to the previous results, the interference cost in the VM BlachSholes is decreasing as the number of the VMs with same applications is increasing.</p><p>The previous discussion leads to a very important observation: when different applications are running within the same host, the interference cost is dominated by the resource contention between these applications and the VMM (i.e. the interference cost of an application is contributed to by direct interference with the VMM, for example, a CPU intensive application's performance degrades due to the L2 cache interference with VMM when a background application outperforms it in I/O operations) and indirect interference is caused by the VMM, for example an I/O application's performance degrades due to interference between Dom0 and other domains running CPU intensive applications. Social Fairness. Fig. <ref type="figure" target="#fig_2">2-b</ref> shows that when two different applications are sharing the host resources, the interference cost of a CPU-heavy application is relatively higher than an I/O application as explained earlier, and the proportion of social fairness is 77%. Moreover, when three VMs are deployed -two of them are running similar applications -the social fairness is relatively high. For example, it is 88% for the case of (CPU, CPU, I/O) and 98% for the case (CPU, I/O, I/O).</p><p>In summary, similar to the previous observation, we observe that social fairness varies according to the resource contention among the diverse applications running in the VMs along with the resource contention between VMs and VMM. In addition, the social interference is inversely proportional to the diversity of the applications that are sharing the same physical host, and it increases as more similar applications are sharing the host.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) The Impacts of the VMM Schedulers on Fairness</head><p>The following experiments evaluate the impacts of the different VMM schedulers used in Xen, for instance, the I/O schedulers and the CPU schedulers, for interference when running CPU and/or I/O intensive applications on two VMs, and Table I presents the result.</p><p>Previous studies have reported on the importance of selecting the right I/O scheduler or CPU scheduler and tuning them according to the running applications <ref type="bibr">[6, 7 , 16, 21]</ref>, however our study tackles a different problem, and use a different approach. We study the impacts of both VMM schedulers and CPU schedulers side by side with the disk I/O schedulers, when diverse applications are running in the VMs, in terms of personal and social fairness, thus, we are not trying to detail the reason of the performance as it is well explained in the aforementioned research papers. Furthermore, as this paper is intending to identify unfairness in the cloud and the consequences of a provider's userunaware administration, for the I/O schedulers we study the impacts of VMM scheduler regardless of the I/O scheduler running on the VM. As shown in Table <ref type="table" target="#tab_0">I</ref>, the performance of different applications, EVMTime, vary slightly according to the selected CPU and I/O schedulers in the VMM layer when one VM is uniquely deployed in the physical host. However, when consolidation is introduced, the variation of the applications' performance and the system throughput (referred to as Cost 1,2 in Table <ref type="table" target="#tab_0">I</ref>, where less cost indicates better system throughput) is increasing according to the resources' alignment policy decided by both CPU and I/O scheduler in the VMM layer.  However, as shown in Table <ref type="table" target="#tab_0">I</ref>, for the same applications, selecting the pair scheduler (SEDF, Anticipatory) leads to the lowest interference score in both VMs, hence it achieves the best system throughput. The interference scores are 0.02 and negative 0.01 for CPU applications and 0.38 and 0.49 for I/O applications. When different applications are running concurrently, the choice of the pair of schedulers is only suboptimal for one application. Here the pair (SEDF, Deadline or Anticipatory) is the best for CPU application while (Credit, CFQ) is the best for the I/O applications.</p><p>Social Fairness. As shown in Table <ref type="table" target="#tab_0">I</ref>, when both VMs are running CPU applications the social fairness varies by 12% with different pairs of schedulers, while the best social fairness is achieved when the pair (SEDF, Deadline) is selected in the VMM layer. For I/O applications the social fairness is nearly the same for all pairs schedulers with a slight advantage to the pairs of schedulers (credit, CFQ) and (SEDF, Deadline). However, the worst social fairness scenario occurred when different applications are sharing the same host, where the pair scheduler (SEDF, Deadline) had a social fairness score of 55.2%, although this pair scheduler achieves the best system throughput. These results are consistent with those in <ref type="bibr" target="#b4">[6]</ref> (i.e. SEDF guarantees better system throughput and worse fairness than Credit in the case of I/O and CPU applications that are sharing the same host).</p><p>In summary, we observe that the choice of an appropriate pairs of schedulers (CPU, disk) at the VMM layer has a significant impact on the application performance inside each VM (personal fairness) and intra-application isolation among different VMs (social fairness). More importantly, in the cloud different workloads may be performed on the same host, and as a result there is no optimal pair of schedulers when diverse applications are introduced, although some of them are sub-optimal for different workloads and can achieve better overall system throughput. However, as this paper is a call to action, we encourage further study in the area of VMM schedulers to consider both personal and social fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PAY-AS-YOU-CONSUME PRICING STYLE</head><p>We argue that because the cloud is an economy-driven distributed system, we should consider the fairness in monetary costs. Therefore, we propose the new pay-as-youconsume pricing scheme, which charges users according to their effective resource consumption. However, due to the existence of interference, it is very hard to accurately determine the effective use of resources among different users. Accordingly, we define the Effective Virtual Machine Time to finish a task: the amount of time required when the VM is the only VM running on the physical machine. As for a VM, we consider the effective virtual machine time to be: virtual machine time less the interference time. Based on the effective virtual machine time, we define the pricing fairness: any user using the same amount of effective virtual machine time is charged at the same price.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= × Cost</head><p>Instance Per Hour EVMTime New Model <ref type="bibr" target="#b1">(3)</ref> Unlike current pricing schemes, our pay-as-you-consume pricing scheme solves the unfairness by charging the users according to their effective resources consumption. Since there are various factors affecting the interference, we propose to use a machine learning model to predict the interference based on the resource usage during the running time and charge users for their effective virtual machine time as shown in Equation <ref type="formula">3</ref>. Thus, the same task tends to have the same cost, resulting in better personal and social fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interference Predication Model</head><p>If only one VM runs on the physical machine, the interference of the VM is zero. If the time for executing a task on a VM without any concurrent VM is t, and the time for executing the same task of the VM with other concurrent VMs is t`, we call the overhead of interference is t`-t.</p><p>We define I i , the interference factor to VM i :</p><formula xml:id="formula_2">= - i i t Ì 1 i t (4)</formula><p>The intra-machine fairness means that: given all VMs {VM 1 , VM 2 , ..., VM n }, running on the same physical machine, we should satisfy the two conditions: (5)</p><p>To predict the interference factor when concurrent VMs are running within the same host, a VM is represented by a vector. The problem is transformed to: given multiple vectors, we estimate the overhead of interference on each vector. The vector includes the following items: • CPU: the CPU time, CPI, the number of L1 data cache misses per instruction, the number of L2 data cache misses per instruction, the number of DTLB misses per instruction.</p><p>• RAM: the average amount of occupied main memory, the working set size. • Disk: the number of I/O operations per second, the amount of data accessed per second, and the average length of the I/O queue per second.</p><p>We want to develop a prediction model:</p><formula xml:id="formula_3">→ (V , V , .... ,V ) I n 0 1 0 (<label>6</label></formula><formula xml:id="formula_4">)</formula><p>where V 0 , V 1 ,.…, and V n are vectors for VM 0 , VM 1 , …, VM n , respectively. I 0 is the interference factor to VM 0 . The idea is to estimate the interference factor for V 0 , with the super vector composed of</p><formula xml:id="formula_5">(V 0 , V 1 … V n ). Naturally, we can see: V 0 is different from V 1 … V n ; (V 0 , any permutation of V 1 … V n )</formula><p>will get the same results I 0 . To train our model, we consider the following definitions:</p><p>• (V 0 , nil, nil… nil) means we schedule the VM alone, and we can get the measurement t. • (V 0 , V 1 , nil… nil) means we schedule two VMs, and we can get the measurement t 0 ' and t 1 '. • (V 0 , V 1 , … V n ) means we schedule n VMs, and we can get the measurement t 0 ', t 1 ', .. , and t n '.</p><p>In machine learning, the accuracy of our model strongly depends on the training data set, and the correct execution of the different benchmarks which represent different workloads with different behaviors. Therefore, we need to expose the characteristics of interferences in the multiple shared layers in virtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Case Study on I/O Applications</head><p>Since the interference between I/O applications is higher than the one between CPU applications, in this paper we illustrate the effectiveness and accuracy of our model with I/O applications. Testing our prediction model, when diverse applications are running, is ongoing work in our group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Experimental setup</head><p>Our experiment is conducted on a physical node, equipped with two 4-core 2.33GHz Xeon processors, 8GB of memory and 1TB of disk, running RHEL5 with kernel 2.6.22, and is connected with 1Gbps Ethernet. All results described in this section are obtained using Xen version 3.4.2. All the virtual machines used in our experiments are configured with 1 VCPU pinned to its own core and 1GB of memory and 60GB of virtual disk. VMM schedulers are set to the defaults: Credit for the CPU scheduler and CFQ for the I/O disk scheduler. We perform our experiments by repeatedly executing the benchmarks. According to previous studies on analyzing and predicting the performance of different workloads of I/O intensive applications <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b26">28]</ref>, we use a set of training workloads, reflecting various types of real-world I/O workloads including sequential and random read and write applications as shown in Table <ref type="table" target="#tab_2">II</ref>. The workloads, shown in Table <ref type="table" target="#tab_2">II</ref>, are generated using Sysbench <ref type="bibr" target="#b27">[29]</ref>. We gather information about four different resource metrics related to CPU and disk I/O, which represent the items of the vector of our prediction system. These statistics are all gathered using vmstat <ref type="bibr" target="#b28">[30]</ref>, therefore, minimizing the effects of the monitoring system on our resource measurements. FFSB <ref type="bibr" target="#b29">[31]</ref> Multi threaded benchmarks that provide I/O operations, we configured FFSB with 128 threads and write operation of 5GB of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Predication Validation</head><p>To evaluate the accuracy of our model, we choose three different widely-used I/O applications in the cloud: sequential write, Postmark, and Flexible File System Benchmark (FFSB) <ref type="bibr" target="#b29">[31]</ref> (shown in Table <ref type="table" target="#tab_2">II</ref>). We use libsvm <ref type="bibr" target="#b11">[13]</ref> to evaluate the feasibility of our model in predicting the interference. Table <ref type="table" target="#tab_3">III</ref> shows the prediction accuracy of our model, for the three aforementioned test workloads, with other VMs that are running I/O workloads chosen from our trained workload.</p><p>We observe that, for the case of seq write which is one of the training model workloads, it is expected to get 97%-100% accuracy, but surprisingly, the accuracy varies between 94% and 97%, which can be explained due to the instability of the VMs performance under Xen <ref type="bibr" target="#b9">[11]</ref>. For the rest of applications our model can achieve a prediction accuracy of 87% on average, where the best is 93% and the worst is 82%. As shown in Table <ref type="table" target="#tab_3">III</ref>, our prediction model can achieve better accuracy when the interference score is relatively high, which can be explained due to the less noise caused by the interference amongst the different VCPUs of domains U and domain 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Discussion on Users Cost Fairness</head><p>To quantify the effectiveness of our prediction model under our pay-as-you-consume pricing scheme using effective resource consumption charging methods, we study the fairness using an example of two VMs running within the same host, each of them representing one user and running two different I/O applications, Postmark and sequential write. As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, our pay-as-you-consume pricing scheme can achieve a personal fairness of 3% in the case of the Postmark application and 2% in the case of the sequential write application which is very small compared to the payas-you-go scheme (28% and 50% for both aforementioned applications). Moreover, with the social fairness given by the fairness of the interference cost of the different VMs within the same physical node, as shown in Fig. <ref type="figure" target="#fig_4">3</ref>, we can achieve 98% while the pay-as-you-go model achieves 86%. We observe that both the personal and social fairness of our model is strongly proportional to the prediction accuracy. For example, the best personal fairness (the extra cost caused by interference) is achieved when the accuracy is 100% and thus the personal interference is zero.</p><p>One may expect that the new proposed model brings fairness to the cloud users, while it could lead to a loss in profit for the providers (i.e. the provider may not be able to recover even the cost of operating). However, previous work demonstrated that virtualization, featured with server consolidation, can significantly benefit the system providers, by achieving reduced server power consumption and close to optimal system throughput <ref type="bibr" target="#b9">[11]</ref>. In summary, cloud providers, using our pay-as-you-consume model, can provide users with stable and fair cost, in particularly personal fairness and social fairness, while gaining considerable positive profit because they have increased trust from users by having more consistent charging. In addition, provider can gain a competitive advantage through pay-as-youconsume pricing scheme in the market of multiple providers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. User's Cost in the Cloud</head><p>A number of studies <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref> have been dedicated to measure the cost of adopting the pay-as-you-go cloud in terms of monetary cost, performance, and availability. Some studies <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b32">34]</ref> have reported on the cost variations in the cloud. Our work quantifies the variations with price fairness and investigates the reason of this variance. Recent studies <ref type="bibr" target="#b33">[35]</ref> have demonstrated a case study of a consumer-centric resource accounting model to verify any discrepancies in consumer's bills. Yao et al. <ref type="bibr" target="#b34">[36]</ref> introduced an accountability service model to unambiguously identify the reason and the responsible party in case of faulty service. In contrast, this paper investigates the interplay between micro-economic issues and the system design and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interference in Shared Environment</head><p>There have been a lot of studies on performance interference in virtualized or shared servers. Boutcher et al. <ref type="bibr" target="#b5">[7]</ref> and Kesavan et al. <ref type="bibr" target="#b14">[16]</ref> have examined the impacts of the choice of disk I/O scheduler in both VMM and VMs on application performance. Despite our work being focused on the monetary cost, a key difference between our work and their work is that we are studying the impact of a provider's administration by selecting the VMM schedulers for I/O and CPU. Mei et al. <ref type="bibr" target="#b35">[37]</ref> have measured the performance interference among two VMs running network I/O workloads that are either CPU bound or network bound and elaborated the impacts of co-locating applications in a virtualized cloud in terms of throughput and resource sharing effectiveness. Our work focuses on fairness in the monetary cost. Koh et al. <ref type="bibr" target="#b6">[8]</ref> have elaborated the performance interference effects between two virtual machines by looking at the system-level workload characteristics. They identified clusters of applications that generate certain types of performance interference and developed mathematical models to predict the performance of a new application from its workload characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>With the pay-as-you-go charging, the public cloud has become an economic market for both cloud users and providers. However, virtualization with server consolidation can cause performance interference, leading to nonguaranteed quality of service. In this study, we investigate the pricing fairness on the pay-as-you-go charging, and introduce the pay-as-you-consume model to resolve the unfairness in the current pay-as-you-go pricing scheme. While the pay-as-you-consume model seemingly reduces the cloud providers' profit, it urges providers to improve their system design and optimization to provide good services and to gain competitive advantages. We have demonstrated a case study on I/O applications for validating the accuracy of our model; interestingly our predication model can achieve up to 90% accuracy regardless the VM consolidations or the I/O workloads. This paper is intended as a call for action, and its goal is to motivate further research on economic concepts in the cloud. We hope the findings in this paper will foster new techniques as well as new pricing schemes in the cloud to really reflect the spirit of economic markets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, the interference costs of VM Postmark when it shares the resources with two VMs and three VMs are nearly 45% and 66%, which indicates that the total VM' cost is two and three times higher than when it uniquely runs within the physical machine. The interference cost can be explained due to I/O congestion which adversely affects the I/O throughput of each VM as shown in Fig.1-b. Fig.1-b explains the previous result as it shows the cumulative distribution function (CDF) of I/O throughput in the virtual machine for the three aforementioned scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. VMs consolidation impact on interference for both PostMark (PM) and BlackScholes (BS) benchmarks: (a) the cost of the same VM with different VM consolidation and application type. (b) CDFs of the I/O size per second in the virtual machine for PostMark benchmark with different VM consolidation (c) Interference cost for each VM and social fairness among all VMs within the physical host.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Applications types impacts on interference: (a) Extra cost by interference, (b) Interference cost for each VM and social fairness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An example of the effectiveness of our prediction model associated with the Pay-As-You-Consume pricing scheme, when running two VMs, each represents different users and runs different I/O applications (Postmark and Sequential write). In this example we achieved accuracy of 86% and 97% respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>PERSONAL AND SOCIAL FAIRNESS WITH DIFFERENT PAIR SCHEDULER (CPU: CREDIT AND SEDF, DISK I/O: CFQ (CF), ANTICIPATORY (AS), NOOP (NP), AND DEADLINE (DL)</figDesc><table><row><cell>BS,BS</cell><cell></cell><cell>PM,PM</cell><cell></cell><cell>BS,PM</cell><cell></cell></row><row><cell>Credit</cell><cell>SEDF</cell><cell>Credit</cell><cell>SEDF</cell><cell>Credit</cell><cell>SEDF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>CF AS NP DL CF AS NP DL CF AS NP DL CF AS NP DL CF AS NP DL CF AS NP DL</head><label></label><figDesc>Personal Fairness. The interference score varies by 35%, 14%, and 124% with different CPU and I/O schedulers' combination in the three scenarios: two CPU applications, two I/O applications and one CPU application concurrently running with one I/O application in the same host, respectively.</figDesc><table><row><cell>EVMTime</cell><cell>559 531 532 536 543 557 550 562 405 366 450 405 377 400 460 387 559 531 532 536 543 557 550 562</cell></row><row><cell>TimeVM1</cell><cell>587 559 570 567 575 583 558 579 738 724 1112 739 734 646 1061 898 687 586 533 555 585 550 589 558</cell></row><row><cell>TimeVM2</cell><cell>584 594 590 584 549 566 542 577 786 787 1422 909 804 780 1424 825 454 418 525 476 458 514 600 437</cell></row><row><cell cols="2">Cost1,2(10 -2 $) 3.25 3.2 3.22 3.2 3.12 3.19 3.06 3.21 4.23 4.2 7.04 4.58 4.27 3.96 6.9 4.79 3.17 2.79 2.94 2.86 2.9 2.95 3.3 2.76</cell></row><row><cell>I1</cell><cell>0.05 0.05 0.07 0.05 0.06 0.04 0.02 0.03 0.45 0.49 0.6 0.45 0.49 0.38 0.57 0.57 0.19 0.09 0.00 0.03 0.07 -0.01 0.07 -0.01</cell></row><row><cell>I2</cell><cell>0.08 0.11 0.1 0.08 0.01 0.02 -0.01 0.03 0.48 0.53 0.68 0.55 0.53 0.49 0.68 0.53 0.06 0.12 0.14 0.15 0.18 0.22 0.23 0.11</cell></row><row><cell>Social Fair</cell><cell>95.3 88.6 96.5 96 67.2 83 78.9 99.6 99.9 99.8 99.5 99 99.8 98.5 99.2 99.9 79.5 98.1 50.3 71.3 84.7 55.4 76.6 55.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell>.</cell><cell>TRAINING AND TESTING WORKLOADS</cell></row><row><cell></cell><cell>Workload</cell><cell>Description</cell></row><row><cell>Training</cell><cell>Sequential write/Read Random</cell><cell>Writing/Reading sequentially different number of files varies from 1-512 files with differen size: 128KB-1GB Writing/Reading randomly differen number of files varies from 1-5 12 files with differen size:</cell></row><row><cell></cell><cell>write/Read</cell><cell>128KB-1GB with different threads number 6-</cell></row><row><cell></cell><cell></cell><cell>512 threads</cell></row><row><cell></cell><cell>Sequential write (Seq Wr)</cell><cell>Writing sequentially 2 GB of data</cell></row><row><cell>Testing</cell><cell>PostMark (PM)</cell><cell>Write 5 GB (1000 files with 5000 KB each)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="2">PREDICATION VALIDATION</cell></row><row><cell></cell><cell>Real I0</cell><cell>Predicted I0</cell><cell>Accuracy</cell></row><row><cell></cell><cell>1.35</cell><cell>1.27</cell><cell>94%</cell></row><row><cell></cell><cell>0.24</cell><cell>0.28</cell><cell>83%</cell></row><row><cell></cell><cell>2.9</cell><cell>2.81</cell><cell>97%</cell></row><row><cell></cell><cell>0.72</cell><cell>0.67</cell><cell>93%</cell></row><row><cell></cell><cell>2.48</cell><cell>2.2</cell><cell>88.7%</cell></row></table><note><p>(Seq Wr, Seq Wr) (FFSB, Seq Wr) (Seq Wr, Seq Wr, Seq Wr) (PM, , Seq Wr, Seq Wr) (FFSB, Ran Wr, Ran Wr)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-4462-5/11 $26.00 © 2011 IEEE DOI 10.1109/SCC.2011.38</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by National 973 Key Basic Research Program under grant No.2007CB310900, the International Cooperation Program funded by Technology Bureau of Wuhan under grant No.201171034311, and a start-up grant No.M58020024 from Nanyang Technological University. We would also like to thank Lidong Zhou from MSR Asia for his insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Q-clouds: managing performance interference effects for QoS-aware clouds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nathuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghaffarkhah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th European conference on Computer systems (EuroSys&apos;10)</title>
		<meeting>of the 5th European conference on Computer systems (EuroSys&apos;10)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">April 13-16, 2010</date>
			<biblScope unit="page" from="237" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comet: Batched Stream Processing for Data Intensive Distributed Computing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Cloud Computing (SOCC&apos;10)</title>
		<meeting>ACM Symposium on Cloud Computing (SOCC&apos;10)<address><addrLine>Indiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">June 10-11, 2010</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LEEN: Locality/Fairness-aware Key Partitioning for MapReduce in the Cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2010 IEEE 2nd International Conference on Cloud Computing Technology and Science (CloudCom&apos;10)</title>
		<meeting>of the 2010 IEEE 2nd International Conference on Cloud Computing Technology and Science (CloudCom&apos;10)<address><addrLine>Indiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12-03">Nov. 30-Dec. 03, 2010</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of the Three CPU Schedulers in Xen</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduling I/O in Virtual Machine Monitors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4 th International Conference on Virtual Execution Environments (VEE&apos;08)</title>
		<meeting>of the 4 th International Conference on Virtual Execution Environments (VEE&apos;08)<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05-07">Mar. 5-7, 2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Does Virtualization Make Disk Scheduling Passe?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boutcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Analysis of Performance Interference Effects in Virtual Environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knauerhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Symposium on In Performance Analysis of Systems &amp; Software (ISPASS&apos;07)</title>
		<meeting>of the IEEE International Symposium on In Performance Analysis of Systems &amp; Software (ISPASS&apos;07)<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">Apr. 25-27, 2007</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Price is Wrong: Understanding What Makes a Price Seem Fair and the True Cost of Unfair Pricing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maxwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Pricing of Options and Corporate Liabilities</title>
		<author>
			<persName><forename type="first">F</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Scholes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Political Economy</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="637" to="654" />
			<date type="published" when="1973-06">June 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed Systems Meet Economics: Pricing in the Cloud</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2 nd USENIX Workshop on Hot Topics in Cloud computing (HotCloud&apos;10)</title>
		<meeting>of the 2 nd USENIX Workshop on Hot Topics in Cloud computing (HotCloud&apos;10)<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06-22">June 22, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-learning Disk Scheduling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Bhargava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="50" to="65" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">LIBSVM : A Library for Support Vector Machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Software available at</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xen</forename><surname>Hypervisor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Homepage</forename></persName>
		</author>
		<ptr target="http://www.xen.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xen and the Art of Virtualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19 th ACM Symposium on Operating Systems Principles (SOSP&apos;03)</title>
		<meeting>of the 19 th ACM Symposium on Operating Systems Principles (SOSP&apos;03)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">Oct. 19-22, 2003</date>
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On disk I/O Scheduling in Virtual Machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kesavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2 nd Workshop on I/O Virtualization</title>
		<meeting>of the 2 nd Workshop on I/O Virtualization<address><addrLine>Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-03-13">Mar. 13, 2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enforing Performance Isolation Across Virtual Machines in Xen</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM/IFIP/USENIX 7 th International Middleware Conference (Middleware&apos;06)</title>
		<meeting>of the ACM/IFIP/USENIX 7 th International Middleware Conference (Middleware&apos;06)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-12-01">Nov. 27-Dec. 1, 2006</date>
			<biblScope unit="page" from="342" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TwinDrivers: semiautomatic derivation of fast and safe hypervisor network drivers from guest OS drivers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Of the 14 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;09)</title>
		<meeting>Of the 14 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;09)<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07-11">Mar. 7-11, 2009</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache-oblivious query processing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd International Conference on Innovative Data Systems Research (CIDR&apos;07)</title>
		<meeting>of the 3rd International Conference on Innovative Data Systems Research (CIDR&apos;07)<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07-10">Jan. 7-10, 2007</date>
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards Modeling and Analysis of Consolidated CMP Servers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aparrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring CPU overhead for I/O Processing in the Xen Virtual Machine Monitor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Annual Technical Conference (USENIX&apos;05)</title>
		<meeting>of the USENIX Annual Technical Conference (USENIX&apos;05)<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">April 10-15, 2005</date>
			<biblScope unit="page" from="24" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://www.microsoft.com/azure/casestudies.mspx" />
		<title level="m">Windows Azure Case Studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://aws.amazon.com/solutions/case-studies/" />
		<title level="m">Amazon Case Studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Postmark: a New File System Benchmark</title>
		<author>
			<persName><forename type="first">J</forename><surname>Katcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Appliance</title>
		<imprint>
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Parsec Benchmark Suite: Characterization and Architectural Implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17 th International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;08)</title>
		<meeting>of the 17 th International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;08)<address><addrLine>Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">Oct. 25-29, 2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Quantitative Measure of Fairness and Discrimination for Resource Allocation in Shared Computer Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hawe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
		<respStmt>
			<orgName>Digital Equipment Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Credit</forename><surname>Scheduler -Xen</surname></persName>
		</author>
		<author>
			<persName><surname>Wiki</surname></persName>
		</author>
		<ptr target="http://wiki.xensource.com/xenwiki/CreditScheduler" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Workload Dependent Performance Evaluation of the Linux2.6 I/O Schedulers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Heger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Linux Symposium</title>
		<meeting>of the Linux Symposium<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-21">2004. July 21-24, 2004</date>
			<biblScope unit="page" from="425" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://sysbench.sourceforge.net/" />
		<title level="m">System performance benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="http://linux.die.net/man/8/vmstat" />
		<title level="m">Virtual memory statistic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://sourceforge.net/projects/ffsb/" />
		<title level="m">Flexible File System Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Amazon S3 for Science Grids: a Viable Solution?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Palankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iamnitchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ripeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garfinkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2008 International Workshop on Data-aware Distributed Computing (DADC&apos;08)</title>
		<meeting>of the 2008 International Workshop on Data-aware Distributed Computing (DADC&apos;08)<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008-06-24">June 24, 2008</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Cost of Doing Science on the Cloud: the Montage Example</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2008 ACM/IEEE conference on Supercomputing (SC&apos;08)</title>
		<meeting>of the 2008 ACM/IEEE conference on Supercomputing (SC&apos;08)<address><addrLine>Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">Nov. 15-21, 2008</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An Evaluation of Amazon&apos;s Grid Computing Services: Ec2, S3 and SQS</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Garfinkel</surname></persName>
		</author>
		<idno>TR-08-07</idno>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Case for Consumer Centric Resource Accounting Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mihoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Molinajimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE 2010 International Conference on Cloud Computing (Cloud&apos;10)</title>
		<meeting>of the IEEE 2010 International Conference on Cloud Computing (Cloud&apos;10)<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2010">July 5-10, 2010</date>
			<biblScope unit="page" from="506" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accountability as a Service for the Cloud</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Services Computing (SCC&apos;10)</title>
		<meeting>of the IEEE International Conference on Services Computing (SCC&apos;10)<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">July 5-10, 2010</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance Measurements and Analysis of Network I/O Applications in Virtualized Cloud</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivathanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE 2010 International Conference on Cloud Computing (Cloud&apos;10)</title>
		<meeting>of the IEEE 2010 International Conference on Cloud Computing (Cloud&apos;10)<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2010">July 5-10, 2010</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
