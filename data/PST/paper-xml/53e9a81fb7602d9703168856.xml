<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TPMSVM: A novel twin parametric-margin support vector machine for pattern recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-04-09">9 April 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xinjun</forename><surname>Peng</surname></persName>
							<email>xjpeng@shnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Shanghai Normal University</orgName>
								<address>
									<postCode>200234</postCode>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Scientific Computing Key Laboratory of Shanghai Universities</orgName>
								<address>
									<postCode>200234</postCode>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Shanghai Normal University</orgName>
								<address>
									<postCode>200234</postCode>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TPMSVM: A novel twin parametric-margin support vector machine for pattern recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-04-09">9 April 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">850C797BB02FD0627F0E986476CE7E1C</idno>
					<idno type="DOI">10.1016/j.patcog.2011.03.031</idno>
					<note type="submission">Received 9 April 2010 Received in revised form 15 February 2011 Accepted 26 March 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Support vector machine Twin support vector machine Nonparallel hyperplanes Heteroscedastic noise structure Parametric-margin model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel twin parametric-margin support vector machine (TPMSVM) for classification is proposed in this paper. This TPMSVM, in the spirit of the twin support vector machine (TWSVM), determines indirectly the separating hyperplane through a pair of nonparallel parametric-margin hyperplanes solved by two smaller sized support vector machine (SVM)-type problems. Similar to the parametric-margin n-support vector machine (par-n-SVM), this TPMSVM is suitable for many cases, especially when the data has heteroscedastic error structure, that is, the noise strongly depends on the input value. But there is an advantage in the learning speed compared with the par-n-SVM. The experimental results on several artificial and benchmark datasets indicate that the TPMSVM not only obtains fast learning speed, but also shows good generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The support vector machine (SVM) is an excellent kernelbased tool for binary data classification and regression <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. This learning strategy introduced by Vapnik and co-worker <ref type="bibr" target="#b0">[1]</ref> is a principled and very powerful method in machine learning algorithms. Within a few years after its introduction the SVM has already outperformed most other systems in a wide variety of applications. These include a wide spectrum of research areas, ranging from pattern recognition <ref type="bibr" target="#b5">[6]</ref>, text categorization <ref type="bibr" target="#b6">[7]</ref>, biomedicine <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, brain-computer interface <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, and financial regression <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, etc.</p><p>One of the main challenges in the classical SVM is the large computational complexity of quadratic programming problem (QPP). In addition, the performance of a trained SVM classifier also depends on the optimal parameter set which is usually found by cross-validation on a training set. The long training time of QPP not only causes the classical SVM to take a long time to train on a large database, but also prevents it from locating the optimal parameter set from a very fine grid of parameters over a large span. To reduce the learning complexity of the classical SVM, various algorithms of the classical SVM have been reported by researchers with comparable classifications ability. One of them is to obtain low-rank approximations on the kernel matrix, by using the greedy approximation <ref type="bibr" target="#b16">[17]</ref>, sampling <ref type="bibr" target="#b17">[18]</ref> or matrix decompositions <ref type="bibr" target="#b18">[19]</ref>. However, the resulting rank of the kernel matrix may still be too high to be handled efficiently. The second kind of approaches to improve the speed are decomposition methods, including the Chunking algorithm <ref type="bibr" target="#b19">[20]</ref>, decomposition method <ref type="bibr" target="#b20">[21]</ref>, sequential minimal optimization (SMO) <ref type="bibr" target="#b21">[22]</ref>, and SVMlight <ref type="bibr" target="#b22">[23]</ref>. However, they need to optimize the entire set of non-zero Lagrangian multipliers, and the generated kernel matrix may still be too large to adapt to memory. The third kind of approaches are to avoid the QPP, such as the core vector machine (CVM) <ref type="bibr" target="#b23">[24]</ref>, ball vector machine (BVM) <ref type="bibr" target="#b24">[25]</ref>, and Lagrangian SVM (LSVM) <ref type="bibr" target="#b25">[26]</ref>. However, it still requires a large matrix for nonlinear kernels. Another kind of approaches are scaling down the training data before the SVM training process. Pavlov et al. <ref type="bibr" target="#b26">[27]</ref> and Collobert et al. <ref type="bibr" target="#b27">[28]</ref> used boosting and a neural-network-based ''gater'' to combine small SVMs. Lee and Mangasarian <ref type="bibr" target="#b28">[29]</ref> proposed the reduced SVM (RSVM), which uses a random subset of the kernel matrix.</p><p>The classical SVM and its extensions discriminate a data point by determining in which half space it lies. Recently, Jayadeva et al. <ref type="bibr" target="#b29">[30]</ref> proposed a twin support vector machine (TWSVM) classifier for binary data classification. In the spirit of the generalized eigenvalue proximal support vector machine (GEPSVM) <ref type="bibr" target="#b30">[31]</ref>, the TWSVM aims at generating two nonparallel planes such that each plane is closer to one of the two classes and is at least one far from the other class. the TWSVM solves a pair of smaller sized QPPs, instead of solving large one as in the classical SVM, makes the learning speed of the TWSVM be approximately 4 times faster than that of the classical SVM. Some extensions to the TWSVM include the least squares version of TWSVM (LS-TWSVM) <ref type="bibr" target="#b31">[32]</ref>, smooth TWSVM <ref type="bibr" target="#b32">[33]</ref>, nonparallel-plane proximal classifier (NPPC) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, and twin support vector regression (TSVR) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Generally, the classical SVM and its extensions assume that the noise level on training data is uniform throughout the domain, or at least, its functional dependency is known beforehand <ref type="bibr" target="#b37">[38]</ref>. The assumption of a uniform noise model, however, is not always satisfied. For instance, for the heteroscedastic noise structure, the amount of noise depends on location. Recently, Hao <ref type="bibr" target="#b38">[39]</ref> aimed at this shortcoming appeared in the classical SVM, and proposed a novel SVM model, called the parametric-margin n-support vector machine (par-n-SVM), based on the n-support vector machine (n-SVM) <ref type="bibr" target="#b37">[38]</ref>. This par-n-SVM considers to find a parametricmargin model of arbitrary shape. This can be useful in many cases, especially when the data has heteroscedastic error structure, i.e., the noise strongly depends on the input value. However, the learning speed of par-n-SVM is still as slow as the classical SVM or n-SVM.</p><p>In this paper, we present a novel parametric-margin SVM model, termed the twin parametric-margin SVM (TPMSVM), in the spirit of the TWSVM <ref type="bibr" target="#b29">[30]</ref>. The proposed TPMSVM aims at generating two nonparallel hyperplanes such that each one determines the positive or negative parametric-margin hyperplane of the separating hyperplane. For this aim, similar to the TWSVM, the TPMSVM also solves two smaller sized QPPs instead of solving large one as in the classical SVM or par-n-SVM. The formulation of TPMSVM is totally different from that of parn-SVM in some respects. First, the TPMSVM solves a pair of smaller sized QPPs, whereas, the par-n-SVM only solves single large QPP, which makes the learning speed of TPMSVM be much faster than the par-n-SVM. Second, the par-n-SVM directly finds the separating hyperplane and parametric-margin hyperplane, while the TPMSVM indirectly determines the separating hyperplane through the positive and negative parametric-margin hyperplanes. In short, the proposed TPMSVM successfully combines the merits of TWSVM, i.e., the fast learning speed, and par-n-SVM, i.e., the flexible parametric-margin. Computational comparisons on the par-n-SVM, TWSVM and SVM in terms of generalization performance, number of support vectors (SVs) and training time are made on several artificial and benchmark datasets, indicating the TPMSVM is not only fast, but also shows comparable generalization.</p><p>The paper is organized as follows: Section 2 briefly dwells on the classical SVM, TWSVM and par-n-SVM, and also introduces the notations used in the rest of paper. Section 3 introduces the linear and nonlinear TPMSVM, at the same time, it also shows the theoretical analysis for the TPMSVM. Section 4 shows some comparisons of our TPMSVM with the par-n-SVM and TWSVM. Section 5 lists the experimental results of the TPMSVM on some artificial and benchmark datasets. Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Let the samples to be classified be denoted by a set of l row vectors X i , i ¼ 1,2, . . . ,l in the m-dimensional real space R m , and y i A fÀ1, þ1g denotes the class to which the ith sample belongs. Without loss of generality, we assume that the matrix X with size of l Â m represents the all training data points, the diagonal matrix Y A R lÂl represents the labels of X, and the matrices X 7 , with sizes of l 7 Â m, represent the data points belonging to classes þ and À, respectively, where l ¼ l þ þl À . In the sequel, we use X to represent the index set of X, and use X 7 to represent the indices of positive and negative samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Support vector machine (SVM)</head><p>As a state-of-the-art of machine learning algorithm, the classical SVM is based on guaranteed risk bounds of statistical learning theory which is known as the structural risk minimization principle. An intuitive geometric interpretation for the linear SVM is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. Consider a classification problem with the data matrix X and label matrix Y, the linear SVM finds the best separating (maximal margin) hyperplane Hðw,bÞ between two classes of samples:</p><p>Hðw,bÞ : w T x þb ¼ 0, where w A R m and b A R. This hyperplane is obtained by solving the following QPP:</p><formula xml:id="formula_0">min 1 2 JwJ 2 2 þce T n s:t: YðXwþ ebÞ Z eÀn, nZ0,<label>ð1Þ</label></formula><p>where J Á J 2 denotes the L 2 -norm, n is the soft margin error vector of X, e is a vector of ones of l dimensions, and the parameter c 40 is the regularization factor that balances the importance between the maximization of the margin width (i.e., the minimization of</p><formula xml:id="formula_1">JwJ 2 2 =2</formula><p>) and the minimization of the training error.</p><p>The dual QPP of the problem <ref type="bibr" target="#b0">(1)</ref> is</p><formula xml:id="formula_2">max e T aÀ 1 2 a T YXX T Ya s:t: e T Ya ¼ 0, 0r arce,<label>ð2Þ</label></formula><p>where a ¼ ða 1 ,a 2 , . . . ,a l Þ T is the Lagrangian vector. After solving this dual QPP, we obtain the following decision function:</p><formula xml:id="formula_3">f ðxÞ ¼ sign ðw T x þ bÞ ¼ sign ða T YXx þbÞ:</formula><p>The complexity of solving the dual problem (2) is between Oðl 2 Þ and Oðl 3 Þ.</p><p>For the nonlinear case, a nonlinear mapping jðÁÞ is introduced in the SVM to map X into a high dimensional feature space H. Under the Mercer theorem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40]</ref>, it is possible to use a kernel kðu,vÞ to represent the inner product in H, i.e., kðu,vÞ ¼ jðuÞ T jðvÞ, such as the Gaussian kernel kðu,vÞ ¼ expfÀgJuÀvJ 2 2 g with parameter g40. Similarly, the decision function for the nonlinear case is of the form f ðxÞ ¼ sign ða T YKðX,xÞþbÞ:</p><p>Generally, the proportion of number of SVs is very small among the training samples, which means the SVM is sparse and has fast test speed. Traditionally, the classical SVM is trained by using decomposition techniques such as the SVMlight <ref type="bibr" target="#b22">[23]</ref> and SMO <ref type="bibr" target="#b21">[22]</ref>, which solve iteratively the dual problem by optimizing a small subset of the variables. Recently, some researchers showed that the SVM can be solved efficiently in primal space. For instance, Lee and Mangasarian <ref type="bibr" target="#b40">[41]</ref> used an approximate function rðx,ZÞ ¼ x þð1=ZÞlnð1 þ expðÀZxÞÞ with Z40 to smooth the loss function l þ ðxÞ ¼ maxðx,0Þ, which leads to a twice differentiable unconstrained optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Twin support vector machine (TWSVM)</head><p>The TWSVM <ref type="bibr" target="#b29">[30]</ref> uses two nonparallel hyperplanes instead of a single hyperplane as in the case of the classical SVM to classify samples. The two nonparallel hyperplanes are obtained by solving two smaller sized QPPs compared with a single large QPP solved by the classical SVM. An intuitive geometric interpretation for the TWSVM is shown in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>For the linear case, the TWSVM determines two nonparallel hyperplanes:</p><formula xml:id="formula_4">f 1 ðxÞ ¼ x T w þ þ b þ ¼ 0 and f 2 ðxÞ ¼ x T w À þ b À ¼ 0:<label>ð3Þ</label></formula><p>The idea in the linear TWSVM is to solve the following two QPPs with objective functions corresponding to one of the two classes and constraints corresponding to the other class:</p><formula xml:id="formula_5">min 1 2 JX þ w þ þe þ b þ J 2 2 þc 1 e T À n s:t: ÀðX À w þ þe À b þ Þ Z e À Àn, nZ0,<label>ð4Þ</label></formula><formula xml:id="formula_6">min 1 2 JX À w À þ e À b À J 2 2 þc 2 e T þ g s:t: X þ w À þ e þ b À Ze þ À g, gZ0,<label>ð5Þ</label></formula><p>where c 1 , c 2 4 0 are the pre-specified penalty factors, n, g are the slack vectors, and e 7 are the vectors of ones of l 7 dimensions, respectively. By introducing the Lagrangian vectors, the dual QPPs of (4) and ( <ref type="formula" target="#formula_6">5</ref>) can be represented as followings 1 :</p><formula xml:id="formula_7">max e T À aÀ 1 2 a T GðH T HÞ À1 G T a s:t: 0 r arc 1 e À , max e T þ bÀ 1 2 b T HðG T GÞ À1 H T b s:t: 0 r brc 2 e þ ,</formula><p>where the matrices H and G are defined as</p><formula xml:id="formula_8">H ¼ ½X þ e þ ,G ¼ ½X À e À :</formula><p>The augmented vectors u 7 ¼ ½w T  7 ,b 7 T are determined by</p><formula xml:id="formula_9">u þ ¼ ÀðH T HÞ À1 G T a, u À ¼ ðG T GÞ À1 H T b,<label>ð6Þ</label></formula><p>which defines the two nonparallel hyperplanes (3). In the above discussion, the matrices H T H and G T G are matrices of size ðmþ 1Þ Â ðmþ 1Þ, where, in general, m is much smaller in comparison to the numbers of positive and negative samples.</p><p>A new data point x A R m is then assigned to the class þ or À , depending on which of the two hyperplanes given by (3) it lies closest to. Thus For the nonlinear case, the TWSVM considers the following two nonparallel kernel-generated hyperplanes in the feature space H:</p><formula xml:id="formula_10">f ðxÞ ¼ argmin þ ,À</formula><formula xml:id="formula_11">Kðx,XÞw þ þb þ ¼ 0,Kðx,XÞw À þb À ¼ 0:<label>ð7Þ</label></formula><p>This kernel TWSVM has the similar pair of primal QPPs. Here we omit the formulations of QPPs of the kernel TWSVM. After optimizing the corresponding dual QPPs, the augmented vectors</p><formula xml:id="formula_12">v 7 ¼ ½w T 7 ,b 7 T are determined by v þ ¼ ÀðS T SÞ À1 R T a, v À ¼ ðR T RÞ À1 S T b,<label>ð8Þ</label></formula><p>which gives the two nonparallel hyperplanes <ref type="bibr" target="#b6">(7)</ref>, where the matrices S and R are defined as</p><formula xml:id="formula_13">S ¼ ½KðX þ ,XÞe þ , R ¼ ½KðX À ,XÞe À : 2.3. Parametric-margin n-support vector machine (par-n-SVM)</formula><p>The par-n-SVM considers a parametric-margin model gðxÞ ¼ z T jðxÞþd instead of the functional margin in the n-SVM. Specifically, the hyperplane f ðxÞ ¼ w T jðxÞþb in the par-n-SVM separates the data if and only if</p><formula xml:id="formula_14">jðX i Þw þb Z jðX i Þz þ d for i A X þ , jðX i Þw þb r ÀjðX i ÞzÀd for i A X À :<label>ð9Þ</label></formula><p>An intuitive geometric interpretation for the par-n-SVM is shown in Fig. <ref type="figure">3</ref>.</p><p>To find f ðxÞ and gðxÞ, the par-n-SVM considers the following constrained optimization problem: 1 If the matrix H T H or G T G is ill-conditioned, we can add a regularization term eE,e40, where E is an identity matrix of appropriate dimension. However, in the following, we shall continue to use H T H or G T G with the understanding that. This regularization term is equivalent to add the terms ðe=2ÞJw 7 J 2 2 in the primal QPPs of TWSVM. Actually, this regularization term can improve the smoothness of the hyperplanes in the feature space by adjusting the value of e. In the experiments, we fix the value of e as 0.1.</p><formula xml:id="formula_15">min 1 2 JwJ 2 2 þ c n 1 2 JzJ 2 2 þ d þ 1 l e T n s:t: YðjðXÞw þ beÞ Z ðjðXÞz þ deÞÀn, d Z0, nZ0,<label>ð10Þ</label></formula><p>where n is the slack vector that measures the amount of violation of the constraints (9), and c and n are the positive constants that set the penalty weights.</p><p>By introducing the Lagrangian function for <ref type="bibr" target="#b9">(10)</ref>, we obtain the dual QPP for the par-n-SVM, which is</p><formula xml:id="formula_16">max À 1 2 a T YKðX,XÞYa þ 1 2cn</formula><p>a T KðX,XÞa s:t: e T Ya ¼ 0, e T aZcn,</p><formula xml:id="formula_17">0 ra r c l e:</formula><p>Solving the above dual QPP, we obtain the Lagrangian vector a, which gives the weight vectors w and z as the linear combinations of jðXÞ:</p><formula xml:id="formula_18">w ¼ jðXÞ T Ya, z ¼ 1 cn</formula><p>jðXÞ T a:</p><p>While the bias terms b and d are determined by exploiting the Karush-Kuhn-Tucker (KKT) conditions as followings:</p><formula xml:id="formula_19">b ¼ À 1 2 ½jðX i Þw þ jðX j ÞwÀjðX i Þz þjðX j Þz, d ¼ 1 2 ½jðX i ÞwÀjðX j ÞwÀjðX i ÞzÀjðX j Þz,</formula><p>where the Lagrangian variables of X i and X j satisfy a i ,a j A ð0,c=lÞ,</p><formula xml:id="formula_20">i A X þ ,j A X À .</formula><p>Therefore, the separating hyperplane f ðxÞ and the parametric-margin function gðxÞ of the resulting par-n-SVM can be shown to take the forms</p><formula xml:id="formula_21">f ðxÞ ¼ a T YKðX,xÞþb, gðxÞ ¼ 1 cn a T KðX,xÞþd:</formula><p>In the following, we call the two nonparallel hyperplanes f ðxÞ 7gðxÞ ¼ 0 as the parametric-margin hyperplanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Twin parametric-margin support vector machine (TPMSVM)</head><p>In this Section, we introduce an efficient learning approach to the par-n-SVM which we have termed as the twin parametricmargin support vector machine (TPMSVM). As mentioned earlier, the TPMSVM is similar to the TWSVM, as it also derives a pair of nonparallel planes around the data points through two QPPs.</p><p>An intuitive geometric interpretation for the TPMSVM is shown in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear TPMSVM</head><p>For the linear case, the TPMSVM finds two hyperplanes in R m :</p><formula xml:id="formula_22">f 1 ðxÞ ¼ w T þ x þ b þ ¼ 0 and f 2 ðxÞ ¼ w T À x þ b À ¼ 0, each</formula><p>one determines the one of parametric-margin hyperplanes. In this paper, we call f 1 ðxÞ and f 2 ðxÞ as the positive and negative parametric-margin hyperplanes, respectively. Specifically, given the training data points ðX,YÞ, f 1 ðxÞ determines the positive parametric-margin hyperplane, and f 2 ðxÞ determines the negative parametric-margin hyperplane. By incorporating the positive and negative parametric-margin hyperplanes, this TPMSVM separates the data if and only if:</p><formula xml:id="formula_23">X i w þ þ b þ Z0 for i A X þ , X i w À þ b À r 0 for i A X À :<label>ð11Þ</label></formula><p>Compared ( <ref type="formula" target="#formula_23">11</ref>) and ( <ref type="formula" target="#formula_14">9</ref>), it can be found that the parametricmargin hyperplanes f 1 ðxÞ ¼ 0 and f 2 ðxÞ ¼ 0 in this TPMSVM are equivalent to f ðxÞ 7gðxÞ ¼ 0 in the par-n-SVM.</p><p>In order to obtain the positive and negative parametric-margin hyperplanes, we consider the following pair of constrained optimization problems:</p><formula xml:id="formula_24">min 1 2 Jw þ J 2 2 þ n 1 l À e T À ðX À w þ þ e À b þ Þþ c 1 l þ e T þ n s:t: X þ w þ þ b þ e þ Z 0Àn, nZ0,<label>ð12Þ</label></formula><formula xml:id="formula_25">and min 1 2 Jw À J 2 2 À n 2 l þ e T þ ðX þ w À þe þ b À Þþ c 2 l À e T À g s:t: X À w À þb À e À r 0 þg, gZ0,<label>ð13Þ</label></formula><p>where c 1 ,c 2 4 0, n 1 ,n 2 4 0 are the regularization parameters which determine the penalty weights, and n,g are the slack vectors.</p><p>Before optimizing the above pair of constrained optimization problems, we now consider the illustrations of the optimization problem <ref type="bibr" target="#b11">(12)</ref>. The first term in the objective function of ( <ref type="formula" target="#formula_24">12</ref>) is to control the model complexity for finding the positive parametricmargin hyperplane. The second term in the objective function of (12) is to minimize the sum of projection values of negative training points on f 1 ðxÞ with parameter n 1 . Optimizing this term leads the negative training points to be as possible as far from the positive parametric-margin hyperplane. The constraints require the projection values of positive training samples on the positive parametric-margin hyperplane be not less than zeros. Otherwise, a slack vector nZ0 is introduced to measure the error. The third term of the objective function minimizes the sum of error variables, which attempts to over-fit the positive training points. We have the similar explanations for the problem <ref type="bibr" target="#b12">(13)</ref>.</p><p>We now consider to optimize the primal QPPs ( <ref type="formula" target="#formula_24">12</ref>) and <ref type="bibr" target="#b12">(13)</ref>. By introducing the Lagrangian function for the problem <ref type="bibr" target="#b11">(12)</ref>, which is</p><formula xml:id="formula_26">Lðw þ ,b þ ,n,a,cÞ ¼ 1 2 Jw þ J 2 2 þ n 1 l À e T À ðX À w þ þ e À b þ Þþ c 1 l þ e T þ n Àa T ðX þ w þ þ b þ e þ þ nÞÀc T n,<label>ð14Þ</label></formula><formula xml:id="formula_27">where a ¼ ða 1 ,a 2 , . . . ,a l þ Þ T and c ¼ ðg 1 ,g 2 , . . . ,g l þ Þ</formula><p>T are the Lagrangian multiplier vectors, then the KKT necessary and sufficient optimality conditions for the problem (12) are given by</p><formula xml:id="formula_28">@L @w þ ¼ w þ þ n 1 l À X T À e À ÀX T þ a ¼ 0 ) w þ ¼ X T þ aÀ n 1 l À X T À e À ,<label>ð15Þ</label></formula><formula xml:id="formula_29">@L @b þ ¼ n 1 l À e T À e À Àe T þ a ¼ 0 ) e T þ a ¼ n 1 ,<label>ð16Þ</label></formula><formula xml:id="formula_30">@L @n ¼ c 1 l þ e þ ÀaÀc ¼ 0, X þ w þ þ b þ e þ ZÀn, nZ0, a T ðX þ w þ þ b þ e þ þ nÞ ¼ 0, aZ0, c T n ¼ 0, cZ0:</formula><p>Since a,cZ0, we have</p><formula xml:id="formula_31">0 ra r c 1 l þ e þ :<label>ð17Þ</label></formula><p>Next, substituting <ref type="bibr" target="#b14">(15)</ref> and ( <ref type="formula" target="#formula_29">16</ref>) into the Lagrangian function <ref type="bibr" target="#b13">(14)</ref> and combining with (17) lead to the dual QPP of (12), which is</p><formula xml:id="formula_32">max À 1 2 a T X þ X T þ aþ n 1 l À e T À X À X T þ a s:t: e T þ a ¼ n 1 , 0 r ar c 1 l þ e þ :<label>ð18Þ</label></formula><p>Solving the above dual QPP, we obtain the Lagrangian vector a, which gives the weight vector w þ according to <ref type="bibr" target="#b14">(15)</ref>. In addition, according to the KKT conditions, the bias term b þ is computed as following:</p><formula xml:id="formula_33">b þ ¼ À 1 jN þ j X i A N þ X i w þ ,</formula><p>where N þ is the index set of positive samples satisfying</p><formula xml:id="formula_34">a i A ð0,c 1 =l þ Þ.</formula><p>Similarly, we obtain the dual QPP of the problem (13), which is</p><formula xml:id="formula_35">max À 1 2 b T X À X T À bþ n 2 l þ e T þ X þ X T À b s:t: e T À b ¼ n 2 , 0 r br c 2 l À e À ,<label>ð19Þ</label></formula><p>and the weight vector w À and bias term b À according to the KKT conditions:</p><formula xml:id="formula_36">w À ¼ n 2 l þ X T þ e þ ÀX T À b, b À ¼ À 1 jN À j X j A N À X j w À ,<label>ð20Þ</label></formula><p>where N À is the index set of negative samples satisfying</p><formula xml:id="formula_37">b j A ð0,c 2 =l À Þ.</formula><p>After optimizing ( <ref type="formula" target="#formula_32">18</ref>) and ( <ref type="formula" target="#formula_35">19</ref>), we construct the classifier for this TPMSVM, which is</p><formula xml:id="formula_38">f ðxÞ ¼ sign ðð ŵ þ þ ŵÀ Þ T x þð b þ þ bÀ ÞÞ,<label>ð21Þ</label></formula><p>where</p><formula xml:id="formula_39">ŵ 7 ¼ w 7 =Jw 7 J 2 , b 7 ¼ b 7 =Jw 7 J 2 :</formula><p>While the region between the parametric-margin hyperplane f 1 ðxÞ ¼ 0 (or f 2 ðxÞ ¼ 0) and f ðxÞ ¼ 0 are the positive (or negative) parametric-margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Nonlinear TPMSVM</head><p>In order to extend our result to the nonlinear case, we consider the following two hyperplanes in the feature space instead of linear hyperplanes:</p><formula xml:id="formula_40">f 1 ðxÞ ¼ w T þ jðxÞþb þ ¼ 0, f 2 ðxÞ ¼ w T À jðxÞþb À ¼ 0:</formula><p>As the above discussion, the above two hyperplanes are obtained through the following two similar QPPs:</p><formula xml:id="formula_41">min 1 2 jjw þ jj 2 2 þ n 1 l À e T À ðjðX À Þw þ þe À b þ Þþ c 1 l þ e T þ n s:t: jðX þ Þw þ þ b þ e þ Z 0Àn, nZ0,<label>ð22Þ</label></formula><formula xml:id="formula_42">and min 1 2 Jw À J 2 2 À n 2 l þ e T þ ðjðX þ Þw À þe þ b À Þþ c 2 l À e T À g s:t: jðX À Þw À þb À e À r 0 þg, gZ0,<label>ð23Þ</label></formula><p>where c 1 ,c 2 40, n 1 ,n 2 4 0 are the regularization parameters which determine the penalty weights, and n,g are the slack vectors.</p><p>By introducing the Lagrangian functions and finding the KKT necessary and sufficient optimality conditions for the problems <ref type="bibr" target="#b21">(22)</ref> and <ref type="bibr" target="#b22">(23)</ref>, we obtain the dual QPPs of ( <ref type="formula" target="#formula_41">22</ref>) and <ref type="bibr" target="#b22">(23)</ref>, which are</p><formula xml:id="formula_43">max À 1 2 a T KðX þ ,X þ Þa þ n 1 l À e T À KðX À ,X þ Þa s:t: e T þ a ¼ n 1 , 0 ra r c 1 l þ e þ ,<label>ð24Þ</label></formula><formula xml:id="formula_44">and max À 1 2 b T KðX À ,X À Þb þ n 2 l þ e T þ KðX þ ,X À Þb s:t: e T À b ¼ n 2 , 0 r br c 2 l À e À :<label>ð25Þ</label></formula><p>After optimizing these two QPPs, we obtain the weight vectors w 7 according to the KKT conditions, which are</p><formula xml:id="formula_45">w þ ¼ jðX þ Þ T aÀ n 1 l À jðX À Þ T e À ,<label>ð26Þ</label></formula><p>and</p><formula xml:id="formula_46">w À ¼ n 2 l þ jðX þ Þ T e þ ÀjðX À Þ T b:<label>ð27Þ</label></formula><p>As for the bias terms b 7 , we have</p><formula xml:id="formula_47">b 7 ¼ À 1 jN 7 j X i A N 7 jðX i Þw 7</formula><p>based on the KKT conditions.</p><p>Similar to the linear case, the separating hyperplane of nonlinear TPMSVM can be constructed as <ref type="bibr" target="#b20">(21)</ref> after optimizing the dual QPPs ( <ref type="formula" target="#formula_43">24</ref>) and (25).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>For the positive (or negative) parametric-margin hyperplane, let define the positive (or negative) training samples X i ,i A X þ (or X j ,j A X À ) with a i 4 0 (or b j 4 0) as the positive (or negative) support vectors since they are important to determine the positive (or negative) parametric-margin hyperplane. Similar to the classical SVM, let define the positive (or negative) training points X i ,i A X þ (or X j ,j A X À ) with f 1 ðX i Þ o0 (or f 2 ðX j Þ o 0) as the positive (or negative) margin errors. According to the KKT conditions, we have a i ¼ c 1 =l þ (or b j ¼ c 2 =l À ) for the positive (or negative) margin error point.</p><p>To theoretically analyze the proposed TPMSVM, we define the fractions of the positive and negative margin errors, and the fractions of the positive and negative SVs, which are</p><formula xml:id="formula_48">1 l þ jfX i : f 1 ðX i Þ o0,i A X þ gj and 1 l À jfX j : f 2 ðX j Þ 40,j A X À gj,<label>and</label></formula><formula xml:id="formula_49">1 l þ jfX i : a i 4 0,i A X þ gj and 1 l À jfX j : b j 4 0,j A X À gj:</formula><p>Now, we analyze the theoretical aspects of the new optimization problems given in ( <ref type="formula" target="#formula_32">18</ref>) and ( <ref type="formula" target="#formula_35">19</ref>). The core aspect can be captured in the proposition stated below.</p><p>Proposition 1. Suppose the TPMSVM obtains the nontrivial positive and negative parametric-margin hyperplanes, then the following statements hold:</p><p>(i) n 1 =c 1 and n 2 =c 2 are the upper bounds on the fractions of positive and negative margin errors, respectively;</p><p>(ii) n 1 =c 1 and n 2 =c 2 are the lower bounds on the fractions of positive and negative SVs, respectively.</p><p>Proof. The proof of this Proposition is similar to that of Proposition 5 in <ref type="bibr" target="#b37">[38]</ref> or Proposition 2 in <ref type="bibr" target="#b38">[39]</ref>. &amp; Proposition 1 states that the values of n 1 =c 1 and n 2 =c 2 can control the bounds of the fractions of SVs and margin errors, and that the values of n i , i¼1,2 are not larger than c i for the TPMSVM classifier.</p><p>The following remark will help us to further understand the parameters of the proposed TPMSVM.</p><p>Remark 1. The setting of the values of n i and c i ,i ¼ 1,2 is very important to the estimations of the parametric-margin hyperplanes f 1 ðxÞ ¼ 0 and f 2 ðxÞ ¼ 0. An overlarge n 1 =c 1 leads f 1 ðxÞ ¼ 0 to shift to the class '' þ '' and vice versa. Similarly, an overlarge n 2 =c 2 leads f 2 ðxÞ ¼ 0 to shift to the class '' À'' and vice versa. Thus, an important work is to determine the parameters of the TPMSVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We mention that the proposed TPMSVM is motivated by the TWSVM and par-n-SVM. We show some comparisons of this TPMSVM and the par-n-SVM and TWSVM in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TPMSVM vs. par-n-SVM</head><p>As mentioned earlier, we have stated that the proposed TPMSVM, similar to the par-n-SVM, is suitable for the case that the noise is heteroscedastic by optimizing the pair of parametricmargin hyperplanes. In other words, it derives the similar characteristics as the par-n-SVM.</p><p>Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref> show the geometric interpretations of the parn-SVM and TPMSVM. It can be found that the two algorithms have the similar geometric interpretations, and the two algorithms are suitable for the case that the data has heteroscedastic error structure. However, the two methods adopt the totally different strategies to find the separating hyperplanes. In the par-n-SVM classifier, it directly finds the separating hyperplanes f ðxÞ ¼ 0 and the parametric-margin gðxÞ through a single QPP <ref type="bibr" target="#b9">(10)</ref>. Whereas, the proposed TPMSVM first optimizes the pair of parametricmargin hyperplanes f 1 ðxÞ ¼ 0 and f 2 ðxÞ ¼ 0 through two smaller sized QPPs ( <ref type="formula" target="#formula_24">12</ref>) and ( <ref type="formula" target="#formula_25">13</ref>), while the separating hyperplane is then indirectly determined by <ref type="bibr" target="#b20">(21)</ref>. Geometrically, the pair of two smaller sized QPPs of the TPMSVM optimizes the hyperplanes f ðxÞ 7gðxÞ ¼ 0 in the par-n-SVM.</p><p>As for the training complexities of the TPMSVM and par-n-SVM, the par-n-SVM optimizes a QPP with l variables, which means its training computational complexity is no more than Oðl 3 Þ. While our TPMSVM solves two QPPs, each QPP is roughly of size (l/2), which means that the learning speed TPMSVM is approximately four times faster than the par-n-SVM in theory. Thus, the learning cost of TPMSVM is less than the par-n-SVM and classical SVM. However, as for the prediction speed, the TPMSVM is obviously slower than the par-n-SVM because not only the TPMSVM is an indirect classifier, but also the weight vectors of its parametricmargin hyperplanes lose the sparsity, see <ref type="bibr" target="#b25">(26)</ref> and <ref type="bibr" target="#b26">(27)</ref>.</p><p>The following remark will help us to further understand the differences of the proposed TPMSVM and the par-n-SVM.</p><p>Remark 2. The parametric-margin hyperplanes f i ðxÞ ¼ 0,i ¼ 1,2 in the TPMSVM may meet somewhere since, in general, they are nonparallel. It means that the margin around this point is about zero, or that the two classes around this point are inseparable. For this case, we can re-define the margin as zero if f 1 ðxÞ r f 2 ðxÞ. However, the classification strategy <ref type="bibr" target="#b10">(11)</ref> and the separating rule <ref type="bibr" target="#b20">(21)</ref> of TPMSVM can successfully deal with this case. On the other hand, the TPMSVM can avoid this problem by adjusting the parameters for inseparable cases. Generally, the hyperplanes f ðxÞ 7gðxÞ ¼ 0 in the par-n-SVM are also nonparallel and then may meet somewhere. Fig. <ref type="figure">3</ref> also confirms this conclusion for the par-n-SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">TPMSVM vs. TWSVM</head><p>As mentioned earlier, both the TPMSVM and TWSVM first optimize a pair of smaller sized QPPs, which are used to construct a pair of nonparallel hyperplanes, while the separating functions are then constructed indirectly. This strategy makes the TPMSVM obtain the similar learning cost as the TWSVM. In addition, both the TPMSVM and TWSVM lose the sparsity because of the formulations of their augmented vectors, which means these two methods have the slow test speeds.</p><p>However, there are some differences in the two classifiers. First, the ends of the TPMSVM and TWSVM are different. The TPMSVM finds a pair of parametric-margin hyperplanes such that each one determines the positive or negative parametric-margin, while the TWSVM find a pair of nonparallel hyperplanes such that each hyperplane is closer to one of the two classes and is at least one far from the other class. Figs. <ref type="figure" target="#fig_3">2</ref> and<ref type="figure">4</ref> also confirm this difference. Second, the constructions of the QPPs of the TPMSVM and TWSVM are totally different, including the objective functions and the constraints. For instance, in the case of the TPMSVM, the number of constraints is the number of points in the same class for each QPP. Whereas, in the case of the TWSVM, the number of constraints is the number of points in the other class. Essentially, this is because the TPMSVM and TWSVM have the different ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To validate the performance of TPMSVM, we investigate the results on several artificial and publicly available benchmark data sets in this Section, which are commonly used in testing machine learning algorithms. In the experiments we compare it with the SVM, par-n-SVM, and TWSVM, respectively. All methods are implemented in MATLAB 7.1<ref type="foot" target="#foot_1">2</ref> on Windows XP running on a PC. In the simulations we consider the linear and Gaussian kernels in these methods. An important problem is the parameter selection for these algorithms. However, there is no explicit way to solve the problem of choosing multiple parameters for SVMs. Although there exits many parameter selection methods for SVms, the most popular method to determine the parameters of SVMs is still the exhaustive search <ref type="bibr" target="#b41">[43]</ref>. For brevity's sake, we set c 1 ¼ c 2 ¼ c for the TWSVM and c 1 ¼ c 2 ¼ c and n 1 ¼ n 2 ¼ n for the TPMSVM. For the values of parameters c's and kernel parameters g 0 s in these algorithms, we select them from the set of values f2 i ji ¼ À9, À8, . . . ,10g by tuning a set comprising of random 30 percent of the sample sets. While we select the values of n and n=c in the par-n-SVM and TPMSVM from the set f0:1,0:2, . . . ,0:9g, respectively. Once the parameters are determined, the tuning sets are returned to the sample set to learn the final classifiers. If no particular claim, we apply the ten-fold cross-validation method on the whole training data to estimate the generalized accuracy. Namely, for each problem we partition the available examples into ten disjoint subsets (called 'folds') of approximately equal size. The classifier is trained on all the subsets except for one, and the validation error is measured by testing it on the subset left out. This procedure is repeated for a total of ten trials, each time using a different subset for validation. In the comparisons, we consider the test accuracies, numbers of SVs and learning CPU time of these methods. Note that in the learning processes of these methods the kernel matrices are cached beforehand optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Artificial examples</head><p>The first example is a two-dimensional heteroscedastic artificial dataset. The two classes of samples consist of uniform points in two half-circles (see Fig. <ref type="figure" target="#fig_5">5</ref>). This dataset has heteroscedastic error structure since six noise points (denoted as ''Ã'') for each class in fixed locations. Fig. <ref type="figure" target="#fig_5">5</ref> shows the learning results of the four algorithms, in which the positive and negative samples are denoted as '' þ'' and '' Â '', and the SVs are marked by extra circles, respectively. It can be seen that the all four learning algorithms obtain the totally correct results on this example. In addition, it is easy to find that, in Fig. <ref type="figure" target="#fig_5">5</ref>, the parametric-margin hyperplanes of both the par-n-SVM and TPMSVM effectively detect these noise points.</p><p>Another example is the artificial-generated Ripley's synthetic dataset <ref type="bibr" target="#b42">[44]</ref>. The Ripley's synthetic dataset includes 250 training points and 1000 test points. Figs. <ref type="figure" target="#fig_6">6</ref> and<ref type="figure" target="#fig_7">7</ref> show the learning results of SVM, par-n-SVM, TWSVM and TPMSVM with the linear and Gaussian kernels. It can be seen that our TPMSVM obtains the comparable classification results with the other methods. To distinctly reflect the learning results of these methods, Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref> list the prediction accuracies, learning CPU time and numbers of SVs of these algorithms on the Ripley's dataset with the linear and Gaussian kernels, respectively. It can be seen that our nonlinear TPMSVM obtains the best classification result among these algorithms. As for the learning time, our method and the TWSVM are more efficient than the SVM and par-n-SVM. This indicates that our TPMSVM successfully reduces the computational costs by optimizing a pair of QPPs. For the numbers of SVs, it can be found that the number of SVs of nonlinear TPMSVM are similar to that of par-n-SVM when n in the par-n-SVM equals to the value of n=c in the TPMSVM. To further illustrate the correctness of Proposition 1, we simulate the nonlinear TPMSVM with different parameter values ðc,n=cÞ A f2 À9 , . . . ,2 10 g Â f0:1, . . . , 0:9g on this Ripley's dataset. Fig. <ref type="figure" target="#fig_8">8</ref> depicts the relations between the parameter values and the fractions of numbers of SVs and margin errors. It can be found that, given c, the value of n=c effectively controls the bounds of fractions of SVs and margin errors.</p><p>The third example is still a two-dimensional artificial dataset, in which the positive class of samples consist of uniform points satisfying x 1 A ½Àp=2,2p,sinx 1 À0:25r x 2 rsinx 1 þ0:25, while the negative class of samples consist of uniform points satisfying x 1 A ½Àp=2,2p, 0:6sinðx 1 =1:05 þ 0:4ÞÀ1:35 rx 2 r0:6sinðx 1 =1:05 þ 0:4Þ À0:85, where x ¼ ½x 1 ,x 2 T (see Fig. <ref type="figure">9</ref>). Obviously, the margin width between two classes of this dataset depends on the input location. Fig. <ref type="figure">9</ref> describes the one-run results of the nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on this dataset. It can be seen that the separating hyperplanes of these classifiers obtain the similar results. However, the proposed TPMSVM obtains a pair of more suitable parametric-margin hyperplanes compared with the other classifiers. Whereas, for the par-n-SVM, the parametric-margin hyperplanes (f ðxÞ 7gðxÞ ¼ 0) are more similar with the the margin hyperplanes (f ðxÞ 7 1 ¼ 0) of SVM. This indicates that the parametric-margin hyperplanes of the proposed method are effective to determine the flexible margin. To further illustrate the performance of the TPMSVM, we add a noise x $ N ð0,0:1 2 Þ for each point of this dataset. Fig. <ref type="figure" target="#fig_1">10</ref> shows the one-run simulation results of the nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on this dataset. It can be found that the proposed TPMSVM can still effectively determine the pair of parametric-margin hyperplanes compared with the other three methods. That is, its parametric-margin hyperplanes can effectively classify noise. On the contrary, the parametric-margin hyperplanes of the par-n-SVM can not effectively determines noise for this dataset. Another interesting phenomenon is that the parametric-margin hyperplanes of the TPMSVM cross each other in Fig. <ref type="figure" target="#fig_1">10</ref>. In fact, this is because the two classes are inseparable around this region. To fairly reflect the performance of the four algorithms, we make ten independent runs on this example. Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref> list the average results of these methods on the no-noise and noise cases, including the average test accuracies, numbers of SVs, and learning CPU time. The results indicate that these methods obtain the similar test accuracies for the no-noise case, while our TPMSVM obtains the slightly better test accuracies for the noise case. However, as the mentioned earlier, our TPMSVM is more suitable for this example since its parametric-margin hyperplanes successfully describe the two classes of points. As for the learning time, our method and the TWSVM are more efficient than the SVM and parn-SVM. As for the numbers of SVs of these methods, it can be found that the numbers of SVs of TWSVM are much fewer than the other three methods. However, the TWSVM still loses the sparsity because of the formulations of its augmented vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmark datasets</head><p>To further test the performance of TPMSVM, we run these algorithms on several publicly available benchmark datasets, <ref type="foot" target="#foot_2">3</ref>which are commonly used in testing machine learning  Some of those problems have more than two classes. For simplicity, we treat the class with the most data points as the first class and the other classes the second class. In the simulations we only consider the linear and Gaussian kernels for these classifiers. Tables <ref type="table" target="#tab_3">5</ref> and<ref type="table" target="#tab_4">6</ref> report the learning results of these algorithms with linear and Gaussian kernels, including the optimal model parameters, the average cross-validation accuracies, the numbers of SVs, and the learning CPU time of one-run. From Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table" target="#tab_3">5</ref> it is seen that there is no significant difference in classification accuracy between the SVM, TWSVM, par-n-SVM, and TPMSVM classifiers. The p-values calculated from the paired t-test indicate that the average testing accuracies are not significant in 5% confidence level for those methods. A possible reason is that these datasets may not have heteroscedastic errors. As for the number of SVs, which is the main factor which affects the testing time, the results indicate that the numbers of SVs obtained by the TWSVM are fewer than those of the SVM, par-n-SVM, and TPMSVM except for some datasets. However, we should remark that the TPMSVM and TWSVM lose the sparsity because of the formulations of augmented vectors. In additions, the learning time of the TPMSVM is much less than the SVM and par-n-SVM. These indicates that our TPMSVM effectively combines the merits of par-n-SVM and TWSVM. To further validate Proposition 1,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, the twin parametric-margin support vector machine (TPMSVM) for binary recognition, motivated by the popular twin support vector machine (TWSVM) <ref type="bibr" target="#b29">[30]</ref>, has been presented. This TPMSVM successfully combines the merits of the TWSVM and par-n-SVM classifiers. On one hand, as the TWSVM classifier, the TPMSVM finds a pair of nonparallel hyperplanes through two smaller sized QPPs rather than one large sized QPP in the par-n-SVM or SVM, which means the TPMSVM is a more efficient learning algorithm compared with the par-n-SVM and SVM. On the other hand, the nonparallel hyperplanes of the TPMSVM are totally different from those of the TWSVM. The TWSVM generates two nonparallel hyperplanes such that each one is closer to one class with L 2 -norm and is at a distance of at least 1 from those of the other class. Whereas, the TPMSVM determines a pair of parametric-margin hyperplanes as the parn-SVM classifier. Similar to the par-n-SVM, the parametric-margin hyperplanes of the TPMSVM automatically adjusts a flexible margin for pattern recognition, and is very suitable for the heteroscedastic error structure.</p><p>In terms of generalization, the experiments have indicated that the TPMSVM yields the comparable generalization performance compared with the SVM, par-n-SVM, and TWSVM. In short, the proposed TPMSVM successfully combines the merits of TWSVM (learning speed) and par-n-SVM (parametric-margin). However, as the TWSVM, the TPMSVM also loses the sparsity because of the formulations of its parametric-margin hyperplanes. Hence, the further work is to improve the sparsity of TPMSVM. Recently, we have presented a sparse TSVR in primal space based on a simple back-fitting strategy <ref type="bibr" target="#b36">[37]</ref>. It is possible to incorporate the same concept in the TPMSVM to improve the sparsity of its parametricmargin hyperplanes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr Pattern Recognition 0031-3203/$ -see front matter &amp; 2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2011.03.031</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Geometric interpretation for SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>fd 7 ðxÞg, where d 7 ðxÞ ¼ jx T w 7 þb 7 j 7 J 2 :</head><label>772</label><figDesc>Jw</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Geometric interpretation for TWSVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 3 .</head><label>43</label><figDesc>Fig. 4. Geometric interpretation for TPMSVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of SVM, TWSVM, par-n-SVM and TPMSVM on Exam.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results of linear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.2 (Ripley's dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.2 (Ripley's dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Relations between parameter values and fractions of numbers of SVs (a) and margin errors (b) of nonlinear TPMSVM on Exam. 2 (Ripley's dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.3 without noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Results of linear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.2 (Ripley's dataset).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.2 (Ripley's</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SVM</cell><cell>TWSVM</cell><cell>par-n-SVM</cell><cell>TPMSVM</cell><cell></cell><cell>SVM ðc,gÞ</cell><cell>TWSVM ðc,gÞ</cell><cell>par-n-SVM ðc,n,gÞ</cell><cell>TPMSVM</cell></row><row><cell></cell><cell>c¼ 2 1</cell><cell>c¼ 2 3</cell><cell>ðc,nÞ ¼ ð2 10 ,0:4Þ</cell><cell>(c,v) ¼(2 10 ,0.3)</cell><cell></cell><cell>¼ ð2 À3 ,2 1 Þ</cell><cell>¼ ð2 0 ,2 1 Þ</cell><cell>¼ ð2 10 ,0:5,2 1 Þ</cell><cell>ðc,n=c,gÞ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>¼ ð2 7 ,0:5,2 2 Þ</cell></row><row><cell>Acc. (%)</cell><cell>89.80</cell><cell>89.70</cell><cell>89.60</cell><cell>89.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># SVs</cell><cell>116</cell><cell>135</cell><cell>117</cell><cell>81</cell><cell>Acc.</cell><cell>91.20</cell><cell>90.60</cell><cell>91.30</cell><cell>91.40</cell></row><row><cell>Time (s)</cell><cell>0.515</cell><cell>0.097</cell><cell>0.621</cell><cell>0.103</cell><cell># SVs</cell><cell>164</cell><cell>70</cell><cell>127</cell><cell>130</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time</cell><cell>1.392</cell><cell>0.113</cell><cell>1.361</cell><cell>0.125</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.3 without noise.</figDesc><table><row><cell></cell><cell>SVM ðc,gÞ ¼ ð2 10 ,2 À2 Þ</cell><cell>TWSVM ðc,gÞ ¼ ð2 3 ,2 À4 Þ</cell><cell>par-n-SVM ðc,n,gÞ ¼ ð2 10 ,0:1,2 -3 Þ</cell><cell>TPMSVM ðc,n=c,gÞ ¼ ð2 10 ,0:1,2 À3 Þ</cell></row><row><cell>Acc.</cell><cell>99.61</cell><cell>99.74</cell><cell>99.88</cell><cell>100.00</cell></row><row><cell># SVs</cell><cell>25.3</cell><cell>9.6</cell><cell>57.2</cell><cell>34.7</cell></row><row><cell>Time</cell><cell>7.258</cell><cell>1.192</cell><cell>6.980</cell><cell>1.275</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on Exam.3 with noise.</figDesc><table><row><cell></cell><cell>SVM ðc,gÞ ¼ ð2 10 ,2 À2 Þ</cell><cell>TWSVM ðc,gÞ ¼ ð2 3 ,2 À4 Þ</cell><cell>par-n-SVM ðc,n,gÞ ¼ ð2 10 ,0:1,2 -3 Þ</cell><cell>TPMSVM ðc,n=c,gÞ ¼ ð2 10 ,0:1,2 À3 Þ</cell></row><row><cell>Acc.</cell><cell>96.42</cell><cell>97.28</cell><cell>96.62</cell><cell>99.87</cell></row><row><cell># SVs</cell><cell>27.2</cell><cell>12.8</cell><cell>58.5</cell><cell>32.6</cell></row><row><cell>Time</cell><cell>6.260</cell><cell>0.991</cell><cell>7.066</cell><cell>1.104</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Results of linear SVM, TWSVM, par-n-SVM and TPMSVM on benchmark datasets.</figDesc><table><row><cell>Dataset</cell><cell>SVM c</cell><cell>TWSVM c</cell><cell>par-n-SVM ðc,nÞ</cell><cell>TPMSVM ðc,n=cÞ</cell></row><row><cell></cell><cell>Acc. (p-value)</cell><cell>Acc. (p-value)</cell><cell>Acc. (p-value)</cell><cell>Acc.</cell></row><row><cell></cell><cell>#SVs/Time</cell><cell>#SVs/Time</cell><cell>#SVs/Time</cell><cell>#SVs/Time</cell></row><row><cell>Cancer</cell><cell>2 3</cell><cell>2 2</cell><cell>(2 10 ,0.1)</cell><cell>(2 4 ,0.1)</cell></row><row><cell>699 Â 9</cell><cell>95.83 7 2.46 (0.7215)</cell><cell>95.42 7 2.28 (0.5825)</cell><cell>96.21 7 1.62 (0.8926)</cell><cell>96.37 7 1.70</cell></row><row><cell></cell><cell>65.4/9.917</cell><cell>72.3/0.910</cell><cell>64.8/9.129</cell><cell>65.1/1.254</cell></row><row><cell>Card</cell><cell>2 1</cell><cell>2 1</cell><cell>(2 10 ,0.5)</cell><cell>(2 7 ,0.5)</cell></row><row><cell>690 Â 51</cell><cell>85.91 7 4.52 (0.8130)</cell><cell>85.59 7 3.08 (0.6532)</cell><cell>86.32 7 4.62 (0.9533)</cell><cell>86.35 7 4.30</cell></row><row><cell></cell><cell>231.4/19.820</cell><cell>251.9/1.482</cell><cell>346.0/18.496</cell><cell>344.1/2.521</cell></row><row><cell>Diabetes</cell><cell>2 4</cell><cell>2 2</cell><cell>(2 10 ,0.4)</cell><cell>(2 10 ,0.4)</cell></row><row><cell>768 Â 8</cell><cell>76.41 7 6.01 (0.7832)</cell><cell>76.45 7 5.70(0.8025)</cell><cell>76.46 7 5.63 (0.8139)</cell><cell>76.62 7 5.22</cell></row><row><cell></cell><cell>307.1/12.426</cell><cell>289.3/0.971</cell><cell>307.7/11.381</cell><cell>304.4/2.108</cell></row><row><cell>German</cell><cell>2 4</cell><cell>2 3</cell><cell>(2 10 ,0.5)</cell><cell>(2 3 ,0.5)</cell></row><row><cell>1000 Â 24</cell><cell>76.31 7 4.26 (0.7589)</cell><cell>76.29 7 4.47 (0.7213)</cell><cell>77.05 7 4.20 (0.9032)</cell><cell>77.10 7 4.12</cell></row><row><cell></cell><cell>484.7/24.472</cell><cell>475.2/0.883</cell><cell>488.7/22.721</cell><cell>487.9/0.965</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Results of nonlinear SVM, TWSVM, par-n-SVM and TPMSVM on benchmark datasets. Time we simulate the nonlinear TPMSVM on the Heartc and WDBC datasets with different parameter values ðc,n=cÞ A f2 0 , . . . ,2 10 gÂ f0:1, . . . ,0:9g. Figs.11 and 12shows the relations between the parameter values and the fractions of numbers of SVs and margin errors on the two dsatasets, respectively. It can be found that, as pointed by Proposition 1, the value of n=c effectively controls the bounds of fractions of numbers of SVs and margin errors.</figDesc><table><row><cell>Dataset</cell><cell>SVM ðc,gÞ</cell><cell>TWSVM ðc,gÞ</cell><cell>par-n-SVM ðc,n,gÞ</cell><cell>TPMSVM ðc,n=c,gÞ</cell></row><row><cell></cell><cell>Acc. (p-value)</cell><cell>Acc. (p-value)</cell><cell>Acc. (p-value)</cell><cell>Acc.</cell></row><row><cell></cell><cell>#SVs/Time</cell><cell>#SVs/Time</cell><cell>#SVs/Time</cell><cell>#SVs/</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Peng / Pattern Recognition 44 (2011) 2678-2692</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Available at: http://www.mathworks.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Available at: ftp.ira.uka.de, http://kdd.ics.uci.edu/, and http://www.liacc.up. pt/ML/old/statlog/datasets.html.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been partly supported by the Innovative Project of Shanghai Municipal Education Commission (11YZ81), the Natural Science Foundation of SHNU (SK200937, SK201030), and the Shanghai Leading Academic Discipline Project (S30405).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cancer</head><p>(2 3 ,2 À 3 ) ( 2 2 ,2 À 3 ) ( 2 10 ,0.1,2 À 3 ) ( 2 4 ,0.1,2   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory<address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Christianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m">An Introduction to Support Vector Machines</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Natural of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training support vector machines: An application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ndellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rouveriol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<meeting><address><addrLine>Chemnitz, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge-based analysis of microarray gene expression data by using support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="262" to="267" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Proceedings of the National Academy of</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A support vector machine approach for detection of microclassification</title>
		<author>
			<persName><forename type="first">I</forename><surname>El-Naqa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1552" to="1563" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kernel Methods in Computational Biology</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint time-frequency-space classification of EEG in a brain-computer interface application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="713" to="729" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sch ölkopf, Support vector channel selection in BCI</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hinterberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Birbaumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1003" to="1010" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Credit rating analysis with support vector machines and neural networks: a market comparative study</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="543" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Support vector machine for regression and applications to financial forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Trafalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE-INNS-ENNS</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonliner prediction of chaotic time series using a support vector machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 IEEE Workshop on Neural Networks for Signal Processing</title>
		<meeting>the 1997 IEEE Workshop on Neural Networks for Signal Processing<address><addrLine>Amelia Island, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularized least squares fuzzy support vector regression for financial time series forecasting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Khemchandani</forename><surname>Jayadeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="138" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sch ölkopf, Sparse greedy matrix approximation for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning<address><addrLine>Stanford, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="911" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sch ölkopf, Sampling techniques for kernel methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient SVM training using low-rank kernel representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="243" to="264" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An improved training algorithm for support vector machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Neural Networks for Signal Processing</title>
		<meeting>the IEEE Workshop on Neural Networks for Signal Processing<address><addrLine>Amelia Island, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making large-scale SVM learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods: Support Vector Machine</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Core vector machines: fast SVM training on very large data sets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="392" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simpler core vector machines with enclosing balls</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kocsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning<address><addrLine>Corvallis, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finite Newton method for Lagrangian support vector machine classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scaling-up support vector machines using boosting algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2219" to="2222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A parallel mixture of SVMs for very large scale problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1105" to="1114" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RSVM: Reduced support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First SIAM International Conference on Data Mining</title>
		<meeting>the First SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Twin support vector machines for pattern classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Khemchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="905" to="910" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multisurface proximal support vector classification via generalized eigenvalues</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="74" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least squares twin support vector machines for pattern classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="7535" to="7543" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Application of smoothing technique on twin support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letter</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1842" to="1848" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Newton&apos;s method for nonparallel plane proximal classifier with unity norm hyperplanes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonparallel plane proximal classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="522" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TSVR: an efficient twin support vector machine for regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="372" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Primal twin support vector regression and its sparse approximation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="2846" to="2858" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New support vector algorithms</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1207" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New support vector algorithms with parameteric insensitive/ margin model</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="60" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type and the connection with the theory of integal equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London, Series A</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="415" to="446" />
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SSVM: a smooth support vector machine for classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computation Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="22" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comparison of methods for multiclass support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Neural Networks</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Currently he is a lecturer in the Department of Mathematics, Shanghai Normal University. His research interests are in the area of applied mathematics, neural networks, statistical learning theory</title>
	</analytic>
	<monogr>
		<title level="m">Xinjun Peng received the M.S. degree in mathematics from Yunnan University, Kunming, in 2005, and the Ph</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>D. from Shanghai University</orgName>
		</respStmt>
	</monogr>
	<note>and SVM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
