<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motif-based Graph Self-Supervised Learning for Molecular Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<email>wanghao3@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
							<email>cheekonglee@tencent.com</email>
							<affiliation key="aff1">
								<address>
									<country>Tencent America</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Motif-based Graph Self-Supervised Learning for Molecular Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being finetuned for specific tasks. However, most existing self-supervised pre-training frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motifbased Graph Self-supervised Learning (MGSSL) by introducing a novel selfsupervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past decade has witnessed the remarkable success of deep learning in natural language processing (NLP) <ref type="bibr" target="#b4">[5]</ref>, computer vision (CV) <ref type="bibr" target="#b12">[13]</ref>, and graph analysis <ref type="bibr" target="#b18">[19]</ref>. Inspired by these developments, researchers in the chemistry domain have also tried to exploit deep learning methods for moleculebased tasks such as retrosynthesis <ref type="bibr" target="#b46">[47]</ref> and drug discovery <ref type="bibr" target="#b7">[8]</ref>. To preserve the internal structural information, molecules can be naturally modeled as graphs where nodes represent atoms and edges denote the chemical bonds. Recently, some works applied Graph Neural Network (GNN) and some of its variants for molecular property prediction and obtained promising results <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Though GNNs have achieved remarkable accuracy on molecular property prediction, they are usually data-hungry, i.e. a large amount of labeled data (i.e., molecules with known property data) is required for training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. However, labeled molecules only occupy an extremely small portion of the enormous chemical space since they can only be obtained from wet-lab experiments or quantum chemistry calculations, which are time-consuming and expensive. Moreover, directly training GNNs on small labeled molecule datasets in a supervised fashion is prone to over-fitting and the trained GNNs can hardly generalize to out-of-distribution data. Similar issues have also been encountered in natural language processing and computer vision. Recent advances in NLP and CV address them by self-supervised learning (SSL) where a model is first pre-trained on a large unlabeled dataset and then transferred to downstream tasks with limited labels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>. For example, the pre-trained BERT language model <ref type="bibr" target="#b4">[5]</ref> is able to learn expressive contextualized word representations through reconstructing the input text-next sentence and masked language predictions so that it can significantly improve the performance of downstream tasks. Inspired by these developments, various self-supervised pre-training methods of GNNs have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. Based on how the self-supervised tasks are constructed, these methods can be classified into two categories, contrastive methods and predictive methods. Contrastive methods force views from the same graph (e.g., sampling nodes and edges from graphs) to become closer and push views from different graphs apart. On the other hand, the predictive methods construct prediction tasks by exploiting the intrinsic properties of data. For example, Hu et.al. <ref type="bibr" target="#b13">[14]</ref> designed node-level pre-training tasks such as predicting the context of atoms and the attributes of masked atoms and bonds. <ref type="bibr" target="#b14">[15]</ref> introduced an attributed graph reconstruction task where the generative model predicts the node attributes and edges to be generated at each step.</p><p>However, we argue that existing self-supervised learning tasks on GNNs are sub-optimal since most of them fail to exploit the rich semantic information from graph motifs. Graph motifs can be defined as significant subgraph patterns that frequently occur <ref type="bibr" target="#b26">[27]</ref>. Motifs usually contain semantic meanings and are indicative of the characteristics of the whole graph. For example, the hydroxide (-OH) functional group in small molecules typically implies higher water solubility. Therefore, it is vital to design motif-level self-supervised learning tasks which can benefit downstream tasks such as molecular property prediction.</p><p>Designing motif-level self-supervised learning tasks brings unique challenges. First, existing motif mining techniques could not be directly utilized to derive expressive motifs for molecular graphs because they only rely on the discrete count of subgraph structures and overlook the chemical validity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. Second, most graph generation techniques generate graphs node-by-node <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref>, which are not suitable for our task of motif generation. Finally, how to unify multi-level self-supervised pre-training tasks harmoniously brings a new challenge. One naive solution to do pre-training tasks sequentially may lead to catastrophic forgetting similar to continual learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>To tackle the aforementioned challenges, we propose Motif-based Graph Self-Supervised Learning (MGSSL) and Multi-level self-supervised pre-training in this paper. Firstly, MGSSL introduces a novel motif generation task that empowers GNNs to capture the rich structural and semantic information from graph motifs. To derive semantically meaningful motifs and construct motif trees for molecular graphs, we leverage the BRICS algorithm <ref type="bibr" target="#b3">[4]</ref> which is based on retrosynthesis from the chemistry domain. Two additional fragmentation rules are further introduced to reduce the redundancy of motif vocabulary. Secondly, a general motif-based generative pre-training framework is designed to generate molecular graphs motif-by-motif. The pre-trained model is required to make topology and attribute predictions at each step and two specific generation orders are implemented (breadth-first and depth-first). Furthermore, to take the multi-scale regularities of molecules into consideration, we introduce Multi-level self-supervised pre-training for GNNs where the weights of different SSL tasks are adaptively adjusted by the Frank-Wolfe algorithm <ref type="bibr" target="#b15">[16]</ref>. Finally, by pre-training GNNs on the ZINC dataset with our methods, the pre-trained GNNs outperforms all the state-of-the-art baselines on various downstream benchmark tasks, demonstrating the effectiveness of our design. The implementation is publicly available at https://github.com/zaixizhang/MGSSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Molecular Property Prediction</head><p>Prediction of molecular properties is a central research topic in physics, chemistry, and materials science <ref type="bibr" target="#b43">[44]</ref>. Among the traditional methods, density functional theory (DFT) is the most popular one and plays a vital role in advancing the field <ref type="bibr" target="#b20">[21]</ref>. However, DFT is very time-consuming and its complexity could be approximated as O(N 3 ), where N denotes the number of particles. To find more efficient solutions for molecular property prediction, various machine learning methods have been leveraged such as kernel ridge regression, random forest, and convolutional neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36]</ref>. To fully consider the internal spatial and distance information of atoms in molecules, many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b24">25]</ref> regard molecules as graphs and explore the graph convolutional network for property prediction. To better capture the interactions among atoms, <ref type="bibr" target="#b7">[8]</ref> proposes a message passing framework and <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref> extend this framework to model bond interactions. <ref type="bibr" target="#b24">[25]</ref> builds a hierarchical GNN to capture multilevel interactions. Furthermore, <ref type="bibr" target="#b31">[32]</ref> integrates GNNs into the Transformer-style architecture to deliver a more expressive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preliminaries of Graph Neural Networks</head><p>Recent years have witnessed the success of Graph Neural Networks (GNNs) for modeling graph data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>. Let G = (V, E) denotes a graph with node attributes X v for v ∈ V and edge attributes e uv for (u, v) ∈ E. GNNs leverage the graph connectivity as well as node and edge features to learn a representation vector (i.e., embedding) h v for each node v ∈ G and a vector h G for the entire graph G. Generally, GNNs follows a message passing paradigm, in which representation of node v is iteratively updated by aggregating the representations of v's neighboring nodes and edges. Specifically, there are two basic operators for GNNs: AGGREGATE(•) and COMBINE(•). AGGREGATE(•) extracts the neighboring information of node v; COMBINE(•) serves as the aggregation function of the neighborhood information. After k iterations of aggregation, v's representation captures the structural information within its k-hop network neighborhood. Formally, the k-th layer of a GNN is:</p><formula xml:id="formula_0">h (k) v = COMBINE (k) h (k−1) v , AGGREGATE (k) (h (k−1) v , h (k−1) u , e uv ) : u ∈ N (v) ,<label>(1) where h (k)</label></formula><p>v denotes the representation of node v at the k-th layer, e uv is the feature vector of edge between u and v and and N (v) represents the neighborhood set of node v. h (0) v is initialized with X v . Furthermore, to obtain the representation of the entire graph h G , READOUT functions are designed to pool node representations at the final iteration K,</p><formula xml:id="formula_1">h G = READOUT h (K) v | v ∈ G .<label>(2)</label></formula><p>READOUT is a permutation-invariant function, such as averaging, sum, max or more sophisticated graph-level pooling functions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-supervised Learning of Graphs</head><p>Graph Self-supervised learning aims to learn the intermediate representations of unlabeled graph data that are useful for unknown downstream tasks. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. Traditional graph embedding methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref> define different kinds of graph proximities, i.e., the vertex proximity, as the self-supervised objective to learn vertex representations. Furthermore, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> proposed to use the mutual information maximization as the optimization objective for GNNs. Recently, more self-supervised tasks for GNNs have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b29">30]</ref>. Based on how the self-supervised tasks are constructed, these models can be classified into two categories, namely contrastive models and predictive models. Contrastive models try to generate informative and diverse views from data instances and perform node-to-context <ref type="bibr" target="#b13">[14]</ref>, node-to-graph <ref type="bibr" target="#b38">[39]</ref> or motif-to-graph <ref type="bibr" target="#b53">[54]</ref> contrastive learning. On the other hand, predictive models are trained in a supervised fashion, where the labels are generated based on certain properties of the input graph data, i.e., node attributes <ref type="bibr" target="#b31">[32]</ref>, or by selecting certain parts of the graph <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>However, few works try to leverage the information of graph motifs for graph self-supervised learning.</p><p>Grover <ref type="bibr" target="#b31">[32]</ref> use traditional softwares to extract motifs and treat them as classification labels. MICRO-Graph <ref type="bibr" target="#b53">[54]</ref> exploits graph motifs for motif-graph contrastive learning. Unfortunately, both methods fail to consider the topology information of motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motif-based Graph Self-supervised Learning</head><p>In this section, we introduce the framework of motif-based graph self-supervised learning (Figure <ref type="figure" target="#fig_0">1</ref>). Generally, the framework consists of three parts: chemistry-inspired molecule fragmentation, motif generation and multi-level self-supervised pre-training for GNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Chemistry-inspired Molecule Fragmentation</head><p>Given a dataset of molecules, the first step of our method is to decompose molecules into several fragments/motifs. Based on these fragments, a molecule graph can be converted into a motif tree structure where each node represents a motif and the edges denotes the relative spatial relationships among motifs. We choose to represent the structure of motifs as a tree because it facilitates the motif generation task. Formally, given a molecule graph G = (V, E), a motif tree T (G) = (V, E, X ) is a connected labeled tree whose node set is V = {M 1 , ..., M n } and edge set is E. X refers to the induced motif vocabulary. Each motif</p><formula xml:id="formula_2">M i = (V i , E i ) is a subgraph of G.</formula><p>There are many ways to fragment a given graph while the designed molecule fragmentation method should achieve the following goals: 1) In a motif tree T (G), the union of all motifs M i should equals G. Formally,</p><formula xml:id="formula_3">i V i = V and i E i E = E.</formula><p>2) In a motif tree T (G), the motifs should have no intersections. That is</p><formula xml:id="formula_4">M i ∩ M j = ∅.</formula><p>3) The induced motifs should capture semantic meanings, e.g., similar to meaningful functional groups in the chemistry domain. 4) The occurrence of motifs in the dataset should be frequent enough so that the pre-trained GNNs can learn semantic information of motifs that can be generalized to downstream tasks. After the molecule fragmentation, the motif vocabulary of the molecule dataset should have a moderate size.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, we show the overview of molecule fragmentation. Generally, there are three procedures, BRICS fragmentation, further decomposition, and motif tree construction. A motif vocabulary can be built via preprocessing the whole molecule dataset following the molecule fragmentation procedures.</p><p>To fragment molecule graphs and construct motif trees, we firstly use the Breaking of Retrosynthetically Interesting Chemical Substructures (BRICS) algorithm <ref type="bibr" target="#b3">[4]</ref> that leverages the domain knowledge from chemistry. BRICS defines 16 rules and breaks strategic bonds in a molecule that match a set of chemical reactions. "Dummy" atoms are attached to each end of the cleavage sites, marking the location where two fragments can join together. BRICS cleavage rules are designed to retain molecular components with valuable structural and functional content, e.g. aromatic rings.</p><p>However, we find BRICS alone cannot generate desired motifs for molecule graphs. This is because BRICS only breaks bonds based on a limited set of chemical reactions and tends to generate several large fragments for a molecule. Moreover, due to the combination explosion of graph structure, we find BRICS produces many motifs that are variations of the same underlying structure (e.g., Furan ring with different combinations of Halogen atoms). The motif vocabulary is large (more than 100k unique fragments) while most of these motifs appear less than 5 times in the whole dataset.</p><p>To tackle the problems above, we introduce a post-processing procedure to BRICS. To alleviate the combination explosion, we define two rules operating on the output fragments of BRICS: (1) Break the bond where one end atom is in a ring while the other end not. (2) Select non-ring atoms with three or more neighboring atoms as new motifs and break the neighboring bonds. The first rule reduces the number of ring variants and the second rule break the side chains. Experiments show that these rules effectively reduce the size of motif vocabulary and improve the occurrence frequency of motifs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Motif Generation</head><p>Here, we present the framework of motif generation for the generative self-supervised pre-training.</p><p>The goal of the motif generation task is to let GNNs learn the data distribution of graph motifs so that the pre-trained GNNs can easily generalize to downstream tasks with several finetuning steps on graphs from similar domains.</p><p>Given a molecule graph G = (V, E) and a GNN model f θ , we first convert the molecule graph to a motif tree T (G) = (V, E, X ). Then we can model the likelihood over this motif tree by the GNN model as p(T (G); θ), representing how the motifs are labeled and connected. Generally, our method aims to pretrain the GNN model f θ via maximizing the likelihood of motif trees, i.e., θ * = argmax θ p(T (G); θ). To model the likelihood of motif trees, special predictions heads for topology and motif label predictions are designed (as shown in the following sections) and optimized along with f θ . After pre-training, only the GNN model f θ is transferred to downstream tasks.</p><p>We note that most existing works on graph generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> follow the auto-regressive manner to factorize the probability objective, i.e., p(T (G); θ) in this work. For each molecular graph, they decompose it into a sequence of generation steps. Similarly in this paper, we interleave the addition of a new motif, and the addition of bonds to connect the newly added motif to the existing partial motif tree. We denote a permutation vector π to determine the motif ordering, in which i π denotes the motif ID of i-th position in permutation π. Therefore, the probability p(T (G); θ) is equivalent to the expected likelihood over all possible permutations, i.e.,</p><formula xml:id="formula_5">p(T (G); θ) = E π [p θ (V π , E π )] ,<label>(3)</label></formula><p>where V π denotes the permuted motif labels and E π denotes the edges among motifs.</p><p>Our formalism permits a variety of orders. For simplicity, we assume that any node ordering π has an equal probability and we also omit the subscript π when illustrating the generative process for one permutation in the following sections. Given a permutation order, the probability of generating motif tree T (G) can be decomposed as follows:</p><formula xml:id="formula_6">log p θ (V, E) = |V| i=1 log p θ (V i , E i | V &lt;i , E &lt;i ).<label>(4)</label></formula><p>At each step i, we use motif attributes V &lt;i and structures E &lt;i of all motifs generated before i to generate a new motif V i and its connection with existing motifs E i .</p><p>Equation 4 describes the autoregressive generative process of motif trees. Then the question is how to choose an efficient generation order and how to model the conditional probability log p θ (V i , E i | V &lt;i , E &lt;i ). In the following sections, we introduce two efficient generation orders, 2)</p><p>3) 4)</p><p>.</p><p>Figure <ref type="figure">3</ref>: Illustration of the motif generation orders. The first row is the illustration of DFS order and the second row is the BFS order.</p><p>breadth-first (BFS) and depth-first (DFS) orders and show the corresponding auto-regressive generative models.</p><p>To generate a motif tree from scratch, we need to first choose the root of motif tree. In our experiments, we simply choose the motif with the first atom in the canonical order <ref type="bibr" target="#b33">[34]</ref>. Then MGSSL generates motifs in DFS or BFS orders (see Figure <ref type="figure">3</ref>). In the DFS order, for every visited motif, MGSSL first makes a topological prediction: whether this node has children to be generated. If a new child motif node is generated, we predict its label and recurse this process. MGSSL backtracks when there is no more children to generate. As for the BFS order, MGSSL generates motif nodes layer-wise. For motifs nodes in the k-th layer, MGSSL makes topological predictions and label predictions. If all the children nodes of motifs in the k-th layer are generated, MGSSL will move to the next layer. We also note that the motif node ordering in BFS and DFS are not unique as the order within sibling nodes is ambiguous. In the experiments, we pre-train in one order and leave this issue and other potential generation orders for future exploration.</p><p>At each time step, a motif node receives information from other generated motifs for making those predictions. The information is propagated through message vectors h ij when motif trees are incrementally constructed. Formally, let Êt be the set of message at time t. The model visits motif i at time t and x i denotes the embedding of motif i, which can be obtained by pooling the atom embeddings in motif i. The message h i,j is updated through previous messages:</p><formula xml:id="formula_7">h i,j = GRU x i , {h k,i } (k,i)∈ Êt,k =j , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where GRU is Gated Recurrent Unit <ref type="bibr" target="#b2">[3]</ref> adapted for motif tree message passing:</p><formula xml:id="formula_9">s i,j = (k,i)∈ Êt,k =j h k,i<label>(6)</label></formula><formula xml:id="formula_10">z i,j = σ(W z x i + U z s i,j + b z ) (7) r k,i = σ(W r x i + U r h k,i + b r ) (8) hi,j = tanh(Wx i + U k=N (i)\j r k,i h k,i )<label>(9)</label></formula><formula xml:id="formula_11">h i,j = (1 − z ij ) s ij + z ij hi,j .<label>(10</label></formula><p>) Topology Prediction: When MGSSL visits motif i, it needs to make binary predictions on whether it has children to be generated. We compute the probability via a one hidden layer network followed by a sigmoid function taking messages and motif embeddings into consideration:</p><formula xml:id="formula_12">p t = σ   U d • τ (W d 1 x i + W d 2 (k,i)∈ Êt h k,i )   , (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where d is the dimension of the hidden layer.</p><p>Motif Label Prediction: When motif i generate a child motif j, we predict the label of child j with:</p><formula xml:id="formula_14">q j = softmax(U l τ (W l h ij )),<label>(12)</label></formula><p>where q j is the distribution over the motif vocabulary X and l is the hidden layer dimension. Let pt ∈ {0, 1} and qj be the ground truth topological and motif label values, the motif generation loss is the sum of cross-entropy losses of topological and motif label predictions:</p><formula xml:id="formula_15">L motif = t L topo (p t , pt ) + j L pred (q j , qj ).<label>(13)</label></formula><p>In the optimization process, minimizing the above loss function corresponds to maximizing the log likelihood in equation 4. Note that in the training process, after topological and motif label prediction at each step, we replace them with their ground truth so that the MGSSL makes predictions based on correct histories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-level Self-supervised Pre-training</head><p>To capture the multi-scale information in molecules, MGSSL is designed to be a hierarchical framework including Atom-level and Motif-level tasks (Figure <ref type="figure" target="#fig_0">1</ref>). For Atom-level pre-training, we leverage attribute masking to let GNNs firstly learn the regularities of the node/edge attributes. In attribute masking, randomly sampled node and bond attributes (e.g., atom numbers, bond types) are replaced with special masked indicators. Then we apply GNNs to obtain the corresponding node/edge embeddings (edge embeddings can be obtained as a combination of node embeddings of the edge's end nodes). Finally, a fully connected layer on top of the embeddings predicts the node/edge attributes. The cross-entropy prediction losses are denoted as L atom and L bond respectively.</p><p>To avoid catastrophic forgetting in sequential pre-training, we unify multi-level tasks and aim to minimize the hybrid loss in pre-training:</p><formula xml:id="formula_16">L ssl = λ 1 L motif + λ 2 L atom + λ 3 L bond ,<label>(14)</label></formula><p>where λ i are weights of losses. However, it is time-consuming to do a grid search to determine the optimal weights. Here, we adapts the MGDA-UB algorithm <ref type="bibr" target="#b36">[37]</ref> from multi-task learning to efficiently solve the optimization problem (Equation <ref type="formula" target="#formula_16">14</ref>). Since MGDA-UB calculates the weights λ i by Frank-Wolfe algorithm <ref type="bibr" target="#b15">[16]</ref> at each training step, we do not have to give weights explicitly. The pseudo codes of the training process is included in the Appendix.</p><p>4 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets and Dataset Splittings. In this paper, we mainly focus on the molecular property prediction tasks, where large-scale unlabeled molecules are abundant whereas downstream labeled data is scarce. Specifically, we use 250k unlabeled molecules sampled from the ZINC15 database <ref type="bibr" target="#b37">[38]</ref> for self-supervised pre-training tasks. As for the downstream finetune tasks, we consider 8 binary classification benchmark datasets contained in MoleculeNet <ref type="bibr" target="#b44">[45]</ref>. The detailed dataset statistics are summarized in the Appendix. We use the open-source package RDKit <ref type="bibr" target="#b21">[22]</ref> to preprocess the SMILES strings from various datasets. To mimic the real-world use case, we split the downstream dataset by scaffold-split <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>, which splits the molecules according to their structures. We apply 3 independent runs on random data splitting and report the means and standard deviations.</p><p>Baselines. We comprehensively evaluate the performance of MGSSL against five state-of-the-art self-supervised pre-training methods for GNNs:</p><p>• Deep Graph Infomax <ref type="bibr" target="#b40">[41]</ref> maximizes the mutual information between the representations of the whole graphs and the representations of its sampled subgraphs. • Attribute masking <ref type="bibr" target="#b13">[14]</ref> masks node/edge features and let GNNs predict these attributes.</p><p>• GCC <ref type="bibr" target="#b29">[30]</ref> designs the pretraining task as discriminating ego-networks sampled from a certain node ego-networks sampled from other nodes. • Grover <ref type="bibr" target="#b31">[32]</ref> predicts the contextual properties based on atom embeddings to encode contextual information into node embeddings. • GPT-GNN <ref type="bibr" target="#b14">[15]</ref> is a generative pretraining task which predicts masked edges and node attributes.  In experiments, we also consider GNN without pre-training (direct supervised finetuning) and MGSSL with different generation orders (BFS and DFS).</p><p>Model Configuration. In the following experiments, we select a five-layer Graph Isomorphism Networks (GINs) as the backbone architecture, which is one of the state-of-the-art GNN methods. Mean pooling is used as the Readout function for GIN. In MGSSL, sum pooling is used to get the embedding of graph motifs. Atom number and chirality tag are input as node features and bond type and direction are regarded as edge features. In the process of pre-training, GNNs are pre-trained for 100 epochs with Adam optimizer and learning rate 0.001. In the finetuning stage, we train for 100 epochs and report the testing score with the best cross-validation performance. The hidden dimension is set to 300 and the batch size is set to 32 for pre-training and finetuning. The split for train/validation/test sets is 80% : 10% : 10%. All experiments are conducted on Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>Results on Downstream Tasks. In Table <ref type="table" target="#tab_0">1</ref>, we show the testing performance on downstream molecular prediction benchmarks using different self-supervised pre-training strategies with GIN.</p><p>We have the following observations: 1) Generally, GNN models can benefit from various selfsupervised pre-training tasks. The average prediction ROC-AUC of all the pre-trained models are better than GNN with direct finetuning. 2) MGSSL methods achieve the best performance on 7 out of 8 benchmarks, demonstrating the effectiveness of motif-based self-supervised pre-training. 3) We also show MGSSL with two motif generation orders in Table <ref type="table" target="#tab_0">1</ref>. Both methods show significant performance improvements on downstream tasks and BFS has a small edge over DFS on average. This may be explained by the fact that MGSSL is required to generate motifs layer-wise in BFS orders, which helps GNNs learn more structural information of motifs. In the following experiments, we use BFS order as the default setting.</p><p>In Figure <ref type="figure" target="#fig_3">4</ref>, we further show the training and testing curves of MGSSL (BFS and DFS) and the selected baselines. Due to the page limits, we select 4 benchmark datasets here. Beyond predictive performance improvement, our pre-trained GNNs have faster training and testing convergence than baseline methods. Since the pre-training is a one-time-effort, once pre-trained with MGSSL, the pre-trained GNNs can be used for various downstream tasks with minimal finetuning overhead.</p><p>Influence of the Base GNN. In Table <ref type="table" target="#tab_1">2</ref> we show that MGSSL is agnostic to the GNN architectures by trying five popular GNN models including GIN <ref type="bibr" target="#b45">[46]</ref>, GCN <ref type="bibr" target="#b18">[19]</ref>, RGCN <ref type="bibr" target="#b32">[33]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref> and DAGNN <ref type="bibr" target="#b23">[24]</ref>. We report the average ROC-AUC and the relative gains on 8 benchmarks. We Due to the combinatorial explosion, its generated motif vocabulary has a size over 100k while more than 90% motifs have frequencies less than 5. On the other hand, JT-VAE <ref type="bibr" target="#b16">[17]</ref> fragments molecules into rings and bonds and has a motif vocabulary size of less than 800. Our methods generate around 12k distinct motifs. By combining different fragmentation strategies, we are able to fragment molecules with intermediate granularities.</p><p>In Figure <ref type="figure">5</ref>, we show the influence of the size of motif vocabulary on 5 benchmark datasets. We can observe that the pre-trained models achieve the optimal performance with the motif vocabulary generated by our method. This may be explained by the following reasons: 1) When the motif segmentation is too coarse and the motif vocabulary is too large, the generated motif trees have fewer nodes. It is harder for GNNs to capture the structural information of motifs. Moreover, the generated motifs have low occurrence frequencies, which prevents GNNs from learning the general semantic information of motifs that can be generalized to downstream tasks. 2) When the motif segmentation is too fine, many generated motifs are single atoms or bonds, which inhibits GNNs from learning higher level semantic information through motif generation tasks.  <ref type="table" target="#tab_2">3</ref>, the multi-level pre-training has larger average ROC-AUC than the two variants. We can have the following interesting insights: 1) The Atom-level pre-training tasks enables GNNs to first capture the atom-level information, which can benefit higher level, i.e., motif-level tasks. 2) Since our multi-level pre-training unifies multi-scale pre-training tasks and adaptively assigns the weights for hierarchical tasks, it can achieve better performance than sequential pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>In this paper, we proposed Motif-based Graph Self-supervised Learning (MGSSL), which pre-trains GNNs with a novel motif generation task. Through pre-training, MGSSL empowers GNNs to capture the rich semantic and structural information in graph motifs. First, a retrosynthesis-based algorithm with two additional rules are leveraged to fragment molecule graphs and derive semantic meaningful motifs. Second, a motif generative pre-training framework is designed and two specific generation orders are considered (BFS and DFS). At each step, the pre-trained GNN is required to make topology and motif label predictions. Furthermore, we designed a multi-level pre-training to unify hierarchical self-supervised tasks. Finally, we conducted extensive experiments to show that MGSSL overperforms all the state-of-the-art baselines on various downstream benchmark tasks. Interesting future work includes 1) Designing more self-supervised pre-training tasks based on graph motifs. 2) Exploring motif-based pre-training in other domains other than molecules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of Motif-based Graph Self-supervised learning (MGSSL). The multi-level pre-training consists of two layers, Atom layer and Motif layer. In the Atom layer, we mask node/edge attributes and let GNNs predict those attributes based on neighboring structures. In the Motif layer, we construct motif trees and perform motif generative pre-training. In each step, based on existing motifs and connections, topology and motif predictions are made iteratively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of molecule fragmentation. Generally, there are three steps: 1) Firstly a molecule graph is cleaved based on BRICS. 2) Further decomposition to reduce the redundancy of motifs 3) Construct motif trees from molecule graphs. A motif vocabulary is built after preprocessing the whole molecule dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training and testing curves of different pre-training strategies on GINs. Solid and dashed lines indicate training and testing curves respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test ROC-AUC (%) performance on molecular property prediction benchmarks using different pre-training strategies with GIN. The rightmost column averages the mean of test performance across the 8 datasets. The best result for each dataset are bolded.</figDesc><table><row><cell cols="3">SSL methods</cell><cell>muv</cell><cell></cell><cell cols="3">clintox</cell><cell>sider</cell><cell></cell><cell>hiv</cell><cell></cell><cell></cell><cell>tox21</cell><cell></cell><cell>bace</cell><cell></cell><cell cols="3">toxcast</cell><cell>bbbp</cell><cell>Avg.</cell></row><row><cell cols="3">No pretrain</cell><cell cols="2">71.7±2.3</cell><cell cols="3">58.2±2.8</cell><cell>57.2±0.7</cell><cell></cell><cell cols="2">75.4±1.5</cell><cell></cell><cell>74.3±0.5</cell><cell cols="2">70.0±2.5</cell><cell></cell><cell cols="3">63.3±1.5</cell><cell>65.5±1.8</cell><cell>67.0</cell></row><row><cell cols="3">Infomax</cell><cell cols="2">75.1±2.8</cell><cell cols="3">73.0±3.2</cell><cell>58.2±0.5</cell><cell></cell><cell cols="2">76.5±1.6</cell><cell></cell><cell>75.2±0.3</cell><cell cols="2">75.6±1.0</cell><cell></cell><cell cols="3">62.8±0.6</cell><cell>68.1±1.3</cell><cell>70.6</cell></row><row><cell cols="3">Attribute masking</cell><cell cols="2">74.7±1.9</cell><cell cols="3">77.5±3.1</cell><cell>59.6±0.7</cell><cell></cell><cell cols="2">77.9±1.2</cell><cell></cell><cell>77.2±0.4</cell><cell cols="2">78.3±1.1</cell><cell></cell><cell cols="3">63.3±0.8</cell><cell>65.6±0.9</cell><cell>71.8</cell></row><row><cell cols="2">GCC</cell><cell></cell><cell cols="2">74.1±1.4</cell><cell cols="3">73.2±2.6</cell><cell>58.0±0.9</cell><cell></cell><cell cols="2">75.5±0.8</cell><cell></cell><cell>76.6±0.5</cell><cell cols="2">75.0±1.5</cell><cell></cell><cell cols="3">63.5±0.4</cell><cell>66.9±0.7</cell><cell>70.4</cell></row><row><cell cols="3">GPT-GNN</cell><cell cols="2">75.0±2.5</cell><cell cols="3">74.9±2.7</cell><cell>59.3±0.8</cell><cell></cell><cell cols="2">77.0±1.7</cell><cell></cell><cell>76.1±0.4</cell><cell cols="2">78.5±0.9</cell><cell></cell><cell cols="3">63.1±0.5</cell><cell>67.5±1.3</cell><cell>71.4</cell></row><row><cell cols="3">Grover</cell><cell cols="2">75.8±1.7</cell><cell cols="3">76.9±1.9</cell><cell>60.7±0.5</cell><cell></cell><cell cols="2">77.8±1.4</cell><cell></cell><cell>76.3±0.6</cell><cell cols="2">79.5±1.1</cell><cell></cell><cell cols="3">63.4±0.6</cell><cell>68.0±1.5</cell><cell>72.3</cell></row><row><cell cols="3">MGSSL (DFS)</cell><cell cols="2">78.1±1.8</cell><cell cols="3">79.7±2.2</cell><cell>60.5±0.7</cell><cell></cell><cell cols="2">79.5±1.1</cell><cell></cell><cell>76.4±0.4</cell><cell cols="2">79.7±0.8</cell><cell></cell><cell cols="3">63.8±0.3</cell><cell>70.5±1.1</cell><cell>73.5</cell></row><row><cell cols="3">MGSSL (BFS)</cell><cell cols="2">78.7±1.5</cell><cell cols="3">80.7±2.1</cell><cell>61.8±0.8</cell><cell></cell><cell cols="2">78.8±1.2</cell><cell></cell><cell>76.5±0.3</cell><cell cols="2">79.1±0.9</cell><cell></cell><cell cols="3">64.1±0.7</cell><cell>69.7±0.9</cell><cell>73.7</cell></row><row><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell>0</cell><cell>20 No Pretrain Attribute Masking 40 Epochs 60 GPT-GNN BFS DFS</cell><cell>80</cell><cell>100</cell><cell>0.2 0.3 0.4 0.9 0.8 0.7 0.6 0.5</cell><cell>0</cell><cell cols="2">20 No Pretrain Attribute Masking 40 Epochs 60 GPT-GNN BFS DFS</cell><cell>80</cell><cell>100</cell><cell>1.0 0.9 0.8 0.7 0.6 0.2 0.3 0.4 0.5</cell><cell>0</cell><cell cols="2">20 No Pretrain Attribute Masking 40 Epochs 60 GPT-GNN BFS DFS</cell><cell>80</cell><cell>100</cell><cell>1.0 0.9 0.8 0.7 0.6 0.2 0.3 0.4 0.5</cell><cell>0</cell><cell cols="2">20 No Pretrain Attribute Masking 40 Epochs 60 GPT-GNN BFS DFS</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) clintox</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) sider</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) hiv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(d) bace</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Compare pre-training gains with different GNN architectures, averaged ROC-AUC (%) on 8 benchmark datasets</figDesc><table><row><cell>Model</cell><cell>GCN</cell><cell>GIN</cell><cell></cell><cell cols="8">RGCN DAGNN GraphSAGE</cell></row><row><cell>No pretrain</cell><cell>68.8</cell><cell>67.0</cell><cell></cell><cell cols="2">68.3</cell><cell>67.1</cell><cell></cell><cell></cell><cell cols="3">68.3</cell></row><row><cell cols="2">MGSSL (BFS) 72.7</cell><cell>73.7</cell><cell></cell><cell cols="2">73.0</cell><cell>72.3</cell><cell></cell><cell></cell><cell cols="3">73.4</cell></row><row><cell>Relative gain</cell><cell cols="2">5.7% 10.0%</cell><cell></cell><cell cols="2">6.9%</cell><cell cols="2">7.7 %</cell><cell></cell><cell cols="3">7.5%</cell></row><row><cell cols="12">observe that all these GNN architectures can benefit from motif-based pre-training tasks. Moreover,</cell></row><row><cell cols="12">GIN achieves the largest relative gain and the best performance after pre-training.</cell></row><row><cell cols="2">Influence of Molecule Fragmentation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Here we show proper molecule frag-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">mentation methods are vital for the</cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell></row><row><cell cols="2">motif-based pre-training. Given dif-</cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell></row><row><cell cols="2">ferent molecule fragmentation methods,</cell><cell>0.70</cell><cell cols="2">-79$(</cell><cell cols="2">,QWHUPHGLDWH 2XU0HWKRG</cell><cell>%5,&amp;6</cell><cell>0.70</cell><cell cols="2">-79$(</cell><cell>,QWHUPHGLDWH 2XU0HWKRG</cell><cell>%5,&amp;6</cell></row><row><cell cols="2">different motif vocabulary are gener-ated with varying sizes. Other than</cell><cell>0.60 0.65</cell><cell></cell><cell>FOLQWR[ VLGHU EEES PXY</cell><cell></cell><cell></cell><cell></cell><cell>0.60 0.65</cell><cell></cell><cell>FOLQWR[ VLGHU EEES PXY</cell></row><row><cell cols="2">the fragmentation method introduced in this paper, we also try other fragmenta-</cell><cell>0.55</cell><cell>10 3</cell><cell>KLY</cell><cell cols="2">10 4 6L]HRIPRWLIYRFDEXODU\</cell><cell>10 5</cell><cell>0.55</cell><cell>10 3</cell><cell>KLY</cell><cell>10 4 6L]HRIPRWLIYRFDEXODU\</cell><cell>10 5</cell></row><row><cell cols="2">tion schemes with different granularities [17, 4]. BRICS alone [4] tends to gener-</cell><cell></cell><cell cols="4">(a) MGSSL (BFS)</cell><cell></cell><cell></cell><cell cols="3">(b) MGSSL (DFS)</cell></row><row><cell cols="2">ate motifs with large numbers of atoms.</cell><cell cols="10">Figure 5: Influence of the size of motif vocabulary</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on multi-level selfsupervised pre-training.Ablation Studies on Multi-level Selfsupervised Pre-training. We perform ablation studies to show the effectiveness of the multi-level pre-training. In Table3, w/o atom-level denotes pre-training GNNs with Motif-level tasks only and the sequential pre-training denotes performing the Motif-level tasks after the Atom-level tasks. As observed from Table</figDesc><table><row><cell>Avg. ROC-AUC</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank the anonymous reviewers for valuable feedback. This research was supported by grants from the National Natural Science Foundation of China (Grants No. 61922073 and U20A20229) and 2021 Tencent Rhino-Bird Research Elite Training Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nemofinder: Dissecting genome-wide protein-protein interactions with meso-scale network motifs</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mong</forename><surname>Li Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the art of compiling and using&apos;drug-like&apos;chemical fragment spaces</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Degen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Wegscheid-Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Zaliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Rarey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ChemMedChem: Chemistry Enabling Drug Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1503" to="1507" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction errors of molecular machine learning models lower than hybrid dft error</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Felix A Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">French</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graphcl: Contrastive self-supervised learning of graph representations</title>
		<author>
			<persName><forename type="first">Hakim</forename><surname>Hafidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounir</forename><surname>Ghogho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Ciblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08025</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Asgn: An active semi-supervised graph neural network for molecular property prediction</title>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheekong</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1746" to="1758" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-consistent equations including exchange and correlation effects</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><forename type="middle">Jeu</forename><surname>Sham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">4A</biblScope>
			<biblScope unit="page">A1133</biblScope>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Greg Landrum. Rdkit documentation. Release</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning of dynamic electron correlation energies from topological atoms</title>
		<author>
			<persName><forename type="first">Arnaldo</forename><forename type="middle">F</forename><surname>James L Mcdonagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Popelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="216" to="224" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Get your atoms in order an opensource implementation of a novel and robust molecular canonicalization algorithm</title>
		<author>
			<persName><forename type="first">Nadine</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Sayle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Landrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2111" to="2120" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hypersorec: Exploiting hyperbolic user and item representations with multiple aspects for social-aware recommendation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mcne: An end-to-end framework for learning multiple conditional network representations of social network</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Application of molecular dynamics simulations in molecular property prediction ii: diffusion coefficient</title>
		<author>
			<persName><forename type="first">Junmei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingjun</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Retroxpert: Decompose retrosynthesis prediction like a chemist</title>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianggang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11248" to="11258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks?</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Subramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12533</idno>
		<title level="m">Motif-driven contrastive learning of graph representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graphmi: Extracting private graph data from graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
