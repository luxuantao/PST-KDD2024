<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Author Disambiguation using Heterogeneous Graph Convolutional Network Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyue</forename><surname>Qiao</surname></persName>
							<email>qiaoziyue@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Du</surname></persName>
							<email>duyi@cnic.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
							<email>yanjie.fu@ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Network Information Center</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Author Disambiguation using Heterogeneous Graph Convolutional Network Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Name Disambiguation</term>
					<term>Network Embedding</term>
					<term>Clustering</term>
					<term>Graph Convolutional Network</term>
					<term>Meta Path</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People share same names in real world. When a digital library user searches for an author name, he may see a mixture of publications by different authors who have the same name. Making distinctions between them is an important prerequisite to improve the quality of services and contents in digital libraries. The general task of author disambiguation is to associate publications which belong to an identical name or names with highly similar spellings to different people entities. In recent years, many researches have been conducted to solve this challenging task. However, some works rely heavily on external knowledge bases and manually annotated data. Some unsupervised learning based works require complex feature engineering. In this paper, we propose a novel and efficient author disambiguation framework which needs no labeled data. We first construct a publication heterogeneous network for each ambiguous name. Then, we use our proposed heterogeneous graph convolutional network embedding method that encodes both graph structure and node attribute information to learn publication representations. After that, we propose a graph enhanced clustering method for name disambiguation that can greatly accelerate the clustering process and need not require the number of distinct persons. Our framework can be continually retrained and applied on incremental disambiguation task when new publications are put in. Experimental results on two datasets show that our framework clearly performs better than several state-of-the-art methods for author disambiguation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Nowadays, digital libraries(DLs) have become an important source of scholarly information retrieval. When an user search an author name on a DL, he/she wants to obtain quick and accurate results from a huge set of publications that correspond to the query name. However, many DLs' search service provide a broad range of publications by different authors who share the same name. Using name disambiguation technologies to automatically distinguish publications of various authors can help users spend less effort to verify the search results. In this paper, we focus on the general task of author disambiguation, which is to associate publications which belong to an identical name or names with highly similar spellings to different people entities. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods <ref type="bibr" target="#b0">[1]</ref>,</p><p>The research is supported by the National Key Research and Development Plan (2017YFC1601504), the Natural Science Foundation of China (61836013), the CNTC (China National Tobacco Corporation ) Science and Technology Major Project (110201901027(SJ-06)), and the Guangdong Provincial Key Laboratory of Biocomputing (2016B030301007).</p><p>1 Corresponding author. <ref type="bibr" target="#b1">[2]</ref> regard this problem as a classification of authors with same name, however, they require labeled data. A widely used and effective idea is to learn publication representations via multiple features, then measure the similarity between publications and identify whether they belong to the same author. However, representation learning based methods for this problem still have challenges.</p><p>The first challenge is how to learn effective representations for publications by extracting and combining different types of features, such as the semantic information of text information and discrete features (e.g., authors, venue). High quality representations play a critical role to quantify distinctions and similarities between publications <ref type="bibr" target="#b0">[1]</ref>. The majority of existing solutions utilize biographical features such as title, abstract, organization, author and venue, as well as relationship features such as citation to generate these representations. Most works design relationships such as coauthorship between publications by these characteristics and construct publication networks, then use graph-based <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> or heuristic methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> to learn the similarity between publications. Some methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> use the the word co-occurrence information in publications' content to design relations between publications, which may lose semantic information. Some methods focus on the content information but cannot measure the high-order relationship among publications and usually leverage supervised algorithms. Besides, many methods solve the problem on homogeneous networks. They either construct network for each type of relation, or combine different relationships to construct one publication network. Those strategies ignore the heterogeneous relationship between publications.</p><p>The second challenge is how to efficiently determine the assignment of publications when we do not exactly know the number of distinct person for an ambiguous name. Some researches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref> assume that the cluster number of ambiguous author K is known in advance, but in real world we actually have no clue about the number. Some <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref> propose different strategies to estimate the number. However, there are usually pre-defined parameters in these strategies. In addition, with the increasing data in digital libraries, clustering process becomes more time-consuming, more efficient clustering strategies for name disambiguation are demanded.</p><p>The third challenge is how to efficiently process the new published publications that may contain potentially ambiguous names. Many methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the whole collection when new records are added in. However, this update solution 978- discards the existing publication representations and clustering results. Besides, it is very computationally expensive as the size of DLs are becoming larger and larger. An ideal solution is to use incremental disambiguation approach that use existing clustering results as well as learned model parameters to generate the partitions of new publications without reprocessing the whole collection.</p><p>In this paper, we propose a framework to address the challenges above to our best. We construct a publication heterogeneous network which contains multiple types of relations. Then we propose a heterogeneous network embedding method based on graph convolutional network as well as a relation weight and meta-path guided random walk strategy. After that, we propose an efficient clustering strategy that does not need to specify the number of clusters or any other parameters. Lastly, we introduce the incremental disambiguation strategies. The main contributions of this work are summarized as follows:</p><p>1) We propose a heterogeneous graph convolutional network embedding method integrating multi-layer and heterogeneous relationships between publications and the semantic information of publications, and learn a lowdimensional representation of each publication with high quality for name disambiguation. 2) We design an efficient and effective framework for author name disambiguation. A graph enhanced cluster strategy is introduced to accelerate the clustering speed without requiring the number of distinct persons. We also propose incremental disambiguation strategies that can process new coming publications.</p><p>3) The proposed method is evaluated and analysed on two benchmark datasets. The performance of our solution is significantly better than several state-of-the-art methods.</p><p>II. PRELIMINARIES Treating publications as nodes, by constructing a network connecting the publication nodes, the relationship information between publications can be well preserved and expressed. Because of the multiplicity of publications attributes, there are many types of relationships between them. Therefore, we can retain these complex contacts and construct a heterogeneous network of publications. Definition 3 (Publication Heterogeneous Network). In our paper, the publication Heterogeneous Network (PHNet) that we construct can be seen as a special case of weighted heterogeneous network which has one type of nodes and three types of relations. For each ambiguous name a, its PHNet can be expressed as G a = (V, E, R). As shown in Figure 1(a), every publication is expressed as a single node in the PHNet and the set of relation types R includes CoAuthor(A), CoVenue(V), and CoTitle(T), which respectively represent three kinds of undirected relations between publications. The detailed definition of each is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Heterogeneous Network</head><formula xml:id="formula_0">). A heterogeneous network is a graph G = (V, E, N , R) in which each node v ∈ V is</formula><p>CoAuthor Each publication has a list of authors, so it can be coauthored by multiple names. For any name a to be disambiguated, suppose two of its publications are p a 1 and p a 2 , which have the author sets A 1 and A 2 respectively. Let the set A 1 denote A 1 excluding the author a, i.e., A CoTitle Titles always contain a lot of topic words to summarize the main content of the publication. For each publication p i 's title, first, we remove the stop words and then transform each word into its word stem. After that, the title is processed into a words set T i . For two publications p 1 and p 2 , if T 1 ∩ T 2 = ∅, there is a CoTitle relation between them, and the value of the CoTitle relation is</p><formula xml:id="formula_1">1 = A 1 − {a}, If A 1 ∩ A 2 = ∅,</formula><formula xml:id="formula_2">|T 1 ∩ T 2 |.</formula><p>Other attributes of publications are also usable in our model when they are available. Considering that some attributes such as affiliation, citation maybe not available or incomplete in some digital libraries, especially the affiliation information usually has the synonym problem <ref type="bibr" target="#b13">[14]</ref> and hard to be cleaned, so in this paper, we only use the three common attributes mentioned above. Except the ambiguous name a, the attributes of publications we used to construct PHNet maybe also ambiguous. For example, if two publications coauthored by a, in the same time, they each have another author b, and these two authors named b are actually two different people. But empirically, the probability of this coincidence happening is so small that the impact of these noises on the downstream model training can be ignored.</p><p>In a PHNet, two nodes p i and p j may be connected via multiple undirected relations. The node sequence linked by these relations can be seen as a path from p i to p j . For example, in Figure <ref type="figure" target="#fig_1">1</ref>, there is a path p 1  Definition 5 (Name Disambiguation Problem). Given an author name a to be disambiguated, we define D a = {p a 1 , p a 2 , ..., p a m } as the collection of m publications coauthored by any person named a, where the p a i means the i-th publication of author a. Each publication contains attributes such as authors list, title and venue (e.g., a conference or journal). Assuming that the number of distinct authors named a is K a , our goal is to distinguish these publications into different author entities automatically by using the attribute information of them. In other words, we want to cluster the publications in D a into K a disjoint clusters C a = {C a 1 , C a 2 , ..., C a k }, so that each publication within a cluster is authored by the same person and every two publications in different clusters have two different authors who share the same name a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head><p>In this section, we first introduce the heterogeneous graph convolutional network embedding method which encodes the textual content of publications and graph structures of PH-Nets to learn the continuous distribution representations of publications. Then, we propose our clustering algorithm to partition the publications. At last, we introduce the incremental disambiguation strategies to disambiguate on new publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Publication Heterogeneous Network Embedding 1) Heterogenous Graph Convolutional Network Encoder:</head><p>In this section, we introduce a Heterogenous Graph Convolutional Network (HGCN) module that operate on local graph neighborhoods to generate embeddings for nodes in PHNet, this model and related graph neural network models can be understood as special cases of a simple differentiable messagepassing model.</p><p>Given a PHNet G = (V, E, R), we start with initialize node attributes, we use Doc2vec <ref type="bibr" target="#b14">[15]</ref> to encode the text information (i.e, title or abstract) of each publication p i ∈ V into a fixed length feature vector u (0) i . For each publication node, it has neighbors with maximal |R| types. Different from regular GCNs, we introduce relation-specific transformations that can aggregate the representations of neighbors under different types of relations. We define a multi-layer HGCN with following layer-wise convolutional rule for calculating the forward-pass update of each node p i :</p><formula xml:id="formula_3">u (l+1) i = ReLU   r∈R j∈N r i 1 c r ij u (l) j W (l) r  <label>(1)</label></formula><p>where u</p><formula xml:id="formula_4">(l) i ∈ R 1×m (l)</formula><p>is the hidden state of node p i in the l-th layer of the neural network and m (l) is the dimension of this layer's representation. ReLU (•) = max(0, •), N r i denotes the set of neighbors indices of node p i under the relation r ∈ R, c r ij = k r i k r j is a normalization constant for the edge (p i , p j ), where φ(p i , p j ) = r and k r i is the sum of the weights of relations connecting to p i and with type r. W (l) r is a layer-specific and relation-specific trainable weight matrix. To ensure that the presentation of a node at each layer also has impact on one's presentation at the next layer, we assume there are single-connections of each type r ∈ R with weight 1 for each node.</p><p>Then, we define L layers in the HGCN model, where the output of the previous layer is the input to the next layer, u (0) i is the input node attribute of 1st-layer. For each publication node p i , HGCN encode the text embedding u (0) i with its local neighborhoods on G to a representation u i , formulated as:</p><formula xml:id="formula_5">u i = u (L) i = θ(u (0) i , G)<label>(2)</label></formula><p>where θ is the parameters in HGCN. In each HGCN convolutional layer, each node receives information of onehop neighbors to update its representation, the information of each node's neighbors under the same relation type share same transformation parameters. When multiple HGCN layers are stacked, the information of one-hop neighbors that each node receives has already includes their neighborhood information through last convolutional layer, with L layers a node can receives information from neighbors across multiple relational types at most L hops away.</p><p>2) Sampling Paths via Meta-path and Relation Weight Guided Random Walks: Our task is to train the HGCN model so that it can encode each publication node in PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Metapath2Vec <ref type="bibr" target="#b16">[17]</ref> which use a random walk strategy and the skip-gram model to learning node representation in network. First, we propose a meta-path and relation weight guided random walk strategy to sampling paths on weighted heterogenous network.</p><p>The meta-path walks can capture correlation of nodes through heterogenous relations and widely used in heterogenous network embedding. Besides, We take the weights of relations in the PHNet into account while sampling paths. Intuitively, the larger value of a relation between two nodes, the more similarity of them. At each step, when the walker is walking to a neighbor, the higher value of a relation which connects the current node to a neighbor node, the more likely it is that this neighbor should be sampled.</p><p>Specifically, given a PHNet G = (V, E, R), the meta-path scheme P is denoted in the form of P = {p t r1 − → p t+1 r2 − → ... r l − → p t+l+1 }, where p t ∈ V , r i ∈ R and l is the length of the composite relations between p t and p t+l+1 . The transition probability when the walker at step t is defined as follows:</p><formula xml:id="formula_6">p(p t+1 |p t , P) =      |(p t ,p t+1 )| ||E t (ri)|| (p t , p t+1 ) ∈ E, φ(p t , p t+1 ) = r i 0 (p t , p t+1 ) ∈ E, φ(p t , p t+1 ) = r i 0 (p t , p t+1 ) / ∈ E</formula><p>(3) where r i ∈ R is the next relation type of φ(p t−1 , p t ) according to the predefined scheme P, and if p t is the first node of the path, r i = r 1 , |(p t , p t+1 )| means the weight of the relation between p t and p t+1 , E t (r i ) denotes the set of relations which connect p t and those whose types are r i . In addition, meta-path is recurrently sampled in G to generate paths. Specifically, first, we successively select one publication node in the PHNet as the first node of the path and generate a meta-path whose length is l, and then choose the last node as the first node of another meta-path. In this way, each random work recursively samples nodes in the network to generate a long path guided by P until it meets the fixed length.</p><formula xml:id="formula_7">Taking the P = {p t CoAuthor −−−−−−→ p t+1 CoT itle − −−−− → p t+2 CoV enue − −−−−− → p t+3 } (abbreviated as "-A-T-V-"</formula><p>) as example, as shown in Figure <ref type="figure" target="#fig_1">1</ref>(a), if the walker is sampling a next node, the current node is p 1 , and the next relation type guided by P is CoAuthor, then all the nodes connected with p 1 with a CoAuthor type relation form the candidate sample set. Assuming that p 2 and p 6 are the only two nodes in this set, according to the value of the CoAuthor type relation they connect with p 1 , the probability of the walker sampling node p 2 is 1/3, and that of node p 6 is 2/3. A demo of path sampling is illustrated in Figure <ref type="figure" target="#fig_3">1(b)</ref>, where the thickness of the arrow represents the probability of walking to the next node. In some cases, the walker meets a relation which is missing for the current node, then it skips this relation and walks to the next relation in the meta-path when this happens.</p><p>Compared with random walk, this meta-path guided random walk strategy avoids the biased influence caused by central nodes and deviation of different relation quantities via the ordered guidance of meta-path <ref type="bibr" target="#b16">[17]</ref>. Besides, our designed random walk strategy considers the multiple possibilities of arrangement of a path, the multiple kinds of relationships between nodes in the path as well as the weight of the relations, which makes the generated paths can well preserve the heterogeneous relationships between nodes.</p><p>3) Weighted Heterogeneous Skip-gram Model: Based on the paths generated in the previous section, we proposed a Weighted Heterogeneous Skip-Gram (WHSG) model to learn the representations of publications on the weighted heterogeneous network. Given a PHNet G = (V, E, R), we aim to learn effective node representations by maximizing probability of any node p i having its neighbor node p c :</p><formula xml:id="formula_8">arg max θ pi∈V r∈R pc∈Nr(pi) |(p c , p i )| log p(p c |p i , θ)<label>(4)</label></formula><p>where N r (p i ) is a set of neighbors of p i , and </p><formula xml:id="formula_9">∀p c ∈ N r (p i ), (p c , p i ) ∈ r,</formula><formula xml:id="formula_10">p(p c |p i , θ) = exp(u i • u c ) |D| j=1 exp(u i • u j )<label>(5)</label></formula><p>where u i represents the embedding vector of p i encoded from its initial features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in <ref type="bibr" target="#b17">[18]</ref> to sample negative nodes to increase the optimization efficiency. Then, the probability can be approximately defined as:</p><formula xml:id="formula_11">log p(p c |p i , θ) ≈ log σ(u i • u c ) + pj ∈D i neg log σ(−u i • u j ) (6)</formula><p>where σ(x) = 1 1+e −x is the sigmoid function and D i neg is a negative node set for p i sampled from a pre-computed node frequency distribution in paths.</p><p>Given the meta-path scheme P and relation weight in the network, we can generate a set of collected random walks RW P by choosing different nodes as the start nodes of paths. The frequency of a neighbor appears in the context of the target node is in proportion to how close the relationship between them. so maximizing the objective in eq.4 is approximately equivalent to minimizing the loss function:</p><formula xml:id="formula_12">L P = − w∈RW P pi∈w pc∈C k i log p(p c |p i , θ) + λ θ 2 (7)</formula><p>where k is the windows size of context, C k i is the context set containing the previous k context nodes and next k context nodes of p i in the walk w. In the generated random walks, the nodes in context set with different windows size represents different kinds of neighbors. For example, when k = 2, the context set contains the 1-hop and 2-hop neighbors. Parameter λ controls penalty of regularization for over-fitting.</p><p>The framework of our proposed network embedding method is illustrated in Figure <ref type="figure" target="#fig_4">2</ref>(a). Our method is similar with many meta-path and skip-gram based heterogeneous network embedding models, comparing with these models, the WHSG model has these main differences: (1) our model can be applied on the weighted heterogenous network and can capture both the relation weight information and heterogenous relations between nodes to generate node representations, (2) and more importantly, the representations are encoded by HGCN propagation model for integrating publication semantic information and local heterogenous graph structure information.</p><p>Finally, we use a mini-batch Adam optimizer to minimize L P . Our model is very efficient, all the parameters of our model are the transformation matrices {W (l) r } in each layer, where r ∈ R and l ∈ {0, ..., L−1}, and the sizes of parameters are independent with the size of publication set. Besides, we use the alias table method <ref type="bibr" target="#b18">[19]</ref> to sample neighbors and negative nodes, which only takes O(1) time when repeatedly sampling nodes from the same discrete distribution. In this paper, we set the number of layers L as 2 and the output dimension of all convolutional layers to be equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clustering Algorithm</head><p>For each author name to be disambiguated, we learn its publications' representations by the network embedding model in Section III-A. Then, clustering algorithms are used to assign publications into disjoint clusters, each belonging to a distinct author entities (shown in Figure <ref type="figure" target="#fig_4">2(b)</ref>). We find that the publication number of each author is skews, which means most of publications belong to a few dominant authors. Hierarchical Agglomerative Clustering (HAC) method works well for skewed data and is widely in many name disambiguation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the HAC has the following drawbacks: its time complexity is high compared with some other clustering methods and it needs to take the number of clusters K as input while determining the number of clusters K is usually a Gordian knot. To address those problems, we propose an efficient clustering strategy without any pre-set parameter called Graph-enhanced Hierarchical Agglomerative Clustering (GHAC).</p><p>For each ambiguous name a, given its PHNet G = (V, E, R) and its publication embeddings, we firstly reconstruct a homogenous publication graph G r = (V, E r ). Each node in G r represent a publication of a, and the weight of the edge e ij ∈ E r between node p i and node p j is formulated as:</p><formula xml:id="formula_13">|e ij | = σ(u i • u j ) • δ((p i , p j ) ∈ E)<label>(8)</label></formula><p>Where σ(•) is the sigmoid function, δ(x) is 1 if x is true and 0 otherwise. So the range of edge weight in G r is (0, 1), We apply the idea of HAC to this weighted homogenous graph G r . The similarity between two clusters is defined as the average of the weights of all their adjacent edges in G r . Firstly, each sample is treated as one single cluster, then the two closest clusters (with biggest similarity) are merged in each step until the number of clusters reaches the specified K.</p><p>When K is unknown, we adopt an optimal modularity <ref type="bibr" target="#b21">[22]</ref> partitioning mechanism to determine the partition of publications. The modularity M of a partition of nodes on graph G r is defined as follows:</p><formula xml:id="formula_14">M = 1 2m pi,pj ∈V |e ij | − w i w j 2m δ(c i = c j )<label>(9)</label></formula><p>Where m = 1 2 pi,pj ∈V |e ij |, which is the sum of weights of all edges in G r , w i = pj ∈V |e ij | representing the sum of weights of all edges connecting to node p i , c i represents the cluster which p i belongs to. Specifically, in GHAC, the modularity M of G a under current partition is calculated after each cluster merging process until there is no edge among clusters, then we choose the partition which achieves largest M as the final clustering result.</p><p>Reconstructing G r takes O(|E r |) time (where |E r | &lt; |E|). For each clustering iteration, to find the two closest clusters takes a maximum of |E r | times adding or comparing operations, so the computational complexity of GHAC when specifying K is O((|V | − K)|E r |). When K is unknown, the value of modularity M at each clustering iteration can be calculated based on the value of which at last iteration because agglomerating clusters just increases the terms in eq.9 at each iteration. So the computational complexity of modularity in the whole clustering process is O(|V |</p><p>2 ), and the computational complexity of GHAC is O(</p><formula xml:id="formula_15">(|V | + |E r |)|V |).</formula><p>It is worth mentioning that there are mainly scalar operations in our algorithm. Compared with traditional clustering algorithm, GHAC introduces topological structure of graph to avoid the distance calculations (mainly vector operations) between disconnecting publications to accelerate the clustering process. To further evaluate the efficiency of GHAC, we also implement sufficient experiments on it in Section IV-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Incremental Disambiguation</head><p>Our designed disambiguation model can efficiently deal with new publication records. Suppose a name a that needs to be disambiguated, we learn the presentations of its m existing publications in the set D a = {p 1 , p 2 , ..., p m } and get the clustering result C a = {c 1 , c 2 , ..., c k } of these publications. Suppose there comes m new publications D a = {p m+1 , p m+2 , ..., p m+m } having the author name a which also need to be assigned to distinct authors. First, we encode the text information of each publication p i in D a into a semantic embedding u (0) i . Then, we use the learned network embedding model to generate the publication representations in D a and update those in D a , There are two strategies:</p><p>Real-time. We take the new-coming publications as nodes and add them into the pre-constructed PHNet and add relations according to the definition 3. Based on the new PHNet G * and the pre-learned parameters θ, we can get the representations of publications:</p><formula xml:id="formula_16">u i = θ(u (0) i , G * ), pi ∈ {D a , D a }<label>(10)</label></formula><p>Incremental training. In this strategy, after updating the PHNet, we sample a few new paths by successively selecting each new publication node in the PHNet as the initial node to generate new path (the number of which is positive correlation to m ) via the random walk strategy introduced in section III-A2, then we use these new paths to train our network embedding model to update the parameters θ. By using updated θ * , the new publication representations are expressed as:</p><formula xml:id="formula_17">u i = θ * (u (0) i , G * ), pi ∈ {D a , D a }<label>(11)</label></formula><p>Unlike other network embedding methods, the parameters in our methods are independent of the size of the training dataset, so it is unnecessary to retrain a new model when new publications are introduced. These two publication representation updating strategies can be alternately used in practice. If a small number of publication are added at a time, we use the real-time strategy to update the publications' representations. If the number of new publications is greater than a predetermined threshold, we can sample new paths on the updated PHNet to incrementally train our model.</p><p>The network G r is reconstructed after updating the publication representations. We add new publication nodes, edges between new nodes as well as edges between new nodes and existing nodes to G r according to eq.8. During the incremental clustering procedure, each publication in D a is treated as one single cluster. We combine them with the clustering result C a of D a as the initial clustering input of GHAC, then clusters are merged in each step to generate new clustering results. After clustering, new publications are partitioned into the existing clusters or new clusters, each represents a different person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We perform several experiments to validate the performance of our proposed author name disambiguation method on two datasets. The results demonstrate the superiority of our proposed method over several state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To evaluate the proposed method, we use two datasets from Aminer<ref type="foot" target="#foot_0">1</ref> and CiteSeeX<ref type="foot" target="#foot_1">2</ref> , which are two widely used benchmarks in author name disambiguation. These datasets both contain several ambiguous author name references with respective label for each person entity. The Aminer disambiguation dataset contains 110 ambiguous names, 1515 distinct authors and 7022 publications. CiteSeeX contains 14 ambiguous names, 468 distinct authors and 8453 publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison Methods</head><p>To validate the performance of our proposed method, we compare our method against several different methods of name disambiguation. The brief descriptions of these baseline methods are as follows: Component: This method simply partitions each PHNet into connected components to generate the clustering results for each name to be disambiguated. Zhang et al. <ref type="bibr" target="#b0">[1]</ref>: This method uses a global metric learning and local linkage learning based on a graph auto-encoder method to learn the publications embeddings, then it propose an end-to-end model to estimate the number of clusters using a recurrent neural network and use HAC to determine the assignment of publications. Xu et al. <ref type="bibr" target="#b3">[4]</ref>: For an ambiguous name, this method build several publication networks for different kinds of relations, then use a merge strategy to coarsen networks. Publications embeddings are learned by a novel network embedding method, the final result of clustering is determined adaptively based on the results of HDBSCAN and AP clustering algorithm. Zhang et al. <ref type="bibr" target="#b4">[5]</ref>: This method constructs three different networks on relational data for each ambiguous name's publication set, and use a network embedding method to learn the publication embeddings, then use a HAC algorithm to cluster publications to different person entity.</p><p>As our framework is mainly based on a heterogeneous network embedding method, we design some author disambiguation methods based on several network embedding model. For a fair comparison, We use them to learn publication representations on the networks with same construction as PHNet, and use HAC to generate the clustering results: DeepWalk <ref type="bibr" target="#b15">[16]</ref>: DeepWalk is a network embedding method based on random walks to learn latent node representations and it is only applicable for homogeneous unweighted network. LINE <ref type="bibr" target="#b22">[23]</ref>: LINE can preserve both the first-order and secondorder proximities of nodes and is applicable for homogeneous weighted network. Metapath2Vec <ref type="bibr" target="#b16">[17]</ref>: Metapath2Vec is applicable for heterogeneous network with binary edges. It uses meta-path-based random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. Hin2Vec <ref type="bibr" target="#b23">[24]</ref>: Hin2Vec is also an unweighted heterogeneous network embedding method that can capture rich semantic of relationships and the details of network structure to learn representations of nodes. GraphSAGE <ref type="bibr" target="#b24">[25]</ref>: GraphSAGE learn node embeddings through different aggregation function form a node local neighborhood, which is similar with our designed HGCN layer, but the difference is the unsupervised loss function and it can not be used for heterogeneous network. We use the GCN module as the aggregator of GraphSAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Settings</head><p>Only three features(title, author, and venue) of publications are used in each method, so the performance of some baselines maybe a little lower than that demonstrated in their own paper. These comparison methods use different kinds of cluster strategy. For example, <ref type="bibr" target="#b4">[5]</ref> need to specify the number of distinct author, <ref type="bibr" target="#b0">[1]</ref> need labeled data to estimate the number. For a fair comparison, we assume the number of clusters is set to real value and choose HAC as the clustering method of <ref type="bibr" target="#b3">[4]</ref>. We use pairwise F1-score <ref type="bibr" target="#b13">[14]</ref> to evaluate the clustering results of our method and the compared ones. We also calculate the Macro-F1 score on each dataset, which is the average of F1 scores of disambiguation results on all ambiguous names. We run all experiments on a desktop computer with Inter Core i7-8700U CPU (3.2 GHz) and 16GB memory.</p><p>There are a few tunable parameters in our method: The learning rate of our network embedding method is 10 −3 , the regularization coefficient λ is 10 −4 . We also set the windows size of context k in weighted heterogeneous skip-gram mode as 2, the meta-path scheme we used is "-A-V-A-T-". We set the dimension of presentation vectors learned by all methods to be 64. The parameter setting for other methods are based on default values or the values specified in their own papers. All the experiments are repeated many times to make sure the results can reflect the performances of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>Table <ref type="table">I</ref> and II show performance between our proposed method and compared methods on two datasets. For example, as shown in Table <ref type="table">I</ref> belongs to 6 distinct real-life authors with one author has 89% publications of all their publications, our method achieves 0.982 F1 score on it. It proves that our method can handle well the imbalance distribution of publications(46% authers have written only one publication in their entire career <ref type="bibr" target="#b25">[26]</ref>, whereas some may have written more than 100 ones). The Macro-F1 score of methods' results on all names in each dataset(shown in last row) indicates that our method significantly outperforms all the baselines(+9.9-39.6% in Aminer, +8.0-62.7% in Cite-SeeX). Paired t-test shows that all improvements are significant at the 0.05 level. The reason why our proposed method has better performance is that our method is able to learn high quality publication representations by our HGCN based heterogenous network embedding method. Among the compared methods, both Zhang et al. <ref type="bibr" target="#b4">[5]</ref> and Xu et al. construct several graphs based on various relationships between publications, and use network embedding based methods to learn representations of publications. However, Zhang et al. <ref type="bibr" target="#b4">[5]</ref> ignore the text information, and Xu et al. just use the word co-occurrence information of text and loss a certain amount of semantic information. Zhang et al. <ref type="bibr" target="#b0">[1]</ref> also use a graph convolutional network based encoder-decoder model but on homogeneous graph that can not extract multi-layer relationship that contains various relation types. However, our designed network embedding model can integrate publication attributes and heterogeneous relationship between publications into embeddings.</p><p>In network embedding, The great superiority of our method to DeepWalk and Metapath2Vec indicates that our designed relation weight and meta-path guided random walks brings large benefit to skip-gram and random walk based network embedding model. LINE does not consider the types of relation in the PHNet, Metapath2Vec and Hin2Vec does not consider different relation weights, while our model focus on learning latent embeddings on weighted heterogeneous networks, it has the ability to integrate these two features of network into effective embeddings. Our method is superior to GraphSAGE which proves that our heterogeneous graph convolutional network aggragator and loss function is more suitable for author disambiguation task.</p><p>Figure <ref type="figure">3</ref> shows the spatial distribution of publications on the embedding spaces learned by our complete method and the performance of our designed clustering strategy. For each name, we can easily observe that in Figure <ref type="figure">3(a,b,c</ref>), publications from different clusters are well separated in embedding space. Figure <ref type="figure">3(d,e,f</ref>) demonstrates the clustering result generated by GHAC method, most of the publications points are assigned into the correct clusters. It proves that in such embedding space, our designed GHAC is very advantageous on the clustering task and can achieve great performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Meta Path Analysis</head><p>To understand the impacts of different meta-path schemes on the network embedding model's performance, we design four kinds of meta-path scheme: "-A-V-","-V-T-","-A-V-T-" and "-A-V-A-T-" and compare them with pure random walk. Then, we use five different random walk strategies respectively to generate paths to train our network embedding model, and From the experiment results in terms of Macro-F1 on two datasets in Figure <ref type="figure" target="#fig_6">4</ref>, we can observe that using these four meta-paths to guide the random walk will achieve better performance than not using meta-paths, and using the metapath "-A-V-A-T-" can obtain relative higher macro-F1 than using other three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Clustering Algorithm Analysis</head><p>As our clustering method is suitable for either the cases of knowing the number of clusters or not, we compare the performance of GHAC with HAC, K-means, Gaussians mixture model (GMM) and Spectral Clustering (SC) by specifying K. We also compare GHAC with Affinity Propagation (AP), DBSCAN, X-means, and Mean-Shift (MS), which does not require the number of clusters.</p><p>Figure <ref type="figure">5</ref> shows average running time required for clustering the publications by each method. GHAC is significantly superior to most methods because it only compute the similarity between publication that has edges in the reconstructed graph, which is usually sparse in a real-life scenario.</p><p>As shown in Figure <ref type="figure">6</ref>(a), the Macro-F1 of GHAC's clus- tering result on Aminer dataset is higher than all the comparison methods, which indicates that the idea of hierarchical agglomerating is effective in solving the skewed publication data. Our method is slightly higher than HAC, which indicates that ignoring the relationship information among publications that has no edge connections in the reconstructed graph can not only speed up the clustering process, but also improve the clustering results. Figure <ref type="figure">6</ref>(b) shows that our method outperforms all other methods when the number of clusters is unknown. The reason is that using optimal modularity partitioning mechanism can well extract the community relations among publications from the reconstructed graph and find the better partition result of publications. We also compared our method with others in predicting the number of distinct authors when K is unknown. As shown in Table <ref type="table">III</ref>, our method achieves a MSLE of 0.206 and an accuracy of 21.1%, which means that our estimated Ks are within a reasonable error range for most names. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Incremental Disambiguation Analysis</head><p>There are two strategies on incremental disambiguation in our framework: "Real-time" and "Incremental training". To evaluate them, we first choose the ambiguous names with over 100 publications in two datasets, and equally split each name's publications into 5 batches. We randomly select one batch as the existing publications and use our framework to get its partition result. Then we use two incremental disambiguation strategies to respectively process each of the rest batches in the sequential order of arrival.</p><p>Figure <ref type="figure" target="#fig_8">7</ref>(a) and Figure <ref type="figure" target="#fig_8">7</ref>(b) show the performances of two incremental strategies on two datasets, the mean and standard deviation were computed at each update. We can observe that with the arrive of each new publications batch, in real-time updating strategy, the macro-F1 scores are slightly decreasing on two datasets, and in incremental training the scores maintain at a high level after 5 batches. These results prove that for a small set of new publications, real-time updating is effective way to disambiguate as it is more time-saving than incremental training, and the incremental training strategy performs better when conducted on large new publication dataset.</p><p>V. RELATED WORK Name Disambiguation. To solve name disambiguation problem, most of existing researches select various features from digital libraries and use them to quantify the similarity of publications, then cluster them into disjoint clusters, each of the which belongs to a distinct person. <ref type="bibr" target="#b7">[8]</ref> propose a Markov random fields based framework to extract multiple types of characteristics and relations in publication database. <ref type="bibr" target="#b0">[1]</ref> use a global metric learning and local linkage graph auto-encoder algorithm to learn the representation of publications, but it requires lots of human labeled data to train the model. <ref type="bibr" target="#b8">[9]</ref> propose a hierarchical agglomerative clustering based approach using the coauthor and title attributes. Other works attempt to utilize graph topology and linkage to solve this problem.</p><p>[3] construct a publication network only by co-author relation and devise a novel similarity metric. Then they use affinity propagation clustering algorithm to group result into clusters. <ref type="bibr" target="#b12">[13]</ref> introduce a pairwise factor graph model which can be extended by incorporation various features. The vast majority of name disambiguation solutions are conducted on static datasets. To disambiguate the new introduced publications in DLs, some works design incremental name disambiguation methods, <ref type="bibr" target="#b26">[27]</ref> propose a probabilistic model that use a rich set of metadata and classifies new publications to existing author entities. <ref type="bibr" target="#b27">[28]</ref> combines several domain-specific heuristics in order to automatically create and update publication clusters and determine the author of each publication.</p><p>Network Embedding. Recently, there has been a growing interest in the network embedding technology. DeepWalk <ref type="bibr" target="#b15">[16]</ref> and Node2Vec <ref type="bibr" target="#b28">[29]</ref> use random walk strategy on network and skip-gram <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> model to learn the representation of each node in network. However, when they are applied on heterogeneous networks, such random walks ignore the types of relations, are biased by highly visible relation types and concentrated nodes, and generate incorporated node paths <ref type="bibr" target="#b31">[32]</ref>. Metapath2Vec <ref type="bibr" target="#b16">[17]</ref> proposes an embedding method on heterogeneous network based on meta-path. Some other methods have offered different solutions to network embedding. LINE <ref type="bibr" target="#b22">[23]</ref> aims to learn the node embedding that preserve both first-order and second-order proximities. Graph neural network (GNN) based methods that applies deep neural networks on graph-structured data are significant developed in recent years, GNNs can also be used as a node encoder to learn network embeddings. DNGR <ref type="bibr" target="#b32">[33]</ref> proposes a deep neural networks based method to learn a low-dimensional vector representation for each vertex by capturing the graph structural information. GCN <ref type="bibr" target="#b33">[34]</ref> and GraphSage <ref type="bibr" target="#b24">[25]</ref> use graph convolution neural networks to obtain node representations. <ref type="bibr" target="#b34">[35]</ref> introduces a relational graph convolutional network to link prediction task and entity classification task. <ref type="bibr" target="#b35">[36]</ref> propose a heterogeneous graph neural network model which considers both types and heterogeneous attributes of nodes. There are a large amount of works on name disambiguation that attempt to use network embedding methods to learn publication embeddings by constructing publication networks. <ref type="bibr" target="#b4">[5]</ref> utilize a network representation learning based approach on three anonymized network, <ref type="bibr" target="#b3">[4]</ref> construct five relationship networks among publications and use a network embedding algorithm to learn representations of publications via their neighbors, <ref type="bibr" target="#b20">[21]</ref> introduce a simple random walk strategy and a graph embedding method on a homogeneous network and use HAC to partition the publications. However, these methods lacks deeper model to learn publication representations. To the best of our knowledge, there has been no heterogeneous graph neural network embedding based methods implemented on name disambiguation problem so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose an effective framework to address the author disambiguation problem. Our framework consists of a novel heterogeneous graph convolutional network embedding method to learn publication representations, an efficient clustering method to determine the partition of publications, and two incremental disambiguation strategies for new introduced publications. Experiments on two datasets show that our pipeline can learn publication representations with higher quality than state-of-the-art methods, the clustering strategy is better than baselines in name disambiguation task, and the incremental disambiguation is effective. In our future work, we will try to apply our framework on distributed computing system to further improve the disambiguation speed on large academic database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>associated with a node type mapping functions φ(v) : V → N and each relation e ∈ E is associated with a relation type mapping function φ(e) : E → R. This suggests that G has multiple node types and relation types, formulated as |N | + |R| &gt; 2. Definition 2 (Weighted Heterogeneous Network). A weighted heterogeneous network G w = (V, E, N , R) is a heterogeneous network in which each relation has a weight. For a relation e ∈ E, its value can be expressed as |e|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of (a) a publication heterogeneous network with one node type, three relation types, and (b) paths sampled by meta-path (r = CoAuthor•CoT itle•CoV enue) and relation weight guided random walks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 r1− → n 2 r2−</head><label>12</label><figDesc>−− → p 3 between p 1 and p 3 , which means p 3 shares some title words with another publication written by one of p 1 's authors. Definition 4 (Meta-path). In a heterogeneous network G = (V, E, N , R), a meta-path is defined as a path in the form of n → ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 CoAuthor=1−</head><label>1</label><figDesc>r l − → n l+1 , where r i ∈ R and n i ∈ N , the sequence of relations r = r 1 • r 2 • ... • r l represents the composite relation including relations from type r 1 to type r l . For example, we can design r = CoAuthor • CoT itle • CoV enue as the compositional relation of a meta-path. Then, in the PHNet, as shown in Figure 1(b), a path p −−−−− → p 4 is generated based on r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of disambiguating the author name a. This framework consists of (a) the publication heterogeneous network embedding method to learn publication representations and (b) the graph-enhanced hierarchical agglomerative clustering method to partition the publications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>θ represents the parameters of the HGCN model, The relation weight |(p c , p i )| in this function is a weight to make those neighbors who has large relation value with p i have higher probability output. p(p c |p i , θ) is defined as a softmax function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 3. t-SNE Visualization of embedding spaces on publications of three different names (D Johnson, M Miller, M Brown). Each color in (a), (b), (c) denotes an ground truth cluster which contains publications of a distinct author entity, while each color in (d), (e), (f) denotes a predicted cluster generated by our graph enhanced hierarchical agglomerative clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 . 6 .</head><label>56</label><figDesc>The average running time of different clustering methods on Aminer dataset. The performance of different clustering methods on Aminer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The performance of two incremental disambiguation strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1-7281-0858-2/19/$31.00 © 2019 IEEE 2019 IEEE International Conference on Big Data (Big Data)</figDesc><table /><note>978-1-7281-0858-2/19/$31.00 ©2019 IEEE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>there is a CoAuthor relation between p a 1 and p a 2 , and the weight of the CoAuthor relation is |A 1 ∩ A 2 |.</figDesc><table><row><cell>CoVenue If two publications are published at the same</cell></row><row><cell>conference or journal, then we create a CoVenue relation</cell></row><row><cell>between them. Usually a publication can only belong to one</cell></row><row><cell>venue. Therefore, if the CoVenue relation exist, its value is</cell></row><row><cell>defined as 1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, considering the name "Jie Tang" which</figDesc><table><row><cell>Avg.</cell><cell>0.698</cell><cell>0.288</cell><cell>0.538</cell><cell>0.646</cell><cell>0.561</cell><cell>0.429</cell><cell>0.507</cell><cell>0.547</cell><cell>0.563</cell><cell>0.523</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.aminer.cn/disambiguation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://clgiles.ist.psu.edu/data/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Author name disambiguation by using deep neural network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACIIDS</title>
		<imprint>
			<biblScope unit="page" from="123" to="132" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On graph-based name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JDIQ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A network-embedding based method for author disambiguation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1735" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the combination of domain-specific heuristics for author name disambiguation: the nearest cluster method</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc ¸alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JODL</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="246" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective string processing and matching for author disambiguation</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3037" to="3064" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel approach for author name disambiguation using ranking confidence</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DASFAA</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic author name disambiguation for growing digital libraries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="412" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCDL</title>
		<imprint>
			<biblScope unit="page" from="296" to="305" />
			<date type="published" when="2004">2004</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adana: Active name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of author name disambiguation techniques: 2010-2016</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asghar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the sampling complexity of topic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multi-level author name disambiguation algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xinhua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Author name disambiguation using graph node embedding method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 23rd International Conference on Computer Supported Cooperative Work in Design (CSCWD)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Author name disambiguation in medline</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Torvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Smalheiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental author name disambiguation for scientific citation data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rollins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="175" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental author name disambiguation by exploiting domain-specific heuristics</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc ¸alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="931" to="945" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">word2vec parameter learning explained</title>
		<author>
			<persName><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2738</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
