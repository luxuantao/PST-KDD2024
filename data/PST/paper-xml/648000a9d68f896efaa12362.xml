<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining and Adapting Graph Conditional Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-05">5 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
							<email>yizhuj2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
							<email>?nponomareva@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining and Adapting Graph Conditional Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-05">5 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.03256v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have shown remarkable performance on graphstructured data. However, recent empirical studies suggest that GNNs are very susceptible to distribution shift. There is still significant ambiguity about why graph-based models seem more vulnerable to these shifts. In this work we provide a thorough theoretical analysis on it by quantifying the magnitude of conditional shift 1 between the input features and the output label. Our findings show that both graph heterophily and model architecture exacerbate conditional shifts, leading to performance degradation. To address this, we propose an approach that involves estimating and minimizing the conditional shift for unsupervised domain adaptation on graphs. In our controlled synthetic experiments, our algorithm demonstrates robustness towards distribution shift, resulting in up to 10% absolute ROC AUC improvement versus the second-best algorithm. Furthermore, comprehensive experiments on both node classification and graph classification show its robust performance under various distribution shifts.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10]</ref> are powerful tools that have showed excellent performance on graph structured data. Interestingly, recent work has revealed that GNNs shows a susceptibility to performance degradation when confronted with data distribution shift, where the data used for training (source data) and inference (target data) come from different distributions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. Consequently, there has been a growing interest in investigating the behavior of GNNs under distribution shift, which demonstrate that both shifts in graph structure and node features can lead to a deterioration in GNN performance <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. However, these works are primarily empirical, and there are still many open questions about both the nature of this susceptibility, as well as how to address it effectively.</p><p>At the same time, extensive research has been conducted to examine the behavior of conventional machine learning models (excluding GNNs) in the presence of domain shift. Two prominent settings that have received significant attention are Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG). When unlabeled target data is available, common UDA approaches including learning Domain Invariant Representation Learning (DIRL) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref> attempts to align latent representations of source and target data. Another approach, Domain Generalization (DG) <ref type="bibr" target="#b0">[1]</ref>, addresses the challenge of training models that can generalize effectively to unseen target domains by leveraging multiple source domains during training.</p><p>In this paper, our focus is on node classification <ref type="bibr" target="#b18">[19]</ref>, a fundamental task in GNNs, where the effectiveness of DIRL methods <ref type="bibr" target="#b13">[14]</ref> has been found to be limited <ref type="bibr" target="#b45">[46]</ref>. To address this limitation, we begin with investigating the underlying distribution of latent representations generated by GNNs. Remarkably, we discover that GNNs can exacerbate the conditional shift (P s (y|h) ? = P t (y|h)), thereby challenging the validity of "no conditional shift" assumption (P s (y|h) ? P t (y|h)) in DIRL methods. In Section 3.2, we provide a thorough theoretical analysis by quantifying the magnitude of conditional shift. Through our investigation into various graph characteristics, we observe that both graph heterophily <ref type="bibr" target="#b44">[45]</ref> and model architecture (specifically, graph convolutions <ref type="bibr" target="#b18">[19]</ref>) exacerbate conditional shifts. Our theoretical results then show that these shifts provably degrade the generalization capabilities of GNNs. Thus, mitigating the conditional shift is crucial for enhancing unsupervised domain adaptation on graphs.</p><p>Inspired by this understanding, we propose a graph conditional shift adaptation method, called GCONDA, to perform graph UDA. First, we estimate the conditional shift as Wasserstein distance W 1 between source label distribution P s (y|h) and estimated pseudo label distribution P t (?|h). Building upon our theoretical results, we incorporate the calculation and minimization of the estimated conditional shift between the source and unlabeled target batch into the training process. Notably, we enhance our approach by incorporating the distribution discrepancy of the latent representation (P s (h), P t (h)) into the estimation of W 1 , which we refer to as GCONDA ++. In Theorem 4.1, we discuss the generalization bound of GNNs with W 1 and the Lipschitz constant of GNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref>. Specifically, our theoretical and practical contributions are the following:</p><p>(i). Derivation of graph conditional shift and its implications. Using a CSBM model, we provide the first provable result (Theorem 3.1) quantifying how GNNs worsen conditional shift. Subsequent analysis (Corollary 3.1.1) identifies graph heterophily and graph convolutions as two contributing factors to the unsatisfactory performance of GNNs under distribution shifts. This finding (Corollary 3.1.2) further offers insights into the practical implications and applications of GNNs.</p><p>(ii). Graph UDA by minimizing conditional shift. Building upon our theoretical findings, we propose GCONDA, a graph UDA method that leverages the minimization of conditional shift. In practice, we observe a strong correlation between the estimated Wasserstein distance and the actual performance of the GNN model. In contrast, other latent representation distances that do not exhibit the same level of correlation (e.g. CMD <ref type="bibr" target="#b40">[41]</ref> in Figure <ref type="figure" target="#fig_3">2</ref>), (iii). Robustness towards different distribution shifts. On synthetic graphs, GCONDA demonstrates a substantial performance advantage over other DIRL baselines, with an absolute AUC_ROC improvement of up to 10%. In the node classification task, GCONDA consistently outperforms competing methods across six real-world datasets, demonstrating superior performance even under various types of shifts. Additionally, when applied to graph classification, our approach leads to performance improvements as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Domain Adaptation. The goal of UDA algorithms is to transfer knowledge from the source onto target data, obtaining good generalization on target distribution. In the theoretical foundational work of domain adaptation, <ref type="bibr" target="#b4">[5]</ref> presented an upper bound of target risk using the performance of the model on source data and introduced a domain discrepancy measure called H-divergence. Since then, many domain adaptation algorithms that minimize differences between source and target domains have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. For example, DANN <ref type="bibr" target="#b13">[14]</ref> achieves domain invariant learning (DIRL) by introducing an adversarial objective to distinguish source and target samples in the latent space. Conditional DANN work -CDAN <ref type="bibr" target="#b24">[25]</ref> -incorporates classifier predictions into the adversarial head, either via linear or multilinear conditioning, further improving UDA performance. Besides, some other work propose to match the distribution in the latent space through probability discrepancy measures like MMD <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> and CMD <ref type="bibr" target="#b40">[41]</ref>. In a recent study <ref type="bibr" target="#b43">[44]</ref>, it was demonstrated that existing methods for UDA suffer from poor generalization when there is variation in the conditional probability P(y|x) across domains. In response to this challenge, Wasserstein distance on joint <ref type="bibr" target="#b11">[12]</ref> or label distribution <ref type="bibr" target="#b22">[23]</ref> are proposed to guide the mapping between source and target samples using optimal transport. Graph Domain Adaptation. Graph Representation Learning introduces new out-of-distribution (OOD) challenges based on the graph structure (including graph size <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>, molecular scaffolds <ref type="bibr" target="#b15">[16]</ref>). The first several studies <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9]</ref> adopted domain invariant learning across source and target graphs assuming covariate shift. On semi-supervised learning, SRGNN <ref type="bibr" target="#b45">[46]</ref> introduced a combination of instance weighting and DIRL techniques to enhance OOD generalization in the presence of localized training data. Other pioneering work tried to capture environment-invariant node properties <ref type="bibr" target="#b34">[35]</ref> and substructures <ref type="bibr" target="#b36">[37]</ref> guided by reinforcement learning based environment generators. In the meantime, theoretical analysis on the generalization bound of Graph Domain Adaptation (GDA) approaches is advancing. The Tree-mover's distance <ref type="bibr" target="#b10">[11]</ref> provided a model-agnostic generalization bound for GNNs when facing distribution shift. Additionally, the first model-based GDA bound <ref type="bibr" target="#b39">[40]</ref> proposed to optimize the Lipschitz constant of GNNs through spectral regularization.</p><p>Existing domain adaptation algorithms for GNNs primarily focused on enhancing model design to achieve improved empirical performance. Unlike all these methods, our work introduces a novel perspective -conditional shift to explain and mitigate the distribution shift for graph data.</p><p>3 Understanding Distribution Shift in GNNs 3.1 Background: Graph UDA Notations. A graph is described by a tuple G(V, A, X), where the nodes V are associated with their features X ? R |V |?d and the adjacency matrix A ? R |V |?|V | describes the connections between nodes. We denote Y , (Y ? Z |V |?|L| ), as labels for all nodes in graph G and x i , y i represent a single node's features and label (y i ? L). A Graph Neural Network g stacks several neural network layers which transform nodes and their neighborhood information into a latent representation g : (X, A) ? H. Each layer of a GNN can be described by:</p><formula xml:id="formula_0">H k = ?( ?H k-1 ? k ),<label>(1)</label></formula><p>where ? is a transformed adjacency matrix that is defined by a specific GNN method.</p><p>The task of node classification takes nodes features X and structure of the graph A to predict labels Y through a GNN encoder g and classifier f . Let the embedding h i be node i's representation calculated by the final activations of a GNN's output H. Then the task of binary node classification predicts the label using classifier f as follows,</p><formula xml:id="formula_1">f (h) = 1, if w T h + b &gt; 0 -1, otherwise<label>(2)</label></formula><p>Graph Unsupervised Domain Adaptation. Given a source and target graph G S (V s , A s , X s ) and G T (V t , A t , X t ), we assume embeddings h s and h t are output by the same GNN. The Unsupervised Domain Adaptation (UDA) algorithm utilizes labeled source {(h s , y s )} data and unlabeled target data {h t }. Let ? denote the expected risk of a binary classification problem defined above, then UDA aims to find a predictive classifier f and GNN g that achieves small target risk ? T (f ? g) on G T .</p><p>To quantify the discrepancy between source and target distributions ? S and ? T , we mainly use Wasserstein distance in this paper. In addition, we denote ? f as the conditional distribution P(y|h) and ? g as the representation distribution P(h). Definition 3.1 (Wasserstein distance). Wasserstein distance is defined between probability distributions ? S and ? T on metric space M, using distance function d, d :</p><formula xml:id="formula_2">M ? M ? R, W p (? S , ? T ) = inf ???(? S ,? T ) E (x,y)?? d(x, y) p 1/p , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where p is the moment of the distance and ? is is a joint probability measure on M ? M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-Invariant</head><p>Representations under Covariate Shift. Covariate shift refers to a change in the distribution of input features (covariates) between the source and target domains. Although labels are unavailable for the target data in UDA setting, DIRL methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b33">34]</ref> for GNNs instead optimize the following objective, assuming the covariate shift P s (y|h) = P t (y|h),</p><formula xml:id="formula_4">min f,g 1 N N i=1 L(y s i , h s i ) + ?W 1 (? g S , ? g S ),<label>(4)</label></formula><p>where h s i is the node representation from GNN's output H, ? g S := P s (h) and ? g T := P t (h) are the marginal distributions of the source and target graphs. The second term minimizes the discrepancy on H, which is known as learning a domain invariant representation. Besides Wasserstein distance <ref type="bibr" target="#b30">[31]</ref>, there are several other notable measures used in DIRL such as CMD <ref type="bibr" target="#b40">[41]</ref> and MMD <ref type="bibr" target="#b23">[24]</ref>.</p><p>Below we the give the formal definition of conditional shift. Definition 3.2 (Conditional Shift). Assume P s (y|x) and P t (y|x) have the same support on x, then conditional shift is defined as ? y|x = d(P s (y|x), P t (y|x))P t (x)dx, d : L ? L ? R + , y ? L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Shift in Graph Neural Networks</head><p>Now, we present theoretical findings on the occurrence of conditional shift in GNNs. Assuming the conditional shift does take place (e.g. covariate shift assumption does not hold), we explore the magnitude of this shift in the input space x and latent space h of GNNs. To quantify this shift, we use the terms ? y|h and ? y|x to represent the conditional shift in the latent space and input space, respectively. To analyze the conditional shift on different graph distributions, we use the CSBM <ref type="bibr" target="#b12">[13]</ref> graph model, an object of recent interest for understanding GNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3]</ref>.</p><formula xml:id="formula_5">Definition 3.3 (Contextual Stochastic Block Model (CSBM)). The CSBM graph is a tuple (A, X, Y ),</formula><p>where A is the node adjacency matrix, X are the nodes features and Y are the nodes labels {y 1 , ..., y n }. These node labels y i are random variables drawn from a Bernoulli distribution (Ber(0.5)), and control the connections between nodes in the graph. a ij ? Ber(p) if y i = y j and a ij ? Ber(q) otherwise. Features are drawn according to</p><formula xml:id="formula_6">x i = y i ? + Zi ? d , y i ? {-1, 1}, ? ? R d is the feature mean and Z i ? R d is a Gaussian random variable.</formula><p>The three parameters of CSBM are ?, p, and q. They respectively control the closeness of the two classes, the generated graph's edge density (e.g. average degree D) and its homophily ratio <ref type="foot" target="#foot_1">2</ref> . By manipulating ? and (p, q), it is possible to generate distribution shifts of varying magnitude in both node features and graph structure.</p><p>To estimate the conditional shift on target CSBM graph G T , we define ? y|x as,</p><formula xml:id="formula_7">? y|x = E x?Pt(x) I arg max y P s (y|x) ? = arg max y P t (y|x) ,<label>(5)</label></formula><p>Setting: The goal of this analysis is to investigate the conditions under which GNNs alleviate such shifts (making covariate shift more likely to hold), or exacerbate them. Here, we use a 1-layer Graph Convolutional Network <ref type="bibr" target="#b18">[19]</ref> as our GNN encoder g<ref type="foot" target="#foot_2">3</ref> . On a CSBM graph (?, p, q), the means of the two classes in the input space are (-?, ?), while in the latent space they are (-p-q p+q ?, p-q p+q ?). Without loss of generality, we assume the distribution shift on feature ? in G T is controlled by ? ? [0, 1], which moves centroids of both classes in the same direction, that is, (-(1 + ?)?, (1 -?)?).</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>, we illustrate how shifts in graph structure and node features can result in conditional shift. When the density or homophily ratio (D ? , p ? /q ? ) changes, the class centroid shifts to different positions, as depicted by <ref type="figure" target="#fig_5">1a</ref>. Similarly, if the Gaussian mean moves towards a different position (e.g. ? ? ? ? in Figure <ref type="figure" target="#fig_5">1b</ref>), it also contributes to the conditional shift. We beging by deriving the conditional shift and expected error in the following theorem: Theorem 3.1 (Conditional Shift in GNNs). Let the source graph G S = CSBM(?, p, q), and a target graph G T = CSBM(? ? , p ? , q ? ), where D and D ? represent their average degrees respectively. Additionally, let ?(?) denote the cumulative distribution function (CDF) of a multivariate Gaussian distribution defined by distance. Then the introduced distribution shift between G S and G T can be quantified via the estimated conditional shift of x and h as:</p><formula xml:id="formula_8">? D p-q p+q ? ? ? D ? p ? -q ? p ? +q ? ? in Figure</formula><formula xml:id="formula_9">? y|x = ? ((1 + ?)???) -?((1 -?)???) 2 , ? y|h = ?(?? ? h,-1 ?) -?(?? ? h,1 ?) 2 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">where ? ? h,1 = ? D ? p ? -q ? p ? +q ? ? - ? D ? ?? and ? ? h,-1 = ? D ? q ? -p ? p ? +q ? ? - ? D ? ??.</formula><p>Proof. See Appendix ?A.1. In the proof, we scale the GCN output of the target graph h ? into a standard Gaussian distribution. Then we can compute ? by comparing the relative position of the optimal classification hyperplane and mean of the Gaussian.</p><p>Supposing two graphs G 1 (?, p 1 , q 1 ), G 2 (?, p 2 , q 2 ) have the same feature distribution and edge density, G 1 is more heterophilous if it has more edges connecting nodes of different classes, that is p 1 &lt; p 2 , q 1 &gt; q 2 . Upon examining the magnitude of the conditional shift ? y|h in the two graphs, we find that</p><formula xml:id="formula_12">? ? h,-1 (G 1 ) &gt; ? ? h,-1 (G 2 ) and ? ? h,1 (G 1 ) &lt; ? ? h,1 (G 2 )</formula><p>. This inequality arises due to the fact that p1-q1 p1+q1 &lt; p2-q2 p2+q2 . In other words, Eq. ( <ref type="formula" target="#formula_9">6</ref>) shows that heterophilous graphs demonstrates a greater degree of conditional shift! Corollary 3.1.1 (GNNs exacerbate Conditional Shift). Assuming only homophily ratio changes p/q ? = p ? /q ? , the conditional shift is always exacerbated by the 1-layer GCN since ? y|x = 0. When there is only a feature shift ??, the shift will be amplified by the GCN as ? D??, potentially leading to larger conditional shifts.</p><p>Here, ? y|x is the conditional shift of a non-graph model (e.g. a multilayer perceptron) and ? y|h is the conditional shift of a GCN. Interestingly, Eq. ( <ref type="formula" target="#formula_9">6</ref> </p><formula xml:id="formula_13">? T (f ) = 1 - ?((1 + ?)???) + ?((1 -?)???) 2 , ? T (f ? g) = 1 - ?(?? ? h,-1 ?) + ?(?? ? h,1 ?) 2 .<label>(7)</label></formula><p>Together with the Corollary 3.1.1, we aim to validate the correlation between conditional shift and target error ? T . Therefore, we trained an MLP, a 1-layer GCN, and a 2-layer GCN on a source CSBM graph where GNNs achieves smaller (? S ? 0) than MLP (? S ? 0.2). During testing, we kept the graph density unchanged (i.e., D = D ? ), while increasing the heterophily ratio q ? /p ? or the deviation in feature mean ?. As shown in Figure <ref type="figure" target="#fig_5">1c</ref> and Figure <ref type="figure" target="#fig_5">1d</ref>, we observed (1) GCNs cannot separate the training data more accurately than MLP when shift is large (i.e., a larger ? T ); (2) the performance gap ? T -? S between the source and target is more pronounced in GCNs, confirming that conditional shift of GNNs leads to a larger performance drop. Having demonstrated that graph inductive bias often exacerbates conditional shift, our focus now turns to exploring potential mitigations of such shift during GNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph UDA by Minimizing Conditional Shift</head><p>In the previous section, we discussed the exacerbated conditional shift for GNNs and how they relate to the performance degradation. Now, we present our approach to mitigate this conditional shift, quantified using the Wasserstein distance, in order to achieve effective graph UDA.</p><p>We first introduce the formal definition of optimal transport used in Eq. ( <ref type="formula" target="#formula_2">3</ref>). Wasserstein distance can be computed as the optimal transport (OT) cost <ref type="bibr" target="#b28">[29]</ref> between two distributions. Let d(u i , v j ) be the distance between two sets of samples {u i } m i=1 and {v j } m j=1 drawn from ? S and ? T respectively. OT solves the following problem:</p><formula xml:id="formula_14">? * = arg min ???(?s,?t) i,j d(u i , v j )?(i, j)<label>(8)</label></formula><p>Specifically, ? is the set of transportation plans that satisfy</p><formula xml:id="formula_15">?(? s , ? t ) = {? ? R m?m + |?1 m = ? ? 1 m = 1 m }.</formula><p>To estimate the empirical conditional shift, we calculate the Wasserstein distance between source label y s and estimated target label ?t = f (g(x t )) as W 1 (? f S , ? f T ). Hereby we introduce the learning problem of unsupervised graph domain adaptation by minimizing conditional shift. Given source labeled data {(x s i , y s i )} N i=1 in G s and unlabeled target data {x t j } N j=1 in G t , we optimize the following loss function,</p><formula xml:id="formula_16">L GCONDA = 1 N i L CE (y s i , ?s i ) + ? W 1 (? f S , ? f T ),<label>(9)</label></formula><formula xml:id="formula_17">W 1 (? f S , ? f T ) = ij ? * ij ? d y s i , ?t j , d(?) = L CE (?)<label>(10)</label></formula><p>where ?i and ?j are predictions on the source and target data produced by the classifier f and GNN encoder g. L CE is the cross-entropy loss. The loss consists of (1) classification loss on G s ; (2) estimated conditional shift W 1 between source and target samples in the batch; ? * ? R N ?N is the optimal transportation plan between node i in source graph G s and j in target G t , ij ? * ij = 1. Besides matching the conditional distribution P(y|h), we propose to also mitigate the discrepancy marginal probability P(h) following ideas from non-GNN UDA works <ref type="bibr" target="#b11">[12]</ref> and call this variant GCONDA ++. We define the distance between source data (h s i , y s i ) and target data (h t j , ?t j ) as,</p><formula xml:id="formula_18">d (h s i , y s i ), (h t j , ?t j ) = ??h s i -h t j ? 2 + ?L CE (y s i , ?t j )<label>(11)</label></formula><p>where h i = g(x i ) is the output of a GNN. GCONDA ++ optimizes both the conditional and marginal distribution, that is,</p><formula xml:id="formula_19">W 1 (? f ?g S , ? f ?g T ).</formula><p>If we set ? = 0, our approach is equivalent to a DIRL method using optimal transport. In our experiments (i.e., Table <ref type="table" target="#tab_0">1</ref>), we confirm this by showing that GCONDA with ? = 0 yields similar results to DIRL baselines.</p><p>Generalization Bound of GCONDA. Next we show the relationship between the estimated conditional shift and the generalization error under distribution shifts. We achieve this by extending theoretical results from <ref type="bibr" target="#b11">[12]</ref>. Theorem 4.1. Suppose F is the hypothesis space of GNNs, ?f ? F,</p><formula xml:id="formula_20">? T (f ) ? ? S (f ) + W 1 (? f S , ? f T ) + ? * + K L K g ?(c),<label>(12)</label></formula><p>where ? * is the joint optimal error, K L is the Lipschitz constant loss function of loss function L, K g is the Lipschitz constant of GNN g and ?(c) is the probabilistic lipschitzness <ref type="bibr" target="#b5">[6]</ref>.</p><p>Proof. See Appendix ?A.2. Assuming a model can generalize well on source and target data (i.e. small ? * ), one can estimate the expected target error through OT cost W 1 and the Lipschitz constant K g of the GNN function. Furthermore, if practitioners aim to improve the generalization on target domain, they can either (1) employ an UDA algorithm (e.g. GCONDA) to minimize W 1 or (2) change the GNN architecture to the one with a smaller K g Lipschitz constant suggested by recent studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref>.  Note that the transportation cost term W 1 in our loss function L GCONDA is an empirical estimation of W 1 (? f S , ? f T ) in the bound. To examine that whether the transportation cost is a good domain adaptation metric, we train a 2-layer graph convolution networks on G S and compute W 1 on G T . The results are presented in Figure <ref type="figure" target="#fig_3">2</ref>. Compared with CMD, W 1 demonstrates a more clear correlation between discrepancy and testing performance on both synthetic graphs (i.e. CSBM) and real graphs (i.e. PubMed) when distribution shifts are present.</p><p>Optimization. We first fix the parameters of GNN g and classifier f to solve the transportation plan ? * using an EMD solver <ref type="bibr" target="#b7">[8]</ref>. Then, we update the parameters of {f, g} through back-propagation of L GCONDA . It is also possible to update our parameters end-to-end with a neural optimal transport solver <ref type="bibr" target="#b21">[22]</ref>. We perform scalable neighborhood sampling on the graph G to obtain source and target subgraph samples for the input of the GNN g. Specifically, we adopt a sub-graph based sampling method -GraphSAINT <ref type="bibr" target="#b41">[42]</ref> to obtain batch of nodes from source and target G s b ? G S , G t b ? G S , respectively. Refer to Appendix ?B.1 for the GCONDA algorithm outline.</p><p>Complexity. In each step, let N be the size of mini-batch and d be the dimension size of hidden representation h i ? R d and L classes, the additional computation cost of our method in each epoch is due to computing the transportation cost matrix C ? R N ?N and solving the optimal transportation ? * . The cost matrix takes O(N 2 (d + L)) time and the EMD solver takes O(N 2 ) to solve the optimal transportation plan. Therefore, the total time complexity of GCONDA is O(N 2 + N 2 (d + L)). Due to the space limit, we conduct experiments on hyperparameter sensitivity and complexity study in Appendix ?C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Synthetic Experiments</head><p>In this section, we empirically validate our theoretical insights regarding the generalization ability and transferability of graph neural networks. We aim to answer the following questions: (a) "How do DIRL methods perform under distribution shift on graphs?" and (b) "Does GCONDA provide any advantages over DIRL for GNNs?"</p><p>We do this using two different families of synthetic graphs: (1) CSBM graphs, specifically syn-csbm-pq and syn-csbm-?, which involve synthetic conditional shifts in both the features and structure. Each sample in the CSBM graph consists of a training and testing graph, where the testing graph demonstrates either a feature shift ? or a structure shift pq. (2) synthetic graphs constructed from real datasets, namely syn-cora and syn-products, with varying homophily ratios as described in previous work <ref type="bibr" target="#b44">[45]</ref>. Detailed numerical results for all of the figures and the graph statistics can be found in Appendix ?C.1. In this section, we compare our method GCONDA with well-known DIRL algorithms including CMD <ref type="bibr" target="#b40">[41]</ref> and CDAN <ref type="bibr" target="#b24">[25]</ref> using graph convolution networks <ref type="bibr" target="#b18">[19]</ref>.</p><p>First, we compare two DIRL algorithms -CDAN and CMD with GCONDA on two synthetic CSBM datasets. We tune the hyperparameters of all three algorithms using validation data obtained from the training graph. As illustrated in Figure <ref type="figure" target="#fig_4">3a</ref> and Figure <ref type="figure" target="#fig_4">3b</ref>, GCONDA outperforms both baselines in the presence of feature and structure distribution shifts. Notably, when the tested graph exhibits increased heterophily (small p/q) or significant feature shifts (large ?), the performance of GNNs is more adversely affected. When distribution shifts are small, CMD enjoys similar to our method performance, confirming that DIRL methods work well with minor conditional shift. However, the difference between two methods becomes significant when testing distribution exhibits large conditional shift. We attribute the sub-optimal performance of DIRL to the fact that it solely optimizes the distribution discrepancy on P(h) while neglecting the significant conditional shift.</p><p>In our second synthetic experiments, we examine the effectiveness of GCONDA on non-CSBM graphs. To do this, we follow the literature <ref type="bibr" target="#b44">[45]</ref> studying GNNs in the low homophily setting, where syn-cora and syn-products are constructed from existing benchmarks via preferential attachment <ref type="bibr" target="#b1">[2]</ref>. We train all the compared methods on the same "easy" graph, which has a homophily ratio of 1.0, for both datasets. Subsequently, we tested the models on target graphs with varying homophily ratios, ranging from 0.0 to 0.9. Based on our theoretical results, a target graph with a low homophily ratio is expected to result in a larger conditional shift. As depicted in Figure <ref type="figure" target="#fig_4">3c</ref> and Figure <ref type="figure" target="#fig_4">3d</ref>, we observe that the performance of the base GCN aligns with our expectations. GCONDA still mitigate the distribution shift better than DIRL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Real Data Experiments</head><p>For real-world graphs, we will compare GCONDA with domain adaptation algorithms designed for neural networks and graph neural networks in both supervised and semi-supervised learning settings.</p><p>Baselines. In addition to the domain adaptation algorithms used in previous sections, we consider the following methods for comprehensive study under distribution shift: (1) MMD <ref type="bibr" target="#b23">[24]</ref> and (2) DANN <ref type="bibr" target="#b13">[14]</ref>. For graph-specific methods, we choose three representative methods: (1) UDAGCN <ref type="bibr" target="#b33">[34]</ref> couples domain adversarial learning with graph attention mechanism; (2) SRGNN-IW <ref type="bibr" target="#b45">[46]</ref> proposes to use instance weighting technique on GNN output embeddings; (3) Graph-EERM <ref type="bibr" target="#b34">[35]</ref> proposes to augment training graph for invariance principles in risk minimization. As for our own ablations, we report the performance of DIRL version of our model (? = 0 in in Eq. ( <ref type="formula" target="#formula_18">11</ref>)) besides two variants of our methods GCONDA and GCONDA ++. All models are trained a single Nvidia A6000 GPU. Configurations of different algorithms on each dataset can be found in Appendix ?B.2  <ref type="table" target="#tab_0">1</ref>) on three semi-supervised learning benchmarks: Cora, Citeseer and PubMed <ref type="bibr" target="#b29">[30]</ref>. We choose the best-performing GNN architecture from their paper -APPNP <ref type="bibr" target="#b19">[20]</ref> and report the Micro-F1, and Macro-F1 for each method and the accuracy loss compared with IID training data. We are able to reproduce the performance gap between IID and OOD training data (? in in Table <ref type="table" target="#tab_0">1</ref>). We begin by noting that most of the general domain adaptation algorithms such as CMD, MMD, and DANN can help improve the performance because conditional shift is small in this setting. Among these algorithms, we find that directly optimizing discrepancy metrics seems to be more effective and robust (smaller average loss and deviation over 100 runs) than adversarial methods (CDAN and DANN) which often require more tuning. Across the three datasets, GCONDA ++ consistently achieves top-2 performance, while GCONDA (i.e., only optimizing conditional shift) generally ranks second best.</p><p>In addition, GCONDA-DIRL demonstrates similar performance to DIRL methods such as CMD and MMD. These observations suggest that the primary improvements stem from minimizing the estimated conditional shift W 1 (? f S , ? f T ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supervised Node and Graph Classification</head><p>In a fully-supervised setting, transfer learning is commonly employed to transfer knowledge across different domains for graph-structured data. This involves training a model on source graphs and inferring on target graphs. We conduct domain adaptation experiments on citation networks <ref type="bibr" target="#b31">[32]</ref> and molecular graphs <ref type="bibr" target="#b17">[18]</ref> for two tasks. The first task involves node classification by introducing domain shift between ACM and DBLP graphs, as well as time shift within the ACM graphs. The second task focuses on graph classification with scaffold shift, where the training and testing molecular graphs have different scaffold patterns. For node classification and graph classification, we adopt a 2-layer GCN <ref type="bibr" target="#b18">[19]</ref> and a 5-layer GraphSAGE <ref type="bibr" target="#b16">[17]</ref>, respectively, following established practices. Specifically, for graph classification, we employ mean pooling to obtain the graph representations.</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we make several key observations: (1) different algorithms exhibit varying performance under different settings, primarily due to the presence of various types of distribution shift; (2) on node classification, GCONDA and its variants usually outperforms the other baselines with a clear margin. This can be attributed to the fact that our approach has been theoretically designed to excel in node classification scenarios; (3) Domain adaptation algorithms, such as DANN, that are originally designed for neural networks exhibit better performance in graph classification tasks, because the graph classification task shares closer similarities with the image domain. Nevertheless, it is noteworthy that GCONDA ++ consistently achieved top-3 rankings across all tasks and highest average improvement (i.e. Avg. ?), indicating our potential usage on graph property predictions. For further details on the dataset and complementary experiments, please refer to Appendix ?C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we establish the first theoretical connection between the inductive bias of GNNs and distribution shift by quantifying conditional shift. Our novel theoretical results show that conditional shift is often exacerbated by GNNs, explaining the limited performance of popular DIRL methods on graph data. To remedy this shift in the latent space, we present a graph domain adaptation framework based on our theoretical results. Using a number different experiments on both synthetic and real data , we demonstrate that our method GCONDA results in a robust improvement on different kinds of domain shifts. As for future work, we have two notable directions to explore: (1) extend our analysis to other types of graph neural networks (2) develop more advanced GNNs following our theoretical results for graph domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theory details</head><p>A.1 Proof of Theorem 3.1</p><p>In Definition 3.3, we made several simplifications on original CSBM model to investigate its OOD generalization w.r.t. structure and feature distribution shifts. The original CSBM(?, ?, p, q) is defined to have two different class means ? and ?. Given training and testing graphs as G S ? CSBM(?, ?, p, q) and G T ? CSBM(? ? , ? ? , p ? , q ? ), we let ? = -? in CSBM by making ? 0 the middle point of original feature mean of two classes. Without loss of generality, we let two graphs have same amount of nodes n = n ? and edge density D = D ? . Here we restate the pseudo conditional shifts ? y|x on the hypothesis function used in Theorem 3.1. In this context, the function d is defined as an indicator function, which serves as a realization of Definition 3.2.</p><formula xml:id="formula_21">? y|x = E x?Pt(x) I arg max y P s (y|x) ? = arg max y P t (y|x) ,<label>(13)</label></formula><p>Theorem A.1 (Conditional Shift in GNNs). Let the source graph G S = CSBM(?, p, q), and a target graph G T = CSBM(? ? , p ? , q ? ), where D and D ? represent their average degrees respectively. Additionally, let ?(?) denote the cumulative distribution function (CDF) of a multivariate Gaussian distribution defined by distance. Then the introduced distribution shift between G S and G T can be quantified via the estimated conditional shift of x and h as: Proof. On a CSBM graph G(?, p, q), the data distribution on feature x is,</p><formula xml:id="formula_22">? y|x = ? ((1 + ?)???) -?((1 -?)???) 2 , ? y|h = ?(?? ? h,-1 ?) -?(?? ? h,1 ?) 2 , (<label>14</label></formula><formula xml:id="formula_23">)</formula><formula xml:id="formula_24">where ? ? h,1 = ? D ? p ? -q ? p ? +q ? ? - ? D ? ?? and ? ? h,-1 = ? D ? q ? -p ? p ? +q ? ? - ? D ? ??.</formula><formula xml:id="formula_25">x i ? N (?, I) , y i = 1, x i ? N (-?, I) , y i = -1<label>(15)</label></formula><p>Since x is a standard Gaussian, the output of the arg max operator is identical to the optimal f * (x). Furthermore, the distributions on the source and target share the same support. Thus, the indicator function in Equation 13 can be simplified as the expected difference in predictions between the optimal source classifier f and the optimal target classifier f ? on the target data, that is,</p><formula xml:id="formula_26">? y|x = E x?Pt(x) (I [f (x) ? = f ? (x)]) ,<label>(16)</label></formula><p>We first discuss the conditional shift on the feature x of the target graphs. Since we assume that the distribution shift on the feature ? is controlled by ?, the centers of the two classes on the target graphs are located at -(1 + ?)? and (1 -?)?:</p><formula xml:id="formula_27">x ? i ? N ((1 -?)?, I) , y i = 1, x ? i ? N (-(1 + ?)?, I) , y i = -1,<label>(17)</label></formula><p>The optimal classifier is f (?, 0) on source graph and f ? (?, ??) on target CSBM graph. We further partition the computation of ? y|x on two classes, that is ? y=1|x + ? y=-1|x . When y ? i = 1, the different predictions (i.e. f (x i ) ? = f ? (x i )) are those samples between 0 and -?? in 1-dimension case. Considering the probability density function P t (x) in Equation <ref type="formula" target="#formula_27">17</ref>, ? y=1|x is calculated as,</p><formula xml:id="formula_28">? y=1|x = 1 ? 2? 0 -?? exp({- (t -(1 -?)?) 2 2 }) dt<label>(18)</label></formula><p>The CDF of the standard Gaussian distribution is denoted by the ? function.</p><formula xml:id="formula_29">?(x) = 1 ? 2? x -? exp({- t 2 2 }) dt<label>(19)</label></formula><p>In standard multivariate (d &gt; 1) Gaussian distribution, we define the CDF as a monotonic function regarding the distance to the Gaussian mean ?(? ? ?).</p><p>To represent the conditional shift use ?, we flip the axis x = -x and translate the distribution into a standard Gaussian N (0, I) by moving 1 -?? as described in Figure <ref type="figure" target="#fig_6">4</ref>. </p><formula xml:id="formula_30">? y=1|x = ?(???) -?(?? -???),<label>(20)</label></formula><formula xml:id="formula_31">? y=-1|x = ?(?? + ???) -?(???),<label>(21)</label></formula><p>Since we have the same amount of nodes for each class, we get the value of conditional shifts on original features and weighted average of two classes,</p><formula xml:id="formula_32">? y|x = ? y=-1|x + ? y=1|x 2 = ?(?? + ???) -?(?? -???) 2 = ? ((1 + ?)???) -?((1 -?)???) 2 ,<label>(22)</label></formula><p>Now, we are ready to discuss the conditional shift on the GCN transformed features h. Now, we are ready to discuss the conditional shift on GCN transformed features h. The feature of a node in a CSBM graph that has been transformed using GCN is obtained as a weighted mean of D (average degree) distinct Gaussian random variables. Among these variables, p p+q constitute the intra-class variables, while q p+q make up the inter-class variables. As a result, the data distribution on h is as follows:</p><formula xml:id="formula_33">h i ? N p -q p + q ?, 1 ? D I , y i = 1, h i ? N q -p p + q ?, 1 ? D I , y i = -1<label>(23)</label></formula><p>We rescale the Gaussian distribution output by graph convolution to standard Gaussian distribution,</p><formula xml:id="formula_34">h i ? N ? D ? p -q p + q ?, I , for y i = 1.</formula><p>Consequently, on target graph,</p><formula xml:id="formula_35">h ? i ? N ? D ? ? p ? (1 -?)? -q ? (1 + ?)? ? p ? + q ? , I , y i = 1, h ? i ? N ? D ? ? q ? (1 -?)? -p ? (1 + ?)? ? p ? + q ? , I , y i = -1<label>(24)</label></formula><p>Let</p><formula xml:id="formula_36">? ? h,1 = ? D ? p ? -q ? p ? +q ? ? - ? D ? ?? and ? ? h,-1 = ? D ? q ? -p ? p ? +q ? ? - ? D ? ??,</formula><p>we are ready to finish the proof by calculation conditional shifts on target data.</p><formula xml:id="formula_37">? y|h = 1 2 ?( ? ?? h,-1 ? h ??? ) -?( ? ?? h,1 ? h ?? h ? = ?(?? ? h,-1 ?) -?(?? ? h,1 ?) 2 .<label>(25)</label></formula><p>Now let's discuss the relative conditional shift on x and h when structure or feature deviates from training, respectively.</p><p>Corollary A.1.1 (GNNs exacerbate Conditional Shift). Assuming only homophily ratio changes p/q ? = p ? /q ? , the conditional shift is always exacerbated by the 1-layer GCN since ? y|x = 0. When there is only a feature shift ??, the shift will be amplified by the GCN as ? D??, potentially leading to larger conditional shifts.</p><p>Proof. When graph structure (p ? , q ? ) changes on target graph while ? remains the same (i.e. ? = 0), ? y|h ? ? y|x = 0 When there is a distribution shift in the feature mean of the class (? &gt; 0), we define ? ? = ? D ? p ? -q ? p ? +q ? ? and obtain the following expression:</p><formula xml:id="formula_38">? x|h = ?(?? + ???) -?(?? -???) 2 , ? y|h = ?(?? ? + ? D ? ???) -?(?? ? - ? D ? ???) 2 .<label>(26)</label></formula><p>Although obtaining a closed-form solution for when GCNs exacerbate conditional shift, i.e., ? x|h &lt; ? y|h , is complicated, we can analyze the effect of varying ? on x and h. We observe that both ? x|h and ? y|h are monotonically increasing functions of ?. In the latent space h w.r.t. ? ? , the magnitude of feature shift is amplified by </p><formula xml:id="formula_39">? D ? .</formula><formula xml:id="formula_40">? T (f ) = 1 - ?((1 + ?)???) + ?((1 -?)???) 2 , ? T (f ? g) = 1 - ?(?? ? h,-1 ?) + ?(?? ? h,1 ?) 2 .<label>(27)</label></formula><p>Proof. We begin by computing the expected target error ? T on x, denoted as ? T (f ). Unlike the calculation of conditional shift, the expected error is evaluated on the target graph G T and can be expressed as follows:</p><formula xml:id="formula_41">? T (f ) = E x?Pt(x) (I [f (x) ? = y]) ,<label>(28)</label></formula><p>We recall that the feature means of the two classes are (-(1 + ?)?, (1 -?)?). For class 1, the optimal f fails to classify</p><formula xml:id="formula_42">x i correctly if w T x i + b &lt; 0, with a distance of |(1 -?)?| or more from (1 -?)?.</formula><p>The probability of such instances can be calculated as 1 -?(|(1 -?)?|). Combining this with class -1, we obtain the following result:</p><formula xml:id="formula_43">? T (f ) = 1 -?(?? + ???) 2 error of class -1 + 1 -?(?? -???) 2 error of class 1<label>(29)</label></formula><p>Similarly, on a source graph, the expected error is ? S (f ) = 1 -?(?) and ?(?) is a monotonically increasing function. Therefore, we have</p><formula xml:id="formula_44">|? S -? T | = ?(|?|) -?(|?+??|)+?(|?-??|) 2</formula><p>. Furthermore, we can calculate ? y|x -|? S -? T | as follows:</p><formula xml:id="formula_45">? y|x -|? S -? T | = ?(?? + ???) -?(???) &gt; 0<label>(30)</label></formula><p>Regarding graph convolution networks, the class centroids after GCN are ? ? h,1 and ? ? h,-1 as calculated in Theorem 3.1. The expected error of a linear classifier f on the output of GCN g is obtained as follows:</p><formula xml:id="formula_46">? T (f ? g) = 1 - ?(?? ? h,-1 ?) + ?(?? ? h,1 ?) 2<label>(31)</label></formula><p>We re-use the definition of ? ? from the proof of Corollary 3.1.1. We can now complet the proof:</p><formula xml:id="formula_47">? h|x -|? S -? T | = ?(?? ? + ? D ? ??? -?(?? ? ?) &gt; 0<label>(32)</label></formula><p>A.2 Proof of Theorem 4.1</p><p>Theorem A.2. Suppose F is the hypothesis space of GNNs, ?f ? F,</p><formula xml:id="formula_48">? T (f ) ? ? S (f ) + W 1 (? f S , ? f T ) + ? * + K L K g ?(c),<label>(33)</label></formula><p>where ? * is the joint optimal error, K L is the Lipschitz constant loss function of loss function L, K g is the Lipschitz constant of GNN g and ?(c) is the probabilistic lipschitzness <ref type="bibr" target="#b5">[6]</ref>.</p><p>Proof. Following the approach in <ref type="bibr" target="#b11">[12]</ref>, we introduce f * as the optimal labeling function in the hypothesis space F, giving us:</p><formula xml:id="formula_49">? T (f ) = E (x,y)?Pt L(y, f (x)) ? E (x,y)?Pt L(y, f * (x) + L(f (x), f * (x)) = E (x,y)?Pt L(f (x), f * (x)) + ? T (f * ) = E (x,f (x))?P f t L(f (x), f * (x)) + ? T (f * ) = ? T (f * ) -? S (f * ) + ? S (f * ) + ? T (f * ) ? |? f T (f * ) -? S (f * )| + ? S (f * ) + ? T (f * ) ? *<label>(34)</label></formula><p>Now we introduce the definition of K g and ?(c) in the theorem. The Lipschitz constant of GNNs has garnered considerable attention in recent studies <ref type="bibr" target="#b10">[11]</ref>. In our analysis, we view the data distribution as rooted subtrees <ref type="bibr" target="#b46">[47]</ref> centered around node i, denoted as x i = T i , where T i are sampled from graph G. We define the Lipschitz constant K g of GNNs as follows:</p><formula xml:id="formula_50">|f (T i ) -f (T j )| ? K g |l(T i ) -l(T j )| ? K g (<label>35</label></formula><formula xml:id="formula_51">)</formula><p>where l : T i ? [0, 1] d is a bounded function maps node features in the rooted subtree to real values, e.g. mean aggregation and normalization in GraphSAGE <ref type="bibr" target="#b16">[17]</ref>.</p><p>Definition A.1 (Probabilistic Transfer Lipschitzness <ref type="bibr" target="#b11">[12]</ref>). Let ? : R ? [0, 1], a labeling function f : X ? R and a joint distribution ? over ? S and ? T , the ?-transfer lipschitzness represents for all c:</p><formula xml:id="formula_52">P (xs,xt) [|f (x s ) -f (x t )| &gt; cd(x s , x t )] ? ?(c)<label>(36)</label></formula><p>Let ? f S = P s (x, y) and ? f T = P f t (x, f (x)) denote the source data distribution and the estimated target data distribution, respectively. ? f T (f * ) can be interpreted as the discrepancy in predictions between f and f * . Given ? * is the optimal transportation plan of GCONDA, we have:</p><formula xml:id="formula_53">|? f T (f * ) -? S (f * )| = L(y, f * (x))d(P f t -P s ) = (L(f (x t ), f * (x t )) -L(y s , f * (x s ))) d? * ((x s , y s ), (x t , f (x t ))) ? (L(f (x t ), f * (x t )) -L(y s , f * (x s ))) d? * ((x s , y s ), (x t , f (x t ))) ? L(f (x t ), f * (x t )) -L(f (x t ), f * (x s )) + L(f (x t ), f * (x s )) -L(y s , f * (x s )) d? * ((x s , y s ), (x t , f (x t ))) ? K L f * (x s ) -f * (x t ) + L(f (x t ), y s )d? * ((x s , y s ), (x t , f (x t )))<label>(37)</label></formula><formula xml:id="formula_54">? c * K L d(x s , x t ) + L(y s , f (x t ))d? * ((x s , y s ), (x t , f (x t ))) + K L K g ?(c)<label>(38)</label></formula><formula xml:id="formula_55">? W 1 f S , ? f T ) + K L K g ?(c)<label>(39)</label></formula><p>Line ( <ref type="formula" target="#formula_37">25</ref>) is a consequence of Lipschitz constant and triangle inequality on L. Line (26) applies ?(c)-transfer lipschitzness on f * (x). The last line ( <ref type="formula" target="#formula_40">27</ref>) is achieved by setting ? = c * K L in Eq. ( <ref type="formula" target="#formula_18">11</ref>) of the main paper. We complete the proof by combining Eq. ( <ref type="formula" target="#formula_49">34</ref>) and Eq. <ref type="bibr" target="#b26">(27)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional discussion on DIRL</head><p>In Section 3.1 of the main paper, we introduced the covariate shift assumption on DIRL, which alternatively assumes a small conditional shift. However, even with this assumption, our synthetic experiment in Section 5 shows that the best DIRL method (i.e. CMD) still yield unsatisfactory results.</p><p>To further illustrate this from a theoretical perspective, we restate an existing study on the conditional shift in DIRL.</p><p>Theorem A.3 (Limits of learning invariant representations under conditional shift). <ref type="bibr" target="#b43">[44]</ref> Suppose markov chain</p><formula xml:id="formula_56">X g -? Z h -? ? and d JS is the Jensen-Shannon distance, ? S (h ? g) + ? T (h ? g) ? 1 2 d JS (D Y S , D Y T ) -d JS (D Z S , D Z T ) 2</formula><p>According to the above theorem, when P(Y |X) is different on source and target, minimizing source risk and H?H-divergence leads to a small JS distance d JS (D Z S , D Z T ). As a consequence, the marginal label shift d JS (D Y S , D Y T ) dominating the the lower bound of joint source and target risk. If conditional shift is large, DIRL cannot achieve accurate predictions on target. In Figure <ref type="figure">5</ref>, we train a domain adversarial neural network <ref type="bibr" target="#b13">[14]</ref> and project the node TSNE embeddings of source and target CSBM graphs. Two different colors indicate class labels, O dots are source data and X are target samples. When the conditional shift is small and covariate shift assumption holds approximately, DANN can separate different classes well for both source and target domains (left). However, when there is large conditional shift, the classification accuracy on target is low because it only minimizes discrepancy between representations, and classes end up intermixed.   <ref type="figure" target="#fig_5">10p</ref>) -1, 0.1), and the p is changing from 0 to 1 within the training process as <ref type="bibr" target="#b33">[34]</ref>. 6. For EERM, we search the best learning rate ? f ? {0.0001, 0.0002, 0.001, 0.005, 0.01} for GNN backbone, the learning rate ? g ? {0.0001, 0.001, 0.005, 0.01} for graph editers, the weight ? ? {0.2, 0.5, 1.0, 2.0, 3.0} for combination, the number of edge editing for each node s ? {1, 5, 10}, the number of iterations T ? {1, 5} for inner update before one-step outer update. 7. For SRGNN-IW ? , the main hyper parameters in the sampler PPR-S are ? ? {0.01, 0.1, 0.5, 1}, ? ? {10, 50, 100, 200, 500}. When the graph is large, ? = 0.001 is set in the local algorithm for sparse PPR approximation. ? ? {0.1, 0.5, 1, 2} is the penalty parameter for the discrepancy regularizer. The lower bound for the instance weight B l is in {0.1, 0.2, 0.5, 1.0}.</p><p>8. Hyperparameters of GCONDA ? and ? are selected between {0.01, 0.1, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Details C.1 Dataset Details</head><p>In the main paper, we perform node classification and graph classification tasks on 11 different datasets with distribution shift. The statistics of these graphs are presented in Table <ref type="table" target="#tab_2">3</ref>. We will now discuss the selection criterion or creation process for each dataset in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSBM Dataset Generation.</head><p>In our experiments, we set the feature size d and average degree D of CSBM graph in Definition 3.3 graph as 128 and 10, respectively.</p><p>For structure shift (i.e. syn-csbm-pq ), each time we first sample a feature mean ? ? N (0, 1 ? d ), where d is the dimension of the feature. Then source graph is generated with a fixed p/q = 5 while each target graph is generated under a random p/q between {1, ..., 10}. Such that we ensure the features of both graph are generated with the same Gaussian distribution and their homophily ratios are different.</p><p>For feature shift (i.e. syn-csbm-?), we generate ? ? by translating mean by ?? and rotate ? ? by ? (from 0 to 60 degrees). In corollary 3.1.2, we use the same ? to describe the classification error. When ? is small, feature shift is small and test feature mean ? ? is close to original feature mean. The rotation is added to avoid trivial adaptation like translation. Figure <ref type="figure" target="#fig_8">6</ref> illustrates the process of creating features shifts in our experiment. The dataset generation code can be found in uploaded code named cSBM_gendata.py. DBLP-ACM Dataset. In the main paper, we conduct the transfer learning experiments with domain shift and time shift for node classification. These experiments use three sets of citation networks, which are constructed on the datasets provided by ArnetMiner <ref type="bibr" target="#b31">[32]</ref>. Specifically, for domain shift, we adopt two sets of ACM-DBLP citation networks of different sizes. The small set namely ACM-DBLP small is proposed by <ref type="bibr" target="#b33">[34]</ref>. It includes the papers extracted from ACMv9 (between years 2000 and 2010) and DBLPv8 (after year 2010). The large set, ACM-DBLP large is constructed on DBLPv12 (before 2017) and ACMv8 (before 2017). As to time shift, we utilize ACMv9 across different time periods, specifically, before or after 2010, to build two citation networks, ACM time .</p><p>In our experiments, we consider these datasets as undirected graphs and each edge representing a citation relation between two papers. The papers are classified to some of the predefined categories according to its research topics. ACM-DBLP small has six categories including"Database", "Data mining", "Artificial intelligent", "Computer vision", "Information Security" and "High Performance Computing". For ACM-DBLP large and ACM time , there are five categories including "Database", "Data mining", "Artificial intelligent", "Computer vision", and "Natural Language Processing". We evaluate our proposed methods by conducting multi-label classification on these three sets of citation networks.</p><p>Graph Classification Datasets. There are 10 molecular propety prediction datasets from Open Graph Benchmark <ref type="bibr" target="#b17">[18]</ref>. These graphs are known to be affected by the scaffold split of the training and testing data. To compare different domain adaptation algorithms, we rank the performance degradation by comparing validation and test accuracy. From Table <ref type="table" target="#tab_1">2</ref> in the main paper, we select the top-3 datasets with the highest degradation: BACE, BBBP, and Clintox. We choose these datasets because they exhibit the most pronounced "negative" distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Complementary Results on Synthetic Domain Adaptation</head><p>In Figure <ref type="figure" target="#fig_10">7</ref>, we provide the test logloss plot of our experiments on CSBM graphs as complimentary results of Figure <ref type="figure" target="#fig_4">3</ref> of the main paper, respectively. The test loss also correlates well with domain adaptation bound introduced in Theorem 4.1. When distribution shift becomes more significant, for example a smaller p/q or larger ?, the target loss increases. In addition, we present the numerical results used to draw Figure <ref type="figure" target="#fig_4">3a</ref> and 3b of the main paper in Table <ref type="table">4</ref> and<ref type="table">Table 5</ref>.  Table <ref type="table">4</ref>: syn-csbm-p/q (Fig. <ref type="figure" target="#fig_4">3a</ref>). Mean ROC and standard deviation per method (with structure shift p/q). 68.1 ? 5.4 85.9 ? 4.0 94.7 ? 3.0 96.9 ? 2.1 98.4 ? 1.3 98.9 ? 1.0 99.4 ? 0.7 99.5 ? 0.5 99.7 ? 0.5 99.6 ? 0.5</p><p>Table <ref type="table">5</ref>: syn-csbm-? (Fig. <ref type="figure" target="#fig_4">3b</ref>). Mean ROC and standard deviation per method (with feature shift ?). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Complementary Results on Supervised Node Classification</head><p>Due to the space limit, we only report the Micro-F1 in the Table <ref type="table" target="#tab_1">2</ref> of the main paper. In Table <ref type="table">6</ref>, we include the results on both Micro-F1 and Macro-F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Hyperparameter and Complexity Study</head><p>Choices of ? and ?. The main difference between GCONDA and GCONDA ++ is the introduction of aligning marginal distribution (? g S , ? g T ) together with conditional shift controlled by hyperparameter ? in Eq.( <ref type="formula" target="#formula_18">11</ref>) of the main paper. In this section we study how varying ?, ? between [0, 1] in GCONDA ++ affects the performance. We conduct 10 runs for each ? while fixing ? = 0.1 on four node classification datasets and vice versa. In Figure <ref type="figure">8a</ref>, we observed that GCONDA ++ does not consistently outperform GCONDA (? = 0) except dataset ACM-DBLP. Because different domains may have different word distributions as node features, and in this case we find that regularizing the representation shift appears to be helpful. In Figure <ref type="figure">8b</ref>, we observe that the performance on all four datasets improves when ? &gt; 0, further validating that minimizing conditional shift is a key factor in our framework. Overall, our performance is not sensitive to the hyper parameters within a reasonable range.</p><p>Time and Space Complexity of GCONDA. We would like to provide further details on training time and extra costs on a non-citation graph from Open Graph Benchmark <ref type="bibr" target="#b17">[18]</ref> -ogbn-proteins. In ogbn-proteins, nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins. The task is to predict the presence of protein functions in a multi-label binary classification setup, where there are 112 kinds of labels to predict in total. It is considered as reasonably large with 132 thousand nodes and 39 million edges. We report the actual running time and actual GPU usage per epoch varying batch size N in Table <ref type="table">7</ref>. We observe that the training time of GCONDA increases only slightly when the batch size is set to 128. The additional space complexity is negligible for all batch sizes. The additional time complexity, as explained earlier, is effectiveness. This consistent high performance across various datasets signifies the potential of our approach to accurately predict graph properties and opens up promising avenues for its practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the conditional shift in a toy 2D latent space. The red points represent the Gaussian means in the latent space. The x-axis represents the direction of feature means (?), and the conditional shifts are indicated by the area with hatching lines, as defined in Eq. 5. The corresponding generalization results for both shifts are shown in (c) and (d).</figDesc><graphic url="image-1.png" coords="5,108.00,75.22,91.08,62.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) shows GCNs introduce a factor of ? D ? , where D ? &gt; 1 for any connected graphs. In other words, GNNs amplify feature shift (by ? D ? ). Corollary 3.1.2 (Relation between conditional shift and generalization). Conditional shift upper bounds the performance gap between source and target, i.e. ? &gt; |? T -? S |. The expected target error ? T for linear classifiers f and GNNs f ? g in section 3.1 are,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) CMD on CSBM (b) W1 on CSBM (c) CMD on PubMed (d) W1 on PubMed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of CMD and transportation cost W1 (ours) for the same GCN model, x-axis the metric value and y-axis is the test ROC AUC. Each point in the plot corresponds to a pair of source and target graph. The Person correlation coefficient r (the larger abs values, the better) is shown on the top-right corner.</figDesc><graphic url="image-5.png" coords="7,120.11,74.02,91.08,68.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Domain adaptation results on synthetic graphs.</figDesc><graphic url="image-9.png" coords="8,108.00,72.00,95.04,59.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 1 .</head><label>1</label><figDesc>Through training with hinge loss, the linear classifier f on original feature x and GNN latent space h have the same optimal hyperplane P = {x|w T x + b = 0} characterized by f (w * , b * ), w * = ? and b * = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Representing ? y=1|x with ?</figDesc><graphic url="image-13.png" coords="16,117.90,169.79,376.20,152.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Corollary A. 1 . 2 (</head><label>12</label><figDesc>Relation between conditional shift and generalization). Conditional shift upper bounds the performance gap between source and target, i.e. ? &gt; |? T -? S |. The expected target error ? T for linear classifiers f and GNNs f ? g are,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of creating feature shift on CSBM graphs.</figDesc><graphic url="image-16.png" coords="21,226.80,146.76,158.40,106.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Testing loss of different DA algorithms. (b) Testing Loss of different DA algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Domain adaptation on datasets constructed from real graphs. We use homophily ratio 1.0 for training and plot the base GCN performance as well as domain adaption algorithms on three test graphs per interval.</figDesc><graphic url="image-17.png" coords="22,116.91,554.23,160.38,100.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semi-supervised classification on three different citation networks with OOD training samples. Results from the original paper<ref type="bibr" target="#b45">[46]</ref> are marked ? . We mark the best and the second best results.GNNs are widely recognized for their effectiveness in node classification tasks, particularly when dealing with a limited amount of labeled data. In semi-supervised classification, source data is a small number of training nodes and target data are all of the remaining nodes in the same graph. Recently, SRGNN<ref type="bibr" target="#b45">[46]</ref> found biased training data in semi-supervised learning can cause dramatic accuracy loss; they provide the algorithm to generate biased training nodes (refered to as OOD training in Table</figDesc><table><row><cell>Method</cell><cell cols="8">Cora Micro-F1 Macro-F1 ?Acc Micro-F1 Macro-F1 ?Acc Micro-F1 Macro-F1 ?Acc Citeseer PubMed</cell></row><row><cell>IID training</cell><cell cols="2">80.8 ? 1.5 80.1 ? 1.3</cell><cell>0</cell><cell>70.2 ? 1.9 66.8 ? 1.7</cell><cell>0</cell><cell cols="2">79.7 ? 1.4 78.8 ? 1.4</cell><cell>0</cell></row><row><cell>OOD training</cell><cell cols="2">71.3 ? 4.1 69.2 ? 3.4</cell><cell>9.5</cell><cell>63.4 ? 1.8 61.2 ? 1.6</cell><cell>6.9</cell><cell cols="2">63.4 ? 4.2 58.7 ? 7.0</cell><cell>16.4</cell></row><row><cell>MMD</cell><cell cols="2">71.5 ? 4.9 69.5 ? 4.6</cell><cell>9.3</cell><cell>64.4 ? 1.2 62.0 ? 1.1</cell><cell>5.9</cell><cell cols="2">66.3 ? 4.2 63.5 ? 5.9</cell><cell>13.4</cell></row><row><cell>CMD  ?</cell><cell cols="2">72.1 ? 4.4 69.8 ? 3.7</cell><cell>8.7</cell><cell>63.9 ? 0.7 61.8 ? 0.6</cell><cell>6.4</cell><cell>69.4? 3.4</cell><cell>67.6 ? 4.0</cell><cell>10.4</cell></row><row><cell>DANN</cell><cell cols="2">71.5 ? 5.0 69.5 ? 4.6</cell><cell>9.3</cell><cell>64.7 ? 1.2 62.3 ? 1.1</cell><cell>5.6</cell><cell cols="2">64.5 ? 4.9 60.6 ? 7.8</cell><cell>15.2</cell></row><row><cell>CDAN</cell><cell cols="2">71.5 ? 5.1 69.5 ? 4.7</cell><cell>9.3</cell><cell>64.6 ? 1.3 62.2 ? 1.2</cell><cell>5.6</cell><cell cols="2">64.1 ? 5.0 59.9 ? 7.9</cell><cell>15.6</cell></row><row><cell>UDAGCN</cell><cell cols="2">36.2 ? 4.5 35.4 ? 4.3</cell><cell>44.6</cell><cell>33.8 ? 5.1 31.5 ? 7.7</cell><cell>36.4</cell><cell cols="2">40.6 ? 6.8 34.9 ? 6.8</cell><cell>39.1</cell></row><row><cell>EERM</cell><cell cols="2">68.3 ? 4.3 66.2 ? 3.9</cell><cell>12.5</cell><cell>62.3 ? 1.0 59.5 ? 1.0</cell><cell>7.9</cell><cell cols="2">61.6 ? 4.8 56.8 ? 7.7</cell><cell>18.1</cell></row><row><cell>SRGNN-IW  ?</cell><cell cols="2">72.0 ? 3.2 69.5 ? 3.7</cell><cell>8.8</cell><cell>66.1 ? 0.9 63.4 ? 0.9</cell><cell>4.2</cell><cell cols="2">66.4 ? 4.0 64.0 ? 5.5</cell><cell>13.4</cell></row><row><cell cols="3">GCONDA-DIRL 71.7 ? 4.7 69.7 ? 4.3</cell><cell>9.1</cell><cell>64.6 ? 1.1 62.2 ? 1.0</cell><cell>5.6</cell><cell cols="2">68.3 ? 3.9 66.5 ? 4.7</cell><cell>11.4</cell></row><row><cell>GCONDA</cell><cell cols="2">71.7 ? 4.7 70.2 ? 2.7</cell><cell>9.1</cell><cell>65.3 ? 0.8 63.3 ? 0.8</cell><cell>4.9</cell><cell cols="2">71.5 ? 2.9 70.4 ? 3.1</cell><cell>8.2</cell></row><row><cell>GCONDA ++</cell><cell>72.6 ? 3.1</cell><cell>70.7? 3.0</cell><cell>8.2</cell><cell>65.6 ? 0.9 63.5 ? 0.9</cell><cell>4.6</cell><cell cols="2">73.0 ? 2.5 71.9 ? 2.5</cell><cell>6.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Domain adaptation on node and graph classification. We mark the best and the second best accuracy.</figDesc><table><row><cell>Method</cell><cell cols="8">Node Classification (Micro-F1) ACM-DBLPsmall ACMtime ACM-DBLPlarge Avg. ? BACE Graph Classification (AUC) BBBP Clintox Avg. ?</cell></row><row><cell>Base model</cell><cell>68.1 ? 2.1</cell><cell>78.8 ? 1.0</cell><cell>81.1 ? 0.2</cell><cell></cell><cell cols="3">64.8 ? 2.8 71.0 ? 8.7 52.8 ? 3.3</cell><cell></cell></row><row><cell>CMD  ?</cell><cell>75.5 ? 4.4</cell><cell>79.4 ? 0.7</cell><cell>75.2 ? 0.8</cell><cell cols="5">+0.97 60.4 ? 1.4 72.0 ? 1.8 55.0 ? 5.0 -0.40</cell></row><row><cell>DANN</cell><cell>70.1 ? 1.8</cell><cell>79.6 ? 0.4</cell><cell>81.6 ? 0.4</cell><cell cols="5">+1.10 67.4 ? 2.9 74.0 ? 2.3 61.6 ? 3.6 +4.80</cell></row><row><cell>CDAN</cell><cell>75.3 ? 4.3</cell><cell>79.3 ? 1.3</cell><cell>82.1 ? 0.3</cell><cell cols="5">+2.90 69.1 ? 1.8 73.5 ? 1.7 57.5 ? 2.4 +3.83</cell></row><row><cell>UDAGCN</cell><cell>66.4 ? 5.1</cell><cell>79.3 ? 0.5</cell><cell>78.3 ? 2.6</cell><cell cols="5">-1.33 67.9 ? 1.4 73.3 ? 2.1 60.7 ? 4.8 + 4.43</cell></row><row><cell>EERM</cell><cell>64.9 ? 3.5</cell><cell>77.3 ? 0.4</cell><cell>81.0 ? 0.4</cell><cell>-1.60</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SRGNN-IW</cell><cell>69.2 ? 1.6</cell><cell>79.5 ? 1.1</cell><cell>81.4 ? 0.4</cell><cell cols="5">0.70 65.2 ? 3.3 71.7 ? 2.8 57.3 ? 3.6 +1.87</cell></row><row><cell>GCONDA-DIRL</cell><cell>71.6 ? 2.3</cell><cell>80.2 ? 0.4</cell><cell>82.3 ? 0.4</cell><cell cols="5">+2.03 65.4 ? 2.4 69.3 ? 4.0 57.9 ? 3.6 +1.33</cell></row><row><cell>GCONDA</cell><cell>74.0 ? 4.7</cell><cell>80.1 ? 0.5</cell><cell>82.1 ? 0.3</cell><cell cols="5">+2.73 64.7 ? 2.0 70.0 ? 4.2 57.2 ? 2.1 +1.10</cell></row><row><cell>GCONDA ++</cell><cell>78.5 ? 4.0</cell><cell>80.3 ? 0.8</cell><cell>82.5 ? 0.3</cell><cell cols="5">+4.43 67.8 ? 2.5 74.4 ? 3.0 61.7 ? 2.4 +4.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dataset Statistics.</figDesc><table><row><cell></cell><cell cols="6">syn-csbm syn-cora syn-products cora citeseer pubmed</cell><cell>DBLP</cell><cell>ACM</cell><cell cols="3">BACE BBBP Clintox</cell></row><row><cell># Graphs</cell><cell>500</cell><cell>30</cell><cell>30</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell cols="2">1513 2039</cell><cell>1478</cell></row><row><cell># Nodes</cell><cell>128</cell><cell>1,490</cell><cell>10,000</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>78,509</cell><cell>23,343</cell><cell>34</cell><cell>24</cell><cell>26</cell></row><row><cell># Edges</cell><cell>1,280</cell><cell>2,965</cell><cell>59,640</cell><cell>5,278</cell><cell>4,614</cell><cell cols="3">44,325 1,001,300 162,106</cell><cell>74</cell><cell>52</cell><cell>56</cell></row><row><cell># Classes</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 6.3 78.7 ? 8.2 87.9 ? 8.8 93.1 ? 8.1 94.9 ? 5.7 97.1 ? 3.6 97.6 ? 4.4 98.8 ? 1.7 98.4 ? 3.0 97.8 ? 4.6 CMD 66.0 ? 5.0 83.7 ? 4.0 93.1 ? 3.3 96.2 ? 2.6 97.9 ? 1.5 98.5 ? 1.4 98.8 ? 1.4 99.1 ? 1.2 99.3 ? 0.9 99.3 ? 1.0 CDAN 62.7 ? 5.9 79.2 ? 8.1 90.0 ? 6.5 94.6 ? 5.4 96.0 ? 4.7 97.9 ? 2.0 98.4 ? 3.6 99.1 ? 1.1 99.1 ? 1.4 98.6 ? 2.7 Ours</figDesc><table><row><cell>Method</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>syn-csbm-p/q 5 6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>GCN</cell><cell>62.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>? 10.7 89.6 ? 9.9 87.9 ? 11.2 82.9 ? 14.3 80.2 ? 13.8 78.1 ? 13.6 69.1 ? 12.8 61.8 ? 13.1 61.1 ? 13.4 56.8 ? 10.2 CMD 97.5 ? 1.9 97.0 ? 1.8 96.9 ? 2.6 97.1 ? 2.5 96.2 ? 5.1 94.2 ? 5.5 89.5 ? 18.5 87.3 ? 15.5 87.2 ? 19.2 80.1 ? 20.2 CDAN 93.5 ? 7.5 90.2 ? 9.3 87.1 ? 11.4 84.4 ? 12.6 79.0 ? 13.5 72.6 ? 12.4 66.6 ? 12.1 60.8 ? 13.0 59.4 ? 12.2 55.5 ? 8.0 Ours 98.1 ? 1.5 97.8 ? 1.5 98.0 ? 1.8 98.1 ? 1.5 97.4 ? 4.1 96.9 ? 1.8 96.3 ? 2.4 95.3 ? 3.3 95.2 ? 3.9 94.0 ? 5.4</figDesc><table><row><cell>Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>syn-csbm-? 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row><row><cell>GCN</cell><cell>91.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Experimental results for graph classification on Drugood datasets (including Accurary and AUC scores).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">lbap_core_ec50</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Assay</cell><cell></cell><cell>Scaffold</cell><cell></cell><cell>Size</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>Base Model</cell><cell>87.89</cell><cell>69.46</cell><cell>70.32</cell><cell>59.66</cell><cell>67.86</cell><cell>61.53</cell></row><row><cell>CMD</cell><cell>70.68</cell><cell>50.15</cell><cell>58.51</cell><cell>47.99</cell><cell>67.22</cell><cell>57.97</cell></row><row><cell>DANN</cell><cell>87.49</cell><cell>64.70</cell><cell>68.36</cell><cell>58.16</cell><cell>67.78</cell><cell>47.58</cell></row><row><cell>CDAN</cell><cell>87.54</cell><cell>69.25</cell><cell>70.88</cell><cell>60.23</cell><cell>68.14</cell><cell>61.10</cell></row><row><cell>UDAGCN</cell><cell>82.79</cell><cell>73.05</cell><cell>72.38</cell><cell>61.23</cell><cell>69.70</cell><cell>61.06</cell></row><row><cell>SRGNN-IW</cell><cell>87.39</cell><cell>74.04</cell><cell>72.05</cell><cell>60.35</cell><cell>68.94</cell><cell>60.33</cell></row><row><cell>GCONDA</cell><cell>88.62</cell><cell>74.40</cell><cell>71.94</cell><cell>60.45</cell><cell>69.42</cell><cell>59.94</cell></row><row><cell>GCONDA ++</cell><cell>88.84</cell><cell>71.22</cell><cell>73.57</cell><cell>61.75</cell><cell>70.34</cell><cell>61.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">lbap_core_ic50</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Assay</cell><cell></cell><cell>Scaffold</cell><cell></cell><cell>Size</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>Base model</cell><cell>81.21</cell><cell>68.34</cell><cell>74.04</cell><cell>63.69</cell><cell>72.80</cell><cell>61.51</cell></row><row><cell>CMD</cell><cell>74.24</cell><cell>68.55</cell><cell>72.54</cell><cell>60.33</cell><cell>68.30</cell><cell>58.14</cell></row><row><cell>DANN</cell><cell>83.22</cell><cell>70.08</cell><cell>76.00</cell><cell>66.37</cell><cell>70.08</cell><cell>63.45</cell></row><row><cell>CDAN</cell><cell>83.06</cell><cell>71.29</cell><cell>76.52</cell><cell>66.42</cell><cell>72.87</cell><cell>64.79</cell></row><row><cell>UDAGCN</cell><cell>81.34</cell><cell>69.89</cell><cell>74.66</cell><cell>63.77</cell><cell>72.96</cell><cell>64.79</cell></row><row><cell>SRGNN-IW</cell><cell>82.91</cell><cell>71.00</cell><cell>75.51</cell><cell>63.80</cell><cell>73.32</cell><cell>64.85</cell></row><row><cell>GCONDA</cell><cell>83.47</cell><cell>72.40</cell><cell>77.77</cell><cell>67.50</cell><cell>73.42</cell><cell>62.50</cell></row><row><cell>GCONDA ++</cell><cell>83.56</cell><cell>71.64</cell><cell>77.36</cell><cell>66.04</cell><cell>73.92</cell><cell>65.87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Conditional shift represents a change in the conditional distribution P(y|x) between the input features x and the corresponding output labels y when moving from the source domain to the target domain.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Homophily ratio calculates the fraction of edges in a graph which connects the nodes that have the same label<ref type="bibr" target="#b44">[45]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>While we present here the results for one-layer GCNs and linear perceptron, our results can be extended to multi-layer graph convolutions with activations in the manner of<ref type="bibr" target="#b3">[4]</ref>. We leave this for future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>(a) Node classification accuracy varying ? (b) Node classification accuracy varying ?</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Theory details 15</head><p>A.   fix ? * and update the weights of f, g ? Eq. ( <ref type="formula">9</ref>) of the main paper 8 end</p><p>In the algorithm, we use node classification with a neighborhood sampler as an example. For graph classification, each sample (G b , x b , y b ) is a different graph sampled from source or target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementations</head><p>We implement our method and all other baselines using torch-geometric library. We list the graph neural network specifications used in our experiments, 1. Synthetic node classification -model architecture: Graph Convolutional Networks <ref type="bibr" target="#b18">[19]</ref>, hidden dimension: 16, activation: SiLU, number of layers: 2, dropout: 0.0 2. Semi-supervised node classification -model architecture: APPNP <ref type="bibr" target="#b19">[20]</ref>, hidden dimension: 32, number of layers:2, dropout: 0.0, 3. Supervised node classification -model architecture: Graph Convolutional Networks <ref type="bibr" target="#b18">[19]</ref>,</p><p>hidden dimension: 128, activation: ReLU, number of layers: 2, dropout: 0.2 4. Supervised graph classification -model architecture: GraphSAGE <ref type="bibr" target="#b16">[17]</ref>, hidden dimension: 300, activation: ReLU, number of layers: 5, dropout: 0.5</p><p>For supervised node classification, we utilized the RandomWalk GraphSAINT <ref type="bibr" target="#b41">[42]</ref> sampler with a batch size of 256, step size of 50, and walk length of 2. We indepdentently run experiments 10 times and report the mean and standard deviation in all table and figures. All models are trained on a single Nvidia A6000 GPU. The code for each experiment can be found in separate folder in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Baseline Hyperparameters</head><p>In our experiments, we employed the following baselines and performed hyperparameter tuning on the validation set. Specifically, each baseline has hyperparameters as follow, primarily influenced by the batch size. Choosing an appropriate batch size, such as 128 or 256, can reduce the computation cost of solving the optimal transportation plan in GCONDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Additional Experiments on GraphOOD Benchmark</head><p>We performed additional experiments on graph classification using six datasets obtained from the data curators of DrugOOD <ref type="bibr" target="#b37">[38]</ref>. The DrugOOD dataset is derived from the ChEMBL website, which houses a large-scale bioassay deposition <ref type="bibr" target="#b27">[28]</ref>. The dataset offers various indicators for splitting, such as assay, scaffold, and size. Furthermore, we applied three different splitting schemes to both IC50 and EC50 categories in DrugOOD. As a result, we obtained six datasets: EC50-? and IC50-?, where the suffix ? denotes the specific splitting scheme (IC50/EC50-assay/scaffold/size). This approach enables us to comprehensively evaluate the performance of our method under different environmental definitions. All six datasets focus on ligand-based affinity prediction (LBAP), where each molecule is labeled as active or inactive. For all datasets, we followed the default training-validation-test split outlined following <ref type="bibr" target="#b37">[38]</ref>. During training, we utilized all molecules in the training set to optimize the model parameters. Subsequently, we selected hyperparameters based on the validation set and reported the results on the test molecule set using the model that achieved the best performance on the validation set.</p><p>For graph classification, to build the base model, we adopt a 4-layer GIN <ref type="bibr" target="#b35">[36]</ref> for node representations and a mean pooling layer for graph representations followed by a linear head to make prediction. The experimental results are presented in Table <ref type="table">8</ref>. Upon the careful observations, we can find several noteworthy discoveries. Firstly, it becomes evident that the performance of different algorithms varies significantly across different settings, predominantly due to the presence of distinct distribution shifts. This implies that algorithm selection should be tailored to the specific characteristics of the dataset and the nature of the distribution shift. In addition, our proposed approach (GCONDA) and its variants consistently outperform the other baseline methods. This persistent superiority can be attributed to the deliberate design of our approach, which prioritizes optimal performance in graph classification scenarios. The underlying techniques and mechanisms employed by our approach effectively leverage the inherent structure and relationships within graph nodes, leading to superior classification accuracy. Furthermore, the standout performance of our approach (GCONDA ++) should not be overlooked. Across all datasets, GCONDA ++ consistently achieved a top ranking, showcasing its robustness and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?ka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolution for semisupervised classification: Improved linear separability and out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Aseem</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimon</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aukosh</forename><surname>Jagannath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06966</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Effects of graph convolutions in deep networks</title>
		<author>
			<persName><forename type="first">Aseem</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimon</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aukosh</forename><surname>Jagannath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09297</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation-can quantity compensate for quality?</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-David</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Urner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="202" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Displacement interpolation using lagrangian mass transport</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 SIGGRAPH Asia conference</title>
		<meeting>the 2011 SIGGRAPH Asia conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph domain adaptation: A generative view</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengzhu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07482</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">89</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tree mover&apos;s distance: Bridging graph metrics and stability of graph neural networks</title>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01906</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint distribution optimal transportation for domain adaptation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiner</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Good</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08452</idno>
		<title level="m">A graph out-of-distribution benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Korotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Filippov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14593" to="14605" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lamda: Label matching deep domain adaptation</title>
		<author>
			<persName><forename type="first">Trung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhat</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6043" to="6054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Subgroup generalization and fairness of graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Is homophily a necessity for graph neural networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06134</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">David</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Patr?cia</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marleen</forename><forename type="middle">De</forename><surname>Veij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eloy</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mar?a</forename><forename type="middle">P</forename><surname>Magari?os</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">F</forename><surname>Mosquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prudence</forename><surname>Mutowo-Meullenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Nowotka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mar?a</forename><surname>Gordillo-Mara??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><forename type="middle">M I</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Junco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Mugumbate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milagros</forename><surname>Rodr?guez-L?pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Radoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Segura-Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chembl: towards direct deposition of bioassay data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="930" to="D940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">M?moire sur la th?orie des d?blais et des remblais</title>
		<author>
			<persName><surname>Gaspard Monge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. Math. Phys. Acad. Royale Sci</title>
		<imprint>
			<biblScope unit="page" from="666" to="704" />
			<date type="published" when="1781">1781</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive graph convolutional networks</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph domain adaptation via theory-grounded spectral regularization</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Dane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00684</idno>
		<title level="m">Domain adaptive network embedding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Des</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11468</idno>
		<title level="m">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Table 6: Full result of supervised node classification. We report mean and standard deviation on Micro and Macro F1</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Transfer learning of graph neural networks with ego-graph information maximization</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Base model 68.1 ? 2.1 68.2 ? 2</title>
		<author>
			<persName><forename type="first">Micro-F</forename></persName>
		</author>
		<idno>78.8 ? 1.0 76.1 ? 0.7 81.1 ? 0.2 79.1 ? 0.2 MMD 65.9 ? 2.2 65.3 ? 3.1 79.0 ? 1.0 76.1 ? 1.0 81.7 ? 0.3 79.6 ? 0.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gconda</surname></persName>
		</author>
		<idno>= 0) 74.0 ? 4.7 73.3 ? 4.9 80.1 ? 0.5 77.2 ? 0.4 82.1 ? 0.3 80.0 ? 0.3 GCONDA (? = 0) 71.6 ? 2.3 71.2 ? 2.6 80.2 ? 0.4 77.3 ? 0.3 82.3 ? 0.4 80.2 ? 0.4 GCONDA 78.5 ? 4.0 78.1 ? 4.3 80.3 ? 0.8 77.3 ? 0.6 82.5 ? 0.3 80.4 ? 0.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Table 7: Additional Time and Space Complexity of GCONDA</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
