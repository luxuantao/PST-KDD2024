<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Pre-Training of GNNs on Heterogeneous Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
							<email>yfang@smu.edu.sg</email>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="middle">2021</forename><surname>Contrastive</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">WeChat Search Application Department</orgName>
								<orgName type="institution">Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Pre-Training of GNNs on Heterogeneous Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482332</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pre-training</term>
					<term>heterogeneous graph</term>
					<term>self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While graph neural networks (GNNs) emerge as the state-of-the-art representation learning methods on graphs, they often require a large amount of labeled data to achieve satisfactory performance, which is often expensive or unavailable. To relieve the label scarcity issue, some pre-training strategies have been devised for GNNs, to learn transferable knowledge from the universal structural properties of the graph. However, existing pre-training strategies are only designed for homogeneous graphs, in which each node and edge belongs to the same type. In contrast, a heterogeneous graph embodies rich semantics, as multiple types of nodes interact with each other via different kinds of edges, which are neglected by existing strategies. In this paper, we propose a novel Contrastive Pre-Training strategy of GNNs on Heterogeneous Graphs (CPT-HG), to capture both the semantic and structural properties in a self-supervised manner. Specifically, we design semantic-aware pre-training tasks at both the relation-and subgraph-levels, and further enhance their representativeness by employing contrastive learning. We conduct extensive experiments on three real-world heterogeneous graphs, and promising results demonstrate the superior ability of our CPT-HG to transfer knowledge to various downstream tasks via pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, graphs have become a powerful abstraction for representing a wide variety of real-world datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. As an emerging tool for performing machine learning on graph-structured data, graph neural networks (GNNs) learn powerful graph representations by recursively aggregating content (i.e., features or embeddings) from neighboring nodes, thus preserving both content and structure information. They have been demonstrated to improve the performance in various graph applications, such as node and graph classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, recommendation systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref> and graph generation <ref type="bibr" target="#b20">[21]</ref>. Generally, a GNN model is trained with (semi-)supervised information in an end-to-end manner, requiring a large volume and variety of labeled data for different downstream tasks. However, in most real-world scenarios, abundant labeled data are usually expensive and even infeasible to obtain, while there exist a large amount of unlabeled data that is easily accessible.</p><p>To make full use of the unlabeled graph-structured data, some recent effort takes inspiration from pre-training techniques in natural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> and computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, and propose to pre-train a GNN model with self-supervised information in a graph <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>. Broadly, we classify them into two categories according to their training corpus: one is pre-training on multiple graphs and then fine-tuning on a new unseen graph for downstream tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>, the other is pre-training on a part of a large-scale graph while transferring learned knowledge to downstream tasks on the remaining part <ref type="bibr" target="#b13">[14]</ref>. For instance, Hu et al. <ref type="bibr" target="#b12">[13]</ref> present some GNN pre-training strategies for the setting of multiple graphs, which utilizes both node-level and graph-level self-supervised information in a series of graphs (e.g., molecular structures in the biochemical domain). On the other hand, GPT-GNN <ref type="bibr" target="#b13">[14]</ref> introduces a self-supervised attributed graph generation task to pre-train a GNN, which learns transferable knowledge some parts of a large graph to facilitate the downstream tasks on other parts of the graph with only a few labels. Although these GNN pretraining methods achieve promising performance, all of them are only designed for homogeneous graphs, in which each node or edge belongs to the same type. In contrast, the so-called heterogeneous graphs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, where multiple types of nodes interact with each other via different kinds of edges, are neglected by existing strategies.</p><p>Objects in a complex interaction system can often be organized into heterogeneous graphs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>, which embody rich semantics and distinct structures resulting from multiple types of nodes and edges. As shown in Figure <ref type="figure">1</ref>(a), a toy heterogeneous graph is constructed for bibliographic data, which consists of nodes of Author, Paper, Conference and Term types, as well as edges of Author-Paper, Paper-Conference and Paper-Term types. Different types of nodes or edges often exhibit varying network properties such as degree and clustering coefficient <ref type="bibr" target="#b22">[23]</ref>. For instance, Conference nodes generally have higher degrees than Author nodes. Moreover, the heterogeneity gives rise to more complex semantic contexts that involve multi-party relationships among a number of nodes, such as {ğ‘ 1 , ğ‘ 1 , ğ‘ 1 , ğ‘ 2 }, which describes the semantic context of "two papers on similar topics from the same author". Beyond the toy example in bibliographic data, heterogeneous graphs are also ubiquitous in a wide range of domains, such as in e-commerce with users, products, brands and shops interacting in various ways, and in biology where diseases, proteins and drugs relating to each other. Given their prevalence and expressiveness, designing effective GNN pre-training strategies for heterogeneous graphs becomes critical.</p><p>Challenges and present work. In this paper, we take the first attempt to pre-train GNN models on a large heterogeneous graph, utilizing the intrinsic semantic and structural information as selfsupervision. However, designing pre-training strategies of GNNs for heterogeneous graph is a non-trivial problem, presenting us with two key challenges.</p><p>(1) How to distinguish and tailor to different types of nodes and edges during pre-training? Existing pre-training strategies for GNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> are only designed for homogeneous graphs, which treat all nodes or edges uniformly. As different node and edge types are the defining characteristics of a heterogeneous graph, it is crucial to encode them into a universal basis intrinsic to the graph, without dependence on any task-related label. (2) How to further preserve high-order semantic contexts during pretraining? As motivated in Figure <ref type="figure">1</ref>(a), a heterogeneous graph often embodies complex multi-party relationships involving a number of nodes and edges. Existing pre-training approaches usually utilize simple connectivity structures between node pairs as self-supervised information, which cannot explain highorder semantics on a heterogeneous graph. Hence, it is vital to design more advanced structures for pre-training in order to encode complex semantic contexts flexibly.</p><p>To tackle the above challenges, we develop a novel Constrastive Pre-Training strategy of GNNs for Heterogeneous Graphs, named CPT-HG, which preserves heterogeneity in terms of not only node or edge differences individually, but also high-order semantic contexts among multiple nodes and edges collectively. More specifically, for the first challenge, we design a relation-level pre-training task to differentiate the relation type between two nodes of different types (e.g., Author-Paper and Paper-Conference relations), to encode a universal basis for downstream tasks. To enhance the representativeness of the relation-level samples, inspired by contrastive learning <ref type="bibr" target="#b41">[42]</ref>, we propose to discriminate negative relation-level samples from positive relation-level samples in a contrastive manner. Specifically, we construct the negative relation-level samples from two aspects: (1) negative samples from inconsistent relations where two nodes share an "incorrect" relation distinct from the positive relation, and (2) negative samples from unrelated nodes where two nodes are not linked at all in the graph. To address the second challenge, we propose a subgraph-level pre-training task on a heterogeneous graph. The heterogeneity gives rise to high-order structures such as meta paths <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> and meta graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> that are capable of capturing various semantic contexts, as shown in Figure <ref type="figure">1</ref>(c) and (d). Considering that meta graph is able to capture richer and subtler semantics than meta path <ref type="bibr" target="#b48">[49]</ref>, in our subgraphlevel strategy, we employ meta graph, rather than meta path, to generate subgraph instances including the positive samples and the negative samples, such that the pre-training can encode high-order semantic contexts that are also relevant to different downstream tasks.</p><p>To summarize, we make the following major contributions in this work.</p><p>â€¢ We address the under-explored setting of GNN pre-training on a heterogeneous graph in a self-supervised manner. â€¢ We propose a novel contrastive pre-training strategy for GNNs on heterogeneous graphs, named CPT-HG, which leverages both the relation-and subgraph-level pre-training tasks to contrastively preserve the rich semantics as a form of transferable knowledge for the downstream tasks. â€¢ On three real-world heterogeneous graphs, we conduct extensive experiments and analysis to demonstrate that our CPT-HG significantly outperforms various state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Network</head><p>In recent years, GNNs have received significant attention due to the ability to model graph-structured data, which can naturally capture both graph structures and feature information associated with the graph <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. Originally proposed, as a framework of utilizing neural networks to learn node representations on graphs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, GNN is extended to convolution neural networks using spectral methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref> and message passing architectures to aggregate neighbors' features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. For example, Kipf et al. <ref type="bibr" target="#b17">[18]</ref> have proposed the graph convolutional networks via allocating firstorder approximation of spectral graph convolutions. Moreover, graph attention networks have been proposed to learn the importance between nodes and its neighbors and fuse the neighbors to perform node classification <ref type="bibr" target="#b37">[38]</ref>. Besides, some studies have attempted to deploy the GNNs on a heterogeneous graph <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>. Most of these GNN models can be used as the base leaner for our proposed pre-training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning</head><p>Contrastive learning has recently become a prominent technique in unsupervised learning, which can achieve state-of-the-art results.</p><p>The key idea of contrastive learning is to contrast semantically similar (positive) and dissimilar (negative) pairs of data points.</p><p>There are some methods based on mutual information that measure the loss in the latent space by contrastive samples from different distribution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>. Deep graph infomax <ref type="bibr" target="#b38">[39]</ref> extends the deep infomax <ref type="bibr" target="#b11">[12]</ref> to graphs and obtains better performance in node classification by learning node representations with the contrastive node and graph encoding. Recently, in computer vision, various frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref> for contrastive learning have been proposed, which design the self-supervised task to learn the representation by distinguishing the similar images from the dissimilar ones. Contrastive learning is also utilized in our model to learn the heterogeneous information, and the loss function, called InfoNCE <ref type="bibr" target="#b36">[37]</ref>, is employed in this paper for contrastive loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-training on Graphs</head><p>To enable more effective learning on graphs, researchers have explored how to pre-train GNNs for node-level representations on unlabeled graph data. Inspired by pre-training techniques in natural language processing <ref type="bibr" target="#b4">[5]</ref> and computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46]</ref> have been proposed to pre-train GNNs with self-supervised information. Navarin et al. <ref type="bibr" target="#b24">[25]</ref> have utilized the graph kernel for pre-training, while another work <ref type="bibr" target="#b12">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we give some formal definitions of heterogeneous graphs and related concepts, and formalize the problem of pretraining on heterogeneous graphs.</p><p>Definition 3.1. Heterogeneous Graph. A heterogeneous graph <ref type="bibr" target="#b30">[31]</ref>, denoted as G = {V, E, A, R, ğœ™, ğœ‘ }, is a form of graph, where V and E denote the sets of nodes and edges, respectively. It is also associated with a node type mapping function ğœ™ : V â†’ A and an edge type mapping function ğœ‘ : E â†’ R, where A and R denote the sets of node and edge types such that |A| + |R| &gt; 2.</p><p>The network schema ğ‘‡ G = (A, R) of a heterogeneous graph specifies type constraints on node objects and relationships between the node objects. These constraints make a heterogeneous graph semi-structured, guiding the exploration of the semantics of the network <ref type="bibr" target="#b33">[34]</ref>.</p><p>An example of heterogeneous graph is illustrated in Figure <ref type="figure">1</ref>(a) on bibliographic data with the network schema in Figure <ref type="figure">1(b)</ref>. Observe that it consists of multiple node types (i.e., author, paper, conference and term) and relation types (e.g., paper-author, paperconference). Definition 3.2. Meta Path. A meta path P <ref type="bibr" target="#b33">[34]</ref> is a path defined on the network schema ğ‘‡ G = (A, R) of the heterogeneous graph G, and is denoted in the form of ğ´ 1</p><formula xml:id="formula_0">ğ‘… 1 âˆ’â†’ ğ´ 2 ğ‘… 2 âˆ’â†’ â€¢ â€¢ â€¢ ğ‘… ğ‘™ âˆ’â†’ ğ´ ğ‘™+1 , which defines a composite relation ğ‘… = ğ‘… 1 â€¢ ğ‘… 2 â€¢ â€¢ â€¢ â€¢ â€¢ ğ‘… ğ‘™ between type ğ´ 1</formula><p>and ğ´ ğ‘™+1 , where o denotes the composition operator on relations.</p><p>Meta path is a basic analysis tool for heterogeneous graphs, which can extract sub-structure from heterogeneous graphs and embody rich semantics. For example, as illustrated in Figure <ref type="figure">1</ref>(c), the meta path "Paper-Conference-Paper" (PCP) indicates the semantics of two papers published in the same conference.</p><formula xml:id="formula_1">Definition 3.3. Meta Graph. A meta graph [8] can be repre- sented as ğ‘€ = (V ğ‘€ , E ğ‘€ , A ğ‘€ , R ğ‘€ ) , where V ğ‘€ âŠ† V, E ğ‘€ âŠ† E constrained by A ğ‘€ âŠ† A and R ğ‘€ âŠ† R, respectively. On a hetero- geneous graph G, let M = {ğ‘€ 1 , ğ‘€ 2 , â€¢ â€¢ â€¢ , ğ‘€ |M | } be a set of meta graphs.</formula><p>In contrast to meta path, a meta graph can be viewed as a combination of multiple meta paths and embodies richer and subtler semantics. On the other hand, a meta path can also be considered as a special case of a meta graph. For example, in Figure <ref type="figure">1(d)</ref>, the meta graph M 1 (P-A/C-P) indicates the semantics of two papers on similar topics from the same author, which can not indicated by a single meta path.</p><p>In the paper, similar to the setting of pre-training strategy on homogeneous graph <ref type="bibr" target="#b13">[14]</ref>, we pre-train the model on a part of largescale heterogeneous graph, while fine-tune the downstream task on the remaining part. Definition 3.4. Pre-training on Heterogeneous Graph. For a heterogeneous graph G, we split the whole graph into two parts for pre-training and fine-tuning. We denote the pre-training graph and fine-tuning graph as G ğ‘ğ‘Ÿğ‘’ and G ğ‘“ ğ‘–ğ‘›ğ‘’ , respectively. The strategies on heterogeneous graph largely follow a two-step paradigm: 1) Pre-training the GNN ğ‘“ ğœƒ based on the graph G ğ‘ğ‘Ÿğ‘’ . The learned parameter ğœƒ is expected to capture the semantic information and structural properties as transferable knowledge. 2) Fine-tuning the pre-trained GNN model with the initial parameter ğœƒ for downstream tasks on the graph G ğ‘“ ğ‘–ğ‘›ğ‘’ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED CPT-HG MODEL</head><p>In this section, we develop a novel contrastive pre-training strategy for GNNs on a heterogeneous graph, which preserves the heterogeneous semantics at both the relation-and subgraph-level. Figure <ref type="figure">2</ref> presents the overall framework of our proposed CPT-HG. We design semantic-aware pre-training tasks at both the relation-and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgraph-level pretext tasks on Metagraph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relation-level Pre-training Task</head><p>In existing methods, positive and negative nodes/edges are only differentiated by network structures, such as through a perturbation of the original graph structures and attributes <ref type="bibr" target="#b38">[39]</ref>. However, a heterogeneous graph brings rich semantics in multiple types of relations, and thus it is crucial for the learned transferable knowledge to encode such semantics for the downstream tasks. Given a positive triple âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ on a heterogeneous graph G, nodes ğ‘¢ âˆˆ V and ğ‘£ âˆˆ V are connected via relation instances of type ğ‘… âˆˆ R, such as âŸ¨ğ‘ 1 , ğ‘ƒ-ğ´, ğ‘ 1 âŸ© in Figure <ref type="figure">1(a)</ref>. Here P ğ‘Ÿğ‘’ğ‘™ is the set of positive triples for the pre-training task. We propose to construct negative samples in two ways, including negative samples from inconsistent relations and negative samples from unrelated nodes.</p><p>Negative Samples from Inconsistent Relations. As motivated, on a heterogeneous graph, it is imperative to distinguish the positive and negative samples under a specific relation, which retains the relational semantics between two nodes. Thus, CPT-HG is designed to have the ability of evaluating the "authenticity" w.r.t. a relation ğ‘… between a given pair of nodes ğ‘¢ and ğ‘£ .</p><p>Given the positive triple âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ , there exists a node ğ‘¤ connecting with ğ‘¢ under a relation type ğ‘… âˆ’ that is inconsistent with ğ‘…. Thus, the triple âŸ¨ğ‘¢, ğ‘… âˆ’ , ğ‘¤âŸ© represents a different semantic context from âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ©. For example, in Figure <ref type="figure">2</ref> Negative Samples from Unrelated Nodes. To discriminate the positive samples from unconnected structures, we further generate negative samples from unrelated nodes. Existing negative sampling methods simply select edges that is not in the graph <ref type="bibr" target="#b5">[6]</ref>. However, such negative samples are believed to be easily distinguishable from the positive samples. Thus, we construct the negative samples from the ğ‘˜-hop neighbors of node ğ‘¢. To be specific, given the positive triple âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ , we define the negative samples from unrelated nodes as:</p><formula xml:id="formula_2">N ğ‘›ğ‘œğ‘‘ğ‘’ âŸ¨ğ‘¢,ğ‘…,ğ‘£ âŸ© = {âŸ¨ğ‘¢, * , ğ‘£ âˆ’ âŸ©|ğ‘£ âˆ’ âˆˆ V}. (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ğ‘£ âˆ’ is the the ğ‘˜-hop neighbors of node ğ‘¢ and * means any composite relation connecting ğ‘¢ and ğ‘£ âˆ’ . In our implementation, we set ğ‘˜ = 5 for negative samples from unrelated nodes. We generate the negative samples from the above two strategies to conduct contrastive learning. Thus, to capture the relation information, we minimize the loss for a positive triple âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ with two types of negative samples. For the negative samples with an inconsistent relation, i.e., N ğ‘Ÿğ‘’ğ‘™ , we calculate the following contrastive loss:</p><formula xml:id="formula_4">L ğ‘Ÿğ‘’ğ‘™1 = âŸ¨ğ‘¢,ğ‘…,ğ‘£ âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ âˆ’ log exp h âŠ¤ ğ‘¢ W ğ‘… h ğ‘£ ğ‘– âˆˆ {ğ‘£ }âˆª{ğ‘¤ | âŸ¨ğ‘¢,ğ‘… âˆ’ ,ğ‘¤ âŸ© âˆˆN ğ‘Ÿğ‘’ğ‘™ âŸ¨ğ‘¢,ğ‘…,ğ‘£âŸ© } exp h âŠ¤ ğ‘¢ W ğ‘… h ğ‘– ,<label>(3)</label></formula><p>where W ğ‘… âˆˆ R ğ‘‘Ã—ğ‘‘ is a learnable relation weight matrix for relation ğ‘…. Here h ğ‘¢ indicates the representation vector of node ğ‘¢, which is generated by any existing GNN architecture.</p><p>For the negative samples from unrelated nodes, i.e., N ğ‘›ğ‘œğ‘‘ğ‘’ , we have the following loss function:</p><formula xml:id="formula_5">L ğ‘Ÿğ‘’ğ‘™2 = âŸ¨ğ‘¢,ğ‘…,ğ‘£ âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ âˆ’ log exp h âŠ¤ ğ‘¢ h v ğ‘– âˆˆ {ğ‘£ }âˆª{ğ‘£ âˆ’ | âŸ¨ğ‘¢, * ,ğ‘£ âˆ’ âŸ© âˆˆN ğ‘›ğ‘œğ‘‘ğ‘’ âŸ¨ğ‘¢,ğ‘…,ğ‘£âŸ© } exp h âŠ¤ ğ‘¢ h i .</formula><p>(4) To integrate the information of the two types of negative samples, we calculate the relation-level pre-training loss as:</p><formula xml:id="formula_6">L ğ‘Ÿğ‘’ğ‘™ = L ğ‘Ÿğ‘’ğ‘™1 + L ğ‘Ÿğ‘’ğ‘™2 . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>So far, we utilize relation discrimination for discovering and persevering the rich semantics in a heterogeneous graph at relation level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Subgraph-level Pre-training Task</head><p>In order to preserve the high-order semantic contexts on a heterogeneous graph, a natural idea is to employ meta paths to explore highorder relations, thereby incorporating richer semantics <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. However, there are two weaknesses of the widely used meta paths for pre-training GNNs on heterogeneous graphs: (1) Meta paths have been shown to have limited ability to characterize rich semantics and extract high-order structures, compared to meta graphs <ref type="bibr" target="#b15">[16]</ref>.</p><p>(2) Starting from a source node, the number of nodes a meta path can reach is often too large, while the number of nodes a meta graph from the same source node can cover is fewer due to a more complex and restrictive structure, which makes meta graph more efficient than meta path. Therefore, we generate meta graph instances to construct the positive samples, which capture the elaborate subgraph structure and the subtle semantic information on a heterogeneous graph. Besides, we take inspiration from computer vision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> to generate queued negative samples and distinguish the positive and negative samples with contrastive learning <ref type="bibr" target="#b36">[37]</ref>.</p><p>Structural Positive Samples. Given a meta graph ğ‘€ âˆˆ M and a source node ğ‘¢, we define and construct a meta graph instance ğ‘š âˆˆ I (ğ‘€) as a set of nodes surrounding ğ‘¢ such that they match the meta graph ğ‘€, where I (ğ‘€) denotes the set of all instances of ğ‘€. For example, in Figure <ref type="figure">2(c)</ref>, for the meta graph ğ‘€ = ğ‘ƒ-(ğ¶/ğ´)-ğ‘ƒ, we have a meta graph instance as ğ‘š = {ğ‘ 1 , ğ‘ 1 , ğ‘ 1 , ğ‘ 2 }. Based on this definition, we generate the meta graph-based structural positive samples w.r.t. the source node ğ‘¢ as</p><formula xml:id="formula_8">P ğ‘ ğ‘¢ğ‘ ğ‘¢ = ğ‘š âˆˆI (ğ‘€), ğ‘€ âˆˆM ğ‘š \ {ğ‘¢}, (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where M is the set of pre-defined meta graphs. Here, we choose the set of meta graphs with the domain knowledge <ref type="bibr" target="#b15">[16]</ref>. Intuitively, the structural positive samples capture not only denser local structural connectivity, but also subtler semantic contexts.</p><p>Queued Negative Samples. To construct negative samples, inspired by the dynamic generation of negative samples <ref type="bibr" target="#b10">[11]</ref>, we design queued negative samples. To be specific, based on positive samples from previous batches in the training process, we generate a negative sample by adding the positive sample from the most recent batch and remove the oldest one. Assuming that the most recent positive sample of the previous training is P ğ‘ ğ‘¢ğ‘ ğ‘– (ğ‘¡ âˆ’ 1) w.r.t. node ğ‘–, then we have the current negative samples as:</p><formula xml:id="formula_10">N ğ‘ ğ‘¢ğ‘ = [P ğ‘ ğ‘¢ğ‘ ğ‘– (ğ‘¡ âˆ’ 1), P ğ‘ ğ‘¢ğ‘ ğ‘— (ğ‘¡ âˆ’ 2), â€¢ â€¢ â€¢ ].<label>(7)</label></formula><p>Note that we randomly initialize the queue of negative samples and then update the queue during model training. For example, in the Figure <ref type="figure">2</ref>(c), the positive sample {ğ‘ 1 , ğ‘ To incorporate high-order semantic contexts, we then model the likelihood that the source node ğ‘¢ is associated with the positive samples while unrelated with the negative samples as follows:</p><formula xml:id="formula_11">L ğ‘ ğ‘¢ğ‘ = âˆ’ ğ‘¢ âˆˆV ğ‘ƒ + âˆˆ P ğ‘ ğ‘¢ğ‘ ğ‘¢ log exp h âŠ¤ ğ‘¢ ğ‘“ (ğ‘ƒ + ) ğ‘ƒ âˆˆ {ğ‘ƒ + }âˆªN ğ‘ ğ‘¢ğ‘ exp h âŠ¤ ğ‘¢ ğ‘“ (ğ‘ƒ) ,<label>(8)</label></formula><p>where ğ‘“ (â€¢) is a pooling function (e.g., mean pooling in our work) for obtaining the representation of nodes. Intuitively, by optimizing L ğ‘ ğ‘¢ğ‘ , CPT-HG is capable of capturing the semantic information through high-order local structures on a heterogeneous graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Optimization</head><p>Altogether, to capture the semantic information and preserve the high-order structures at relation-and subgraph-levels, we minimize the following loss for a heterogeneous graph G:</p><formula xml:id="formula_12">L = L ğ‘ ğ‘¢ğ‘ + ğœ†L ğ‘Ÿğ‘’ğ‘™ ,<label>(9)</label></formula><p>where ğœ† is a balancing coefficient. We adopt the Adam <ref type="bibr" target="#b16">[17]</ref> optimizer to train the proposed CPT-HG and repeat the relation-and subgraph-levels pre-training tasks for more iterations until the model converges.</p><p>Algorithm. The model training for CPT-HG is outlined in Algorithm 1. Its basic idea is to generate the negative samples at both levels for conducting contrastive learning. First, we initialize an empty negative queue to store the previously-sampled structural positive samples as more meaningful negative samples. Given node ğ‘¢ in the heterogeneous graph, we generate the negative relationlevel samples from inconsistent relations and unrelated nodes in line 4. If we sample too many negative relation-level samples that meet the condition in Eq. ( <ref type="formula">1</ref>) and Eq. ( <ref type="formula" target="#formula_2">2</ref>), we remove some negative samples randomly if the number of negative samples exceeds the maximum setting. Generate the negative samples with Eq. ( <ref type="formula">1</ref>) and Eq. (</p><p>Calculate L ğ‘Ÿğ‘’ğ‘™ by Eq. ( <ref type="formula" target="#formula_6">5</ref>)</p><p>6:</p><p>Generate the structural positive samples P ğ‘ ğ‘¢ğ‘ ğ‘¢ with Eq. ( <ref type="formula" target="#formula_8">6</ref>)</p><p>7:</p><p>for each structural positive sample ğ‘ƒ + âˆˆ P ğ‘ ğ‘¢ğ‘ ğ‘¢ do 8:</p><p>Calculate L ğ‘ ğ‘¢ğ‘ by Eq. ( <ref type="formula" target="#formula_11">8</ref>)</p><p>9:</p><p>end for 10:</p><p>Update N ğ‘ ğ‘¢ğ‘ with the structural positive sample ğ‘ƒ + 11: end for 12: Calculate L by Eq. ( <ref type="formula" target="#formula_12">9</ref>) 13: Update parameters ğœƒ 14: return the GNN model ğ‘“ ğœƒ * predefined meta graphs, we generate the structural positive samples to preserve high-order semantic contexts. Then, we calculate the subgraph-level loss for each structural positive samples. After that, we progressively update the negative queue by adding the latest structural positive samples and remove the oldest ones in line 10. Moreover, the structural samples extracted from different nodes can bring in the high-order semantic contexts for contrastive learning. Then, we calculate the loss in Eq. ( <ref type="formula" target="#formula_12">9</ref>) to capture the semantic information and preserve the high-order structures. Afterwards, we can obtain pre-trained model parameters for downstream tasks.</p><p>Discussion. The proposed pre-training framework can be universally applied to different GNN models since the pre-training tasks are not related to the implementation of GNNs. In contrast to the previous pre-training methods on a homogeneous graph, our approach can deal with various types of nodes and relations and fuse rich semantic contexts on a heterogeneous graph. In particular, our method can be applied to pre-train both heterogeneous <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref> and homogeneous <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref> GNNs alike. Our semantics-aware pre-training tasks directly enable homogeneous GNNs to capture transferable semantics on the one hand, and further enhance heterogeneous GNNs in their semantic expressiveness and generalization ability to downstream tasks on the other hand.</p><p>We next conduct a complexity analysis of our pre-training procedure. The time complexity consists of two parts: (1) the time complexity of learning node representations with GNNs, which depends on the architectures of the base GNN. Here, we denote it as ğ‘‹ ; (2) the time complexity of generating pre-training tasks, which is linear w.r.t. the number of the negative samples. We denote the number of negative samples at both relation-and subgraph-levels as ğ‘˜ 1 and ğ‘˜ 2 , respectively. The average size of structural positive samples and positive triples is denoted as |P ğ‘ ğ‘¢ğ‘ | and |P ğ‘Ÿğ‘’ğ‘™ |, respectively. Thus, the time complexity of our CPT-HG is ğ‘‚ ğ‘‹</p><formula xml:id="formula_14">+ ğ‘ ğ‘˜ 1 |P ğ‘Ÿğ‘’ğ‘™ | + ğ‘˜ 2 |P ğ‘ ğ‘¢ğ‘ | ğ‘‘ 2 ,</formula><p>where ğ‘‘ and ğ‘ is the embedding dimension and the number of nodes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first conduct extensive experiments on three realworld datasets to evaluate model performance, and then investigate the underlying mechanism of CPT-HG with two ablated models and different GNN architectures. Lastly, we explore the impact of the model settings on task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Dataset. We conduct experiments on three datasets, namely DBLP <ref type="bibr" target="#b8">[9]</ref>, Yelp <ref type="bibr" target="#b43">[44]</ref> and Aminer <ref type="bibr" target="#b34">[35]</ref>. The detailed statistics of three datasets are summarized in Table <ref type="table" target="#tab_4">1</ref> Pre-Training and Fine-Tuning Setting. We pre-train a GNN model and adopt the pre-trained model weights to initialize models for downstream tasks. Then we fine-tune the GNN models according to the specific downstream tasks on an unseen fine-tuning graph and evaluate the model performance. Specifically, for each dataset, we randomly split the whole graph into two graphs for pre-training and fine-tuning. In DBLP dataset, we randomly split DBLP into pre-training and fine-tuning graphs which respectively contains 50% authors and the other associated nodes. In Yelp dataset, with the ratio of 3:1 of Business node, we randomly split Yelp into pretraining and fine-tuning graphs. In Aminer dataset, we construct the pre-training graph from 11 random fields, and the left is the fine-tuning graph.</p><p>Baselines. We compare our proposed CPT-HG with the state-ofart baselines, including a no pre-train method, three graph neural network based methods with unsupervised objectives (i.e., GAE, EdgePred, DGI), and a pre-training method (i.e., GPT-GNN).</p><p>â€¢ No pre-train method adopts the GNN model to learn node representations and then conducts downstream tasks on the finetuning graph.</p><p>â€¢ GAE <ref type="bibr" target="#b18">[19]</ref> focuses on a traditional link prediction task. It randomly masks out a fixed proportion of the edges and train the model to reconstruct these masked edges. â€¢ EdgePred <ref type="bibr" target="#b9">[10]</ref> predicts the connectivity of node pairs and forces connected nodes to have similar node embeddings. â€¢ DGI <ref type="bibr" target="#b38">[39]</ref> maximizes local mutual information across the graph's patch representations. â€¢ GPT-GNN <ref type="bibr" target="#b13">[14]</ref> is a state-of-art model for pre-training GNNs, which reconstructs the attributes and structure of the input graph to learn the transferable knowledge from the input graph.</p><p>It is worth noting that our CPT-HG can be implemented for different GNN models. Here, we mainly study HGT <ref type="bibr" target="#b14">[15]</ref> expressive and state-of-the-art GNN architecture for heterogeneous graphs. We also experiment with other popular architectures in Section 5.3, including GCN <ref type="bibr" target="#b17">[18]</ref>, GAT <ref type="bibr" target="#b37">[38]</ref>, GraphSAGE <ref type="bibr" target="#b9">[10]</ref> and RGCN <ref type="bibr" target="#b29">[30]</ref>.</p><p>Parameter Settings. We implement our CPT-HG with PyTorch <ref type="bibr" target="#b26">[27]</ref> and adopt Adaptive Moment Estimation (Adam) <ref type="bibr" target="#b16">[17]</ref> optimizer to train the proposed CPT-HG. In the pre-training procedure, we set the dimension of node representation to 64, the number of the base GNN layers to 2, and the head of attention to 1 for all methods. The learning rate is arrange from [0.01, 0.008, 0.005, 0.001]. For the balancing coefficient ğœ† in Eq. ( <ref type="formula" target="#formula_12">9</ref>), we set it as 0.5 in the pre-training procedure. We use early stopping based the performance on validation set with a patience of 10 epochs for model training. For the other parameters of the baselines, we optimize them empirically under the guidance of literature. For our CPT-HG, the maximum number of negative samples from inconsistent relations and unrelated nodes are set to 100 and 200, respectively. The maximum number of queued negative samples at subgraph-level is set to 100. In the pre-training procedure at both levels, we will remove some negative samples randomly if the number of negative samples exceeds the maximum setting. During the experiment, we observe that the number of negative samples, which are generated according to the aforementioned rules, falls within the range of maximum setting in most cases and thus the removal is usually needless to be performed.</p><p>Evaluation Protocol. Following the previous work <ref type="bibr" target="#b13">[14]</ref>, we first pre-train the model (including the baselines and our CPT-HG ) by utilizing the self-supervised information in the pre-training graph. Then we fine-tune the pre-trained model with labeled information in downstream tasks (e.g., link prediction). The downstream experiments are run with 10 random seeds, and we report the average experiment results and standard deviation on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head><p>In this section, we empirically compare CPT-HG to baselines in two downstream tasks, including link prediction and node classification.</p><p>Link Prediction. After pre-training our CPT-HG and the baselines, we apply the pre-trained model on the fine-tuning graph to predict edges. Specifically, we consider the prediction of Paper-Term in DBLP dataset, Business-Location in Yelp dataset, as well as Paper-Author and Paper-Conference in Aminer dataset. During the fine-tuning process, we randomly divide the edges to be predicted (e.g., Paper-Author in Aminer) with the ratio of 8:1:1 to construct the training, validation and test sets. Following <ref type="bibr" target="#b29">[30]</ref> , we randomly sample the same number of unconnected node pairs as the training set to serve as negative samples for model optimization. At last, we minimize the cross-entropy loss to train the GNN model in fine-tuning process, and evaluate the prediction performance with MRR metric <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Table <ref type="table" target="#tab_5">2</ref> demonstrates the link prediction performance on three datasets. Overall, our CPT-HG consistently yields the best performance among all methods on three datasets, which brings an MRR improvement by 2.16%-6.81% compared to the best baseline. The significant improvement attributes to the structural and semantics information modeling for heterogeneous graphs. Compared to the no pre-train baseline, our CPT-HG significantly improves the link prediction performance by 5.83%, 2.64% and 10.85% on three datasets, respectively. The improvements suggest that the contrastive pre-training on heterogeneous graphs is capable of learning transferable and informative knowledge for the downstream tasks. Among different baselines, conventional graph neural network based methods (e.g., GAE) achieve the unsatisfactory performance due to the insufficient use of pre-training graphs. The GPT-GNN performs better by generative pre-training on subgraphs, so as to learn transferable knowledge. However, they still underperform our proposed CPT-HG. We believe the reason is that the relation-and subgraph-level pre-training tasks allow our CPT-HG to make full use of structures and semantics on a heterogeneous graph.</p><p>Node Classification. To evaluate model performance in node classification task, the node representation learned by the pre-trained model is fed into a linear classifier to predict the node label. Following <ref type="bibr" target="#b29">[30]</ref>, we randomly split the labeled nodes with the ratio of 1:2:7 for training, validation and test set. Since there is no label information in Yelp dataset, here we conduct experiments on DBLP and Aminer datasets, and adopt the accuracy as evaluation metric.</p><p>As presented in Table <ref type="table" target="#tab_6">3</ref>, we can find that CPT-HG performs consistently much better than all baselines on two datasets. As observed in link prediction tasks, no pre-train method is least competitive due to the lack of rich information in pre-training subgraphs. Generally, GNN based methods which combine the structure and feature information, e.g., DGI, usually perform better than no pre-train method. This indicates that the pre-training process with self-supervision provides useful and discriminated information for node classification. On DBLP dataset, we also observe that the baseline GAE generally achieve better performance than other methods. The reason might be that GAE mainly on modeling relations while relations in DBLP brings much more effective information for node representation than structural information.  <ref type="table" target="#tab_7">4</ref> and Figure <ref type="figure" target="#fig_4">3</ref>, we report the performance of two ablated models and the improvement over no pre-train baseline in link prediction and node classification tasks. Overall, all the comparison methods are better than no pretrain baseline as they all achieve significant improvements in three datasets. In particular, our complete CPT-HG achieves the greatest improvement in most cases, indicating the necessary for jointly capturing semantic relations and subgraph structures for pre-training heterogeneous graphs. Compared to CPT-HG ğ‘ ğ‘¢ğ‘ , the improvement brought by CPT-HG ğ‘Ÿğ‘’ğ‘™ is more significant. Despite CPT-HG ğ‘ ğ‘¢ğ‘ encodes structures of graphs, the semantic relations encoded in  CPT-HG ğ‘Ÿğ‘’ğ‘™ seem more important for node representations in heterogeneous graph <ref type="bibr" target="#b22">[23]</ref>. We also observe that CPT (i.e., HGT, RGCN) and there homogeneous GNNs (i.e., GCN, GAT, GraphSAGE), for studying the universality of CPT-HG. Since similar trends are observed in the three datasets, here we only report the link prediction performance on the largest Aminer dataset. As presented in Table <ref type="table" target="#tab_8">5</ref>, our proposed CPT-HG is capable of enhancing the downstream task performance for most GNN architectures. Besides, the pre-trained HGT achieves the best performance gain among all the GNN models. We think the reason is that CPT-HG can enhance the GNN model performance with the transferable semantic and structural properties on heterogeneous graph. We also observe that the pre-trained GAT obtains an unsatisfactory performance and the reason may be that it's intractable to learn a proper attention heads between pre-training and fine-tuning graphs, and thus it performs worse than no pre-train model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Setting Analysis</head><p>Lastly, we investigate the impact of experimental settings on the model performance, including the impact of meta graphs and the size of the pre-training subgraph. Similar to previous analysis, we showcase the results on Aminer dataset since the observations are similar in other datasets.</p><p>Impact of Meta Graphs. Following some previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b48">49]</ref>, we empirically validate the performance of CPT-HG with different meta graphs, including PAP, PCP, PATP and PACP. For example, we only utilize the meta graph PAP to conduct the subgraph-level pre-training tasks, denoted as CPT-HG ğ‘ƒğ´ğ‘ƒ and we utilize all meta graphs for our original CPT-HG. In Table <ref type="table" target="#tab_9">6</ref>, we report the results of different meta graphs compared to the original model. We observe that pre-training our CPT-HG with some meta graphs (e.g., PAP and PCP) hardly learns semantics and structures on a heterogeneous graph and even reduces the model performance. of the meta graph becomes more complex (e.g., PACP), our CPT-HG achieves more performance gains by encoding more structural information in a graph. In all, the CPT-HG with meta graphs (i.e., PATP and PACP) achieves the best and significant performance improvement due to preserving more rich and subtle semantics, which confirms the benefit of meta graph employed in CPT-HG .</p><p>Impact of Pre-training Subgraph Size. We also explore how the size of pre-training subgraph would affect the model performance, and we utilize {10%, 50%, 100%} percentages of pre-training subgraphs to pre-train the base GNN for downstream tasks. The fine-tuning setting is the same as that in Section 5.2 for fair comparison. In Table <ref type="table" target="#tab_10">7</ref>, we observe that CPT-HG consistently improves the link prediction performance with more pre-training dataset. When the pre-training subgraph is much small (e.g., 10%), our CPT-HG hardly learns useful semantic and structural information for the downstream task, leading to poor performance. This phenomenon demonstrates that the superiority of GNN pre-training requires a large-scale pre-training subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we took the first attempt to pre-train GNN models on a heterogeneous graph, and presented a novel contrastive pre-training strategy for GNNs on heterogeneous graphs, named CPT-HG. To capture the semantic and structural information on the graph in a transferable form, we leverage both the relationand subgraph-level pre-training tasks through contrastive learning, which utilize self-supervised information intrinsic to heterogeneous graphs. At the relation level, we designed a pre-training task to differentiate the relation type between two nodes, which encodes the fundamental characteristics of a heterogeneous graph. At the subgraph-level, we proposed a pre-training task to differentiate the subgraph instances of different meta graphs, which encodes the high-order semantic contexts. Extensive experiments on three real-world heterogeneous graphs demonstrated promising results and the superior ability of our CPT-HG to transfer knowledge to various downstream tasks via pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 ğ‘ 2 ( 2 Figure 1 :</head><label>1221</label><figDesc>Figure 1: A toy example of heterogeneous graph for bibliographic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>Figure 2: The overall framework of CPT-HG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), the triple âŸ¨ğ‘ 1 , ğ‘ƒ-ğ´, ğ‘ 1 âŸ© under the relation ğ‘ƒ-ğ´ indicates that the paper ğ‘ 1 is written by the author ğ‘ 1 , while the relation ğ‘ƒ-ğ¶ connecting papers and conferences, e.g., âŸ¨ğ‘ 1 , ğ‘ƒ-ğ¶, ğ‘ 1 âŸ©, represents that the paper ğ‘ 1 is published in the conference ğ‘ 1 , which can be treated as a negative sample of âŸ¨ğ‘ 1 , ğ‘ƒ-ğ´, ğ‘ 1 âŸ©. Thus, we construct the negative triples âŸ¨ğ‘¢, ğ‘… âˆ’ , ğ‘¤âŸ© with the inconsistent relation ğ‘… âˆ’ drawn from the inconsistent relation set R âˆ’ = R \ {ğ‘…}. Formally, for the positive triple âŸ¨ğ‘¢, ğ‘…, ğ‘£âŸ© âˆˆ P ğ‘Ÿğ‘’ğ‘™ , we define negative samples from inconsistent relations, denoted as N ğ‘Ÿğ‘’ğ‘™ , as follows: N ğ‘Ÿğ‘’ğ‘™ âŸ¨ğ‘¢,ğ‘…,ğ‘£ âŸ© = âŸ¨ğ‘¢, ğœ‘ (ğ‘¢, ğ‘¤), ğ‘¤âŸ©|(ğ‘¢, ğ‘¤) âˆˆ E, ğœ‘ (ğ‘¢, ğ‘¤) âˆˆ R âˆ’ , (1) where nodes ğ‘¢ and ğ‘¤ are connected via relation instances of type ğ‘… âˆ’ and we treat edge types as directed edge (e.g., P-A and A-P are two different relation types). Due to the large scale of N ğ‘Ÿğ‘’ğ‘™ , here we uniformly sample a subset of N ğ‘Ÿğ‘’ğ‘™ for each inconsistent relations to pre-train our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Improvement of different ablated models over no pre-train baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> , ğ‘ 2 } will replace the oldest negative sample in the next batch. It is worth noting that, if the negative samples are generated by a random selection, they are often easily distinguishable from the positive samples on the ground that nodes in negative samples are very likely to be unrelated. Besides, different nodes may have the same structural positive samples. For example, both node ğ‘ 1 and ğ‘ 3 have the same structural positive samples, e.g., {ğ‘ 1 , ğ‘ 1 , ğ‘ 2 } in Figure2(a), which lead to suboptimal contrastive learning. To prevent this situation, for the subgraph-level pre-training task for node ğ‘ 1 , we remove such negative samples from the queue.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Heterogeneous graph G, a GNN model ğ‘“ ğœƒ 1: Initialize model parameters ğœƒ with Xavier initialization 2: Randomly initialize queued negative samples N ğ‘ ğ‘¢ğ‘</figDesc><table /><note>After the negative sampling at the relation level, we calculate the relation-level loss in line 5. Then, following the Algorithm 1 Algorithm framework of CPT-HG Require: 3: for each node ğ‘¢ in G do 4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. â€¢ DBLP is extracted from the computer science bibliography website 1 . According to the domains of published papers, the authors in DBLP are labeled with four research areas, including Database, Data Mining, Artificial Intelligence, and Information Retrieval. â€¢ Yelp is a widely used benchmark dataset, which contains a network of businesses, users, locations and reviews from Yelp Inc 2 . â€¢ Aminer is a bibliographic graphs 3 . The papers in Aminer are labeled with 17 research fields, e.g., Artificial Intelligence, which are used for node classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the three datasets.</figDesc><table><row><cell>, the most</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Experiment results (MRR(%) Â± std) in link prediction task on the three datasets. The best method is bolded, and the second best is underlined.</figDesc><table><row><cell>Dataset</cell><cell>Link Type</cell><cell>No pre-train</cell><cell>GAE</cell><cell>EgePred</cell><cell>DGI</cell><cell>GPT-GNN</cell><cell>CPT-HG</cell><cell>Improv.</cell></row><row><cell>DBLP</cell><cell>Paper-Term</cell><cell cols="6">12.34 Â± 1.43 12.51 Â± 0.71 12.61 Â± 0.44 12.47 Â± 0.68 12.71 Â± 0.35 13.06 Â± 0.42</cell><cell>2.75%</cell></row><row><cell>Yelp</cell><cell>Business-Location</cell><cell cols="6">45.83 Â± 0.42 45.92 Â± 0.52 46.10 Â± 0.31 45.57 Â± 0.64 46.04 Â± 0.75 47.04 Â± 0.71</cell><cell>2.03%</cell></row><row><cell>Aminer</cell><cell>Paper-Conference Paper-Author</cell><cell cols="6">39.23 Â± 1.75 40.31 Â± 0.78 39.86 Â± 1.17 40.74 Â± 1.35 41.37 Â± 0.76 42.17 Â± 1.23 5.63 Â± 0.73 5.71 Â± 0.41 5.62 Â± 0.87 5.84 Â± 0.52 6.02 Â± 0.45 6.43 Â± 0.54</cell><cell>1.93% 6.81%</cell></row><row><cell cols="3">Dataset Labeled Node Type No pre-train</cell><cell>GAE</cell><cell>EgePred</cell><cell>DGI</cell><cell>GPT-GNN</cell><cell>CPT-HG</cell><cell>Improve.</cell></row><row><cell>DBLP</cell><cell>Author</cell><cell cols="6">87.45 Â± 0.43 90.56 Â± 0.73 89.24 Â± 0.57 88.26 Â± 0.66 89.57 Â± 0.45 91.45 Â± 0.54</cell><cell>0.98%</cell></row><row><cell>Aminer</cell><cell>Paper</cell><cell cols="6">92.17 Â± 0.56 92.72 Â± 0.32 93.41 Â± 0.46 92.37 Â± 0.25 93.75 Â± 0.67 96.32 Â± 0.43</cell><cell>2.74%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Experiment results (Accuracy(%) Â± std) in the node classification task on DBLP and Aminer datasets. The best method is bolded, and the second best is underlined.</figDesc><table><row><cell>Downstream Task</cell><cell></cell><cell cols="2">Link Prediction</cell><cell></cell><cell cols="2">Node Classification</cell></row><row><cell>Dataset</cell><cell>DBLP</cell><cell>Yelp</cell><cell>Aminer</cell><cell></cell><cell>DBLP</cell><cell>Aminer</cell></row><row><cell>Link/Labeled Node Type</cell><cell>Paper-Term</cell><cell>Business-Location</cell><cell>Paper-Conference</cell><cell>Paper-Author</cell><cell>Paper</cell><cell>Author</cell></row><row><cell>No pre-train</cell><cell>12.34 Â± 1.43</cell><cell>45.83 Â± 0.42</cell><cell>39.23 Â± 1.75</cell><cell>5.63 Â± 0.73</cell><cell cols="2">87.45 Â± 0.43 92.17 Â± 0.56</cell></row><row><cell>CPT-HG ğ‘ ğ‘¢ğ‘</cell><cell>12.65 Â± 0.42</cell><cell>47.15 Â± 0.44</cell><cell>41.54 Â± 0.33</cell><cell>6.04 Â± 0.51</cell><cell cols="2">89.57 Â± 0.61 94.14 Â± 0.54</cell></row><row><cell>CPT-HG ğ‘Ÿğ‘’ğ‘™</cell><cell>12.79 Â± 0.56</cell><cell>46.74 Â± 0.65</cell><cell>41.75 Â± 0.65</cell><cell>6.24 Â± 0.15</cell><cell cols="2">92.45 Â± 0.54 95.16 Â± 0.32</cell></row><row><cell>CPT-HG</cell><cell>13.06 Â± 0.42</cell><cell>47.04 Â± 0.71</cell><cell>42.17 Â± 1.23</cell><cell>6.43 Â± 0.54</cell><cell cols="2">91.45 Â± 0.54 96.32 Â± 0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Analysis of different ablated models in various downstream tasks.Ablation Study. We attempt to understand how relation-level and subgraph-level pre-training tasks facilitate the contrastive pretraining on heterogeneous graphs. Towards this end, we conduct an ablation study and consider two ablated variants of CPT-HG namely CPT-HG ğ‘ ğ‘¢ğ‘ and CPT-HG ğ‘Ÿğ‘’ğ‘™ . CPT-HG ğ‘ ğ‘¢ğ‘ only includes the subgraph-level pre-training task for contrastive pre-training that models structure properties on a heterogeneous graph, while CPT-HG ğ‘Ÿğ‘’ğ‘™ only employs relation-level pre-training task to capture semantic information. In Table</figDesc><table><row><cell>5.3 Model Analysis</cell></row><row><cell>In this section, we investigate the underlying mechanism of CPT-</cell></row><row><cell>HG from two perspectives: the ablation study of relation and subgraph-</cell></row><row><cell>level pre-training tasks, as well as the generality analysis of CPT-HG</cell></row><row><cell>for different GNN architectures.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>-HG ğ‘Ÿğ‘’ğ‘™ method achieves the smallest improvement in Yelp dataset, which is reasonable since relations in Yelp is less informative with small number of different types of nodes, as shown in the statistic of Yelp dataset in Table1. On the other hand, CPT-HG ğ‘ ğ‘¢ğ‘ significantly improves the link prediction performance by focusing on modeling subgraph structures in a graph. Analysis of different GNN architectures in link prediction on Aminer dataset.</figDesc><table><row><cell>Base Model</cell><cell cols="2">No pre-train</cell><cell cols="2">CPT-HG</cell><cell>Improvement</cell></row><row><cell>HGT</cell><cell cols="2">5.63 Â± 0.73</cell><cell cols="2">6.42 Â± 0.54</cell><cell>14.0%</cell></row><row><cell>RGCN</cell><cell cols="2">4.15 Â± 0.43</cell><cell cols="2">4.49 Â± 0.50</cell><cell>7.55%</cell></row><row><cell>GCN</cell><cell cols="2">4.79 Â± 0.81</cell><cell cols="2">5.14 Â± 0.62</cell><cell>7.31%</cell></row><row><cell>GAT</cell><cell cols="2">4.83 Â± 0.77</cell><cell cols="2">4.32 Â± 0.89</cell><cell>-10.6%</cell></row><row><cell>GraphSAGE</cell><cell cols="2">5.47 Â± 0.52</cell><cell cols="2">6.02 Â± 0.62</cell><cell>10.24%</cell></row><row><cell cols="2">Model</cell><cell cols="2">Accuracy</cell><cell>Improvement</cell></row><row><cell cols="2">No pre-train</cell><cell cols="2">92.17 Â± 0.56</cell><cell>-</cell></row><row><cell cols="2">CPT-HG ğ‘ƒğ´ğ‘ƒ</cell><cell cols="2">91.52 Â± 0.56</cell><cell>-0.7%</cell></row><row><cell cols="2">CPT-HG ğ‘ƒğ¶ğ‘ƒ</cell><cell cols="2">90.45 Â± 0.41</cell><cell>-1.9%</cell></row><row><cell cols="2">CPT-HG ğ‘ƒğ´ğ‘‡ ğ‘ƒ</cell><cell cols="2">93.12 Â± 0.52</cell><cell>+1.0%</cell></row><row><cell cols="2">CPT-HG ğ‘ƒğ´ğ¶ğ‘ƒ</cell><cell cols="2">94.71 Â± 0.46</cell><cell>+2.6%</cell></row><row><cell cols="2">CPT-HG</cell><cell cols="2">96.32 Â± 0.56</cell><cell>+4.5%</cell></row></table><note>GNN Architecture Analysis. Since our proposed CPT-HG is not limited to the architectures of GNNs, we further apply CPT-HG to five GNN architectures, including two heterogeneous GNNs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Analysis of different meta graphs in node classification on Aminer dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>As the structure Compare the pre-training performance Gain with different percentage of pre-training datasets. Evaluate the Paper-Author link prediction on Aminer.</figDesc><table><row><cell>Percentage</cell><cell>MRR</cell><cell>Improvement</cell></row><row><cell>No pre-train</cell><cell>5.63 Â± 0.56</cell><cell>-</cell></row><row><cell>10%</cell><cell>5.54 Â± 0.16</cell><cell>-1.5%</cell></row><row><cell>50%</cell><cell>5.87 Â± 0.41</cell><cell>+4.2%</cell></row><row><cell>100%</cell><cell>6.43 Â± 0.56</cell><cell>+14.0%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://dblp.uni-trier.de</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.yelp.com/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.aminer.cn/citation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, 61772082, 61702296, 62002029) and the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151). All opinions, findings, conclusions and recommendations are those of the authors and do not reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICML. 21-29</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">MichaÃ«l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1902.07243</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic proximity search on graphs with metagraph-based learning</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 32nd International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>WWW. 2704-2710</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta Structure: Computing Relevance in Large Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Mamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1611.07308</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Deep Generative Models of Graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to Pre-train Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation Structure-Aware Heterogeneous Information Network Embedding</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4456" to="4463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Gori</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monfardini</forename><surname>Gabriele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scarselli</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNN</title>
				<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pre-training Graph Neural Networks with Kernels</title>
		<author>
			<persName><forename type="first">NicolÃ²</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Van Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<idno>abs/1811.06930</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<editor>NIPS, H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;AlchÃ©-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>SIGKDD. 1150-1160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TNN</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A Survey of Heterogeneous Information Network Analysis</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: a structural analysis approach</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD Explor</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Path-Sim: Meta Path-Based Top-K Similarity Search in Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">AMiner: Mining Deep Knowledge from Big Scholar Data</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>WWW. 373</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contrastive Multiview Coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="1906">2019. 1906.05849</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>WWW. 2022-2032</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Comprehensive Survey on Graph Neural Networks. In TNNLS</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graph Wavelet Neural Network</title>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="0216">2020. 2004.00216</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Neural Network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep Learning on Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In TKDE</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph Neural Networks: A Review of Methods and Applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
