<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fuzzy rule based decision trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-08-14">14 August 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xianchang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center of Information and Control</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2, Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Ganjingzi, Dalian</settlement>
									<region>Liaoning</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Sciences</orgName>
								<orgName type="institution">Dalian Ocean University</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center of Information and Control</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2, Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Ganjingzi, Dalian</settlement>
									<region>Liaoning</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
							<email>wpedrycz@ualberta.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<postCode>T6R 2V4 AB</postCode>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">King Abdulaziz University</orgName>
								<address>
									<postCode>21589</postCode>
									<settlement>Jeddah</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Systems Research Institute</orgName>
								<orgName type="institution">Polish Academy of Sciences</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lishi</forename><surname>Zhang</surname></persName>
							<email>lishizhangcc@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center of Information and Control</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2, Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Ganjingzi, Dalian</settlement>
									<region>Liaoning</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Sciences</orgName>
								<orgName type="institution">Dalian Ocean University</orgName>
								<address>
									<postCode>116023</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Research Center of Information and Control</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2, Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Ganjingzi, Dalian</settlement>
									<region>Liaoning</region>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fuzzy rule based decision trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-08-14">14 August 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">C6AC431DBF1727BC6F123C8817B85470</idno>
					<idno type="DOI">10.1016/j.patcog.2014.08.001</idno>
					<note type="submission">Received 11 December 2013 Received in revised form 16 July 2014 Accepted 4 August 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Decision tree Fuzzy classifier Fuzzy rules Fuzzy confidence</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new architecture of a fuzzy decision tree based on fuzzy rulesfuzzy rule based decision tree (FRDT) and provides a learning algorithm. In contrast with "traditional" axis-parallel decision trees in which only a single feature (variable) is taken into account at each node, the node of the proposed decision trees involves a fuzzy rule which involves multiple features. Fuzzy rules are employed to produce leaves of high purity. Using multiple features for a node helps us minimize the size of the trees. The growth of the FRDT is realized by expanding an additional node composed of a mixture of data coming from different classes, which is the only non-leaf node of each layer. This gives rise to a new geometric structure endowed with linguistic terms which are quite different from the "traditional" oblique decision trees endowed with hyperplanes as decision functions. A series of numeric studies are reported using data coming from UCI machine learning data sets. The comparison is carried out with regard to "traditional" decision trees such as C4.5, LADtree, BFTree, SimpleCart, and NBTree. The results of statistical tests have shown that the proposed FRDT exhibits the best performance in terms of both accuracy and the size of the produced trees.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Decision trees are one of the most well-known methods used for extracting classification rules from data. There are several reasons behind their visibility and broad applicability. First, in many cases the accuracy of decision trees is comparable or higher than the accuracy of other classification models <ref type="bibr" target="#b0">[1]</ref>. Second, most decision trees do not require a large number of parameters to be adjusted in their design <ref type="bibr" target="#b1">[2]</ref>. Third, due to their intuitively appealing topology, the resulting classification models become easy to comprehend <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Decision trees are one of the most well-known classification methods, and there are many decision tree induction algorithms encountered in the literature. These traditional decision trees could generally be divided into three categories: decision trees (such as the classic ID3 <ref type="bibr" target="#b4">[5]</ref> and C4.5 <ref type="bibr" target="#b5">[6]</ref>, and the recently proposed decision trees <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>), fuzzy decision trees (such as the wellknown Fuzzy ID3 <ref type="bibr" target="#b13">[14]</ref> and the recently proposed fuzzy decision trees <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> and oblique decision trees (such as the well-known CART <ref type="bibr" target="#b21">[22]</ref>, and other oblique decision trees <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>).</p><p>Decision trees and fuzzy decision trees grow in a top-down way when we successively partition the training data into subsets having similar or the same output (class labels). Usually, the growth of the tree terminates when all data associated with a node belong to the same class <ref type="bibr" target="#b28">[29]</ref>. Most of the decision trees and fuzzy decision trees partition the training data into subsets by involving in this process only a single feature, thus, the boundaries of partition regions are parallel to one of the axis of the feature space (the structure of the classification regions is thus axisparallel). When the data are more suitable to be partitioned by hyperplanes that are not axis-parallel, the decision trees and fuzzy decision trees may produce complicated structures (typically oversized) and yield inaccurate results <ref type="bibr" target="#b2">[3]</ref>. In this case, oblique decision trees are more suitable. Most oblique decision trees are associated with a linear decision function positioned at each node <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. However, Erick and Chandrika pointed out that oblique trees are difficult to interpret <ref type="bibr" target="#b27">[28]</ref>.</p><p>In light of these observations, we can draw a conclusion that axis-parallel decision trees cannot find the oblique geometric structures. On the other hand, oblique decision trees exhibit a lack of linguistic interpretation and transparency. Motivated by this, in this study, we develop a new class of decision treefuzzy Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr rule based decision tree (FRDT) for data with continuous features to capture the oblique geometric structure of class region and endow with a readable linguistic interpretation. The main idea is to form a node for each class at each level of the hierarchy while every node implies an oblique geometric structure represented by a fuzzy rule. The fuzzy rules are extracted by the proposed association rules extraction algorithm (AREA). The formation of these fuzzy rules is guided by a criterion of Fuzzy Confidence, which is used here instead of the criteria of impurity measures (such as Gain ratio <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and Gini index <ref type="bibr" target="#b21">[22]</ref>) being commonly considered in decision trees.</p><p>The main procedure of the proposed FRDT can be highlighted as follows (refer to Fig. <ref type="figure" target="#fig_0">1</ref>). At the root node of the tree, a single fuzzy rule is extracted by AREA to mine a major oblique geometric structure for every class to form a "pure" (homogeneous) leaf node. The samples which are not covered by these fuzzy rules of first layer are arranged in an additional "impure" node (including a mixture of data coming from different classes). If the impure node is not empty, the growth of the FRDT is realized by expanding the impure node as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In this procedure some new fuzzy rules are formed for the impure node. In the sequel, these new fuzzy rules are employed to produce new pure nodes, meanwhile a new impure node is added to collect all patterns for which class assignment cannot be realized by using these new rules existing at this layer of the tree. The process is repeated until the termination criterion (such as the newly added impure node becomes empty) has been satisfied.</p><p>The proposed FRDT is different from the axis-parallel decision trees and the oblique decision trees in the sense that the former just considers only a single feature at each node, while the latter takes into account a hyperplane as a decision function. FRDT exhibits a number of visible differences. First of all, at each nonterminal node of the proposed tree, fuzzy rules are employed to describe some oblique geometric structures and to extract some pure leaf nodes. Obviously, this gives rise to a completely new geometry of the partition of the feature space which becomes quite different from the one associated with the "traditional" decision trees. Then, an additional impure node is added for those samples that cannot be assigned to the classes by fuzzy rules of each layer of the tree. The growth of the FRDT tree is realized by just expanding the single added impure node at each layer, while in case of the traditional trees, we have to check each node and expand all non-impure nodes. Thus, the tree of FRDT has only a single trunk (namely the tree of FRDT has only one non-leaf node at each layer), and save great more searching time. Finally, searching of the fuzzy rule realized by the proposed AREA algorithm is guided by Fuzzy Confidence, which is used instead of those impurity measures (such as Gain ratio and Gini index) in the "traditional" decision trees.</p><p>The proposed FRDT arises as a new architecture of the fuzzy decision tree based on fuzzy rules. The additional impure node is added for those samples that cannot be assigned to the classes with the defined fuzzy rules at every layer of the tree. Some new fuzzy rules are formed on the additional impure node. The samples located at the impure node give rise to some fuzzy rules for the new leaf nodes. These leaf nodes do not affect the new fuzzy rules in the next layer. The same holds for the successive layers. Along with the growth of the tree, more new fuzzy rules are formed for the samples in the impure nodes. Thus new fuzzy rules are determined for the added impure node as the growth of the FRDT tree and the fuzzy rules control the expanding of the FRDT tree progresses.</p><p>It has been found experimentally that the proposed FRDTs do not require pruning. A series of numeric studies are reported for UCI machine learning data sets. A comparative analysis is completed for other trees such as C4.5, LADtree, BFTree, SimpleCart, and NBTree. The results of statistical tests have shown that the proposed FRDT exhibits the best performance in terms of both accuracy and the size of the produced trees.</p><p>The paper is organized as follows: Section 2 introduces how to generate fuzzy rules and provides some illustrative examples. Section 3 outlines an overall architecture of FRDT and the algorithm of FRDT. In Section 4, we evaluate the proposed FRDT by running it on both synthetic data set and benchmark data sets, and offer a thorough parametric analysis of the tree. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Generation of fuzzy rules</head><p>The crucial design problem of FRDT is how to extract the major geometric structures with linguistic interpretation. In this paper, a geometric structure with linguistic interpretation is corresponding to the antecedent clauses of a fuzzy "if-then" rule, the antecedent part of fuzzy rule consists of fuzzy number, the inferred class is the consequence of the fuzzy rule. First of all, let us look at a way of forming fuzzy numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formation of fuzzy numbers</head><p>The data can be represented in the form X ¼ ½x ij being an n Â m matrix. Each column of X corresponds to a given feature (variable), and each row corresponds to a pattern (data point). Let f j ðj ¼ 1; 2; …; mÞ denote the j-th column (feature) of X, x i ¼ ½x i1 ; x i2 ; …; x im ði ¼ 1; 2; …; nÞ denote the i-th pattern, x ij be the jth feature value of x i , and y i A f1; 2; …; cg be a class label of x i , c be the number of classes, X contain c classes X l ðl ¼ 1; 2; …; cÞ.</p><p>Quite commonly, asymmetric trapezoidal and triangular forms of fuzzy numbers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> are studied. The membership functions of these fuzzy numbers are shown in Fig. <ref type="figure">2</ref>. Here a onedimensional feature space is described in terms of four fuzzy sets, "big", "medium big", "medium small", and "small". We assume that the number of fuzzy numbers defined for the j-th feature f j is equal to the number of classes c. Let f k;j A F denote the k-th ðk ¼ 1; 2; …; cÞ fuzzy number formed for the j-th feature f j . Each triangular fuzzy number f k;j is characterized by three parameters ms k À 1;j , ms k;j and ms k þ 1;j as shown in Fig. <ref type="figure">2</ref>. Each trapezoidal fuzzy number f k;j is characterized by two parameters. If k ¼1, trapezoidal fuzzy number f 1;j has two parameters ms 1;j and ms 2;j . If k ¼c, trapezoidal fuzzy number f c;j has two parameters ms c À 1;j and ms c;j .</p><p>The total number of the parameters of the fuzzy numbers f k;j ðk ¼ 1; 2; …; cÞ defined for the j-th feature f j is equal to c which is the least number required to describe c classes. In this study, we do not consider how to optimize these parameters, the values of the parameters ms k;j (k ¼ 1; 2; …; c) are taken as the mean values of all patterns falling within the k-th class X k when considering feature f j . Let fms 0 k;j ; k ¼ 1; 2; …; cg assume the values given by formula (1) and fms k;j ; k ¼ 1; 2; …; cg be the same values after sorting them in an ascending order.</p><formula xml:id="formula_0">ms 0 k;j ¼ ∑ x i A X k x ij jX k j :<label>ð1Þ</label></formula><p>As shown in Fig. <ref type="figure">2</ref>, the boundaries formed of triangular fuzzy number f k;j is ðms k À 1;j þ ms k;j Þ=2 and ðms k;j þ ms k þ 1;j Þ=2, the boundaries of trapezoidal fuzzy number f 1;j is ðms 1;j þ ms 2;j Þ=2, and the boundaries of trapezoidal fuzzy number f c;j is ðms c À 1;j þ ms c;j Þ=2. This fuzzy number f k;j defined in this way can explicitly describe kth class X k for feature f j , and in this manner the linguistic interpretation of fuzzy numbers becomes evident.</p><p>Example 1. We consider a synthetic data set as an illustrative example. The synthetic data set consists 1000 samples, 500 points are coming from class 1, and 500 points are belong to class 2. The data is shown in Fig. <ref type="figure">3</ref>. Let X ¼ ½x ij be a 1000 Â 2 matrix representing data, thus x i ði ¼ 1; 2; …; 1000Þ denotes the i-th sample, f j ðj ¼ 1; 2Þ denotes the j-th feature of X. Due to c¼ 2, there are two fuzzy numbers for each feature as follows: f 1;1 -"the value of feature 1 is small", f 2;1 -"the value of feature 1 is big", f 1;2 -"the value of feature 2 is small", and f 2;2 -"the value of feature 2 is big". Their membership degrees computed according to formula (1) are shown in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Formulation of fuzzy rule</head><p>Fuzzy rule is used to describe classes of patterns <ref type="bibr" target="#b31">[32]</ref>. For instance, we have R: IF the value of x i 1 is small and the value of x i 2 is big, THEN x i belongs to class 1.</p><p>Considering fuzzy numbers introduced in Example 1, the fuzzy "if-then" rule can be rewritten as follows:</p><p>R: IF x i is f 1;1 and f 2;2 , THEN x i belongs to class 1.</p><p>For each set of fuzzy numbers A D F, ∏ f A A f represents a conjunction of the fuzzy numbers in A. For instance, let A ¼ ff 1;1 ; f 2;2 g D F, a new fuzzy set "f 1;1 and f 2;2 " with the linguistic interpretation "the value of feature 1 is small and the value of feature 2 is big" can be formally represented as ∏ f A A f ¼ f 1;1 f 2;2 . Thus, the antecedent part of fuzzy rule R reads as follows:</p><p>R:</p><formula xml:id="formula_1">IF x i is f 1;1 f 2;2 , THEN x i belongs to class 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generation of fuzzy rules</head><p>Fuzzy rules play a pivotal role in the growth of the FRDT. In order to generate a simple and effective decision tree, we introduce a simple fuzzy rule extraction method guided by Fuzzy Confidence.</p><p>A classical association between properties A and B can be defined by the form A ) B indicating that an element satisfying property A also satisfies B. Two indices used to measure the validity of such association rule, namely Support and Confidence were proposed by <ref type="bibr" target="#b32">[33]</ref>, that is, Support, SuppðA ) BÞ ¼ jA \ Bj=jXj, and Confidence, Conf ðA ) BÞ ¼ jA \ Bj=jAj. For classification propose, these two indices can be generalized to the corresponding fuzzy set based consequent part, namely the Fuzzy Support and Fuzzy Confidence, defined as follows <ref type="bibr" target="#b33">[34]</ref>: </p><formula xml:id="formula_2">FSuppðA ) lÞ ¼ ∑ x A X l AðxÞ jXj<label>ð2Þ</label></formula><formula xml:id="formula_3">FConf ðA ) lÞ ¼ ∑ x A X l AðxÞ ∑ x A X AðxÞ<label>ð3Þ</label></formula><formula xml:id="formula_4">AðxÞ ¼ ∑ f A A f ðxÞ=jAj ð<label>4Þ</label></formula><p>where A is a set of fuzzy numbers, f(x) is the membership function x belonging to f, l is a class label (1 r l r c), the symbol j Á j denotes the cardinality of a set that means jAj is the number of fuzzy numbers of A. In fact, A ) l can also be represented as a fuzzy "ifthen" rule in the form: IF x is A, THEN x's label is l. Fuzzy Support and Fuzzy Confidence can be used for evaluating the fuzzy if-then rule. Wang and Liu et al. developed an axiomatic fuzzy set association rules extraction method based on Fuzzy Support and Fuzzy Confidence <ref type="bibr" target="#b33">[34]</ref>; it has been shown that fuzzy association rules can efficiently represents the geometry of regions of individual classes.</p><p>For building the tree, we propose an association rules extraction algorithm (AREA) (see Algorithm 1) to extract only a single rule for each class to form a major oblique geometric structure. Fuzzy Confidence is employed as a criterion to select several promising features characterizing by high values of Fuzzy Confidence. First of all, for classes X l , l is the class label of X l , we select w (1 r w r minðMaxL; cnmÞ) fuzzy numbers F w D F with high values of Fuzzy Confidence, where MaxL is the maximum length of fuzzy rule. That is, 8 f A F w , 8 g A F\F w , FConf ðf ) lÞ 4 FConf ðg ) lÞ. F w is a set of fuzzy numbers, and joint the fuzzy numbers in F w as the candidate of the antecedent part of fuzzy "if-then" rule of classes X l , the inferred class l is the consequence of the fuzzy rule. Where, MaxL is the confined maximum length of fuzzy rule (the number of fuzzy numbers of the antecedent part of fuzzy rule). Then, the parameter w needs to be optimized to determine the best antecedent part of fuzzy rule. Apparently, as the increase of w, the Fuzzy Confidence of F w as the antecedent part of a fuzzy rule will be decreased. When the value of the parameter w increases, the number of fuzzy numbers of the antecedent part of the fuzzy rule increases as well, and the resulting fuzzy rule starts covering less samples. When the value of the parameter w decreases, the number of fuzzy numbers of the antecedent part of the fuzzy rule decreases as well, and the resulting fuzzy rule starts covering more samples. We determine an optimal value of w as follows:</p><formula xml:id="formula_5">w ¼ minfvjFConf ðF v ) lÞÀFConf ðF v þ 1 ) lÞ 4β; 1 r v r minðMaxL; cnmÞg<label>ð5Þ</label></formula><p>where β is a certain threshold (say, 0.02) is used to set up a sound balance between the length of the fuzzy rule and its Fuzzy Confidence level.</p><p>The geometric structure of regions of individual may be consisted by several oblique geometric structures (a given class is made up of several oblique geometry regions). The proposed AREA algorithm is employed to mine a major oblique geometric structure of the class by using a single fuzzy if-then rule. The form of the fuzzy set (membership function) is used to express the major oblique geometry structure of a classifier. In general, the obtained fuzzy rule involves several fuzzy numbers thus resulting in the lower length of the fuzzy rule. Input: Xa set of training data, consists of c classes X l ; ðl ¼ 1; …; cÞ, l is the class label of X l , F ¼ ff k;j g; k ¼ 1; …; c; j ¼ 1; …; m denote all of the fuzzy numbers of features on X, MaxL is the maximum length of fuzzy rule, β is a penalty coefficient used to control the length of the fuzzy rule. Output: R l is the obtained fuzzy rule of the l-th class 1. For each class X l 2.</p><p>w ¼ minðMaxL; cnmÞ; 3.</p><p>For v ¼ 1; …; minðMaxL; cnmÞ 4.</p><p>Compute</p><formula xml:id="formula_6">F v and F v þ 1 ; 5. if FConf ðF v ) lÞÀFConf ðF v þ 1 ) lÞ 4 β 6.</formula><p>w¼ v; 7.</p><p>break; 8. end 9. end 10.</p><p>R l ( The antecedent part of R l is F w and the consequent part is l; 11. end 3. Overall architecture of the FRDT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The algorithm of FRDT</head><p>The architecture of FRDT is developed using fuzzy "if-then" rules that are treated as generic building blocks of the tree. We start with the overall training data positioned at the root node of the tree structure. The way of building the tree implies a specific way in which we allocate elements of data X to each node. A node comes with a subset of X, namely X 1;1 ; X 1;2 ; …; X 1;c 1 (c 1 ¼ c is the number of classes of X), which is determined by the first layer of fuzzy rules R 1;j ðj ¼ 1; …; c 1 Þ respectively. X is classified into c 1 classes so that the data points that are with same linguistic are put together. R 1;j is used to obtain the major region for class X j . We include an additional node (which is the only one non-leaf node of this layer) composed of these samples that cannot be classified by the fuzzy rules present at this layer, that is X 1;δ . In general, X i;δ is a subset of X comprising patterns which cannot be classified by the fuzzy rules of the i-th layer. This subset is then treated as a candidate set for further refinements.</p><p>In the sequel, the node X 1;δ is refined by splitting it into c 2 classes as visualized in Fig. <ref type="figure">5</ref> (c 2 is the number of classes of X 1;δ ). The fuzzy rules of the second layer of the tree R 2;j ðj ¼ 1; …; c 2 Þ are obtained by AREA algorithm on X 1;δ . R 2;j is used to obtain the major geometry for j-th class of X 1;δ . This process is repeated until one of the following three conditions has been satisfied: (1) X i;δ ¼ |, (2) The data points in X i;δ belong to the same class, (3) X i;δ ¼ X i À 1;δ that means the fuzzy rules of this layer of tree cannot classify any data in X i À 1;δ and are discarded. The pseudocode of FRDT is shown in Algorithm 2.</p><p>Algorithm 2. Fuzzy rule based decision tree algorithm FRDT(X, F, MaxL, β, δ).</p><p>Input: X is a set of training data composed of c classes X 1 ; X 2 ; …; X c , 0o δ o 1, i represents the i-th layer of the tree.</p><p>Output: R i;l , X i;δ 1. X 0;δ ¼ X, i¼1; 2.</p><p>While X i À 1;δ a | 3.</p><p>c i ( the number of classes in X i À 1;δ ; 4.</p><p>IF c i ¼ 1 break; end 5.</p><p>R i;l ðl ¼ 1; …; c i Þ ( AREAðX i À 1;δ ; F; MaxL; βÞ;</p><formula xml:id="formula_7">6. Compute X i;δ ðR i;1 ; R i;2 ; …; R i;c i Þ by Definition 1; 7. IF X i;δ ¼ X i À 1;δ or X i;δ ¼ ∅ break; end 8. i ¼ i þ 1; 9.</formula><p>end</p><p>The process of building the tree is guided by choosing a certain threshold δ. The smaller the value of δ, the lower purity of the leaf node is, and subsequently the smaller the size of the tree becomes. Commonly, the FRDT does not require pruning. For the completeness of the construct, each node is characterized by a fuzzy "if-then" rule, and a list of patterns which membership degree belonging to the rule resides at the node are equal to or higher than δ. Moreover, each pattern shown on this list comes with a degree of belongingness (membership) to that node.    Remark 1. The intermediate rules are generated using the "impure" node at the upper layer of the tree, and these intermediate rules of this layer determine the impure node of the current layer of the tree. The tree corresponds to a nested format of these rules. The final rules obtained by FRDT are shown in Fig. <ref type="figure">6</ref>. From Fig. <ref type="figure">6</ref>, we can see that the intermediate rules are a part of the final rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Design aspect of FRDT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">The number of leaf nodes at each layer c i</head><p>The number of leaf nodes of this layer c i (that is the number of fuzzy rules of this layer) is an essential design parameter. In order to generate fuzzy rules that are non-redundant as much as possible making the size of the tree is as small as possible, only one rule is adopted as a fuzzy descriptor of a class, meaning that the number of fuzzy rules at the i-th layer is equal to the number of classes present X i À 1;δ . On the other hand, this helps extract the most essential geometric structure which resides within each leaf node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Generation of non-leaf nodes X i;δ</head><p>Let R i;1 ; R i;2 ; …; R i;c i be the fuzzy rules of the i-th layer and A i;j be the antecedent part of fuzzy rule R i;j . Let X i;δ be a non-leaf node (subset of X) consists of these samples that cannot be classified by the fuzzy rules of the i-th layer. More formally, we describe this set of patterns as follows: Definition 1. X i;δ ðR i;1 ; R i;2 ; …; R i;c i Þ DX i À 1;δ is called a δ-cut uncertain patterns set of X i À 1;δ with regard to the following condition is satisfied. If 8 x A X i;δ , 8 j A 1; 2; …; c i , A i;j ðxÞ o δ.</p><p>Where X 0;δ ¼ X, A i;j ðxÞ is the compatibility grade of fuzzy rule R i;j with x, that is, the membership degree of x belonging to the fuzzy rule R j , and A i;j ðxÞ is computed with the use of formula (4). f(x) represents the membership degree of x belonging to the fuzzy number f. In fact, X i;δ is a subset of X of the patterns which cannot be clearly distinguished by rules R i;1 ; R i;2 ; …; R i;c i .  Fig. <ref type="figure" target="#fig_2">14</ref>. Membership functions of fuzzy numbers formed for petal length, sepal width, petal length, and petal width. β¼0.1, the obtained FRDT is displayed in Fig. <ref type="figure">7</ref>. The obtained fuzzy rules are presented in Fig. <ref type="figure">8</ref>.</p><p>Some new data to be classified are illustrated in Figs. 9 and 10. The fuzzy boundaries are shown in Fig. <ref type="figure" target="#fig_6">11</ref> between the fuzzy rules of the first layer of the tree. The final boundaries of the obtained tree and the main geometric structures are shown in Fig. <ref type="figure" target="#fig_7">12</ref>. It becomes apparent that the new architecture of a fuzzy decision tree supports a versatile topology of the classification boundaries (Fig. <ref type="figure" target="#fig_7">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Analysis of time complexity</head><p>We first analyze the time complexity of generating fuzzy rules of the AREA. The maximum length of fuzzy rules generated by AREA for each class is MaxL. Thus, the time complexity of AREA is OðMaxLncnnÞ. FRDT algorithm is a non-iterative method, the number of layers is n in the worst situation that is each layer of the resulting tree for the training data set can classify only a single sample. Thus, the total time complexity of the proposed FRDT method is OðMaxLncnnnnÞ. However, in general, the number of the layer of the tree is no more than 10, thus the total time complexity of the FRDT method becomes OðMaxLncnnn10Þ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental studies</head><p>In this section, we present experimental results showing the effectiveness and interpretability of our decision tree learning algorithm. In these studies, according to the principle of Occam's Razor, the length of fuzzy rule obtained by AREA method should be lower. Thus, the value of parameter β is set to 0.02 to keep the length of fuzzy rule lower, and the maximum length of fuzzy rule MaxL is 5 that is because five features are sufficient to describe a region of a class. We test the performance of the algorithm on synthetic data and data sets coming from the UCI Repository of machine learning <ref type="bibr" target="#b34">[35]</ref>, see Table <ref type="table" target="#tab_0">1</ref>. The synthetic data set has 10 000 points, where 5000 points belong to class 1, and 5000 points belong to class 2, see Fig. <ref type="figure" target="#fig_2">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with other decision trees</head><p>In this experiment, a ten-fold cross validation is carried out. The performance of the tree is quantified by taking the mean coming with a ten times 10-fold cross validation. To obtain optimal FRDT, a two-step cross-validation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> is used to determine the best value of the parameter δ. After comparing the performance of all training models using different values of the parameter δ ðδ ¼ 0:1; 0:11; …; 0:6Þ, the best value of parameter obtained by cross-validation is used to build the FRDT tree.</p><p>In this experiment, we present a detailed comparative analysis when using "traditional" decision tree classifiers: C4.5 decision tree (C4.5) <ref type="bibr" target="#b5">[6]</ref>, Best-first decision tree (BFT) <ref type="bibr" target="#b37">[38]</ref>, a multi-class alternating decision tree (LAD) <ref type="bibr" target="#b38">[39]</ref>, Simple Cart (SC) <ref type="bibr" target="#b21">[22]</ref>, and NBTree decision tree (NB) <ref type="bibr" target="#b39">[40]</ref>. The details can be found in the software Weka <ref type="bibr" target="#b40">[41]</ref> (http://www.cs.waikato.ac.nz/ml/weka/). For all the algorithms we used their Weka implementation, and the values of the parameters for all the algorithms were set to their defaults being available in the toolkit. For example, the minimal number of instances per leaf is set to 2 and the pruning lever is set to 25% for C4.5.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The number of misclassified testing samples and standard deviation for FRDT and other decision tree classifiers, the best scores are indicated in boldface.  We illustrate the performance of the FRDT classifier for the Iris data. There is a 150 Â 4 data matrix X ¼ ðx ij Þ 150Â4 with data evenly distributed across three classes: iris-setosa, iris-versicolor, and irisvirginica. There are four features: sepal length and width, and petal length and width (all given in centimeters). Let M ¼ ff 1 ; f 2 ; f 3 ; f 4 g be the set of features, x i ¼ ðx i1 ; x i2 ; x i3 ; x i4 Þ ði ¼ 1; 2; …; 150Þ be the i-th sample. According to formula (1), we can obtain fuzzy numbers set F ¼ ff k;j j 1 r k r 3; 1 r j r 4g with their parameters determined by formula <ref type="bibr" target="#b0">(1)</ref>, where, the semantics of the fuzzy numbers in F is f 1;1 : "short sepal length", f 2;1 : "mid sepal length", f 3;1 : "long sepal length", f 1;2 : "short sepal width", f 2;2 : "mid sepal width", f 3;2 : "long sepal width"; f 1;3 : "short petal length", f 2;3 : "mid petal length", f 3;3 : "long petal length", f 1;4 : "short petal width", f 2;4 : "mid petal width", f 3;4 : "long petal width". Their membership functions are shown in Fig. <ref type="figure" target="#fig_2">14</ref>. Fig. <ref type="figure" target="#fig_9">15</ref> shows the obtained FRDT decision tree produced for the training data with δ¼0.6. Referring to Fig. <ref type="figure" target="#fig_9">15</ref>, the obtained fuzzy rules are shown in Fig. <ref type="figure" target="#fig_10">16</ref>.</p><p>The tree formed by the C4.5 decision tree is shown in Fig. <ref type="figure" target="#fig_11">17</ref>. Comparing the C4.5 and FRDT trees, one can clearly conclude that the structure of the tree is apparently different, and the size of the tree of FRDT is smaller than the one being formed by the C4.5.</p><p>The misclassified testing samples and standard deviation for the experiments on different data sets are summarized in Table <ref type="table">2</ref>. The best scores are shown in boldface. The FRDT decision tree produces high accuracy and outperforms the other methods.</p><p>The results presented in Table <ref type="table">2</ref> offer some insight into the performance of the algorithms. However, those results do not provide enough support for drawing a strong conclusion in favor or against any of the studied methods. To arrive at strong evidence, we resort ourselves to statistical testing of the result. The Holm test <ref type="bibr" target="#b41">[42]</ref> is based on the relative performance of classifiers in terms of their ranks: for each data set, the methods to be compared are sorted according to their performance, i.e., each method is assigned a rank (in case of ties, average ranks are assigned <ref type="bibr" target="#b41">[42]</ref>). The test statistics for comparing the two classifiers is expressed as</p><formula xml:id="formula_8">z ¼ Rank j À Rank k SE<label>ð6Þ</label></formula><p>where, k is the number of classifiers, N is the number of data set,</p><formula xml:id="formula_9">Rank i ¼ ð∑ N i ¼ 1 r j i Þ=N, SE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi kðk þ 1Þ=ð6nNÞ p</formula><p>, r i j is the rank of the classifier j on the i-th data set. The z value is used to find the corresponding probability (p) from the table of normal distribution, which is then compared with an appropriate α. We denote the ordered p values by p 1 , p 2 ,…, so that p 1 r p 2 r⋯ r p k À 1 . The Holm's step-down procedure compares each p i with α=ðk À iÞ, but differ in the order of the tests, and it starts with the most significant p value. If p 1 is below α=ðk À 1Þ, the corresponding hypothesis (the two classifiers have the same performance) is rejected and we are allowed to compare p 2 with α=ðk À 2Þ. If the second hypothesis is rejected, the test proceeds with the third one, etc. As soon as a certain null hypothesis cannot be rejected, all the remaining hypotheses are retained as well.</p><p>In this studies, Rank LAD ¼ 5:17, Rank BFT ¼ 4:17, Rank SC ¼ 3:75, Rank C4:5 ¼ 3:5, Rank NBT ¼ 3:0, Rank FRDT ¼ 1:4, and with α ¼ 0:05, k ¼6 and N ¼12, the standard error is SE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ð6 Â 7Þ=ð6 Â 12Þ p ¼ 0:76. The Holm procedure rejects the first, the second, the third, the fourth and the fifth hypothesis since the corresponding p values are smaller than the adjusted α's (see Table <ref type="table" target="#tab_1">3</ref>). This shows that FRDT performs significantly better than NBT, SC, BFT, LAD and C4.5 at the significance level α ¼ 0:05.  Table <ref type="table" target="#tab_2">4</ref> shows the comparison of the size of the tree expressed in terms of the number of leaves and the number of nodes, which are constructed on the training data set. The size of the trees obtained from FRDT is significantly lower than those being produced by the NBT, SC, BFT, LAD and C4.5. The main reason behind this comes from the fact that the introduced method considers several features at each node, whereas in the "traditional" decision trees one considers only a single feature at each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parametric analysis</head><p>In order to study the relationship between δ and the accuracy of FRDT, we run 10-fold cross-validation ten times with δ ¼ 0:1; 0:11; …; 0:6. The estimated performance is taken as the mean computed over 10 results ten times on data sets, and the accuracies for testing data are shown in Figs. <ref type="figure" target="#fig_2">18</ref> and<ref type="figure" target="#fig_2">19</ref>. One can see that the accuracy is very slightly affected by the values of δ. The parameter δ can control the size of the tree of FRDT, if δ is too low, the tree will end up having a single layer. In general, the higher the value of δ, the larger the size of the tree is (see Figs. <ref type="figure">20</ref> and<ref type="figure" target="#fig_2">21</ref>). In fact, the parameter δ can affect the number of sample in the added impure nodes of the proposed FRDT tree. If δ is bigger, the margin of fuzzy rules of different classes is also bigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this study, we have proposed a new architecture of a fuzzy decision tree using fuzzy rule called FRDT and evaluated the performance of the proposed algorithm using well-known benchmark data sets. By comparing accuracy and the size of the trees with other "conventional" decision trees, the results of statistical tests have shown that the proposed FRDT exhibit the best performance in terms of accuracy and the size of the resulting tree. The accuracy is slightly affected by the value of δ. The parameter δ can effectively control the size of the proposed tree and also can affect the margin of fuzzy rules of different classes. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overall structure of fuzzy rule-based classification tree.</figDesc><graphic coords="2,61.28,58.61,213.68,214.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig. 2. Triangular and trapezoidal forms of fuzzy number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 .</head><label>1</label><figDesc>Association rules extraction algorithm AREA(X, F, MaxL, β).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The structure of fuzzy rule classification tree.</figDesc><graphic coords="4,330.08,330.59,213.91,188.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 . 8 . 2 Fig. 9 .</head><label>7829</label><figDesc>Fig. 7. The tree formed for the synthetic data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The samples classified by the fuzzy rules located at the second layer of the tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The fuzzy boundaries of fuzzy rules of the first layer of the tree.</figDesc><graphic coords="5,51.92,363.83,212.69,107.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The boundaries of fuzzy rules of the tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Example 2 .Fig. 13 .</head><label>213</label><figDesc>Fig. 13. The synthetic data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. FRDT for iris data with δ ¼0.6.</figDesc><graphic coords="7,74.72,58.61,167.14,74.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The fuzzy rule obtained by FRDT on the iris data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The tree obtained by C4.5 decision tree for iris data.</figDesc><graphic coords="7,38.21,473.12,240.24,92.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>.</head><label></label><figDesc>Fig. 17. The tree obtained by C4.5 decision tree for iris data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .Fig. 19 .</head><label>1819</label><figDesc>Fig.<ref type="bibr" target="#b17">18</ref>. Accuracy of FRDT for testing data for different δ for iris, wine, wdbc, credit, heart, and haberman data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 20 .Fig. 21 .</head><label>2021</label><figDesc>Fig.<ref type="bibr" target="#b19">20</ref>. The size of FRDT decision tree versus δ for iris, wine, wdbc, credit, heart and haberman data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Statistics of data sets used in the experiments along with the number of features (m), number of classes (c), and number of samples (n).</figDesc><table><row><cell>No.</cell><cell cols="2">Data set</cell><cell></cell><cell>m</cell><cell>c</cell><cell>n</cell><cell></cell></row><row><cell>1</cell><cell cols="3">synthetic</cell><cell>2</cell><cell>2</cell><cell>10,000</cell><cell></cell></row><row><cell>2</cell><cell cols="2">iris</cell><cell></cell><cell>4</cell><cell>3</cell><cell>150</cell><cell></cell></row><row><cell>3</cell><cell cols="2">wine</cell><cell></cell><cell>13</cell><cell>3</cell><cell>178</cell><cell></cell></row><row><cell>4</cell><cell cols="2">wdbc</cell><cell></cell><cell>30</cell><cell>2</cell><cell>569</cell><cell></cell></row><row><cell>5</cell><cell cols="2">credit</cell><cell></cell><cell>14</cell><cell>2</cell><cell>690</cell><cell></cell></row><row><cell>6</cell><cell cols="2">heart</cell><cell></cell><cell>13</cell><cell>2</cell><cell>270</cell><cell></cell></row><row><cell>7</cell><cell cols="3">haberman</cell><cell>3</cell><cell>2</cell><cell>306</cell><cell></cell></row><row><cell>8</cell><cell cols="3">newthyroid</cell><cell>5</cell><cell>3</cell><cell>215</cell><cell></cell></row><row><cell>9</cell><cell cols="2">wobc</cell><cell></cell><cell>9</cell><cell>2</cell><cell>699</cell><cell></cell></row><row><cell>10</cell><cell cols="3">column_3C</cell><cell>6</cell><cell>3</cell><cell>310</cell><cell></cell></row><row><cell>11</cell><cell cols="3">waveform1</cell><cell>21</cell><cell>3</cell><cell>5000</cell><cell></cell></row><row><cell>12</cell><cell cols="3">waveform2</cell><cell>40</cell><cell>3</cell><cell>5000</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f 1,1</cell><cell></cell><cell>f 1,2</cell></row><row><cell></cell><cell>membership degree</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>f 2,1 f 3,1</cell><cell>membership degree</cell><cell>0.5</cell><cell>f 2,2 f 3,2</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>5.006</cell><cell>5.936</cell><cell>6.588</cell><cell></cell><cell></cell><cell>0</cell><cell>2.77 2.974</cell><cell>3.428</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sepal length</cell><cell></cell><cell></cell><cell>sepal width</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f 1,3</cell><cell></cell><cell>f 1,4</cell></row><row><cell></cell><cell>membership degree</cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell>f 2,3 f 3,3</cell><cell>membership degree</cell><cell>0.5</cell><cell>f 2,4 f 3,4</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>1.462</cell><cell></cell><cell>4.26</cell><cell>5.552</cell><cell></cell><cell>0</cell><cell>0.246</cell><cell>1.326</cell><cell>2.026</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">petal length</cell><cell></cell><cell></cell><cell>petal width</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>The Holm test.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>i</cell><cell>Classifier</cell><cell>z</cell><cell>p</cell><cell>α=ðk À iÞ</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>LAD</cell><cell>(5.17 À 1.4)/0.76¼ 4.9</cell><cell>0.0</cell><cell>0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>BFT</cell><cell>(4.17</cell><cell></cell></row><row><cell>Data set</cell><cell>BFT</cell><cell>C4.5</cell><cell>LAD</cell><cell>SC</cell><cell>NBT</cell><cell></cell><cell>FRCT</cell></row><row><cell>synthetic</cell><cell>67.7 7 4.7</cell><cell>607 3.9</cell><cell>838.7 7 14.4</cell><cell>60.8 7 3.8</cell><cell>105.5 7 17.2</cell><cell></cell><cell>9.2 7 1.4</cell></row><row><cell>iris</cell><cell>8.4 7 1.3</cell><cell>7.9 7 1.2</cell><cell>8.3 7 1.3</cell><cell>8.7 7 1.5</cell><cell>9.8 7 1.9</cell><cell></cell><cell>6.2 7 10.6</cell></row><row><cell>wine</cell><cell>18.6 7 2.2</cell><cell>12.17 2.4</cell><cell>237 3.2</cell><cell>18.7 7 3.0</cell><cell>77 2.4</cell><cell></cell><cell>10.8 7 1.5</cell></row><row><cell>wdbc</cell><cell>39.6 7 3.7</cell><cell>35.57 2.8</cell><cell>53.2 7 6.9</cell><cell>38.9 7 3.5</cell><cell>34.4 7 4.6</cell><cell></cell><cell>28.2 7 1.7</cell></row><row><cell>credit</cell><cell>106.27 3.6</cell><cell>1117 4.9</cell><cell>141.6 7 12.8</cell><cell>105.7 7 4.5</cell><cell>109.2 7 4.1</cell><cell></cell><cell>102.3 7 4.8</cell></row><row><cell>heart</cell><cell>61.5 7 4.6</cell><cell>597 6.1</cell><cell>74.6 7 5.2</cell><cell>59.2 7 4.4</cell><cell>51.5 7 3.3</cell><cell></cell><cell>44.6 7 3.0</cell></row><row><cell>haberman</cell><cell>84.4 7 4.5</cell><cell>85.2 7 3.4</cell><cell>90.2 7 7.4</cell><cell>81.9 7 3.7</cell><cell>87 7 4</cell><cell></cell><cell>81.2 7 2.3</cell></row><row><cell>newthyroid</cell><cell>15.2 7 1.7</cell><cell>15.9 7 2.1</cell><cell>23.9 7 2.2</cell><cell>17.5 7 2.0</cell><cell>16.4 7 3.0</cell><cell></cell><cell>14.4 7 3.1</cell></row><row><cell>wobc</cell><cell>38.8 7 3.8</cell><cell>34.9 7 3.1</cell><cell>42.7 7 5.3</cell><cell>36.8 7 2.6</cell><cell>25.4 7 3.0</cell><cell></cell><cell>33.07 2.6</cell></row><row><cell>co1umn_3C</cell><cell>61.8 7 4.7</cell><cell>57.2 7 3.7</cell><cell>70.37 5.3</cell><cell>59.3 7 4.0</cell><cell>59.8 7 4.7</cell><cell></cell><cell>57.1 7 2.4</cell></row><row><cell>waveform1</cell><cell>11577 11.9</cell><cell>1169 7 25.2</cell><cell>1056 7 24.9</cell><cell>11267 12.8</cell><cell>928 7 27.4</cell><cell></cell><cell>1048 7 15.7</cell></row><row><cell>waveform2</cell><cell>11867 24.7</cell><cell>12377 23.1</cell><cell>1060 7 32.3</cell><cell>11677 15.2</cell><cell>1008 7 36.3</cell><cell></cell><cell>1069 7 20.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>The size of the trees and standard deviation for FRDT and other constructs of decision trees.</figDesc><table><row><cell cols="2">Data set</cell><cell>BFT</cell><cell></cell><cell>C4.5</cell><cell></cell><cell>LAD</cell><cell>SC</cell><cell>NBT</cell><cell>FRCT</cell></row><row><cell cols="2">synthetic</cell><cell cols="2">81.8 7 4.2</cell><cell cols="2">68.6 75.0</cell><cell>10.17 0.6</cell><cell>69.7 7 7.6</cell><cell>20.3 7 13.2</cell><cell>3.07 0.0</cell></row><row><cell>iris</cell><cell></cell><cell cols="2">9.3 7 2.1</cell><cell cols="2">4.6 70.59</cell><cell>7.0 7 0.3</cell><cell>7.4 7 2</cell><cell>4.4 7 2.9</cell><cell>4.07 0.0</cell></row><row><cell cols="2">wine</cell><cell cols="2">10.6 7 2.7</cell><cell cols="2">9.6 71.2</cell><cell>13 7 5.1</cell><cell>10.3 7 3.2</cell><cell>3.97 2.6</cell><cell>4.2 7 0.3</cell></row><row><cell cols="2">wdbc</cell><cell cols="2">16.5 7 4.6</cell><cell cols="2">22.4 73.9</cell><cell>16.2 7 2.6</cell><cell>12.6 7 4.4</cell><cell>18.2 7 3.6</cell><cell>7.8 7 0.4</cell></row><row><cell cols="2">credit</cell><cell cols="2">30.3 7 23.3</cell><cell cols="2">51.7 712.1</cell><cell>8.8 7 2.2</cell><cell>10.5 7 10.6</cell><cell>14.2 7 7.7</cell><cell>7.3 7 1.4</cell></row><row><cell cols="2">heart</cell><cell cols="2">28.8 7 11.9</cell><cell cols="2">34.6 75.7</cell><cell>15.6 7 1.2</cell><cell>15.4 7 8.1</cell><cell>9.6 7 3.7</cell><cell>6.97 1.1</cell></row><row><cell cols="2">haberman</cell><cell cols="2">20.2 7 22.5</cell><cell cols="2">21.8 711.4</cell><cell>8.4 7 1.9</cell><cell>3.8 7 3.8</cell><cell>9.9 7 6.8</cell><cell>4.5 7 0.5</cell></row><row><cell cols="2">newthyroid</cell><cell cols="2">13.6 7 2.8</cell><cell cols="2">14.9 72.1</cell><cell>11.8 7 1.9</cell><cell>11.8 7 3.6</cell><cell>7.6 7 3.2</cell><cell>9.8 7 1.0</cell></row><row><cell cols="2">wobc</cell><cell cols="2">31.0 7 12.4</cell><cell cols="2">23.5 75.5</cell><cell>13.6 7 3.2</cell><cell>15.9 7 7.1</cell><cell>5.77 5.6</cell><cell>10.0 7 1.8</cell></row><row><cell cols="2">column_3C</cell><cell cols="2">27.3 7 11.2</cell><cell cols="2">23.2 75.7</cell><cell>9.8 7 0.9</cell><cell>13.3 7 8.3</cell><cell>16.0 7 5.1</cell><cell>7.7 7 0.8</cell></row><row><cell cols="2">waveform1</cell><cell cols="2">343.4 7 89.2</cell><cell cols="2">541.5 728.5</cell><cell>30.4 7 1.8</cell><cell>125.7 7 45.3</cell><cell>47.5 7 36.9</cell><cell>23.07 2.0</cell></row><row><cell cols="2">waveform2</cell><cell cols="2">283.9 7 97.7</cell><cell cols="2">591.9 724.3</cell><cell>29.8 7 2.6</cell><cell>98.3 7 34.0</cell><cell>94.5 7 43.4</cell><cell>4.17 0.4</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0.75 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell>iris</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>wine</cell><cell></cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell>wdbc</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>credit</cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell>heart</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>haberman</cell><cell></cell></row><row><cell></cell><cell>0.1 0.55</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">parameter δ</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Natural Science Foundation of China under Grant 61175041, Boshidian Funds 20110041110017 and Canada Research Chair (CRC) Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>There are no conflicts of interest..</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An empirical comparison of decision trees and other classification methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BOAT-optimistic decision tree construction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data (SICMOD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric decision tree</title>
		<author>
			<persName><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., B: Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="192" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SLIQ: a fast scalable classifier for data mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Database Technology-EDBT&apos;96</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-valued attribute and multi-labeled data decision tree algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rank entropy-based decision trees for monotonic classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2052" to="2064" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new node splitting measure for decision tree construction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2725" to="2731" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy preserving decision tree learning using unrealized data sets</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Weber-Jahnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="353" to="364" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification trees for time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Douzal-Chouakria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amblard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1076" to="1091" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arabic script web page language identifications using decision tree neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Selamat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="133" to="144" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical linear support vector machine</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rodriguez-Lujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huerta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4414" to="4427" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fuzzy-ID3: a class of methods for automatic knowledge acquisition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second International Conference on Fuzzy Logic and Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The development of fuzzy decision trees in the framework of axiomatic fuzzy set logic</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="325" to="342" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum ambiguity-based sample selection in fuzzy decision tree induction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1491" to="1505" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GA guided cluster based fuzzy decision tree for reactive ion etching modeling: a data mining approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Semicond. Manuf</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extraction of fuzzy rules from fuzzy decision trees: an axiomatic fuzzy sets (AFS) approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving generalization of fuzzy if-then rules by maximizing fuzzy entropy, Fuzzy Syst</title>
		<author>
			<persName><forename type="first">X.-Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fuzzy binary decision tree for biometric based personal authentication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanmandlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpretable knowledge extraction from emergency call data based on fuzzy unsupervised decision tree</title>
		<author>
			<persName><forename type="first">F</forename><surname>Barrientos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Belmont, California</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wadsworth International Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New algorithms for learning and pruning oblique decision trees</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., C: Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="494" to="505" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inducing oblique decision trees with evolutionary algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cantu-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="54" to="68" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An evolutionary algorithm for oblique decision tree induction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kretowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Artificial Intelligence and Soft Computing, (ICAISC 2004)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="432" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A support vector machine approach to decision trees</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks Proceedings, 1998. IEEE World Congress on Computational Intelligence. The 1998 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2396" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Linear tree, Intell. Data Anal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using evolutionary algorithms to induce oblique decision trees</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cantú-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Genetic and Evolutionary Computation Conference</title>
		<meeting>eeding of Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1053" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A combined neural network and decision trees model for prognosis of breast cancer relapse</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jerez-Aragonés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gómez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñoz-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alba-Conejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="45" to="63" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The fuzzy regression approach to peak load estimation in power distribution systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nazarko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zalewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="809" to="814" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A fuzzy coherent rule mining algorithm</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3422" to="3428" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="111" to="127" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Database mining: a performance perspective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="914" to="925" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining axiomatic fuzzy set association rules for classification problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="202" to="210" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI machine learning repository</title>
		<meeting><address><addrLine>Irvine, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">213</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of California, School of Information and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chih-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chih-Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chih-Jen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science &amp; Information Engineering, National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A parsimony fuzzy rule-based classifier using axiomatic fuzzy set theory and support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="5180" to="5193" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Best-first decision tree learning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiclass alternating decision trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FR3: a fuzzy rule learner for inducing reliable classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hullermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>China</surname></persName>
		</author>
		<title level="m">Xianchang Wang received the M.S. degrees in mathematics from Dalian Maritime University</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Dalian University of Technology, and he is currently a lecturer of School of Science, Dalian Ocean University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include machine learning and data mining</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">He has proposed AFS theory and a coauthor of four books. His research interests include algebra rings, combinatorics, topology molecular lattices, AFS (axiomatic fuzzy sets) theory and its applications, knowledge discovery and representations, data mining, pattern recognition and hitch diagnoses, analysis and design of intelligent control systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Shenyang</surname></persName>
		</author>
		<author>
			<persName><surname>China</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China respectively, and the Ph.D. degree in control theory and control engineering from Northeastern University</title>
		<meeting><address><addrLine>Jilin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989. 2003. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Northeastern Normal University in 1986 and Jilin University ; Dalian University of Technology and Department of Applied Mathematics, Dalian Maritime University, a Guest Professor of the ARC Research Center of Excellence in PIMCE, Curtin University of Technology, Australia ; Electrical and Computer Engineering, University of Alberta ; Curtin University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a Professor in Research Center of Information and Control. Dr. Liu is a recipient of the 2002 Wufu-Zhenhua Best Teacher Award of the Ministry of Communications of People&apos;s Republic of China</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">He also holds an appointment of special professorship in the School of Computer Science, University of Nottingham, UK. In 2009 Dr. Pedrycz was elected as a foreign member of the Polish Academy of Sciences. In 2012 he was elected a Fellow of the Royal Society of Canada. Witold Pedrycz has been a member of numerous program committees of IEEE conferences in the area of fuzzy sets and neurocomputing</title>
	</analytic>
	<monogr>
		<title level="m">His main research directions involve Computational Intelligence, fuzzy modeling and Granular Computing, knowledge discovery and data mining, fuzzy control, pattern recognition, knowledge-based neural networks, relational computing, and Software Engineering</title>
		<meeting><address><addrLine>Edmonton, Canada; Warsaw, Poland; Jilin</addrLine></address></meeting>
		<imprint>
			<publisher>Witold Pedrycz is a Professor and Canada Research Chair</publisher>
			<date type="published" when="1989">2008. 1989</date>
		</imprint>
		<respStmt>
			<orgName>CRC-Computational Intelligence) in the Department of Electrical and Computer Engineering, University of Alberta ; Dalian Ocean University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include machine learning, applications of topology in knowledge discovery and data mining. concept analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
