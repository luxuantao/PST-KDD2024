<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosted key-frame selection and correlated pyramidal motion-feature representation for human action recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-10-12">12 October 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<addrLine>Mappin Street</addrLine>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<addrLine>Mappin Street</addrLine>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Rockett</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<addrLine>Mappin Street</addrLine>
									<postCode>S1 3JD</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosted key-frame selection and correlated pyramidal motion-feature representation for human action recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-10-12">12 October 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">04D89AD834BF1A612322FC319EEABDAA</idno>
					<idno type="DOI">10.1016/j.patcog.2012.10.004</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action recognition Pyramidal motion features Boosted key-frame selection Correlograms</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a novel method for human action recognition based on boosted key-frame selection and correlated pyramidal motion feature representations. Instead of using an unsupervised method to detect interest points, a Pyramidal Motion Feature (PMF), which combines optical flow with a biologically inspired feature, is extracted from each frame of a video sequence. The AdaBoost learning algorithm is then applied to select the most discriminative frames from a large feature pool. In this way, we obtain the top-ranked boosted frames of each video sequence as the key frames which carry the most representative motion information. Furthermore, we utilise the correlogram which focuses not only on probabilistic distributions within one frame but also on the temporal relationships of the action sequence. In the classification phase, a Support-Vector Machine (SVM) is adopted as the final classifier for human action recognition. To demonstrate generalizability, our method has been systematically tested on a variety of datasets and shown to be more effective and accurate for action recognition compared to the previous work. We obtain overall accuracies of: 95.5%, 93.7%, and 36.5% with our proposed method on the KTH, the multiview IXMAS and the challenging HMDB51 datasets, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition has attracted a great deal of attention due to its potential value and wide usage in a variety of areas, such as video search and retrieval, intelligent surveillance systems and human-computer interaction.</p><p>Typically, a scheme for human action recognition is based on either global or local feature extraction. Local feature methods usually follow the path of using an unsupervised technique, such as the method of Dolla Â´r et al. <ref type="bibr" target="#b0">[1]</ref>, to directly extract interest points from raw data followed by a 'bag of features' scheme to construct a code-book onto which raw features are mapped; histograms are normally used to represent each action. Finally, these representations are fed to a classifier. Methods based on local features, however, cannot usually achieve good results for human action recognition due to a dependence on the feature detector adopted. A good detector can extract more meaningful and significant features but identifying the best feature detector a priori is not generally possible. Also, it is obvious that just using a histogram representation of actions in a 'bag of features' scheme may lead to the loss of some discriminatory information in both the spatial and temporal dimensions, and influence the final recognition accuracies.</p><p>On the other hand, global feature methods consider the action as a volume in space/time and do not need feature detectionsuch methods are considered to be more holistic and informative for action recognition. Global features, however, can be sensitive to the background in the action sequence so that clutter, and indeed partial occlusion, may have a significant influence on action recognition accuracy.</p><p>In this paper, a new method based on key-frame selection is developed for human action recognition which identifies the frames carrying the most discriminative information. We extract a Pyramidal Motion Feature (PMF) for each frame of an action sequence but since not all the motion features are necessarily useful for action recognition, we use the AdaBoost learning algorithm <ref type="bibr" target="#b1">[2]</ref> to select key frames for each action sequence. Each of these boosted key frames represents a typical motion pattern at one instant of the action sequence and the probabilistic distribution and temporal relationships of these frames are represented by a correlogram. Finally, a Support-Vector Machine (SVM) <ref type="bibr" target="#b2">[3]</ref> is utilised as a classifier for recognizing actions. From a series of systematic experiments, we demonstrate that our method achieves results superior to the previously published work.</p><p>The main contributions of this paper are summarised as follows: Firstly, a Pyramidal Motion Feature (PMF) is proposed to represent action sequences. We have applied an optical-flow algorithm combined with biologically inspired features to produce a new feature descriptor which is informative for action recognition.</p><p>Secondly, we demonstrate that the efficacy of the AdaBoost learning algorithm for selecting key frames from each action video sequence. Unlike the general 'bag of features' approach, a correlogram is used to represent the co-occurrence probability of an action within the boosted key frames.</p><p>We compare the results of our method with the other previously published techniques. In terms of the final recognition accuracies, our methods are shown to be more accurate for human action recognition.</p><p>The remainder of this paper is organised as follows: In Section 2 we survey some related work. In Section 3 we describe details of our method; experimental results are given in Section 4. We draw conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Much previous work on human action recognition has used frame representations. The silhouette representation, which records the pose of an action at a particular instant, was combined with a correlogram by Shao et al. <ref type="bibr" target="#b3">[4]</ref> to achieve action classification. Wang et al. <ref type="bibr" target="#b4">[5]</ref> have also applied an extended Histogram of Oriented Gradients (HOG) algorithm to represent frames for action recognition. Other frame representation approaches have been presented in <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Not all the frames in a sequence, however, are relevant to the action in question since some capture less meaningful information, or even describe a pose common to all action sequences, which may have a big influence on final classification performance.</p><p>Due to the above-mentioned drawbacks, another body of work has sought to select the most representative frames as key for action recognition instead of using the whole action sequence. Zhao and Elgammal <ref type="bibr" target="#b8">[9]</ref> have proposed a method for human action recognition based on selecting key frames from a video sequence and representing them with the distribution of local motion features and their spatio-temporal arrangements. In their method, a small set of the most discriminative frames are selected by comparing their discriminative power for each independent action. A scheme of frame-by-frame voting is used for the action classification. Cooper and Foote <ref type="bibr" target="#b9">[10]</ref> have presented a key-frame selection technique based on capturing similarity to the represented segment and preserving the differences with other segment key frames. In addition, Zhuang et al. <ref type="bibr" target="#b10">[11]</ref> have used unsupervised clustering for key-frame selection. Cao et al. <ref type="bibr" target="#b11">[12]</ref> have developed the novel approach of key-pose selection which utilises a PageRank-based centrality measure to select key poses for action recognition. Gong et al. <ref type="bibr" target="#b12">[13]</ref> have also used a key-pose selection technique based on a local-motion energy optimization criterion to identify the frames with the most discriminatory pose motion information.</p><p>In this paper, we base our architecture for key-frame selection on the AdaBoost learning algorithm. The use of AdaBoost for feature selection in computer vision is fairly recent although it has mostly been applied to 2D data, principally for face recognition <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. The work by Fathi and Mori <ref type="bibr" target="#b16">[17]</ref> used mid-level motion features for action recognition by adopting boosted lowlevel optical flow information. Kellokumpu et al. <ref type="bibr" target="#b17">[18]</ref> have also used AdaBoost to select the most discriminative features for human action recognition. Other researchers have applied an AdaBoost feature-selection scheme for real-time object detection <ref type="bibr" target="#b18">[19]</ref>, fast pedestrian recognition <ref type="bibr" target="#b19">[20]</ref>, and the retrieval of actions in movies <ref type="bibr" target="#b20">[21]</ref> with remarkable improvements in recognition accuracy compared to the previous methods. We too apply the AdaBoost algorithm as a critical part of our methodology for selecting the most discriminative key frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In the area of action recognition, attention is frequently focused on changes in the position of a subject with respect to time. Since motion information can reasonably describe action, we extract motion features by applying an optical-flow algorithm. In this paper, we extract Pyramidal Motion Features (PMFs) from the optical-flow information in each frame of an action sequence and use a supervised machine learning method (AdaBoost) to select the subset of frames with the most discriminatory motion features. A correlogram is then utilised to represent the action sequence instead of the more usual histogram representation-this correlogram representation is demonstrated to be more accurate for action recognition. Finally, we employ a Support-Vector Machine (SVM) to classify the actions. The overall structure of our method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequence pre-processing and optical flow extraction</head><p>We pre-process all raw video sequences by determining 3D bounding boxes which localise the particular actions. For both spatial and temporal dimensions, these bounding boxes are scaled by linear interpolation to equal sizes for all types of actions.</p><p>In order to produce stable measures of the moving regions of the action sequences, we first subtracted adjacent frames. Thus subtracting frames n i Ã¾ 1 and n i gives an estimate of the difference frame midway between n i Ã¾ 1 and n i , n i Ã¾ Ã°1=2Ã . Applying the Lucas-Kanade <ref type="bibr" target="#b21">[22]</ref> algorithm to calculate the optical flow between adjacent difference frames n i Ã¾ Ã°1=2Ã and n iÃÃ°1=2Ã produces an estimate of the optical flow vector field F in frame n i . (Experimentally, the initial differencing operation yielded a more stable flow field estimate since it reduces the effect of background clutter.) The optical flow vector field F is then split into horizontal and vertical components F x and F y . To reduce the effects of noise we have applied a spatial Gaussian blur (s x,y Â¼ 2) to the magnitude images of F x and F y . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Biologically inspired feature framework</head><p>Our biologically inspired feature extraction framework is based on the work of Olva and Torralba <ref type="bibr" target="#b22">[23]</ref> which simulates the scene classification processes in the mammalian visual cortex. In computer vision, biologically inspired features are attractive for visual recognition since they encode intensity information while tolerating motion, translation, and scaling. The feature extraction process focuses on various orientations and intensity contrasts.</p><p>Our approach is motivated by mechanisms in the visual cortex which comprise four different layers, i.e. S1, C1, S2, and C2-see Hubel and Wiesel <ref type="bibr" target="#b23">[24]</ref>. The C1 images mimic complex cells in the visual cortex and form an orientation feature map using the HMAX model <ref type="bibr" target="#b24">[25]</ref>. We first calculate S1 feature maps by applying a Gabor convolution kernel at multiple scales and orientations to the target images. The Gabor filter function is as follows:</p><formula xml:id="formula_0">GÃ°x,yÃ Â¼ exp Ã Ã°X 2 Ã¾ g 2 Y 2 Ã 2s 2 ! cos 2p l x<label>Ã°1Ã</label></formula><p>where X Â¼ x cos yÃy sin y and Y Â¼ x sin yÃ¾y cos y.</p><p>In our method, we define Gabor filters at eight different scales from 5 Ã 5 to 19 Ã 19 at size increments of 2 pixels. In addition, four different orientations (0-1351 at 45 degrees increments) are adopted. In this way, 8 Ã 4Â¼32 S1 feature maps are calculated. To obtain the C1 images, we apply the max pooling technique at various window sizes (from 6 Ã 6 to 12 Ã 12 with size increments of 2 pixels) to down-sample the adjacent scales of the S1 feature maps with the same orientation. In max pooling, we consider an n Ã n image patch in the S1 image and take as the corresponding pixel value of the n-fold down-sampled C1 image, the maximum value in the n Ã n S1 image patch. A similar multiple-orientation mechanism seems to be used in the human brain to perceive visual actions while maintaining invariance to motion, scale, and translation.</p><p>In addition to the C1 images, we construct intensity images also inspired by bioscience. Intensity images simulate the perception of nerve cells which typically respond to the regions of an input image where large changes of colour occur. Thus they focus on the intensity information which is commonly one of the most significant features for representing target objects in computer vision. Song and Tao <ref type="bibr" target="#b25">[26]</ref> have also recently proposed such a scheme for scene classification.</p><p>In this paper, we build intensity images by adopting a centresurround technique which has been widely used in the area of image analysis, especially for object edge enhancement and detection. We first apply a Gaussian filter with different sampling scales to our smoothed optical-flow images 9F x 9 and 9F y 9 to construct an image pyramid of decreasing size with increasing scale. In our experiments, we define seven different levels (s 2 Â¼ 5, 7, 9, 11,y) in the pyramid. Intensity images are obtained by computing the differences between various images in the Gaussian pyramid as illustrated in Fig. <ref type="figure">2</ref>. A surround image of a smaller scale is subtracted from a centre image to yield an intensity image; since the surround image has smaller dimension than the centre image, it is up-sampled appropriately using linear interpolation. We have considered the pairings of centre images 2 and 3, and the surround images 6 and 7, giving four intensity images (2-5, 2-6, 3-6, and 3-7).</p><p>In this way, we obtain C1 and intensity images from the smoothed 9F x 9 and 9F y 9 optical-flow images. The C1 and intensity images are recoded as a single vector by concatenating the data originating from both the F x and F y images. This vector thus comprises the pyramidal motion feature for this frame. Fig. <ref type="figure">3</ref> depicts the flow of the processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaboost key-frame selection</head><p>In this paper, each frame of an action sequence is represented by a Pyramidal Motion Feature (PMF) as described above. Since not all frames in a sequence are relevant to the corresponding action, we employ the AdaBoost learning algorithm to select key frames sufficiently discriminatory to be distinguished from others.</p><p>The AdaBoost learning algorithm is a widely used machine learning method which employs an ensemble of weak classifiers to construct a strong classifier for pattern classification. Given a training set fÃ°x 1 ,y 1 Ã, . . . ,Ã°x N ,y N Ãg where x i is the feature vector, in our case the pyramidal motion feature vector (Section 3.2). y i A fÃ¾1,Ã1g is the class label of the corresponding feature vector. AdaBoost initialises all training data with equal weight values, D i . For each training iteration, the values of weights are updated depending on the learning results, i.e. the weights of incorrectly classified patterns are increased and, conversely, the weights of correctly classified patterns are decreased. Therefore, with increasing numbers of iterations, the classifier focuses more on incorrectly classified patterns. Finally, all the weak classifiers are assembled into one strong one. In our method, we adopt Classification And Regression Trees (CART) <ref type="bibr" target="#b26">[27]</ref> as our base (weak) classifiers. We continue running the AdaBoost algorithm until all patterns in the training set are correctly classified. After the AdaBoost learning procedure, we select as our reduced set of selected features those patterns with the smallest weight values since this implies that they are relatively easy to classify and hence highly discriminatory.</p><p>The conventional AdaBoost learning algorithm is limited to two-class problems. To address the present multi-class problem, we use the 'one-against-the-rest' technique to extend binary classification to multiple classes. For instance, we first label all feature vectors from one type of action as the positive samples, and the remaining vectors belonging to all the other types of actions as the negative samples. Then, the AdaBoost algorithm is run to select the more discriminatory features for the positive class. We repeat this procedure for each type of action (class). In this way, the more discriminatory feature representations of individual frames in a given sequence are selected to represent each type of action. Key frame selection is illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The correlogram of selected frames</head><p>The selected key frames are used as inputs to a 'bag of features' scheme although the histograms originally used with this approach only indicate the probability of feature distributions. Instead of using histograms, our method represents each action video with a correlogram which considers both the statistical distribution, and the relationship among all frames in the temporal dimension. A correlogram extends the K-bins of a 1D histogram representation (where K is the size of the codebook obtained by K-means clustering) to a more informative K Ã K matrix representation.</p><p>Since some spatial and temporal information may be lost by using a histogram representation during the traditional bag of words procedure, we take such omitted information into an account and represent it with a correlogram matrix which was first proposed by Huang et al. <ref type="bibr" target="#b27">[28]</ref> for image indexing. Inspired by Huang's work, we form our pyramidal motion feature correlogram. Each element in our correlogram matrix is calculated as the co-occurrence probability of two frames taking place with a certain time offset of each other-see Eq. ( <ref type="formula">2</ref>). where WÃ°cluster i ,frameÃ Â¼ expÃ°ÃJcentreÃframeJ 2 =2s 2 Ã: Dt indicates the time offset. Frame and centre denote the pyramidal motion feature vector of one particular frame and the centre of a certain cluster (i.e. a visual word produced by K-means clustering). We use the Euclidean distance between the frame and centre.</p><p>In this paper, we use K Â¼120 and to make the correlogram of a key-frame representation more distinctive, we construct it for three different time lags (Dt Â¼ 1,2,4). This correlogram matrix is reordered into a single vector to facilitate later classification using a SVM. The dimensions of this single vector are also reduced by supervised Linear Discriminant Analysis (LDA) prior to classification to remove redundancy. The details of the correlogram representation for action recognition can be found in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We have systematically evaluated our method using three different datasets: KTH <ref type="bibr" target="#b29">[30]</ref>, IXMAS <ref type="bibr" target="#b30">[31]</ref>, and HMDB51 <ref type="bibr" target="#b31">[32]</ref> to demonstrate generalizability, and have compared our results with the other previously published reports.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows examples of the bounding boxes superimposed on typical frames from the three datasets. It is easy to see that these bounding boxes are rather coarsely determined, much larger than the human figures performing the actions, and do not necessarily locate the subject in the centre of the bounding box. Our approach is, therefore, fairly robust to determination of the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KTH dataset</head><p>The KTH dataset contains six types of human action examples (i.e. boxing, hand clapping, hand waving, jogging, running, and walking) featuring 25 different subjects. Each action is performed in four scenarios: outdoors, outdoors with scale variation, outdoors with different clothes, and indoors.</p><p>Following the pre-processing step mentioned in <ref type="bibr" target="#b32">[33]</ref>, the coarse 3D bounding boxes were extracted from all the raw action sequences and further normalised to the equal size of 100 Ã 100 Ã 60. The Pyramidal Motion Feature (PMF) was then extracted from each frame of the pre-processed action sequences and a total of 34,800 features were obtained. We ran the AdaBoost selection procedure six times, once per action type, to select the top 10 discriminatory frames for each action video sequence. Fig. <ref type="figure" target="#fig_4">6</ref> shows the discrimination levels of frames in the KTH dataset. We can observe that different actions have different sets of key discriminative frames.</p><p>As is customary with this dataset, we performed ''leave-oneout'' cross validation over each of the 25 subjects (leave-oneperson-out) to assess the accuracy of action recognition. The average accuracy is 95.5% on the KTH dataset and the corresponding confusion matrix is shown in Fig. <ref type="figure" target="#fig_5">7</ref>. It is clear that good class separation has been obtained for all classes, the greatest confusion being between jogging and running which are two actions that would intuitively seem hard to reliably differentiate.</p><p>Since KTH is one of the most widely used datasets for human action recognition, the present and some previously published  results are compared in Table <ref type="table" target="#tab_3">1</ref>. The present method outperforms all the previously published results on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">IXMAS dataset</head><p>The Inria Xmas Motion Acquisition Sequences (IXMAS) motion dataset 1 is composed of 11 daily human actions performed by 10 actors and recorded from five different viewpoints. This is a more challenging dataset than KTH because actors can choose various positions or orientations in which to perform actions.</p><p>We extracted the bounding boxes by using foreground masks which are provided with the original dataset and then normalised them to the size of 100 Ã 100 Ã 75. We have further extracted the Pyramidal Motion Feature (PMF) for each frame. In the key-frame selection phase, the top 10 discriminatory frames were selected using AdaBoost for each action sequence. The corresponding correlogram was computed using the selected features and a SVM employed for classification. Using ''leave-one-out'' cross validation, we have tested the performance of our method on each single-view camera as well as for the fused multiple-view cameras (i.e. using all the action sequences recorded by the five cameras together, and depicted in Fig. <ref type="figure" target="#fig_6">8(a)</ref>). The overall camera fusion recognition accuracy reaches 93.7%, and the confusion matrix for the multiple-view cameras results is shown in Fig. <ref type="figure" target="#fig_6">8(b)</ref>. For comparison, Table 2 also compares our recognition results with those published in the other reports from which it is clear that our method outperforms all others, even the result of Weinland et al. <ref type="bibr" target="#b37">[38]</ref> in which 3D reconstruction was applied before action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">HMDB51 dataset</head><p>We have also experimented with the HMDB51 actionrecognition dataset which collects action data from a variety of existing movies and online videos. The HMDB51 dataset contains 6849 clips divided into 51 action categories. Each category consists of at least 101 action clips. In our experiments, bounding boxes have been extracted from all the sequences using the masks released with dataset and initialised to the size of 250 Ã 300 Ã 120. Due to the high computational costs, we ran the AdaBoost algorithm to select only the top 25 frames from each action clip. We followed the evaluation approach of Kuehne et al. <ref type="bibr" target="#b31">[32]</ref> and split our data into three groups over which the average  1 http://4drepository.inrialpes.fr/public/viewgroup/6. accuracy is 36.5% for 3-fold cross-validation; the corresponding results are illustrated in Table <ref type="table" target="#tab_1">3</ref>. As far as we are aware, this is the first report of action recognition with the HMDB51 dataset. We only compare our results with the original paper <ref type="bibr" target="#b31">[32]</ref> and our method achieves an obvious improvement on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Summary of results</head><p>For comparison, we summarise the performance of four different recognition methods: our proposed method (PMFÃ¾ AdaBoostÃ¾CorrelogramÃ¾SVM), the same procedure without the Adaboost selection scheme (PMF Ã¾CorrelogramÃ¾SVM), a scheme utilising a histogram to represent actions for classification instead of a correlogram (PMFÃ¾HistogramÃ¾SVM), and direct use of AdaBoost for selection and classification (PMFÃ¾AdaBoost). Comparative results are shown in Table <ref type="table" target="#tab_2">4</ref> from which it is clear that the methods applying Adaboost selection and the correlogram representations achieve the highest recognition accuracy on the KTH, IXMAS, and HMDB51 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have employed the AdaBoost learning algorithm to select the most discriminative frames for a human  action recognition task. Instead of using hand-crafted interest point detectors and the original 'bag of features' approach, here we have utilised a supervised algorithm (i.e. AdaBoost) to select the subset of key, most discriminatory frames which are described by a Pyramidal Motion Feature (PMF). A correlogram is then used to form representations of each action sequence which is finally classified by a Support-Vector Machine (SVM). We have demonstrated that our boosted key-frame selection scheme produces an improvement in action recognition performance on three datasets: KTH, IXMAS, and HMDB51.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The main structure of our proposed method.</figDesc><graphic coords="2,147.77,614.39,309.60,115.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Illustration of the formation intensity images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The left sub-figure shows four boosted key-frame samples for the 'Handwaving' action. The middle sub-figure illustrates the final boundary for two kinds of patterns (e.g. one from 'Handwaving' and the other from all other action types); note that the key frames are remote from the decision surface. The right sub-figure shows the classification error with the increasing iterations during the AdaBoost learning.</figDesc><graphic coords="5,77.97,58.64,429.84,122.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of the bounding boxes employed on typical frames from the three datasets. Row 1 Â¼KTH dataset, row 2Â¼ IXMAS dataset, and row 3 Â¼ HMDB51 dataset.</figDesc><graphic coords="5,77.97,227.03,429.84,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) to (f) show the discrimination levels of sequence frames for different actions in the KTH dataset calculated from the corresponding weight values after Adaboost selection. (Here we plot discrimination level Â¼ 100 e ÃÃ°weightÃ 2 for convenience of display.) The frames with the highest discrimination levels are selected as key frames. The red bars in each sub-figure indicate the top ten key frames selected for each action type. (a) Boxing, (b) Handclapping, (c) Handwaving, (d) Jogging, (e) Running, (f) Walking.</figDesc><graphic coords="6,42.80,58.64,519.84,296.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The confusion matrix of final classification results for the KTH dataset.</figDesc><graphic coords="6,78.07,401.39,180.00,170.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. IXMAS dataset. (a) Multi-view sketch map on the IXMAS dataset. (b) The confusion matrix of the multiple camera fusion result.</figDesc><graphic coords="7,77.98,192.18,429.84,213.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,147.77,58.64,309.60,226.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,147.77,323.46,309.60,398.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Classification accuracies (%) of different methods for both single and multiple camera view on the IXMAS dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Camera view</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cam1</cell><cell>Cam2</cell><cell>Cam3</cell><cell>Cam4</cell><cell>Cam5</cell><cell>Cam1-5</cell></row><row><cell>Our method</cell><cell>84.7</cell><cell>89.0</cell><cell>85.6</cell><cell>84.5</cell><cell>80.1</cell><cell>93.7</cell></row><row><cell>Varma and Babu [39]</cell><cell>76.4</cell><cell>74.5</cell><cell>73.6</cell><cell>71.8</cell><cell>60.4</cell><cell>81.3</cell></row><row><cell>Liu and Shah [37]</cell><cell>76.7</cell><cell>73.3</cell><cell>72.1</cell><cell>73.1</cell><cell>-</cell><cell>82.8</cell></row><row><cell>Wu et al. [40]</cell><cell>81.9</cell><cell>80.1</cell><cell>77.1</cell><cell>77.6</cell><cell>73.4</cell><cell>88.2</cell></row><row><cell>Weinland et al. [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Classification accuracies (%) on the HMDB51 dataset.</figDesc><table><row><cell>Methods</cell><cell>Splits</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>Split 1</cell><cell>Split 2</cell><cell>Split 3</cell><cell></cell></row><row><cell>Our method</cell><cell>38.3</cell><cell>40.8</cell><cell>30.4</cell><cell>36.5</cell></row><row><cell>Kuehne et al. [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparison of recognition performance (%) with/without the AdaBoost algorithm on the KTH, IXMAS, and HMDB51 datasets.</figDesc><table><row><cell>Methods</cell><cell>Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell>KTH</cell><cell>IXMAS</cell><cell>HMDB51</cell></row><row><cell>PMFÃ¾ HistogramÃ¾ SVM</cell><cell>82.7</cell><cell>76.3</cell><cell>28.3</cell></row><row><cell>PMFÃ¾ CorrelogramÃ¾ SVM</cell><cell>84.6</cell><cell>81.1</cell><cell>31.6</cell></row><row><cell>PMFÃ¾ AdaBoost</cell><cell>91.7</cell><cell>86.8</cell><cell>33.8</cell></row><row><cell>PMFÃ¾ AdaBoostÃ¾ Correlogram Ã¾SVM</cell><cell>95.5</cell><cell>93.7</cell><cell>36.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Comparison of action recognition accuracies in percentage (%) on the KTH dataset for different methods.</figDesc><table><row><cell>Methods</cell><cell>Actions</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dolla Â´r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd European Conference on Computational Learning Theory (EuroCOLT&apos;95)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action recognition using correlogram of body poses and spectral regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human action recognition by semilatent topic models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1762" to="1774" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action recognition by shape matching to key frames</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Workshop on Models versus Exemplars in Computer Vision</title>
		<meeting><address><addrLine>Kauai, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information theoretic key frame selection for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC 2008)</title>
		<meeting><address><addrLine>Leeds, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative techniques for keyframe selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>The Netherlands</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive key frame extraction using unsupervised clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<meeting><address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selecting key poses on manifold for pairwise action recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="168" to="177" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic key pose selection for 3D human action recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonza Â´lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Articulated Motion and Deformable Objects (AMDO&apos;10)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Face verification using Gabor wavelets and AdaBoost</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="404" to="407" />
		</imprint>
	</monogr>
	<note>International Conference on Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: a comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature selection using AdaBoost for face expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piyanuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IASTED International Conference on Visualization, Imaging, and Image Processing</title>
		<meeting><address><addrLine>Marbella, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="261" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition by learning mid-level motion features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic textures for human movement recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>PietikÈinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Image and Video Retrieval</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="470" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining AdaBoost learning and evolutionary search to select features for real-time object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treptow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Congress on Evolutionary Computation</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2107" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast pedestrian detection using a cascade of boosted covariance features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1140" to="1151" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pe Â´rez, Retrieving actions in movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Image Understanding Workshop</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Receptive fields and functional architecture of monkey striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="243" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Biologically inspired feature manifold for scene classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification and regression trees: a powerful yet simple technique for ecological data analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>De'ath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Fabricius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3178" to="3192" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image indexing using color correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Silhouette analysis based action recognition via exploiting human poses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2012.2203731</idno>
		<ptr target="http://dx.doi.org/10.1109/TCSVT.2012.2203731" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3D exemplars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J E G T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Hough transform-based voting framework for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2061" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning human actions via information maximization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">More generality in efficient multiple kernel learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition using context and appearance distribution features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">He is currently working toward the Ph.D. degree in the Department of Electronic and Electrical Engineering, the University of Sheffield, UK. His research interests include human action recognition, scene and object classification, and genetic programming for visual feature extraction</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu Received The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Information Engineering from Xi&apos;an Jiaotong University</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>representation and description</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ling Shao is currently a Senior Lecturer (Associate Professor) in the Department of Electronic and Electrical Engineering at the University of Sheffield. Before joining Sheffield University, he worked for four years as a Senior Scientist in Philips Research, The Netherlands. His research interests include computer vision, pattern recognition, and video processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ustc)</surname></persName>
		</author>
		<author>
			<persName><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has organized several workshops with top conferences, such as ICCV, ACM Multimedia, and ECCV. He has been serving as Program Committee member for many international conferences, including CVPR, ECCV, ICIP, ICASSP, ICME, ICMR, ACM MM, CIVR, BMVC, etc</title>
		<meeting><address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Ling Shao received the B.Eng. degree in Electronic Engineering from the University of Science and Technology of China ; Robotics Research Group from the University of Oxford. Dr ; University of Manchester Institute of Science and Technology (UMIST ; Electronic and Electrical Engineering, University of Sheffield, England</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests are in image processing, statistical pattern recognition, and multiobjective genetic programming</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
