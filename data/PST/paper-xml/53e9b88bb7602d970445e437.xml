<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning-Based, Automatic 2D-to-3D Image and Video Conversion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
							<email>jkonrad@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Prakash Ishwar, Senior Member, IEEE</roleName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<email>wangmeng1985@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">are with Google Inc</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
							<email>chenwu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">are with Google Inc</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debargha</forename><surname>Mukherjee</surname></persName>
							<email>debargha@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">are with Google Inc</orgName>
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning-Based, Automatic 2D-to-3D Image and Video Conversion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CCFC536E51AC4DB41A9B3B9FA86DA0A1</idno>
					<idno type="DOI">10.1109/TIP.2013.2270375</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D images</term>
					<term>stereoscopic images</term>
					<term>image conversion</term>
					<term>nearest neighbor classification</term>
					<term>cross-bilateral filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite a significant growth in the last few years, the availability of 3D content is still dwarfed by that of its 2D counterpart. To close this gap, many 2D-to-3D image and video conversion methods have been proposed. Methods involving human operators have been most successful but also timeconsuming and costly. Automatic methods, which typically make use of a deterministic 3D scene model, have not yet achieved the same level of quality for they rely on assumptions that are often violated in practice. In this paper, we propose a new class of methods that are based on the radically different approach of learning the 2D-to-3D conversion from examples. We develop two types of methods. The first is based on learning a point mapping from local image/video attributes, such as color, spatial position, and, in the case of video, motion at each pixel, to scene-depth at that pixel using a regression type idea. The second method is based on globally estimating the entire depth map of a query image directly from a repository of 3D images (image + depth pairs or stereopairs) using a nearest-neighbor regression type idea. We demonstrate both the efficacy and the computational efficiency of our methods on numerous 2D images and discuss their drawbacks and benefits. Although far from perfect, our results demonstrate that repositories of 3D content can be used for effective 2D-to-3D image conversion. An extension to video is immediate by enforcing temporal continuity of computed depth maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE availability of 3D-capable hardware today, such as TVs, Blu-Ray players, gaming consoles, and smartphones, is not yet matched by 3D content production. Although constantly growing in numbers, 3D movies are still an exception rather than a rule, and 3D broadcasting (mostly sports) is still minuscule compared to 2D broadcasting. The gap between 3D hardware and 3D content availability is likely to close in the future, but today there exists an urgent need to convert the existing 2D content to 3D.</p><p>A typical 2D-to-3D conversion process consists of two steps: depth estimation for a given 2D image and depth-based rendering of a new image in order to form a stereopair. While the rendering step is well understood and algorithms exist that produce good quality images, the challenge is in estimating depth from a single image (video). Therefore, throughout this paper the focus is on depth recovery and not on depth-based rendering, although we will briefly discuss our approach to this problem later.</p><p>There are two basic approaches to 2D-to-3D conversion: one that requires a human operator's intervention and one that does not. In the former case, the so-called semi-automatic methods have been proposed where a skilled operator assigns depth to various parts of an image or video. Based on this sparse depth assignment, a computer algorithm estimates dense depth over the entire image or video sequence. The involvement of a human operator may vary from just a few scribbles to assign depth to various locations in an image to a precise delineation of objects and subsequent depth assignment to the delineated regions.</p><p>In the case of automatic methods, no operator intervention is needed and a computer algorithm automatically estimates the depth for a single image (or video). To this effect, methods have been developed that estimate shape from shading, structure from motion or depth from defocus. Although such methods have been shown to work in some restricted scenarios they do not work well for arbitrary scenes. In an attempt to equip 3D TVs, Blu-Ray players and gaming consoles with real-time automatic 2D-to-3D conversion, consumer electronics manufacturers have developed simpler techniques that rely on various heuristic assumptions but such methods fail on more challenging scenes. Recently, machine-learning-inspired methods have been proposed to automatically estimate the depth map of a single monocular image by applying image parsing. Although restricted to architectural scenes, these methods opened a new direction for 2D-to-3D conversion. We will review both semi-automatic and automatic methods in Section II in detail.</p><p>The methods we propose in this paper, carry the "big data" philosophy of machine learning. In consequence, they apply to arbitrary scenes and require no manual annotation. Our datadriven approach to 2D-to-3D conversion has been inspired by the recent trend to use large image databases for various computer vision tasks, such as object recognition <ref type="bibr" target="#b17">[18]</ref> and image saliency detection <ref type="bibr" target="#b18">[19]</ref>. In particular, we propose a new class of methods that are based on the radically different approach of learning the 2D-to-3D conversion from examples.</p><p>We develop two types of methods. The first one is based on learning a point mapping from local image/video attributes, such as color, spatial position, and motion at each pixel, to scene-depth at that pixel using a regression type idea. The second one is based on globally estimating the entire depth map of a query image directly from a repository of 3D images (image + depth pairs or stereopairs) using a nearest-neighbor regression type idea. Early versions of our learning-based approach to 2D-to-3D image conversion, either suffered from high computational complexity <ref type="bibr" target="#b7">[8]</ref> or were tested on only a single dataset <ref type="bibr" target="#b8">[9]</ref>. Here, we introduce the local method and evaluate the qualitative performance and the computational efficiency of both the local and global methods against those of the Make3D algorithm <ref type="bibr" target="#b13">[14]</ref> and a recent method proposed by Karsch et al. <ref type="bibr" target="#b6">[7]</ref>. We demonstrate the improved quality of the depth maps produced by our global method relative to stateof-the-art methods together with up to 4 orders of magnitude reduction in computational effort. We also discuss weaknesses of both proposed methods.</p><p>The paper is organized as follows. In Section II, we review the state of the art in 2D-to-3D image conversion. In Section III, we describe the conversion based on local point transformation and in Section IV we provide details of the global approach to the conversion. In Section V, we show numerous experimental results and we conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STATE OF THE ART</head><p>There are two types of 2D-to-3D image conversion methods: semi-automatic methods, that require human operator intervention, and automatic methods, that require no such help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semi-Automatic Methods</head><p>To date, this has been the more successful approach to 2Dto-3D conversion. In fact, methods that require a significant operator intervention in the conversion process, such as delineating objects in individual frames, placing them at suitable depths, and correcting errors after final rendering, have been successfully used commercially by such companies as Imax Corp., Digital Domain Productions Inc. (formerly In-Three Inc.), etc. Many films have been converted to 3D using this approach.</p><p>In order to reduce operator involvement in the process and, therefore, lower the cost while speeding up the conversion, research effort has recently focused on the most labor-intensive steps of the manual involvement, namely spatial depth assignment. Guttmann et al. <ref type="bibr" target="#b5">[6]</ref> have proposed a dense depth recovery via diffusion from sparse depth assigned by the operator. In the first step, the operator assigns relative depth to image patches in some frames by scribbling. In the second step, a combination of depth diffusion, that accounts for local image saliency and local motion, and depth classification is applied. In the final step, disparity is computed from the depth field and two novel views are generated by applying half of the disparity amplitude. The focus of the method proposed by Agnot et al. <ref type="bibr" target="#b0">[1]</ref> is the application of cross-bilateral filtering to an initial depth map. The authors propose to use a library of initial depth maps (smooth maps consistent with the 3D perspective of outdoor scenes or rooms) from which an operator can choose one that best corresponds to the image being converted. They also suggest estimation of the initial depth map based on image blur but show only one very simple example; this initialization is unlikely to work well in more complex cases. Phan et al. <ref type="bibr" target="#b11">[12]</ref> propose a simplified and more efficient version of the Guttmann et al. <ref type="bibr" target="#b5">[6]</ref> method using scale-space random walks that they solve with the help of graph cuts. Liao et al. <ref type="bibr" target="#b9">[10]</ref> further simplify operator involvement by first computing optical flow, then applying structure-from-motion estimation and finally extracting moving object boundaries. The role of an operator is to correct errors in the automatically computed depth of moving objects and assign depth in undefined areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automatic Methods</head><p>The problem of depth estimation from a single 2D image, which is the main step in 2D-to-3D conversion, can be formulated in various ways, for example as a shape-fromshading problem <ref type="bibr" target="#b19">[20]</ref>. However, this problem is severely under-constrained; quality depth estimates can be found only for special cases. Other methods, often called multi-view stereo, attempt to recover depth by estimating scene geometry from multiple images not taken simultaneously. For example, a moving camera permits structure-from-motion estimation <ref type="bibr" target="#b16">[17]</ref> while a fixed camera with varying focal length permits depthfrom-defocus estimation <ref type="bibr" target="#b15">[16]</ref>. Both are examples of the use of multiple images of the same scene captured at different times or under different exposure conditions (e.g., all images of the Statue of Liberty). Although such methods are similar in spirit to the methods proposed here, the main difference is that while these methods use images known to depict the same scene as the query image, we use all images available in a large repository and automatically select suitable ones for depth recovery.</p><p>Several electronics manufacturers have developed real-time 2D-to-3D converters that rely on stronger assumptions and simpler processing than the methods discussed above, e.g., faster-moving or larger objects are assumed to be closer to the viewer, higher frequency of texture is assumed to belong to objects located further away, etc. Although such methods may work well in specific scenarios, in general it is very difficult, if not impossible, to construct heuristic assumptions that cover all possible background and foreground combinations. Such real-time methods have been implemented in Blu-Ray 3D players by LG, Samsung, Sony and others. DDD offers its TriDef 3D software for PCs, TVs and mobile devices. However, these are proprietary systems and no information is available about the assumptions used.</p><p>Recently, machine-learning-inspired techniques employing image parsing have been used to estimate the depth map of a single monocular image <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Such methods have the potential to automatically generate depth maps, but currently work only on few types of images (mostly architectural scenes) using carefully-selected training data (precise, laserscanned depth estimates or manually-annotated semantic depth classes). In the quest to develop data-driven approaches to 2D-to-3D conversion we have also been inspired by the recent trend to use large image databases for various computer vision tasks, such as object recognition <ref type="bibr" target="#b17">[18]</ref> and image saliency detection <ref type="bibr" target="#b18">[19]</ref>. In our first attempt, we developed a method that fuses SIFT-aligned depth maps selected from a large 3D database, however this approach proved to be computationally demanding <ref type="bibr" target="#b7">[8]</ref>. Subsequently, we skipped the costly SIFTbased depth alignment and used a different metric (based on histogram of gradients) for selecting most similar depth fields from a database. We observed no significant quality degradation but a significant reduction of the computational complexity <ref type="bibr" target="#b8">[9]</ref>. Very recently, Karsch et al. <ref type="bibr" target="#b6">[7]</ref> have proposed a depth extraction method based on SIFT warping that essentially follows our initial, unnecessarily complex, approach to depth extraction <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. 2D-TO-3D CONVERSION BY LEARNING A LOCAL POINT TRANSFORMATION</head><p>The first class of conversion methods we are presenting is based on learning a point transformation that relates local lowlevel image or video attributes at a pixel to scene-depth at that pixel. Once the point transformation is learned, it is applied to a monocular image, i.e., depth is assigned to a pixel based on its attributes. This is in contrast to methods described in Section IV where the entire depth map of a query is estimated directly from a repository of 3D images (image + depth pairs or stereopairs) using a nearest-neighbor regression type idea.</p><p>A pivotal element in this approach is a point transformation used to compute depth from image attributes. This transformation can be estimated either by training on a groundtruth dataset, the approach we take in this paper, or defined heuristically. Let</p><formula xml:id="formula_0">I = {( I 1 , d 1 ), ( I 2 , d 2 ), ..., ( I K , d K )} denote a training dataset composed of K pairs ( I k , d k )</formula><p>, where I k is a color image (usually in Y U V format) and d k is the corresponding depth field. We assume that all images and depth fields have the same spatial dimensions. Such a dataset can be constructed in various ways. One example is the Make3D dataset <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref> that consists of images and depth fields captured outdoors by a laser range finder. Another example is the NYU Kinect dataset <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr">[22]</ref> containing over 100 k images and depth fields captured indoors using a Kinect camera.</p><p>Examples of low-level video attributes that can be leveraged to compute relative depth of a pixel include color, spatial location, and local motion. Although the dependence of depth on pixel color, location and motion may seem counter intuitive, it is enough to observe that a bluish color is often associated with a distant sky, the bottom of a picture usually depicts ground close to the camera and a moving object stays in front of the background. These three attributes are in fact used in the YouTube 2D-to-3D conversion service recently released on-line.</p><p>Given a training set I consisting of K image-depth pairs, one can, in principle, learn a general regression function that maps a tuple of local features such as (color,location,motion) to a depth value, i.e., f : (color, location, motion) -→ depth.</p><p>However, to ensure low run-time memory and processing costs, we learn a more restricted form of transformation:</p><formula xml:id="formula_1">f [color, x, motion] = w c f c [color]+w l f l [x]+w m f m [motion].</formula><p>We now discuss how the individual color-depth, locationdepth, and motion-depth transformations as well as the weights are learned.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows a sample video frame with depth maps estimated from color, location and motion cues separately, as well as the final combined depth map. In order to obtain a colordepth transformation f c , we first transform the Y U V space, commonly used in compressed images and videos, to the H SV color space. We found out that the saturation component (S) provides little depth discrimination capacity and therefore we limit the transformation attributes to hue (H ) and value (</p><formula xml:id="formula_2">V ). Let [H k [x], S k [x], V k [x]] T be the H SV components of a pixel at spatial location x quantized to L levels. The depth mapping f c [h, v], h, v = 1, .</formula><p>.., L is computed as the average of depths at all pixels in I with hue h and value v:</p><formula xml:id="formula_3">f c [h, v] = K k=1 x 1(H k [x] = h, V k [x] = v)d k [x] K k=1 x 1(H k [x] = h, V k [x] = v) (1)</formula><p>where 1(A) is the indicator function which equals one if A is true and equals zero otherwise.</p><p>Fig. <ref type="figure">2</ref>(a) shows the transformation f c computed from a dataset I of, mostly, outdoor scenes. Note a large dark patch around reddish colors indicating that red elements of a scene are located closer to the camera. A large bright patch around bright-bluish colors is indicative of a far-away sky. The bright patch around yellow-orange colors is more difficult to classify but may be due to the distant sun as many videos have been captured outdoors. The location-depth transformation f l is simply the average depth computed from all depth maps in I at the same location:</p><formula xml:id="formula_4">f l [x] = 1 K K k=1 d k [x].<label>(2)</label></formula><p>Fig. <ref type="figure">2</ref>(b) shows an example of the above transformation computed from the same dataset I. It is clear that locations in the center and towards the bottom of the image usually correspond to objects that are closer to the camera. In addition to color and spatial attributes, video sequences may contain motion attributes relevant to depth recovery. In this case, local motion between consecutive video frames is of interest. The underlying assumption in the motion-depth transformation is that moving objects are closer to the viewer than the background. In order to estimate the motion-depth transformation f m , the basic idea is to first compute local motion between consecutive video frames, then extract a moving object mask from this motion, and, finally, assign a distinct depth (smaller than that of the background) to this mask. This brings the moving objects closer to the viewer. The estimation of local motion may be accomplished by any optical flow method, e.g., <ref type="bibr" target="#b1">[2]</ref>, but may also require global motion compensation, e.g., <ref type="bibr" target="#b4">[5]</ref>, in order to account for camera movements. A simple thresholding of the magnitude of local motion produces a moving object's mask. However, since such masks are often noisy some form of smoothing may be needed. Cross-bilateral filtering <ref type="bibr" target="#b3">[4]</ref> controlled by the luminance of the video frame, in which the estimated local motion is anchored, usually suffices.</p><p>In the final step, the local transformation outputs are linearly combined to produce the final depth field (Fig. <ref type="figure" target="#fig_0">1</ref>). While the location-depth weight w l may be kept constant, the motiondepth weight w m can be adjusted in proportion to the number of pixels deemed moving in the image being converted. Therefore, the color-depth weight w c equals 1 -w l -w m . Assuming that the image to be converted to 3D is the left image of a fictitious stereopair, the right image is rendered from the left image and the inferred depth field. An example of such rendering is discussed in Section IV-D.</p><p>IV. 2D-TO-3D CONVERSION BASED ON GLOBAL NEAREST-NEIGHBOR DEPTH LEARNING While 2D-to-3D conversion based on learning a local point transformation has the undisputed advantage of computational efficiency -the point transformation can be learned off-line and applied basically in real time -the same transformation is applied to images with potentially different global 3D scene structure. This is because this type of conversion, although learning-based, is based on purely local image/video attributes, such as color, spatial position, and motion at each pixel. To address this limitation, in this section we develop a second method that estimates the global depth map of a query image or video frame directly from a repository of 3D images (image + depth pairs or stereopairs) using a nearest-neighbor regression type idea.</p><p>The approach we propose here is built upon a key observation and an assumption. The key observation is that among millions of 3D images available on-line, there likely exist many whose 3D content matches that of a 2D input (query) we wish to convert to 3D. We are also making an assumption that two images that are photometrically similar also have similar 3D structure (depth). This is not unreasonable since photometric properties are often correlated with 3D content (depth, disparity). For example, edges in a depth map almost always coincide with photometric edges.</p><p>Given a monocular query image Q, assumed to be the left image of a stereopair that we wish to compute, we rely on the above observation and assumption to "learn" the entire depth field from a repository of 3D images I and render a stereopair in the following steps:</p><p>1) search for representative depth fields: find k 3D images in the repository I that have most similar depth to the query image, for example by performing a k nearest-neighbor (kNN) search using a metric based on photometric properties, 2) depth fusion: combine the k representative depth fields, for example, by means of median filtering across depth fields, 3) depth smoothing: process the fused depth field to remove spurious variations, while preserving depth discontinuities, for example, by means of cross-bilateral filtering, 4) stereo rendering: generate the right image of a fictitious stereopair using the monocular query image and the smoothed depth field followed by suitable processing of occlusions and newly-exposed areas. Specific details of these steps depend on the type of 3D images contained in the repository. The above steps apply directly to 3D images represented as an image + depth pair. However, in the case of stereopairs a disparity field needs to be computed first for each left/right image pair. Then, each disparity field can be converted to a depth map, e.g., under a parallel camera geometry assumption, with fusion and smoothing taking place in the space of depths. Alternatively, the fusion and smoothing can take place in the space of disparities (without converting to depth), and the final disparity used for right-image rendering.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the block diagram of our approach. The sections below provide a description of each step and some highlevel mathematical detail. In these sections, Q R is the right image which is being sought for each query image Q, while d Q is the query depth (ground truth) needed to numerically evaluate the performance of a depth computation. Again, we assume that a 3D dataset I is available by means of laser range finding, Kinect-based capture or disparity computation. The goal is to find a depth estimate d and then a right-image estimate Q R given a 2D query image Q and the 3D dataset I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. kNN Search</head><p>There exist two types of images in a large 3D image repository: those that are relevant for determining depth in a 2D query image, and those that are irrelevant. Images that are not photometrically similar to the 2D query need to be rejected because they are not useful for estimating depth (as per our assumption). Note that although we might miss some depth-relevant images, we are effectively limiting the number of irrelevant images that could potentially be more harmful to the 2D-to-3D conversion process. The selection of a smaller subset of images provides the added practical benefit of computational tractability when the size of the repository is very large.</p><p>One method for selecting a useful subset of depth-relevant images from a large repository is to select only the k images that are closest to the query where closeness is measured by some distance function capturing global image properties such as color, texture, edges, etc. As this distance function, we use the Euclidean norm of the difference between histograms of oriented gradients (HOGs) <ref type="bibr" target="#b2">[3]</ref> computed from two images. Each HOG consists of 144 real values (4 × 4 blocks with 9 gradient direction bins) that can be efficiently computed.</p><p>We perform a search for top matches to our monocular query Q among all images I k , k = 1, ..., K in the 3D database I. The search returns an ordered list of image + depth pairs, from the most to the least photometrically similar vis-à-vis the query. We discard all but the top k matches (kNNs) from this list.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows search results for two outdoor query images performed on the Make3D dataset #1. Although none of the four kNNs perfectly matches the corresponding 2D query, the general underlying depth is somewhat related to that expected in the query. In Fig. <ref type="figure" target="#fig_5">5</ref> we show search results for two indoor query images (office and dining room) performed on the NYU Kinect dataset. While some of the retained images share local 3D structures with the query image, e.g., a large table in the dining room, other images do not. Again, the general depth is somewhat related to that expected in the query.</p><p>The average photometric similarity between a query and its k-th nearest neighbor usually decays with the increasing k. While for large databases, larger values of k may be appropriate, since there are many good matches, for smaller databases this may not be true. Therefore, a judicious selection of k is important. We discuss the choice of k in Section V. We denote by K the set of indices i of image + depth pairs that are the top k photometrically-nearest neighbors of the query Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Fusion</head><p>In general, none of the NN image + depth pairs (I i , d i ), i ∈ K match the query Q accurately (Figs. <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref>). However, the location of some objects (e.g., furniture) and parts of the background (e.g., walls) is quite consistent with those in the respective query. If a similar object (e..g, building, table) appears at a similar location in several kNN images, it is likely that such an object also appears in the query, and the depth field being sought should reflect this. We compute this depth field by applying the median operator across the kNN depths at each spatial location x as follows:</p><formula xml:id="formula_5">d[x] = median{d i [x] ∀i ∈ K}. (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Examples of the fused depth fields d are shown in the central column of Fig. <ref type="figure" target="#fig_7">7</ref> for two NYU Kinect examples. Although these depths are overly smooth, they provide a globallycorrect, although coarse, assignment of distances to various areas of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-Bilateral Filtering (CBF) of Depth</head><p>While the median-based fusion helps make depth more consistent globally, the fused depth is overly smooth and locally inconsistent with the query image due to edge misalignment between the depth fields of the kNNs and the query image. This, in turn, often results in the lack of edges in the fused depth where sharp object boundaries should occur and/or the lack of fused-depth smoothness where smooth depth is expected.  In order to correct this, similarly to Agnot et al. <ref type="bibr" target="#b0">[1]</ref>, we apply cross-bilateral filtering (CBF). CBF is a variant of bilateral filtering, an edge-preserving image smoothing method that applies anisotropic diffusion controlled by the local content of the image itself <ref type="bibr" target="#b3">[4]</ref>. In CBF, however, the diffusion is not controlled by the local content of the image under smoothing but by an external input. We apply CBF to the fused depth d using the query image Q to control diffusion. This allows us to achieve two goals simultaneously: alignment of the depth edges with those of the luminance Y in the query image Q and local noise/granularity suppression in the fused depth d. This is implemented as follows:</p><formula xml:id="formula_7">d[x] = 1 γ [x] y d[y]h σ s (x -y)h σ e (Y [x] -Y [y]), γ [x] = y h σ s (x -y)h σ e (Y [x] -Y [y]), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where d is the filtered depth field and h σ (x) = exp(-    In Fig. <ref type="figure" target="#fig_6">6</ref>, we show an example of median-fused depth field after cross-bilateral filtering. Clearly, the depth field is overall smooth (slowly varying) while depth edges, if any, are aligned with features in the query image. Fig. <ref type="figure" target="#fig_7">7</ref> compares the fused depth before cross-bilateral filtering and after. The filtered depth preserves the global properties captured by the unfiltered depth field d, and is smooth within objects and in the background. At the same time it keeps edges sharp and aligned with the query image structure.</p><formula xml:id="formula_9">x 2 /2σ 2 )/2πσ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stereo Rendering</head><p>In order to generate an estimate of the right image Q R from the monocular query Q, we need to compute a disparity δ from the estimated depth d. Assuming that the fictitious image pair (Q, Q R ) was captured by parallel cameras with baseline B and focal length f , the disparity is simply δ</p><formula xml:id="formula_10">[x, y] = B f / d[x],</formula><p>where x = [x, y] T . We forward-project the 2D query Q to produce the right image:</p><formula xml:id="formula_11">Q R [x + δ[x, y], y] = Q[x, y]<label>(5)</label></formula><p>while rounding the location coordinates (x + δ[x, y], y) to the nearest sampling grid point. We handle occlusions by depth ordering: if</p><formula xml:id="formula_12">(x i + δ[x i , y i ], y i ) = (x j + δ[x j , y i ], y i )</formula><p>for some i, j , we assign to the location</p><formula xml:id="formula_13">(x i + δ[x i , y i ], y i ) in Q R an RGB value from that location (x i , y i ) in Q whose disparity δ[x i , y i ]</formula><p>is the largest. In newly-exposed areas, i.e., for x j such that no x i satisfies (x j , y i ) = (x i + δ[x i , y i ], y i ), we apply simple inpainting using inpaint_nans from Mat-labCentral. Applying a more advanced depth-based rendering method would only improve this step of the proposed 2D-to-3D conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We have tested our approach on two datasets: the Make3D dataset #1 <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref> composed of 534 outdoor images with depth fields captured by a laser range finder and the NYU Kinect dataset <ref type="bibr" target="#b14">[15]</ref>, [22] composed of 1449 pairs of RGB images and corresponding depth fields. Note that the Make3D images are of 1704 × 2272 resolution but the corresponding depth fields are only of 55 × 305 spatial resolution and relatively coarse quantization. Therefore, for computational efficiency, we have re-sized the images to 240 × 320 resolution. On the other hand, the Kinect dataset consists of both images and depth fields at 640 × 480 resolution and the depth precision is relatively high (11 bits).</p><p>In order to evaluate the performance of the proposed algorithms quantitatively, we first applied leave-one-out cross-validation (LOOCV) as follows. We selected one image + depth pair from a database as the 2D query (Q, d Q ) treating the remaining pairs as the 3D image repository I based on which a depth estimate d and a right-image estimate Q R are computed. As the quality metric, we used normalized cross-covariance between the estimated depth d and the ground-truth depth d Q defined as follows:</p><formula xml:id="formula_14">C = 1 Nσ d σ d Q x ( d[x] -μ d )(d Q [x] -μ d Q ) (<label>6</label></formula><formula xml:id="formula_15">)</formula><p>where N is the number of pixels in d and d Q , μ d and μ d Q are the empirical means of d and d Q , respectively, while σ d and σ d Q are the corresponding empirical standard deviations. The normalized cross-covariance C takes values between -1 and +1 (for values close to +1 the depths are very similar and for values close to -1 they are complementary). An important parameter in the global algorithm is the number of nearest neighbors k to be used. We selected k by running the LOOCV test for each image in the Kinect database for all k from 1 to 120 and averaging the resulting crosscovariance C across all tests. The average C rapidly rose for small k, achieved maximum at k = 45 and then gently rolledoff. Therefore, in all experiments with the global algorithm we used k = 45.</p><p>Since most of the fully-automatic 2D-to-3D conversion methods have been developed by 3D equipment manufacturers, the employed algorithms are proprietary. The only automatic 2D-to-3D conversion methods for which we were able to find a run-time code was Make3D developed by Saxena et al. <ref type="bibr" target="#b13">[14]</ref> and a recent method by Karsch et al. <ref type="bibr" target="#b6">[7]</ref>. Make3D estimates 3D scene structure from a single still image of MAKE3D ALGORITHM <ref type="bibr" target="#b13">[14]</ref>, AND A METHOD BY KARSCH et al. <ref type="bibr" target="#b6">[7]</ref> an unstructured environment by supervised learning of 3D position and orientation of small homogeneous patches in the image. The original Make3D algorithm was trained on images from the Make3D dataset #1 and associated laser-scanned depth maps of mostly architectural structures. Admittedly, it was not optimized for indoor scenes that the Kinect depth dataset is composed of, however there is no option provided to re-train Make3D on other datasets. The method of Karsch et al. is essentially based on our earlier work using SIFT flow <ref type="bibr" target="#b7">[8]</ref>. It consists of finding nearest neighbors using high-level features (they use k = 7), followed by SIFT-flow to warp the k depth fields to the current image, and optimization to combine the warped depths while imposing a smoothness constraint and a global depth prior. They do not, however, use median depth fusion and CBF.</p><p>Table <ref type="table" target="#tab_1">I</ref> shows experimental results obtained from 534 LOOCV tests on the Make3D dataset #1 using various algorithms. Similarly, Table <ref type="table" target="#tab_2">II</ref> shows results for 1449 LOOCV tests on the Kinect dataset. The performance of each algorithm has been captured by the average and median of cross-covariance C (6) across all LOOCV tests.</p><p>The local method has been trained on the Make3D and Kinect datasets, respectively, i.e., the transformations f c and f l (transformation f m is not used since both datasets contain only still images), have been learned by analyzing depth-color and depth-location relationships in all image-depth pairs of either dataset. We have used weights w c = 0.3 and w l = 0.7 in our experiments. The global method and the method by Karsch et al. <ref type="bibr" target="#b6">[7]</ref> have no training phase but learn the depth from k best examples found for each query image. As we have already mentioned, the Make3D algorithm <ref type="bibr" target="#b13">[14]</ref> has been trained on the Make3D dataset and there is no option available to re-train it on the Kinect dataset.</p><p>Clearly, for both datasets the global method with crossbilateral filtering of the fused depths outperforms all other algorithms, although the same algorithm without the filtering performs very similarly. The numerical gain from filtering the fused depth is rather small since its greatest impact is at depth edges (re-alignment with edges in the query image). Consequently, it affects the normalized cross-covariance at just a few pixels. The Make3D algorithm performs almost as well as our global method on the Make3D dataset. However, this result is biased towards high values of C since the test images in our LOOCV test include images from  <ref type="figure" target="#fig_6">6</ref> and<ref type="figure" target="#fig_7">7</ref>. Note a higher consistency of depth fields and a better depth edge alignment with ground truth for the global method than other methods. We would like to point out that although C values shown in Fig. <ref type="figure" target="#fig_7">7</ref> are slightly lower for depth fields after cross-bilateral filtering, the depth edge alignment with the query and the high piece-wise depth smoothness are both perceptually beneficial in 3D viewing.</p><p>In Fig. <ref type="figure">8</ref>, we show anaglyph images constructed from (Q, Q R ) image pairs for the ground truth depth d Q , and the estimated depths d using the proposed global approach and Make3D on both datasets. Neither conversion is flawless. However, the skew of the building and lamp post in the first row and the edge jaggedness on the building wall in the second row for images produced by the Make3D algorithm are visually disturbing <ref type="foot" target="#foot_0">1</ref> . Similarly, errors on the bulletin board in the office image (third row) and under the chest of drawers in the dining room image (bottom row) produced by Make3D cause visual discomfort when viewed with anaglyph glasses.</p><p>In terms of the computational complexity, a comparison of these algorithms is rather difficult. An extended variant of the proposed local algorithm, that includes the motion transformation f m , is currently in use by YouTube for automatic conversion of monoscopic videos to stereo. This implementation works on YouTube's distributed computing infrastructure and it would be unfair to compare its speed to our experiments with the other algorithms on a single PC. The global algorithm including CBF requires about 2-3 s for the complete LOOCV test on the Make3D dataset and 5-6s on the Kinect dataset,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Proposed global approach Make3D</p><p>Fig. <ref type="figure">8</ref>. Anaglyph images generated using the ground-truth depth and depths estimated by the proposed global and Make3D algorithms. In order to appreciate depth, these images should be viewed in color (e.g., in a PDF version of the paper) through red-blue or red-cyan anaglyph glasses.</p><p>implemented in C and performed on a 32-core 3.0 GHz Xeon E5 CPU running 12 threads. As for the other algorithms, we simply ran the provided code on the same computer. The Make3D algorithm required about 4h and 12h in the respective tests, whereas the Karsch et al. algorithm needed almost 9h and over 26h, respectively. This is not surprising since the Make3D algorithm uses an involved depth learning step 2 , whereas the Karsch et al. algorithm uses SIFT warping, which is computationally very intensive, followed by depth optimization. 2 We must note at this point that, due to Make3D's complexity, the depth learning step was performed on reduced-resolution Kinect images and depth fields (80 × 60) as opposed to full-resolution (640 × 480). Had we used fullresolution data, we would have to wait over 4 weeks for Make3D's output. We believe that depth learning at low resolution is acceptable if depth edges are aligned with photometric boundaries, because depth varies smoothly within objects and background. The estimated depth fields d were interpolated to full resolution prior to the right-image rendering.</p><p>In fact, in our very first paper where we proposed a learningbased method for 2D-to-3D conversion <ref type="bibr" target="#b7">[8]</ref> we did indeed use SIFT depth alignment prior to depth fusion. We quickly realized, however, that gains offered by SIFT warping are small for the computational effort required. For example, the additional step of estimating SIFT flow from a query and each of the kNN images, and the subsequent SIFTbased depth warping increased the run time from about 5-6s to 16h in a complete LOOCV test on the Kinect dataset while offering no increase in the average C value and a 0.01 increase in the median C value to C = 0.76 <ref type="bibr" target="#b8">[9]</ref>. Clearly, warping the depths associated with the k best-matched images is counterproductive yet costly for individual depths are often quite different from the query.</p><p>In addition to LOOCV tests on the Make3D dataset #1, where the test image may belong to the set of original 400 training images, we also applied the test used by . This is to be expected since LOOCV uses more training images. The performance of the local method appears to be only marginally affected. This can be attributed to the use of a fixed point mapping. Both Make3D and our global method with CBF experience a significant performance drop but Make3D continues to trail behind our method. The method of Karsch et al. appears to be more robust, even improving slightly in terms of the average C value, but it takes about 2 hours to execute while processing 12 images in parallel. In contrast, Make3D takes about 30mins and our global method with CBF takes about 1 second to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have proposed a new class of methods aimed at 2D-to-3D image conversion that are based on the radically different approach of learning from examples. One method that we proposed is based on learning a point mapping from local image attributes to scene-depth. The other method is based on globally estimating the entire depth field of a query directly from a repository of image + depth pairs using nearestneighbor-based regression. We have objectively validated our algorithms' performance against state-of-the-art algorithms. While the local method was outperformed by other algorithms, it is extremely fast as it is, basically, based on table lookup. However, our global method performed better than the state-of-the-art algorithms in terms of cumulative performance across two datasets and two testing methods, and has done so at a fraction of CPU time. Anaglyph images produced by our algorithms result in a comfortable 3D experience but are not completely void of distortions. Clearly, there is room for improvement in the future. With the continuously increasing amount of 3D data on-line and with the rapidly growing computing power in the cloud, the proposed framework seems a promising alternative to operator-assisted 2D-to-3D image and video conversion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of depth estimation from color, spatial location and motion. Black indicates smallest depth.</figDesc><graphic coords="3,125.99,54.17,360.14,117.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Fig. 2 .</head><label>12</label><figDesc>Fig. 2. (a) Color-depth transformation f c [h, v] obtained from hue h and value (intensity) v, and (b) location-depth transformation f l [x] obtained from pixel located at x = [x, y] T , both computed from dataset I of, mostly, outdoor scenes. Black indicates smallest depth. The hue above ranges from red on left through orange, green, cyan, blue and purple to red, again, on right. It is best to view this figure in a PDF version of the paper.</figDesc><graphic coords="4,110.03,169.97,124.58,94.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Block diagram of the overall algorithm; algorithmic details for each block are provided in the sections below.</figDesc><graphic coords="5,56.99,53.69,234.38,169.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. RGB image and depth field of two 2D queries (left column), and their four nearest neighbors (columns 2-5) retrieved using the Euclidean norm on the difference between histograms of gradients<ref type="bibr" target="#b2">[3]</ref>. All image + depth pairs are from the Make3D dataset #1 (see Section V).</figDesc><graphic coords="6,64.43,448.25,90.74,121.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. RGB image and depth field of two 2D queries (left column), and their four nearest neighbors (columns 2-5) retrieved using the Euclidean norm on the difference between histograms of gradients<ref type="bibr" target="#b2">[3]</ref>. All image + depth pairs are from the NYU Kinect dataset (see Section V).</figDesc><graphic coords="7,65.39,289.13,90.86,68.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Query images from Fig. 4 and depth fields: of the query, depth estimated by the local transformation method, depth estimated by the global transformation method (with CBF) and depth computed using the Make3D algorithm. Normalized depth cross-covariances (see equation (6) in Section V) are included under each estimated depth field. luminance discontinuities, the weight h σ e (Y [x]-Y [y]) is small and thus the contribution of d[y] to the output is small. However, when Y [y] is similar to Y [x] then h σ e (Y [x] -Y [y]) is relatively large and the contribution of d[y] to the output is larger. In essence, depth filtering (smoothing) is happening along (and not across) query edges.</figDesc><graphic coords="7,64.43,534.77,90.74,121.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Query images from Fig.5and depth fields: of the query, estimated depth by the global method after median-based fusion and after the same fusion and CBF, and depth computed using the Make3D algorithm. Normalized depth cross-covariances (see equation<ref type="bibr" target="#b5">(6)</ref> in Section V) are included under each estimated depth field.</figDesc><graphic coords="8,64.43,148.01,90.74,68.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I AVERAGE</head><label>I</label><figDesc>AND MEDIAN NORMALIZED CROSS-COVARIANCE C COMPUTED ACROSS ALL IMAGES IN THE MAKE3D DATASET #1 IN AN LOOCV TEST (SEE TEXT FOR DETAILS) FOR THE PROPOSED METHODS,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AVERAGE</head><label>II</label><figDesc>AND MEDIAN NORMALIZED CROSS-COVARIANCE C COMPUTED ACROSS ALL IMAGES IN THE NYU KINECT DATASET IN AN LOOCV TEST (SEE TEXT FOR DETAILS) FOR THE PROPOSED METHODS,MAKE3D<ref type="bibr" target="#b13">[14]</ref>, AND A METHOD BY KARSCH et al.<ref type="bibr" target="#b6">[7]</ref> the database on the Make3D algorithm was trained (400 training images). Not surprisingly, the same algorithm applied to the Kinect dataset fairs rather poorly but, as we already mentioned, re-training was not possible. The Karsch et al. method does not perform as well as the global algorithm on either dataset. Finally, the local method achieves a consistent but low performance which is not surprising given its simplicity.Examples of computed depth fields for various algorithms are shown in Figs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AVERAGE</head><label>III</label><figDesc>AND MEDIAN NORMALIZED CROSS-COVARIANCE C COMPUTED ACROSS 134 TEST IMAGES IN THE MAKE3D DATASET #1 USING THE PROPOSED METHODS, MAKE3D ALGORITHM [14], AND A METHOD BY KARSCH et al. [7]. ALL METHODS WERE TRAINED ON 400 IMAGES OF THE SAME DATASET AND EXCLUDED THE TEST IMAGES Saxena et al. [14]. Namely, the 534 images of this dataset were divided into 134 test images and 400 training images (on which the Make3D algorithm was trained). We selected our test image from the test set and used the training set to find the k nearest neighbors. Table III shows the numerical results obtained. Comparing with the results of Table I we can observe, roughly, a global performance drop across all methods</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It is interesting to note that the anaglyph image based on the ground-truth depth in the second row of Fig.8exhibits significant distortions on the vertical edges around the doors and the windows in the mid-right. This ground-truth depth was captured at low-resolution (significantly different horizontally and vertically) by a laser-range finder (Make3D dataset #1). Such distortions are largely absent from the anaglyph image derived from the global method's depth (center column). The underlying depth seems locally more accurate, than the laser-scanned depth, likely due to resolution.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to acknowledge Geoffrey Brown for the implementation of the very first, SIFT-based, variant of the 2D-to-3D image conversion method reported here.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the U.S. National Science Foundation (NSF) under Award CCF-0905541.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A 2D to 3D video and image conversion technique based on a bilateral filter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Angot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2010-02">Feb. 2010</date>
			<biblScope unit="volume">7526</biblScope>
			<biblScope unit="page">75260</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast bilateral filtering for the display of high-dynamic-range images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auto-directed video stabilization with robust L1 optimal camera paths</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-automatic stereo extraction from video footage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guttmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="775" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic 2D-to-3D image conversion using 3D examples from the Internet</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
			<biblScope unit="volume">8288</biblScope>
			<biblScope unit="page">82880</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2D-to-3D image conversion by learning depth from examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video stereolization: Combining motion analysis with user interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualizat. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1079" to="1088" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-automatic 2D to 3D image conversion using scale-space random walks and a graph cuts based depth prior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rzeszutek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Androutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int. Conf. Image Process</title>
		<meeting>18th IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Workshops</title>
		<meeting>Int. Conf. Comput. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth from defocus: A spatial domain approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Subbarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Surya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="294" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geometrically constrained structure from motion: Points on planes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Workshop 3D Struct. Multiple Images Large-Scale Environ</title>
		<meeting>Eur. Workshop 3D Struct. Multiple Images Large-Scale Environ</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image saliency: From intrinsic to extrinsic context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape-from-shading: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999-08">Aug. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html" />
		<title level="m">NYU Depth V1</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note>Make3D</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
