<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weighted locally linear embedding for dimension reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaozhang</forename><surname>Pan</surname></persName>
							<email>yaozhang.pan@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Social Robotics Lab</orgName>
								<orgName type="institution" key="instit1">Interactive Digital Media Institute</orgName>
								<orgName type="institution" key="instit2">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuzhi</forename><surname>Sam Ge</surname></persName>
							<email>samge@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Social Robotics Lab</orgName>
								<orgName type="institution" key="instit1">Interactive Digital Media Institute</orgName>
								<orgName type="institution" key="instit2">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abdullah</forename><surname>Al Mamun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Social Robotics Lab</orgName>
								<orgName type="institution" key="instit1">Interactive Digital Media Institute</orgName>
								<orgName type="institution" key="instit2">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weighted locally linear embedding for dimension reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">608D555BE3FB98D1EF25E81F482DE779</idno>
					<idno type="DOI">10.1016/j.patcog.2008.08.024</idno>
					<note type="submission">Received 3 August 2007 Received in revised form 27 May 2008 Accepted 20 August 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Nonlinear dimensionality reduction Manifold learning Feature extraction Locally linear embedding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The low-dimensional representation of high-dimensional data and the concise description of its intrinsic structures are central problems in data analysis. In this paper, an unsupervised learning algorithm called weighted locally linear embedding (WLLE) is presented to discover the intrinsic structures of data, such as neighborhood relationships, global distributions and clustering. The WLLE algorithm is motivated by locally linear embedding (LLE) algorithm and cam weighted distance, a novel distance measure which usually gives a deflective cam contours for equal-distance contour in classification for an improved classification. It is a major advantage of the WLLE to optimize the process of intrinsic structure discovery by avoiding unreasonable neighbor searching, and at the same time, allow the discovery adapt to the characteristics of input data set. Furthermore, the algorithm discovers intrinsic structures which can be used to compute manipulative embedding for potential classification and recognition purposes, thus can work as a feature extraction algorithm. Simulation studies demonstrate that the WLLE can give better results in manifold learning and dimension reduction than LLE and neighborhood linear embedding (NLE), and is more robust to parameter changes. Experiments on face images data sets and comparison to other famous face recognition methods such as kernel-PCA (KPCA) and kernel direct discriminant analysis (KDDA) are done to show the potential of WLLE for real world problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many problems in machine learning begin with the preprocessing of raw high-dimensional signals, such as face images, speech spectrograms, EEG and ECG signals for medical diagnose. For convenience of subsequent operations such as classification <ref type="bibr" target="#b0">[1]</ref>, image processing <ref type="bibr" target="#b1">[2]</ref> and outlier detection <ref type="bibr" target="#b2">[3]</ref>, the preprocessing should extracts and highlights the inherent properties hidden in the high-dimensional observations and represents the intrinsic structures in a more compact and efficient way. However, the representations must be learned or discovered automatically in the case that no prior knowledge about the data is known. Automatic methods which discover hidden structures from the statistical regularities of large data sets can be studied in the general framework of unsupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>The strategies and methodologies to solve this problem can be categorized into linear and nonlinear methods. Principal component analysis (PCA) is a linear projection method that emphasizes on the features of observations with large variability that can be discovered using cross correlation <ref type="bibr" target="#b5">[6]</ref>. Classical multidimensional scaling (MDS) 0031-3203/$ -see front matter Â© 2008 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2008.08.024 seeks to preserve pairwise distance and simple formations of observations such as triangle <ref type="bibr" target="#b6">[7]</ref>. Due to the nonlinear relationships of high-dimensional observations in nature, several geometry-oriented methods are introduced by mapping high-dimensional inputs into low-dimensional embeddings nonlinearly such as ISOMAP, by which the geodesic relationship among the input data and the calculated low-dimensional embeddings remains consistent <ref type="bibr" target="#b7">[8]</ref>.</p><p>Linear techniques based on PCA or linear discriminant analysis (LDA) cannot provide reliable and robust solutions to nonlinearity distribution such as face patterns. As a result, nonlinear techniques such as kernel-PCA (KPCA) <ref type="bibr" target="#b8">[9]</ref>, generalized discriminant analysis (GDA), and kernel direct discriminant analysis (KDDA) <ref type="bibr" target="#b1">[2]</ref> was proposed to solve the problem of nonlinearity in data distribution. The KPCA was proposed in Ref. <ref type="bibr" target="#b8">[9]</ref>, which is as simple as standard PCA because no nonlinear optimization is involved. However, it may have trouble when very large number of observations is needed to be processed. KDDA is proposed and used to do face recognition to deal with the nonlinearity and small sample size of the face pattern's distribution and data sets <ref type="bibr" target="#b1">[2]</ref>. All of these kernel-based algorithms have a further problem of large training sample, overfitting, and finding suitable kernel functions for each specific data set.</p><p>Local linear embedding (LLE) is an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserved embeddings of high-dimensional inputs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Unlike clustering methods for local dimension reduction <ref type="bibr" target="#b9">[10]</ref>, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima <ref type="bibr" target="#b3">[4]</ref>. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds. It eliminates the need to estimate pairwise distances between widely separated data points and recovers global nonlinear structure from locally linear fits <ref type="bibr" target="#b4">[5]</ref>. It is suitable to solve the problem of dimension reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computing <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>However, LLE algorithm, as well as many other machine learning and pattern recognition algorithms, such as nearest neighbor classifier <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, radial basis function networks <ref type="bibr" target="#b15">[16]</ref>, support vector machines (SVMs) for classification <ref type="bibr" target="#b16">[17]</ref>, k-means algorithm for clustering <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, actually rely on a distance metric. As a direct result, the performance of the method depends critically on the choice of appropriate metric. Many early works have been carried out to relax this restriction, such as optimal metric for k-nearest neighbor density estimation <ref type="bibr" target="#b14">[15]</ref>, optimal local metric <ref type="bibr" target="#b13">[14]</ref> and optimal global metric <ref type="bibr" target="#b19">[20]</ref>. More recent research along this line continued to develop various locally adaptive metrics <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> for metric learning-based algorithms; in Ref. <ref type="bibr" target="#b25">[26]</ref>, how to find a better distance measure for similarity estimation was discussed, and a group of new distance measures are derived and proved to be more efficient in feature extraction than traditional Euclidean and Manhattan distances; in Ref. <ref type="bibr" target="#b26">[27]</ref>, the authors extended the LLE procedure with a weighting scheme by associating weights with face images to represent their probability of occurrence, and obtained better performance on face recognition.</p><p>The existing methods handle this problem only from the aspect of the query point. They analyze the measurement space emanating from the query point, and study how the distance measure should be changed or weighted. These approaches only examine a small local region surrounding the query sample, as such the most of the inter-prototype information is neglected. To solve this problem, cam weighted distance for improving nearest neighbor finding is developed in Ref. <ref type="bibr" target="#b27">[28]</ref>. The "cam weighted distance" was so named because that it usually gives a deflective cam contours for equal-distance contour in classification as mentioned and shown in Ref. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr">Fig. 1]</ref>. This method optimizes the distance measure with respect to the analysis of the inter-prototype relations. Since the prototypes are not isolated instances, the nearby prototypes actively affect the confidence level of the information provided by the prototype being considered. As a result, to improve distance measure globally, we should consider both variances with its own orientation and discrimination with respect to its different surroundings of each prototype.</p><p>In this paper, we proposed weighted locally linear embedding (WLLE) by modifying the LLE algorithm based on the weighted distance measurement to improve the dimension reduction and internal feature extraction performance especially for the deformed distributed data. In the case that data distribution is deformed because of the attraction, repulsion, strengthening effect and weakening effect each sample point receives from its neighbors, Euclidean distance for measuring the similarity will lead to performance decline. By taking into account the distribution information surrounding each prototype to optimize the distance measure, we can improve the neighbor finding procedure of LLE algorithm and avoid the redundancy and overlapping due to improper neighbor selection. Better neighbors selection will make the dimension reduced representations more accurate to represent the internal feature, characteristic, and structure of the high-dimensional data.</p><p>The main contributions of the paper are as follows:</p><p>(i) a novel weighted distance measurement for neighborhood searching is adopted to solve the problem of neighbors redundancy and overlapping when the samples are not welldistributed; (ii) a modified LLE based on the weighted distance measurement called WLLE is presented to give better performance of internal feature extraction; (iii) the problem that the LLE cannot give faithful embeddings for a kind of difficult data set, in which some data are noisy, sparse or weakly connected, is solved by WLLE; and (iv) the WLLE algorithm is tested using several manifolds and images, the results of simulations demonstrate that the WLLE not only has better performance in manifold learning, but also is more robust to parameter changes.</p><p>The rest of the paper is organized as follows. The main features of LLE algorithm and modified NLE algorithm are briefly introduced in Section 2. Then weighted distance measurement is introduced in Section 3 for modification of LLE in the following sections. In Section 4, weighted distance measurement is adopted to form a new nonlinear dimension reduction algorithm, WLLE. Simulation studies on both artificial manifold data sets and real-world data sets are given in Section 5. Then, the motivation and origin of this work and the main advantages of WLLE are discussed in Section 6. Section 7 concludes this work at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LLE and NLE</head><p>For ease of the forthcoming discussion, we firstly introduce the main features of the LLE algorithm. It is an unsupervised learning algorithm that attempts to map high-dimensional data to lowdimensional, neighborhood-preserved embeddings. It is based on the simple geometric intuitions: (i) each high-dimensional data point and its neighbors lie on or close to a locally linear patch of a manifold <ref type="bibr" target="#b3">[4]</ref>, and (ii) the local geometric characterization in original data space is unchanged in the output data space. From a mathematic point of view, the problem LLE attempts to resolve is: given a set X = [x 1 , x 2 , ... , x N ], where x i (i = 1, ... , N) is ith node on a high-dimensional manifold embedded in R D , i.e., x i â R D , and then find a set Y = [y 1 , y 2 , ... , y N ] in R d , where d&gt;D such that the intrinsic structure in X can be represented by that of Y.</p><p>The neighbor finding process of LLE is usually carried out using the grouping technique such as k-nearest neighbors (KNN) or choosing neighbors within a ball of fixed radius ( -neighborhoods) based on Euclidean distance for each data point in the given data set. These neighbors are then used to reconstruct the given point by linear coefficients. The KNN method is widely used due to its simplicity and ease of implementation. However, due to the complexity, nonlinearity and variety of high-dimensional input samples, the K is difficult to choose properly to obtain a acceptable level of redundancy and overlapping. A small K leads to possible isolation of nodes. For the extreme case where K = 0, all nodes are totally separated and no intrinsic structure can be observed. A large K increases redundancy and overlapping. For instance, if K = N -1, each individual node is directly connected to the rest of the nodes such that all nodes belongs to one cluster, no matter what the exact number of clusters is.</p><p>Therefore, the choice of K affects the tradeoff between the redundancy present in the structure and the number of isolated nodes. Thus, an adaptive scheme to select K is more appropriate for finding neighbors. Based on this idea, a modified algorithm named NLE was proposed <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. It is an adaptive scheme that select neighbors according to the inherent properties of the input data substructures. By defining d ij the Euclidean distance from mode x j to x i and S i the data set containing all the neighbor of x i , the neighbor finding procedure of NLE for a given point x i can be summarized as follows: This modified adaptive neighbor searching algorithm not only solves the problem of redundancy and overlapping but also avoids the trial and error operation, which is a obvious shortcoming of KNN algorithm. However, according to NLEs neighborhood selection criterion, the number of neighbor selected to be used is usually small. Generally speaking, for a data point x, the NLE determine the nearest neighbor x 1 to be the first neighbor point; from the second nearest neighbor, say, x 2 , it will be considered a neighbor of x only when the distance between x 2 and x 1 is larger than the distance between x 2 and x. As a result, it is not surprising that the number of neighbors selected by NLE algorithm is usually much smaller than other algorithms. For example, according to the experiment on two peaks data sample, the average number of neighbor for 1000 samples chosen by NLE is only 3.74 <ref type="bibr" target="#b30">[31]</ref>. This may result from the strict neighbor selection rules of NLE. Besides this experiment, we have also do many other simulations and find the neighbor size by NLE algorithm is smaller than other ones. In that case, the reconstruction information may not be enough for data reconstruction.</p><formula xml:id="formula_0">(i) If d ij = min{d im },</formula><p>After carefully considering the LLE and NLEs neighbor selection criterion, we propose a new algorithm by using weighted distance measurement in neighbor searching. The new algorithm can solve the problem of redundancy in LLE and avoid NLEs problem that no enough data are chosen as neighbors at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distribution deformation and weighted distance</head><p>In the data manipulation like nearest neighbor searching, each datum can be regarded as the center of a probability distribution and the similarity of its neighbors to the datum can be measured by Euclidean distance with the assumption that samples are welldistributed. However, because of the attraction, repulsion, strengthening effect and weakening effect between data, the standard normal distributions will be greatly deformed. Obviously, neglecting such a deformation and still using Euclidean distance to measure the similarity will lead to performance decline. As mentioned in Ref. <ref type="bibr" target="#b3">[4]</ref>, the data set should be sufficient and well-sampled, otherwise the performance of LLE algorithm will not be good enough. For example, as illustrated in Fig. <ref type="figure">1</ref>, the samples are not well-distributed, data density Fig. <ref type="figure">1</ref>. Select nearest neighbors using -neighborhoods algorithm by Euclidean distance (solid line) and weighted distance (dash line). changes sharply within a small area, the query point is marked by a cross, and its neighbors marked by circles. We use -neighborhoods algorithm to finding nearest neighbors of the query point from its neighbors. For this deformed distribution data set, -neighborhoods method based on standard Euclidean distance measurement selects neighbors from a single direction, and these neighbors are closely gathered. Obviously, if we use these chosen neighbors to reconstruct the query point, the information captured in this direction will have serious redundancy; at the same time, no information from other directions are reserved for query point reconstruction. These chosen neighbors cannot represent and reconstruct the query point well, most internal features and intrinsic structure will be lost after dimension reduction by LLE.</p><p>To solve this problem, we introduce the weighted distance measurement motivated by Ref. <ref type="bibr" target="#b27">[28]</ref>. The main idea of the weighted distance measurement is giving a different but appropriate distance scale to each prototype to make the distance measure more reasonable for representing the global distribution of the data set. Fig. <ref type="figure">1</ref> shows the advantages of this scaled adaptive distance measurement. The modified -neighborhoods method based on weighted distance measurement select neighbors more reasonable than the one based on standard Euclidean distance by giving the prototype data with high density a smaller weight scaling while those with low density a larger weight scaling. Thus, the previous redundancy and deficiency problem can be solved.</p><p>As defined in Ref. <ref type="bibr" target="#b27">[28]</ref>, a simple yet effective transformation to simulate the possible deformation of data distribution is constructed.</p><formula xml:id="formula_1">Definition 1 (Deformed distribution). Consider a d-dimensional ran- dom vector Y = (Y 1 , Y 2 , ... , Y d ) T that takes a standard d-dimensional normal distribution N(0, I), that is, it has a probability density function f (y) = 1 (2 ) d/2</formula><p>e -1/2y T y (1)</p><p>Let a random vector X be defined by the transformation</p><formula xml:id="formula_2">X = a + b Y T Y Y (2)</formula><p>where Y denotes the original well-distributed data set, a &gt; b 0 are the parameters reflect overall scale and orientation of distribution, is a normalized vector denoting the deformation orientation, Y = Y T Y, and X represent the deformed distribution with parameters a and b in the direction , denoted as X = D d (a, b, ) <ref type="bibr" target="#b27">[28]</ref>.</p><p>According to the definition, a deformed distribution biases towards a specific direction, which makes it an eccentric distribution. Thus, the assumption that data are well-distributed is dissatisfied for Euclidean distance to describe the similarity between data points. Instead, an inverse transformation Y =X/(a+b cos ) is used to restore the deformation, and then we can measure the distance normally since the transformed distribution is not eccentric anymore. This is the main idea of the weighted distance measurement. This weighted distance redresses the deformation problem and should be more reasonable to evaluate the similarity for data set that is not well-distributed.</p><formula xml:id="formula_3">Definition 2 (Weighted distance). Assume that x 0 â R d is the center of a deformed distribution D d (a, b, ). The weighted distance from a point x â R d to x 0 is defined to be Dist(x 0 , x) = x -x 0 a + b (x -x 0 ) T x -x 0 (3) or Dist(x 0 , x) = x -x 0 /(a + b cos ) ( 4 )</formula><p>where is the angle between vectors xx 0 and , and 1/(a + b cos ) is the weight of the distance from x to x 0 <ref type="bibr" target="#b27">[28]</ref>.</p><p>One disadvantage of the weighted distance measurement is just a weighted distance, but not a metric, since Dist(x 0 , x) may not equal to Dist(x, x 0 ) under the definition of weighted distance. In fact, it has been discussed in Ref. <ref type="bibr" target="#b31">[32]</ref> that non-Euclidean or non-metric measures can be informative in statistical learning algorithms.</p><p>To facilitate parameter estimation for weighted distance, we first present some properties.</p><formula xml:id="formula_4">Theorem 1. If a random vector X = D d (a, b, ), then E(X) = c 1 b and E( x ) = c 2 a</formula><p>, where c 1 and c 2 are constants. <ref type="formula">5</ref>)</p><formula xml:id="formula_5">c 1 = â 2 ((d + 1)/2) (d/2)d (</formula><formula xml:id="formula_6">c 2 = â 2 ((d + 1)/2) (d/2) (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where d is the dimensionality of the random vector X; is the Gamma function</p><formula xml:id="formula_8">(k) = â 0 t k-1 e -t dt (k &gt; 0) [28].</formula><p>The expected value (or expectation) of a random variable is the sum of the probability of each possible outcome of the experiment multiplied by its value. Thus, for the random vector X, which has a deformed distribution D d (a, b, ), X = D d (a, b, ), we can calculate its expectation using the origin point of this distribution, x i , and its k-nearest neighbors, X i = {x i1 , x i2 , ... , x ik }, approximately.</p><p>First, we convert X i to a set of vectors</p><formula xml:id="formula_9">V i = {v i1 , v i2 , ... , v ik }, where v ij = x ij -x i , j = 1, 2, . . . , k. Then, we calculate the mean of v ij , Äi = k j=1 v ij /k (7)</formula><p>to estimate E(X), and the mean of</p><formula xml:id="formula_10">v ij , Li = k j=1 v ij /k (8)</formula><p>to estimate E( X ). As is a normalized vector denoting the deformation orientation of the deformed distribution, it can be calculated as</p><formula xml:id="formula_11">Ë i = Äi Äi<label>(9)</label></formula><p>Since is a normalized vector with unity gain, according to Theorem 1, E(X) = c 1 b and E( x ) = c 2 a, and with the approximation of expectation, E(X) and E( X ), we can easily obtain the parameters a, b:</p><formula xml:id="formula_12">Ã¢i = Li c 2 (10) bi = Äi c 1 (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Weighted locally linear embedding</head><p>The weighted distance can measure the similarity more reasonable for the deformed-distributed data set than standard Euclidean distance and is suitable for many distance based methods. Accordingly, in this paper, we propose a novel dimension reduction algorithm, WLLE which use weighted distance measurement to improve the dimension reduction and internal feature extraction performance especially for the deformed distributed data.</p><p>For data points X = {x 1 , x 2 , ... , x N } in the high-dimensional space R D , the goal of dimension reduction is to calculate a representative in low-dimensional space R d for the high-dimensional data, where d&gt;D.</p><p>We attempt to express data point number x i as a linear combination of its k-nearest neighbors x j , j = 1, 2, . . . , k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xi =</head><formula xml:id="formula_13">jâ i w ij x j (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where i is the neighborhood of sample x i . In the original algorithm, standard Euclidean metric is used to select the nearest neighbors. However, in this work, we utilize the weighted distance measurement as in Section 3 in order to improve performance when the data set are deformed-distributed. The optimal weight matrix w ij for data reconstruction can be obtained by minimizing the approximation error const function</p><formula xml:id="formula_15">(W) = i x i - jâ i (w ij x j ) 2 (13) subject to the constraints j / â i â w ij = 0 (<label>14</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">jâ i w ij = 1<label>(15)</label></formula><p>where w i = [w i1 , ... , w ik ] are the weights connecting sample x i to its neighbors. The first constraint says that only data points in the neighborhood of data point i should be used in the reconstruction of xi , while the second constraint imposes invariance to translation.</p><p>To calculate the optimal weights, we first rewrite the approximation error cost function <ref type="bibr" target="#b12">(13)</ref> as</p><formula xml:id="formula_18">(W) = x i -xi = x i jâ i w ij - jâ i (w ij x j ) = jâ i w ij kâ i w ik (x i -x j ) T (x i -x k )<label>(16)</label></formula><p>By defining</p><formula xml:id="formula_19">C i (j, k) = (x i -x j ) T (x i -x k ) (<label>17</label></formula><formula xml:id="formula_20">)</formula><p>and applying a Lagrange multiplier i , the approximation error becomes</p><formula xml:id="formula_21">(W i ) = jâ i w ij kâ i w ik C i (j, k) + i â â â jâ i w ij -1 â â â <label>(18)</label></formula><p>The optimal weights are found by requiring the partial derivatives with respect to each weight w ii to be zero,</p><formula xml:id="formula_22">j (w i ) jw ij = kâ i w ik C i (j, k) + i = 0, âj â i (<label>19</label></formula><formula xml:id="formula_23">)</formula><p>The desired solution w i is found by simply solving the equations,</p><formula xml:id="formula_24">kâ i C i (j, k)w ik = 1 (20)</formula><p>and then rescale the weights so that they sum to one.</p><p>In unusual cases, it can arise that the matrix ( <ref type="formula" target="#formula_19">17</ref>) is singular or nearly singular. In this case, the least square problem for finding the weights does not have a unique solution. In order to guarantee numerical stability we regulate C by</p><formula xml:id="formula_25">C i (j, k) â C i (j, k) + r I (<label>21</label></formula><formula xml:id="formula_26">)</formula><p>where r &gt;trace(C) is a small constant to be defined as part of the algorithm, and I is an identical matrix. The final step of LLE is to compute a low-dimensional embedding based on the reconstruction weights w ij of the high-dimensional inputs x i . The high-dimensional data are mapped into the lowdimensional space R d by requiring reconstruction to work as well as possible. This leads to another minimization problem <ref type="bibr" target="#b28">[29]</ref>, the low-dimensional outputs y i , i = 1, 2, . . . , N are found by minimizing the cost function,</p><formula xml:id="formula_27">(Y) = i y i - jâ i w ij y j 2 (22)</formula><p>where Y = [y 1 , ... , y N ] consist of the data points embedded into the low-dimensional space. This minimization problem is not well-posed without further constraints. Zero mean and unity covariance is used in the LLE algorithm to make the problem well-posed. In other words, Y should obey the constraints</p><formula xml:id="formula_28">N i=1 y i = 0 (23) 1 N YY T = I (24)</formula><p>where the first constraint is to assure that coordinates y i can be translated by a constant displacement without affecting the cost, while the second constraint imposes unit covariance of the embedding vectors.</p><p>In matrix form, the cost function can be written as</p><formula xml:id="formula_29">(Y) = Tr[(Y -YW) T (Y -YW)] = Tr[(Y -YW)(Y -YW) T ] = Tr[Y(I -W)(I -W) T Y T ] = Tr[YMY T ] (<label>25</label></formula><formula xml:id="formula_30">)</formula><p>where the symmetric matrix</p><formula xml:id="formula_31">M = (I -W)(I -W) T (<label>26</label></formula><formula xml:id="formula_32">)</formula><p>The minimum of Eq. ( <ref type="formula" target="#formula_29">25</ref>) subject to the constraint of Eq. ( <ref type="formula">24</ref>) can be obtained by finding the d smallest eigenvectors of M. The minimal value of (Y) equals the sum of the eigenvalues of M. Notice that</p><formula xml:id="formula_33">M1 = (I -W)(I -W) T 1 = 0 (<label>27</label></formula><formula xml:id="formula_34">)</formula><p>due to the requirement jâ i w ij = 1. Therefore, the smallest eigenvalue is automatically zero with corresponding eigenvector 1, here 1 is a vector in which all elements are 1. Since the eigenvectors are mutually orthogonal discarding it fulfills the constraint of Eq. ( <ref type="formula">23</ref>).</p><p>To summarize, the d-dimensional embedding Y â R dÃN consists of eigenvector number 2, . . . , d + 1 as its rows.</p><p>The whole procedure of dimension reduction as well as the construction of weighted distance measurement are detailed in Algorithm 1.</p><p>Algorithm 1. Weighted locally linear embedding procedure Phase 1: Construct Weighted Distance Given a raw high-dimensional data set D = {x i }, i = 1, 2, . . . , N, x i â R D , and a parameter k w , for an arbitrary datum x i â D, 1: Find k w -nearest neighbors X i = {x i1 , x i2 , ... , x ik w }, X i â D by compare the Euclidean distances between all neighbor points and the query point. 2: Obtain V i with its elements to be calculated as v ij = x ij -</p><p>x i , j = 1, . . . , k w . 3: Calculate Äi and Li , according to Eqs. ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>). 4: Estimate a i , b i , i by using Äi and Li , according to Eqs. <ref type="bibr" target="#b9">(10)</ref>, <ref type="bibr" target="#b10">(11)</ref> and ( <ref type="formula" target="#formula_11">9</ref>). Phase 2: Search Neighborhood For an arbitrary datum x i â D, i = 1, 2, . . . , N, find k-nearest neighbors based on the weighted distance 1:Calculate the weighted distance from x i to âx j â D, j i according to Eq. ( <ref type="formula">3</ref>). 2: Find the k-nearest neighbor X j = {x j1 , x j2 , ... , x jk }, X j â D, which satisfy</p><formula xml:id="formula_35">Dist(x j , x i ) &lt; Dist(x k , x i )<label>(28)</label></formula><p>for âx j â X j , âx i â D and âx k â D / â X j . Phase 3: Calculate Optimal Reconstruction Weights 1: Compute local covariance matrix according to Eq. ( <ref type="formula" target="#formula_19">17</ref>). 2: Regulate the local covariance matrix according to Eq. ( <ref type="formula" target="#formula_25">21</ref>).</p><p>3: Compute the reconstruction weights according to Eq. ( <ref type="formula">20</ref>). Phase 4: Compute Low-Dimensional Embedding 1: Construct a symmetric N Ã N matrix according to Eq. ( <ref type="formula" target="#formula_31">26</ref>). 2: Calculate eigenvalues and eigenvectors of the symmetric matrix (26). 3: Obtain low-dimensional embedding using bottom d + 1 eigenvectors (according to smallest d + 1 eigenvalues) of matrix <ref type="bibr" target="#b25">(26)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental evaluation</head><p>To evaluate the dimension reduction and feature extraction effect of the WLLE, the results of several sets of experiments are presented in this section. The WLLE algorithm is tested on artificial manifold data sets and compared to other manifold learning algorithms such as LLE, NLE, ISOMAP and Laplacian Eigenmaps. Among these artificial manifold data sets, the Swiss roll and Toroidal helix are used to test the ability to unfold the uniformly distributed manifolds; Gaussian distribution and punched sphere are used to test the ability to unfold the no-uniformly distributed manifolds; the 3D clusters is used to test the clustering ability for all the dimension reduction algorithms.</p><p>Two real-world data sets, different subjects' faces and different poses of a face, are used to demonstrate the practical value of the algorithm we proposed. LLE, NLE, PCA, KPCA and KDDA are also applied to these real-world data sets to do comparison with WLLE. The first experiment is using a subset of the UMIST face data <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, face images of five different individuals, to compare the feature extraction performance of all the algorithms for later manipulation of classification. The second experiment is using a data set which contains 698 images of different poses of a face to compare the manifold learning performance of high-dimensional data set for all the algorithms.</p><p>Table <ref type="table" target="#tab_1">1</ref> displays all the data sets that have been used in the experiments and briefly summaries their major characteristics, such as the number of samples, the number of neighbors, the dimension of the data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on artificial manifold</head><p>In this section, we present the experimental results of WLLE tested on several standard manifolds, and compare WLLE we proposed to other dimension reduction methods such as LLE, NLE, ISOMAP and Laplacian Eigenmaps.</p><p>For comparison of the embedding property, we have conducted all the manifolds embedding with the three algorithms, LLE, NLE and WLLE. For each data set, every algorithm is used to obtain a 2D embedding. Figs. <ref type="figure">2-</ref> Swiss roll is a randomly sampled plane is rolled up into a spiral. Fig. <ref type="figure">2</ref> shows the sampled Swiss roll and the embedding results for it by the five algorithms, and we can see that the embedding effects of the five algorithms are quite different. LLE and ISOMAP unrolls the 3D data set into a plane, we can see the neighborhood relationships of LLE and ISOMAP are preserved well from the color coding, and the shape of ISOMAP's embedding is smoother than that of LLE. Both NLE and WLLE unroll the original data set to a 2D roll, while the shape of WLLE is more regular. According to the color, the neighborhood relationships in 3D are preserved in the lower dimension embeddings of these methods. The results can be viewed from different aspects.  On one side, from the "manifold embedding" point of view, the goal of manifold embedding is to find a Euclidean representation of the original data points, and the Isomap and LLE algorithms yield better embedding results than the NLE and WLLE. On the other side, from "feature extraction" point of view, the embedding results given by the NLE and WLLE keep both the local neighborhood relationship among the data and the global shape and distribution of the original data set, which means more intrinsic features have been reserved. The result of Laplacian Eigenmaps is an ark line which makes less sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample</head><p>Toroidal helix is a one-dimension curve coiled around a helix. There is a small amount of noise in the sampling along the helix. The dimension reduction method should unravel the coil into a circle. From Fig. <ref type="figure">3</ref>, we can see that the LLE algorithm maps the 3D Toroidal helix into 2D circle in a shape of triangular. ISOMAP and Laplacian Eigenmaps give a 2D embedding of a perfectly regular circle, which means it uncoiled the Toroidal helix string. Both WLLE and NLE can embed the original 3D helix to a flower-like shape circle, which means more properties of the original data set are preserved in the 2D embedding. Further more, the result of WLLE has a very regular shape, while the NLE gives a deformed shape result. The data set of Gaussian distribution is drawn from a Gaussian distribution, a good nonlinear dimensionality reduction method should form concentric circles. In Fig. <ref type="figure" target="#fig_1">4</ref>, from the top right and bottom right, we can see both WLLE, LLE and ISOMAP algorithm give a good concentric circle 2D embedding of the 3D Gaussian distribution, but NLE and Laplacian Eigenmaps cannot correctly embed the density property of the original data set and gives deformed shape of circles.</p><p>The punched sphere is the bottom 3 4 of a sphere which is sampled no-uniformly. The sampling is densest along the top rim and sparsest on the bottom of the sphere. Its intrinsic structure should be 2D concentric circles. From Fig. <ref type="figure" target="#fig_2">5</ref>, the LLE algorithm has some problem to correctly reconstruct the original data set in 2D embedding; the NLE algorithm gives a plane with correct distribution but the shape is not circle; the WLLE and ISOMAP algorithm give better embeddings as an exact concentric circles preserve exactly the density information of the original data set: densest along the top rim with red color and sparsest on the bottom of the sphere with blue color. The embedding given by Laplacian Eigenmaps seems the most correct 2D embedding for the 3D punched sphere. It unfold the punched sphere exactly in both density of distribution and order of colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Clustering</head><p>In this clusters data set, random 3D points are assigned to tight non-overlapping data clusters. Since most nonlinear dimension reduction techniques require a connected data set, the clusters are randomly connected with blue 1D line segments. A good nonlinear dimension reduction technique should preserve clusters, as shown by the color groupings. In Fig. <ref type="figure" target="#fig_3">6</ref>, a three-clusters data set is shown at the top left, the embedding result of this data set by WLLE, NLE and LLE is shown at top right, bottom left and bottom right, respectively. We can see that the WLLE embeds the 3D clusters faithfully to 2D clusters, so does the NLE. The result of ISOMAP is also meaningful and easy to identify. However, the result of LLE is not good enough since two clusters of the three in original data set shrink to one point or even disappear in its 2D embedding, which cause problem to distinguish the clusters and the connection line between clusters. The result of Laplacian Eigenmaps may have the same problem as LLE.  Besides the embedding performance, computational cost is also a critical issue to evaluate an algorithm. To summarize the computational cost for all these artificial manifold data sets, Table <ref type="table" target="#tab_3">2</ref> shows the computational time (second) of each algorithm for all the artificial manifold data sets. From the table, WLLE has higher computational cost than LLE and Laplacian Eigenmaps, but lower computational cost than NLE and ISOMAP. All the computation are done in a computer with Pentium 4 cpu 2.8 GHz, 1 GB of ram to assure the same environment of computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Face images</head><p>Face recognition is the one of the most popular research topic in pattern recognition during this decade <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. It can be widely used in entertainment, information security, intelligent robotics and so on. Recently, great development has been done by researchers on both algorithm and system. A critical part in face recognition is the dimension reduction algorithm for feature extraction.</p><p>In this area, global feature extraction algorithm such as PCA, LDA and all the methods based on combination of this two gave many good results in applications on facial recognition. Later, as a nonlinear extension of PCA, KPCA <ref type="bibr" target="#b8">[9]</ref> has shown great advantages on data representation for clustering and classification of complicated facial data set. Based on the very observation that null subspace contains useful information for clustering, in Ref. <ref type="bibr" target="#b1">[2]</ref>, Lu et al. proposed KDDA, which is combination of KPCA and direct linear discriminant analysis (DLDA). Another combination of LDA and KPCA, called complete kernel Fisher discriminant (CKFD), has been proposed in Ref. <ref type="bibr" target="#b37">[38]</ref>. All these kernel based methods have a major disadvantage in that the selection of kernel function and its parameters is usually made by trial and error or based on experience, which greatly weaken the practical value of these methods. Moreover, the final projections are related to all the training samples, so that the requirements for training samples are usually strict.  Compare to these kernel based methods, LLE has its own advantages because of unsupervised property. On one hand, it do not need training samples, which is especially helpful for small sample size of the face pattern's distribution. On the other hand, it only has one simple parameter, K number of neighbors selected, to be chosen, which make it easy to be applied. However, the performance of original LLE will decline when the data distribution is not well or uniformed distributed. Another problem is that the algorithm is not robust to parameter changes, which will be discussed in Section 6. To combat these problems, we proposed WLLE which may have better performance for complicated data set such as face images.</p><formula xml:id="formula_36">0.2 PCA -2 -1 0 1 2 -3 -2 -1 0 1 2 3 WLLE -3 -2 -1 0 1 2 -2 -1 0 1 2 3 NLE -2 -1 0 1 -2 -1<label>0</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Classification of different faces</head><p>To demonstrate the face recognition performance of WLLE and compare to other famous methods for face recognition, in this section, we utilize the UMIST face database <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> for experiment, which consists of 564 images of 20 people in PGM format, approximately 220 Ã 220 pixels in 256 shades of grey. Each covering a range of poses from profile to frontal views. Subjects cover a range of race/sex/appearance. The original 220 Ã 220 PGM format face images were cropped to 112 Ã 92 images, a standardized image size commonly used in face recognition experiment. In our experiment, we extract a typical subset of the UMIST face database, which contains face images of five different individuals, each individual has 20 face images covering range from profile to frontal views. As such, the subset we used in this experiment are 100 samples with dimensionality of 10 304 in five classes (different individuals). Six types of low-dimensional representations are produced from the face images subset by using different feature extraction algorithms, PCA, KPCA, KDDA, WLLE, LLE and NLE. For PCA, KPCA and KDDA, all of the face images in subset are used in both the training procedure to generate subspaces and the testing procedure to project them onto the generated subspaces. For each image, its projections in the first two most important features bases are visualized in the first row of Fig. <ref type="figure">7</ref>. For WLLE, LLE, and NLE, the high-dimensional face image data are mapped into 2D embeddings, which are shown in the second row of Fig. <ref type="figure">7</ref>.</p><p>The low-dimensional representations produced by the algorithms are quite different. Among them, the KDDA-based result and WLLEbased result showed better clustering property, but the other four algorithms result in some overlapping between different classes of the face data, which may make them non-separable. Especially, the result of KDDA is fairly linear separable, which may result from its separability criteria-based algorithm. Unlike the diffuse shape of the five classes in result by KDDA, the result by WLLE gives a parallel shape of the five classes. Although no overlapping between different classes, the short distance between clusters indicates that the WLLEbased feature representation is less linear separable than the KDDAbased result. Overall, simply inspection of Fig. <ref type="figure">7</ref> indicates that the feature representations produced by KDDA and WLLE outperform, in view of separability, the ones produced by PCA, KPCA, LLE and NLE. This will later proved by feeding these 2D features obtained by six algorithms into a simple SVM classifier.</p><p>Since the objective of this experiment is to compare the performance of different feature extraction algorithms, we keep the parameters of SVM unchanged during all the experiments. The performance is evaluated using average error rate of eight runs for each algorithm, which obtained by dividing total number of  misclassifications by product of number of samples and number of runs.</p><p>Noted that the performance of kernel-based methods are greatly affected by what kernel function chosen and the parameter changes of the function, we use a RBF kernel function for both KPCA and KDDA, and record the error rate with different kernel parameter, the scale value 2 for RBF kernel. Fig. <ref type="figure" target="#fig_6">8</ref> shows the error rates and computational cost as functions of 2 within the rage from 1e2 to 1e7 for algorithms KPCA and KDDA. Either error rate or computational cost indicate that the KDDA algorithm outperform KPCA.</p><p>The only parameter for LLE-based methods is the number of neighbors, K. As such, we record the error rate with different K for LLE and WLLE algorithm. The NLE algorithm do not need choose parameter K. Fig. <ref type="figure" target="#fig_7">9</ref> shows the error rates and computational cost as functions of K within the range from 8 to 88 for algorithms WLLE and LLE. Although the computational cost of WLLE is higher than LLE, the error rate shows the classification performance of WLLE outperform that of LLE.</p><p>For comparison of the all the six algorithm, we use optimal parameter in the experiment shown in Table <ref type="table" target="#tab_4">3</ref>, which shows the optimal parameter ranges, average computational cost and average error rate in the optimal range. From Table <ref type="table" target="#tab_4">3</ref>, it can be easily observed that PCA is the most simple algorithm but the classification performance is not satisfactory. KDDA has low computational cost and excellent classification performance, and is the best algorithm for this face recognition problem. KPCA and NLE have high computational costs and average classification error rates. WLLE shows good performance for classification, but the computational cost is  relatively high compare to KDDA and LLE. Although the smallest error rates for WLLE and LLE are almost the same, one can see from Fig. <ref type="figure" target="#fig_7">9</ref> that WLLE gives the optimal performance for a much larger range of parameter than LLE, which means WLLE is more robust to parameter changes than LLE. This will be discussed in detail later in Sections 6.3 and 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Manifold learning of different poses of a face</head><p>In next simulation study, the feature extraction algorithms are used to find the coherent relationship among a set of face images <ref type="bibr" target="#b7">[8]</ref>. This data set contains N = 698 gray images at a resolution of 64 Ã 64, and are different poses from left side view through front view to right side view of the same face. The input datum x i of X is constructed by formatting the image pixel column by column from left to right and concatenation them to form the column vector.</p><p>The computed 2D embeddings by WLLE are shown in Fig. <ref type="figure" target="#fig_8">10</ref>, several face images are shown next to the corresponding embedding point. These embeddings form an arch-bridge shape. To a certain extent, it is identical to the motion trajectory of the faces. From the left end of the arch, through middle peak till the right end of the arch, the embeddings are corresponding to the left pose face, front face and right pose face. Although the face images are high-dimensional data, the 2D embeddings of the face images are related to meaningful attributes of the motion of the subject head in the images. Thus, if a new face image is given, we can compute its corresponding embedding and identify the face direction by finding its position in Fig. <ref type="figure" target="#fig_8">10</ref>.</p><p>For comparison, the same data are also processed by PCA, KPCA and KDDA, which are shown in Figs. <ref type="figure">11,</ref><ref type="figure"></ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comments to the experiment result</head><p>The WLLE method improved LLE method by using cam weighted distance measurement in neighbor selection phase to avoid redundant and insufficient neighbor selections. In this section, a number of numerical experiments have been done to fully demonstrate the properties of the proposed WLLE algorithm. Its main advantages and disadvantages can be summarized as follows.</p><p>(i) WLLE has good performance for both uniform distribution and non-uniform distribution. Especially for non-uniform distribution as in Figs. <ref type="figure" target="#fig_1">4</ref> and<ref type="figure" target="#fig_2">5</ref>, WLLE has performance as good as ISOMAP, but its computational cost is much lower than ISOMAP as shown in Table <ref type="table" target="#tab_3">2</ref>; (ii) WLLE is relatively robust to parameter changes as shown in Fig. <ref type="figure" target="#fig_7">9</ref>. This property will be discussed in detail later in Sections 6.3 and 6.4; (iii) WLLE is helpful in both human face recognition and facial poses identification as shown in Figs. 7 and 10; (iv) compare to KDDA, WLLE cause much higher computational cost but its performance is not better, as shown in Table <ref type="table" target="#tab_4">3</ref>; and (v) the performance of WLLE will decline when the number of neighbors is too small or too large, which can be seen from Fig. <ref type="figure" target="#fig_7">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We conclude by tracing the origin of this work, discuss the main advantages of WLLE compare to other dimension reduction methods and possible future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Early motivation: a difficult example for LLE</head><p>The embeddings of LLE are optimized to preserve the geometry of nearby inputs. Though the collective neighborhoods of these inputs are overlapping, the coupling between faraway inputs can be severely attenuated if the data are noisy, sparse, or weakly connected. Thus, the most common failure mode of LLE is to map faraway inputs to nearby outputs in the embedding space <ref type="bibr" target="#b4">[5]</ref>.</p><p>A difficult example for LLE was mentioned in Ref. <ref type="bibr" target="#b4">[5]</ref>, and this example is shown at the top left of Fig. <ref type="figure" target="#fig_11">14</ref>, where the data were generated from the volume of a 3D "barbell". For this example, LLE algorithm does not lead to reasonable results. It is arguable that in this case this data set can be considered to belong to a collection of manifolds of different dimensionality. Thus, the weakly connected component cause difficult for giving faithful embedding. The possible resolution for this problem lies on identifying weakly connected components or varying the number of neighbors K per data point.</p><p>NLE uses an adaptive neighbor selection method, which determines different neighbor size for each data, to substitute the KNN or -neighborhood neighbor selection used in LLE. However, its simple neighbors selection algorithm used for reducing redundancy cannot help for solving the weakly connected components problem caused by multiple dimensionality. Further more, the too small neighbor size decided by NLE makes the problem even worse.</p><p>The WLLE algorithm proposed in this paper chooses neighbors based on a modified distance measurement, and this weighted distance measurement can strengthen the connection of the weakly connected components in non-uniform sampled data. Thus, based on this distance measurement, the neighborhoods selection algorithm can adopt the weakly connected components as neighbors as well as the strongly connected components, and the weakly connected components can be identified. The top right of Fig. <ref type="figure" target="#fig_11">14</ref> shows the embedding result by WLLE for the "barbell" data set. It is easy to see WLLE preserves the clusters and the connection line faithfully. Compare to the result of WLLE, NLE and LLE both give a worse embedding result as shown at bottom left and bottom right of Fig. <ref type="figure" target="#fig_11">14</ref>.</p><p>In the figure we can see that the clusters and connection line is difficult to be identified in 2D embeddings by NLE and LLE, and most of the properties in original 3D data set is lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Computational complexity analysis</head><p>All these three algorithms mainly consist of three phases: the nearest neighbor phase; the optimal reconstruction weights phase; and reconstruction of low embedding phase. Furthermore, both the NLE and the WLLE have one more steps than the LLE in nearest neighbor phase.</p><p>Given a dimension reduction problem with N data points of D dimension, the computational complexity of the nearest neighbor phase for LLE (using KNN method) is O(DN 2 ), while the NLE has computational complexity of O(DN 2 + ND), with an extra step for estimation neighborhood, where is the number of neighbors selected which is unfixed value because the NLE determines the number of neighbors adapt to the data set distribution. The WLLE has computational complexity of O(ND + 2DN 2 ) in nearest neighbor phase, contains an extra step of parameters estimation. It is noted that in this phase our new algorithm WLLE involves more computations than original LLE, but the WLLE is computational competitive with the NLE. The nearest neighbor step is simple to implement but can be time consuming for large data set (N &gt; 10 4 ). However, many techniques such as the K-D trees or ball trees can be used to compute the neighbors in O(N log N) time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>The following two steps are the same for all the three algorithms. The optimal reconstruction weights phase is typically the least expensive step of the whole algorithm, with the computation scales O(DNK 3 ), where K is the number of neighbors decided in former step. This is the number of operations required to solve a K Ã K set of linear equations for each data point.</p><p>The final step, low embedding reconstruction phase, is typically the most computationally expensive, as computing the bottom eigenvectors scales as O(dN 2 ), where d is the dimension of reconstructed embedding. However, there are many techniques currently for speeding up the eigenvector problems. For example, specialized methods for sparse, symmetric eigenvalue problems <ref type="bibr" target="#b39">[40]</ref> can be used to reduce the complexity; for very large problems, one can consider alternative embedding cost function, such as direct descent by conjugate gradient methods <ref type="bibr" target="#b40">[41]</ref>, stochastic gradient descent <ref type="bibr" target="#b41">[42]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Stability problem</head><p>The results of LLE are typically stable over a range of neighborhood sizes. The size of that range depend on features of the data set, such as the sampling density, distribution and manifold geometry.</p><p>There are several criteria for choosing neighborhood size. First, the dimensionality of embeddings d should be strictly less than the number of neighbors, K. Some margin between d and K is generally necessary to obtain a topology-preserving embedding, but the exact relation between K and the faithfulness of the resulting embedding remains an important open problem. Second, the LLE algorithm is based on the assumption that a data point and its nearest neighbors can be modeled as locally linear; for curved data sets, choosing K too large will in general violate this assumption.</p><p>Figs. <ref type="bibr" target="#b14">15</ref> and 16 shows a range of embeddings discovered by LLE algorithm and WLLE algorithm, all on the same data set but using different numbers of nearest neighbors, K. From these two figures, the results of WLLE show a wider stable range of neighbor size, but the results of LLE are easily break down as K becomes too small or large. Especially, when the number of nearest neighbors K is set too large, the embedding will jump across folds. It is difficult to faithfully unravel the manifold because that large K makes a data point and its nearest neighbors hard to be modeled as locally linear. However, the embeddings of WLLE under the same situation is much better, since the neighbor selection algorithm adopted in WLLE ensures the locally linear between data point and its neighbors even the neighbor size is quite large. Finally, in the case that the original data is itself low-dimensional, which may result in K &gt; D, the local reconstruction weights are no longer uniquely defined since each data point can be reconstructed perfectly from its neighbors. In this case, some further regularization must be added to break the degeneracy. In the procedure of calculating optimal reconstruction weights as described in Section 4, one thing should be mentioned is that in some unusual cases, the local covariance matrix (17) will be singular or nearby singular, for example when there are more neighbors than input dimensions (K &gt; D), or when the data points are not in general position. In these cases, the local covariance matrix must be conditioned by adding a small multiple of identity matrix as in Eq. ( <ref type="formula" target="#formula_25">21</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Sensitivity to parameters</head><p>Besides the number of nearest neighbors K, there are many other facts affect the embedding result. Most important aspects are the features of the data set, such as the sampling density, distribution and manifold geometry. For example, Swiss roll is a randomly sampled plane which is rolled up into a spiral. One parameter of the data set is "Z scaling", the height of the spiral. It is obviously that a smaller value of "Z scaling" will make the folding more compact and thus harder to unravel. Fig. <ref type="figure" target="#fig_13">17</ref> shows the effect of the spiral height on embeddings of both LLE and WLLE. The left column is the original Swiss roll data with different spiral height, and the second and third column are the 2D embeddings by LLE algorithm and WLLE algorithm with respect to different spiral height, respectively.</p><p>It is shown in Fig. <ref type="figure" target="#fig_13">17</ref> that the spiral height of Swiss roll has great effect on the embedding results of LLE. When the "Z scaling" becomes small, the 2D representing of the Swiss roll cannot keep the regular shape well. However, the result of WLLE seems more robust, the change of spiral height has little effect on its results.</p><p>Another example of sensitivity to parameter changes is the effect of neighbor size, which is discussed in Section 6.4 and shown in Figs. <ref type="figure" target="#fig_2">15</ref> and<ref type="figure" target="#fig_3">16</ref>. We know that WLLE algorithm is more robust with different neighbor size than normal LLE algorithm. For LLE, if the number of nearest neighbors K is set too large, the embedding will jump across folds.</p><p>As a result, we may conclude that the WLLE algorithm has better performance than normal LLE with parameter changes, which means the WLLE is more robust in application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have presented an unsupervised learning algorithm to discover the intrinsic structures of data, such as neighborhood relationships, global distributions and clustering. The proposed algorithm optimized the process of intrinsic structure discovery by avoiding unreasonable neighbor searching, and at the same time, let the discovery adapt to the characteristics of input data set. Furthermore, it is able to discover intrinsic structures of data simultaneously, and the discovered structures can be used to compute manipulative embedding for potential classification and recognition purposes. Simulation studies and comparison with LLE and NLE demonstrated that the WLLE can give better result in manifold learning and dimension reduction and is more robust to parameter changes. Experiments on face images data sets have shown the potential of WLLE in practical problem such as face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>5 show the embedding results of these three algorithms for the manifold data sets. In each figure, the sampled data set is shown at the top left, in a 3D representation; the embedding result by WLLE is shown at the top middle; the result by NLE is shown at top right; the result of LLE is shown at bottom left and the result of ISOMAP and Laplacian Eigenmaps are shown at bottom middle and bottom right, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of punched sphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. 3-D clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 2 3 LLEFig. 7 .</head><label>137</label><figDesc>Fig. 7. Dimension reduction result of UMIST face data by six different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Comparison of error rates and computational cost as functions of 2 for KPCA and KDDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of error rates and computational cost as functions of K for WLLE and LLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. 2D embeddings of different pose face images by WLLE.</figDesc><graphic coords="10,71.24,190.96,205.62,168.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>, respectively. All of them show some patterns according to the different poses of face images. The PCA and KPCA map the left and right views of the face to the top and bottom part of the embedding, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .Fig. 12 .Fig. 13 .</head><label>111213</label><figDesc>Fig. 11. 2D embeddings of different pose face images by PCA.</figDesc><graphic coords="10,341.00,324.95,198.81,162.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. A difficult data set for LLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Effect of neighborhood size on LLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Comparison of sensitivity to spiral height of Swiss roll.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>âm â 1, 2, . . . , N, then x j is regarded as a neighbor of the node x i , we initialS i = {x j }. (ii) If d ik = 2 ed min{d im }, âm â 1,2, . . . , N, then x k is regard as a neighbor of node x i if d jk &gt; d ik . (iii) When S i contains two or more elements, for âm â S i , if d jm &gt; d ji and d jm &gt; dmi are satisfy, then S i = S i âª {x m }.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Experimental data sets description</figDesc><table><row><cell>Data set</cell><cell>Samples</cell><cell>Neighbor size</cell><cell>Dimension</cell></row><row><cell>Swiss roll</cell><cell>1000</cell><cell>12</cell><cell>3</cell></row><row><cell>Toroidal helix</cell><cell>1000</cell><cell>24</cell><cell>3</cell></row><row><cell>Gaussian</cell><cell>1000</cell><cell>12</cell><cell>3</cell></row><row><cell>Punctured sphere</cell><cell>1000</cell><cell>24</cell><cell>3</cell></row><row><cell>3-D clusters</cell><cell>1000</cell><cell>24</cell><cell>3</cell></row><row><cell>Different face images</cell><cell>100</cell><cell>24</cell><cell>10 304</cell></row><row><cell>Different pose images</cell><cell>698</cell><cell>8</cell><cell>4096</cell></row><row><cell>Sample Data</cell><cell>WLLE</cell><cell>NLE</cell><cell></cell></row><row><cell>LLE</cell><cell>ISOMAP</cell><cell cols="2">Laplacian Eigenmap</cell></row></table><note><p><p>Fig. 2.</p>Example of Swiss roll.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Computational time comparison</figDesc><table><row><cell>Algorithm</cell><cell>WLLE</cell><cell>NLE</cell><cell>LLE</cell><cell>ISOMAP</cell><cell>Laplacian Eigenmaps</cell></row><row><cell>Swiss roll</cell><cell>8.4</cell><cell>11.8</cell><cell>3.1</cell><cell>95.2</cell><cell>1.2</cell></row><row><cell>Toroidal helix</cell><cell>6.2</cell><cell>12.1</cell><cell>1.3</cell><cell>125.5</cell><cell>1.1</cell></row><row><cell>Gaussian distribution</cell><cell>7.3</cell><cell>10.9</cell><cell>3.9</cell><cell>100.7</cell><cell>0.99</cell></row><row><cell>Punched sphere</cell><cell>6.8</cell><cell>7.9</cell><cell>6.9</cell><cell>100.2</cell><cell>1.3</cell></row><row><cell>3D clusters</cell><cell>5.5</cell><cell>10.6</cell><cell>2.2</cell><cell>74.1</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Classification error rates and computational time comparison</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Parameter</cell><cell>Computational time (s)</cell><cell>Error rate (%)</cell></row><row><cell>PCA</cell><cell>N.A.</cell><cell></cell><cell>0.5</cell><cell>47</cell></row><row><cell>KPCA</cell><cell cols="2">2 : 2e2-8e2</cell><cell>16.5</cell><cell>10</cell></row><row><cell>KDDA</cell><cell>2</cell><cell>2e2</cell><cell>1.7</cell><cell>0</cell></row><row><cell>LLE</cell><cell cols="2">K : 24-36</cell><cell>2.4</cell><cell>1</cell></row><row><cell>WLLE</cell><cell cols="2">K : 24-72</cell><cell>13.3</cell><cell>0</cell></row><row><cell>NLE</cell><cell>N.A.</cell><cell></cell><cell>11.5</cell><cell>8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Dr. D. Grahan and Dr. N. Allinson for providing the UMIST face database.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bumptrees for efficient function, constraint and classification learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Omohundro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990. 1991</date>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="693" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition using kernel direct discriminant analysis algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="126" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Leroy</surname></persName>
		</author>
		<title level="m">Robust Regression and Outlier Detection</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multidimensional</forename><surname>Scaling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><forename type="middle">M</forename><surname>Ãller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Adaptive Neural Network Control of Robotic Manipulators</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face contour extraction from front-view images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1167" to="1179" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human face detection in a complex background</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The optimal distance measure for nearest neighbor classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="622" to="627" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization of k-nearest neighbor density estimates</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="320" to="326" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rbf-based neurodynamic nearest neighbor classification in real pattern space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muezzinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="747" to="760" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detection of epileptic spike-wave discharges using SVM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Mamun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE International Conference on Control Applications, Suntec City</title>
		<meeting>the 2007 IEEE International Conference on Control Applications, Suntec City<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="467" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A modified version of the k-means algorithm with a distance based on cluster symmetry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="674" to="680" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The global k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="461" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An optimal global nearest neighbor metric</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Flick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="314" to="318" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Flexible metric nearest neighbor classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Stanford, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminant adaptive nearest neighbor classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="607" to="616" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Similarity metric learning for a variable-kernel classifier</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="85" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Locally adaptive metric nearest-neighbor classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1281" to="1285" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric learning by knowledge embedding</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distance learning for similarity estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face recognition with weighted locally linear embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mekuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Canadian Conference on Computer and Robot Vision</title>
		<meeting>the Second Canadian Conference on Computer and Robot Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="290" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving nearest neighbor classification with cam weighted distance</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature representation based on intrinsic structure discovery in high dimensional space</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 2006 IEEE International Conference on Robotics and Automation<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3399" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neighborhood linear embedding for intrinsic structure discovery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl. J</title>
		<imprint/>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hand gesture recognition and tracking based on distributed locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics, Automation and Mechatronics</title>
		<meeting>IEEE International Conference on Robotics, Automation and Mechatronics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-euclidean or nonmetric measures can be informative</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spillmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural, Syntactic, and Statistical Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Characterizing virtual eigensignatures for general purpose face recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition: From Theory to Applications</title>
		<title level="s">NATO ASI Series F, Computer and Systems Sciences</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Fogelman-Soulie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="446" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<ptr target="http://images.ee.umist.ac.uk/danny/database.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reconstruction and analysis of multi-pose face images based on nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic human face location in a complex background using motion and color information</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1877" to="1889" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face-texture model based on sgld and its application in face detection in a color scene</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1007" to="1017" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">KPCA plus LDA: a complete kernel fisher discriminant framework for feature extraction and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Yu</forename><surname>Yang Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">N-Body problems in statistical learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2001</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jacobi-Davidson style QR and QZ algorithms for the reduction of matrix pencils</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Fokkema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L G</forename><surname>Speijpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Vandervorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="125" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flannery</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C: The Art of Scientific Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><forename type="middle">M</forename><surname>Ãller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">is founding Director of Social Robotics Lab of Interactive Digital Media Institute, and Director of Edutainment Robotics Lab of the Department of Electrical and Computer Engineering, the National University of Singapore. He has (co)-authored three books: Adaptive Neural Network Control of Robotic Manipulators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Control Systems Technology, IEEE Transactions on Neural Networks, and Automatica. He also serves as a book Editor of the Taylor &amp; Francis Automation and Control Engineering Series. His current research interests include social robotics, multimedia fusion, adaptive control, intelligent systems and artificial intelligence. About the Author-ABDULLAH AL MAMUN received B.Tech</title>
		<editor>
			<persName><forename type="middle">D</forename><surname>Ph</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Dic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Sc</surname></persName>
		</editor>
		<editor>
			<persName><surname>Eng</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">2004 and 2006. 1998. 2005</date>
		</imprint>
		<respStmt>
			<orgName>Harbin Institute of Technology, China ; Department of Electrical and Computer Engineering at the National University of Singapore ; Indian Institute of Technology, Kharagpur ; Electrical and Computer Engineering, National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>India in 1985, and the Ph.D. degree from the National University of Singapore in 1997. In his professional career, he worked as Research Engineer at the Data Storage Institute, Singapore and as Staff Engineer at Maxtor Peripherals prior to joining the faculty of the department of. His research interest includes precision servomechanism, mechatronics, intelligent control, and autonomous mobile robots</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
