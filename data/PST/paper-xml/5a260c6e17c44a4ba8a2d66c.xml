<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yi-Ping Phoebe Chen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Technology</orgName>
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<postCode>3086</postCode>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4658FD6BD88E8C19876A8F9AE280FC2C</idno>
					<idno type="DOI">10.1109/TNNLS.2016.2574840</idno>
					<note type="submission">received January 18, 2016; revised April 5, 2016; accepted May 28, 2016.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimized Structure of the Traffic Flow Forecasting Model With a Deep Learning Approach</head><p>Hao-Fan Yang, Tharam S. Dillon, Life Fellow, IEEE, and Yi-Ping Phoebe Chen, Senior Member, IEEE Abstract-Forecasting accuracy is an important issue for successful intelligent traffic management, especially in the domain of traffic efficiency and congestion reduction. The dawning of the big data era brings opportunities to greatly improve prediction accuracy. In this paper, we propose a novel model, stacked autoencoder Levenberg-Marquardt model, which is a type of deep architecture of neural network approach aiming to improve forecasting accuracy. The proposed model is designed using the Taguchi method to develop an optimized structure and to learn traffic flow features through layer-bylayer feature granulation with a greedy layerwise unsupervised learning algorithm. It is applied to real-world data collected from the M6 freeway in the U.K. and is compared with three existing traffic predictors. To the best of our knowledge, this is the first time that an optimized structure of the traffic flow forecasting model with a deep learning approach is presented. The evaluation results demonstrate that the proposed model with an optimized structure has superior performance in traffic flow forecasting.</p><p>Index Terms-Deep learning, forecasting, neural network (NN) applications, stacked denoising autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I MPROVING the accuracy of traffic flow forecasting can bring benefit to the intelligent traffic management on traffic efficiency and congestion reduction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Traffic flow forecasting is quite a complex process that is affected by many factors, such as traffic patterns, applied areas, data collection, and so on. In recent computational traffic flow forecasting approaches, researchers have to select the traffic features and modeling parameters from the obtained data according to some fundamental assumptions applied in the past literature. That is, the correctness of these assumptions may crucially affect forecasting accuracy. However, if the volume of data is big enough, the hidden statistical information and potential correlations will be revealed by the data set itself <ref type="bibr" target="#b2">[3]</ref>. Therefore, if a large volume of traffic data is adopted, we may be able to avoid many failures caused by assumptions and the accuracy of traffic prediction can be improved by learning the information and correlations hidden in the data <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Computational forecasting approaches, such as neural networks (NNs) <ref type="bibr" target="#b4">[5]</ref>, Bayesian modeling <ref type="bibr" target="#b5">[6]</ref>, statistical modeling <ref type="bibr" target="#b6">[7]</ref>, fuzzy logic <ref type="bibr" target="#b7">[8]</ref>, and hybrid modeling <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, have been widely used in traffic flow forecasting. These approaches, particularly NNs, are proven to be useful in discovering the statistical rules hidden in the traffic data and predicting the future traffic flow. However, most of them are shallow traffic models and the computed forecasting results need to be more accurate. Moreover, with widespread traffic sensing facilities and advancements in sensing technologies <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, the amount of collected traffic flow data can be greatly increased. The dawning of the big data era brings opportunities to greatly promote the improvement of prediction accuracy <ref type="bibr" target="#b12">[13]</ref>. These inspire us to extend and apply the concept of deep learning for traffic flow forecasting. Deep learning is a type of machine learning method based on learning representations of data, and it has been successfully applied to assist in many fields, for instance, classification tasks, information processing, pattern recognition, and object detection <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The concept of deep learning is to use deep architectures (multiple layers of nonlinear processing units) to extract and transform the inherent features in the data from the lowest level to the highest level, and every continuous layer uses the output from the previous layer as input <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>. As traffic flow prediction is a complicated process, deep learning related algorithms can represent traffic features from the obtained data without prior knowledge, which may greatly improve forecasting accuracy.</p><p>In this paper, we propose a novel model, stacked autoencoder Levenberg-Marquardt (SAE-LM) model, which is a type of deep architecture of the NN approach aiming to improve the accuracy of short-term traffic flow forecasting. The SAE-LM model is designed using the Taguchi method to develop an optimized structure and to learn traffic flow features through layer-by-layer feature granulation with a greedy layerwise unsupervised learning algorithm and is applied to real-world data collected from the M6 freeway in the U.K. To the best of our knowledge, this is the first time that an optimized structure of the traffic flow forecasting model with a deep learning approach is presented. Using a solid experimental setup and execution, we obtained useful results that can be used for further research.</p><p>The rest of this paper is organized as follows. Section II reviews the current studies on NNs for traffic flow forecasting and expresses the motivation. Section III explains the methodologies and designs used in this paper. Section IV details the experimental results and evaluates the performance of the SAE-LM model with an optimized configuration. Finally, the conclusion is given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CURRENT APPLICATIONS OF NEURAL NETWORKS FOR TRAFFIC FLOW FORECASTING AND MOTIVATION</head><p>Traffic flow forecasting is based on collected traffic data to make predictions about the likely traffic flow changes in the short term, and it can provide the predictive functionality for traffic management and control strategies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>. A number of traffic flow forecasting models have been developed to improve transportation efficiency and a wide variety of techniques have been adopted to optimize these models. Using NNs is one of the most popular approaches in traffic flow forecasting because of its learning capabilities. NNs are a kind of information processing technique, which can be trained to learn a relationship in a data set. Even if the underlying relationships in a data set are not apparent, the NN-based model is still capable of generalizing accurate predictions due to its nonlinear and nonparametric nature <ref type="bibr" target="#b17">[18]</ref>. Many NN-based forecasting models have been applied to improve transportation efficiency and have been proven to be effective in solving problems within the complex relationships involved, for instance, traffic pattern recognition, traffic data classification, and traffic flow prediction <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Hybrid exponential smoothing and the Levenberg-Marquardt (LM) algorithm were used to train NNs for short-term traffic flow forecasting in <ref type="bibr" target="#b20">[21]</ref>. The forecasting model was developed based on the traffic flow data (traffic speeds and headways) collected from detector stations located on the Mitchell freeway in Western Australia. The exponential smoothing approach was applied to remove the lumpiness from the captured traffic flow data, as this lumpiness may reduce the generalization capability <ref type="bibr" target="#b21">[22]</ref>. The LM algorithm was then adopted to train NNs since it provides a numerical solution to the nonlinear problem, which can minimize a function over a space of the function parameters, and also, it is stable and can generate good convergence <ref type="bibr" target="#b22">[23]</ref>. A three-layered NN was implemented to predict the future traffic flow condition. The experimental results showed that the proposed NN-based forecasting model can deliver forecasting results with the mean of the forecasted error less than 13%.</p><p>An integration of the particle swarm optimization algorithm and NNs was adopted to develop short-term traffic flow predictors in <ref type="bibr" target="#b8">[9]</ref>. Simple multilayer (three layers) NN-based models were used to address the strongly nonlinear characteristics of short-term traffic flow data. The particle swarm optimization algorithm was adopted to represent both structures and parameters of short-term traffic flow predictors. That is, optimal structures and parameters can be determined automatically without involving some of the fundamental assumptions applied in the past literature or trial-and-error methods based on human expertise <ref type="bibr" target="#b23">[24]</ref>. The proposed model was applied to predict traffic speeds and densities on a section of freeway in Western Australia. The forecasting accuracy was evaluated using mean absolute error (MAE) and root-mean-square error (RMSE). The experimental results indicated that the proposed model is able to predict the traffic flow conditions with an average MAE of less than 16% and an average RMSE of less than 17% on all forecasted business days, which is more accurate than the Kalman filter <ref type="bibr" target="#b24">[25]</ref> and genetic algorithms <ref type="bibr" target="#b25">[26]</ref>.</p><p>A radial basis function NN (RBFNN) model was proposed in <ref type="bibr" target="#b26">[27]</ref> to exploit the spatiotemporal features from the volume of adjacent intersection to further optimize forecasting performance. The RBFNN is a three-layer feedforward NN with one radial basis layer (hidden layer), which can uniformly approximate any continuous function with a prospected accuracy. It also has good local generalization abilities and faster convergence speed compared with other NN-based methods <ref type="bibr" target="#b27">[28]</ref>. The traffic volume data (the amount of vehicles, a maximum of 15-min traffic flow rate, and a minimum of 15-min traffic flow rate) were collected by the loop detectors in Baotou City, China. The forecasting accuracy was evaluated by mean absolute deviation (MAD), mean absolute percentage error (MAPE), and RMSE. The results of the experiments showed that using more related input can improve the performance, with the MAD, MAPE, and RMSE being 13.6%, 12.2%, and 16.3%, respectively.</p><p>In summary, a large number of NN-based short-term traffic flow predictors have been developed, but most of them are shallow traffic models. The traffic features and parameters used in these models are selected according to some fundamental assumptions or are determined by applying trial-and-error methods. If we can find a way to reduce the uncertainty brought on by some of the assumptions and lower the time consumed by trial-and-error methods, the accuracy and efficiency of traffic flow forecasting can be greatly improved. Moreover, although the existing traffic flow predictors can deliver decent forecasting results, there is still some room for accuracy improvement. Inspired by the above reasons, this paper proposes a deep architecture of NN-based approach, the SAE-LM model, to reduce the uncertainty caused by some assumptions and improve the accuracy and efficiency of traffic flow forecasting. The optimized structure of the proposed model is designed using the Taguchi method, which has been successfully used to optimize NNs for traffic flow forecasting <ref type="bibr" target="#b4">[5]</ref>. The Taguchi method uses an orthogonal array to study the effect of design factors simultaneously by a planning matrix, and it reduces the number of experiments dramatically compared with the trial-and-error method <ref type="bibr" target="#b28">[29]</ref>. For instance, for an NN-based forecasting model with five design factors, each of which contains three levels, using a trial-and-error method requires 243 (3 5 ) experiments. When an orthogonal array L 18 (3 5 ) is used, only 18 experiments are needed to evaluate the effect of each design factor to determine the optimum condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGIES</head><p>In this section, the methodologies of the proposed SAE-LM model and the model design based on the Taguchi method are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stacked Autoencoder Levenberg-Marquardt Model</head><p>A stacked autoencoder (SAE) model uses autoencoders as building blocks to create a deep architecture <ref type="bibr" target="#b29">[30]</ref>.</p><p>The autoencoder is typically implemented as a one-hiddenlayer NN, which is trained to reproduce its input. It is used as building blocks to train deep networks, where each hidden layer is associated with an autoencoder that can be trained separately. The amount of output nodes is equal to the amount of input nodes, and one input node is trained at a time. In general, an autoencoder takes a vector input x, encodes it to a hidden layer h, and decodes it to a reconstruction z. For example, given a set of data {x <ref type="bibr" target="#b0">(1)</ref>, and then the hidden representation h (l+1) (x (l)  i ) is mapped back into a reconstruction z (l+2) (x (l)  i ) according to <ref type="bibr" target="#b1">(2)</ref>. Equations ( <ref type="formula" target="#formula_15">1</ref>) and ( <ref type="formula">2</ref>) are shown as</p><formula xml:id="formula_0">(l) 1 , x (l) 2 , . . . , x (l) n } of n training samples, where x (l) i ∈ R D denotes the unit i in layer l, an input x (l) i is first mapped to a latent representation h (l+1) (x (l) i ) based on</formula><formula xml:id="formula_1">h(x) = f (W 1 * x + b 1 )</formula><p>(1)</p><formula xml:id="formula_2">z(x) = g(W 2 * h(x) + b 2 ) (2)</formula><p>where W 1 , W 2 , b 1 , and b 2 are the encoding matrix, decoding matrix, encoding bias vector, and decoding bias vector, respectively. f (•) and g(•) are the activation functions, and we follow the custom of using the same activation function for both mappings (encoding and decoding). Since the inputs in this research (i.e., the collected traffic data) are real numbers, we use the squared loss function L(x, z) to measure the reconstruction error <ref type="bibr" target="#b13">[14]</ref>. The model parameters, denoted by θ , can be obtained by minimizing the average difference between the input x and the reconstruction z (i.e., reconstruction error), as shown in</p><formula xml:id="formula_3">θ = arg min θ L(x, z) = arg min θ 1 n n i=1 x (l) i -z x (l) i 2 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>There is a serious issue concerning autoencoders in that if the hidden units are the same size or greater than the input units, an autoencoder could potentially learn the identity function and become useless (e.g., just by copying the input) <ref type="bibr" target="#b13">[14]</ref>. However, the experiment results in <ref type="bibr" target="#b29">[30]</ref> show that the autoencoder with more hidden units than inputs can yield useful representations when trained with stochastic gradient descent. Moreover, current practice indicates that adding noise in the encoding or constraining the encoding by sparseness can successfully ensure that the above issue does not occur <ref type="bibr" target="#b30">[31]</ref>. In this study, we choose to impose a sparsity constraint <ref type="bibr" target="#b31">[32]</ref> on the hidden units. For a sigmoid activation function, a neuron is seen as being active if its output value is close to 1 or being inactive if its output value is close to 0 (note that different activation functions may have different output values). We constrain the neurons to be inactive most of the time. Suppose a (l+1) j (x) denotes the activation of unit j in layer l + 1 when the network is given a specific input x. Let</p><formula xml:id="formula_5">ρ j = 1 n n i=1 a (l+1) j x (l) i (4)</formula><p>be the average activation of unit j and impose the constraint where ρ is a sparsity parameter whose value is close to zero. To achieve this, an extra penalty term that penalizes ρ j deviating from ρ is added as follows:</p><formula xml:id="formula_6">ρ j = ρ (5)</formula><formula xml:id="formula_7">N j =1 ρ log ρ ρ j +(1-ρ) log 1 -ρ 1 -ρ j (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where N is the number of neurons in the hidden layer and the index j is summing over the hidden units in the network. This penalty term is based on the concept of Kullback-Leibler (KL) divergence, where</p><formula xml:id="formula_9">KL(ρ ρ j ) = ρ log ρ ρ j + (1 -ρ) log 1 -ρ 1 -ρ j . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>The KL divergence penalty function (Algorithm l) has the property that KL(ρ ρ j ) = 0 if ρ j = ρ, which provides the constraint by sparseness on the coding. If the autoencoder is implemented using the backpropagation algorithm, the gradient descent will be performed exactly on the cost function L Sparse (x, z) <ref type="bibr" target="#b31">[32]</ref>. The overall cost function (reconstruction error with a sparsity constraint) can then be formulated as</p><formula xml:id="formula_11">L Sparse (x, z) = L(x, z) + α N j =1 KL(ρ ρ j ) (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where α is the weight of the sparsity term.</p><p>In order to create a deep architecture, autoencoders are stacked one by one from the bottom layer. Having trained the Algorithm 2 Levenberg-Marquardt Algorithm (LM) Parameters: μ -control parameter for LM to train the network, g-gradient, S-sum of squared errors, H-Hessian matrix, I-identity matrix, J-Jacobian matrix, e-a vector of network errors; Initialize μ =0.001, μ_max=1 * 10^10, μ_min=1 * 10^(-10) % Rearrange all the weights in a vector (V) for i=1:max_i; H= J T J; g=J T e; dx=g * (H+ μ * I) bottom layer autoencoder [the first hidden layer h (2) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(x (1)</head><p>i )] to minimize the reconstruction error of the raw input, its hidden unit outputs are used as input for the next autoencoder [the second hidden layer h (3) (h (2) x (1)  i )]. Iterating above process until the desired number of auto-encoders is added. The last hidden layer's output is taken as input to a supervised learning algorithm to fine-tune all the parameters of this deep architecture with respect to the supervised criterion <ref type="bibr" target="#b32">[33]</ref>. In this research, we use the LM <ref type="bibr" target="#b22">[23]</ref> on top of the whole network to train the input generated by the last autoencoder. The LM (Algorithm 2) is adopted since it can provide a numerical solution to the nonlinear problem minimizing a function over a space of the function parameters and also it is stable and can generate good convergence <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> gives an illustration of the development of the proposed SAE-LM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Description and Forecasting Model</head><p>The traffic flow condition is sensed by inductive loops built into the road surface located at several junctions (J6, J7, J17, J18, J34, and J35) on the M6 freeway in the U.K. on the working days between March 3, 2014 and March 28, 2014 (20 working days). Both sides of the observed road sections are three lanes in width, and the road length between J6 to J7 is 7.3 km, J17 to J18 is 6.4 km, and J34 to J35 is 7.2 km. The collected traffic data were divided into six road links data sets, namely, R1a (J6 to J7), R1b (J7 to J6), R2a (J17 to J18), R2b (J18 to J17), R3a (J34 to J35), and R3b (J35 to J34). In all the six data sets, the traffic flow data collected from the first 15 working days are used to develop the forecasting model, The sensed traffic flow condition is collected every 1 min from many sensing stations (S 1 , S 2 , S 3 , . . . , S a ), which are located at the observed road sections. S i captures two traffic condition measures, the average vehicle speed Vi (t) and the average headway ĥi (t) between time t and time t + T S , where T S is the sampling time. Since the data is collected every 1 min, T S can be set as 1. In light of the current and past traffic conditions, future traffic conditions can be forecasted by the proposed SAE-LM model. The current traffic condition is denoted by the current average speed Vi (t) and current average headway ĥi (t). The past traffic condition is indicated by the past average speed Vi (tk) and past average headway ĥi (tk), which was collected by S i at time (tk) with i = 1, 2, 3, . . . , a and k = 1, 2, 3, . . . , p, whereas the past traffic data within p sampling time period are collected. Two data sets {V (l)  1 , V (l) 2 , . . . , V (l) a } and {h (l) 1 , h (l) 2 , . . . , h (l) a } can be generated, where</p><formula xml:id="formula_13">V (l) i , h (l)</formula><p>i ∈ R D denotes the collected traffic conditions in layer l. Suppose the outputs of the last autoencoder are z (l+2) (V (l)  i ) and z (l+2) (h (l) i ), the future traffic condition can then be calculated by the SAE-LM model, which is indicated by ĥ L (t + m) passing through the Lth sensing station S L at time (t + m), where the future traffic condition with m sampling time ahead is forecasted. The SAE-LM </p><formula xml:id="formula_14">ĥ L (t + m) = a i=1 M j =1 β V j,i C γ V 0, j,i p k=1 γ V k, j,i z (l+2) V (l) i + β h j,i C γ h 0, j,i p k=1 γ h k, j,i z (l+2) h (l) i +α (9)</formula><p>where M is the number of nodes in the hidden layer, C(•) is the activation function of the hidden set, V</p><p>= Vi (tk) and h (1)  i = ĥi (tk), and</p><formula xml:id="formula_16">β V j,i , β h j,i , γ V 0, j,i , γ h 0, j,i , γ V k, j,i , γ h k, j,i</formula><p>, and α 0 are the parameters of the proposed forecasting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Taguchi Method and Experiment Design</head><p>The Taguchi method is applied to obtain the optimized structure of the proposed traffic flow forecasting model, the SAE-LM model, which is trained with the SAE and LM algorithms to establish a short-term traffic forecasting predictor. The Taguchi method is used to optimize the topology of the SAE-LM model by the following three sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Identification of Design Factors</head><p>In the design of the optimized structure of the SAE-LM model, the considered design factors and their corresponding levels are as follows and are presented in Table <ref type="table" target="#tab_1">I</ref>.</p><p>1) Design Factor i: The number of training nodes is determined by the sampling time period. Hence, in this research, we consider sampling times of 30, 60, 240, and 600 min as Levels 1, 2, 3, and 4, respectively. 2) Design Factor ii: For the activation functions of the autoencoders, f (•) and g(•), the most commonly used transfer functions for deep structures are Sigmoid, Tanh, LogSig, and SoftSign <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b33">[34]</ref>. We follow the practice of using the same activation function for f (•) and g(•), and Sigmoid, Tanh, LogSig, and SoftSign functions are set as Levels 1, 2, 3, and 4, respectively. 3) Design Factor iii: The number of hidden nodes and the number of hidden layers are important since they determine the size of the SAE-LM model for the shortterm traffic flow predictor. The minimum number of hidden nodes recommended by <ref type="bibr" target="#b34">[35]</ref> is log 2 (N), where N is the number of training nodes. In addition, the maximum number of hidden nodes suggested by <ref type="bibr" target="#b35">[36]</ref> to solve modeling problems is 50. Therefore, we set log 2 (N), 2 × log 2 (N), 3 × log 2 (N), and 50 as Levels 1, 2, 3, and 4, respectively. 4) Design Factor iv: The number of hidden layers is a significant design factor, and in this research, it depends on how many autoencoders are stacked. For example, if two autoencoders are stacked, the number of hidden layers is three (i.e., the number of autoencoders plus the last one hidden layer, which is trained by LM). The accuracy rate can be improved with more autoencoders in the forecasting model; however, the computational time will increase dramatically. With a different number of autoencoders used in the forecasting model, the relationship between the accuracy rate and computational time is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Since the accuracy rate and computational time are both important in traffic forecasting, we select the number of autoencoders that can generate predictions with more than 80% accuracy rate in less than 5 s. Hence, according to the results, three, four, five, and six hidden layers (two, three, four, and five autoencoders) are considered as Levels 1, 2, 3, and 4, respectively. 5) Design Factor v: For the activation functions of the last hidden set, C(•), Logsig, Tansig, and Purelin functions are commonly used for traffic forecasting networks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Therefore, Logsig, Tansig, Purelin, and the function that is the same as the one used for the autoencoders (design factor ii) are set as Levels 1, 2, 3, and 4, respectively. Note: Levels 1 and 4 of design factor v are the same (Logsig) when Level 3 of design factor ii is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Specification of Performance Measure</head><p>To evaluate the effectiveness of the SAE-LM model for traffic forecasting, we use two performance indexes, the MAPE and variance absolute percentage error (VAPE). MAPE and VAPE indicate the mean and variance of the difference between the actual and predicted values, respectively. Mathematical models for MAPE and VAPE are defined as</p><formula xml:id="formula_17">MAPE(%) = 1 n n t =1 A t -F t A t × 100% (10) VAPE(%) = Var A t -F t A t × 100% (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where A t is the observed traffic flow condition, F t is the predicted traffic flow condition, and n is the number of forecasted points. The lower the value of MAPE and VAPE, the higher the accuracy of the predicted result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Trial Design and Results Analysis</head><p>An orthogonal array L 16 (4 5 ) is used for the trial design due to the fact that there are five design factors and each one contains four levels. Based on the combination of the levels of design factors, 16 main trials are conducted to obtain the optimized structure of the SAE-LM model. For each main trial corresponding to each row of the adopted orthogonal array L 16 ( <ref type="formula">4</ref>  <ref type="table" target="#tab_2">II</ref>.</p><p>A total of 1280 trials, 16 main trials over 20 days using four traffic data sets (R1a, R1b, R2a, and R2b), were conducted to evaluate the effectiveness and robustness of the SAE-LM model using the Taguchi method design. If the full factorial design is used, 81 920 (4 5 × 20 × 4) trials need to be carried out, where four levels in each of the five design factors over the 20 observed days using four traffic data sets are used for the traffic forecasting model design. Compared with the number of trials required by the Taguchi method with the full factorial design, 80 640 trials (81 920-1280) can be saved in the design of the SAE-LM model. This demonstrates that using the method in this research is reasonable and effective.</p><p>As shown in Table <ref type="table" target="#tab_2">II</ref>, the tenth main trial with a 240 sampling time, Tanh function for the autoencoders, 50 hidden nodes, 5 hidden layers, and Logsig function for the last hidden set achieves the smallest MAPE and VAPE. It is much smaller compared with the 11th and 12th main trials, which utilize the same sampling time. The top three ranks are the 5th, 10th, and 16th main trials, which denotes that the SAE-LM model with five hidden layers can achieve better predicted results than using three, four, or six hidden layers. Moreover, the 1st, 7th, 12th, and 14th main trials show that MAPE and VAPE perform poorly, which indicates that applying only three hidden layers (two autoencoders) cannot generate accurate forecasting results. Comparing the results of the 3rd, 5th, 10th, and 16th main trials, we found that when the SAE-LM model with five hidden layers, the sampling time was 240 min. The accuracy decreases greatly when reducing the sampling time from 240 to 30 min. The above results clearly demonstrate Fig. <ref type="figure">3</ref>. Effects of each design factor at each of the four levels. The level with the largest effect (bold) of each design factor are Level 3 for design factor i, Level 4 for design factor ii, Level 3 for design factor iii, Level 3 for design factor iv, and Level 3 for design factor v.</p><p>that the proposed SAE-LM model with a 240-min sampling time and five hidden layers (four autoencoders) can generate the predicted results with high accuracy.</p><p>The effects of each design factor can be separated since the combinations of design factors of each trial are orthogonal <ref type="bibr" target="#b37">[38]</ref>. The effects of each design factor at each of the four levels are calculated by taking the average from Table <ref type="table" target="#tab_2">II</ref> for a design factor at a given level. For example, the Level 2 of the design factor iii is in the 2nd, 5th, 12th, and 15th main trials, and the average of this level is 0.1465 (MAPE) and 0.0091 (VAPE). Since a lower value of MAPE and VAPE indicates better performance, the lower the average value obtained means the larger the effect. The effects of each design factor at each of the four levels are displayed in Fig. <ref type="figure">3</ref>. An intuitive method, range analysis, is applied to express the sensitivity of the design factors to the performance values. In this research, this is computed by the difference between the values with the largest and the smallest effect for each design factor, and the results are shown in Table <ref type="table" target="#tab_2">III</ref>. The order of the sensitivity of the five design factors is iv</p><formula xml:id="formula_19">&gt; i &gt; ii &gt; v &gt; iii in MAPE and iv &gt; i &gt; iii &gt; v &gt; ii in VAPE.</formula><p>From the above order, we can find that design factor iv and According to Fig. <ref type="figure">3</ref> and Tables <ref type="table" target="#tab_2">II</ref> and<ref type="table" target="#tab_2">III</ref>, the optimized structure design of the SAE-LM model in traffic flow forecasting can be inferred as design factor i with Level 3, design factor ii with Level 4, design factor iii with Level 3, design factor iv with Level 3, and design factor v with Level 3. That is, the SAE-LM model with a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERFORMANCE EVALUATION OF THE SAE-LM MODEL WITH AN OPTIMIZED STRUCTURE</head><p>This section demonstrates the accuracy and efficiency of the SAE-LM model with the optimized structure for traffic flow forecasting by comparing the predictors introduced in Section II, which are hybrid exponential smoothing and the LM algorithm with NNs (EXP-LM), particle swarm optimization algorithm with NNs (PSONN), and RBFNNs.</p><p>Performance Evaluation: The traffic data in R3a and R3b are used for performance evaluation. Table <ref type="table" target="#tab_3">IV</ref> shows the accuracy rate of the traffic prediction on each of the observed days. The results show that the proposed SAE-LM model is more accurate than the EXP-LM, PSONN, and RBFNN models for the prediction of the traffic flow. Importantly, SAE-LM is able to generate forecasting results with approximately 90% accuracy, which is 5% better on most of the observed days compared with the other predictors. For the SAE-LM, the prediction performance is relatively stationary from 89.55% to 91.15%. The difference between the maximum and minimum accuracy rates for SAE-LM is 1.6%, which is better than 2.53% for EXP-LM, 3.71% for PSONN, and 2.42% for RBFNN.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> presents the output of the SAE-LM, EXP-LM, PSONN, and RBFNN models for traffic flow prediction on March 24-28, 2014. The actual (observed) traffic flow is also included in Fig. <ref type="figure" target="#fig_4">4</ref> for comparison. As can be seen in Fig. <ref type="figure" target="#fig_4">4</ref>, the predicted traffic flow of all four models has similar traffic patterns to the actual traffic flow. However, an obvious difference occurs between the values of the actual and predicted traffic flows at 07:00 to 18:00 on each of the observed days, especially in the forecast results generated by the EXP-LM, PSONN, and RBFNN models. This phenomenon implies that the proposed SAE-LM model can forecast more accurately than the other models when the traffic flow is rising and falling rapidly over a short period of time. In contrast, in the period during which the traffic flow does not change sharply (18:00 to 24:00 and 00:00 to 07:00), all four models generate forecasting results with high accuracy. The reason for this phenomenon is that some hidden statistical information and correlations in the forecasted data set cannot be revealed. It is thought that none of the traffic flow predictors is able to learn well from the input except for the proposed SAE-LM model, which reconstructs the observed traffic data through an encoding and decoding process to predict traffic flow.</p><p>We compare the percentage error of the traffic flow forecasting results of each model, as illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>. As can be seen in Fig. <ref type="figure" target="#fig_5">5</ref> sampling time to predict the traffic flow, we can infer that the SAE-LM model may not be suitable for forecasting the traffic flow using data samples with a highly smooth distribution. However, the results described in the previous paragraph show that the SAE-LM model is effective in dealing with lumpy data. In fact, the actual traffic flow data is always lumpy. Therefore, based on the results in Figs. <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref> and Table <ref type="table" target="#tab_3">IV</ref>, the SAE-LM model is demonstrated to be more effective and promising for traffic flow prediction in practice than the other models.</p><p>In terms of efficiency concerns <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, the computational time is also an important factor in traffic flow forecasting. We measure the computational time on a 3.40-GHz Intel i7 CPU, 64-b operating system, and 16.0-GM RAM. The computational time of each traffic predictor in the training and forecasting stages is shown in Table <ref type="table" target="#tab_4">V</ref>. The RBFNN is the fastest compared with the other predictors in both training and forecasting. However, although the SAE-LM consumes slightly more time (2.25 s in the training stage and 0.53 s in the forecasting stage), it produces more accurate results than RBFNN (average increase of 7.33% for R3a and 6.39% for R3b). V.</p><p>In recent times, has focused on developing and optimizing traffic flow predictors in order to obtain more accurate predictions and solve traffic congestion problems based on many computational models. However, most of them are shallow traffic models and the computed traffic flow forecasting results need to be more accurate. Therefore, this paper proposes a novel predictor, SAE-LM, with an optimized structure. Due to the fact that the trial-and-error method is very time consuming with too many design factors involved, the Taguchi method is adopted to improve the effectiveness of the design of traffic flow forecasting model. The proposed SAE-LM with an optimized structure is applied to real-world data collected from the M6 freeway in the U.K. and compared with the existing traffic predictors, EXP-LM, PSONN, and RBFNN. The evaluation results indicate that the SAE-LM model with an optimized structure is an accurate and efficient approach to traffic flow forecasting. In addition, it has superior performance (about 90% accuracy rate) in traffic flow forecasting and is the most suitable approach to deal with the lumpy data in this research. Some significant findings include the following.</p><p>1) Although the accuracy of traffic flow forecasting may be increased by applying more autoencoders, the time consumed in the training and forecasting stages increases dramatically. With limited computational time, the SAE-LM model with five hidden layers (four autoencoders) can generate the most accurate predicted results.</p><p>2) The SAE-LM is able to generate forecasting results with approximately 90% accuracy rate, which is 5% more than the other predictors. In addition, the prediction IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS performance of the SAE-LM is relatively stationary (only 1.6% difference), which is 0.93% better than the EXP-LM, 2.11% better than the PSONN, and 0.82% better than the RBFNN.</p><p>3) The disadvantage of the SAE-LM model is that it may not be able to perform well if the observed traffic data has a highly smooth distribution. However, when lumpy traffic data are collected, the SAE-LM has superior performance in traffic flow forecasting. Since the observed traffic flow data are always lumpy in nature, the SAE-LM model is demonstrated to be more effective and promising for traffic flow prediction in practice than the other models. Future research will focus on the following areas: improving the accuracy and efficiency of the SAE-LM, adopting other learning algorithms to the last hidden set, selecting the appropriate imputation strategy to deal with missing data, and solving congestion problems that occur in complex roadways.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Kullback-Leibler Divergence Parameters: KLD(A, B)-Kullback-Leibler divergence of matrix A and B, Matrix A-sparsity parameter, Matrix B-average activation of each hidden neuron, Nc()-number of columns, Nr()-number of rows, Array Sc()-the sum of column's values on each row; C = repeat copies of Sc(A) into a 1-by-Nc(A) block arrangement; D = repeat copies of Sc(B) into a 1-by-Nc(B) block arrangement; E = repeat copies of B into a Nr(A)-by-1 block arrangement; if Nr(B) == 1 B = element-wise divide B by the sum of B; A = element-wise divide A by C; F = element-wise multiply A by log(E); Replacing the not a number element in F by 0; KLD(A,B)=Sc(F); else if Nr(B) == Nr(A) B = element-wise divide B by D; A = element-wise divide A by C; G = element-wise divide A by B; F = element-wise multiply A by log(G); Replacing the not a number element in F by 0; KLD(A,B)=Sc(F); end;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Development of the optimized structure of the SAE-LM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Relationship between accuracy rate and computational time for a different number of autoencoders. The traffic forecasting model is trained with a 120-min sampling time, the SoftSign activation function for all hidden layers, and ten hidden nodes. The computational time is measured on a 3.40-GHz Intel i7 CPU, 64-b operating system, and 16.0-GM RAM.</figDesc><graphic coords="5,338.75,70.61,195.74,120.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5 ), the training set of the collected traffic data in the traffic data sets (R1a, R1b, R2a, and R2b) is used to develop the SAE-LM model. The results of three random days (March 24, 2014; March 27, 2014; and March 28, 2014) and the average results of the testing set with respect to the 16 main trials are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance evaluation of SAE-LM, EXP-LM, PSONN, and RBFNN traffic flow forecasting predictors for the R3a data set. The value of traffic flow is represented by the vehicle/minute, and the time index starts at 00:00 on March 24, 2014 and ends at 24:00 on March 28, 2014. Since the performance of each model for R3a is similar to R3b, we display only the forecasting results for R3a.</figDesc><graphic coords="8,50.51,313.37,255.14,119.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Percentage error of traffic flow forecasting results for (a) R3a and (b) R3b. The percentage error is calculated without absolute value. The ovals in (b) indicate the time periods when the forecasting result of SAE-LM has a larger percentage error than the other models. The closer the percentage value to 0%, the less the difference between the predicted and actual values.</figDesc><graphic coords="9,50.51,58.97,255.14,119.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) and (b), the percentage error value of the SAE-LM model is closer to 0% compared with those of the other models in almost all time periods. Only in a few time periods, especially those shown by the ovals in Fig. 5(b) (12:00 to 16:00 on March 26-28, 2014), the SAE-LM model has the worst percentage error result of all four traffic predictors. The raw traffic data in R3b shows a highly smooth distribution between 08:00 to 12:00 on March 26-28, 2014. Since the SAE-LM model uses a 240-min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DESIGN</head><label>I</label><figDesc>FACTORS FOR OPTIMIZING THE STRUCTURE</figDesc><table /><note><p>OF THE PROPOSED SAE-LM MODEL forecasting model is formulated as</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ORTHOGONAL</head><label>II</label><figDesc>ARRAY L 16(4 5 ) AND EXPERIMENTAL RESULTSTABLE III EFFECTS AND THE SENSITIVITY OF EACH DESIGN FACTORdesign factor i are always the primary factors in the proposed SAE-LM forecasting model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV FORECASTING</head><label>IV</label><figDesc>ACCURACY OF SAE-LM, EXP-LM, PSONN, AND RBFNN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPUTATIONAL</head><label>V</label><figDesc>TIME COMPARISON IN SECONDS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>YANG et al.: OPTIMIZED STRUCTURE OF THE TRAFFIC FLOW FORECASTING MODEL</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Australia Research Linkage Grants Scheme.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hao-Fan Yang received the master's degree in information technology (software architecture) from the Queensland University of Technology, Brisbane, QLD, Australia, in 2010. He is currently pursuing the Ph.D. degree with La Trobe University, Melbourne, VIC, Australia.</p><p>He has been undertaking research on data mining to more efficiently improve the performance of prediction models in many areas. His current research interests include short-term traffic forecasting and real-time congestion management. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A dynamic traffic assignment model for highly congested urban networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ben-Akiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. C, Emerg. Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="62" to="82" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The evolution of urban traffic control: Changing policy and technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Waterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cherrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Snell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Planning Technol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data mining with big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selection of significant on-road sensor data for short-term traffic flow forecasting using the Taguchi method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khadem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Informat</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="266" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel-based naive Bayes classifier for breast cancer prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biol. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-discounted event detection in sports video</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. A, Syst., Humans</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1009" to="1024" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fuzzy rule-based system approach to combining traffic count forecasts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlaftis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Rec., J. Transp. Res. Board</title>
		<imprint>
			<biblScope unit="volume">2183</biblScope>
			<biblScope unit="page" from="120" to="128" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prediction of short-term traffic variables using intelligent swarm-based neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Control Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantum polarized ligand docking investigation to understand the significance of protonation states in histone deacetylase inhibitors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalyaanamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Molecular Graph. Model</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Remote Sensing and Image Interpretation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lillesand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chipman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introductory Remote Sensing Principles and Concepts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Routledge</publisher>
			<pubPlace>Evanston, IL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural network modeling for big data weather forecasting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Granularity, Big Data, and Computational Intelligence</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="389" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Orton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A real-time freeway network traffic surveillance tool</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Messmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Control Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimized and meta-optimized neural networks for short-term traffic flow prediction: A genetic approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Vlahogianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Karlaftis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Golias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. C, Emerg. Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="234" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Testing and comparing neural network and statistical approaches for predicting transportation time series</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Vlahogianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Karlaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Rec., J. Transp. Res. Board</title>
		<imprint>
			<biblScope unit="volume">2399</biblScope>
			<biblScope unit="page" from="9" to="22" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural network architectures and learning algorithms</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Ind. Electron. Mag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural-network-based models for short-term traffic flow forecasting using a hybrid exponential smoothing and Levenberg-Marquardt algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Development of neural network based traffic flow predictors using pre-processed data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K F</forename><surname>Yiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization and Control Methods in Industrial Engineering and Construction</title>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved computation for Levenberg-Marquardt training</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wilamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="930" to="937" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">G.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Control</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic prediction of traffic volume through Kalman filtering theory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Okutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Stephanedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. B, Methodol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel genetic-algorithm-based neural network for short-term load forecasting</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H F</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K S</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="793" to="799" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Traffic volume forecasting based on radial basis function neural network with the consideration of traffic flows at the adjacent intersections</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. C, Emerg. Technol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive computation algorithm for RBF neural network</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="342" to="347" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yokoyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Taguchi Methods: Design of Experiments</title>
		<title level="s">Taguchi Methods Series</title>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CS294A Lect. Notes</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning: Methods and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the optimal number of hidden nodes in a neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Auda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Can. Conf. Elect. Comput. Eng</title>
		<meeting>IEEE Can. Conf. Elect. Comput. Eng</meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="918" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Solving local minima problem with large number of hidden nodes on two-layered feed-forward artificial neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="3640" to="3643" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Traffic flow control using neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Traffic</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="50" to="52" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Quality Engineering Using Robust Design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Phadke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient parallel framework for HEVC motion estimation on many-core processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2077" to="2089" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A highly parallel framework for HEVC coding unit partitioning tree decision on many-core processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="573" to="576" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
